{"pageProps":{"questions":[{"id":"08ZRAoolKppCNbcuvO4J","question_id":211,"answers_community":["B (56%)","A (22%)","C (22%)"],"answer_images":[],"answer_ET":"B","unix_timestamp":1729959300,"exam_id":13,"question_text":"You developed a BigQuery ML linear regressor model by using a training dataset stored in a BigQuery table. New data is added to the table every minute. You are using Cloud Scheduler and Vertex AI Pipelines to automate hourly model training, and use the model for direct inference. The feature preprocessing logic includes quantile bucketization and MinMax scaling on data received in the last hour. You want to minimize storage and computational overhead. What should you do?","answer":"B","answer_description":"","discussion":[{"comment_id":"1362308","content":"Selected Answer: B\nB is the right solution.\nKeep in mind that it is asking for a solution where you \"minimize storage and computational overhead\". You end up storing more data with A and D. While in C you create more computational overhead. All solutions would work perfectly fine, but B matches best with the requirements in the question.","timestamp":"1740605580.0","poster":"Wuthuong1234","upvote_count":"1"},{"comment_id":"1332391","upvote_count":"1","poster":"Ankit267","content":"Selected Answer: B\nBQ is sufficient","timestamp":"1735302480.0"},{"poster":"AB_C","content":"Selected Answer: A\nWhile the TRANSFORM clause can perform preprocessing, it's applied during model creation, not for inference. You'll need to recalculate statistics for each inference request, increasing computational overhead.","timestamp":"1732720620.0","upvote_count":"1","comments":[{"content":"This is wrong\nThis tutorial introduces data analysts to BigQuery ML. BigQuery ML enables users to create and execute machine learning models in BigQuery using SQL queries. This tutorial introduces feature engineering by using the TRANSFORM clause. Using the TRANSFORM clause, you can specify all preprocessing during model creation. The preprocessing is automatically applied during the prediction and evaluation phases of machine learning.\n\nhttps://cloud.google.com/bigquery/docs/bigqueryml-transform","timestamp":"1733820900.0","upvote_count":"1","poster":"Omi_04040","comment_id":"1324455"}],"comment_id":"1318742"},{"comment_id":"1317862","poster":"shubhachandra","upvote_count":"2","timestamp":"1732585860.0","content":"Selected Answer: B\nThe TRANSFORM clause in BigQuery ML allows you to directly define feature preprocessing logic (such as quantile bucketization and MinMax scaling) within the SQL query itself. This approach minimizes storage and computational overhead because:\n\nNo additional storage: Statistics for preprocessing are calculated on-the-fly during model training and inference, without needing to store preprocessed data or statistics separately.\nIntegrated workflow: The preprocessing logic is tightly coupled with the model creation process, ensuring consistency between training and inference without external dependencies."},{"upvote_count":"1","comment_id":"1316799","timestamp":"1732394400.0","content":"Selected Answer: B\nB is the best option because: \n1) TRANSFORM saves processing, storage and computation by performing feature preprocessing directly within the CREATE MODEL.\n2) This method integrates preprocessing with model training, streamlining the entire process.","poster":"lunalongo"},{"poster":"f084277","upvote_count":"2","comment_id":"1312769","content":"Selected Answer: C\nDocs say BQ is not suitable for full-pass transformations such as Minmax.","timestamp":"1731695760.0"},{"comment_id":"1303328","upvote_count":"1","timestamp":"1729959300.0","poster":"carolctech","content":"Selected Answer: A\nA) Preprocessing and staging the data in BigQuery before training and inference, is the most efficient approach because: \n1) You can use BQ’s optimized processing by preprocessing data before training\n2) Avoiding redundant calculations, by directly using the preprocessed data (already bucketized and scaled) for training and inference; \n3) Reducing storage by keeping only preprocessed data, not raw data and statistics separately."}],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/150320-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"choices":{"B":"Use the TRANSFORM clause in the CREATE MODEL statement in the SQL query to calculate the required statistics.","A":"Preprocess and stage the data in BigQuery prior to feeding it to the model during training and inference.","D":"Create SQL queries to calculate and store the required statistics in separate BigQuery tables that are referenced in the CREATE MODEL statement.","C":"Create a component in the Vertex AI Pipelines directed acyclic graph (DAG) to calculate the required statistics, and pass the statistics on to subsequent components."},"topic":"1","timestamp":"2024-10-26 18:15:00"},{"id":"gXSShPyXXKxvuk2TNiW5","answers_community":["B (70%)","D (30%)"],"question_text":"You have trained a model on a dataset that required computationally expensive preprocessing operations. You need to execute the same preprocessing at prediction time. You deployed the model on AI Platform for high-throughput online prediction. Which architecture should you use?","answer":"B","exam_id":13,"question_id":212,"unix_timestamp":1623265200,"url":"https://www.examtopics.com/discussions/google/view/55005-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"poster":"SparkExpedition","timestamp":"1626222180.0","upvote_count":"31","comment_id":"405764","content":"Supporting B ..https://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt1#where_to_do_preprocessing"},{"timestamp":"1623265200.0","comments":[{"content":"I also agree with B, this is how I would advise clients to do it as well","timestamp":"1632316680.0","upvote_count":"4","poster":"q4exam","comment_id":"449528"}],"poster":"inder0007","content":"I think it should b B","comment_id":"378489","upvote_count":"14"},{"timestamp":"1740088560.0","upvote_count":"1","content":"Selected Answer: B\nDataflow has autoscale. And in my experience, you use Cloud Functions to small stuff.","poster":"IrribarraC","comment_id":"1359482"},{"timestamp":"1735229880.0","upvote_count":"1","poster":"ship123","content":"Selected Answer: D\nYou are an ML engineer who has trained a model on a dataset that required computationally expensive preprocessing operations. You need to execute the same preprocessing at prediction time. You deployed the model on the Vertex AI\nplatform for high‐throughput online prediction. Which architecture should you use?\n\nAnswer is . Send incoming prediction requests to a Pub/Sub topic. Set up a Cloud Function that is triggered when messages are published to the Pub/Sub topic. Implement your\npreprocessing logic in the Cloud Function. Submit a prediction request to the Vertex AI platform using the transformed data. Write the predictions to an outbound Pub/Sub queue.","comment_id":"1331986"},{"poster":"rajshiv","upvote_count":"1","comment_id":"1322988","content":"Selected Answer: D\nB is incorrect. Dataflow is a great option for large-scale data processing but may introduce additional complexity and overhead for a real-time prediction scenario where you just need to preprocess data on-the-fly. This is more appropriate for batch processing or when large volumes of data need to be processed in parallel. \nOption D is better as it leverages Pub/Sub, Cloud Functions, and AI Platform to preprocess data and obtain predictions without needing complex infrastructure or additional systems like Dataflow or Cloud Spanner.","timestamp":"1733539140.0"},{"content":"Selected Answer: B\nDataflow is superior to Cloud Functions for doing data transformations at high volume. The answer is clearly B.","upvote_count":"2","timestamp":"1731381780.0","poster":"f084277","comment_id":"1310418"},{"comments":[{"timestamp":"1731381720.0","poster":"f084277","comment_id":"1310416","upvote_count":"1","content":"You are incorrect. Dataflow can handle MUCH higher volumes of data than Cloud Functions"},{"timestamp":"1729534080.0","upvote_count":"1","poster":"desertlotus1211","comment_id":"1301212","content":"Dataflow is ideal for handling computationally expensive preprocessing operations, as it scales automatically and can process the data in a distributed manner."}],"content":"Selected Answer: D\nD. The issue with B is that DataFlow does not work well with high throughput","timestamp":"1719469020.0","upvote_count":"1","poster":"bludw","comment_id":"1237905"},{"upvote_count":"1","comment_id":"1225537","poster":"PhilipKoku","content":"Selected Answer: B\nB) Pub/Sub + Dataflow","timestamp":"1717680300.0"},{"content":"Selected Answer: B\nWent with B, using dataflow for large amount data transformation is the best option","upvote_count":"3","timestamp":"1688735700.0","comment_id":"945707","poster":"Liting"},{"comment_id":"945600","poster":"SamuelTsch","content":"Selected Answer: B\nI went to B. \nA is completely wrong. C: 1st cloud spanner is not designed for high throughput, also it is not for preprocessing. D: cloud function could not be get enough resource to do the high computational transformation.","upvote_count":"2","timestamp":"1688730480.0"},{"content":"Selected Answer: B\nBecause the concern here is high throughput and not specifically the latency so better to go with option B","comment_id":"920184","poster":"ashu381","timestamp":"1686415200.0","upvote_count":"1"},{"poster":"Voyager2","upvote_count":"1","comment_id":"910465","content":"Selected Answer: D\nB. Send incoming prediction requests to a Pub/Sub topic. Transform the incoming data using a Dataflow job. Submit a prediction request to AI Platform using the transformed data. Write the predictions to an outbound Pub/Sub queue\nhttps://dataintegration.info/building-streaming-data-pipelines-on-google-cloud","timestamp":"1685466540.0"},{"upvote_count":"1","comment_id":"892707","content":"Selected Answer: B\nWent with B","timestamp":"1683608520.0","poster":"M25"},{"content":"Selected Answer: D\nI think it's D as B is not a good choice because it requires you to run a Dataflow job for each prediction request. This is inefficient and can lead to latency issues.","comment_id":"882313","poster":"e707","comments":[{"comment_id":"883175","upvote_count":"2","timestamp":"1682648040.0","content":"Yes i agree Dataflow can introduce latency","poster":"lucaluca1982"}],"timestamp":"1682578560.0","upvote_count":"3"},{"content":"Selected Answer: D\nI go for D. Option B has Dataflow that it is more suitable for batch","timestamp":"1681346580.0","poster":"lucaluca1982","upvote_count":"1","comment_id":"868955"},{"comment_id":"849302","upvote_count":"1","content":"Selected Answer: B\nIt's B","poster":"SergioRubiano","timestamp":"1679663520.0"},{"timestamp":"1671442500.0","poster":"MithunDesai","comment_id":"749676","upvote_count":"1","content":"Selected Answer: B\nyes ans B"},{"comment_id":"738929","upvote_count":"1","content":"Selected Answer: B\nB\nPubsub + DataFlow + Vertex AI (AI Platform)","timestamp":"1670497560.0","poster":"hiromi"},{"timestamp":"1660138560.0","poster":"suresh_vn","content":"Selected Answer: B\nShould be B. Dataflow is BEST option for preprocessing training , testing data both","comment_id":"645003","upvote_count":"2"},{"poster":"sachinxshrivastav","comment_id":"643215","timestamp":"1659766080.0","upvote_count":"1","content":"Selected Answer: B\nAnswer should be B"},{"upvote_count":"2","timestamp":"1655137380.0","poster":"Mohamed_Mossad","content":"Selected Answer: B\n- using options eliminatios , A totally wrong , D also not valid as cloud functions is not sutiable for heavy data workflows\n- answer between B,D will vote for B as dataflow is the best solution while dealing with heavy data workflows","comment_id":"615852"},{"upvote_count":"1","poster":"gcp2021go","comments":[{"comments":[{"upvote_count":"1","content":"I understand that, but question about the \"for high-throughput online prediction\", is it dataflow more suitable for online?","timestamp":"1641843000.0","comment_id":"521081","poster":"fdmenendez"}],"content":"Because, most of the time where you need to execute a full transformation pipeline and you have a comparison between dataflow and cloud function it's recommended to go with dataflow. It's a solution more prepared to solve those cases.","poster":"kaike_reis","comment_id":"477208","timestamp":"1636759080.0","upvote_count":"2"},{"poster":"Grkrish2002","comment_id":"495555","timestamp":"1638845640.0","content":"Computationally expensive is the keyword. Cloud function will not be suitable for these kind of preprocessing workloads","upvote_count":"5"}],"comment_id":"467157","content":"Why not D?","timestamp":"1635114540.0"}],"answer_images":[],"isMC":true,"answer_description":"","topic":"1","answer_ET":"B","timestamp":"2021-06-09 21:00:00","question_images":[],"choices":{"C":"Stream incoming prediction request data into Cloud Spanner. Create a view to abstract your preprocessing logic. Query the view every second for new records. Submit a prediction request to AI Platform using the transformed data. Write the predictions to an outbound Pub/Sub queue.","A":"Validate the accuracy of the model that you trained on preprocessed data. Create a new model that uses the raw data and is available in real time. Deploy the new model onto AI Platform for online prediction.","B":"Send incoming prediction requests to a Pub/Sub topic. Transform the incoming data using a Dataflow job. Submit a prediction request to AI Platform using the transformed data. Write the predictions to an outbound Pub/Sub queue.","D":"Send incoming prediction requests to a Pub/Sub topic. Set up a Cloud Function that is triggered when messages are published to the Pub/Sub topic. Implement your preprocessing logic in the Cloud Function. Submit a prediction request to AI Platform using the transformed data. Write the predictions to an outbound Pub/Sub queue."}},{"id":"9lrgFO8Jnxxmd2pv8avL","answer":"A","question_id":213,"answers_community":["A (67%)","D (33%)"],"choices":{"B":"Run two separate hypertuning jobs, a linear regression job for 50 trials, and a DNN job for 50 trials. Compare their final performance on a common validation set, and select the set of hyperparameters with the least training loss.","D":"Run one hypertuning job for 100 trials. Set num_hidden_layers and learning_rate as conditional hyperparameters based on their parent hyperparameter training_method.","C":"Run one hypertuning job with training_method as the hyperparameter for 50 trials. Select the architecture with the lowest training loss, and further hypertune it and its corresponding hyperparameters tor 50 trials.","A":"Run one hypertuning job for 100 trials. Set num_hidden_layers as a conditional hyperparameter based on its parent hyperparameter training_method, and set learning_rate as a non-conditional hyperparameter."},"question_images":[],"isMC":true,"answer_ET":"A","timestamp":"2024-10-25 19:52:00","url":"https://www.examtopics.com/discussions/google/view/150251-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"upvote_count":"1","poster":"kaneup","comment_id":"1410226","content":"Selected Answer: D\nthis is D","timestamp":"1742953800.0"},{"poster":"River3000","content":"This should be D, as the question stated that 'linear regression and deep neural network (DNN), within the same module', This typically means that even the linear regression model is trained using gradient-based optimization (such as SGD or Adam), rather than using a closed-form solution.\n\nSo, the phrase \"within the same module\" implies that the linear model also relies on gradient descent, and thus the learning_rate parameter is applicable for training both models—even though the DNN additionally uses the num_hidden_layers parameter for its architecture.","upvote_count":"1","timestamp":"1741985640.0","comment_id":"1395720"},{"content":"Selected Answer: A\nA & D for obvious reasons.\nWhy A ? DNN with 1 num_hidden_layer is equivalent to linear regression model therefore num_hidden_layer is conditional, though learning_rate can be hyperparametrized for both DNN( one hidden layer i.e. linear regression & >1 hidden layer i.e. DNN).\nTherefore A is the right answer","upvote_count":"1","comment_id":"1332395","timestamp":"1735302840.0","poster":"Ankit267"},{"poster":"Pau1234","comment_id":"1327021","timestamp":"1734288900.0","content":"Selected Answer: A\nAgree with Omi_04040. num_hidden_layers is only relevant to the DNN model and not the linear regression model, according to the documentation","upvote_count":"2"},{"poster":"Omi_04040","timestamp":"1733827080.0","comment_id":"1324494","content":"Selected Answer: A\nAnswer is A since 'learning rate' cannot be shared \nThis question is a literal spinoff from this paragraph\n\nhttps://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview#conditional_hyperparameters","upvote_count":"2"},{"timestamp":"1733574060.0","content":"Selected Answer: D\nA is incorrect because both num_hidden_layers and learning_rate are hyperparameters specific to the DNN model. Since both hyperparameters need to be conditional on training_method being DNN, making only one of them conditional is not sufficient. The problem has two model architectures: linear regression and DNN. Depending on the model architecture, the hyperparameters change: 1) For DNN, the hyperparameters are num_hidden_layers and learning_rate while 2) For linear regression, these hyperparameters are not relevant. Hence I vote D.","comment_id":"1323095","upvote_count":"2","poster":"rajshiv"},{"upvote_count":"3","comment_id":"1316795","timestamp":"1732393560.0","poster":"lunalongo","content":"A is the best option because: \n\nRunning one single job with conditional logics added to hyperparameters settings avoids unnecessary computing usage and comparison efforts. \nOnly num_hidden_layers needs to be set as a conditional hyperparameter under training_method; no explicit conditional logic is needed for learning_rate -- the latter is intelligently ignored by Vertex AI when linear regression is the training_method.\n\nB, C and D are less suitable because B and C run 2 separate jobs; D runs only one job, but it's hyperparameter tuning strategy adds redundant processing, even if it's true that the learning_rate is irrelevant for linear regression methods.\n\nThe underlying logics behind it:\nAs a STRUCTURAL hyperparameter, num_hidden_layers is intrinsically tied to the DNN's architecture definition.\nAs a TRAINING hyperparameter, learning_rate is linked to the training process, not directly tied to the architecture definition."},{"timestamp":"1729957440.0","comment_id":"1303316","content":"Selected Answer: A\nThe best approach is A and here's why:\n\nThe use of the 100 trials in a single job by using conditional hyperparameters maximizes budget efficiency. \n\nThe number of hidden layers should be conditional, because it is relevant only for NON-LINEAR models like neural networks (which is DNN's case) and not for linear models -- where hidden layers don't exist.\n\nLearning rate is relevant for both models, unless the question stated that the regression model used a closed-form solution, not a gradient-based optimization method.","poster":"carolctech","upvote_count":"3"},{"timestamp":"1729878720.0","upvote_count":"1","poster":"JDpmle2024","comment_id":"1302969","content":"Selected Answer: D\nThis would allow you to first set the type of job, and only after that any other parameters. So first, select training_method. If training_method is DNN, then you specify the other parameters."}],"topic":"1","exam_id":13,"unix_timestamp":1729878720,"question_text":"You developed a Python module by using Keras to train a regression model. You developed two model architectures, linear regression and deep neural network (DNN), within the same module. You are using the training_method argument to select one of the two methods, and you are using the learning_rate and num_hidden_layers arguments in the DNN. You plan to use Vertex AI's hypertuning service with a budget to perform 100 trials. You want to identify the model architecture and hyperparameter values that minimize training loss and maximize model performance. What should you do?","answer_images":[],"answer_description":""},{"id":"IWFs8p1OsGOi4doufgFL","question_images":[],"question_id":214,"answer":"D","answers_community":["D (88%)","13%"],"question_text":"You work for a hospital. You received approval to collect the necessary patient data, and you trained a Vertex AI tabular AutoML model that calculates patients' risk score for hospital admission. You deployed the model. However, you're concerned that patient demographics might change over time and alter the feature interactions and impact prediction accuracy. You want to be alerted if feature interactions change, and you want to understand the importance of the features for the predictions. You want your alerting approach to minimize cost. What should you do?","topic":"1","timestamp":"2024-10-25 19:58:00","discussion":[{"comment_id":"1324500","upvote_count":"3","content":"Selected Answer: D\nspecifically concerned about changes in feature interactions and their impact on predictions. Feature attribution drift monitoring directly addresses this by tracking how the importance of different features (and their interactions) changes over time.   \n\nhttps://cloud.google.com/vertex-ai/docs/model-monitoring/monitor-explainable-ai","poster":"Omi_04040","timestamp":"1733829300.0"},{"timestamp":"1732796640.0","comment_id":"1319220","poster":"e821027","upvote_count":"2","content":"Selected Answer: D\nBut the interest is also to understand the importance of features for the predictions."},{"comment_id":"1318751","poster":"AB_C","content":"Selected Answer: D\nWhy other options are less suitable:\n\nA and B (Feature Drift Monitoring): While basic feature drift monitoring can detect changes in feature distributions, it doesn't directly address your concern about changes in feature interactions and their impact on predictions.\nC (Sampling Rate of 1): Analyzing 100% of the prediction requests for feature attribution drift can be expensive, especially if you have high traffic.","timestamp":"1732721100.0","upvote_count":"2"},{"poster":"JDpmle2024","comment_id":"1302972","timestamp":"1729879080.0","content":"Selected Answer: B\nThis is feature drift (features are changing) and not feature attribution drift (features are having different effects on the prediction).","upvote_count":"1"}],"isMC":true,"exam_id":13,"answer_ET":"D","answer_images":[],"choices":{"A":"Create a feature drift monitoring job. Set the sampling rate to 1 and the monitoring frequency to weekly.","D":"Create a feature attribution drift monitoring job. Set the sampling rate to 0.1 and the monitoring frequency to weekly.","C":"Create a feature attribution drift monitoring job. Set the sampling rate to 1 and the monitoring frequency to weekly.","B":"Create a feature drift monitoring job. Set the sampling rate to 0.1 and the monitoring frequency to weekly."},"unix_timestamp":1729879080,"url":"https://www.examtopics.com/discussions/google/view/150253-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":""},{"id":"h5zHPLew8TFpRuM0KKeG","unix_timestamp":1732721340,"answer":"B","topic":"1","answer_description":"","answer_ET":"B","answer_images":[],"question_images":[],"choices":{"D":"Run the TFX pipeline in Dataflow by using the Apache Beam TFX orchestrator. Set the appropriate Vertex AI permissions in the job to publish metadata in Vertex AI.","A":"Run the TFX pipeline in Vertex AI Pipelines. Configure the pipeline to use Vertex AI Training jobs with distributed processing.","C":"Run the TFX pipeline in Dataproc by using the Apache Beam TFX orchestrator. Set the appropriate Vertex AI permissions in the job to publish metadata in Vertex AI.","B":"Run the TFX pipeline in Vertex AI Pipelines. Set the appropriate Apache Beam parameters in the pipeline to run the data preprocessing steps in Dataflow."},"answers_community":["B (100%)"],"isMC":true,"question_text":"You are developing a TensorFlow Extended (TFX) pipeline with standard TFX components. The pipeline includes data preprocessing steps. After the pipeline is deployed to production, it will process up to 100 TB of data stored in BigQuery. You need the data preprocessing steps to scale efficiently, publish metrics and parameters to Vertex AI Experiments, and track artifacts by using Vertex ML Metadata. How should you configure the pipeline run?","question_id":215,"exam_id":13,"timestamp":"2024-11-27 16:29:00","url":"https://www.examtopics.com/discussions/google/view/152152-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"comment_id":"1318756","poster":"AB_C","content":"Selected Answer: B\nA (Vertex AI Training jobs): While Vertex AI Training jobs are useful for model training, they are not the primary way to scale data preprocessing within a TFX pipeline.\nC and D (Dataproc and Dataflow with Apache Beam TFX orchestrator): While you can run TFX pipelines on Dataproc or Dataflow directly, using Vertex AI Pipelines as the orchestrator provides better integration with Vertex AI services and simplifies metadata tracking and experiment management.","upvote_count":"4","timestamp":"1732721340.0"}]}],"exam":{"isMCOnly":true,"isBeta":false,"id":13,"isImplemented":true,"name":"Professional Machine Learning Engineer","provider":"Google","numberOfQuestions":304,"lastUpdated":"11 Apr 2025"},"currentPage":43},"__N_SSP":true}