{"pageProps":{"questions":[{"id":"SJw7gg8DQZ60fNz9CEQm","answer_images":[],"topic":"1","choices":{"B":"Read replicas","D":"Automated backups","E":"Semisynchronous replication","C":"Binary logging","A":"Sharding"},"question_id":11,"discussion":[{"poster":"kopper2019","content":"Ans) C and D\nCloud SQL. If you use Cloud SQL, the fully managed Google Cloud MySQL database, you should enable automated backups and binary logging for your Cloud SQL instances. This allows you to perform a point-in-time recovery, which restores your database from a backup and recovers it to a fresh Cloud SQL instance","comments":[{"timestamp":"1682846880.0","content":"And: a read-replica won't help against \"catastrophic failures\" like accidental deletions","poster":"HenkH","comments":[{"upvote_count":"4","poster":"RVivek","timestamp":"1691445180.0","comment_id":"801544","content":"catastrophic failure means disaster like a zonal datacenter level failure or regional failure"}],"comment_id":"708374","upvote_count":"5"}],"timestamp":"1641077520.0","upvote_count":"38","comment_id":"396341"},{"comment_id":"403708","upvote_count":"11","poster":"victory108","content":"C. Binary logging\nD. Automated backups","timestamp":"1641887700.0"},{"content":"Selected Answer: CD\nBinary logging and automated backups","timestamp":"1735932540.0","upvote_count":"1","poster":"plumbig11","comment_id":"1336130"},{"comment_id":"1107389","poster":"[Removed]","timestamp":"1719540960.0","content":"CD\n\nBinary Logging: Binary logging in MySQL records changes to the database. It can be used for backup and replication, and it's essential for point-in-time recovery. With binary logging, you can roll your database forward to any point in time, minimizing data loss.\n\nAutomated Backups: Automated backups periodically take a snapshot of your database. In the event of a catastrophic failure, you can restore your database to the state it was in at the time of the last backup. This can also help minimize data loss.\n\nWhile read replicas and semisynchronous replication can enhance availability and performance, they do not directly minimize data loss.\nAlso, you cannot create a read replica without enabling Automated backups and Enable binary logging\n\nSharding can improve performance but it's not directly aimed at data loss prevention.\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups","upvote_count":"4"},{"comment_id":"1073973","timestamp":"1716027540.0","poster":"odacir","content":"Selected Answer: CD\nPrerequisites for creating a read replica\n\nBefore you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:\n\n Automated backups must be enabled.\n Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these logs.\n At least one backup must have been created after binary logging was enabled.\nhttps://cloud.google.com/sql/docs/mysql/replication#requirements","upvote_count":"5"},{"content":"CD\nBefore being able to create a read replica, you have to make sure \"binary logging and automated backup\" are enabled. So picking only D or C without the other one makes no sense.\nhttps://cloud.google.com/sql/docs/mysql/replication/create-replica","poster":"someone2011","timestamp":"1710332940.0","comment_id":"1006472","upvote_count":"1"},{"timestamp":"1709685840.0","content":"The correct answers are C. & D.\n\n\nBinary logging is a feature of MySQL that records all changes made to the database. This log can be used to restore the database to a previous state in case of a failure.\n\nAutomated backups are regularly scheduled backups of the database. They are the most reliable way to ensure that data is not lost in case of a catastrophic failure.","comment_id":"999976","upvote_count":"4","poster":"HRS1954"},{"content":"C and D. Read replicas wont work in this case.","upvote_count":"1","poster":"heretolearnazure","timestamp":"1708872240.0","comment_id":"990067"},{"comment_id":"935599","upvote_count":"1","timestamp":"1703702820.0","poster":"FaizAhmed","content":"C. Binary logging\nD. Automated backups"},{"content":"Selected Answer: CD\nBinary logging records changes to the data, which can help you recover data and minimize data loss during an unexpected failure. Automated backups create regular backups of your database, allowing you to restore the database to a specific point in time in case of a catastrophic failure.","timestamp":"1696062060.0","upvote_count":"3","comment_id":"856779","poster":"taer"},{"comment_id":"826898","timestamp":"1693651440.0","upvote_count":"1","poster":"abbottWang","content":"Selected Answer: CD\nbackup data automatically"},{"poster":"telp","comments":[{"comment_id":"877142","poster":"medi01","content":"So you are going with a SINGLE instance of MySQL for a critical business application.","upvote_count":"1","timestamp":"1697962920.0"}],"content":"CD => the answer B is for performance issue. The question focus on data loss prevention.","comment_id":"823897","upvote_count":"1","timestamp":"1693143240.0"},{"timestamp":"1690634100.0","comment_id":"791723","comments":[{"poster":"r1ck","upvote_count":"2","timestamp":"1692385260.0","comment_id":"813505","content":"sure, binary logging starts Automatically upon configuring read-replica?? \n- Don't think so,\nhttps://cloud.google.com/sql/docs/mysql/replication/create-replica"}],"upvote_count":"3","content":"Selected Answer: BD\nB and D:\nNo need to explain D, but B... here is why\nWhen you set up a read replica, automaticaly binary logging is activated. Then, in case of desaster, you can promote manually a read replica and it will have all data before the desaster occurs.","poster":"okixavi"},{"comments":[{"timestamp":"1699449060.0","content":"Yes, you are correct that creating a read replica requires binary logging to be enabled on the primary instance. However, the purpose of a read replica is to scale read traffic and offload it from the primary instance, not to prevent data loss in case of catastrophic failure. While enabling binary logging is a requirement for creating a read replica, it is not the primary purpose of a read replica. IMO the two features that should be implemented to ensure minimum data loss in case of catastrophic failure are Binary logging and Automated backups.","comment_id":"892051","poster":"mmathiou","upvote_count":"3"}],"content":"B and D are correct answers as per below reference,\n1. Before you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:\n\nAutomated backups must be enabled.\n\n2. Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these logs.\n\n3. At least one backup must have been created after binary logging was enabled.\n\n\n\nIt means creating read replica already covers binary logging.\n\n\n\nPlease read the following references for more information\n\nhttps://cloud.google.com/solutions/cloud-sql-mysql-disaster-recovery-complete-failover-fallback\n\nhttps://medium.com/google-cloud/cloud-sql-recovering-from-regional-failure-in-10-minutes-or-less-mysql-fc055540a8f0\n\nReplication in Cloud SQL | Cloud SQL for MySQL | Google Cloud","timestamp":"1690595580.0","upvote_count":"1","comment_id":"791374","poster":"Jeena345"},{"comment_id":"763487","upvote_count":"2","poster":"examch","content":"Selected Answer: CD\nC and D are correct answers,\n\nBackups help you restore lost data to your Cloud SQL instance. Additionally, if an instance is having a problem, you can restore it to a previous state by using the backup to overwrite it. Enable automated backups for any instance that contains necessary data. Backups protect your data from loss or damage.\n\nEnabling automated backups, along with binary logging, is also required for some operations, such as clone and replica creation.\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#what_backups_provide","timestamp":"1688264580.0"},{"comments":[{"upvote_count":"2","content":"Option B, read replicas, is not a recommended approach. Read replicas are copies of a database that can be used to offload read traffic from the primary database. While read replicas can improve the performance of a database, they are not specifically designed to protect against data loss in case of catastrophic failure.\n\nOption E, semisynchronous replication, is not a recommended approach. Semisynchronous replication is a method of replicating data between a primary database and one or more secondary databases. While semisynchronous replication can help to ensure that data is replicated quickly and accurately, it is not specifically designed to protect against data loss in case of catastrophic failure.","timestamp":"1687425480.0","comment_id":"753217","poster":"omermahgoub"}],"poster":"omermahgoub","timestamp":"1687425480.0","content":"C. Binary logging\nD. Automated backups\n\nBinary logging is a feature of MySQL that records all changes made to the database in a binary log file. By enabling binary logging on your Cloud SQL instance, you can use the log file to recover your database in case of catastrophic failure.\n\nAutomated backups are a feature of Cloud SQL that allows you to automatically create and retain backups of your database. By enabling automated backups, you can restore your database in case of catastrophic failure or other data loss events.\n\nOption A, sharding, is not a recommended approach. Sharding is a technique for distributing data across multiple servers to improve performance and scalability. While sharding can help to improve the performance of a database, it is not specifically designed to protect against data loss in case of catastrophic failure.","upvote_count":"4","comment_id":"753216"},{"timestamp":"1683892080.0","content":"Selected Answer: CD\ncd is ok","poster":"megumin","comment_id":"716710","upvote_count":"1"},{"timestamp":"1682985000.0","upvote_count":"1","comment_id":"709546","poster":"diasporabro","content":"Selected Answer: CD\nI see Read Replicas as more of a performance thing, than DR thing"},{"timestamp":"1681724100.0","content":"Selected Answer: BD\nBD\nWhen you create a new instance in the Google Cloud console, both Automated backups and Enable point-in-time recovery are automatically enabled. Point-in-time recovery uses binary logs. \nTherefore the best protection will be given by Automated Backup and a Read Replica in another zone.\nUsually a catastrophic failure assume loss of a datacentre or even a zone. Backup images are distributed on different zones but can take time to restore. Replicas can be accessed faster.","comment_id":"697237","poster":"minmin2020","comments":[{"poster":"fiercedog","comment_id":"743879","timestamp":"1686644340.0","content":"If you use cloud SDK,you need to specify the command option: https://cloud.google.com/sdk/gcloud/reference/sql/instances/create#--enable-point-in-time-recovery","upvote_count":"1"}],"upvote_count":"3"},{"comment_id":"672547","content":"Binary logging and Automated backups will help .. C, D is right","upvote_count":"2","poster":"AzureDP900","timestamp":"1679163240.0"},{"poster":"hisunilarora","content":"C and D are correct\nhttps://cloud.google.com/sql/docs/mysql/replication\nBefore you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:\n\nAutomated backups must be enabled.\nBinary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these logs.\nAt least one backup must have been created after binary logging was enabled.","upvote_count":"2","comment_id":"651158","timestamp":"1677236760.0"},{"content":"Choose C, D as right answer without any second thought !","timestamp":"1672696620.0","upvote_count":"2","poster":"AzureDP900","comment_id":"626294"},{"timestamp":"1671516060.0","poster":"Nirca","content":"Selected Answer: CD\nData lose is being managed via datafiles/database backups. Once a day or once every time interval. Between backups the logs(redo/bin logs/archive logs) must be copied on a continuous manner. Only that way point in time recovery or full recovery without data loss might be successful. Else you will be able to backup-restore. AND not backup-restore-recovery.","comment_id":"618961","upvote_count":"6"},{"timestamp":"1666858620.0","poster":"Holly11","content":"C and D","upvote_count":"1","comment_id":"592973"},{"comment_id":"582412","poster":"gaojun","upvote_count":"1","timestamp":"1665143820.0","content":"obviously, CD"},{"timestamp":"1659273420.0","content":"C and D\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#what_backups_provide","comment_id":"537195","poster":"mbenhassine1986","upvote_count":"3"},{"timestamp":"1656430440.0","poster":"ehgm","content":"The feature in Cloud Console is: Enable point-in-time recovery \nDescription: Allows you to recover data from a specific point in time, down to a fraction of a second. Enables binary logs (required for replication). Make sure your storage can support the days of logs you're retaining.","upvote_count":"3","comment_id":"511392"},{"upvote_count":"1","timestamp":"1655582940.0","comment_id":"504486","poster":"ttosl","comments":[{"poster":"GMats","content":"Question does not speak about Availability.It speaks about minimum data loss.","timestamp":"1656894000.0","comment_id":"516199","upvote_count":"2"}],"content":"Selected Answer: BD\nRead replica can be created in another region. HA can only have a second server in different zone. In the worst case that the whole zone is taken out. Just promote read replica to primary. Back up is a must, of course."},{"comment_id":"497528","content":"Go for C.D","poster":"haroldbenites","timestamp":"1654759620.0","upvote_count":"1"},{"timestamp":"1654128360.0","comment_id":"492087","poster":"vincy2202","content":"Selected Answer: CD\nC & D are the correct choices","upvote_count":"2"},{"upvote_count":"1","poster":"nehaxlpb","timestamp":"1653023340.0","content":"answer is CD \nBinary logging is supported on read replica instances (MySQL 5.7 and 8.0 only). You enable binary logging on a replica with the same API commands as on the primary, using the replica's instance name instead of the primary's instance name.","comment_id":"482315"},{"poster":"aa_desh","comment_id":"444336","upvote_count":"5","timestamp":"1647240900.0","content":"Ans is B and D\nPrerequisites for creating a read replica\n\n1. Before you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:\n\n2. Automated backups must be enabled.\n\n3. Binary logging must be enabled which requires point-in-time recovery to be enabled.\n\nAt least one backup must have been created after binary logging was enabled.\nSo it means Read Replica automatically covers binary logging\n\nPlease read following for more information\nhttps://cloud.google.com/sql/docs/mysql/replication#requirements"},{"timestamp":"1645113780.0","content":"Ans - C, D : Binary logging and Automated backups are needed for point-in-time recovery of data.","comment_id":"426371","upvote_count":"4","poster":"AnilKr"},{"upvote_count":"4","poster":"VishalB","timestamp":"1641662940.0","content":"Answer : C & D\n-Point-in-time recovery allows you to recover data from specific point in time, down to a fraction of a second by enabling binary logs(required for replication).\n-Automated backups use a 4-hour backup window. The backup starts during the backup window. When possible, schedule backups when your instance has the least activity.","comment_id":"402027"},{"timestamp":"1641342360.0","comment_id":"398664","poster":"nohel","upvote_count":"1","content":"Answer: B,C\nI think this question is indirectly asking about Point-in-time recovery, \nC) binary log is a set of log files that contain information about data modifications made to a MySQL server instance. \nD) Automated backups will help take daily backups of the MySQL \n\nTo recover from a catastrophic failure i.e MySQL instance crashed, someone deleted an imp table, etc. we will simply restore to the last automated backup before the failure and replay the binary logs till a specific time/position to reach the desired state of MySQL.\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#retention\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/pitr","comments":[{"timestamp":"1644285360.0","poster":"Linus11","upvote_count":"1","comment_id":"421421","content":"Then why did you say B,C instead of C, D?"}]},{"poster":"XDevX","timestamp":"1641153000.0","upvote_count":"3","comment_id":"397090","content":"C and D"},{"poster":"manhmaluc","upvote_count":"4","timestamp":"1640878740.0","comment_id":"394775","content":"C and D\n\nCloud SQL. If you use Cloud SQL, the fully managed Google Cloud MySQL database, you should enable automated backups and binary logging for your Cloud SQL instances. This allows you to perform a point-in-time recovery, which restores your database from a backup and recovers it to a fresh Cloud SQL instance."}],"isMC":true,"answer":"CD","answer_description":"","timestamp":"2021-06-30 15:39:00","answer_ET":"CD","question_images":[],"question_text":"You are implementing a single Cloud SQL MySQL second-generation database that contains business-critical transaction data. You want to ensure that the minimum amount of data is lost in case of catastrophic failure. Which two features should you implement? (Choose two.)","url":"https://www.examtopics.com/discussions/google/view/56369-exam-professional-cloud-architect-topic-1-question-108/","answers_community":["CD (76%)","BD (24%)"],"unix_timestamp":1625060340,"exam_id":4},{"id":"ogcY0ZoVQO7nq9HlSsEH","choices":{"D":"Use a unique identifier for each individual. Upon a deletion request, overwrite the column with the unique identifier with a salted SHA256 of its value.","B":"When ingesting new data in BigQuery, run the data through the Data Loss Prevention (DLP) API to identify any personal information. As part of the DLP scan, save the result to Data Catalog. Upon a deletion request, query Data Catalog to find the column with personal information.","C":"Create a BigQuery view over the table that contains all data. Upon a deletion request, exclude the rows that affect the subject's data from this view. Use this view instead of the source table for all analysis tasks.","A":"Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier."},"question_id":12,"answer_images":[],"answer":"A","question_images":[],"answer_ET":"A","timestamp":"2021-06-30 16:12:00","isMC":true,"discussion":[{"poster":"milan74","comments":[{"poster":"AmitAr","comment_id":"602826","comments":[{"poster":"BeCalm","content":"Deletion is implied in \"Upon a deletion request, query Data Catalog to find the column with personal information.\"","comment_id":"834517","upvote_count":"1","timestamp":"1678406460.0"}],"timestamp":"1652772300.0","upvote_count":"11","content":"(A) - Primary task is \"legislation requires you to delete\" .. and B is not deleting.\nonly A is deleting"},{"poster":"zanfo","timestamp":"1647187620.0","comment_id":"567018","content":"I want to delete all the informations about the user, not only those individuate by DLP. ALL THE INFORMATIONS of the users...B is not correct! the correct is A","upvote_count":"8"},{"timestamp":"1660791060.0","upvote_count":"12","poster":"Ishu_awsguy","content":"There is no need of DLP.\nAll the data is sensitive but only upon user request it needs deletion.\nSo A should be the correct answer.","comment_id":"648258"},{"timestamp":"1637938440.0","comment_id":"487465","comments":[{"timestamp":"1639295460.0","poster":"mgm7","comment_id":"499859","upvote_count":"6","content":"B is not masking the data but identifying where it is to take action on at later date if required"}],"content":"as PhilipKoku mentioned below:\nA) is the correct answer. B) is only masking the data and then when a request is received, it identified the record but it doesn’t delete it. D) Is masking the ID.","upvote_count":"12","poster":"Arad"},{"content":"I think \"SUCH\" is misleading, and you are reading too much into it. The scope is delete the user account data, the field-level content types are not relevant. If this is not the case then the question needs to be reworded properly. \nIs there a technical reason to not use option A? Delete the rows (e.g. BQ DELETE command does that).\nB does not propose to use DLP to delete the data. It scans it during ingestion (i.e. before the delete request is raised). Data Catalog stores a reference to the data only. B proposes to query Data Catalog \"where is the sensitive data\". It never actually proposes the deletion activity.\nAlso, \"legislation\" e.g. GDPR lets the user demand delete all account data, not just \"delete certain PHI fields\".","timestamp":"1692904500.0","upvote_count":"5","poster":"[Removed]","comment_id":"989434"}],"upvote_count":"85","content":"According to me, the question states \"The association collects a large amount of health data, such as sustained injuries.\" and the nuance on the word such => \" Current legislation requires you to delete \"SUCH\" information upon request of the subject. \" So from that point of view the question is not to delete the entire user records but specific data related to personal health data. With DLP you can use InfoTypes and InfoType detectors to specifically scan for those entries and how to act upon them (link https://cloud.google.com/dlp/docs/concepts-infotypes)\nI would say B.","comment_id":"402605","timestamp":"1625823900.0"},{"poster":"XDevX","content":"IMHO a) is the correct answer because it is easier to operate. The question is not how to mask data and so on but just to delete data on request, so I don't think that we have to use for just the deletion of specific data DLP.","timestamp":"1625062320.0","comment_id":"394815","upvote_count":"36"},{"timestamp":"1741068720.0","content":"Selected Answer: B\nOption A doesn't Delete all instances of PII, what happens if there is personal information on the descriptions? such as \" sustained injuries\". Delete just the identifiers is not complete.","upvote_count":"1","poster":"halifax","comment_id":"1364756"},{"upvote_count":"1","poster":"david_tay","timestamp":"1740312240.0","content":"Selected Answer: A\nB is pointless as any health data is sensitive data, and also the fact that it did not mention deletion. Hence A is the correct answer.","comment_id":"1360485"},{"content":"Selected Answer: A\nUse a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.","upvote_count":"1","comment_id":"1336131","poster":"plumbig11","timestamp":"1735932600.0"},{"comment_id":"1332103","content":"Selected Answer: A\nA. Use a unique identifier for each individual.","timestamp":"1735242960.0","poster":"rrope","upvote_count":"1"},{"timestamp":"1734414300.0","comment_id":"1327764","poster":"andyk87","upvote_count":"1","content":"Selected Answer: B\nOption B is better when the requirement is to delete only the PII health data, not all data related to the individual."},{"poster":"Sephethus","upvote_count":"2","content":"It had better be A, if not then you're not a good organization","timestamp":"1718721240.0","comment_id":"1232475"},{"poster":"hitmax87","timestamp":"1715907900.0","upvote_count":"1","comment_id":"1212637","content":"Selected Answer: B\nData Loss Prevention must have!"},{"content":"Selected Answer: B\nI vote for B. \nI had some doubts whether A was correct, but:\n - I'm not convinced by the argument \"only A talks about deleting\" (it would be too easy if it was about choosing an answer containing the word \"delete\" ;)\n - the question says \"design a solution that can accommodate such a request\" - I'm not very fluent in english, but \"accommodate\" imho means more \"facilitate\" than \"accomplish\" here\n - I think that the task is about deleting health data not everything related with unique identifier\n- Data Catalog allows you to manage data, knowing in which datasets and in which tables what data is stored. Answer \"A\" somehow imposes the data model - each table with data related to a given individual must contain the ID of this individual (in a real data model this does not have to be the case).","poster":"Gino17m","comment_id":"1207297","upvote_count":"3","timestamp":"1714992960.0"},{"comment_id":"1177440","poster":"Djenko","timestamp":"1710859260.0","upvote_count":"2","content":"Selected Answer: A\nShould be A)"},{"content":"Selected Answer: A\nA is correct. As for option B: While DLP is valuable for identifying sensitive data, it might not be sufficient for this specific case. DLP cannot necessarily determine an individual's right to deletion based solely on data classification. Additionally, relying on Data Catalog to store the results adds unnecessary complexity and potential inconsistencies.","poster":"mesodan","upvote_count":"3","timestamp":"1709548920.0","comment_id":"1165507"},{"comment_id":"1142497","poster":"Gall","content":"Selected Answer: B\nB. The A removes all data, not SUCH only.","timestamp":"1707246060.0","upvote_count":"2"},{"poster":"NoCrapEva","timestamp":"1706619960.0","comment_id":"1135808","content":"Selected Answer: A\nAns. B assumes you will delete the Personal Information found in the Catalog... Some people are reading GDPR into this question (we are not told what country and what legislation). The question states you must delete all information (not just personal informarion) on request. Ans B is a red herring !\nAnswer must = A","upvote_count":"2"},{"poster":"Roro_Brother","content":"Selected Answer: A\n(A) - Primary task is \"legislation requires you to delete\" .. and B is not deleting.\nonly A is deleting","timestamp":"1703670900.0","upvote_count":"5","comment_id":"1106697"},{"comment_id":"1087791","upvote_count":"1","poster":"Jconnor","content":"Well, A would delete all rows with the identifier, I guess including the ones that are not confidential, also what does it mean unique identifier? each user is unique already. Ridiculous. B would identify the columns that contain personal data, but B is prone to errors as changes in legislation of what is consider injury would be excluded and all data would need to be re- ingested. Unfortunately B is closer and less damaging than A.","timestamp":"1701709740.0"},{"upvote_count":"2","poster":"thewalker","content":"Selected Answer: B\nEither A or B is the answer. \nA - will delete all the info about the subject, which is not the intension. Only the sensitive data to be deleted. Hence, B.","comment_id":"1069028","timestamp":"1699850880.0"},{"poster":"Anubhav451","comment_id":"1055390","content":"B is correct.. Check in chatGPT also.","comments":[{"content":"chatgpt select A\nThe most appropriate solution for accommodating the deletion request of personal health data stored in BigQuery, as per current legislation, would be:\n\nA. Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.\n\nHere's why this approach is suitable:\n\nUnique Identifier: Assigning a unique identifier to each individual is a standard practice in managing and querying datasets. It helps in precisely identifying and isolating records associated with a specific individual.\n\nDirect Deletion of Rows: Upon receiving a deletion request, you can directly delete all rows associated with the individual's unique identifier. This approach ensures that the data is completely removed from your dataset, complying with the legislative requirement to delete personal information upon request.","timestamp":"1702205880.0","poster":"DA95","upvote_count":"1","comment_id":"1092436"}],"timestamp":"1698399360.0","upvote_count":"1"},{"upvote_count":"1","content":"From one side A is an easy way, low effort, to implement this solution, but if we think like an architect and like an exam question, B is more complete and a better solution, since it can mask all the sensitive information, not only for the users that request it, but for all, which is a best practice.","comment_id":"1040654","timestamp":"1697028360.0","poster":"JPA210"},{"comment_id":"1026392","timestamp":"1696584660.0","upvote_count":"2","content":"Selected Answer: B\nWe do not need to delete entire recrod of sports person but some health information collected by association. B would be correct answer.","poster":"AdityaGupta"},{"content":"Selected Answer: B\nThe problem I see with A is that it doesn't offer you a way to find the original subject's information once they request for their information to be deleted (no mapping from the unique identifier back to their person). Only B offers the solution design for this ability. The deletion step may not be included in B, but the ability to delete is always present. You're designing the ability to accommodate the request, which is to look up the individual who is asking for their information to be deleted.","upvote_count":"1","comment_id":"1009271","poster":"TopTalk","timestamp":"1694892480.0"},{"upvote_count":"1","content":"Selected Answer: B\nDLP its correct product to deal with personal data","comment_id":"1003391","poster":"duzapo","timestamp":"1694281500.0"},{"poster":"heretolearnazure","comment_id":"990071","upvote_count":"1","content":"A seems to be the right answer","timestamp":"1692967980.0"},{"poster":"red_panda","upvote_count":"2","content":"Selected Answer: B\nIt's B. We have not to delete dentire rows","timestamp":"1688901300.0","comment_id":"947156"},{"poster":"Hgwells","comment_id":"932957","upvote_count":"1","content":"Selected Answer: B\nThe question requires deletion of PII data only, i.e PHI columns only. There is no way to keep track of all PHI columns with just option A","timestamp":"1687643700.0"},{"content":"Selected Answer: B\nWhat's wrong in B... it seems ok as well","timestamp":"1686792540.0","upvote_count":"1","poster":"Atanu","comment_id":"923640"},{"upvote_count":"1","timestamp":"1686230040.0","poster":"red_panda","comment_id":"918310","content":"Selected Answer: B\nAnswer is B in my opinion.\nIt ask to delete some informations, not ALL informations about customers"},{"timestamp":"1685791020.0","content":"B - this is a more comprehensive solution that tackles PII concerns and helps with identifying \"such\" health information and safely delete.","upvote_count":"1","comment_id":"913480","poster":"JohnWick2020"},{"content":"Selected Answer: A\nUsing a unique identifier for each individual in BigQuery allows you to efficiently locate and delete specific rows of data related to a particular individual. When a deletion request comes in, you can easily find and remove all the rows associated with that unique identifier. This approach ensures that you comply with the legislation requiring the deletion of personal information upon request while maintaining the integrity and structure of the remaining data.","poster":"JC0926","upvote_count":"3","comment_id":"868410","timestamp":"1681306140.0","comments":[{"comment_id":"868411","upvote_count":"2","timestamp":"1681306140.0","content":"Other options are either less efficient or do not fully address the requirement to delete personal information:\n\nB. While DLP API and Data Catalog can help identify personal information, they do not provide an efficient way to delete specific data when a deletion request is made. This method could also be time-consuming and error-prone.\n\nC. Creating a BigQuery view does not actually delete the data; it only hides it from the view. The personal information would still be stored in the source table, which does not comply with the legislation.\n\nD. Overwriting the unique identifier with a salted SHA256 of its value only obfuscates the data but does not remove it, again not complying with the legislation's requirement to delete personal information.","poster":"JC0926"}]},{"comment_id":"856782","poster":"taer","upvote_count":"2","content":"Selected Answer: A\nUsing a unique identifier for each individual allows you to easily find and delete all the rows associated with a specific individual upon their deletion request. This approach complies with the legislation requiring the deletion of personal information upon request and ensures that the individual's data is no longer available in BigQuery.","timestamp":"1680250980.0"},{"poster":"nvragavan","timestamp":"1680236580.0","comment_id":"856594","content":"Selected Answer: B\nThe de-identification process in DLP can delete/reformat sensitive information. Please check Google Cloud Tech channel video below \nhttps://youtu.be/nJKNLl9W-2E?list=PLIivdWyY5sqK9j4_JkC8j1mY4JEGgLdcD&t=179","comments":[{"content":"The answer didn't say use DLP to remove the data. It said scan it during ingestion for identification. Ingestion is before the user asked for deletion. The ingestion would maintain the data, not delete it as a first step. Data Catalog would not delete the data later, it would reference the location of record storage.","timestamp":"1692904020.0","comment_id":"989428","upvote_count":"1","poster":"[Removed]"}],"upvote_count":"1"},{"comment_id":"834516","poster":"BeCalm","upvote_count":"1","content":"Selected Answer: B\nThe ask is for deletion of the sensitive data, not the entirety of all data associated with a unique identifier.","timestamp":"1678406400.0"},{"timestamp":"1677806220.0","comment_id":"827577","content":"Selected Answer: A\noption A, option B is valid if the data is new, it says \"When ingesting \"new data\" in....,\nbut what happens with the previously stored data, it doesn't solve it. \nThe heading or start of option B, overrides the rest of the paragraph.","upvote_count":"5","poster":"romandrigo"},{"timestamp":"1676836440.0","upvote_count":"1","poster":"r1ck","content":"how about 1000 - 5000 - 10000 unique individuals?\nwill you be creating unique ID's for each as per #A ?\n.. it has to be #B","comments":[{"content":"What is the problem with that ? There are no technical limitation for it. Generated a UUID.","comment_id":"853574","upvote_count":"1","poster":"jlambdan","timestamp":"1680029460.0"}],"comment_id":"814471"},{"timestamp":"1672657260.0","poster":"examch","content":"Selected Answer: B\nB is the correct answer,\n\nData catalog can be used to delete entry for the table or dataset, data catalog can be used with DLP, for identification of sensitive data in the tables. \n\nData Catalog provides three main functions:\n\nSearching for data entries for which you have access\nTagging data entries with metadata\nProviding column-level security for BigQuery tables\nIn addition, Data Catalog can leverage the results of a Cloud Data Loss Prevention (DLP) scan to identify sensitive data directly within Data Catalog in the form of tag templates.\n\nhttps://cloud.google.com/data-catalog/docs/samples/data-catalog-delete-entry\nhttps://cloud.google.com/data-catalog/docs/concepts/overview#functions","upvote_count":"4","comment_id":"763658"},{"timestamp":"1672065180.0","comment_id":"757542","upvote_count":"2","poster":"Praveen_G","content":"Selected Answer: B\nB is the correct answer. The request is to delete only 'such information' meaning some specific columns, not the entire rows as mentioned in option A. So, B is the correct answer where it helps to find specific columns."},{"timestamp":"1671965220.0","content":"Selected Answer: A\nthe only answer that actually delete data is A","upvote_count":"2","comment_id":"755585","poster":"thamaster"},{"upvote_count":"6","timestamp":"1671708060.0","comments":[{"content":"Option A will delete entire row but the request is to delete only 'such information' meaning specific columns. B is suitable for that","comments":[{"content":"The thing causing confusion to me is that B doesnt mention the last step of actually deleting the info. But if we assume that the purpose of identifying \"such information\" is to actually delete (as an implicit thing), B is right. I think the answer in B is not worded completely.","comment_id":"796587","upvote_count":"2","poster":"VSMu","timestamp":"1675386960.0"}],"upvote_count":"2","comment_id":"757546","timestamp":"1672065300.0","poster":"Praveen_G"},{"content":"Option C, creating a BigQuery view over the table that contains all data, is not a recommended approach. While a view can be used to exclude certain rows from a query, it does not permanently delete the data from the underlying table. Therefore, this approach would not enable you to fully comply with a request to delete personal data.\n\nOption D, overwriting the column with the unique identifier with a salted SHA256 of its value, is not a recommended approach. While this approach would obscure the value of the identifier, it would not permanently delete the data from the table. This approach would also make it more difficult to identify and delete the data of a specific individual when a request is made.","upvote_count":"3","comment_id":"753221","timestamp":"1671708060.0","poster":"omermahgoub"}],"poster":"omermahgoub","content":"Answer is A. Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.\n\nUsing a unique identifier for each individual allows you to easily identify and delete the data of a specific person when a request is made. You can then simply delete all rows from BigQuery that contain this identifier to fulfill the request.\n\nOption B, running the data through the Data Loss Prevention (DLP) API to identify any personal information and saving the result to Data Catalog, is not a recommended approach. While the DLP API can be used to identify personal information in data, it is not necessary for this specific task. Additionally, saving the result to Data Catalog would not enable you to delete the data from BigQuery.","comment_id":"753219"},{"comment_id":"716712","poster":"megumin","timestamp":"1668261120.0","content":"Selected Answer: A\nA is ok","upvote_count":"2"},{"content":"Problem w with A, Your customer will give you their name not unique I'd. How do you translate it to your I'd? B gives you a catalogue to translate their personal info to where it is stored.","upvote_count":"2","poster":"BiddlyBdoyng","timestamp":"1667408880.0","comments":[{"content":"we suppose that when the customer enters his name, a unique id will be created and paired in another column in the BigQuery data-set...","timestamp":"1669289160.0","comment_id":"725769","upvote_count":"1","poster":"ale_brd_111"}],"comment_id":"709942"},{"comment_id":"706456","content":"Selected Answer: B\nA - will delete everything including other information that they may want to keep\nB - Is a better solution as you can locate and delete selected data and also described in https://cloud.google.com/dlp#section-8\nC and D are irrelevant","upvote_count":"3","timestamp":"1666960740.0","poster":"minmin2020"},{"content":"I will choose A","comment_id":"672555","poster":"AzureDP900","upvote_count":"1","timestamp":"1663517880.0"},{"timestamp":"1663483080.0","poster":"AhmedH7793","content":"Selected Answer: A\nAnswer is (A), (B) is only masking the data without any deletion, and no need for that. \nThink it in KISS principle, don't overwhelm simple things.","upvote_count":"2","comment_id":"672045"},{"content":"Selected Answer: A\nA is the answer.\nB and C does not delete data which is wrong.","comment_id":"670024","timestamp":"1663252980.0","poster":"zellck","upvote_count":"2"},{"poster":"Sbgani","content":"Selected Answer: B\nAns is definitely B, Key is they are collecting ages too and don't want to delete the other information's","upvote_count":"1","comments":[{"upvote_count":"1","content":"I take back this, when we delete \"SUCH\" info from this table then there is no value for other data of that user.Hence deleting the ROW is fine ans A","comment_id":"668595","poster":"Sbgani","timestamp":"1663129380.0"}],"comment_id":"667102","timestamp":"1662993300.0"},{"content":"Selected Answer: A\nThe questions says \" current legislacion requires you to delete\" \nNot hide or protect sensitive data, Answer has to be A! \nEasy as that, don't over think it.","poster":"alexandercamachop","comment_id":"665810","timestamp":"1662870240.0","upvote_count":"3"},{"comments":[{"timestamp":"1663253040.0","comment_id":"670026","content":"B does not delete any data at all. It only says query upon a deletion request.","poster":"zellck","upvote_count":"1"}],"poster":"aut0pil0t","content":"Selected Answer: B\nDefinitely B. There is no requirement to delete \"all\" data of a particular user. The requirement is to delete sensitive data only.","upvote_count":"2","comment_id":"657111","timestamp":"1662103140.0"},{"comments":[{"comment_id":"677831","content":"Did you pass?","timestamp":"1664024040.0","upvote_count":"1","poster":"cloud_enth0325"},{"timestamp":"1665834420.0","upvote_count":"1","poster":"josericardomcastro","content":"Did you pass? what was your answer?","comment_id":"695369"}],"timestamp":"1660442760.0","upvote_count":"2","content":"I got this question in exam.","poster":"ACE_ASPIRE","comment_id":"646515"},{"comment_id":"644990","content":"vote A\nQ didn't want you to mask PII, \"A\" is enough to do the job.","upvote_count":"1","poster":"backhand","timestamp":"1660137840.0"},{"upvote_count":"2","poster":"patashish","comment_id":"644862","timestamp":"1660120140.0","content":"B is correct answer."},{"content":"Selected Answer: B\nB is good. It means delete the entry from the query result.\nA would mean delete the entire individual that may have other non-sensitive value such as name","upvote_count":"3","timestamp":"1659598380.0","comment_id":"642223","poster":"midgoo","comments":[{"timestamp":"1663253100.0","comment_id":"670027","upvote_count":"1","content":"B does not delete any data at all. It only says query upon a deletion request.","poster":"zellck"}]},{"content":"y vote for B","comment_id":"630461","poster":"diego_alejandro","upvote_count":"2","timestamp":"1657622520.0"},{"comment_id":"626296","upvote_count":"2","poster":"AzureDP900","timestamp":"1656792480.0","content":"A is right"},{"timestamp":"1656406500.0","comment_id":"623859","upvote_count":"3","poster":"ZLT","content":"Selected Answer: A\nWho decides what the correct answer is on this site?? B is clearly too complicated and does not delete the data as per the requirements"},{"content":"Selected Answer: A\nB is not deleting.\nonly A is deleting","comment_id":"615084","upvote_count":"3","poster":"H_S","timestamp":"1654982400.0"},{"content":"DLP will not \"delete\" the data, which is the primary ask, so go for option A","upvote_count":"2","timestamp":"1650360840.0","comment_id":"588077","poster":"nkit"},{"content":"Selected Answer: A\nI got similar question on my exam. Answered A.","comments":[{"upvote_count":"4","timestamp":"1654334460.0","poster":"pratiksoni","content":"did you pass?","comment_id":"611370"}],"poster":"[Removed]","upvote_count":"6","comment_id":"545493","timestamp":"1644611940.0"},{"timestamp":"1643620140.0","comments":[{"content":"yes, not delete the whole row, just need to delete some fields. i would like to vote B","upvote_count":"3","poster":"gaojun","timestamp":"1648621380.0","comment_id":"578090"}],"comment_id":"536773","upvote_count":"3","poster":"TharaLN","content":"The question stated \"upon request of the subject\", meaning no need to delete all the data immediately unless you have been requested to do so, therefore correct ans is B"},{"comment_id":"527702","timestamp":"1642609080.0","upvote_count":"2","content":"Got this question in my exam, answered A","poster":"technodev"},{"content":"Totally sure it's A","timestamp":"1642417500.0","poster":"DoVale","upvote_count":"3","comment_id":"525707"},{"timestamp":"1639042260.0","poster":"haroldbenites","comment_id":"497533","upvote_count":"2","content":"Go for A."},{"timestamp":"1638972000.0","upvote_count":"2","comment_id":"496872","content":"Selected Answer: A\nVote A","poster":"nqthien041292"},{"content":"Selected Answer: A\nA is correct. \nMarked B does not say delete of data.","poster":"[Removed]","timestamp":"1638899400.0","upvote_count":"2","comment_id":"496221"},{"comment_id":"488106","timestamp":"1638015180.0","upvote_count":"3","poster":"pakilodi","content":"Selected Answer: A\nit's A. Data should be deleted"},{"upvote_count":"2","poster":"mudot","comment_id":"487592","timestamp":"1637955360.0","content":"Selected Answer: A\nwhy complicate it with B, when masking is not required. \nThis question is 'forget me' request option of europe that u can send to search engines"},{"poster":"pakilodi","content":"Selected Answer: A\nit's A. Data should be deleted","timestamp":"1637414400.0","upvote_count":"2","comment_id":"482561"},{"timestamp":"1637207820.0","poster":"Neo_ACE","upvote_count":"1","content":"Answer is A","comment_id":"480435"},{"comment_id":"472394","poster":"MaxNRG","timestamp":"1636003020.0","content":"Yesterday passed the exam, I've chosen A. Use a unique identifier for each individual. \nUpon a deletion request, delete all rows from BigQuery with this identifier.","upvote_count":"6","comments":[{"upvote_count":"1","timestamp":"1636492680.0","comment_id":"475075","poster":"ghuria","content":"Do you think a lot of questions have come from exam topics?"}]},{"poster":"PhilipKoku","timestamp":"1634791620.0","comment_id":"465534","content":"A) is the correct answer. B) is only masking the data and then when a request is received, it identified the record but it doesn’t delete it. D) Is masking the ID.","upvote_count":"2"},{"comments":[{"poster":"MaxNRG","content":"https://cloud.google.com/dlp#section-6","timestamp":"1634625960.0","upvote_count":"1","comment_id":"464490"},{"comment_id":"642226","content":"Health is one of the infotype for DLP\nhttps://cloud.google.com/dlp/docs/infotypes-reference\nWe can also define custom infotype","poster":"midgoo","upvote_count":"1","timestamp":"1659598560.0"}],"timestamp":"1634625900.0","poster":"MaxNRG","upvote_count":"2","comment_id":"464489","content":"DLP is about \"personal information\", not \"health data, such as sustained injuries\", so I choose A - delete records upon request"},{"poster":"[Removed]","upvote_count":"3","content":"The thing that kills me about this is we are suppose to read the question and parse out the nuance, but I would like to point out that the \"B\" answer never says \"... and delete the records\" it just says query haha. just sayin....","comment_id":"461281","timestamp":"1634076600.0"},{"content":"will go with B as DLP is a best practice to be considered while implementing healthcare information of individuals.","upvote_count":"5","comment_id":"452655","poster":"vijaigcp","timestamp":"1632765420.0"},{"upvote_count":"4","content":"I go with B. Though the requirement doesn't explicitly ask you to mask PII, as a best practice DLP needs to be considered and in conjunction with Data Catolog its providing the option of selectively deleting the data.","timestamp":"1631433720.0","poster":"sgarch","comment_id":"443350"},{"timestamp":"1629208680.0","poster":"AnilKr","content":"It should be B, as question does not intent to delete whole data rather it intends to delete specific data.","comment_id":"426370","upvote_count":"4"},{"upvote_count":"4","timestamp":"1628945940.0","poster":"iloveexam","comment_id":"424792","content":"I would say A, because:\n- DLP in option B is to identify personal information, while the legistration requires to delete \"health information\"\n- identifier in option A does not belong to PII. It's an internal identifier created just for the program to uniquely identify those rows that are related to the subject who requests deletion."},{"poster":"firecloud","comment_id":"416182","upvote_count":"2","content":"It's B Cloud DLP integrates natively with Data Catalog. When you use a Cloud DLP action to scan your BigQuery tables for sensitive data, it can send results directly to Data Catalog in the form of a tag template.","timestamp":"1627473900.0"},{"timestamp":"1627444740.0","poster":"hello_aws","content":"B for me \nA) explains how we delete based on request\nB) explains how we design a solution to deal with the data","comment_id":"415875","upvote_count":"3"},{"poster":"arsav","upvote_count":"9","timestamp":"1627120560.0","comment_id":"413079","content":"A is easier option. but the question is talking about the legislation requirement. if the law enforces to delete some health information of the sports association member, how do you find such law enforced data with option A. \n\nso the right option would be to use DLP, answer is B\nhttps://cloud.google.com/dlp/docs/sending-results-to-dc\n\nand also the good design would be always to ingest the data with such DLP, as the customer data may have PII information. injecting the data into BigQuery with unique identifier is not a good design if your data has PII information."},{"timestamp":"1625983260.0","poster":"victory108","upvote_count":"8","comment_id":"403712","content":"A. Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier."},{"upvote_count":"3","content":"B is better.\nA will delete row. then you will not just delete sensitive data.","poster":"taoj","timestamp":"1625914800.0","comment_id":"403285"},{"timestamp":"1625550420.0","poster":"JeffClarke111","comment_id":"399682","content":"A is ok","upvote_count":"2"},{"upvote_count":"9","content":"Passed the exam last week, I chose A","comment_id":"398014","poster":"siddharthmehta72","comments":[{"comment_id":"402724","poster":"Priyaahuja","timestamp":"1625835120.0","upvote_count":"1","content":"Can you share your experience?"},{"timestamp":"1625425560.0","poster":"gcpexam_ca","upvote_count":"2","comment_id":"398602","content":"Congratulations Sid, have you got questions from examtooics?"}],"timestamp":"1625374860.0"},{"content":"B https://cloud.google.com/bigquery/docs/scan-with-dlp","upvote_count":"3","comment_id":"397294","timestamp":"1625285400.0","poster":"Rom0817"},{"upvote_count":"2","comments":[{"comment_id":"401431","timestamp":"1625707500.0","upvote_count":"3","poster":"kopper2019","content":"I took this answer form someone else but I think is A, DLP is not required here so A"}],"poster":"kopper2019","timestamp":"1625172720.0","comment_id":"396340","content":"Ans) B says"}],"topic":"1","answers_community":["A (62%)","B (38%)"],"exam_id":4,"unix_timestamp":1625062320,"answer_description":"","question_text":"You are working at a sports association whose members range in age from 8 to 30. The association collects a large amount of health data, such as sustained injuries. You are storing this data in BigQuery. Current legislation requires you to delete such information upon request of the subject. You want to design a solution that can accommodate such a request. What should you do?","url":"https://www.examtopics.com/discussions/google/view/56381-exam-professional-cloud-architect-topic-1-question-109/"},{"id":"cN1LRik5UfzOLM11rFCX","isMC":true,"question_images":[],"question_text":"Your customer is moving an existing corporate application to Google Cloud Platform from an on-premises data center. The business owners require minimal user disruption. There are strict security team requirements for storing passwords.\nWhat authentication strategy should they use?","answer_images":[],"topic":"1","unix_timestamp":1571921520,"timestamp":"2019-10-24 14:52:00","discussion":[{"upvote_count":"80","content":"The correct answer is B. \nGCDS tool only copies the usernames, not the passwords. And more over strict security requirements for the passwords. Not allowed to copy them onto Google, I think. \n\nFederation technique help resolve this issue. Please correct me if I am wrong.","comments":[{"upvote_count":"18","timestamp":"1698756960.0","poster":"brss39","content":"B is the answer. Why ?\n\nGCDS syncs passwords - Ok but which passwords? Clients need to provide a new password for accessing Google Cloud after GCDS sync.\nGoogle recognizes the user because GCDS populated the user list. The user is\nredirected to a standard Google sign-in screen where they enter their standard username and Google Cloud-specific password. \nThe issue here is the two sets of passwords. Even if a user manually sets them both to the same value, they aren’t managed in a single place. If you need to update your password, you’d have to do that in AD and then again in Google Cloud Identity. In some cases, this approach can allow for better separation between your on-premises environment and Google Cloud, but it’s also one more password to manage for your users.","comment_id":"1058792","comments":[{"upvote_count":"3","poster":"Robert0","content":"This should be the top comment. It explains in detail the proccess","comment_id":"1217734","timestamp":"1716576660.0"}]},{"upvote_count":"8","content":"Passwords are also synchronized:\nhttps://support.google.com/a/answer/6120130?hl=en&ref_topic=2679497","timestamp":"1661701320.0","comment_id":"653020","poster":"Neferith"},{"comment_id":"379155","timestamp":"1623342900.0","poster":"ExamTopicsFan","upvote_count":"11","content":"GCDS synchronises password as well and that is the reason why B is the correct answer. Only in B the password doesn't get copied to GCP."},{"timestamp":"1665973920.0","upvote_count":"3","poster":"zr79","comment_id":"696749","content":"C is the answer"},{"timestamp":"1592199360.0","content":"you mistaken GCDS for GSPS, from google site \"GSPS won't sync an Active Directory password with a Google Account until it's changed.\" this if from google to for GCDC \"Using GCDS–The recommended way to add users to your Google Account in an Active Directory environment is with Google Cloud Directory Sync (GCDS). GCDS automatically syncs user accounts in your Google domain with user accounts in your Active Directory system.\"","poster":"hafid","comment_id":"110567","upvote_count":"9"}],"poster":"gcp_aws","comment_id":"83814","timestamp":"1588624080.0"},{"timestamp":"1571921520.0","comments":[{"content":"B is supported read https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on","poster":"Gobblegobble","comment_id":"126697","timestamp":"1593936360.0","comments":[{"upvote_count":"3","poster":"tsys","timestamp":"1614901800.0","comment_id":"303710","content":"There is no mention SSO is needed."}],"upvote_count":"4"},{"upvote_count":"5","comment_id":"151607","poster":"tartar","comments":[{"poster":"tartar","upvote_count":"11","comment_id":"157912","timestamp":"1597391700.0","content":"miss typed.. C is ok"}],"content":"B is ok.","timestamp":"1596679680.0"},{"poster":"nitinz","comment_id":"303181","upvote_count":"3","content":"B, you dont want to store password as per security guidelines provided in question.","timestamp":"1614840540.0"},{"content":"GCDS syncs user accounts and some other LDAP attributes but not the passwords, with hybrid connectivity to GCP, SAML (or federation) is the preferred method.\n\nAnswer should be \"B\"\n\nhttps://cloud.google.com/solutions/patterns-for-authenticating-corporate-users-in-a-hybrid-environment\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-synchronizing-user-accounts#deciding_what_to_provision","timestamp":"1593158820.0","upvote_count":"16","comments":[{"comment_id":"529221","content":"This is the best answer so far.","poster":"squishy_fishy","timestamp":"1642780140.0","upvote_count":"1"},{"timestamp":"1601706060.0","poster":"SamirJ","comments":[{"content":"In Active Directory, passwords are stored as write-only. They can't be read through any interface, such as LDAP. Therefore, conventional synchronization methods (for example, Google Cloud Directory Sync) can't access them. The only way to read passwords is to capture them when they’re set or changed.","timestamp":"1735457220.0","comment_id":"1333385","upvote_count":"1","poster":"MilkyMist"}],"upvote_count":"5","comment_id":"192038","content":"GCDS does sync passwords. Please refer - https://support.google.com/a/answer/6120130. Since the question says client wants to move to GCP , C should be the answer."},{"poster":"BiddlyBdoyng","upvote_count":"1","content":"The article implies that ADFS is best but suggests you also need the GCDS. This makes sense, you need the users in Google to allocate permissions but you don't want to copy the passwords across hence ADFS.","timestamp":"1687366200.0","comment_id":"929741"}],"poster":"cetanx","comment_id":"120315"}],"comment_id":"17183","upvote_count":"22","content":"\"A\" will syncronise passwords between on pre-mise and the GCP, this duplicates the existing strategy plus Google's \"built-in\" encryption of\nall the data. \"B\" does not support the moving to GCP. \"C\" The directory sync tool copies the filesystem settings between servers, UNIX filesystems\nhave permission settings built in and passwords to log into the permission groups, syncing these would set GCP up the same way their on-premises\nis, plus Google's \"built-in\" encryption. \"D\" disrupts the users, so this is not correct. The debate should be between \"A\" and \"C\", \"C\" includes\n\"A\" according to (https://cloud.google.com/solutions/migrating-consumer-accounts-to-cloud-identity-or-g-suite-best-practices-federation) so\nchoose \"C\"","poster":"Eroc"},{"upvote_count":"1","content":"Selected Answer: B\nB is most correct answer","comment_id":"1364403","poster":"j_vish","timestamp":"1741006080.0"},{"upvote_count":"1","comment_id":"1357670","poster":"rahulrawat0789","content":"Selected Answer: B\nIts B, because of strict security requirements. User will not be forced move passwords to Google cloud","timestamp":"1739781720.0"},{"poster":"hpf97","content":"Selected Answer: B\nA&C: would also copy password in GCP, so will certainly not satisfy security requirement\nB : Recommended, no service interruption, no change for users, quick to set up\nhttps://cloud.google.com/architecture/identity/best-practices-for-federating\nD : Just kidding solution","upvote_count":"1","timestamp":"1738069680.0","comment_id":"1347909"},{"poster":"Ekramy_Elnaggar","timestamp":"1731267720.0","content":"Selected Answer: B\n1. Minimal User Disruption: Federated authentication allows users to use their existing corporate credentials to access the application in Google Cloud. This eliminates the need for them to create and remember new passwords, minimizing disruption and improving user experience.\n\n2. Strict Security Requirements: SAML 2.0 is a widely used, secure standard for authentication and authorization. It allows the existing identity provider (IdP) to handle password management and security policies, ensuring compliance with the security team's requirements.\n\n3. Centralized Identity Management: Federation keeps identity management centralized within the existing corporate infrastructure. This simplifies user management and reduces the overhead of managing identities in multiple places.","upvote_count":"2","comment_id":"1309599"},{"content":"Selected Answer: B\ncross-domain SSO can be achieved by SAML","comment_id":"1299248","timestamp":"1729173960.0","upvote_count":"1","poster":"selected"},{"comment_id":"1279894","poster":"JohnJamesB1212","content":"Selected Answer: B\nI think B is correct","upvote_count":"1","timestamp":"1725684000.0"},{"comment_id":"1277347","upvote_count":"1","content":"Selected Answer: B\nMinimal user disruption: By federating authentication via SAML 2.0, users can continue using their existing corporate credentials without having to manage or remember new passwords.\nSecurity requirements: SAML 2.0 federation allows your organization to maintain control over user authentication and password management within the existing Identity Provider (IdP). Passwords do not need to be stored in Google’s systems, which aligns with strict security requirements.","poster":"maxdanny","timestamp":"1725347640.0"},{"timestamp":"1723573680.0","content":"B is right one . Because C While Google Cloud Directory Syc (GCDS) helps sync users between an on-premises directory and Google, it does not address the password management aspect. Users may still face disruptions as this method might not handle existing passwords securely.","comment_id":"1265290","upvote_count":"1","poster":"Manishjb006"},{"poster":"Hungdv","content":"Choose B","timestamp":"1723096200.0","upvote_count":"1","comment_id":"1262328"},{"poster":"kingfighers","upvote_count":"1","comment_id":"1227639","timestamp":"1717990980.0","content":"the most convenient way is B, but the principle of this kind of exam is use cloud provider's native tools, so the C is correct.. this principle is also used on aws"},{"content":"Selected Answer: B\nB. Federate authentication via SAML 2.0 to the existing Identity Provider.\n\nHere's why:\n\nSecurity: SAML 2.0 allows for secure single sign-on (SSO) without storing passwords on Google's side. It ensures that authentication happens against the corporate Identity Provider (IdP), which maintains control over the user credentials.\n\nMinimal Disruption: Users can continue to use their existing corporate credentials to access the application on GCP without having to remember a new set of credentials or go through a password change process.\n\nCompliance: It satisfies the security team's requirements for password storage by ensuring that passwords remain within the corporate boundary.\n\nIntegration: SAML is widely supported and can be integrated with many IdPs, allowing for a seamless transition to cloud-based resources while leveraging existing identity management infrastructure.","poster":"santoshchauhan","timestamp":"1709978700.0","upvote_count":"6","comment_id":"1169396"},{"upvote_count":"2","timestamp":"1708313460.0","content":"The correct answer is C.\nGoogle Cloud Directory Sync will provide federated authentications.\nB is wrong because SAML is used for Single sign-on. It also doesn't mention how the cloud can be authenticated to the existing Identity Provider. SAML by itself is not enough to do the job.","poster":"lisabisa","comment_id":"1153691"},{"upvote_count":"2","content":"Selected Answer: B\nFederating authentication aligns with strict security team requirements for password storage, as it avoids the need to store or sync passwords outside the corporate environment.","poster":"xxoox","comment_id":"1153688","timestamp":"1708313040.0"},{"poster":"hzaoui","content":"Selected Answer: B\nMinimal User Disruption:\n\nUsers continue using their existing corporate credentials for both on-premises and GCP applications, avoiding password resets or new account creations.\nSecurity Team Requirements:\n\nGCP doesn't store or manage corporate passwords; authentication relies on the existing Identity Provider (IdP), meeting strict password storage requirements.","timestamp":"1704989520.0","comment_id":"1119946","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nB is a preferred solution nowadays, that's why:\n\nhttps://cloud.google.com/architecture/framework/security/identity-access#use_a_single_identity_provider","timestamp":"1700605560.0","comment_id":"1076786","poster":"02fc23a"},{"upvote_count":"1","content":"Selected Answer: C\nGCDS is better as it is a corporate application. The requirements for storing password can be met by GCP. As GCP has many security features\nFor SAML, the corporate needs to have Identity provider service such as the one provided by Google, Facebook","comment_id":"1069987","poster":"nideesh","comments":[{"upvote_count":"1","content":"Also the application needs to be modified to use identity provider service, if they are going by choice B","comment_id":"1069995","poster":"nideesh","timestamp":"1699937340.0"}],"timestamp":"1699936680.0"},{"content":"Selected Answer: B\nmain reason for B are strict storage requirements.","timestamp":"1699215960.0","poster":"asciimo","comment_id":"1063251","upvote_count":"1"},{"comment_id":"1039131","poster":"Arun_m_123","content":"B is the correct answer","upvote_count":"1","timestamp":"1696908240.0"},{"comment_id":"1015444","timestamp":"1695524340.0","poster":"jrisl1991","content":"Selected Answer: B\nI think it's B because they want minimal user disruption, and only this option focuses on using the same password. Plus, they want to move ONE existing corporate application, not all their infrastructure. \n\nA. I don't think this meets a strict security requirement, and if they eventually need to change the password, I think this would not be synced or may have issues syncing both passwords. \nC. We don't want to provision new users; we want to keep users with minimal disruption and doing what they do already taking the least steps possible. \nD. Probably a terrible security practice; if anything, we would like them to use one password and sign in from there. \n\nB seems to me the most fitting.","upvote_count":"2"},{"content":"The question is ambiguous, though C is the righter answer :-)\nhttps://cloud.google.com/architecture/identity/reference-architectures\nGCP uses GCDS to sync On-prem Azure Directory/LDAP user/groups. It assumes that all on-prem IdP are active directory, which might not be the case.","timestamp":"1695489900.0","upvote_count":"2","poster":"yilexar","comment_id":"1015139"},{"poster":"daidaidai","timestamp":"1692667320.0","comment_id":"986994","upvote_count":"7","content":"Selected Answer: B\nB. Federate authentication via SAML 2.0 to the existing Identity Provider - Federated authentication allows users to sign in to the Google Cloud Platform using the same credentials they use for their corporate accounts. It delegates the authentication process to an existing Identity Provider (IdP) that the company uses on-premises. This approach minimizes user disruption, as users don't have to remember a separate set of credentials for Google Cloud, and it allows the company to maintain its existing security policies and password storage requirements."},{"comment_id":"984953","poster":"didek1986","upvote_count":"1","content":"Selected Answer: C\nIt is C. You move to gcp so copy and use from gcp now.","timestamp":"1692421140.0"},{"timestamp":"1691829000.0","content":"I don`know what GCDS has to do with passwords, it has to be B","poster":"rescolar","upvote_count":"1","comment_id":"979257"},{"timestamp":"1687803660.0","poster":"Rothmansua","upvote_count":"2","content":"Selected Answer: C\nFederation would connect to existing Identity Provider that runs who knows where. \nUsing GCDS corporate accounts will create application user identities in GCP and will let you use those identities in the Cloud (as the question objective implies)","comment_id":"934712"},{"comment_id":"930134","content":"C is the preferred solution in 2023","upvote_count":"2","poster":"621db32","timestamp":"1687406760.0"},{"comment_id":"919597","content":"Selected Answer: C\nC seems more appropriate because it meets the requirements and is simple.","upvote_count":"2","poster":"nescafe7","timestamp":"1686338700.0"},{"comment_id":"868984","timestamp":"1681349940.0","content":"C is the correct answer","upvote_count":"1","poster":"ChewSena"},{"poster":"kratosmat","upvote_count":"2","comment_id":"866521","timestamp":"1681150500.0","content":"Selected Answer: C\nThe federation could be the best option, but it has a strict requirement, the company must have an Identity Provider SAML 2."},{"comment_id":"850705","poster":"taer","content":"Selected Answer: B\nB, 100%","upvote_count":"1","timestamp":"1679801700.0"},{"timestamp":"1679656920.0","poster":"PankajKapse","content":"Selected Answer: B\nhttps://partner.cloudskillsboost.google/course_sessions/2653258/video/195360\n\ngoogle cloud directory sync only syncs objects like users and groups without their passwords once a user is synced to cloud identity, it needs to also have a password or a means of authentication there are a few options to choose from (a) different password for google cloud account (b) federated identity with on-prem security (c) external identity provider","comment_id":"849249","upvote_count":"2"},{"content":"B definitively cannot be a universally correct response due to limitations with GCDS as per the documentation.\n\nNot all password formats are supported. For details, see Additional user attributes.\n\nIf you use Active Directory, you can use Password Sync to sync user passwords from Active Directory to your Google domain. For details, see Sync passwords with Active Directory.\n\nhttps://support.google.com/a/answer/6120130?hl=en&ref_topic=2679497","upvote_count":"2","timestamp":"1678163700.0","comment_id":"831552","poster":"BeCalm"},{"content":"Selected Answer: B\nGCDS is for provisioning, not authentication","timestamp":"1678163100.0","comment_id":"831550","upvote_count":"1","poster":"BeCalm"},{"poster":"MestreCholas","timestamp":"1677771120.0","comment_id":"827046","content":"Selected Answer: B\nB) is Right ->Use federated authenticated via SAML 2.0 to the existing identity provider. User's passwords are stored on-premise, authentication happens on-premise, there is no user disruption, and on successful authentication, the access token is shared to access application or gcp services.\nC) Is wrong. provision users in google using the google cloud directory sinc tool -> with google cloud directory sync, it only hashes the password as salted SHA512 and gets synced from the source. Plus this may break the strict password requirement. Your credential details are now stored in 2 places","upvote_count":"2"},{"poster":"JC0926","timestamp":"1677740340.0","comment_id":"826558","content":"Selected Answer: B\nFor this scenario, the most appropriate authentication strategy would be B. Federate authentication via SAML 2.0 to the existing Identity Provider. This approach can achieve single sign-on and will not cause too much disruption to existing users. Additionally, since there are strict security team requirements, SAML federated authentication can ensure the security of password storage and transmission. \n\nOther options may cause issues with security and user experience, such as \noption A, which may lead to security issues with password synchronization, \noption C, which may require password reassignment, \noption D, which may cause user confusion or forgotten passwords.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1677510480.0","comment_id":"823862","poster":"lokiinaction","content":"the question is asking about authentication strategy, not the user management or provisioning strategy, so once users are provisioned into GCP through GCDS, then the authentication strategy is federating the authentication request. https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction \nso I still don't get why C is the answer..."},{"poster":"CosminCiuc","timestamp":"1674899700.0","comment_id":"790491","content":"GCDS works with Active Directory. The question does not specify what is the current on-premises Identity Provider. I would think that B is the correct answer.","upvote_count":"3"},{"comment_id":"788558","upvote_count":"1","poster":"Laso","content":"Selected Answer: B\nI think the option is B, C doesn't syncronize pwd, only provisioning","timestamp":"1674723600.0"},{"timestamp":"1673346900.0","comment_id":"771300","upvote_count":"1","content":"Here the biz user is moving just 1 application and strict security reqmnts so federation is the right option.","poster":"PST21"},{"poster":"examch","content":"Selected Answer: C\nYou can sync password in GCDS,\nWhy do I have to configure GCDS to sync passwords?\nThe default password sync settings in GCDS are used to define how GCDS creates passwords for new user accounts. If you don’t want to customize an initial account password, no action is required. Just use the default settings.\n\nIf you're using Active Directory you can use Password Sync to sync user passwords from Active Directory to your Google domain. \n\nhttps://support.google.com/a/answer/7177266?hl=en#zippy=%2Cwhy-do-i-have-to-configure-gcds-to-sync-passwords","upvote_count":"1","comment_id":"754808","timestamp":"1671877680.0"},{"upvote_count":"2","timestamp":"1671722040.0","comment_id":"753422","content":"A good authentication strategy for this situation would be to federate authentication via SAML 2.0 to the existing Identity Provider (Option B).","poster":"[Removed]"},{"upvote_count":"2","timestamp":"1671448080.0","poster":"surajkrishnamurthy","comment_id":"749745","content":"Selected Answer: B\nB is the correct answer"},{"content":"Why would the answer be B? The study material I have never mentions Security assertion markup language.....","comment_id":"738487","poster":"jay9114","comments":[{"comments":[{"timestamp":"1670537220.0","upvote_count":"1","poster":"jay9114","content":"SAML allows for you to access content from a different system...on your system - in a secure way...","comment_id":"739514"}],"upvote_count":"1","comment_id":"739502","timestamp":"1670536500.0","content":"SAML (Security Assertion Markup Language) - https://auth0.com/intro-to-iam/what-is-saml#saml-20-benefits-and-use-cases","poster":"jay9114"}],"timestamp":"1670460360.0","upvote_count":"1"},{"timestamp":"1670343420.0","content":"Selected Answer: B\nB seems correct.","comment_id":"736984","upvote_count":"1","poster":"anupamgogoi"},{"timestamp":"1669695540.0","content":"Selected Answer: B\nThe correct option is B. \n\nStop confusing GCDS, this tool doesn't sync passwords, you are misleading people here, there is a logical reason why Google has 2 tools for syncing from AD.. GCDS to sync entities (Users, Groups, Ous, etc) and Google Password Sync to sync passwords from AD to Google but in this case the option A is not a solid pick because the tool triggers only when there is a change of password from AD while is syncing so will still require users to reset their passwords in order to sync the first time.","poster":"jaxclain","upvote_count":"2","comment_id":"729895"},{"content":"B (federated login with SAML assertion using existing IDP)","comment_id":"727925","upvote_count":"1","poster":"angelumesh","timestamp":"1669524900.0"},{"timestamp":"1667640000.0","poster":"megumin","upvote_count":"2","comment_id":"711633","content":"Selected Answer: C\nC is correct"},{"poster":"Mahmoud_E","timestamp":"1666457640.0","upvote_count":"1","content":"B is the right answer https://cloud.google.com/architecture/authenticating-corporate-users-in-a-hybrid-environment","comment_id":"701669"},{"timestamp":"1666457520.0","comment_id":"701666","poster":"Mahmoud_E","content":"Selected Answer: B\nB is the correct answer","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nB no brainer, this method makes the AD or Azure ADFS whatever your identity provide is the single source of truth and doesn't not sync passwords.","timestamp":"1666457460.0","comment_id":"701664","poster":"Mahmoud_E"},{"comment_id":"699798","poster":"MarcoEscanor","upvote_count":"1","content":"a question to those who have chosen B. \n\nFederate authentication via SAML 2.0 will work with on prem data center?","timestamp":"1666261680.0"},{"content":"Selected Answer: B\nhttps://cloud.google.com/architecture/identity/best-practices-for-federating","poster":"0xE8D4A51000","comment_id":"698877","timestamp":"1666171920.0","upvote_count":"1"},{"timestamp":"1665973860.0","comment_id":"696748","upvote_count":"1","content":"when in doubt, choose an answer that is not outside Google Cloud products. Why would you choose a third-party solution while Google Cloud provides one?\nAnswer is B \nBut my reasoning for this question is I know Directory sync could sync the password and the management does not want an overhead to manage the users","poster":"zr79","comments":[]},{"timestamp":"1665636540.0","poster":"minmin2020","content":"Selected Answer: B\nB. Federate authentication via SAML 2.0 to the existing Identity Provider","upvote_count":"2","comment_id":"693618"},{"timestamp":"1663677720.0","comment_id":"674153","poster":"holerina","upvote_count":"2","content":"correct answer is B , as SAML is the fastest"},{"timestamp":"1663050420.0","poster":"VedaSW","comment_id":"667716","upvote_count":"2","content":"Selected Answer: C\nB is not an option, with assumption that the current Identity Provider is hosted in the on prem data center. Assuming the question is about moving away from on prem and shut down on prem permanently, so you left with migration options. Hence C is a better answer."},{"timestamp":"1660717080.0","upvote_count":"2","content":"Selected Answer: B\n\nConsidering another aspect mentioned in the question: \"The business owners require minimal user disruption.\" \nThis internally implies usage of SSO, else users will have to authenticate twice depending upon \"Corporate\" app's hosting environment being GCP or on premise data center.","comment_id":"647938","poster":"RitwickKumar"},{"upvote_count":"2","content":"vote B\nGCDS supports a limited set of password operations. It can import passwords only in an LDAP attribute that stores passwords in plain text, Base64, unsalted MD5, or unsalted SHA-1 format. Other password-encrypted and salted hashes are not supported.\nhttps://support.google.com/a/answer/6124474#passwords&zippy=%2Chow-will-you-synchronize-passwords","comment_id":"635413","timestamp":"1658548260.0","poster":"backhand"},{"content":"Selected Answer: B\nThe correct answer is B.","poster":"pfilourenco","comment_id":"620814","upvote_count":"2","timestamp":"1655967120.0"},{"timestamp":"1653992580.0","content":"Selected Answer: B\nB - Federated approach, as it doesn't syncs password","poster":"Superr","upvote_count":"2","comment_id":"609665"},{"comment_id":"600901","poster":"ashii007","content":"Definitely not C. GCDS assumes same user has enterprise account as well as Google account which is not the case here.","timestamp":"1652403480.0","upvote_count":"1"},{"content":"Selected Answer: C\nIts C , GCDS does sync password","poster":"amxexam","timestamp":"1651937220.0","comment_id":"598177","upvote_count":"1"},{"upvote_count":"1","poster":"Venket","content":"B is the answer","comment_id":"594733","timestamp":"1651276140.0"},{"content":"I don't understand why everybody assumes, that the passwords musn't be stored @Google. \"There are strict security team requirements for storing passwords.\" -> say's it must be secure. It does NOT say, they must remain at the existing identity provider. Storing Passwords in the GCP is not secure (that is what I read out of everybody's answers)...","poster":"cpi_web","timestamp":"1648721100.0","comment_id":"578786","upvote_count":"3"},{"comment_id":"577938","upvote_count":"1","poster":"[Removed]","timestamp":"1648598520.0","content":"B\n\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction"},{"timestamp":"1647342600.0","comment_id":"568310","content":"I go with C considering high availability and strict security for passwords requirements","poster":"slars2k","upvote_count":"1"},{"timestamp":"1647094500.0","upvote_count":"1","content":"C…looks good","poster":"SAMBIT","comment_id":"566196"},{"comments":[{"comment_id":"661981","poster":"gee1979","timestamp":"1662525300.0","content":"good catch!\n\n\nvictory108 1 year, 3 months ago\nB. Federate authentication via SAML 2.0 to the \"existing Identity Provider\"","upvote_count":"1"}],"content":"The correct answer is B.\nUnderstanding single sign-on\nBy using Google Cloud Directory Sync, you've already automated the creation and maintenance of users and tied their lifecycle to the users in Active Directory.\n\nAlthough Google Cloud Directory Sync provisions user account details, it doesn't synchronize passwords. Whenever a user needs to authenticate in Google Cloud, the authentication must be delegated back to Active Directory, which is done by using AD FS and the Security Assertion Markup Language (SAML) protocol. This setup ensures that only Active Directory has access to user credentials and is enforcing any existing policies or multi-factor authentication (MFA) mechanisms. Moreover, it establishes a single sign-on experience between your on-premises environment and Google.\n\nFor more details on single sign-on, see Single sign-on\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on","comment_id":"545118","timestamp":"1644561960.0","poster":"jpco","upvote_count":"3"},{"comment_id":"532185","content":"Selected Answer: B\nI chose B.","timestamp":"1643120460.0","upvote_count":"2","poster":"AWS56"},{"poster":"[Removed]","comment_id":"529374","content":"Selected Answer: B\nI am aligned towards B, but below suggests that answer is C:\nhttps://books.google.ca/books?id=eSTLDwAAQBAJ&pg=PT30&lpg=PT30&dq=%22Your+customer+is+moving+an+existing+corporate+application+to+Google+Cloud+Platform%22&source=bl&ots=fKqwoveJvD&sig=ACfU3U2dlmR8iqtivYSqgorfARF-z-dQZA&hl=en&sa=X&redir_esc=y#v=onepage&q=%22Your%20customer%20is%20moving%20an%20existing%20corporate%20application%20to%20Google%20Cloud%20Platform%22&f=false","timestamp":"1642798380.0","upvote_count":"2"},{"timestamp":"1642285620.0","comment_id":"524476","poster":"sjmsummer","upvote_count":"2","content":"I chose A earlier, but by reading comments, B seems to be with implied security requirements better. So I will choose B."},{"content":"Selected Answer: B\nI chose B.","poster":"OrangeTiger","upvote_count":"2","timestamp":"1640764200.0","comment_id":"511937","comments":[{"poster":"OrangeTiger","upvote_count":"1","comment_id":"511938","content":"https://support.google.com/cloudidentity/answer/6087519?hl=ja","timestamp":"1640764320.0"}]},{"content":"Selected Answer: C\nA Password sync need AD sync\nD No sense\n\nRemaining options is B or C.\n'There are strict security team requirements for storing passwords.' is written in the question.\nDirectory sync is saved password.\nSAML dont need save password.\nSo i chose C.\n\nBut I'm not confident, because it's different from everyone's answer :(","comments":[],"timestamp":"1640764140.0","comment_id":"511933","poster":"OrangeTiger","upvote_count":"1"},{"poster":"vincy2202","comment_id":"508413","upvote_count":"2","timestamp":"1640339100.0","content":"Selected Answer: C\nThis is a weird question, since neither B or C can function independently. IMHO, the solution should be a combination of both B & C. However since C is the first step towards setting up of the Authentication set up, I will choose C as an option."},{"timestamp":"1640026020.0","content":"Selected Answer: B\nWhen you provision users in Google using the Google Cloud Directory Sync tool, GCDS creates default passwords automatically, and this causes disruption. You need a GSuite admin to share the passwords with the users so that they can now login to the application on GCP using the new credentials.","comment_id":"505634","upvote_count":"2","poster":"ABO_Doma"},{"content":"Selected Answer: B\nThey have strict security team to store the passwords so we dont need google we can delegate passwords to client and use SAML","timestamp":"1639655880.0","poster":"ABO_Doma","upvote_count":"2","comment_id":"502898"},{"comment_id":"501097","timestamp":"1639461060.0","upvote_count":"2","content":"Selected Answer: B\nAmong the answers, the minimal disruption would be B - SAML SSO. If you copy passwords to GCP, password lifecycle management need to be addressed. Also, the password needs to constantly synced between GCP and OnPremise store.","poster":"phantomsg"},{"timestamp":"1638792540.0","content":"I too suggest B and got with comment from gcp_aws user","poster":"anjuagrawal","upvote_count":"1","comment_id":"495108"},{"timestamp":"1638548580.0","upvote_count":"1","comment_id":"493243","content":"Go for B.","poster":"haroldbenites"},{"poster":"duocnh","timestamp":"1638141900.0","comment_id":"489474","content":"Selected Answer: B\nvote B","upvote_count":"2"},{"comment_id":"469887","poster":"dlpkmr98","content":"correct answer is B.. check this link - https://cloud.google.com/architecture/identity/overview-google-authentication#external_saml_identity_provider","upvote_count":"1","timestamp":"1635526860.0"},{"content":"Its C , GCDS does sync password","comment_id":"440760","poster":"amxexam","upvote_count":"1","timestamp":"1630997820.0"},{"content":"The correct answer is C even I was sure for B.\nHere the reference that explains perfectly the reason:\nhttps://cloud.google.com/architecture/identity/reference-architectures#using_an_external_idp","upvote_count":"1","poster":"kaste08","timestamp":"1630572240.0","comment_id":"437743"},{"content":"Guys, looks like koppers questions on the new case studies have been removed.\nAny one got those questions? thnx","poster":"Braindump","upvote_count":"3","comment_id":"429979","timestamp":"1629723780.0"},{"comment_id":"428462","poster":"Sonu_xyz","timestamp":"1629511500.0","content":"Answer is B","upvote_count":"1"},{"poster":"rikoko","upvote_count":"1","timestamp":"1629315660.0","comment_id":"427039","content":"Correct answer is B.\nThe question is about Authentication strategy - GCDS only stores only account (no passwords). It is not an authentication strategy."},{"comment_id":"365233","upvote_count":"1","content":"I believe that Answer is B.\nClient is only moving its application to the Cloud, this is not a complete migration.\n- There are strict password rules for a reason, so password-db should only be authenticated and not copied migrated.\n- Keeping the passwords in two places increase the attack possibilities and vulnerabilities you will need to secure and patch.","timestamp":"1621835940.0","poster":"ArthurL20"},{"poster":"Ashoks","content":"Answer should be B","timestamp":"1621822980.0","upvote_count":"1","comment_id":"365139"},{"timestamp":"1621320540.0","comment_id":"360170","upvote_count":"1","poster":"victory108","content":"B. Federate authentication via SAML 2.0 to the existing Identity Provider"},{"poster":"Skr12","comment_id":"357649","upvote_count":"1","timestamp":"1621061340.0","content":"B is the right answer. Minimal disruption and passwords are not moved anywhere fulfilling the security requirements."},{"comment_id":"353876","timestamp":"1620661440.0","upvote_count":"1","poster":"un","content":"B is the correct answer"},{"poster":"get2dd","timestamp":"1613860620.0","upvote_count":"2","comment_id":"295423","content":"B. Federate authentication via SAML 2.0 to the existing Identity Provider\nAbove the correct answer as per Whizlabs explanation."},{"timestamp":"1613344200.0","comment_id":"290557","upvote_count":"2","poster":"Joyjit_Deb","content":"I would go with \"B\", as the question is focused on having minimal user impact."},{"timestamp":"1611374820.0","comment_id":"274274","upvote_count":"1","poster":"bnlcnd","content":"Answer should be B.\nBoth A & C require sync identity data from on-prem to cloud. This should be less secure than federate on-prem AD with GCP Cloud Directory."},{"timestamp":"1611365760.0","content":"Why do you talk about LDAP and AD if the question doesnt state anywhere nothing about these systems being used? It's an unknown identity provider, the best choice would be SAML, without more information you cant choice anything for sure","comment_id":"274128","upvote_count":"3","poster":"Nastrand"},{"comment_id":"269871","upvote_count":"2","poster":"VenV","content":"Minimal user disruption - integrate with Saml.. we are done - B should be appropriate","timestamp":"1610921640.0"},{"content":"I would go with B. because the question is only talking about \"one\" corporate application. you should not replicate your user from your corporate directory to google only for one app. If the question was about fully migrating cloud option B is preferred. SAML is the way to go when your integration any new corp cloud Application.\n\nB is the way to go.","upvote_count":"1","poster":"ybe_gcp_cert","comment_id":"267087","timestamp":"1610636400.0"},{"poster":"dlzhang","comment_id":"260736","timestamp":"1609904520.0","content":"B is correct","upvote_count":"1"},{"poster":"Prakzz","upvote_count":"1","content":"C is correct","timestamp":"1608834840.0","comment_id":"251716"},{"poster":"pepYash","content":"B should be the answer. \"Security team has strict requirements about storing passwords\"\nwell, if GCP already provides a way to integrate external authentication, then why not.\n\nHere:\nhttps://cloud.google.com/architecture/identity/best-practices-for-federating\n\nAll Google services, including Google Cloud, Google Marketing Platform, and Google Ads, rely on Google Sign-In to authenticate users. Instead of manually creating and maintaining user accounts in Cloud Identity or Google Workspace for each employee, you can federate Cloud Identity or Google Workspace with your external identity provider (IdP) such as Active Directory or Azure Active Directory. Setting up federation typically entails the following:\n\nAutomatically provisioning relevant user accounts from an external authoritative source to Cloud Identity or Google Workspace.\nEnabling users to use an external IdP to authenticate to Google services.","upvote_count":"2","comment_id":"214961","timestamp":"1604796660.0"},{"comment_id":"187428","upvote_count":"1","content":"\"What authentication strategy should they use?\"\nI think only C is addressing authentication.\n\nThe rest of the choices are password management or user accounts management. (It does not do authentication)","comments":[{"poster":"VedaSW","upvote_count":"2","content":"Sorry, I meant B, not C.","comment_id":"187429","timestamp":"1601095200.0"}],"poster":"VedaSW","timestamp":"1601095140.0"},{"poster":"nominee","comment_id":"181461","upvote_count":"2","timestamp":"1600414020.0","content":"This link https://cloud.google.com/solutions/authenticating-corporate-users-in-a-hybrid-environment#resources-that-use-google-as-idp seems to point to use SAML approach. For Corporate Applications and the use case, it is not a must have to setup users in Google Cloud. Authenticating with existing provider should be sufficient.\nMy answer B"},{"poster":"AshokC","comment_id":"179349","timestamp":"1600094280.0","content":"B sounds right answer as per https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on","upvote_count":"2"},{"poster":"Ale1973","upvote_count":"1","timestamp":"1598764740.0","comment_id":"169786","content":"Best Option is B in my opinion.\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on\n“Although Cloud Directory Sync provisions user account details, it doesn't synchronize passwords. Whenever a user needs to authenticate in Google Cloud, the authentication must be delegated back to Active Directory, which is done by using AD FS and the Security Assertion Markup Language (SAML) protocol. This setup ensures that only Active Directory has access to user credentials and is enforcing any existing policies or multi-factor authentication (MFA) mechanisms. Moreover, it establishes a single sign-on experience between your on-premises environment and Google.\nOnly option B meets Security Team requirement about password storage"},{"content":"Correct answer is B","timestamp":"1598573700.0","comment_id":"167924","poster":"Kabiliravi","upvote_count":"1"},{"poster":"vibhavchavan","timestamp":"1598489400.0","upvote_count":"1","comment_id":"167203","content":"Question is about authentication strategy. GCDS synch B has nothing to do with authentication. GCPS would have been an option for authentication strategy. However the best way to adhere strict authentication without disturbing existing iDP is option A using SAML or JWT token."},{"timestamp":"1596679560.0","poster":"teeess","comment_id":"151606","content":"Answer B - becuase we are talking about authentication and that is done via B.\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on","upvote_count":"2"},{"upvote_count":"7","poster":"ewredtrfygi","content":"Definitely C. Not sure why so many answers here say GCDS doesn't sync passwords - it does so securely:\nhttps://support.google.com/a/answer/6120130\nGCDS is a one-way sync from the original IdP to GCP, meaning the original IdP is never modified. This ensures the org can continue to operate on-prem with no disruption while they build out their cloud environment and authenticate with the same on-prem credentials. When the migration is done it's a simple switch over to the GCP-based IdP as it been kept in-sync with the on-prem IdP.","comments":[{"poster":"pca2b","timestamp":"1614883380.0","content":"C\nhttps://support.google.com/a/answer/6258071#additional_user","comment_id":"303529","upvote_count":"1"}],"timestamp":"1596577500.0","comment_id":"150749"},{"poster":"premesh","timestamp":"1595608740.0","upvote_count":"2","content":"question says - The business owners require minimal user disruption.\n\nAnswer C says - provision users.\n\nCorrect answer is B","comment_id":"142899"},{"timestamp":"1594878660.0","poster":"zzaric","upvote_count":"1","comment_id":"136223","content":"B is correct."},{"content":"correct option will be B as that is standard google architecture and for minimum disruption you should always move federation to existing one.","poster":"saurabh1805","timestamp":"1594297500.0","upvote_count":"1","comment_id":"130583"},{"timestamp":"1594033200.0","upvote_count":"1","comment_id":"127710","poster":"pupi08","content":"it's B"},{"timestamp":"1593936120.0","poster":"Gobblegobble","comment_id":"126694","upvote_count":"1","content":"B is right answer"},{"timestamp":"1592889420.0","poster":"mlantonis","comment_id":"117045","upvote_count":"1","content":"I also believe it is B"},{"upvote_count":"1","poster":"kaush","timestamp":"1592869200.0","comment_id":"116907","content":"Answer is B, This is the extract from gcp documentation \nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on"},{"content":"As the question is about strict password security which may imply storing password on corporate datacenter and minimal distruption which may imply no need to resupply passwords , federated authentication with SAML with onsite Directory as single source of truth IdP seems legit .Correct answer should be B","timestamp":"1592868960.0","poster":"kaush","comment_id":"116903","upvote_count":"1"},{"upvote_count":"1","comment_id":"114570","timestamp":"1592638680.0","poster":"syu31svc","comments":[{"comment_id":"117333","upvote_count":"1","content":"Disregard; answer is B considering the question stating strict requirements","timestamp":"1592908680.0","poster":"syu31svc"}],"content":"Answer is C; B would be the answer if it's Identity Platform\nhttps://cloud.google.com/identity-platform/docs/web/saml"},{"comment_id":"108066","timestamp":"1591907340.0","poster":"Pupina","upvote_count":"3","content":"I think is B. I think it is a typicall lift&shift scenario with less disruption and where security team do not want to use google as IdP. Instead they prefer to extend current identity provider to Google Cloud https://cloud.google.com/solutions/authenticating-corporate-users-in-a-hybrid-environment"},{"poster":"bubai01","content":"C is Correct as Option B comes once GCDS is setup . Also GCDS doesn't sync password which are kept in On-Premises","timestamp":"1591408260.0","upvote_count":"1","comment_id":"103518"},{"poster":"Nirms","comment_id":"100834","upvote_count":"6","timestamp":"1591103580.0","content":"B is the correct answer"},{"poster":"jakpen","comment_id":"99361","upvote_count":"4","content":"Agree on B.\nD is obviously user disruption.\nBoth A & C will sync accounts including passwords on cloud. Although GCDS can be installed on an on-premises server, it's still synchronizing passwords.\nB works well for authentication on cloud application and it supports single sign-on.","timestamp":"1590935280.0","comments":[{"poster":"olg","comment_id":"182457","upvote_count":"1","timestamp":"1600544280.0","content":"Although Cloud Directory Sync provisions user account details, it doesn't synchronize password, B should be right answer."}]},{"poster":"Ziegler","content":"C looks a correct answer","upvote_count":"2","timestamp":"1590765000.0","comment_id":"98305"},{"comment_id":"98083","poster":"AD2AD4","upvote_count":"4","timestamp":"1590736680.0","content":"Final Decision to go with Option C","comments":[{"upvote_count":"1","comment_id":"152997","content":"Final decision updated to B","timestamp":"1596883440.0","poster":"Gobblegobble"}]},{"timestamp":"1589705340.0","content":"For me the answers is most probably B. The question says \"The business owners require minimal user disruption.\"\n\nFrom https://support.google.com/a/answer/2920764?hl=en\nIn Active Directory, passwords are stored as write-only. They can't be read through any interface, such as LDAP. Therefore, conventional synchronization methods (for example, Google Cloud Directory Sync) can't access them. The only way to read passwords is to capture them when they are set or changed.\n\nSo all users would be required to update their passwords for answer C.","upvote_count":"4","comment_id":"90440","poster":"amp87"},{"content":"I'll go with B since it maintains the source of identities within the customer without making assumptions on whether the customer needs direct access to Google.\nA does not seem reasonable because there's nothing mentioned about needing to log into Google itself.\nC does not seem reasonable because there's nothing mentioned about the need to log into Google.\nD does not seem reasonable because there should be a single source of truth for passwords.","comment_id":"86269","upvote_count":"5","poster":"clouddude","timestamp":"1589065800.0","comments":[{"timestamp":"1615006140.0","content":"Agree, in fact the authentication of customer's existing application is based on the local directory. \nBoth A & C is used to sync local directory to Google's directory, but this would require switching the application's authentication approach from local directory to Google's directory. This would definitely bring downtime and user disruption. \n\nSo the answer should be B.","comment_id":"304532","poster":"HCL","upvote_count":"1"}]},{"timestamp":"1588831560.0","content":"B. the question is about authentication not sync","poster":"Applehph","comment_id":"84859","upvote_count":"6"},{"poster":"2g","timestamp":"1580389680.0","content":"answer: C","upvote_count":"6","comment_id":"44698"},{"comment_id":"42595","content":"I think is C, maybe that \"strict requirments for storing password\" means to just host them on-premises and not in cloud, SAML would provide this, passwords are stores on-premises but user is authenticated in GCP.","timestamp":"1579965360.0","comments":[{"comment_id":"42597","poster":"Jos","upvote_count":"10","timestamp":"1579965420.0","content":"I meant B :)"}],"poster":"Jos","upvote_count":"3"},{"content":"Agree C","timestamp":"1578768840.0","poster":"AWS56","upvote_count":"6","comment_id":"37812"}],"url":"https://www.examtopics.com/discussions/google/view/7133-exam-professional-cloud-architect-topic-1-question-11/","exam_id":4,"answer_description":"","question_id":13,"choices":{"C":"Provision users in Google using the Google Cloud Directory Sync tool","D":"Ask users to set their Google password to match their corporate password","A":"Use G Suite Password Sync to replicate passwords into Google","B":"Federate authentication via SAML 2.0 to the existing Identity Provider"},"answers_community":["B (79%)","C (21%)"],"answer_ET":"B","answer":"B"},{"id":"18A7xfnEBatM07eIk3oN","choices":{"D":"Google Kubernetes Engine","B":"GKE On-Prem","A":"App Engine","C":"Compute Engine"},"question_id":14,"answers_community":["A (85%)","Other"],"answer_description":"","exam_id":4,"answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/56840-exam-professional-cloud-architect-topic-1-question-110/","unix_timestamp":1625172600,"question_text":"Your company has announced that they will be outsourcing operations functions. You want to allow developers to easily stage new versions of a cloud-based application in the production environment and allow the outsourced operations team to autonomously promote staged versions to production. You want to minimize the operational overhead of the solution. Which Google Cloud product should you migrate to?","isMC":true,"timestamp":"2021-07-01 22:50:00","answer":"A","discussion":[{"upvote_count":"36","comment_id":"396338","timestamp":"1625172600.0","poster":"kopper2019","content":"A. App Engine"},{"upvote_count":"24","content":"Answer should be A as only with App Engine we have a default service account which allows the user to deploy the changes per project. for GKE we may have to configure additional permission for both DEV and Operations team to deploy the changes.\n\nhttps://cloud.google.com/appengine/docs/standard/php/service-account","timestamp":"1627122600.0","poster":"arsav","comment_id":"413088"},{"content":"A. App Engine.\n\nExplanation:\nWhy A is correct:\n\nApp Engine: Google App Engine is a fully managed platform-as-a-service (PaaS) that allows developers to build and deploy applications quickly and easily without worrying about managing the underlying infrastructure. It supports continuous integration and continuous deployment (CI/CD) processes, enabling developers to stage new versions of applications easily.\n\nStaging and Promotion: App Engine has built-in support for traffic splitting and versioning, which allows you to stage new versions of your application and gradually promote them to production. This can be done with minimal operational overhead, making it ideal for scenarios where operational functions are outsourced.\n\nMinimal Operational Overhead: Since App Engine is fully managed, it reduces the operational burden significantly, making it easier for the outsourced operations team to handle promotions and manage the application.","comment_id":"1232483","poster":"Sephethus","timestamp":"1718723100.0","upvote_count":"2"},{"timestamp":"1717435440.0","upvote_count":"1","content":"Selected Answer: A\nA. App Engine","poster":"JaimeMS","comment_id":"1223724"},{"content":"Is this answers are really correct or misleading to us..?","timestamp":"1713503100.0","poster":"jaisonPathiyil","upvote_count":"3","comment_id":"1198328"},{"content":"Selected Answer: A\nWhile both GKE and App Engine offer functionalities for deploying cloud-based applications, App Engine is more managed service compared to GKE, resulting in lower operational overhead.","timestamp":"1709549280.0","upvote_count":"2","poster":"mesodan","comment_id":"1165511"},{"upvote_count":"3","content":"Selected Answer: A\nI did my exam today and saw this question. But I am sure A was the answer due to the operational overhead phrase","poster":"Anandmrk","timestamp":"1708529820.0","comment_id":"1155650"},{"timestamp":"1703737680.0","upvote_count":"2","poster":"[Removed]","content":"A\n\n\nwhy not D?\nIt requires more operational overhead compared to App Engine, as it involves managing the Kubernetes infrastructure.","comment_id":"1107396"},{"comment_id":"1069021","poster":"thewalker","content":"Selected Answer: D\nIn the question, we see \"cloud-based application\" - I assume, it means cloud native -> dockers/containers -> K8s -> GKE.\nHence, D is my option.","timestamp":"1699850100.0","upvote_count":"2"},{"timestamp":"1696584900.0","content":"Selected Answer: D\nI agreed with \"omermahgoub\" the answer should be D.\nAs you will bundle the application and its dependencies into container image and deploy. All environments will have same image deployed from Dev, TEST, Staging to PROD. There will be less operational overheard for operations team.","upvote_count":"1","comment_id":"1026395","poster":"AdityaGupta"},{"comment_id":"1007824","content":"Selected Answer: A\nA\n\nApp Engine reduces ops overhead","timestamp":"1694711340.0","poster":"nocrush","upvote_count":"2"},{"poster":"Frusci","upvote_count":"2","timestamp":"1694411400.0","comment_id":"1004475","content":"Selected Answer: A\nA. You deploy your new version to App Engine without setting it as the default version. The ops team then just has to make it the default version when they want to promote it. Simplest answer."},{"content":"App Engine because of less ovehead.","poster":"heretolearnazure","upvote_count":"1","timestamp":"1692968100.0","comment_id":"990074"},{"content":"Answer is A.\nBy process of elimination you arrive at App Engine or GKE. Now the requirement is to \"to minimize the operational overhead of the solution\". \nOn the IaaS to PaaS spectrum, this can only be App Engine!\n\nIaaS = Compute Engine.\nHybrid = GKE (engineering heavy).\nPaaS = App Engine.","poster":"JohnWick2020","comment_id":"913485","timestamp":"1685791500.0","upvote_count":"3"},{"poster":"Atanu","timestamp":"1685435760.0","comment_id":"910076","upvote_count":"3","content":"Selected Answer: A\n\"You want to minimize the operational overhead of the solution\" ..This sentence is they key to go with Option A. GKE carries overhead as it's not purely PaaS."},{"timestamp":"1683610620.0","poster":"Sur_Nikki","content":"It should be D. As GKE is considered to be the master product/service for creating a deployment and managing and keeping all the environments in SYNC","comment_id":"892805","upvote_count":"1"},{"comment_id":"840440","timestamp":"1678926780.0","upvote_count":"1","content":"Selected Answer: A\nA is right as the requirement is to deploy new changes and manage the application with no operational overhead.","poster":"sithin_nair"},{"timestamp":"1678240500.0","comment_id":"832450","upvote_count":"4","poster":"Deb2293","content":"Selected Answer: A\nApp Engine could be a valid option for this scenario, as it provides a managed platform for deploying and scaling web applications that can be updated with new versions. App Engine also provides a default service account that developers can use to deploy changes and manage the application.\n\nWhile GKE provides more fine-grained control over the underlying infrastructure and deployment processes, it may not be necessary for all use cases. If the application is relatively simple and does not require extensive customization or management, App Engine could be a good choice.\n\nIn the end, the choice of platform depends on the specific needs and requirements of the application and organization, and both App Engine and GKE could be valid options. It is important to evaluate each platform's strengths and weaknesses to determine which one is the best fit for the application and organization."},{"timestamp":"1677761160.0","comment_id":"826904","upvote_count":"1","poster":"abbottWang","content":"Selected Answer: A\nApp engine already has a default account which allows user to deploy change"},{"comment_id":"773141","timestamp":"1673501340.0","poster":"wences","content":"Selected Answer: D\nIn this one, we are talking about some sort of control degree over the platform, so GKE is the clear option, enabling outsourced and internal teams to be as dynamic as possible, the devil is in details but thinking in business requirement is my approach.","upvote_count":"3"},{"comment_id":"762841","upvote_count":"9","timestamp":"1672512060.0","poster":"windsor_43","content":"The Answer is A.\n\nJust had my exam today with a pass, this question was in the exam. Dated 31/12/22"},{"comments":[{"poster":"omermahgoub","content":"Option A, App Engine, is not a recommended approach. While App Engine is a fully managed platform for deploying and scaling web applications, it does not provide the same level of flexibility and control as GKE when it comes to managing the deployment and scaling of applications.\n\nOption B, GKE On-Prem, is not a recommended approach. GKE On-Prem is a version of GKE that can be deployed on-premises or in a virtual private cloud (VPC). It is not designed to be used as a cloud-based application platform.\n\nOption C, Compute Engine, is not a recommended approach. While Compute Engine is a flexible and powerful platform for running virtual machines, it requires more operational overhead to manage the deployment and scaling of applications compared to GKE. It does not provide the same level of built-in monitoring and logging capabilities as GKE.","timestamp":"1671708180.0","upvote_count":"1","comment_id":"753224","comments":[{"content":"But using GKE requires you to deploy the app as containerized app. Secondly you need to also maintain GKE cluster (additional Ops) unless you are using GKE Autopilot. Since the question did not mention containerized app, App Engine seems to be the correct answer (A).","timestamp":"1675387860.0","upvote_count":"2","poster":"VSMu","comment_id":"796600"}]}],"content":"To allow developers to easily stage new versions of a cloud-based application in the production environment and allow the outsourced operations team to autonomously promote staged versions to production while minimizing the operational overhead of the solution, you should migrate to Google Kubernetes Engine (GKE).\n\nGKE is a fully managed service for deploying and managing containerized applications on Google Cloud. It allows you to easily deploy and scale your applications, and provides features such as rolling updates and canary deployments, which can be used to stage new versions of your application in the production environment. GKE also has built-in monitoring and logging capabilities, which can help the outsourced operations team to identify and resolve issues in the production environment.","upvote_count":"3","timestamp":"1671708180.0","comment_id":"753223","poster":"omermahgoub"},{"comment_id":"716713","upvote_count":"1","timestamp":"1668261240.0","poster":"megumin","content":"Selected Answer: A\nA is ok"},{"comment_id":"705192","poster":"Balaji_Sakthi","content":"app engine","timestamp":"1666842120.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"697249","timestamp":"1666000080.0","poster":"minmin2020","content":"Selected Answer: A\nA. App Engine"},{"poster":"abirroy","upvote_count":"7","comment_id":"673664","content":"Selected Answer: A\nI chose A due to the following statement - You want to minimize the operational overhead of the solution","timestamp":"1663625220.0"},{"upvote_count":"2","content":"A is correct","poster":"AzureDP900","comment_id":"672559","timestamp":"1663517940.0"},{"poster":"alexandercamachop","content":"Selected Answer: A\nAnswer is A.\nSecret is \"minimal overhead\" = not managing infrastructure, which means App Engine.","upvote_count":"3","timestamp":"1662870360.0","comment_id":"665812"},{"upvote_count":"2","comment_id":"642686","timestamp":"1659666420.0","content":"Selected Answer: A\nhttps://cloud.google.com/appengine/docs/standard/php/service-account","poster":"DrishaS4"},{"content":"I would go with A after re-reading the question twice :)\nYou want to minimize the operational overhead of the solution --- only app engine is valid in that context......rest all cluster/vm creations which does require operational team to provision the infrastructure needed for running apps","poster":"sgofficial","comment_id":"640796","timestamp":"1659376620.0","upvote_count":"1"},{"timestamp":"1657623120.0","poster":"diego_alejandro","content":"A is correct.....App Engine is server-less and requires no overhead in operations. The other options require configuration and overhead managing the service.","upvote_count":"1","comment_id":"630464"},{"timestamp":"1657129560.0","upvote_count":"2","comment_id":"628031","content":"06/30/2022 Exam","poster":"mv2000"},{"timestamp":"1656792720.0","content":"There is no need of Kubernetes for given use case, I will go with Google App Engine. A is right!","poster":"AzureDP900","comment_id":"626297","upvote_count":"2"},{"comment_id":"598163","timestamp":"1651935720.0","upvote_count":"4","content":"Got this question in my exam. I selected A.","poster":"tannV"},{"upvote_count":"1","timestamp":"1650361020.0","comment_id":"588078","content":"Because of \"minimize operational overhead\" ...App Engine is better choice","poster":"nkit"},{"upvote_count":"2","comment_id":"582419","content":"Go for A","poster":"gaojun","timestamp":"1649333280.0"},{"upvote_count":"2","comment_id":"572291","timestamp":"1647871620.0","content":"Selected Answer: A\nCould be A or D, but based on\nhttps://cloud.google.com/appengine/docs/standard/python/roles#separation_of_deployment_and_traffic_routing_duties\n\nI think the most simple is App Engine","poster":"jvale"},{"comment_id":"548042","timestamp":"1644959640.0","content":"02/15/21 exam","upvote_count":"1","poster":"azureaspirant","comments":[{"timestamp":"1645875060.0","poster":"Paragkk","upvote_count":"3","comment_id":"556644","content":"very useful info"}]},{"content":"Got this question in my exam, answered A","poster":"technodev","upvote_count":"2","timestamp":"1642609140.0","comment_id":"527703"},{"upvote_count":"2","content":"Selected Answer: A\nApp Engine is server-less and requires no overhead in operations. The other options require configuration and overhead managing the service.","poster":"PhilipKoku","comment_id":"497987","timestamp":"1639076940.0"},{"upvote_count":"1","poster":"haroldbenites","comment_id":"497538","timestamp":"1639042440.0","content":"Go for A."},{"comment_id":"490965","upvote_count":"2","poster":"Craigenator","content":"Selected Answer: A\nAgree its A","timestamp":"1638303420.0"},{"timestamp":"1638157740.0","poster":"cdcollector","content":"Container registry for applications allows clean separation of code generation vs deployment","upvote_count":"1","comment_id":"489578"},{"comment_id":"488109","timestamp":"1638015360.0","content":"Selected Answer: A\nit's A","upvote_count":"1","poster":"pakilodi"},{"content":"Selected Answer: B\nDEV can deploy to GKE-onprem and then ops can promote them to GKE [Read Anthos use cases]","poster":"mudot","upvote_count":"2","comment_id":"487595","timestamp":"1637955540.0"},{"timestamp":"1637844720.0","comment_id":"486654","content":"Selected Answer: A\nvote A","upvote_count":"3","poster":"joe2211"},{"timestamp":"1637719320.0","comment_id":"485550","content":"is A because, App Engine is PaaS so you have less operational overhead, in GKE you have administrative tasks around the management of kubernetes clusters.","poster":"Arlima","upvote_count":"1"},{"poster":"[Removed]","timestamp":"1637626800.0","upvote_count":"3","content":"Selected Answer: A\nA is right and appropriate","comment_id":"484649"},{"timestamp":"1637392980.0","content":"answer id D\nGoogle Kubernetes Engine is a managed, production-ready environment that allows portability across different clouds as well as on premises environments.\n\nWe don't known what cloud thid party is using so eliminate AppEngine","poster":"nehaxlpb","comment_id":"482323","comments":[{"content":"GKE is a GCP specific service as is AppEngine so the last statement doesn't stand correct","comment_id":"673666","upvote_count":"1","timestamp":"1663625340.0","poster":"abirroy"}],"upvote_count":"3"},{"content":"App Engine - is a better answer","poster":"rottzy","timestamp":"1632905280.0","comment_id":"453967","upvote_count":"2"},{"content":"GKE does not allow staging a version .. so A.","upvote_count":"3","poster":"vickynag","comment_id":"407010","timestamp":"1626347040.0"},{"comments":[{"content":"I think the answer is A","poster":"Urban_Life","timestamp":"1639541220.0","comment_id":"501850","upvote_count":"1"},{"poster":"abirroy","timestamp":"1663625400.0","comment_id":"673668","upvote_count":"1","content":"Production version is also deployed in AppEngine"}],"upvote_count":"2","comment_id":"403803","poster":"Urban_Life","content":"If it's a production version, go with GKE","timestamp":"1625994120.0"},{"timestamp":"1625982780.0","poster":"victory108","upvote_count":"4","comment_id":"403705","content":"A. App Engine"},{"timestamp":"1625916000.0","upvote_count":"1","poster":"taoj","comment_id":"403291","content":"if the operational overhead doesn't equal to the service price. the A is the simplest"},{"comment_id":"401768","upvote_count":"1","poster":"DuncanK53","content":"Think D. Doesn't mention 'minimal infrastructure admin' like a later question thereby implying A. But A does work tbf.","timestamp":"1625738700.0"},{"timestamp":"1625565300.0","upvote_count":"3","content":"ans is A","comment_id":"399841","poster":"dguillenca"},{"timestamp":"1625550660.0","comment_id":"399684","poster":"JeffClarke111","upvote_count":"4","content":"A or D seems both to apply... I would go for A"},{"poster":"XDevX","upvote_count":"6","comment_id":"397092","timestamp":"1625248260.0","comments":[{"poster":"amxexam","upvote_count":"2","content":"Please give reason why D","comment_id":"439529","timestamp":"1630824900.0"}],"content":"IMHO d is correct"}],"topic":"1","answer_ET":"A"},{"id":"ajcrjJIxHMwqgGpym8h5","isMC":true,"choices":{"C":"Deploy the development and acceptance applications on a managed instance group and enable autoscaling.","B":"Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after office hours and start them just before office hours.","A":"Create a shell script that uses the gcloud command to change the machine type of the development and acceptance instances to a smaller machine type outside of office hours. Schedule the shell script on one of the production instances to automate the task.","D":"Use regular Compute Engine instances for the production environment, and use preemptible VMs for the acceptance and development environments."},"answer":"B","question_text":"Your company is running its application workloads on Compute Engine. The applications have been deployed in production, acceptance, and development environments. The production environment is business-critical and is used 24/7, while the acceptance and development environments are only critical during office hours. Your CFO has asked you to optimize these environments to achieve cost savings during idle times. What should you do?","answers_community":["B (73%)","C (25%)","3%"],"discussion":[{"upvote_count":"38","comment_id":"400853","content":"B is the answer.\n\nhttps://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs \nSchedule VMs to auto start and stop: The benefit of a platform like Compute Engine is that you only pay for the compute resources that you use. Production systems tend to run 24/7; however, VMs in development, test or personal environments tend to only be used during business hours, and turning them off can save you a lot of money! \n\nhttps://cloud.google.com/blog/products/storage-data-transfer/save-money-by-stopping-and-starting-compute-engine-instances-on-schedule\n\nCloud Scheduler, GCP’s fully managed cron job scheduler, provides a straightforward solution for automatically stopping and starting VMs. By employing Cloud Scheduler with Cloud Pub/Sub to trigger Cloud Functions on schedule, you can stop and start groups of VMs identified with labels of your choice (created in Compute Engine). Here you can see an example schedule that stops all VMs labeled \"dev\" at 5pm and restarts them at 9am, while leaving VMs labeled \"prod\" untouched","comments":[{"comment_id":"640799","timestamp":"1659377280.0","content":"Excellent ......even the good CFO is telling leave the office after 5.oo and come next day to work :)","poster":"sgofficial","upvote_count":"16"},{"poster":"Ric350","comment_id":"631924","timestamp":"1657916760.0","upvote_count":"2","content":"Great answer and documentation. Def B"},{"timestamp":"1659207840.0","upvote_count":"19","poster":"rzygor","comment_id":"639784","content":"Question says that dev/test are \"not critical\", it doesn't mean that they are not needed at all ..."}],"poster":"pamepadero","timestamp":"1625660220.0"},{"timestamp":"1625172540.0","content":"Ans ) B , assuming VM doesn't need to be up after office hours .","upvote_count":"26","comment_id":"396337","poster":"kopper2019"},{"timestamp":"1735932780.0","content":"Selected Answer: B\nCloud scheduler considering the requisites.","poster":"plumbig11","upvote_count":"1","comment_id":"1336132"},{"comments":[{"timestamp":"1740314580.0","comment_id":"1360499","content":"problem is you won't know what time preemptible VMs will shut down. There is only a brief warning from Google. Hence not suitable since they need to be up during work hours.","poster":"david_tay","upvote_count":"1"}],"timestamp":"1735416660.0","comment_id":"1333137","content":"Selected Answer: D\nBy utilizing preemptible VMs for the acceptance and development environments, you can significantly reduce costs without compromising the availability and performance of your critical production environment.","poster":"rrope","upvote_count":"1"},{"comment_id":"1289102","timestamp":"1727284680.0","content":"Selected Answer: C\nStop the dev and acceptance envs is super weird. Any critical problems or overtimes will be an issue with this approach. Simple auto scaling environment is a good solution IMHO","poster":"25lion52","upvote_count":"1"},{"poster":"Gino17m","upvote_count":"1","content":"B is right answer","comment_id":"1198646","timestamp":"1713534900.0"},{"comment_id":"1191390","content":"Selected Answer: B\nAgree with B","poster":"dija123","upvote_count":"1","timestamp":"1712553840.0"},{"content":"Answer D\n\nA: too complex and maybe small or zero saving if you can't find a valid smaller machine type\nB: Not valid. Question says that PRE environments are not critical after office hours. But it doesn't say no service at all\nC: Some risk is introduced if you have different architecture on PRE than PRO envs\nD: It's the only valid and realiable option. Simple and effective. It's my choice. In a real scenario I will first start with this and then review if the savings are enough before more complicated choices","upvote_count":"3","poster":"spuyol","comments":[{"comment_id":"1198644","timestamp":"1713534840.0","upvote_count":"1","content":"Ad. \"B: Not valid. Question says that PRE environments are not critical after office hours. But it doesn't say no service at all\"\n\nBut the Question says that PRE environments are critical during office hours, so you can't use preemptible VMs - \"Compute Engine might stop (preempt) these instances if it needs to reclaim the compute capacity for allocation to other VMs\"","poster":"Gino17m"}],"comment_id":"1139467","timestamp":"1706978280.0"},{"poster":"the1dv","timestamp":"1705367160.0","upvote_count":"3","comment_id":"1123800","comments":[{"poster":"spuyol","content":"some risks are added if you have different architecture on PRO and PRE envs","upvote_count":"1","comment_id":"1139458","timestamp":"1706977740.0"}],"content":"Selected Answer: C\nMIG's with autoscaling will scale to Zero if not needed"},{"timestamp":"1704247080.0","content":"B for sure","comment_id":"1112417","upvote_count":"1","poster":"AWS_Sam"},{"timestamp":"1703136300.0","comment_id":"1102199","content":"Selected Answer: C\nAlso managed instance group reduces instances in case of low/no-traffic incurring lesser charges. Ideally, its a cleaner approach considering the ask is to optimize during \"idle time\". Incase people are working in different time zones, late shifts it doesnt make sense to trigger shutdown at a predefined times.","poster":"parthkulkarni998","upvote_count":"3"},{"timestamp":"1700310900.0","comment_id":"1073984","content":"Selected Answer: B\nB is the answer. But today, you don't need complicated CRON + CF. Auto shutdown by cron expression it's a feature built in: https://cloud.google.com/compute/docs/instances/schedule-instance-start-stop","upvote_count":"2","poster":"odacir"},{"poster":"werdy92","comment_id":"1052146","timestamp":"1698085020.0","upvote_count":"5","content":"really wondering why not C...Not critical is not equivalent with not running at all....","comments":[{"comment_id":"1102198","poster":"parthkulkarni998","content":"Also managed instance group reduces instances in case of low/no-traffic incurring lesser charges. Ideally, its a cleaner approach considering the ask is to optimize during \"idle time\". Incase people are working in different time zones, late shifts it doesnt make sense to trigger shutdown at a predefined times.","upvote_count":"2","timestamp":"1703136240.0"}]},{"comment_id":"964575","poster":"wooloo","upvote_count":"6","timestamp":"1690452360.0","content":"\"are only critical during office hours\" does not mean it could be completely stopped. So may the option C correct?"},{"upvote_count":"3","comments":[{"comment_id":"1226756","timestamp":"1717855200.0","upvote_count":"1","poster":"ccpmad","content":"Just exactly what I have thought. It is enough with instance schedule\". But GCP wants you to spend money and use cloud functions LOL"}],"timestamp":"1679731680.0","poster":"mifrah","content":"In my opinion B is over-engineered:\nWhy not just add an \"instance schedule\" for start/stop the Compute Engines?\nWhy creating a scheduler and writing a Cloud Function...","comment_id":"849914"},{"upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations\nB seems close to this Google provided service option, the extra step should be using idle VM recommendations to find and stop idle VM instances to reduce waste of resources","comment_id":"819672","timestamp":"1677182160.0","poster":"MaryMei"},{"comments":[{"content":"Yes, but preemptibles use GCP excess resources so you will achieve the opposite of the desired effect, during office hours, they will underperform in the best case (worst case will stop altogether) and, during non-office hours, preemptibles will work well...","poster":"DevOpsifier","comment_id":"922323","upvote_count":"1","timestamp":"1686668820.0"}],"upvote_count":"3","content":"Since the price of preemptibles is 1/4 the price of a standard machine D costs far less than B since office hours are 1/3 of whole day. It costs less to keep them running 24h as preemptibles.","comment_id":"814231","poster":"PAUGURU","timestamp":"1676822280.0"},{"upvote_count":"3","poster":"windsor_43","comment_id":"762844","content":"The Answer is B.\n\nJust had my exam today with a pass, this question was in the exam. Dated 31/12/22\nThanks to this site it was by far my most valuable","timestamp":"1672512180.0"},{"upvote_count":"1","timestamp":"1671708720.0","comment_id":"753230","content":"The correct answer is B. Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after office hours and start them just before office hours.\n\nOne way to optimize the cost of your Compute Engine environments is to stop non-critical instances when they are not in use. In this case, you could use Cloud Scheduler to trigger a Cloud Function that will stop the acceptance and development environments after office hours and start them just before office hours. This will allow you to take advantage of the cost savings of not running these environments during idle times, while still ensuring that they are available during office hours when they are critical.","poster":"omermahgoub","comments":[{"content":"Option A, creating a shell script to change the machine type of the development and acceptance instances, is not relevant because it does not address the issue of cost optimization during idle times. \n\n\nOption C, using a managed instance group with autoscaling, is not recommended because it would not allow you to take advantage of the cost savings of stopping the instances during idle times. \n\nOption D, using preemptible VMs for the acceptance and development environments, is also not recommended because preemptible VMs may be terminated by Google at any time, which would not be suitable for workloads that are critical during office hours.","upvote_count":"1","comment_id":"753232","timestamp":"1671708780.0","poster":"omermahgoub"}]},{"comment_id":"752367","upvote_count":"1","timestamp":"1671629640.0","content":"Selected Answer: B\nAs B seems the best answer, sadly it's not precise, as you need pub/sub in between.\nhttps://cloud.google.com/scheduler/docs/overview","poster":"oms_muc"},{"comment_id":"716720","content":"Selected Answer: B\nB is ok","timestamp":"1668261720.0","poster":"megumin","upvote_count":"1"},{"poster":"BiddlyBdoyng","upvote_count":"1","comment_id":"710670","content":"Problem with C is it might offer no cost saving if during office hours we see constant use of min might scale. B is best, C\n can enhance B","timestamp":"1667495400.0"},{"comment_id":"652762","timestamp":"1661643840.0","poster":"pp0709","comments":[{"poster":"melono","timestamp":"1665909780.0","content":"D sayes regular VMs for prod\nWhat are you talking about?\nOn the other hand, not critical does not mean they are not needed at all.","comment_id":"696104","upvote_count":"1"}],"content":"If I get this question in the exam, I will choose B and here are the reasons - \nThe key is 1) Acceptance and Develeopment are CRITICAL during office hours. If you use Preemprible, while it is guranteed to reduce cost, it can also get preempted during business Critical hours. Hence D is incorrect. A and C are obvious deletes and hence B","upvote_count":"2"},{"content":"Selected Answer: B\nThe debate between option B and option C reminds me of the fact that in IT industry there are no business hours :)\n\nThinking from regulations point of view as well, after business hours means employees are not expected to work if off office timeframe. In case there is an incident and someone has to work test out the things, then they can spin up the instance.\n\nHence if is better not to \"assume\" that employee(s) would be working after the business hours and we should be good to stop the instances in the relevant projects/environments.","poster":"RitwickKumar","upvote_count":"3","comment_id":"648969","timestamp":"1660912620.0"},{"poster":"cloudinit","comment_id":"648624","content":"Selected Answer: B\nThe answer is B.\nSee the below from the document: https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs\nSchedule VMs to auto start and stop: The benefit of a platform like Compute Engine is that you only pay for the compute resources that you use. Production systems tend to run 24/7; however, VMs in development, test or personal environments tend to only be used during business hours, and turning them off can save you a lot of money! For example, a VM that runs for 10 hours per day, Monday through Friday costs 75% less to run per month compared to leaving it running.","timestamp":"1660861080.0","upvote_count":"1"},{"poster":"patashish","timestamp":"1660120500.0","content":"B is right","comment_id":"644867","upvote_count":"1"},{"comment_id":"630471","timestamp":"1657623960.0","upvote_count":"8","content":"Ans is C. It states Acceptance and development are \"only critical\" during business hs, it doesn't mean they are not used after bs hs. Using a MIG can help reducing the infrastructure while still allowing to use those environments.","poster":"diego_alejandro"},{"comment_id":"628430","content":"Ans is C. It states Acceptance and development are \"only critical\" during business hs, it doesn't mean they are not used after bs hs. Using a MIG can help reducing the infrastructure while still allowing to use those environments.","upvote_count":"1","timestamp":"1657213860.0","poster":"cbarg"},{"timestamp":"1657129620.0","comment_id":"628033","content":"06/30/2022 Exam","upvote_count":"2","poster":"mv2000"},{"content":"B is right","comment_id":"626301","poster":"AzureDP900","upvote_count":"1","timestamp":"1656793080.0"},{"upvote_count":"1","content":"B is right !","timestamp":"1656792960.0","poster":"AzureDP900","comment_id":"626300"},{"comment_id":"617766","content":"Selected Answer: B\nB is the correct answer. It is the most cost efficient/optimal","poster":"MarcExams","timestamp":"1655477340.0","upvote_count":"1"},{"timestamp":"1654982520.0","poster":"H_S","content":"Selected Answer: B\nAns ) B , assuming VM doesn't need to be up after office hours .","comment_id":"615085","upvote_count":"1"},{"content":"07/05/22 exam","timestamp":"1651935720.0","poster":"tannV","comment_id":"598165","upvote_count":"3"},{"timestamp":"1650346320.0","poster":"welkinwalker","comment_id":"588002","upvote_count":"3","content":"Selected Answer: C\nB is not right, because you can't stop the develop environment. \nD is wrong, since using preemptive instance will hamper the development environments continuity"},{"upvote_count":"1","content":"Go for B, \"The production environment is business-critical and is used 24/7\"","comment_id":"582422","poster":"gaojun","timestamp":"1649333640.0"},{"poster":"brvinod","timestamp":"1648470600.0","upvote_count":"2","content":"Selected Answer: B\nB is the correct answer. The link given in the answer explanation is correct but the answer is B. ( https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs\nGo to: \"2. Only pay for the compute you need\" section in the page.)","comment_id":"576849"},{"content":"Selected Answer: B\nB it is","timestamp":"1646500260.0","poster":"ss909098","comment_id":"561534","upvote_count":"1"},{"upvote_count":"2","timestamp":"1643643720.0","poster":"mbenhassine1986","comment_id":"537207","content":"Clearly here , ANSWER B\nhttps://cloud.google.com/blog/products/storage-data-transfer/save-money-by-stopping-and-starting-compute-engine-instances-on-schedule"},{"content":"No make sense D.\nA business requirement: \"the acceptance and development environments are only critical during office hours\"\n\nWe have many reasons not to choose D:\n- Compute Engine might stop preemptible instances at any time;\n- Compute Engine always stops preemptible instances after they run for 24 hours;\n- Can't automatically restart when there is a maintenance event;\n- Are finite Compute Engine resources, so they might not always be available","comment_id":"511491","poster":"ehgm","timestamp":"1640717760.0","upvote_count":"1"},{"comment_id":"510352","upvote_count":"1","poster":"Pramodkumarnayak","timestamp":"1640613360.0","content":"Ans )B because here specific time need to act i.e off office hours . preemptible Can be stopped by GCP any time (preempted) within 24 hours."},{"content":"Selected Answer: B\n\"critical\" during business hours, and (d) pre-emtiple doesn't meet this requirement.","comment_id":"506150","poster":"dk2u90fh","upvote_count":"5","timestamp":"1640094660.0"},{"poster":"PhilipKoku","comment_id":"497996","upvote_count":"2","content":"Selected Answer: B\nB) Enables to stop the environments when they are not required allowing you to pay for only when the resources are required.","timestamp":"1639078620.0"},{"content":"Go for C.","poster":"haroldbenites","comment_id":"497554","upvote_count":"4","timestamp":"1639043040.0"},{"content":"IMO, B is the correct answer\n\nFrom Google - \"Compute Engine might stop preemptible instances at any time due to system events. The probability that Compute Engine stops a preemptible instance for a system event is generally low, but might vary from day to day and from zone to zone depending on current conditions.\"\n\nIt should be interpreted that the said machines can get disrupted during office hours as well. Hence it only satisfies one requirement - which is LOW COST. But it does not guarantee that it will always be online during office hours. Hence Option D seems a misfit.","comment_id":"496815","timestamp":"1638966780.0","poster":"menon_sarath","upvote_count":"4"},{"comment_id":"493681","timestamp":"1638618360.0","poster":"daniva","content":"Selected Answer: B\ncorrect answer should be B - why is the suggested answer D?","upvote_count":"1"},{"poster":"pakilodi","upvote_count":"1","content":"Selected Answer: B\nB is the answer","timestamp":"1638457200.0","comment_id":"492603"},{"comment_id":"492142","upvote_count":"1","poster":"vincy2202","timestamp":"1638419580.0","content":"Selected Answer: B\nB is the correct answer.\nhttps://cloud.google.com/blog/products/compute/5-best-practices-compute-engine-cost-optimization"},{"comment_id":"490971","timestamp":"1638303720.0","upvote_count":"1","poster":"Craigenator","content":"Selected Answer: B\nAnswer is B"},{"content":"Selected Answer: B\nVote B","comment_id":"487882","poster":"nqthien041292","upvote_count":"1","timestamp":"1637994120.0"},{"content":"Selected Answer: B\nnot 'D' because - you wouldnt want your dev and acceptance envs to go with a minimim notice","upvote_count":"1","poster":"mudot","timestamp":"1637955840.0","comment_id":"487597"},{"upvote_count":"1","poster":"DMC1163","timestamp":"1637605140.0","comment_id":"484452","content":"B should be the answer. Acceptance and dev is critical during office hours and so you wouldn't want them to be preempted during those hours. So, can't be D."},{"upvote_count":"1","timestamp":"1637423520.0","content":"Question has started with \"Your company is running its application workloads on Compute Engine\" and it is not about executing application but sharing workload to secondary system. If it is about workloads then Answer is \"D\" and if about application then Answer is \"B\".","comment_id":"482675","poster":"Nikki17"},{"poster":"ravisar","content":"B is the answer - https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs . Use Cloud Scheduler and Pub/Sub to automate shutdown and start of development Vms during/after office hours","timestamp":"1637353020.0","comment_id":"482069","upvote_count":"1"},{"comment_id":"473726","timestamp":"1636257420.0","upvote_count":"1","content":"vote for B","poster":"exam_war"},{"upvote_count":"2","poster":"[Removed]","timestamp":"1634841900.0","content":"B is right here.\n D can't be taken because dev machines are critical during working hours. Preemptive machine are good for batch work that can be resumed or other slow jobs if resources consume is higher and machines are shutdown.","comment_id":"465823"},{"content":"is correct because the development environments are not required afteroffice hours and thus, it makes sense to stop them during the time that theyare not in use and then start the instances during office hours. This way, it ispossible to optimize the cost savings to the maximum, and Cloud Schedulerand Cloud Functions work very well for this task.","upvote_count":"3","comment_id":"458870","poster":"imranmani","timestamp":"1633628700.0"},{"upvote_count":"1","poster":"rottzy","timestamp":"1632905520.0","content":"\"non-prod is critical during business hours\"\nprem-VM will run during non-business hours for non-prod = so how is it more efficient?","comment_id":"453969"},{"comment_id":"453818","upvote_count":"2","timestamp":"1632891000.0","poster":"AnilKr","content":"B is correct - cloud scheduler>>cloud function>> start/stop VM"},{"timestamp":"1630996320.0","content":"D is the right answer","poster":"dmalamo","comments":[{"content":"You're trying to go for cost savings. Cloud functions will completely shut down the env, hence the only cost will be of persistent disks and any static IPs. However, if you use a managed instance then it will not scale to 0 and there will be a higher cost. The Cloud Scheduler task can be tweaks to stop and start or be manually triggered for an cycle start, if necessary, so answer is B!","comment_id":"441032","poster":"pr2web","upvote_count":"1","timestamp":"1631036340.0"}],"upvote_count":"1","comment_id":"440744"},{"comment_id":"430528","upvote_count":"1","poster":"meh_33","timestamp":"1629788640.0","content":"B is fine"},{"poster":"Papafel","upvote_count":"3","timestamp":"1626503100.0","comment_id":"408284","content":"B is the correct answer!"},{"comment_id":"403706","poster":"victory108","timestamp":"1625982840.0","content":"B. Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after office hours and start them just before office hours.","upvote_count":"6"},{"content":"Considering WFH, Overtime....Definitely C","poster":"taoj","timestamp":"1625916480.0","comment_id":"403296","upvote_count":"3"},{"content":"Correct Ans B\n- You could label instances that only developers use during business hours with “env: development.” You could then use Cloud Scheduler to schedule a serverless Cloud Function to shut them down over the weekend or after business hours and then restart them when needed. Here is an architecture diagram and code samples that you can use to do this yourself.","timestamp":"1625761800.0","upvote_count":"4","comment_id":"402050","poster":"VishalB"},{"comments":[{"comment_id":"401433","timestamp":"1625707860.0","content":"not critical during out of working hours so power off and power on B","upvote_count":"3","poster":"kopper2019"},{"upvote_count":"1","content":"Explain to the executive that requested, how you will be spending every day N hours daily just for spinning up, configuring, populating data for those 2 environments. Sounds like a Deal!\n\nNope, Answer is B, you want to have some savings, but not at the cost of jeopardizing your entire development life cycle.","timestamp":"1627429260.0","comment_id":"415775","poster":"poseidon24"}],"poster":"DiegoMDZ","upvote_count":"8","comment_id":"400973","timestamp":"1625667240.0","content":"NOT CRITICAL isn't the same that NOT USED... so C is my option..."},{"timestamp":"1625436240.0","content":"IMO b and c are both good but I will pick B because google's best practice blog on VM cost-saving explicitly mentions cloud scheduler + cloud functions + business hours, \n\"Automation is greatly simplified using a label—a key-value pair applied to various Google Cloud services. For example, you could label instances that only developers use during business hours with “env: development.” You could then use Cloud Scheduler to schedule a serverless Cloud Function to shut them down over the weekend or after business hours and then restart them when needed. \"\nhttps://cloud.google.com/blog/products/compute/5-best-practices-compute-engine-cost-optimization","comments":[{"comment_id":"398659","upvote_count":"3","comments":[{"comment_id":"1124812","upvote_count":"1","poster":"SSS987","content":"Most convincing explanations for both \"Why B\" and \"Why not C\". Thanks. I think they have badly-worded the question - they should have used the same phrase - \"used only during business hours\"!","timestamp":"1705482540.0"}],"timestamp":"1625436600.0","content":"One more thing is that question mentions that \"the applications HAVE BEEN deployed in production, development and acceptance environments.\"\nThis makes me think that the environments are already up and running, and going with option C will require tearing down of existing environments and creation of MIG related pre-requisites and then switching the dev and acceptance workloads to these new environments. This is too much work TBH","poster":"nohel"}],"upvote_count":"4","poster":"nohel","comment_id":"398654"},{"comment_id":"398124","timestamp":"1625385600.0","poster":"manishi","comments":[{"upvote_count":"1","timestamp":"1625707980.0","poster":"kopper2019","comment_id":"401436","content":"why have 1 running out of office when they are not needed, they are critical during working hours leave as they are and power off after 5pm"}],"content":"IMO answer should be C. It is clear that A & D are ruled out as stated by XDevX below.\nin c you are shutting down the VM. The requirement says that during after office hours the dev and acceptence are not critical. They didn't say they dont need it at all. \nif they would have said they dont use them after office hours then B would be the answer.\nBut C will minimize the number of VM's based on the usage . May be 1 vm will be running.","upvote_count":"4"},{"comment_id":"398015","timestamp":"1625374920.0","poster":"siddharthmehta72","content":"B was my answer, passed last week","upvote_count":"5"},{"poster":"Ssoumya","upvote_count":"1","comment_id":"397458","content":"Schedule VMs to auto start and stop: The benefit of a platform like Compute Engine is that you only pay for the compute resources that you use. Production systems tend to run 24/7; however, VMs in development, test or personal environments tend to only be used during business hours, and turning them off can save you a lot of money! For example, a VM that runs for 10 hours per day, Monday through Friday costs 75% less to run per month compared to leaving it running. \n\nTo get started, here’s a serverless solution that we developed to help you automate and manage automated VM shutdown at scale.","timestamp":"1625304840.0"},{"poster":"XDevX","comment_id":"395617","timestamp":"1625122080.0","upvote_count":"8","comments":[{"poster":"6b13108","upvote_count":"1","timestamp":"1701996900.0","comment_id":"1090691","content":"Great Explanation and doc!., only for this I was convinced to move from C to B... Thanks XDevx"}],"content":"IMHO d) is the wrong answer - test and development are criticial during business hours, so you cannot use preemptible instances during business hours.\n\nWe can exclude a) as that makes no sense considering that there are easy to manage alternatives.\n\nI see 2 realistic alternatives: b) and c) \nConsidering that we don't know how the load is during business hours (e.g. only 1 VM for dev and only 1 VM for test), it is difficult to choose c) - in case of 100 VMs in a MIG, we might consider to scale down to 1 and argue that 1 VM for 16 hours (non-office hours), that means alternative c), is cheaper than a lot of machines running 8 hours and then are stopped for 16 hours, alternative b). \n\nAs we don't know the details we have to choose b), considering that google propagates the approach.\nSee: https://cloud.google.com/blog/products/storage-data-transfer/save-money-by-stopping-and-starting-compute-engine-instances-on-schedule"}],"answer_images":[],"answer_description":"","question_images":[],"unix_timestamp":1625122080,"topic":"1","timestamp":"2021-07-01 08:48:00","exam_id":4,"answer_ET":"B","question_id":15,"url":"https://www.examtopics.com/discussions/google/view/56686-exam-professional-cloud-architect-topic-1-question-111/"}],"exam":{"id":4,"isBeta":false,"isImplemented":true,"isMCOnly":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":279,"provider":"Google","name":"Professional Cloud Architect"},"currentPage":3},"__N_SSP":true}