{"pageProps":{"questions":[{"id":"ToFgRh11r7kGWWeGlcMi","topic":"1","discussion":[{"comment_id":"1116609","timestamp":"1704718140.0","upvote_count":"7","poster":"einchkrein","content":"Serve the last state data directly from Cloud SQL to the API.\nHere's why this option is most suitable:\n\nBigQuery for Analytics: BigQuery is an excellent choice for storing and analyzing large datasets like your 10 PB of historical product data. It is designed for handling big data analytics efficiently and cost-effectively.\n\nCloud SQL for Last State Data: Cloud SQL is a fully managed relational database that can effectively handle the storage of the last known state of products. Storing this subset of data (about 10 GB) in Cloud SQL allows for optimized and faster query performance for your API needs. Cloud SQL can comfortably handle the requirement of up to 1000 QPS with sub-second latency.\n\nSeparation of Concerns: This approach separates the analytics workload (BigQuery) from the operational query workload (Cloud SQL). This separation ensures that analytics queries do not interfere with the operational performance of the API and vice versa."},{"comment_id":"1124012","timestamp":"1705393200.0","poster":"datapassionate","content":"Selected Answer: D\nD. 1. Store the historical data in BigQuery for analytics.\n2. In a Cloud SQL table, store the last state of the product after every product change.\n3. Serve the last state data directly from Cloud SQL to the AP\n\nThis approach leverages BigQuery's scalability and efficiency for handling large datasets for analytics. BigQuery is well-suited for managing the 10 PB of historical product data. Meanwhile, Cloud SQL provides the necessary performance to handle the API queries with the required low latency. By storing the latest state of each product in Cloud SQL, you can efficiently handle the high QPS with sub-second latency, which is crucial for the API's performance. This combination of BigQuery and Cloud SQL offers a balanced solution for both the large-scale analytics and the high-performance API needs.","upvote_count":"7"},{"upvote_count":"1","content":"Selected Answer: D\nWhy not A? Because BQ API 100 reqs per second for API method. Other possible limits do not meet the 1000 QPS requirement. Yes, max number of tabledata.list reqs per second is 1000 but we won't always call tabledata.list all time.\nhttps://cloud.google.com/bigquery/quotas#api_request_quotas","comment_id":"1353806","timestamp":"1739091600.0","poster":"zanhsieh"},{"comment_id":"1326611","poster":"clouditis","content":"Selected Answer: A\nA is the most plausible option - Cloud SQL can not retrieve results out with 1 second latency as the requirement here is, with BQ MV\"s that could be a possibility as its pre-computed.","timestamp":"1734210840.0","upvote_count":"2"},{"poster":"ToiToi","content":"Selected Answer: A\nWhy A? Because:\n\nMaterialized View for API: A materialized view in BigQuery pre-computes the last known state of each product. This ensures that your API can quickly retrieve the latest product information without needing to query the entire historical dataset.   \nBigQuery for API Serving: BigQuery can handle high query volumes with low latency, meeting your requirement of 1000 QPS with sub-second latency.\nCost-Effectiveness: This solution avoids the need for a separate database like Cloud SQL, minimizing costs and management overhead.\n\nWhy not D:\nWhile Cloud SQL is a good option for transactional workloads, it's not as cost-effective or scalable as BigQuery for analytical queries on 10 PB of data. It might also not be the ideal choice for serving high-volume API requests with low latency.","upvote_count":"1","timestamp":"1730406480.0","comment_id":"1305583"},{"comments":[{"upvote_count":"1","content":"what transactional workload? you just need to provide latest status for each product through an API. Select from a 10GB BQ MV will provide the result in 1 sec.","timestamp":"1738083540.0","comment_id":"1348040","poster":"Ryannn23"}],"content":"Selected Answer: D\nWhy not A:\nServing data directly from BigQuery to the API may not meet the low latency requirements for high QPS operations, as BigQuery is optimized for analytical queries rather than transactional workloads.","comment_id":"1225117","timestamp":"1717641960.0","upvote_count":"1","poster":"Anudeep58"},{"poster":"josech","content":"Selected Answer: A\nMaterialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency. Materialized views can optimize queries with high computation cost and small dataset results. https://cloud.google.com/bigquery/docs/materialized-views-intro#use_cases\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro","timestamp":"1716067260.0","upvote_count":"1","comment_id":"1213474"},{"content":"Selected Answer: D\nWhy D is the best choice:\n\nCost-Effective Analytics: BigQuery excels at handling large datasets (10 PB) and complex analytical queries. Its columnar storage and massively parallel processing make it ideal for analyzing historical product data.\nHigh-Performance API: Cloud SQL provides a managed relational database service optimized for transactional workloads. It can easily handle the 1000 QPS requirement with low latency, ensuring fast API responses.\nSeparation of Concerns: Storing historical data in BigQuery and the last known state in Cloud SQL separates analytical and transactional workloads, optimizing performance and cost for each use case.","comment_id":"1191238","timestamp":"1712530380.0","upvote_count":"1","poster":"CGS22"},{"poster":"JyoGCP","content":"Selected Answer: D\nOption D","timestamp":"1708396140.0","comment_id":"1154428","upvote_count":"1"},{"comment_id":"1152216","timestamp":"1708115220.0","poster":"ML6","upvote_count":"1","content":"Selected Answer: D\nBigQuery = data warehouse that is optimized for querying and analyzing large datasets using SQL. Can easily process petabytes of data.\nCloud SQL = designed for transactional workloads and traditional relational database use cases, such as web applications, e-commerce platforms, and content management systems."},{"poster":"Matt_108","timestamp":"1705145400.0","content":"Selected Answer: D\nOption D is the right one, compared to option A, Cloud SQL is more efficient and cost effective for the amount of time the data needs to be accessed by the api","upvote_count":"3","comment_id":"1121562"},{"timestamp":"1704282960.0","upvote_count":"2","poster":"scaenruy","comment_id":"1112724","comments":[{"timestamp":"1708954920.0","poster":"RenePetersen","upvote_count":"2","content":"I believe the latency of BigQuery is too high to accommodate the sub-second latency requirement.","comment_id":"1159802"}],"content":"Selected Answer: A\nA. 1. Store the historical data in BigQuery for analytics.\n2. Use a materialized view to precompute the last state of a product.\n3. Serve the last state data directly from BigQuery to the API."}],"unix_timestamp":1704282960,"answer_ET":"D","answer_images":[],"question_images":[],"question_id":151,"answer":"D","answer_description":"","isMC":true,"question_text":"You migrated a data backend for an application that serves 10 PB of historical product data for analytics. Only the last known state for a product, which is about 10 GB of data, needs to be served through an API to the other applications. You need to choose a cost-effective persistent storage solution that can accommodate the analytics requirements and the API performance of up to 1000 queries per second (QPS) with less than 1 second latency. What should you do?","url":"https://www.examtopics.com/discussions/google/view/130177-exam-professional-data-engineer-topic-1-question-234/","exam_id":11,"answers_community":["D (71%)","A (29%)"],"timestamp":"2024-01-03 12:56:00","choices":{"B":"1. Store the products as a collection in Firestore with each product having a set of historical changes.\n2. Use simple and compound queries for analytics.\n3. Serve the last state data directly from Firestore to the API.","A":"1. Store the historical data in BigQuery for analytics.\n2. Use a materialized view to precompute the last state of a product.\n3. Serve the last state data directly from BigQuery to the API.","D":"1. Store the historical data in BigQuery for analytics.\n2. In a Cloud SQL table, store the last state of the product after every product change.\n3. Serve the last state data directly from Cloud SQL to the API.","C":"1. Store the historical data in Cloud SQL for analytics.\n2. In a separate table, store the last state of the product after every product change.\n3. Serve the last state data directly from Cloud SQL to the API."}},{"id":"4Q7lCDIsbcZHMxSGLXYp","discussion":[{"comment_id":"1160059","upvote_count":"8","poster":"cuadradobertolinisebastiancami","content":"D\n\n* Transformations are in Dataproc and BigQuery. So you don't need operators for GCS (A and B can be discard)\n* \"There is no fixed schedule for when the new data arrives.\" so you trigger the DAG when a file arrives\n* \"The transformation jobs are different for every table. \" so you need a DAG for each table.\n\nThen, D is the most suitable answer","timestamp":"1708980060.0"},{"poster":"choprat1","comment_id":"1351116","content":"Selected Answer: D\nmanaging indidivuals DAGs is the best way when they're too different","upvote_count":"1","timestamp":"1738618200.0"},{"upvote_count":"3","content":"Selected Answer: C\nA single shared DAG is efficient to manage, and table-specific transformations can be handled using parameters (e.g., passing table names and configurations dynamically).\nTriggering the DAG using a Cloud Storage object notification and a Cloud Function ensures the workflow starts immediately upon data arrival.\nEvent-driven architecture minimizes delays and provides the freshest data to users.\nEfficient, maintainable, and event-driven.","poster":"f74ca0c","comment_id":"1330668","timestamp":"1734926400.0"},{"poster":"8ad5266","comments":[{"comment_id":"1354290","poster":"plum21","upvote_count":"1","content":"It's possible to generate multiple DAGs programatically. That's the reason for C. https://cloud.google.com/blog/products/data-analytics/optimize-cloud-composer-via-better-airflow-dags -> look at #5","timestamp":"1739168460.0"}],"timestamp":"1719418500.0","content":"Selected Answer: C\nThis explains why it's not D:\nmaintainable workflow to process hundreds of tables and provide the freshest data to your end users\n\nHow is creating a DAG for each of the hundreds of tables maintainable?","comment_id":"1237569","upvote_count":"3"},{"timestamp":"1708257180.0","upvote_count":"1","poster":"JyoGCP","content":"Selected Answer: D\nOption D","comment_id":"1153238"},{"content":"Selected Answer: D\nOption D, which gets triggered when the data comes in and accounts for the fact that each table has its own set of transformations","timestamp":"1705145700.0","upvote_count":"3","comment_id":"1121564","poster":"Matt_108"},{"comment_id":"1115315","comments":[{"comment_id":"1160058","content":"It says that the transformations for each table are very different","upvote_count":"2","poster":"cuadradobertolinisebastiancami","timestamp":"1708980000.0"},{"comment_id":"1122224","timestamp":"1705201380.0","upvote_count":"5","poster":"AllenChen123","content":"Same question, why not use single DAG to manage as there are hundreds of tables."}],"timestamp":"1704559800.0","upvote_count":"3","content":"why not C?","poster":"Jordan18"},{"content":"Selected Answer: D\n- Option D: Tailored handling and scheduling for each table; triggered by data arrival for more timely and efficient processing.","upvote_count":"2","poster":"raaad","comment_id":"1113963","timestamp":"1704392160.0"},{"upvote_count":"1","timestamp":"1704284640.0","comment_id":"1112736","poster":"scaenruy","content":"Selected Answer: D\nD. \n1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Dataproc and BigQuery operators.\n2. Create a separate DAG for each table that needs to go through the pipeline.\n3. Use a Cloud Storage object trigger to launch a Cloud Function that triggers the DAG."}],"url":"https://www.examtopics.com/discussions/google/view/130178-exam-professional-data-engineer-topic-1-question-235/","question_id":152,"topic":"1","choices":{"D":"1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Dataproc and BigQuery operators.\n2. Create a separate DAG for each table that needs to go through the pipeline.\n3. Use a Cloud Storage object trigger to launch a Cloud Function that triggers the DAG.","A":"1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Cloud Storage, Dataproc, and BigQuery operators.\n2. Use a single shared DAG for all tables that need to go through the pipeline.\n3. Schedule the DAG to run hourly.","B":"1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Cloud Storage, Dataproc, and BigQuery operators.\n2. Create a separate DAG for each table that needs to go through the pipeline.\n3. Schedule the DAGs to run hourly.","C":"1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Dataproc and BigQuery operators.\n2. Use a single shared DAG for all tables that need to go through the pipeline.\n3. Use a Cloud Storage object trigger to launch a Cloud Function that triggers the DAG."},"answer_images":[],"answer_ET":"D","answer_description":"","unix_timestamp":1704284640,"question_text":"You want to schedule a number of sequential load and transformation jobs. Data files will be added to a Cloud Storage bucket by an upstream process. There is no fixed schedule for when the new data arrives. Next, a Dataproc job is triggered to perform some transformations and write the data to BigQuery. You then need to run additional transformation jobs in BigQuery. The transformation jobs are different for every table. These jobs might take hours to complete. You need to determine the most efficient and maintainable workflow to process hundreds of tables and provide the freshest data to your end users. What should you do?","answer":"D","question_images":[],"exam_id":11,"timestamp":"2024-01-03 13:24:00","answers_community":["D (57%)","C (43%)"],"isMC":true},{"id":"ydApFLOhtiZK9gxtb9hE","choices":{"C":"Create a highly available Cloud SQL instance in region A. Create a highly available read replica in region B. Scale up read workloads by creating cascading read replicas in multiple regions. Promote the read replica in region B when region A is down.","B":"Create a highly available Cloud SQL instance in region A. Scale up read workloads by creating read replicas in multiple regions. Promote one of the read replicas when region A is down.","D":"Create a highly available Cloud SQL instance in region A. Scale up read workloads by creating read replicas in the same region. Failover to the standby Cloud SQL instance when the primary instance fails.","A":"Create a highly available Cloud SQL instance in region Create a highly available read replica in region B. Scale up read workloads by creating cascading read replicas in multiple regions. Backup the Cloud SQL instances to a multi-regional Cloud Storage bucket. Restore the Cloud SQL backup to a new instance in another region when Region A is down."},"question_id":153,"topic":"1","answer_description":"","exam_id":11,"answers_community":["C (83%)","Other"],"url":"https://www.examtopics.com/discussions/google/view/130179-exam-professional-data-engineer-topic-1-question-236/","answer_images":[],"isMC":true,"question_images":[],"timestamp":"2024-01-03 13:41:00","discussion":[{"content":"Selected Answer: C\nOption C: Because HA read replica in multiple regions. \nNotA: Coz restore from back up is time taking\nNotB: No HA in Multiple regions read replica\nNot D: Only one region mentioned.","timestamp":"1706925720.0","poster":"rohan.sahi","comment_id":"1138953","upvote_count":"10"},{"comment_id":"1113991","comments":[{"timestamp":"1705202040.0","content":"Why not B?","comments":[{"upvote_count":"5","timestamp":"1705395060.0","poster":"datapassionate","content":"Why not B:\nWhile B option scales up read workloads across multiple regions, it doesn't specify high availability for the read replica in another region. In the event of a regional outage, promoting a non-highly available read replica might not provide the desired uptime and reliability.","comment_id":"1124028"}],"upvote_count":"1","poster":"AllenChen123","comment_id":"1122228"}],"poster":"raaad","upvote_count":"5","content":"Selected Answer: C\n- Combines high availability with geographic distribution of read workloads. \n- Promoting a highly available read replica can provide a quick failover solution, potentially meeting low RTO and RPO requirements.\n\n=====\nWhy not A:\nRestoring from backup to a new instance in another region during a regional outage might not meet low RTO and RPO requirements due to the time it takes to perform a restore.","timestamp":"1704394860.0"},{"upvote_count":"1","poster":"mi_yulai","comment_id":"1304362","content":"Why C? Is it possible to have HA enable in different regions? How the synchronization in disk will wokr for HA?","timestamp":"1730193060.0"},{"content":"Selected Answer: B\nhttps://cloud.google.com/sql/docs/mysql/replication\n\nThis option involves having read replicas in multiple regions, allowing you to promote one of them in the event of a failure in region A. While there may still be a brief interruption during the failover, it is likely to be less than the time required for the synchronization of cascading read replicas.","timestamp":"1705913340.0","poster":"tibuenoc","upvote_count":"1","comment_id":"1128443"},{"upvote_count":"1","timestamp":"1705146060.0","comments":[{"timestamp":"1705146120.0","poster":"Matt_108","content":"Why not others: \nApproach A: This approach requires you to restore a backup from a different region, which could take some time. This could result in a significant RPO (Recovery Point Objective) for the database. Additionally, the restored instance may not be physically located in the same region as the readers, which could impact performance.\nApproach C: This approach requires you to promote the read replica in region B, which could result in a temporary interruption to the readers while the promotion is taking place. Additionally, the read replica in region B may not be able to handle the same level of read traffic as the primary instance in region A.\nApproach D: This approach does not provide the same level of scalability as the other approaches, as you are limited to read replicas in the same region. Additionally, failover to the standby instance could result in a temporary interruption to the readers.","comment_id":"1121571","comments":[{"upvote_count":"3","comment_id":"1121575","timestamp":"1705146360.0","content":"Ignore my previous messages, it's C :D","poster":"Matt_108"}],"upvote_count":"1"}],"poster":"Matt_108","comment_id":"1121570","content":"Selected Answer: B\nTo me, it's B. it provides: \nHigh availability: The highly available Cloud SQL instance in region A will ensure that the database remains accessible even if one of the zones in the region becomes unavailable.\nScalability: The read replicas in multiple regions will enable you to scale up the read capacity of the database to support the demands of readers from various geographic regions.\nMinimal interruptions: When region A is down, one of the read replicas in another region will be promoted to become the new primary instance. This will ensure that there is no interruption to the readers."},{"upvote_count":"1","content":"Selected Answer: A\nA. \nCreate a highly available Cloud SQL instance in region Create a highly available read replica in region B. Scale up read workloads by creating cascading read replicas in multiple regions. Backup the Cloud SQL instances to a multi-regional Cloud Storage bucket. Restore the Cloud SQL backup to a new instance in another region when Region A is down.","timestamp":"1704285660.0","comment_id":"1112748","poster":"scaenruy"}],"question_text":"You are deploying a MySQL database workload onto Cloud SQL. The database must be able to scale up to support several readers from various geographic regions. The database must be highly available and meet low RTO and RPO requirements, even in the event of a regional outage. You need to ensure that interruptions to the readers are minimal during a database failover. What should you do?","answer":"C","answer_ET":"C","unix_timestamp":1704285660},{"id":"fxzaSsDdpsS7dzp5QSwz","answers_community":["C (93%)","7%"],"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/130180-exam-professional-data-engineer-topic-1-question-237/","exam_id":11,"isMC":true,"answer_images":[],"discussion":[{"comments":[{"upvote_count":"2","content":"In correct Option is A because you want a programatic way whereas datafusion is codeless solution and also dataflow is cost effective","comments":[{"upvote_count":"2","comment_id":"1122235","poster":"AllenChen123","timestamp":"1705202820.0","content":"You are saying Option C"}],"timestamp":"1704721320.0","comment_id":"1116645","poster":"qq589539483084gfrgrgfr"}],"comment_id":"1114009","content":"Selected Answer: C\n- Programmatic Flexibility: Apache Beam provides extensive control over pipeline design, allowing for customization of data transformations, including integration with Cloud DLP for sensitive data masking.\n- Streaming and Batch Support: Beam seamlessly supports both streaming and batch data processing modes, enabling flexibility in data loading patterns.\n- Cost-Effective Processing: Dataflow offers a serverless model, scaling resources as needed, and only charging for resources used, helping optimize costs.\n- Integration with Cloud DLP: Beam integrates well with Cloud DLP for sensitive data masking, ensuring data privacy before loading into BigQuery.","timestamp":"1704395580.0","upvote_count":"11","poster":"raaad"},{"upvote_count":"1","content":"Selected Answer: C\nOption C","poster":"JyoGCP","comment_id":"1153407","timestamp":"1708272240.0"},{"poster":"tibuenoc","timestamp":"1705914720.0","comment_id":"1128460","upvote_count":"2","content":"Selected Answer: C\nC is correct. Using Dataflow as Python as programming and BQ as sink. \n\nA is incorrect - DataFusion is Code-free as the main propose"},{"upvote_count":"1","timestamp":"1704285720.0","content":"Selected Answer: A\nA. \nUse Cloud Data Fusion to design your pipeline, use the Cloud DLP plug-in to de-identify data within your pipeline, and then move the data into BigQuery.","comments":[{"comment_id":"1304332","upvote_count":"1","poster":"ggg24","timestamp":"1730188800.0","content":"Data Fusion support only Batch and Streaming is required"},{"poster":"chrissamharris","upvote_count":"1","comment_id":"1212421","content":"Incorrect, that's a low-code solution. Doesnt meet this specific requirement: \"You need to do this in a programmatic way\"","timestamp":"1715864580.0"}],"comment_id":"1112749","poster":"scaenruy"}],"topic":"1","unix_timestamp":1704285720,"timestamp":"2024-01-03 13:42:00","answer_description":"","question_images":[],"choices":{"A":"Use Cloud Data Fusion to design your pipeline, use the Cloud DLP plug-in to de-identify data within your pipeline, and then move the data into BigQuery.","B":"Use the BigQuery Data Transfer Service to schedule your migration. After the data is populated in BigQuery, use the connection to the Cloud Data Loss Prevention (Cloud DLP) API to de-identify the necessary data.","C":"Create your pipeline with Dataflow through the Apache Beam SDK for Python, customizing separate options within your code for streaming, batch processing, and Cloud DLP. Select BigQuery as your data sink.","D":"Set up Datastream to replicate your on-premise data on BigQuery."},"question_text":"You are planning to load some of your existing on-premises data into BigQuery on Google Cloud. You want to either stream or batch-load data, depending on your use case. Additionally, you want to mask some sensitive data before loading into BigQuery. You need to do this in a programmatic way while keeping costs to a minimum. What should you do?","question_id":154,"answer":"C"},{"id":"hAmd3BygpntSttm6ykPO","discussion":[{"poster":"raaad","content":"Selected Answer: A\n- AEAD cryptographic functions in BigQuery allow for encryption and decryption of data at the column level. \n- You can encrypt specific data fields using a unique key per user and manage these keys outside of BigQuery (for example, in your application or using a key management system). \n- By \"deleting\" or revoking access to the key for a specific user, you effectively make their data unreadable, achieving crypto-deletion. \n- This method provides fine-grained encryption control but requires careful key management and integration with your applications.","timestamp":"1720114440.0","upvote_count":"9","comment_id":"1114025"},{"comment_id":"1153414","poster":"JyoGCP","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/aead-encryption-concepts\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/aead_encryption_functions","upvote_count":"3","timestamp":"1723990860.0"},{"comment_id":"1112753","timestamp":"1720003560.0","content":"Selected Answer: A\nA. \nImplement Authenticated Encryption with Associated Data (AEAD) BigQuery functions while storing your data in BigQuery.","upvote_count":"2","poster":"scaenruy"}],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/130181-exam-professional-data-engineer-topic-1-question-238/","unix_timestamp":1704285960,"isMC":true,"exam_id":11,"question_images":[],"answer_description":"","choices":{"C":"Create a customer-managed encryption key (CMEK) in Cloud KMS. Use the key to encrypt data before storing in BigQuery.","D":"Encrypt your data during ingestion by using a cryptographic library supported by your ETL pipeline.","A":"Implement Authenticated Encryption with Associated Data (AEAD) BigQuery functions while storing your data in BigQuery.","B":"Create a customer-managed encryption key (CMEK) in Cloud KMS. Associate the key to the table while creating the table."},"question_id":155,"answer_ET":"A","answer":"A","answer_images":[],"timestamp":"2024-01-03 13:46:00","answers_community":["A (100%)"],"question_text":"You want to encrypt the customer data stored in BigQuery. You need to implement per-user crypto-deletion on data stored in your tables. You want to adopt native features in Google Cloud to avoid custom solutions. What should you do?"}],"exam":{"isMCOnly":true,"provider":"Google","name":"Professional Data Engineer","isBeta":false,"isImplemented":true,"id":11,"numberOfQuestions":319,"lastUpdated":"11 Apr 2025"},"currentPage":31},"__N_SSP":true}