{"pageProps":{"questions":[{"id":"GqaWB3A4LuXjP98qaHQ0","timestamp":"2020-03-22 06:48:00","answer":"A","question_text":"You have developed three data processing jobs. One executes a Cloud Dataflow pipeline that transforms data uploaded to Cloud Storage and writes results to\nBigQuery. The second ingests data from on-premises servers and uploads it to Cloud Storage. The third is a Cloud Dataflow pipeline that gets information from third-party data providers and uploads the information to Cloud Storage. You need to be able to schedule and monitor the execution of these three workflows and manually execute them when needed. What should you do?","exam_id":11,"question_images":[],"answers_community":["A (100%)"],"choices":{"A":"Create a Direct Acyclic Graph in Cloud Composer to schedule and monitor the jobs.","C":"Develop an App Engine application to schedule and request the status of the jobs using GCP API calls.","D":"Set up cron jobs in a Compute Engine instance to schedule and monitor the pipelines using GCP API calls.","B":"Use Stackdriver Monitoring and set up an alert with a Webhook notification to trigger the jobs."},"url":"https://www.examtopics.com/discussions/google/view/17209-exam-professional-data-engineer-topic-1-question-108/","answer_description":"","discussion":[{"comment_id":"66949","content":"Should be A","timestamp":"1616420760.0","poster":"[Removed]","upvote_count":"36"},{"comment_id":"66819","timestamp":"1616392080.0","poster":"Rajokkiyam","comments":[{"upvote_count":"1","comment_id":"708468","poster":"MisuLava","content":"the jobs are not interdependent. just 3 individual jobs","timestamp":"1698760920.0"}],"content":"Create dependency in Cloud Composer and schedule it.","upvote_count":"22"},{"timestamp":"1729841400.0","upvote_count":"1","content":"yes answer A","poster":"maxu","comment_id":"1053550"},{"upvote_count":"1","comment_id":"911336","content":"Selected Answer: A\nCloud Composer. No doubt","poster":"forepick","timestamp":"1717163220.0"},{"upvote_count":"1","content":"A is correct","timestamp":"1703965500.0","comment_id":"762271","poster":"AzureDP900"},{"upvote_count":"1","timestamp":"1703699520.0","content":"Selected Answer: A\nCloud composer's DAG would manage the dependencies","comment_id":"758876","poster":"dconesoko"},{"timestamp":"1701777240.0","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/composer/docs/concepts/overview\nCloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers.","comment_id":"735899","upvote_count":"2","poster":"zellck"},{"upvote_count":"2","timestamp":"1688601300.0","poster":"danielfootc","comment_id":"627639","content":"This should be A"},{"poster":"medeis_jar","upvote_count":"3","timestamp":"1673032860.0","content":"Selected Answer: A\nhttps://cloud.google.com/composer/docs/how-to/using/writing-dags","comment_id":"518513"},{"content":"Selected Answer: A\nCloud Composer is a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centers.\nhttps://cloud.google.com/composer/?hl=en","comment_id":"514688","timestamp":"1672608180.0","upvote_count":"5","poster":"MaxNRG"},{"timestamp":"1671639300.0","content":"Selected Answer: A\nA\nThough the jobs are not dependent, they are data-driven. Refer to the below link:\nhttps://cloud.google.com/blog/topics/developers-practitioners/choosing-right-orchestrator-google-cloud","comments":[{"upvote_count":"1","timestamp":"1672609380.0","content":"nice article thanks!","poster":"MaxNRG","comment_id":"514701"}],"comment_id":"506252","upvote_count":"6","poster":"kishanu"},{"timestamp":"1669390500.0","comment_id":"486756","poster":"JG123","upvote_count":"3","content":"Selected Answer: A\nCloud Composer"},{"comment_id":"486753","poster":"JG123","upvote_count":"3","content":"Correct: A","timestamp":"1669390440.0"},{"comment_id":"421948","content":"should be option A","timestamp":"1660024980.0","poster":"sandipk91","upvote_count":"5"},{"content":"Vote for A","comment_id":"396868","poster":"sumanshu","timestamp":"1656765120.0","upvote_count":"4"},{"comment_id":"285213","upvote_count":"3","timestamp":"1644203760.0","poster":"someshsehgal","content":"COrrect A: Couldn't understand why a option with no connection with actual problem has been given as correct option (D)"},{"content":"I'll go for A","timestamp":"1637302920.0","comment_id":"222487","poster":"arghya13","upvote_count":"2"},{"poster":"Tanmoyk","upvote_count":"2","timestamp":"1631587680.0","content":"Should be A","comment_id":"179033"},{"poster":"atnafu2020","upvote_count":"4","timestamp":"1629655020.0","comment_id":"163796","content":"A\nis correct Answer"},{"upvote_count":"2","content":"A is correct.\nIn the option D , the api calls don’t resolve the desired monitoring.","timestamp":"1629542940.0","poster":"haroldbenites","comment_id":"162881"},{"upvote_count":"3","content":"Dataload job to BQ is dependent on the other 2 jobs for it to know when a file has been created in the GCS. So I'll go with A","comment_id":"134263","timestamp":"1626204180.0","comments":[{"content":"You don't know if they are related or not. They COULD be, but we don't know.","upvote_count":"2","comment_id":"185553","timestamp":"1632419280.0","poster":"Diqtator"}],"poster":"tprashanth"},{"content":"Three workflows are independent. They need to be executed & monitored manually. Cloud composer will add no value. We dont need to connect three workflows in a single workflow.\nOption D is a simple and efficient solution. Not all solutions require use of Google managed services. Answer is \"D\"","comment_id":"128965","comments":[{"content":"In the option D , the api calls don’t resolve the desired monitoring.","upvote_count":"2","comment_id":"162880","timestamp":"1629542880.0","poster":"haroldbenites"}],"upvote_count":"3","timestamp":"1625661900.0","poster":"dg63"},{"poster":"Rajuuu","timestamp":"1625544840.0","content":"A. Cloud COmposer is the solution.","upvote_count":"2","comment_id":"127413"},{"content":"Answer: A\nDescription: Cloud composer is used to schedule the interdependent jobs","comment_id":"68774","timestamp":"1616906400.0","poster":"[Removed]","upvote_count":"10","comments":[{"comment_id":"500156","content":"but thery are not","poster":"marioferrulli","timestamp":"1670869200.0","upvote_count":"1"}]}],"unix_timestamp":1584856080,"question_id":11,"topic":"1","answer_images":[],"isMC":true,"answer_ET":"A"},{"id":"qG9qVPgvMVsqw78tszYo","discussion":[{"comment_id":"668637","upvote_count":"13","timestamp":"1663134300.0","poster":"TNT87","content":"Answer C E\nBy not acknowleding the pulled message, this result in it be putted back in Cloud Pub/Sub, meaning the messages accumulate instead of being consumed and removed from Pub/Sub. The same thing can happen ig the subscriber maintains the lease on the message it receives in case of an error. This reduces the overall rate of processing because messages get stuck on the first subscriber. Also, errors in Cloud Function do not show up in Stackdriver Log Viewer if they are not correctly handled."},{"comment_id":"1337932","upvote_count":"1","poster":"Pime13","content":"Selected Answer: DE\nWhile poor error handling can cause issues, it would likely result in errors being logged rather than an increased message processing rate without errors, excluding C","timestamp":"1736338440.0"},{"poster":"clouditis","content":"Selected Answer: DE\nNot C, its talking about the unknown!","timestamp":"1734293640.0","upvote_count":"1","comment_id":"1327041"},{"upvote_count":"1","content":"Selected Answer: DE\nThe issue is that the acknowledgment is not sent back to sub properly. D, E should be correct.","comment_id":"1302166","poster":"SamuelTsch","timestamp":"1729710660.0"},{"upvote_count":"1","content":"Selected Answer: DE\nD. The subscriber code cannot keep up with the messages.\n\nIf the processing rate of the subscriber (Cloud Functions) is lower than the incoming message rate, it can lead to a backlog of messages. This would result in higher-than-expected message rates, as messages accumulate while waiting to be processed.\n\nE. The subscriber code does not acknowledge the messages that it pulls.\n\nIf messages are not acknowledged properly, Pub/Sub will keep retrying to deliver them, which can lead to the same messages being sent repeatedly. This could also contribute to the perception that the message processing rate is very high.\n\nBoth of these issues can lead to unanticipated behavior in your message processing pipeline without generating errors that would be logged in Cloud Logging.","comment_id":"1288315","poster":"Preetmehta1234","timestamp":"1727126280.0"},{"timestamp":"1723428840.0","upvote_count":"1","poster":"JamesKarianis","content":"Selected Answer: CD\nThe code in the CF can't keep up with the amount of messages, thus C D is a better fit","comment_id":"1264419"},{"timestamp":"1682480340.0","content":"Selected Answer: DE\nRef chatgpt\nOption C, \"Error handling in the subscriber code is not handling run-time errors properly,\" suggests that the subscriber code may not be correctly handling errors that occur during message processing. If the subscriber code encounters an error that it cannot handle, such as a syntax error or a network issue, it may stop processing messages, leading to a slowdown in message processing.\n\nHowever, the lack of error logs in Cloud Logging suggests that there are no errors being logged, which makes it less likely that this is the primary cause of the observed behavior. Additionally, while incorrect error handling could contribute to the issue, it may not be the primary reason why the message processing rate is much higher than anticipated.","comment_id":"881067","upvote_count":"1","poster":"mialll","comments":[{"timestamp":"1699711680.0","content":"Chat says about Option C: \"it may stop processing messages, leading to a slowdown in message processing\" - but is doesn't say there's a slowdown in the question. It says it's increased.\n\nI would replace C with D. If the Cloud Function isn't capable of processing messages as quickly as they arrive, the backlog will grow, leading to higher processing rates as the function continuously tries to catch up. This scenario might not generate errors in Cloud Logging if the function is simply falling behind.","poster":"GCPete","upvote_count":"1","comment_id":"1067882"}]},{"upvote_count":"1","comment_id":"834636","poster":"midgoo","comments":[{"timestamp":"1687179600.0","content":"C. Error handling in the subscriber (Cloud Functions) code is not handling run-time errors properly.\n\nThis would mean to have error logs in Cloud Logging as CF by default logs to it.","poster":"cetanx","upvote_count":"1","comment_id":"927507"}],"content":"Selected Answer: CE\nC - as no error shown in Cloud Logging\nBetween D & E, both could lead to the problem. I have worked with lot of PubSub issues, most of them are due to the bottleneck at the code where it takes too long to process 1 message and causes backlog. E could lead to backlog too, but it is too obvious and not likely to happen in reality.\nHowever, when I ask AI the same question, it said C and E","timestamp":"1678422540.0"},{"poster":"musumusu","timestamp":"1676555340.0","upvote_count":"3","content":"Answer D&E\nI am not in the favour of C, error handling is a side factor but not the primary cause. \nFirst check the configuration access. \nDoes subscriber has enough acknowledge policies (option E)\nDoes sub have ability to keep up the message( enough network, cpu and capable codes) (option D)\noption C is just a part of option D somewhere showing incapable handling","comment_id":"810712"},{"comments":[{"comment_id":"985466","timestamp":"1692487080.0","upvote_count":"1","content":"Like TNT87 mentioned the message processing rate is high \"meaning the messages accumulate instead of being consumed and removed from Pub/Sub.\"","poster":"squishy_fishy"}],"upvote_count":"3","poster":"desertlotus1211","comment_id":"781619","content":"My question is: 'What is the actual problem?'\n- That there is no logs in Cloud Logging?\n- That Pub/Sub is having a problem?\n- Or there an actual problem?\n- Is there an actual error?\n\nSo what is Pub/Sub the message processing rate is high...Does that mean there is a problem?\n\nThoughts?","timestamp":"1674163380.0"},{"timestamp":"1672429920.0","upvote_count":"1","content":"C, E seems correct","comment_id":"762275","poster":"AzureDP900"},{"timestamp":"1662947280.0","comment_id":"666534","poster":"MounicaN","upvote_count":"3","content":"D might also be right? \nSubscriber might not be provisioned enough"},{"upvote_count":"3","content":"Selected Answer: CE\nC. Error handling in the subscriber code is not handling run-time errors properly.\nE. The subscriber code does not acknowledge the messages that it pulls.","comment_id":"658419","poster":"AWSandeep","timestamp":"1662206940.0"}],"answer_images":[],"isMC":true,"answers_community":["DE (50%)","CE (40%)","10%"],"answer":"DE","answer_ET":"DE","answer_description":"","timestamp":"2022-09-03 14:09:00","exam_id":11,"topic":"1","choices":{"B":"Total outstanding messages exceed the 10-MB maximum.","A":"Publisher throughput quota is too small.","D":"The subscriber code cannot keep up with the messages.","C":"Error handling in the subscriber code is not handling run-time errors properly.","E":"The subscriber code does not acknowledge the messages that it pulls."},"url":"https://www.examtopics.com/discussions/google/view/79780-exam-professional-data-engineer-topic-1-question-109/","question_images":[],"unix_timestamp":1662206940,"question_id":12,"question_text":"You have Cloud Functions written in Node.js that pull messages from Cloud Pub/Sub and send the data to BigQuery. You observe that the message processing rate on the Pub/Sub topic is orders of magnitude higher than anticipated, but there is no error logged in Cloud Logging. What are the two most likely causes of this problem? (Choose two.)"},{"id":"fJUY9SSM41Sc9ciB3L1r","url":"https://www.examtopics.com/discussions/google/view/79682-exam-professional-data-engineer-topic-1-question-11/","answers_community":["C (100%)"],"question_id":13,"choices":{"B":"Use a sliding time window with a duration of 60 minutes.","A":"Use a fixed-time window with a duration of 60 minutes.","C":"Use a session window with a gap time duration of 60 minutes.","D":"Use a global window with a time based trigger with a delay of 60 minutes."},"isMC":true,"question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0000700003.png"],"discussion":[{"upvote_count":"33","poster":"vetaal","timestamp":"1667693160.0","content":"There are 3 windowing concepts in dataflow and each can be used for below use case\n1) Fixed window\n2) Sliding window and\n3) Session window.\n\nFixed window = any aggregation use cases, any batch analysis of data, relatively simple use cases.\n\nSliding window = Moving averages of data\nSession window = user session data, click data and real time gaming analysis.\n\nThe question here is about user session data and hence session window.\n\nReference:\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines","comment_id":"712084"},{"upvote_count":"3","comment_id":"1342412","timestamp":"1737171480.0","content":"Selected Answer: C\nThe answer is C because session window is specifically designed to handle use cases where activity is grouped by gaps.\n\nA. Fixed-time window divides data into non-overlapping, equally-size intervals but do not track gaps in user activity.\nB. Sliding-time window process overlapping intervals and are better suited for periodic aggregation.\nD. Global windows process all data over the pipeline's lifetime and rely on custom triggers to handle time-based logic. It is technically possible but unnecessarily complex so no.","poster":"cqrm3n"},{"poster":"rtcpost","timestamp":"1727154840.0","comment_id":"1050481","content":"Selected Answer: C\nC. Use a session window with a gap time duration of 60 minutes.\n\nA session window with a gap time duration of 60 minutes is appropriate for capturing user sessions where there has been no interaction on the site for 1 hour. It allows you to group user activity within a session, and when the session becomes inactive for the defined gap time, you can evaluate whether the user added more than $30 worth of products to the basket and has not completed a transaction.\n\nOptions A and B (fixed-time window and sliding time window) might not capture the specific session-based criteria of inactivity and user interaction effectively.\n\nOption D (global window with a time-based trigger) is not suitable for capturing user sessions and checking inactivity based on a specific time duration. It's more appropriate for cases where you need a single global view of the data.","upvote_count":"3"},{"upvote_count":"1","poster":"RT_G","timestamp":"1699382340.0","content":"Selected Answer: C\nSession window since the question specifically talks about a specific user for a fixed duration.","comment_id":"1065083"},{"upvote_count":"1","timestamp":"1699054200.0","poster":"rocky48","comment_id":"1061825","content":"Selected Answer: C\nSession window = user session data, click data and real time gaming analysis."},{"timestamp":"1696646220.0","comment_id":"1027020","poster":"imran79","upvote_count":"2","content":"The basket abandonment system needs to determine if a user hasn't interacted with the site for 1 hour, has added products worth more than $30, and hasn't completed a transaction. Therefore, the pipeline should account for periods of user activity and inactivity. A session-based windowing approach is appropriate here.\n\nThe right choice is:\n\nC. Use a session window with a gap time duration of 60 minutes.\n\nSession windows group data based on periods of activity and inactivity. If there's no interaction for the duration of the gap time (in this case, 60 minutes), a new window is started. This would help identify users who haven't interacted with the site for the specified duration, fulfilling the requirement for the basket abandonment system."},{"comment_id":"1016893","upvote_count":"1","timestamp":"1695652440.0","content":"Selected Answer: C\nsession windows can divide a data stream representing user activity\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#session-windows","poster":"MikkelRev"},{"content":"Selected Answer: C\nC - The best option for this use case.","poster":"Chesternut999","upvote_count":"2","timestamp":"1678824300.0","comment_id":"839225"},{"comment_id":"835652","upvote_count":"2","timestamp":"1678508700.0","poster":"bha11111","content":"Selected Answer: C\nSession window is used for these type of scenario"},{"comment_id":"799143","timestamp":"1675628700.0","upvote_count":"2","content":"C. Use a session window with a gap time duration of 60 minutes.\n\nA session window would be the most appropriate option to use in this case, as it would allow you to group events into sessions based on time gaps. In this case, the gap time of 60 minutes could be used to define a session, and if there is no interaction from the user for 60 minutes, a new session would be created. By using a session window, you can track the behavior of the user during each session, including the products added to the basket, and determine if the conditions for sending a message have been met (i.e., the user has added more than $30 worth of products to the basket and has not completed a transaction).","poster":"samdhimal"},{"timestamp":"1666250400.0","content":"Only C is feasible for this question","comment_id":"699632","poster":"kennyloo","upvote_count":"1"},{"content":"Selected Answer: C\nC. Use a session window with a gap time duration of 60 minutes.","upvote_count":"1","comment_id":"658078","timestamp":"1662180600.0","poster":"AWSandeep"}],"exam_id":11,"answer":"C","topic":"1","answer_images":[],"answer_ET":"C","answer_description":"","unix_timestamp":1662180600,"question_text":"You are designing a basket abandonment system for an ecommerce company. The system will send a message to a user based on these rules:\n✑ No interaction by the user on the site for 1 hour\nHas added more than $30 worth of products to the basket\n//IMG//\n\n✑ Has not completed a transaction\nYou use Google Cloud Dataflow to process the data and decide if a message should be sent. How should you design the pipeline?","timestamp":"2022-09-03 06:50:00"},{"id":"5UagEhZZgVoUCMgoDlUf","discussion":[{"timestamp":"1632311460.0","poster":"[Removed]","comment_id":"66952","upvote_count":"16","content":"Correct - B"},{"content":"Answer: B\nDescription: ParDo is used to do transformation and create side output","poster":"[Removed]","comment_id":"68777","upvote_count":"12","timestamp":"1632804540.0"},{"timestamp":"1726180740.0","content":"Selected Answer: B\nA - SideInput is often used to validate data, however, we need to create the SideInput first. When using SideInput to filter data, it is actually another ParDo call.\nC, D - This is common way to filter too, but we will need the key in order to partition or GroupByKey\nB - ParDo is the most basic method, it can do anything to the PCollection","upvote_count":"3","poster":"midgoo","comment_id":"837490"},{"content":"B. Add a ParDo transform in Cloud Dataflow to discard corrupt elements.","poster":"AzureDP900","comment_id":"762278","timestamp":"1719770160.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1717580940.0","comment_id":"735895","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/beam-programming-model#concepts\nParDo is the core parallel processing operation in the Apache Beam SDKs, invoking a user-specified function on each of the elements of the input PCollection. ParDo collects the zero or more output elements into an output PCollection. The ParDo transform processes elements independently and possibly in parallel.","poster":"zellck"},{"timestamp":"1704535500.0","poster":"Pime13","comment_id":"627784","content":"Selected Answer: B\nvote B :https://beam.apache.org/documentation/programming-guide/#pardo \n\nFiltering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it.\nFormatting or type-converting each element in a data set. If your input PCollection contains elements that are of a different type or format than you want, you can use ParDo to perform a conversion on each element and output the result to a new PCollection.\nExtracting parts of each element in a data set. If you have a PCollection of records with multiple fields, for example, you can use a ParDo to parse out just the fields you want to consider into a new PCollection.\nPerforming computations on each element in a data set. You can use ParDo to perform simple or complex computations on every element, or certain elements, of a PCollection and output the results as a new PCollection.","upvote_count":"4"},{"poster":"medeis_jar","comments":[{"content":"I agree with B","comment_id":"762274","poster":"AzureDP900","upvote_count":"1","timestamp":"1719769920.0"}],"upvote_count":"2","timestamp":"1688664240.0","comment_id":"518516","content":"Selected Answer: B\nFiltering with ParDo. ParDo is a Beam transform for generic parallel processing. ParDo is useful for common data processing operations/"},{"timestamp":"1688239800.0","upvote_count":"6","comment_id":"514695","poster":"MaxNRG","content":"Selected Answer: B\nB: ParDo is a Beam transform for generic parallel processing. ParDo is useful for common data processing operations, including:\na. Filtering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection, or discard it.\nb. Formatting or type-converting each element in a data set.\nc. Extracting parts of each element in a data set.\nd. Performing computations on each element in a data set.\nA does not help\nC Partition is a Beam transform for PCollection objects that store the same data type. Partition splits a single PCollection into a fixed number of smaller collections. Again, does not help\nD GroupByKey is a Beam transform for processing collections of key/value pairs. GroupByKey is a good way to aggregate data that has something in common"},{"upvote_count":"4","timestamp":"1672671360.0","content":"vote for 'B', ParDo can discard the elements.\n\nhttps://beam.apache.org/documentation/programming-guide/","poster":"sumanshu","comment_id":"396887"},{"timestamp":"1657603800.0","content":"B - seems to be better option since we need to filter out, question does not specify that we do need to store it into different Pcollection.\nhttps://beam.apache.org/documentation/transforms/python/overview/ \nParDo is general purpose whereas partition splits the elements into do different pcollections.\nhttps://beam.apache.org/documentation/transforms/python/elementwise/partition/","upvote_count":"3","comment_id":"265350","poster":"DeepakKhattar"},{"upvote_count":"3","timestamp":"1652934900.0","content":"B is correct","comment_id":"222496","poster":"arghya13"},{"content":"Should be B. The Partition transform would require the element identifying the valid/invalid records for partitioning the pcollection that means there is some logic to be executed before the Partition transformation is invoked. That logic can be implemented in a ParDO transform and which can both identify valid/invalid records and also generate two PCollections one with valid records and other with invalid records.","timestamp":"1648018260.0","comment_id":"185006","upvote_count":"7","poster":"SteelWarrior"},{"upvote_count":"3","comment_id":"162888","poster":"haroldbenites","content":"B is correct","timestamp":"1645448460.0"},{"upvote_count":"4","poster":"Archy","comment_id":"148150","timestamp":"1643651520.0","content":"B, ParDo is useful for a variety of common data processing operations, including:\n\nFiltering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it."},{"timestamp":"1642110180.0","poster":"tprashanth","comment_id":"134269","content":"Looks like C it is\nhttps://beam.apache.org/documentation/programming-guide/","upvote_count":"2","comments":[{"upvote_count":"5","comment_id":"163935","content":"according this link its \nPardo\n* Filtering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it.\n* But Partition just splitting which is is a Beam transform for PCollection objects that store the same data type. Partition splits a single PCollection into a fixed number of smaller collections.","poster":"atnafu2020","comments":[{"comment_id":"241038","upvote_count":"1","content":"Seems like two answers may be correct. With ParDo you can discard corrupt data. With Partition you can split the data into two PCollections: corrupt and ok. You stream ok data further to BigQuery and corrupt data to some other storage for analysis. If one is not interested in analysis, then ParDo is enough.","timestamp":"1654955760.0","poster":"xrun"}],"timestamp":"1645571940.0"}]},{"timestamp":"1641567180.0","content":"Correct answer should be \"C\". A Pardo transform will allow the processing to happening in parallel using multiple workers. Partition transform will allow data to be partitions in two different Pcollections according to some logic. Using partition transform once can split the corrupted data and finally discard it.","comment_id":"128971","upvote_count":"5","poster":"dg63"},{"poster":"Rajuuu","timestamp":"1641450120.0","comment_id":"127416","upvote_count":"4","content":"Correct B."},{"timestamp":"1640699520.0","upvote_count":"5","content":"Correct - B","comment_id":"121831","poster":"norwayping"}],"answers_community":["B (100%)"],"answer":"B","choices":{"C":"Add a Partition transform in Cloud Dataflow to separate valid data from corrupt data.","B":"Add a ParDo transform in Cloud Dataflow to discard corrupt elements.","A":"Add a SideInput that returns a Boolean if the element is corrupt.","D":"Add a GroupByKey transform in Cloud Dataflow to group all of the valid data together and discard the rest."},"answer_images":[],"question_id":14,"unix_timestamp":1584885060,"isMC":true,"exam_id":11,"answer_ET":"B","timestamp":"2020-03-22 14:51:00","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/17252-exam-professional-data-engineer-topic-1-question-110/","question_images":[],"question_text":"You are creating a new pipeline in Google Cloud to stream IoT data from Cloud Pub/Sub through Cloud Dataflow to BigQuery. While previewing the data, you notice that roughly 2% of the data appears to be corrupt. You need to modify the Cloud Dataflow pipeline to filter out this corrupt data. What should you do?","topic":"1"},{"id":"mMw5704SbFTJu2hlUlO4","topic":"1","answers_community":["A (100%)"],"answer_description":"","timestamp":"2020-03-22 13:15:00","discussion":[{"content":"should be A","upvote_count":"35","poster":"[Removed]","timestamp":"1632305700.0","comment_id":"66933"},{"upvote_count":"18","content":"Answer: A\nDescription: Partition is the solution for reducing cost and time","comment_id":"68779","poster":"[Removed]","timestamp":"1632804720.0","comments":[{"timestamp":"1637596860.0","poster":"willbot","content":"but how would recreating tables with 3 years of data, maintain the ability to conduct sql queries during that time?","comment_id":"93963","comments":[{"timestamp":"1681768200.0","comment_id":"463722","content":"Recreating the new table, the old table will still have new data coming, then append the difference to the new table.","poster":"squishy_fishy","upvote_count":"2"}],"upvote_count":"1"}]},{"timestamp":"1717778280.0","content":"Selected Answer: A\nAnswer: A, has no cost to reload the data, Also Partition is the solution for reducing cost and time","upvote_count":"1","comment_id":"738233","poster":"odacir"},{"comment_id":"735884","comments":[{"upvote_count":"1","content":"A is right","comment_id":"762277","poster":"AzureDP900","timestamp":"1719770100.0"}],"timestamp":"1717579920.0","poster":"zellck","upvote_count":"3","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables\nA partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\n\nYou can partition BigQuery tables by:\n- Time-unit column: Tables are partitioned based on a TIMESTAMP, DATE, or DATETIME column in the table."},{"content":"Selected Answer: A\nit is not B in the sense of cost-effective certainly. read below in limitation\nhttps://cloud.google.com/bigquery/docs/querying-wildcard-tables#limitations\nCurrently, cached results are not supported for queries against multiple tables using a wildcard even if the Use Cached Results option is checked. If you run the same wildcard query multiple times, you are billed for each query.","poster":"John_Pongthorn","comment_id":"673958","upvote_count":"1","timestamp":"1710932280.0"},{"timestamp":"1710931920.0","poster":"John_Pongthorn","comment_id":"673957","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard\nPartitioning is recommended over table sharding, because partitioned tables perform better","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nA AND D , they are the most likely choiced but the questionn want \nissue as cost-effectively as possible while maintaining the ability to conduct SQL queries. \n1 table may be cheaper so partition is better than wildcarf","timestamp":"1710400020.0","poster":"John_Pongthorn","comment_id":"668621"},{"timestamp":"1698241800.0","poster":"Didine_22","content":"Selected Answer: A\nanswer A","comment_id":"591753","upvote_count":"2"},{"comment_id":"518518","timestamp":"1688664300.0","poster":"medeis_jar","upvote_count":"2","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/partitioned-tables"},{"poster":"MaxNRG","content":"Selected Answer: A\nA. Partiotioning\nhttps://cloud.google.com/bigquery/docs/partitioned-tables","comment_id":"516247","timestamp":"1688442840.0","upvote_count":"1"},{"poster":"Tomi1313","comments":[{"comment_id":"683636","upvote_count":"1","poster":"John_Pongthorn","timestamp":"1711814340.0","content":"Partitioning is recommended over table sharding, because partitioned tables perform better\nThis is a google recommendation nowaday."}],"content":"Why not D? You can use SQL.\nThis is the cheapest and fastest option \nhttps://cloud.google.com/bigquery/docs/querying-wildcard-tables","timestamp":"1688128980.0","upvote_count":"2","comment_id":"513451"},{"poster":"StefanoG","content":"Selected Answer: A\nThe D solution is obviously discarded. \nThe request NOT require ONLY LAST 30-90 days, so the C solution is not the right solution. \nIn addition to this, the request ask to keep the possibility to made queries, so B is wrost. \nIs not mandatory make the queries while you make the modify so the right answer is A","upvote_count":"4","timestamp":"1685443200.0","comment_id":"490676"},{"upvote_count":"1","comment_id":"475024","timestamp":"1683652560.0","content":"B sounds more feasible. \nThe point is 'historical' data, not new table/data. Recreating tables from the past three years is a lot of work. Might as well export the table and run analyses there. No cost for exporting in BigQuery.","poster":"JayZeeLee"},{"comment_id":"396958","upvote_count":"5","timestamp":"1672677240.0","poster":"sumanshu","content":"Vote for A"},{"timestamp":"1652935200.0","comment_id":"222497","upvote_count":"5","content":"I will go with Option A","poster":"arghya13"},{"timestamp":"1652149200.0","poster":"Alasmindas","comments":[{"poster":"karthik89","timestamp":"1660967820.0","content":"but how will you append the data that is older than 90days in to the master table?","comment_id":"294804","upvote_count":"2"}],"upvote_count":"3","comment_id":"216354","content":"I will go with Option A, although at first instance I felt Option C would be correct. \nOption A : Because partitioning will help to address both the concerns mentioned in the question - i.e. faster query and reducing cost.\nOption C : Modifying the data pipeline to store last 30-90 days data would have possible, if there was a point mentioned that only the latest data (30-90 days) is kept and the older data - beyond 90 days is moved to the master table. Since that point is mot mentioned, we will land up having multiple - 30-90 days data in separate tables + the master table."},{"poster":"Cloud_Enthusiast","timestamp":"1652101560.0","content":"Answer is A. Recreating the DDL with new parition is easy and does not require any changes on applications that read data from it","upvote_count":"4","comment_id":"216018"},{"poster":"SteelWarrior","timestamp":"1648018680.0","upvote_count":"5","content":"Should be A. With partitions the performance will improve for selecting 30-90 days data. Also the storage cost will reduce as the old partitions (not updated in last 90 days) will qualify for Long-Term storage rates.","comment_id":"185008"},{"poster":"haroldbenites","content":"A is correct","timestamp":"1645467660.0","comment_id":"163073","upvote_count":"3"},{"poster":"Rajuuu","comments":[{"content":"I think C will be more cost effective than using A as recreating the whole DDL is more expensive..","poster":"Rajuuu","comment_id":"131916","timestamp":"1641888420.0","comments":[{"comment_id":"134271","poster":"tprashanth","timestamp":"1642110360.0","upvote_count":"4","content":"No, if a seperate table is maintained for last 30-90 days data, we end up creating a table on daily basis"}],"upvote_count":"1"}],"content":"PArtition the tables is the key for query improvement.","comment_id":"127419","upvote_count":"2","timestamp":"1641450540.0"},{"upvote_count":"2","timestamp":"1640403780.0","content":"Has to be Option [A]","poster":"dambilwa","comment_id":"119024"}],"exam_id":11,"question_id":15,"answer_ET":"A","question_images":[],"answer_images":[],"choices":{"D":"Write an Apache Beam pipeline that creates a BigQuery table per day. Recommend that the Data Science team use wildcards on the table name suffixes to select the data they need.","A":"Re-create the tables using DDL. Partition the tables by a column containing a TIMESTAMP or DATE Type.","B":"Recommend that the Data Science team export the table to a CSV file on Cloud Storage and use Cloud Datalab to explore the data by reading the files directly.","C":"Modify your pipeline to maintain the last 30ג€\"90 days of data in one table and the longer history in a different table to minimize full table scans over the entire history."},"url":"https://www.examtopics.com/discussions/google/view/17248-exam-professional-data-engineer-topic-1-question-111/","answer":"A","question_text":"You have historical data covering the last three years in BigQuery and a data pipeline that delivers new data to BigQuery daily. You have noticed that when the\nData Science team runs a query filtered on a date column and limited to 30`\"90 days of data, the query scans the entire table. You also noticed that your bill is increasing more quickly than you expected. You want to resolve the issue as cost-effectively as possible while maintaining the ability to conduct SQL queries.\nWhat should you do?","unix_timestamp":1584879300,"isMC":true}],"exam":{"name":"Professional Data Engineer","id":11,"isMCOnly":true,"numberOfQuestions":319,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Google"},"currentPage":3},"__N_SSP":true}