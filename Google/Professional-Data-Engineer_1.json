{"pageProps":{"questions":[{"id":"SbIqo6GDOPUPe4SrlvLf","question_id":1,"topic":"1","isMC":true,"unix_timestamp":1662122760,"discussion":[{"comment_id":"657434","content":"Answer is C.\nBad performance of a model is either due to lack of relationship between dependent and independent variables used, or just overfit due to having used too many features and/or bad features.\n\nA: Threading parallelisation can reduce training time, but if the selected featuers are the same then the resulting performance won't have changed\nB: Serialization is only changing data into byte streams. This won't be useful.\nC: This can show which features are bad. E.g. if it is one feature causing bad performance, then the dropout method will show it, so you can remove it from the model and retrain it.\nD: This would become clear if the model did not fit the training data well. But the question says that the model fits the training data well, so D is not the answer.","poster":"henriksoder24","timestamp":"1662122760.0","upvote_count":"28"},{"comment_id":"1401479","timestamp":"1742544780.0","upvote_count":"1","content":"Selected Answer: C\nCorrect answer is C.\nA - Is not Threading because it is used to accelerate the training in order to reduce training time.\nB - Is not Serialization because it transforms (serializes into bytes) the training data but does not increase or change the original nature.\nD - Is not dimensionality reduction because the model fits the training data.\n\nhttps://docs.google.com/document/d/1VV6vkkjShXDgPLSG6V_7-0dweLmZTUnYiTSxo6C5ERY/edit?tab=t.0","poster":"Paxtons_Aunders"},{"timestamp":"1741446600.0","comment_id":"1366619","content":"Selected Answer: C\nCorrect answer is C.\nA - Is not Threading because it is used to accelerate the training in order to reduce training time.\nB - Is not Serialization because it transforms (serializes into bytes) the training data but does not increase or change the original nature.\nD - Is not dimensionality reduction because the model fits the training data.","upvote_count":"1","poster":"monyu"},{"timestamp":"1741000020.0","comment_id":"1364361","content":"Selected Answer: C\nC. Dropout Methods\nDropout is a regularization technique commonly used in neural networks to prevent overfitting learns-google.blogspot.com It helps improve the generalization of the model by randomly setting a fraction of the neurons to zero during each training iteration, which prevents the network from relying too heavily on specific neurons. This, in turn, can lead to better performance on new, unseen data.","upvote_count":"1","poster":"nocoxe"},{"poster":"Ahamada","comment_id":"1362309","timestamp":"1740605640.0","upvote_count":"1","content":"Selected Answer: C\nDropout methods is the solution here to resolve overfitting issue"},{"poster":"onschamekh","content":"Selected Answer: C\nDropout is a specific technique to prevent overfitting by randomly disabling a certain percentage of neurons during training. This helps the network avoid relying too heavily on a subset of neurons, thereby improving its ability to generalize to new data.","comment_id":"1343636","upvote_count":"1","timestamp":"1737381180.0"},{"content":"Selected Answer: C\nCan we expect similar questions like this in GCP exam as well?","comment_id":"1334170","poster":"jithinlife","upvote_count":"1","timestamp":"1735566240.0"},{"poster":"SamuelTsch","content":"Selected Answer: C\nIt occurs overfitting problem. A general idea is to simplify the model. A GENERALIZATION related method should be used.","comment_id":"1300752","timestamp":"1729485240.0","upvote_count":"2"},{"poster":"rtcpost","content":"Selected Answer: C\nC. Dropout Methods\nDropout is a regularization technique commonly used in neural networks to prevent overfitting. It helps improve the generalization of the model by randomly setting a fraction of the neurons to zero during each training iteration, which prevents the network from relying too heavily on specific neurons. This, in turn, can lead to better performance on new, unseen data.","comment_id":"1050457","upvote_count":"1","timestamp":"1727154060.0"},{"comment_id":"1060864","upvote_count":"1","timestamp":"1727154060.0","content":"Selected Answer: C\nA: Threading parallelisation can reduce training time, but if the selected featuers are the same then the resulting performance won't have changed\nB: Serialization is only changing data into byte streams. This won't be useful.\nC: This can show which features are bad. E.g. if it is one feature causing bad performance, then the dropout method will show it, so you can remove it from the model and retrain it.\nD: This would become clear if the model did not fit the training data well. But the question says that the model fits the training data well.\n\nSo, C is the answer.","poster":"rocky48"},{"upvote_count":"1","poster":"trashbox","timestamp":"1715040720.0","comment_id":"1207631","content":"Selected Answer: C\nDropout Methods are useful to prevent a TensorFlow model from overfitting"},{"timestamp":"1689069180.0","content":"Selected Answer: C\nAnswer is C. Dropout methods are used to mitigate overfitting. Hence, it is commonly used in training phase and it's beneficial for test-time performance.","comment_id":"948846","poster":"azmiozgen","upvote_count":"1"},{"timestamp":"1686304080.0","upvote_count":"1","poster":"dgteixeira","content":"Selected Answer: C\nAnswer is C","comment_id":"919153"},{"comment_id":"904812","upvote_count":"1","timestamp":"1684837440.0","poster":"AmmarFasih","content":"Selected Answer: C\nDropout is a regularization technique commonly used in model training with TensorFlow and other deep learning frameworks. It is employed to prevent overfitting, a phenomenon where a model learns to perform well on the training data but fails to generalize well to new, unseen data."},{"poster":"IgnacioBL","content":"Selected Answer: C\nAnswer is C","timestamp":"1678797840.0","upvote_count":"2","comment_id":"838837"},{"comment_id":"810149","timestamp":"1676510160.0","poster":"Morock","upvote_count":"1","content":"Selected Answer: C\nDropout is a regularization method to remove random selection of fixed number of unit in a neural network layer. So pick C for this question."},{"upvote_count":"1","content":"Selected Answer: C\nbecouse it is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.","timestamp":"1676484360.0","comment_id":"809827","poster":"enghabeth"},{"content":"C. Dropout Methods\n\nDropout is a regularization technique that can be used to prevent overfitting of the model to the training data. It works by randomly dropping out a certain percentage of neurons during training, which helps to reduce the complexity of the model and prevent it from memorizing the training data. This can improve the model's ability to generalize to new data and reduce the risk of poor performance when tested against new data.","upvote_count":"4","comment_id":"784777","poster":"samdhimal","comments":[{"upvote_count":"4","comment_id":"784778","timestamp":"1674430620.0","content":"A. Threading: it's not a method to address overfitting, it's a technique to improve the performance of the model by parallelizing the computations using multiple threads.\n\nB. Serialization: it's a technique to save the model's architecture and trained parameters to a file, it's helpful when you want to reuse the model later, but it doesn't address overfitting problem.\n\nD. Dimensionality Reduction: it's a technique that can be used to reduce the number of features in the data, it's helpful when the data contains redundant or irrelevant features, but it doesn't address overfitting problem directly.","poster":"samdhimal"}],"timestamp":"1674430620.0"},{"timestamp":"1673064420.0","content":"answer is likely to be C, but D (dimensionality reduction) can also be used to mitigate overfitting too! Not sure which one is the correct answer.","upvote_count":"2","poster":"korntewin","comment_id":"768225"},{"comment_id":"755049","poster":"Brillianttyagi","content":"Selected Answer: C\nCorrect","timestamp":"1671903480.0","upvote_count":"1"},{"timestamp":"1670668440.0","poster":"odacir","comment_id":"740847","upvote_count":"1","content":"Answer is C.\nWe are in an over feting problem with nerual-net model.\nRead at least the abstract of this.\nhttps://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf"},{"poster":"ovokpus","comment_id":"720738","content":"Selected Answer: C\nCorrect","upvote_count":"3","timestamp":"1668709380.0"},{"upvote_count":"1","poster":"Atnafu","timestamp":"1666587840.0","content":"C is the correct answer. \nDropouts: are special layers that you can add to control some operations or add functionalities. If training has an issue you can take them out. \nIn the question, it fits training but not new data, i.e., overfitting.\nSolution for overfitting is dropout","comment_id":"702702"}],"choices":{"D":"Dimensionality Reduction","C":"Dropout Methods","B":"Serialization","A":"Threading"},"question_text":"Your company built a TensorFlow neutral-network model with a large number of neurons and layers. The model fits well for the training data. However, when tested against new data, it performs poorly. What method can you employ to address this?","answer":"C","answers_community":["C (100%)"],"answer_images":[],"timestamp":"2022-09-02 14:46:00","answer_description":"","answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/79414-exam-professional-data-engineer-topic-1-question-1/","question_images":[],"exam_id":11},{"id":"OZeiJlidm9h9Dy52I84j","answers_community":["BDE (53%)","BDF (42%)","5%"],"question_id":2,"exam_id":11,"choices":{"F":"Use Google Stackdriver Audit Logging to determine policy violations.","B":"Restrict access to tables by role.","A":"Disable writes to certain tables.","E":"Segregate data across multiple tables or databases.","D":"Restrict BigQuery API access to approved users.","C":"Ensure that the data is encrypted at all times."},"answer_images":[],"topic":"1","answer":"BDE","question_images":[],"question_text":"Your company is in a highly regulated industry. One of your requirements is to ensure individual users have access only to the minimum amount of information required to do their jobs. You want to enforce this requirement with Google BigQuery. Which three approaches can you take? (Choose three.)","unix_timestamp":1584258240,"discussion":[{"comment_id":"529998","content":"correct option -> B. Restrict access to tables by role. \nReference: https://cloud.google.com/bigquery/docs/table-access-controls-intro\n\ncorrect option -> D. Restrict BigQuery API access to approved users. \n***Only approved users will have access which means other users will have minimum amount of information required to do their job.***\nReference: https://cloud.google.com/bigquery/docs/access-control\n\ncorrect option -> F. Use Google Stackdriver Audit Logging to determine policy violations.\nReference: https://cloud.google.com/bigquery/docs/table-access-controls-intro#logging\n\nA. Disable writes to certain tables. ---> Read is still available(not minimal access)\nC. Ensure that the data is encrypted at all times. ---> Data is encrypted by default.\nE. Segregate data across multiple tables or databases. ---> Normalization is of no help here.","poster":"samdhimal","comments":[{"upvote_count":"27","comment_id":"784833","timestamp":"1727154720.0","content":"I was WRONG. I am not sure why s o many upvotes lol.\n\nI think this is the correct answer:\nB. Restrict access to tables by role.\nD. Restrict BigQuery API access to approved users.\nE. Segregate data across multiple tables or databases.\n\nRestrict access to tables by role: You can use BigQuery's access controls to restrict access to specific tables based on user roles. This allows you to ensure that users can only access the data they need to do their job.\nRestrict BigQuery API access to approved users: By using Cloud Identity and Access Management (IAM) you can control who has access to the BigQuery API, and what actions they are allowed to perform. This will help to ensure that only authorized users can access the data.\nSegregate data across multiple tables or databases: You can use multiple tables or databases to separate different types of data, so that users can only access the data they need. This will prevent users from seeing data they shouldn't have access to.","comments":[{"poster":"directtoking","comment_id":"1334204","upvote_count":"1","timestamp":"1735570980.0","content":"If there is \"Restrict access to tables by role\", then what is the requirement for \"Segregate data across multiple tables or databases\"?"},{"content":"Option A is incorrect because disabling writes to certain tables would prevent users from updating the data which is not in line with the goal of providing access to the minimum amount of information required to do their jobs.\nOption C is incorrect because while data encryption is important for security it doesn't specifically help with providing users access to the minimum amount of information required to do their jobs.\nOption F is incorrect because while Google Stackdriver Audit Logging can help to determine policy violations it does not help to enforce the access controls and segregation of data.","timestamp":"1674435600.0","comment_id":"784834","upvote_count":"6","poster":"samdhimal","comments":[{"content":"There is no database in Bigquery, only datasets. I would pick it if it says \"tables and datasets\".","timestamp":"1676647200.0","comments":[{"timestamp":"1715793540.0","comment_id":"1212061","content":"What about cloud SQL and Spanner","poster":"billalltf","upvote_count":"1"}],"poster":"[Removed]","comment_id":"812055","upvote_count":"4"}]}],"poster":"samdhimal"}],"timestamp":"1642874220.0","upvote_count":"39"},{"poster":"IsaB","content":"Yes. Access control on table level is now possible in BigQuery : https://cloud.google.com/bigquery/docs/table-access-controls-intro","upvote_count":"15","comments":[{"poster":"axantroff","upvote_count":"1","comment_id":"1056915","timestamp":"1698592800.0","content":"Thanks. For me, this type of answer is more valuable because even as time passes, I can revisit existing solutions and ideas and refresh the concepts of the initial question. It helped me on the ACE exam"}],"comment_id":"172720","timestamp":"1599143100.0"},{"upvote_count":"1","content":"Selected Answer: BDE\nB - Use IAM to define granular permissions.\nD - Only authorised users or systems can query or manipulate BigQuery data.\nE - By segregating data into different tables or datasets, specific permissions can be assigned to each data subset.","comment_id":"1339958","poster":"cqrm3n","timestamp":"1736783220.0"},{"upvote_count":"3","poster":"NicolasN","timestamp":"1727154780.0","content":"Selected Answer: BDE\nI disagree with [F]. It's too late for a \"highly regulated industry\" to detect access violations by audit logs.\n[E] is a more reasonable answer, since it is a kind of row-level security, especially the times when BigQuery row-level security wasn't available.\nIt is a practice still recommended (even with row-level sec. available) for the extreme scenario that:\n(Through repeated observation of query duration when querying tables with row-level access policies,) \"a user could infer the values of rows that otherwise might be protected by row-level access policies\"\n\"If you are sensitive to this level of protection, we recommend using separate tables to isolate rows with different access control requirements, instead.\"\nSource: \nhttps://cloud.google.com/bigquery/docs/best-practices-row-level-security#limit-side-channel-attacks","comment_id":"725166"},{"comment_id":"730624","comments":[{"content":"E says segregate across multiple tables or databases, this is not the pattern of BigQuery, in BQ there is only one database, and you can organize your data in datasets...","poster":"odacir","comment_id":"740917","upvote_count":"1","timestamp":"1670671620.0"}],"content":"Selected Answer: BDE\nE seems more sensible to be as the question concentrates more on table access restriction than access violation, policy violations can only be determined through stackdriver how ever we cant restrict the access to tables.Probably option E should be considered as, by segregating the data into diferrent tables , we can restrict access to tables.","upvote_count":"3","timestamp":"1727154780.0","poster":"Asheesh1909"},{"upvote_count":"4","poster":"rtcpost","timestamp":"1727154720.0","content":"Selected Answer: BDE\nB. Restrict access to tables by role: You can define roles in BigQuery and grant specific permissions to these roles to control who can access particular tables.\n\nD. Restrict BigQuery API access to approved users: You can control access to the BigQuery API and, consequently, to the underlying data by ensuring that only approved users or services can make API requests.\n\nE. Segregate data across multiple tables or databases: You can separate data into different tables or databases based on user access requirements, which allows you to limit users' access to specific data sets.\n\nThese approaches, when used together, can help you enforce data access controls in a regulated environment. Options A, C, and F are also important considerations but are not direct methods for enforcing fine-grained access control to specific data.","comment_id":"1050479"},{"comment_id":"1263988","content":"B. Restrict access to tables by role.\n\nUse IAM roles and permissions to control access to specific datasets or tables based on the user’s role.\nD. Restrict BigQuery API access to approved users.\n\nLimit API access to only those users or services that need it, ensuring that unauthorized users cannot interact with the data.\nE. Segregate data across multiple tables or databases.\n\nOrganize data in a way that separates sensitive information, allowing more granular control over who has access to specific datasets.\nThese options directly contribute to enforcing the principle of least privilege, ensuring users can only access the data necessary for their roles.","poster":"SatyamKishore","timestamp":"1727154720.0","upvote_count":"1"},{"content":"Selected Answer: BDE\nYou want to enforce this requirement with Google BigQuery -> BDE","timestamp":"1702634280.0","poster":"MaxNRG","upvote_count":"1","comment_id":"1097179"},{"poster":"RT_G","upvote_count":"1","content":"Selected Answer: BDF\nBDF. We are fairly unanimous with options B and D. I'm going with F because it does help identifying policy violations which is also one aspect to be considered when designing access controls. Option D only indicates segregating into multiple tables and databases which may or may not help with controlling access leaving it open-ended for the architect to decide.","comment_id":"1065081","timestamp":"1699382100.0"},{"upvote_count":"1","comment_id":"1061060","content":"Selected Answer: BDF\nIn Google BigQuery, you can organize and segregate data across multiple tables within the same dataset, but you cannot directly segregate data into separate databases. BigQuery uses a flat namespace structure where data is organized into datasets and tables within those datasets. Datasets are the highest level of organization within BigQuery.\n\nSo i'm sticking with BDF","timestamp":"1698983160.0","poster":"rocky48"},{"timestamp":"1697798100.0","upvote_count":"1","comment_id":"1048678","content":"Selected Answer: BDE\nB. Restrict access to tables by role.\nD. Restrict BigQuery API access to approved users.\nE. Segregate data across multiple tables or databases.","poster":"RheaZzang"},{"comment_id":"987088","timestamp":"1692680580.0","poster":"AnonymousPanda","upvote_count":"2","content":"Selected Answer: BDF\nBDF as per other answers"},{"poster":"nescafe7","timestamp":"1690696200.0","comment_id":"966895","content":"Selected Answer: BDF\nRegarding E or F, opinions seem to be divided into two parts.\n\nI think E is insufficient because it seems that appropriate conditions must be additionally described for table or dataset separation.\n\nF is also emphasized in Google's official textbook. You need to ensure that it is operating well as set up through monitoring.\n\nSo, BDF!","upvote_count":"4"},{"upvote_count":"3","comments":[{"timestamp":"1690482060.0","comment_id":"964963","content":"I was thinking the same thing. I thought dataset access gave you access to all tables within it, and that you couldn't restrict access on the table level.","poster":"FP77","upvote_count":"1"}],"comment_id":"961743","content":"Selected Answer: DEF\nWhy B is correct? Access control can only be applied on dataset and views, not on partitions and tables. => So it is not possible to restrict access to table, but only to dataset. Can someone help me understand why in this scenario B is correct?","timestamp":"1690211460.0","poster":"Liting"},{"content":"Selected Answer: BDF\nOption E says \"...or databases\". The data housing service in question in BigQuery and the context is to design that support BigQuery access delegation. Seems random to include moving to another database as an option. If it did not mention databases and stopped at just tables, then E would also be the right option","poster":"KK0202","comment_id":"938724","timestamp":"1688090100.0","upvote_count":"2"},{"upvote_count":"2","poster":"Oleksandr0501","timestamp":"1682325720.0","content":"B. Restrict access to tables by role: This approach can be used to control access to tables based on user roles. Access controls can be set at the project, dataset, and table level, and roles can be customized to provide granular access controls to different groups of users.\n\nD. Restrict BigQuery API access to approved users: This approach involves using IAM (Identity and Access Management) to control access to the BigQuery API. Access can be granted or revoked at the project or dataset level, and policies can be customized to control access based on user roles, IP addresses, and other factors.\n\nE. Segregate data across multiple tables or databases: This approach involves breaking down large datasets into smaller, more manageable tables or databases. This helps to ensure that individual users have access only to the minimum amount of information required to do their jobs, and reduces the risk of data breaches or policy violations.","comment_id":"879167"},{"content":"Selected Answer: BDE\nF won't avoid undesired access, only detect after it already happened.\nE makes it easier to control access.","upvote_count":"5","timestamp":"1678976880.0","comment_id":"841046","poster":"juliobs"},{"comment_id":"817509","poster":"jin0","content":"I think BDF.\nSegregating the table is needed to try to distribute into a few of dataset not only I said but it looks like complicated.","timestamp":"1677045300.0","upvote_count":"1"},{"timestamp":"1675840860.0","upvote_count":"5","comment_id":"801725","poster":"SidneyHod","content":"Selected Answer: BDE\nI don't choose stack driver because it suits more for audit"},{"comments":[{"timestamp":"1673470980.0","content":"It's not about an audit... it's about violation for least privileges. this will aid in correcting","poster":"desertlotus1211","comment_id":"772894","upvote_count":"1"}],"poster":"Nirca","content":"Selected Answer: BDF\nanswer F -> audit is not part of the question; so I would not go for it\nanswer E -> Yes - due to segregation you can add different rights to different data/users","comment_id":"771309","upvote_count":"3","timestamp":"1673347860.0"},{"poster":"Jackalski","comment_id":"759739","upvote_count":"6","timestamp":"1672225920.0","comments":[{"comment_id":"772897","content":"E is an administrative nightmare... it's about given a user only the privileges to do their function. So it's all based on their service account.","upvote_count":"1","poster":"desertlotus1211","timestamp":"1673471100.0"}],"content":"Selected Answer: BDE\nanswer F -> audit is not part of the question; so I would not go for it\nanswer E -> Yes - due to segregation you can add different rights to different data/users"},{"poster":"Nirca","upvote_count":"4","comment_id":"741660","timestamp":"1670760420.0","content":"Selected Answer: BDF\nare the right answers"},{"timestamp":"1670671920.0","content":"Selected Answer: BDF\nBDF\n A. Disable writes to certain tables. ← No sense, to write it at user or group level no table level\n B. Restrict access to tables by role. ← YES, Best recommended practices \n C. Ensure that the data is encrypted at all times. ← Data is always encrypted at rest and in flight with BQ.\n D. Restrict BigQuery API access to approved users. ← YES, best recommended practices\n E. Segregate data across multiple tables or databases ← BigQuery not have multiple databases, it has data sets and tables, and it's nor necessary to segregate the data, you have ACL to control access to column level, etc.\n F. Use Google Stackdriver Audit Logging to determine policy violations. ← Good practice, if your industry is regulated, have audit logging.","comment_id":"740927","upvote_count":"4","poster":"odacir"},{"timestamp":"1669680240.0","comment_id":"729733","upvote_count":"1","poster":"hauhau","content":"Option E doesn't mention about restrict access"},{"content":"BDE is correct\nF is not an option for 'ensure individual users have access only to the minimum amount of information required to do their jobs'","timestamp":"1669637160.0","poster":"sfsdeniso","upvote_count":"1","comment_id":"729061"},{"upvote_count":"1","comment_id":"725702","timestamp":"1669282500.0","content":"Answer BDF","poster":"Atnafu"},{"content":"Selected Answer: BDF\nCorrect: BDF","comment_id":"707946","poster":"MisuLava","upvote_count":"2","timestamp":"1667150880.0"},{"content":"D B and F. Restrict in the first place; then you don't have write issues. Write was not even mentioned in the prompt; just access. This is more of a logical question. I believe E should be included","poster":"rowan_","comment_id":"647413","upvote_count":"1","timestamp":"1660616820.0"},{"content":"B,D,E,F looks like valid,\nSince option E is Segregate data across multiple tables or databases here it is given databases which is not valid as there is only datasets in BigQuery","comment_id":"510105","upvote_count":"3","poster":"prasanna77","timestamp":"1640591700.0"},{"upvote_count":"2","poster":"MaxNRG","timestamp":"1636310160.0","comment_id":"474010","content":"It's an old question, old answers were DEF imho:","comments":[{"comment_id":"474012","content":"now BDF is ok","timestamp":"1636310160.0","upvote_count":"1","poster":"MaxNRG"}]},{"comment_id":"462028","content":"Ans: B, D and F","upvote_count":"1","timestamp":"1634215440.0","poster":"anji007"},{"timestamp":"1633749960.0","comment_id":"459471","content":"Is these questions enough to clear the exam?","poster":"kippy_sane","upvote_count":"1"},{"upvote_count":"1","content":"Vote for BDF","comments":[{"upvote_count":"1","poster":"Darpan15","comment_id":"404445","timestamp":"1626074520.0","content":"Hi Sumanshu, I have exam in few days. \nCan you please tell me how many questions are coming approximately from this Exam Topic?\n\nAnd above BDF is correct right?"}],"comment_id":"401704","timestamp":"1625734080.0","poster":"sumanshu"},{"comment_id":"319810","poster":"lbhhoya82","upvote_count":"1","timestamp":"1616650320.0","content":"Correct : B, D, F"},{"content":"B,D,F is correct","poster":"sid091","comment_id":"300814","upvote_count":"2","timestamp":"1614520500.0"},{"comment_id":"285708","poster":"senthil836","content":"Correct Answer is D,E, F","timestamp":"1612722180.0","upvote_count":"3"},{"timestamp":"1612627320.0","poster":"naga","comment_id":"284935","upvote_count":"2","content":"Correct BDF"},{"upvote_count":"2","comment_id":"281725","timestamp":"1612252500.0","content":"Correct answers : BDF","poster":"someshsehgal"},{"content":"https://cloud.google.com/bigquery/docs/table-access-controls-intro\nTable level ACL is now possible.","upvote_count":"2","comment_id":"214145","timestamp":"1604675880.0","poster":"MunnurS"},{"content":"the question may out-of-date, table level access control is now available https://cloud.google.com/bigquery/docs/table-access-controls-intro","poster":"Darlee","upvote_count":"3","comment_id":"193837","timestamp":"1601946480.0","comments":[{"poster":"DeepakKhattar","comment_id":"213800","upvote_count":"3","timestamp":"1604625300.0","content":"Since new features/options are released on regular basis, so what should be the strategy before going for exam since its difficult to figure out all the changes what has happened recently"}]},{"timestamp":"1597655700.0","poster":"PRABHUKKARTHI","upvote_count":"4","content":"Option BDF: \nA is not because, we cannot restrict write access to particular tables as the access is provided at dataset level. \nC is not because by default automatically encrypted by Google \nE is not meaningful","comment_id":"159808"},{"poster":"mikey007","upvote_count":"2","content":"B,D,F is correct","timestamp":"1595352120.0","comment_id":"140464"}],"answer_ET":"BDE","url":"https://www.examtopics.com/discussions/google/view/16642-exam-professional-data-engineer-topic-1-question-10/","answer_description":"","isMC":true,"timestamp":"2020-03-15 08:44:00"},{"id":"pDqV83WBHhzGS1fbvvjS","answers_community":["B (100%)"],"answer_ET":"B","timestamp":"2020-03-19 17:39:00","isMC":true,"question_images":[],"unix_timestamp":1584635940,"answer_description":"","discussion":[{"content":"I think we need a pipeline, so it's B to me.","comment_id":"66024","upvote_count":"29","timestamp":"1584635940.0","poster":"jvg637"},{"poster":"[Removed]","timestamp":"1584891060.0","upvote_count":"16","content":"Correct - B","comment_id":"66988"},{"content":"Selected Answer: B\nFor real-time analysis and quick data availability, the appropriate option is the combination of the pipeline with BigQuery.","upvote_count":"1","poster":"Erg_de","comment_id":"1306807","timestamp":"1730702400.0"},{"timestamp":"1701462000.0","poster":"Helinia","content":"“need the data to be available within 1 minute of ingestion for real-time analysis” → low latency requirement → Dataflow streaming\n\nThe database can either be BQ or BigTable for this kind of requirement in data volume and latency. But it mentioned that the destination has to be BQ, so B.","upvote_count":"1","comment_id":"1085479"},{"upvote_count":"3","poster":"NeoNitin","timestamp":"1691236380.0","content":"ANSWER b.\nFULL question ihave if you nee mail me \nneonitin6ATtherate......","comment_id":"972970","comments":[{"poster":"aryaavinash","timestamp":"1693942020.0","content":"full email id please ?","comment_id":"999841","upvote_count":"1"}]},{"poster":"Oleksandr0501","comment_id":"880648","content":"Selected Answer: B\nI think we need a pipeline, so it's B to me.))","timestamp":"1682441340.0","upvote_count":"3"},{"upvote_count":"2","timestamp":"1681731480.0","comment_id":"872645","content":"Selected Answer: B\nI think we need a pipeline, so it's B to me.","poster":"votinhluombikip"},{"poster":"JANCAI","comment_id":"830638","content":"Why the answer from the <reveal answer> is C??","upvote_count":"1","timestamp":"1678091520.0"},{"comment_id":"735931","upvote_count":"1","content":"Selected Answer: B\nB is the answer.","timestamp":"1670244240.0","poster":"zellck"},{"poster":"Prasha123","content":"Selected Answer: B\nNeed pipeline so its B","timestamp":"1668376320.0","comment_id":"717539","upvote_count":"1"},{"comment_id":"675317","upvote_count":"7","poster":"sedado77","timestamp":"1663777440.0","content":"Selected Answer: B\nI got this question on sept 2022. Answer is B"},{"content":"Selected Answer: B\nomg. B only","comment_id":"518497","upvote_count":"2","poster":"medeis_jar","timestamp":"1641496080.0"},{"timestamp":"1640875620.0","poster":"MaxNRG","upvote_count":"7","content":"Selected Answer: B\nIs B, if we expect a growth we’ll need some buffer (that will be pub-sub) and the dataflow pipeline to stream data in big query.\nThe tabledata.insertAll method is not valid here.","comment_id":"513456"},{"upvote_count":"3","poster":"hendrixlives","comment_id":"504512","timestamp":"1639869780.0","content":"Selected Answer: B\nB, streaming with dataflow"},{"timestamp":"1637981100.0","upvote_count":"1","comment_id":"487806","content":"Wrong answer shown again by examtopics.com\nAns: B","poster":"JG123"},{"content":"B => with dataflow you can parallelize data ingestion","comment_id":"453452","poster":"Ysance_AGS","upvote_count":"2","timestamp":"1632837480.0","comments":[{"content":"And make it streaming","timestamp":"1635974400.0","poster":"szefco","comment_id":"472292","upvote_count":"1"}]},{"poster":"sandipk91","content":"B is the right answer","timestamp":"1628450760.0","upvote_count":"2","comment_id":"421798"},{"content":"Vote for B","timestamp":"1625156460.0","upvote_count":"4","comment_id":"396180","poster":"sumanshu"},{"timestamp":"1608056460.0","upvote_count":"6","poster":"felixwtf","comment_id":"244833","content":"You need a pipeline because this type of operation can be easily parallelized, as the ingestion can be divided between into chunks (PCollections) and handled by many workers.","comments":[{"upvote_count":"3","poster":"felixwtf","timestamp":"1608056460.0","comment_id":"244834","content":"so, B is the right answer."}]},{"timestamp":"1599559020.0","upvote_count":"3","content":"You need a pipeline to feed streaming data , Answer : B","poster":"Tanmoyk","comment_id":"175788"},{"poster":"haroldbenites","timestamp":"1598004600.0","content":"B is correct","comment_id":"162857","upvote_count":"2"},{"poster":"dambilwa","timestamp":"1593051420.0","upvote_count":"2","comment_id":"119033","content":"Option [B] is correct. 'C' is incorrect because there's a different method -'tabledata.insertAll' for streaming inserts"},{"timestamp":"1592549640.0","upvote_count":"3","comments":[{"content":"One day has 60 * 24 = 1,440 minutes = 1,440 batch jobs. This is still not exceeding the limit of 1,500. However, I still think it is B. :)","upvote_count":"4","timestamp":"1597903380.0","poster":"krizt9","comment_id":"162002"}],"comment_id":"113732","content":"The following limits apply when you load data into BigQuery.\nLoad jobs per table per day — 1,500 (including failures)\nLoad jobs per project per day — 100,000 (including failures)\n\nso batch loading will not be able handle the expected capacity - so answer mut be B","poster":"Callumr"},{"timestamp":"1585372980.0","content":"Answer: B\nDescription: Cloud dataflow is used to stream data to bigquery in near realtime","upvote_count":"9","comment_id":"68762","poster":"[Removed]"},{"content":"Answer B : this use case ask for streaming data.","comment_id":"66811","upvote_count":"6","poster":"Rajokkiyam","timestamp":"1584855180.0"}],"topic":"1","choices":{"B":"Use a Cloud Dataflow pipeline to stream data into the BigQuery table.","A":"Use bq load to load a batch of sensor data every 60 seconds.","C":"Use the INSERT statement to insert a batch of data every 60 seconds.","D":"Use the MERGE statement to apply updates in batch every 60 seconds."},"question_text":"You have a requirement to insert minute-resolution data from 50,000 sensors into a BigQuery table. You expect significant growth in data volume and need the data to be available within 1 minute of ingestion for real-time analysis of aggregated trends. What should you do?","answer":"B","question_id":3,"url":"https://www.examtopics.com/discussions/google/view/16982-exam-professional-data-engineer-topic-1-question-100/","answer_images":[],"exam_id":11},{"id":"LYik3YEktKJRR36FoUTg","question_id":4,"unix_timestamp":1584855300,"answer_images":[],"choices":{"B":"Export the records from the database as an Avro file. Copy the file onto a Transfer Appliance and send it to Google, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.","A":"Export the records from the database as an Avro file. Upload the file to GCS using gsutil, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.","D":"Export the records from the database as an Avro file. Create a public URL for the Avro file, and then use Storage Transfer Service to move the file to Cloud Storage. Load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.","C":"Export the records from the database into a CSV file. Create a public URL for the CSV file, and then use Storage Transfer Service to move the file to Cloud Storage. Load the CSV file into BigQuery using the BigQuery web UI in the GCP Console."},"question_text":"You need to copy millions of sensitive patient records from a relational database to BigQuery. The total size of the database is 10 TB. You need to design a solution that is secure and time-efficient. What should you do?","answers_community":["B (50%)","A (46%)","4%"],"isMC":true,"exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/17204-exam-professional-data-engineer-topic-1-question-101/","answer_description":"","question_images":[],"discussion":[{"comments":[{"content":"With reasonable network connectivity (for example, 1 Gbps), transferring 100 TB of data online takes over 10 days to complete. If this rate is acceptable, an online transfer is likely a good solution for your needs. If you only have a 100 Mbps connection (or worse from a remote location), the same transfer takes over 100 days. At this point, it's worth considering an offline-transfer option such as Transfer Appliance.","comment_id":"1342578","poster":"grshankar9","timestamp":"1737215280.0","upvote_count":"1"},{"timestamp":"1594665780.0","content":"https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets\nThe table shows for 1Gbps, it takes 30 hrs for 10 TB. Generally, corporate internet speeds are over 1Gbps. I'm inclined to pick A","upvote_count":"5","poster":"tprashanth","comments":[{"comments":[{"upvote_count":"1","content":"Security is not a concern, the data is encrypted at rest as well as in transit","poster":"grshankar9","comment_id":"1342574","timestamp":"1737215040.0"}],"content":"SAY MY NAME!\nYou need to Transfer Sensitive Patient information, over public ISP you shouldn't do that.","upvote_count":"3","poster":"BigQuery","timestamp":"1638638580.0","comment_id":"493845"},{"timestamp":"1685538720.0","upvote_count":"2","comment_id":"911310","poster":"forepick","content":"If you transfer 10TBs over the wire, your network will be blocked for the entire transfer time. This isn't something a company would be happy to swallow."}],"comment_id":"134228"},{"upvote_count":"19","timestamp":"1601373120.0","content":"Answer is B,gsutil has a limit of 1TBaccording to Google documentation,if data is morethan 1TBthen we have to use Transfer Appliance.","comments":[{"timestamp":"1737214620.0","upvote_count":"1","poster":"grshankar9","comment_id":"1342570","content":"While gsutil itself doesn't have a strict data limit, the underlying Google Cloud Storage service does, allowing for individual object sizes up to 5 Terabytes (TiB). This means that when using gsutil to transfer data, the maximum file size you can upload or download is 5 TiB"},{"poster":"grshankar9","upvote_count":"1","comment_id":"1342579","content":"According to Google documentation, for files of size > 1TB STS is to be used","timestamp":"1737215400.0"},{"timestamp":"1628092860.0","content":"The answer is clearly seen here: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options","comment_id":"419809","upvote_count":"9","poster":"Yiouk"}],"poster":"TNT87","comment_id":"189583"},{"upvote_count":"4","timestamp":"1672453020.0","comment_id":"762435","poster":"AzureDP900","content":"B is right answer"}],"poster":"Ganshank","upvote_count":"63","content":"You are transferring sensitive patient information, so C & D are ruled out. Choice comes down to A & B. Here it gets tricky. How to choose Transfer Appliance: (https://cloud.google.com/transfer-appliance/docs/2.0/overview)\nWithout knowing the bandwidth, it is not possible to determine whether the upload can be completed within 7 days, as recommended by Google. So the safest and most performant way is to use Transfer Appliance.\nTherefore my choice is B.","comment_id":"76731","timestamp":"1587357420.0"},{"poster":"SSV","timestamp":"1594086480.0","content":"Answer should be B: A is also correct but it has its own limit. It allows only 5TB data upload at a time to cloud storage. \nhttps://cloud.google.com/storage/quotas\nI will go with B","comments":[{"content":"5Tb \"for individual objects\". Create smaller AVRO files.","comment_id":"262487","upvote_count":"2","timestamp":"1610102400.0","poster":"VASI"},{"timestamp":"1610102700.0","comment_id":"262489","poster":"VASI","content":"AVRO compression can reduce file size to a tenth","upvote_count":"3"}],"upvote_count":"8","comment_id":"128500"},{"comment_id":"1342589","timestamp":"1737216660.0","poster":"grshankar9","content":"Selected Answer: A\nC and D are ruled out owing to limitations of Cloud Storage with regards to max file size of 5 TB ( Refer - https://cloud.google.com/storage-transfer/docs/known-limitations-transfer#:~:text=performance%20uploads%3A%206GiB-,Scaling%20limitations,of%20Gbps%20in%20transfer%20speed ). The answer should be between A & B. if we were to assume bandwidth of 1 GBPS, it would only take about 1 day. Even if the bandwidth were a tenth of 1 GBPS, it would take about 10 days to transfer 10 TB. With a transfer appliance, it would take minimum of 25 days. I would go with A.","upvote_count":"1"},{"comment_id":"1327001","content":"Selected Answer: B\nB - only because it says sensitive data, you can never be sure uploading over internet!","upvote_count":"1","poster":"clouditis","timestamp":"1734286200.0"},{"timestamp":"1727217300.0","poster":"Preetmehta1234","comment_id":"1288800","comments":[{"upvote_count":"1","content":"gsutil is deprecated and has been replaced by gcloud storage. Gcloud storage is faster and requires less manual optimization for uploads and downloads","timestamp":"1737215520.0","poster":"grshankar9","comment_id":"1342581"}],"upvote_count":"1","content":"Selected Answer: B\nAlso, \nFor your scenario with 10 TB of data in Cloud SQL, if you export to Avro without specifying compression, you can expect the resulting Avro file to be around the same size, potentially slightly smaller depending on the data characteristics. Here in this question, there is no mentioning about compression. \n\nSo let's not assume that the data being used in Avro format will get compressed. \n\nIf Google cloud storage itself, can't handle an object of size greater than 5 TB, there is no point of using gsutil"},{"comment_id":"1288797","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/storage-transfer/docs/known-limitations-transfer\n\nCloud Storage 5TiB object size limit\nCloud Storage supports a maximum single-object size up 5 tebibytes. If you have objects larger than 5TiB, the object transfer fails for those objects for either Cloud Storage or Storage Transfer Service.","timestamp":"1727216880.0","poster":"Preetmehta1234"},{"content":"Selected Answer: D\nwhile Option A is feasible and could work depending on specific requirements and security measures implemented, Option D (exporting as Avro, using Storage Transfer Service, and then loading into BigQuery) generally offers a more secure, efficient, and managed approach for transferring sensitive patient records into BigQuery from a relational database.Avro files uploaded to GCS will need to be secured. While GCS itself offers security features like IAM policies and access controls, using a public URL (as suggested in Option A) introduces additional security concerns.","comment_id":"1239275","poster":"hussain.sain","upvote_count":"1","timestamp":"1719666780.0"},{"comment_id":"1205335","upvote_count":"1","content":"Selected Answer: B\nto securely transfer data and looking at the size of data B is the correct option.","poster":"Naresh_4u","timestamp":"1714633500.0"},{"poster":"GCanteiro","timestamp":"1706875200.0","comment_id":"1138496","content":"Selected Answer: A\nIMO \"A\" is the most suitable option since the transfer appliance could take 25 days to get the appliance and then 25 days to ship it back and have the data available.\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#transfer-speeds","upvote_count":"1"},{"comment_id":"1102562","poster":"TVH_Data_Engineer","timestamp":"1703165760.0","content":"Selected Answer: B\nGiven the sensitivity of the patient records and the large size of the data, using Google's Transfer Appliance is a secure and efficient method. The Transfer Appliance is a hardware solution provided by Google for transferring large amounts of data. It enables you to securely transfer data without exposing it over the internet.","upvote_count":"1"},{"comment_id":"1088148","timestamp":"1701743460.0","content":"Selected Answer: B\nOption B combines security, efficiency, and ease of use, making it a suitable choice for transferring sensitive patient records to BigQuery.","poster":"rocky48","upvote_count":"1"},{"upvote_count":"4","content":"Selected Answer: A\n10 TB is nothing. With a single 10 GB interconnect you could transfer the data in 3 hours or even with a 1 GB speeds without interconnect you could transfer it in one weekend. The transfer appliance will take 25 days to get the appliance and then 25 days while you wait for the data to be available that is not \"time-efficient\" at all. I go with A instead of B.","timestamp":"1699407000.0","poster":"spicebits","comments":[{"poster":"spicebits","timestamp":"1699407300.0","content":"I got the 25 days + 25 days from here: https://cloud.google.com/transfer-appliance/docs/4.0/overview#transfer-speeds","comment_id":"1065278","upvote_count":"2"}],"comment_id":"1065275"},{"content":"Selected Answer: A\ntransfer appliance will take time more than gsutil. and we did not mention yet if the location of the organization has google data centre","timestamp":"1695448200.0","comment_id":"1014679","upvote_count":"3","poster":"A_Nasser"},{"timestamp":"1692581040.0","upvote_count":"2","content":"Selected Answer: D\nAs per Google recommendation above 1TB of transfer from onprem or from Google cloud or other cloud storage like s3 etc we need to use storage transfer service.","poster":"DineshVarma","comment_id":"986153"},{"content":"Transfer Appliance would take 20 days for epected turnaround time. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#expected%20turnaround:~:text=The%20expected%20turnaround%20time%20for%20a%20network%20appliance%20to%20be%20shipped%2C%20loaded%20with%20your%20data%2C%20shipped%20back%2C%20and%20rehydrated%20on%20Google%20Cloud%20is%2020%20days.\n\nThe best answer would be A. \nIf gsutil consume/leverage 100MB it would take 12 days and more time-efficient than B. \nThis is a reasonable assumption. \nhttps://cloud.google.com/static/architecture/images/big-data-transfer-how-to-get-started-transfer-size-and-speed.png","poster":"arien_chen","timestamp":"1692483360.0","comment_id":"985448","upvote_count":"1"},{"upvote_count":"1","comment_id":"983420","timestamp":"1692264000.0","poster":"Colourseun","content":"I will go with \" A\" because of the transition time to take transfer appliance to Google and that also depends in the organisation location. gsutil works anywhere internet is available."},{"poster":"aewis","timestamp":"1689337560.0","comment_id":"951519","upvote_count":"1","content":"Selected Answer: B\nA will take crazy time if the organization didnt have a dedicated link"},{"timestamp":"1688988060.0","poster":"ZZHZZH","comment_id":"948029","upvote_count":"3","content":"Selected Answer: A\nTransfer Appliance is not as time-efficient when you have enough bandwitdh. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer_appliance_for_larger_transfers"},{"comment_id":"931315","timestamp":"1687505580.0","content":"Selected Answer: B\nThere is no \"cost effective\", if this is not a clear case for the appliance than what is?","upvote_count":"1","poster":"WillemHendr"},{"comment_id":"921572","content":"Selected Answer: A\nA is the answer, the question states the following facts:\n- Total size of database 10TB.\n- Solution needs to be:\n * Secure\n * Time-efficient\n\nTotal size of database:\nwill be significantly reduced in an avro file compression (up to 90% compression)\n\nSecure transfer:\nEven if we are dealing with sensitive data, data is encrypted when in transit while using `gsutils cp` to upload the data to GCS. https://cloud.google.com/storage/docs/gsutil/addlhelp/SecurityandPrivacyConsiderations#transport-layer-security\n\nTime-Efficient:\ngsutil could upload 10TB of data in 30 hours (or 1TB if its avro compressed first in 3 hours)","poster":"Ender_H","timestamp":"1686586920.0","upvote_count":"6"},{"timestamp":"1686314640.0","content":"Selected Answer: A\nIt has to be gsutil.\nIn this documentation: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options\nIt states that if it meets the projects deadline, use gsutil.\n\nAlso, for Transfer Appliance, here (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer_appliance_for_larger_transfers), it states:\nThe expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google Cloud is 20 days.\n\nEven with 100 Mbps, for 10 TB, it's 12 days. Almost half! of the Transfer Appliance.\n\nIt's, of course, option A.","poster":"dgteixeira","comment_id":"919319","upvote_count":"1"},{"poster":"forepick","content":"BTW, all options are talking about exporting RDBMS as a SINGLE file of 10TB.\nGCS object is limited to 5TB, pay attention","comments":[{"content":"Would it not be compressed in Avro?","timestamp":"1702137000.0","comment_id":"1091887","poster":"KanchanC","upvote_count":"1"}],"upvote_count":"1","timestamp":"1685538480.0","comment_id":"911306"},{"content":"Selected Answer: B\nA and B are the only options with no public access, and 10TB is too large for gsutil, therefore - B","comment_id":"911303","timestamp":"1685538360.0","upvote_count":"1","poster":"forepick"},{"timestamp":"1682091060.0","upvote_count":"1","content":"Selected Answer: B\ncatch here is \"security\" so I would prefer option B","comment_id":"876674","poster":"Prudvi3266"},{"poster":"Prudvi3266","comment_id":"875482","content":"Selected Answer: B\nOption B is correct as the per google doc gsutil is recommended for load less than 1TB","upvote_count":"1","timestamp":"1681986180.0"},{"upvote_count":"1","comment_id":"852858","poster":"MagnusCrn","timestamp":"1679985240.0","content":"Selected Answer: B\nFor only once time, it's better Transfer Appliance. It's not every week or month upload. Therefore my choice is B."},{"comment_id":"843658","content":"Selected Answer: B\nA depends on bandwidth and compression rate of Avro.\nIf B is an option (transfer appliance is available), it would work for sure. Could take weeks though.","timestamp":"1679223540.0","poster":"juliobs","upvote_count":"1"},{"timestamp":"1678337340.0","comment_id":"833601","upvote_count":"2","poster":"midgoo","content":"Selected Answer: A\nCouple of things to look at:\n1. Option B is 'Transfer Applicance' not 'Transfer Service'. And Transfer Applicance is NOT that time-efficient for just 10TB of data (note that you need to do a lot of steps for Transfer Applicance like shipping, setup, etc). Even with bandwidth jut 100Mbps (that is the 4G speed on my phone), I only need 12 days to upload all the data (assuming they cannot be compressed)\n2. Option A is better. Typically, a database export is 1/10th of the database. Let's say we can only export from 10TB to 5TB (to fit in GCS maximum allowed size). It still makes sense to go with option A. Unless the question say 'Transfer Service', we will definitely have a tough choice here."},{"poster":"Booqq","timestamp":"1677819180.0","upvote_count":"1","content":"gsutil through gsutil is not time efficient","comment_id":"827668"},{"content":"Selected Answer: B\ngsutil is recommended for data less than 1 TB. In this question, the data is 10 TB, so A is ruled out as well.\nIn my opinion, B is the correct answer.","upvote_count":"2","comment_id":"787740","timestamp":"1674657300.0","poster":"Acheto"},{"poster":"zellck","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options","timestamp":"1670243940.0","upvote_count":"1","comment_id":"735926"},{"upvote_count":"1","timestamp":"1668974340.0","comment_id":"722961","content":"Selected Answer: A\nFor me it's A, at the beginning I thought it was B but I don't really think transfer appliance is time-efficient, as I don't think I classify gsutil as an insecure transfer method. It obviously depends on the bandwidth but it would be very weird to me consider a really low bandwidth as the standard.","comments":[{"content":"“ Export the records from the database as an Avro file”, here it mentioned 'an', unless Avro can compress 10TB to less than 1 then we cant use GSutil given the limit is 1TB","timestamp":"1672671480.0","comment_id":"763798","poster":"wan2three","upvote_count":"1"}],"poster":"cajica"},{"content":"According to this it's B\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google","poster":"Atnafu","comment_id":"716356","timestamp":"1668210120.0","upvote_count":"1"},{"upvote_count":"1","poster":"clouditis","content":"B, as gsutil is only for 1 Tb or less data","comment_id":"676599","timestamp":"1663886580.0"},{"timestamp":"1663644060.0","poster":"John_Pongthorn","content":"Selected Answer: B\nIn sense of speek(time-effective)\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#transfer-speeds\nransfer Appliance, you can receive the appliance and capture 300 terabytes of data in under 25 days. Your data can be accessed in Cloud Storage within another 25 days, all without consuming any outbound network bandwidth.","upvote_count":"1","comment_id":"673759"},{"content":"Selected Answer: B\nless than 1TB . gsutil\nmore than 1TB, transfer service \nwhen the internet speeds aren't good, transfer appliance","upvote_count":"2","poster":"MounicaN","timestamp":"1662875340.0","comment_id":"665840"},{"timestamp":"1662532980.0","poster":"YorelNation","upvote_count":"1","content":"\"time efficient solution\" 7 days transfer appliance is pretty long if you ask me.","comment_id":"662103"},{"content":"Selected Answer: B\nB for sure\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options","comment_id":"653705","upvote_count":"1","timestamp":"1661817240.0","poster":"ducc"},{"content":"Selected Answer: B\nB is most secured way\nBecause it is patient information, it is needed to be very secured","poster":"ducc","upvote_count":"1","timestamp":"1661487840.0","comment_id":"652044"},{"upvote_count":"1","comment_id":"649534","timestamp":"1661028600.0","content":"A is better than B in speed. It takes two plus weeks for B. Both methods encrypted data during transfer.","poster":"t11"},{"poster":"[Removed]","upvote_count":"2","content":"\"Acquiring a Transfer Appliance is straightforward. In the console, you request a Transfer Appliance, indicate how much data you have, and then Google ships one or more appliances to your requested location. You're given a number of days to transfer your data to the appliance (\"data capture\") and ship it back to Google.\n\nThe expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google Cloud is 20 days.\"\n\nsounds like a long time to me. I'm going with A.","comment_id":"639799","timestamp":"1659212220.0"},{"upvote_count":"1","poster":"BJPJowee","comment_id":"628679","timestamp":"1657267560.0","content":"A is the only valid answer! The key word is \"time-efficient\". Although gsutil has a limit of 1TB per file smaller files can be created. So splitting it into multiple files will be an idea. B is not time-efficient, you will have to ship to google cloud. What if your data center is remote with no fast delivery service?"},{"timestamp":"1656270720.0","comment_id":"622760","upvote_count":"2","poster":"IJFIJD","content":"Selected Answer: B\nThe gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB) over a typical enterprise-scale network, from a private data center to Google Cloud. We recommend that you include gsutil in your default path when you use Cloud Shell\nits Transfer appliace : B"},{"upvote_count":"1","poster":"mahx","content":"Answer is B , gsutil can support only small transfer sizes (up to 1 TB)","comment_id":"621388","timestamp":"1656032280.0"},{"content":"Selected Answer: B\nSensitive Patient information - hence offline Transfer is recommended. Which is only possible via Transfer Appliance","poster":"SnowLearner","comment_id":"618535","upvote_count":"2","timestamp":"1655616900.0"},{"upvote_count":"2","timestamp":"1652165100.0","comment_id":"599439","content":"Selected Answer: B\nchoose gsutil for less than 1TB: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google, therefore B","poster":"mn_1311"},{"timestamp":"1647449580.0","upvote_count":"2","comment_id":"569178","content":"The answer is B.","poster":"rafaelgildin"},{"content":"Selected Answer: B\nYou are transferring sensitive patient information, so C & D are ruled out. Choice comes down to A & B. Here it gets tricky. How to choose Transfer Appliance: (https://cloud.google.com/transfer-appliance/docs/2.0/overview)\nWithout knowing the bandwidth, it is not possible to determine whether the upload can be completed within 7 days, as recommended by Google. So the safest and most performant way is to use Transfer Appliance.","timestamp":"1646742720.0","upvote_count":"2","comment_id":"563229","poster":"Arkon88"},{"comments":[{"timestamp":"1649347800.0","content":"Transfer Appliance is an excellent option, especially when a fast network connection is unavailable and it's too costly to acquire more bandwidth. Check the link above.","upvote_count":"1","comment_id":"582527","poster":"tavva_prudhvi"},{"comment_id":"582536","poster":"tavva_prudhvi","timestamp":"1649348640.0","content":"And as the BW is not given you cannot have your own assumptions to get the answer as A, gsutil has limitations upto 1TB, Hence we can choose B.","upvote_count":"1"}],"timestamp":"1644916560.0","poster":"Jambalaja","upvote_count":"2","content":"Selected Answer: A\nSecure and **time-efficient**. Transfer appliance takes too long. Public URL is ruled out because insecure. Finally, A is left over. Question does not state whether SQL database is on prem or on cloud. A suits best.","comment_id":"547621"},{"upvote_count":"1","poster":"ankit789adad","content":"I agree choice is between A and B. However Transfer Appliance involves shipping the appliance to the Org , loading the data and shipping it back to Google hence i would go with A","timestamp":"1643859840.0","comment_id":"539379"},{"timestamp":"1642344720.0","comment_id":"525013","upvote_count":"1","content":"Selected Answer: A\nNeed to be quick and safe.","poster":"Bhawantha"},{"comment_id":"524943","timestamp":"1642337640.0","upvote_count":"2","poster":"lord_ryder","content":"Selected Answer: B\nSensitive data so C and D (publi url) are excluded\nAnswer is B: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options"},{"timestamp":"1642155060.0","upvote_count":"3","comment_id":"523474","poster":"mananksh","content":"Selected Answer: A\nthe location of the RDBMS is not specified hence cant surely say B. Therefore I ll vote for A."},{"comment_id":"522698","poster":"annie1196","content":"A : Because nowhere it is mentioned that relational database is on cloud or on premise. As transfer appliance can be used for loading from on premise only, so B is ruled out.","timestamp":"1642059900.0","upvote_count":"4"},{"content":"Selected Answer: A\nC and D no since it is sensitive data so making it publicly available doesn't make sense. B - Transfer Appliance setup takes up to 2 weeks. Only A makes sense.","poster":"medeis_jar","comment_id":"518499","upvote_count":"2","timestamp":"1641496260.0"},{"comments":[{"comment_id":"543808","poster":"Novice1308","timestamp":"1644415620.0","upvote_count":"3","content":"Document says, if it is equal to or greater than 10TB. Use transfer-appliance. With that in mind, answer should be B"}],"comment_id":"513464","upvote_count":"2","poster":"MaxNRG","timestamp":"1640876340.0","content":"Selected Answer: A\nPublic url is not secure, we can exclude D and C.\nNow, between A and B depends on the clients connectivity. Normally A should be the best option if we are not in a remote-badly-connected factory.\nTransfer Appliance is bad option if data size is less than 100 TB because it requires hardware installation. check this\nhttps://cloud.google.com/transfer-appliance/"},{"timestamp":"1639981920.0","comment_id":"505233","poster":"kishanu","upvote_count":"1","content":"A and B. Help me I am in dilemma :D\nA - Correct, gsutil has a limitation of 1TB, but is it time-efficient?\nB - Correct, but the Transfer appliance would be at least a week to provide the data in Google cloud"},{"upvote_count":"4","timestamp":"1639873380.0","content":"Selected Answer: B\nB is the best choice for the options given.\n\nC&D are directly discarded because the public URL makes them not safe.\n\nFor those talking about the limit of 5TB with gsutil... this limit is for objects on GCS, so it also applies to Transfer Appliance. In both cases, the total of 10TB must be compressed/cut. (See https://cloud.google.com/storage/quotas#objects AND https://cloud.google.com/transfer-appliance/docs/4.0/known-limitations#object-limit )\n\nBut gsutil is recommended UP to 1TB, and Transfer Appliance FROM 10TB. So with the information in the question, B is the only option possible.","poster":"hendrixlives","comment_id":"504532"},{"content":"B. \nNo C,D, as they are using public urls, not safe for sensitive data. \nSaw a lot of As here. It says here to use gsutil when it's <1TB data. Some argue that Avro filed could be compressed to around 1TB. But it's not for sure. Based on the information we have, B is the best choice.","comments":[{"upvote_count":"1","comment_id":"474993","timestamp":"1636480920.0","poster":"JayZeeLee","content":"https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options"}],"poster":"JayZeeLee","upvote_count":"1","comment_id":"474992","timestamp":"1636480860.0"},{"upvote_count":"2","poster":"shroffshivangi","timestamp":"1633982760.0","content":"Considering these limitations on transfer appliance according to the doc here: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options \nand the question asking to be 'time-sensitive' \n\nWith this option, consider the following:\n\nTransfer Appliance requires that you're able to receive and ship back the Google-owned hardware.\nDepending on your internet connection, the latency for transferring data into Google Cloud is typically higher with Transfer Appliance than online.\nTransfer Appliance is available only in certain countries.\nThe two main criteria to consider with Transfer Appliance are cost and speed. With reasonable network connectivity (for example, 1 Gbps), transferring 100 TB of data online takes over 10 days to complete. If this rate is acceptable, an online transfer is likely a good solution for your needs.\n\n\nMy answer would be A.","comment_id":"460785"},{"upvote_count":"1","timestamp":"1633446660.0","comment_id":"457774","content":"A: valid up to 1 TB\nB: Transfer Appliance --> It would take more than one week to upload your data over the network, it doesn't seeems a time efficient solution.\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability\nC: CSV file is not the most suitable format for loading on BQ\nD: Avro is the preferred format for loading data on BQ, even if the url is public Transfer service for on-premises data encrypts your data over an HTTPS session with TLS for the public internet \nhttps://cloud.google.com/storage-transfer/docs/on-prem-security#in-flight\nSo for me the correct answer is D","poster":"sergio6"},{"comment_id":"454949","timestamp":"1633009620.0","poster":"Chelseajcole","upvote_count":"2","content":"Vote for A on the assumption that Avro compression ratio is above 90%. So the key is 10 TB. Let's say 10 TB can be compressed to less than 1TB and we should go with gsutil. Otherwise, it leaves you with no option but B. Think about Transfer Appliance really need to take physical transportation time and cost. \n\nSo the 10TB size and Avro format leads me decide answer should A"},{"comments":[{"timestamp":"1625850720.0","upvote_count":"2","content":"Sorry, should be \"B\". Even though Transfer Appliance is limited to some countries, transfers using gsutil are limited up to 5TB per file and alternative \"A\" states a single file, that is, singular.","comment_id":"402857","poster":"gaco"}],"comment_id":"402854","poster":"gaco","upvote_count":"4","content":"People are getting confused between Transfer Service and Transfer Appliance. The last one is basically a truck to transport your data to the cloud. It is only available in some countries.\nAnswer is A.","timestamp":"1625850420.0"},{"comment_id":"399413","upvote_count":"1","timestamp":"1625514300.0","poster":"awssp12345","content":"as per https://cloud.google.com/storage-transfer/docs/overview#gsutil \nTransferring more than 1 TB from another Cloud Storage region\nUse Storage Transfer Service.\n\nSo B makes sense.","comments":[{"timestamp":"1679223840.0","poster":"juliobs","comment_id":"843662","content":"Transfer *Service* is not the same as Transfer *appliance*.","upvote_count":"1"}]},{"timestamp":"1625215380.0","upvote_count":"2","content":"Vote for 'B'\nhttps://cloud.google.com/storage-transfer/docs/overview#gsutil\nTransferring more than 1 TB from on-premises Use Transfer service for on-premises data.","comment_id":"396712","poster":"sumanshu"},{"comment_id":"384182","comments":[{"timestamp":"1649348280.0","content":"Dont make such assumptions just go with the given questions, its simply B","comment_id":"582533","poster":"tavva_prudhvi","upvote_count":"1"}],"content":"A. \nWhy: 10TB is the size of the DataBase, but AVRO rate compression is near 90%, so the resulting file will be <5TB and can easily be uploaded with gsutil.","timestamp":"1623932880.0","upvote_count":"2","poster":"sblivia"},{"upvote_count":"1","content":"B:\nhttps://cloud.google.com/storage-transfer/docs/overview#gsutil\nmaximum size of gsutil is 1 TB, then we will go with B.","timestamp":"1615508940.0","poster":"daghayeghi","comment_id":"308450"},{"poster":"funtoosh","comment_id":"294625","content":"Answer B; Very similar to a few other questions on Linux Academy, GSUTIL won't work for 10 TB for data. Transfer Appliance service is secure and reliable .. just like AWS Snowball","upvote_count":"2","timestamp":"1613771220.0"},{"poster":"daghayeghi","comment_id":"292903","content":"B:\nMax size of gsutil is 5TB, then we have to use Transfer Appliance.","timestamp":"1613596800.0","upvote_count":"1"},{"poster":"[Removed]","comment_id":"278967","upvote_count":"1","timestamp":"1611888420.0","content":"Guys..don't put answers simply as A,B,C,D\nWithout your explanation.\n\nEg: for this question if you simply put answers is A(wrong one), what is the use.its mislead the readers"},{"poster":"AnilKr","content":">1TB <20TB use gsutil, Use Transfer Applicance for >=20TB","comment_id":"266007","upvote_count":"2","timestamp":"1610511360.0"},{"timestamp":"1609871880.0","upvote_count":"1","comment_id":"260483","poster":"bobby8521","content":"Should be A\nby transfering through gsutil, GCP provides this serivces as a secure communication and its easy to setup when compared to other options."},{"upvote_count":"2","content":"gsutil uses Google's public endpoint to transfer the data (unless we use VPN) - so goes through public internet. I feel in this case transfer appliance will be safe. I am picking B","comment_id":"222299","poster":"snamburi3","timestamp":"1605738420.0"},{"content":"Appliance will come from Netherland then we need to send to Belgium..time constraint also transportation physicaly not secure less than 20tb appliance is not optimal..so option A is ok.","upvote_count":"1","timestamp":"1605622020.0","poster":"kavs","comment_id":"221122","comments":[{"poster":"tavva_prudhvi","timestamp":"1649348160.0","content":"Dont make you own assumptions about the bandwidth, as its unknown. Go with the most optimal one.","upvote_count":"1","comment_id":"582532"}]},{"timestamp":"1605173160.0","comments":[{"comment_id":"421932","content":"you can't load to bigquery directly from transfer appliance to A is the correct answer","poster":"sandipk91","timestamp":"1628485200.0","upvote_count":"1","comments":[{"poster":"sandipk91","upvote_count":"1","content":"my bad, so yes option B is the correct answer as gsutil is for data less than 1 TB and storage transfer appliance is the most secure option","timestamp":"1628485680.0","comment_id":"421936"}]}],"content":"Option B (Transfer appliance), reasons as follows:-\na) Option A : Is good from AVRO file format standpoint, that what is most preferred for BQ - however as the question says - 10TB of data is most secure and time efficient manner. Using \"gsutil\" to transfer data of the tune of 10TB without knowing BW and speed transfer is not a preferred option, considering \"gsutil is meant for application based usage and for smaller amount of of data transfer.\nb) Option B : Makes the transfer - secure / fast (considering the fact that BW speed was unknown) and is a good option considering the volume of data (10TB)\nC and D - rules out as public URL does not meet the security requirement in the question.","comment_id":"217803","upvote_count":"2","poster":"Alasmindas"},{"comments":[],"content":"I will go with Option A : \"gsutil\" - reasons are as follows:-\na) Transfer appliance is typically suitable for very large data transfer, typically from hundreds of terabytes up to 1 petabyte..\nb) Although gsutil has a limitation of maximum size of a single object to be with 5TB but we can have more than one object (in this case - 2 object totaling to 10GB)","comment_id":"215964","upvote_count":"1","poster":"Alasmindas","timestamp":"1604929320.0"},{"upvote_count":"5","content":"Answer is B : https://cloud.google.com/storage-transfer/docs/overview#gsutil . Gsutil is for less than a TB","poster":"Cloud_Enthusiast","comment_id":"215323","timestamp":"1604848200.0"},{"poster":"ksharma161076","upvote_count":"2","comment_id":"212955","content":"https://cloud.google.com/transfer-appliance/docs/2.2/overview\nAns: A. as mentioned in the docs:\nTransfer Appliance is a good fit for your data transfer needs if:\n\n-You are an existing Google Cloud Platform (GCP) customer.\n-Your data size is greater than or equal to 20 TB.\n-Your data resides in locations that Transfer Appliance is available.\n-It would take more than one week to upload your data.","timestamp":"1604515560.0","comments":[{"comment_id":"212957","poster":"ksharma161076","content":"Hence choosing A, because size in question is 10TB, and Transfer Appliance is good fit when data size is >=20TB","upvote_count":"1","timestamp":"1604515740.0"}]},{"poster":"JohnHD","content":"Answer A: Not B...:\nGoogle recommends that enterprises use Transfer Appliance in cases where it would take them over a week to upload data to the cloud via the internet, or when an enterprise needs to migrate over 60 TB of data.","comment_id":"210714","upvote_count":"2","timestamp":"1604261280.0"},{"timestamp":"1601373420.0","content":"Transfer scenario Recommendation\nTransferring from another cloud storage provider Use Storage Transfer Service\nTransferring less than 1 TB from on-premises Use gsutil\nTransferring more than 1 TB from on-premises Use Transfer service for on-premises data\nUse this guidance as a starting point. The specific details of your transfer scenario will also help you determine which tool is more appropriate.\n\nhttps://cloud.google.com/storage-transfer/docs/overview","poster":"TNT87","upvote_count":"2","comment_id":"189586"},{"content":"Answer should be A. The most time efficient would be using gsutil as with 10gbps 10TB will take approx 3 Hrs while with Transfer Appliance it will take atleast a week or so.","upvote_count":"1","poster":"SteelWarrior","comment_id":"184973","timestamp":"1600832400.0"},{"timestamp":"1600768020.0","poster":"girgu","content":"Correct ans : B\nTransfer Appliance is a good fit for your data transfer needs if:\n\nYou are an existing Google Cloud Platform (GCP) customer.\nYour data size is greater than or equal to 10TB.\nYour data resides in locations that Transfer Appliance is available.\nIt would take more than one week to upload your data.","comment_id":"184307","upvote_count":"2"},{"upvote_count":"1","poster":"atnafu2020","comment_id":"177929","timestamp":"1599869400.0","content":"gsutil \nFor multi-threaded transfers, use gsutil -m.\nSeveral files are processed in parallel, increasing your transfer speeds.\nFor a single large file, use Composite transfers.\nSTA-when to use\nBandwidth is available, but cannot be acquired in time to meet your deadline.\nDepending on your internet connection, the latency for transferring data into Google Cloud is typically higher with Transfer Appliance than online."},{"upvote_count":"2","content":"https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#transfer-options\n------------------------------------------------------------------------------------------------------------------------------------------------Your private data center to Google Cloud \nEnough bandwidth to meet your project deadline for less than a few TB of data \n・gsutil\nYour private data center to Google Cloud\nEnough bandwidth to meet your project deadline for more than a few TB of data \n・Storage Transfer Service for on-premises data\n------------------------------------------------------------------------------------------------------------------------------------------------For example, if the problem statement says that the files are compressed and parallel..., then I think it's A. But the problem just says \"secure and time-efficient\".\nTherefore, I think B is the correct answer.","poster":"kino2020","timestamp":"1599727020.0","comment_id":"176988","comments":[{"content":"Correction:\nYour private data center to Google Cloud\nNot enough bandwidth to meet your project deadline\n・Transfer Appliance","poster":"kino2020","timestamp":"1599727260.0","comment_id":"176991","upvote_count":"1"}]},{"upvote_count":"2","timestamp":"1598004960.0","comment_id":"162864","poster":"haroldbenites","content":"B is correct.\nThe appliance is for TB of data.\nGsutil is for GB of data.\nStorage transfer service is used to transfer from others clouds, like AWS for example."},{"comment_id":"157311","upvote_count":"4","content":"Answer B:\nAvro most efficient file type, gsutil makes more sense for on-premise to GCP for file size <1TB, Transfer Appliance is a secure way and with 10TB size lower bandwidths can be an issue for using transfer service","timestamp":"1597322220.0","poster":"FARR"},{"comments":[{"comment_id":"164812","timestamp":"1598235120.0","poster":"dragon123","content":"Avro file is splittable...","upvote_count":"2"}],"upvote_count":"2","content":"The maximum size of one file on GCS is 5TB. \nB is correct","poster":"gvaf4","comment_id":"134891","timestamp":"1594734180.0"},{"content":"Option A & B are very close. Would go with Option A as it makes the transfer even more secure by eliminating any 3rd party intervention.","poster":"dambilwa","comment_id":"119030","timestamp":"1593050520.0","upvote_count":"1"},{"content":"Google recommends that enterprises use Transfer Appliance in cases where it would take them over a week to upload data to the cloud via the internet, or when an enterprise needs to migrate over 60 TB of data. Answer A is correct!","poster":"chele","comment_id":"100074","timestamp":"1591021920.0","upvote_count":"2"},{"upvote_count":"2","poster":"arnabbis4u","comment_id":"88058","comments":[{"timestamp":"1594257240.0","poster":"Rajokkiyam","upvote_count":"3","content":"Its a Sensitive data. Creating a Public URL is not recommended.","comment_id":"130263"},{"comment_id":"222296","upvote_count":"2","poster":"snamburi3","timestamp":"1605738180.0","content":"Transfer service is different from Transfer appliance"}],"timestamp":"1589336160.0","content":"Correct Answer D. Since the data size is more than 1 TB, Google recommends Storage Transfer Service for On-Premise.\n\nhttps://cloud.google.com/storage-transfer/docs/overview#should_you_use_gsutil_or"},{"comment_id":"68763","content":"Answer: A\nDescription: Avro format is compressed and gsutil enables parallel copying","poster":"[Removed]","timestamp":"1585373220.0","upvote_count":"5"},{"timestamp":"1584885480.0","content":"should be A","poster":"[Removed]","upvote_count":"1","comments":[{"comments":[{"poster":"g2000","comment_id":"280779","timestamp":"1612121820.0","content":"A is wrong. max size for GCS is 5TB.\nhttps://cloud.google.com/storage/quotas#objects","upvote_count":"1"}],"comment_id":"67190","content":"Selected - A","timestamp":"1584944280.0","poster":"[Removed]","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"B is not time efficient","timestamp":"1599868680.0","comment_id":"177921","poster":"atnafu2020"},{"upvote_count":"2","poster":"[Removed]","timestamp":"1611888300.0","content":"Your both choices are wrong. \n10TB doesn't required transfer appliance\n10TB can't be used with gsutil","comment_id":"278965"}],"comment_id":"66965","upvote_count":"1","poster":"[Removed]","content":"Could be B as well. bit confusing","timestamp":"1584887760.0"}],"comment_id":"66953"},{"upvote_count":"3","timestamp":"1584855300.0","content":"Answer A : AVRO fastest way to import data to BQ and C.Storage preferable mode to store teh data.","poster":"Rajokkiyam","comment_id":"66812"}],"topic":"1","answer_ET":"B","answer":"B","timestamp":"2020-03-22 06:35:00"},{"id":"tr9qQ0hHMp0QXr5an5Mr","choices":{"B":"Partition the inventory balance table by item to reduce the amount of data scanned with each inventory update.","D":"Use the BigQuery bulk loader to batch load inventory changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.","C":"Use the BigQuery streaming the stream changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.","A":"Leverage BigQuery UPDATE statements to update the inventory balances as they are changing."},"unix_timestamp":1584855420,"url":"https://www.examtopics.com/discussions/google/view/17205-exam-professional-data-engineer-topic-1-question-102/","answer_description":"","timestamp":"2020-03-22 06:37:00","discussion":[{"timestamp":"1640876580.0","comments":[{"comment_id":"1098803","poster":"MaxNRG","comments":[{"comment_id":"1098804","poster":"MaxNRG","upvote_count":"3","comments":[{"upvote_count":"1","content":"Here's why the other options are less suitable:\n\nA. Leverage BigQuery UPDATE statements: While technically possible, this approach is inefficient for frequent updates as it requires individual record scans and updates, affecting performance and potentially causing data race conditions.\n\nB. Partition the inventory balance table: Partitioning helps with query performance for large datasets, but it doesn't address the need for near real-time updates.\n\nD. Use the BigQuery bulk loader: Bulk loading daily changes is helpful for historical data ingestion, but it won't provide near real-time updates necessary for the dashboard.","comment_id":"1098809","timestamp":"1702809960.0","comments":[{"content":"Option C offers the following advantages:\n\nStreams inventory changes near real-time: BigQuery streaming ingests data immediately, keeping the inventory movement table constantly updated.\nDaily balance calculation: Joining the movement table with the historical balance table provides an accurate view of current inventory levels without affecting the actual balance table.\nNightly update for historical data: Updating the main inventory balance table nightly ensures long-term data consistency while maintaining near real-time insights through the view.\nThis approach balances near real-time updates with efficiency and data accuracy, making it the optimal solution for the given scenario.","upvote_count":"1","poster":"MaxNRG","comment_id":"1098810","timestamp":"1702809960.0"}],"poster":"MaxNRG"}],"content":"The key reasons this is better than the other options:\n\nUsing BigQuery UPDATE statements (option A) would be very inefficient for thousands of updates per hour. It is better to batch updates.\nPartitioning the inventory balance table (option B) helps query performance, but does not solve the need to incrementally update balances.\nUsing the bulk loader (option D) would require batch loading the updates, which adds latency. Streaming inserts updates with lower latency.\nSo option C provides a scalable architecture that streams updates with low latency while batch updating the balances only once per day for efficiency. This balances performance and accuracy needs.","timestamp":"1702809540.0"}],"upvote_count":"4","content":"C is better\n\nThe best approach is to use BigQuery streaming to stream the inventory changes into a daily inventory movement table. Then calculate balances in a view that joins the inventory movement table to the historical inventory balance table. Finally, update the inventory balance table nightly (option C).","timestamp":"1702809540.0"},{"timestamp":"1689324780.0","comment_id":"951383","content":"There are still limitations on DML statements (2023) e.g. only 2 concurrent UPDATES and up to 20 queued hence not appropriate for this scenario:\nhttps://cloud.google.com/bigquery/quotas#data-manipulation-language-statements","poster":"Yiouk","upvote_count":"2","comments":[{"content":"option A:what limitation here 1500/perday okay in question we will get max 24 jobs hourly updated okay,\nnow speed 5 operation /10 sec , 1 operation 2sec , and we are getting new update in 1 hour so we have time 3600 sec and we need to update around 1000 update according to speed take 2000sec still we have 1600 sec rest to getting new update so .\nthats why I thing DML is best option for this work","comments":[{"content":"In question it mentioned several thousands of updates every hour, several thousands could be 20-30 thousands as well. Where it is mentioned for only 1000 updates?","poster":"Nandababy","comment_id":"1098078","upvote_count":"1","timestamp":"1702723080.0"}],"poster":"NeoNitin","timestamp":"1689635520.0","comment_id":"954690","upvote_count":"2"}]}],"poster":"MaxNRG","content":"Selected Answer: A\nA - New correct answer\nC - Old correct answer (for 2019)","upvote_count":"33","comment_id":"513466"},{"content":"C is correct.\nIt says “update Every hour”\nAnd need “ accuracy”","upvote_count":"25","poster":"haroldbenites","comments":[],"timestamp":"1598005440.0","comment_id":"162867"},{"upvote_count":"1","comment_id":"1302150","content":"Selected Answer: C\nBigQuery is not optimized for updating statement. So, go to C.","timestamp":"1729707780.0","poster":"SamuelTsch"},{"timestamp":"1721541600.0","poster":"edre","comment_id":"1252251","content":"Selected Answer: C\nThe answer is C because the requirement is near real-time","upvote_count":"1"},{"content":"Selected Answer: C\nThe best approach is to use BigQuery streaming to stream the inventory changes into a daily inventory movement table. Then calculate balances in a view that joins the inventory movement table to the historical inventory balance table. Finally, update the inventory balance table nightly (option C).","comment_id":"1098806","poster":"MaxNRG","comments":[{"comment_id":"1098807","content":"The key reasons this is better than the other options:\n\nUsing BigQuery UPDATE statements (option A) would be very inefficient for thousands of updates per hour. It is better to batch updates.\n\nPartitioning the inventory balance table (option B) helps query performance, but does not solve the need to incrementally update balances.\n\nUsing the bulk loader (option D) would require batch loading the updates, which adds latency. Streaming inserts updates with lower latency.\n\nSo option C provides a scalable architecture that streams updates with low latency while batch updating the balances only once per day for efficiency. This balances performance and accuracy needs.","upvote_count":"1","timestamp":"1702809600.0","poster":"MaxNRG"}],"upvote_count":"1","timestamp":"1702809600.0"},{"upvote_count":"3","timestamp":"1701841800.0","content":"Selected Answer: C\nOption C.\n\nUsing the BigQuery streaming to stream changes into a daily inventory movement table and calculating balances in a view that joins it to the historical inventory balance table can help you achieve the desired performance and accuracy. You can then update the inventory balance table nightly. This approach can help you avoid the overhead of scanning large amounts of data with each inventory update, which can be time-consuming and resource-intensive.\nLeveraging BigQuery UPDATE statements to update the inventory balances as they are changing (option A) can be resource-intensive and may not be the most efficient way to achieve the desired performance.","poster":"rocky48","comment_id":"1089027"},{"comment_id":"1079028","upvote_count":"1","timestamp":"1700800740.0","poster":"AnonymousPanda","content":"Selected Answer: C\nAs per other answers C"},{"comment_id":"1062919","timestamp":"1699190880.0","poster":"Nirca","upvote_count":"1","content":"Selected Answer: A\nSimple and will work"},{"content":"Selected Answer: C\nAnswer is C. Why because “Update” limits is 1500/per day, and the question say: You have several thousand updates to inventory every hour. So is impossible to use updates all the time.","poster":"odacir","timestamp":"1697810820.0","upvote_count":"2","comment_id":"1048849"},{"content":"Selected Answer: A\nA. Leverage BigQuery UPDATE statements to update the inventory balances as they are changing - is so simple and RIGHT!","comment_id":"1026368","poster":"Nirca","upvote_count":"1","timestamp":"1696581240.0"},{"upvote_count":"2","comment_id":"1009157","poster":"brookpetit","timestamp":"1694874600.0","content":"Selected Answer: C\nC is more universal and sustainable"},{"comment_id":"948036","content":"Selected Answer: C\nUPDATE is too expensive. Joining main and delta tables is the right wat to capture data change.","timestamp":"1688988420.0","upvote_count":"3","poster":"ZZHZZH"},{"poster":"euro202","upvote_count":"2","timestamp":"1688739000.0","content":"Selected Answer: C\nI think the answer is C. The question is about maximizing performance and accuracy, it's ok if we need expensive JOINs. BigQuery has a daily quota of 1500 UPDATEs, and the question talks about several thousand updates every hour.","comments":[{"upvote_count":"1","timestamp":"1695792360.0","poster":"jackdbd","comment_id":"1018473","content":"DML statements do not count toward the number of table modifications per day.\nhttps://cloud.google.com/bigquery/quotas#data-manipulation-language-statements\n\nSo I would go with A.","comments":[{"upvote_count":"1","comment_id":"1018476","poster":"jackdbd","timestamp":"1695792480.0","content":"Sorry, wrong link. Here is the correct one: https://cloud.google.com/bigquery/quotas#standard_tables"}]}],"comment_id":"945758"},{"comment_id":"941708","timestamp":"1688379540.0","upvote_count":"1","poster":"vaga1","content":"Selected Answer: A\nC create a view that joins to a table seems dumb to me"},{"poster":"forepick","content":"Selected Answer: C\nToo frequent updates are way too expensive in an OLAP solution. This is much more likely to stream changes to the table(s) and aggregate these changes in the view.\n\nhttps://stackoverflow.com/questions/74657435/bigquery-frequent-updates-to-a-record","upvote_count":"3","timestamp":"1685539140.0","comment_id":"911318"},{"poster":"streeeber","upvote_count":"1","timestamp":"1681016160.0","comment_id":"865286","content":"Selected Answer: C\nHas to be C. \nDML has hard limit of 1500 operations per table per day: https://cloud.google.com/bigquery/quotas#standard_tables"},{"comments":[{"upvote_count":"1","content":"option A:what limitation here 1500/perday okay in question we will get max 24 jobs hourly updated okay,\nnow speed 5 operation /10 sec , 1 operation 2sec , and we are getting new update in 1 hour so we have time 3600 sec and we need to update around 1000 update according to speed take 2000sec still we have 1600 sec rest to getting new update so .\nthats why I thing DML is best option for this work","timestamp":"1689635580.0","poster":"NeoNitin","comment_id":"954692"}],"poster":"lucaluca1982","comment_id":"850778","upvote_count":"2","content":"Selected Answer: C\nUpdate action is not efficient","timestamp":"1679813160.0"},{"comment_id":"833673","poster":"midgoo","content":"Selected Answer: A\nThis question has 2 parts:\n1. Query the table in real-time\n2. Update the table with thousands of records per hour ~ 10 updates per second\n\nWithout (1), C seems to be the good approach by using staging table to buffer the update using Change Data Capture method. However, that method will make the query expensive due to the JOIN. So A is a better choice here.","upvote_count":"3","timestamp":"1678346100.0"},{"content":"Answer : A\nDon't be confused anymore. \nCurrent case: Data updates in EVERY HOUR, according to that we need to update the table immediately.\nA. Update statement/ query as soon as new data landed, current case is good. If data is frequently updating every second then Option C was the right answer. \nB. Partition table is good to increase query performance but this is not the demand. \nOption D, this is also not batch load condition, and unnecssory work","upvote_count":"1","comment_id":"820622","timestamp":"1677250020.0","poster":"musumusu"},{"poster":"techtitan","content":"Selected Answer: C\nNew inventory table is the realtime inventory updates table. Data has to be streamed in this realtime. The historical inventory balance table can be updated nightly.","upvote_count":"2","comment_id":"808310","timestamp":"1676374140.0"},{"comments":[{"upvote_count":"1","content":"It also says ' near real-time inventory dashboard'... and to have accurate data. That eliminates answer C.","timestamp":"1674159180.0","poster":"desertlotus1211","comment_id":"781572"}],"comment_id":"781571","timestamp":"1674159000.0","upvote_count":"5","content":"Answer is A: https://cloud.google.com/blog/topics/developers-practitioners/inventory-management-bigquery-and-cloud-run/\n\nleverage UPDATE statement .... there is another part to use Cloud Run. The update statement is what's important here. Inventory changes every hour. Nightly inventory updates doesn't help.","poster":"desertlotus1211"},{"poster":"jkhong","content":"Selected Answer: C\nWhat this question is about is actually Change Data Capture. I'm surprised nobody has mentioned this as of yet.. (https://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#before_you_begin)\n\n3 methods of CDC is:\ni. Immediate consistency approach -> view table which joins main table and delta table (expensive if queries are made frequently as join operations need to be executed)\nii. Cost-optimized approach -> merge delta and main periodically, insert / delete from main and delete delta entries in delta table (cheaper queries, but not immediate)\niii. Hybrid approach","timestamp":"1671641040.0","upvote_count":"2","comment_id":"752574","comments":[{"upvote_count":"1","poster":"jkhong","content":"Hybrid approach would be a combination of (i) and (ii).\n\n\nWhat C and D is recommending is essentially the Hybrid approach. A staging table will be maintained (i.e. the data streamed for the daily inventory movement). The immediate consistency approach representing the view and cost-optimized approach representing nightly inventory balance update. I'd pick C because streaming is still important to keep data updated. For anyone who has had experience in processing transactions, A would be a nightmare for managing data warehouses... BigQuery's suggested approach for updates is always append, and is not built for high load UPDATEs","timestamp":"1671641040.0","comment_id":"752575"}]},{"poster":"zellck","comment_id":"735923","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns#dml_statements_that_update_or_insert_single_rows\n\nBest practice: Avoid point-specific DML statements (updating or inserting 1 row at a time). Batch your updates and inserts.\n\nFor example, you could load your set of replacement records into another table, then write the DML statement to update all values in the original table if the non-updated columns match.","timestamp":"1670243640.0","upvote_count":"1"},{"content":"Selected Answer: C\nC streaming is correct. updata has limit","timestamp":"1669818900.0","upvote_count":"1","poster":"hauhau","comment_id":"731601"},{"poster":"maksi","upvote_count":"1","timestamp":"1668166200.0","comment_id":"716009","content":"C is Correct\n\nAccording to Documentation, you can perform 5 UPDATE operations per 10 seconds, so as a result during an hour we will be able to perform only 1800 UPDATE operations. I'm not sure, in the Description it's mentioned that we need to perform SEVERAL thousands operations, not 1.8. https://cloud.google.com/bigquery/quotas#standard_tables"},{"comment_id":"712607","upvote_count":"2","timestamp":"1667763720.0","content":"Selected Answer: C\nC it is","poster":"cloudmon"},{"poster":"NicolasN","content":"Selected Answer: C\nI'm afraid that even the DML statements per day are unlimited nowadays (2022), the answer [C] remains the best option for the near real-time constraint, since updating row-by-row in BigQuery is considered by Google an anti-pattern.\nQuoting from https://cloud.google.com/bigquery/docs/best-practices-performance-patterns#dml_statements_that_update_or_insert_single_rows :\nBest practice: Avoid point-specific DML statements (updating or inserting 1 row at a time). Batch your updates and inserts.\n... BigQuery DML statements are intended for bulk updates. UPDATE and DELETE DML statements in BigQuery are oriented towards periodic rewrites of your data, not single row mutations.\n... If your use case involves frequent single row inserts, consider streaming your data instead.\n\nThrough this prism: [C] (streaming) > [D] (batch) > A [row-by-row]","comment_id":"711887","upvote_count":"9","timestamp":"1667666580.0"},{"timestamp":"1664537460.0","upvote_count":"2","comments":[{"upvote_count":"1","content":"Another link \nStreaming inserts and timestamp-aware queries\nhttps://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery#Streaming%20inserts%20and%20timestamp-aware%20queries","timestamp":"1664538300.0","poster":"John_Pongthorn","comment_id":"683510"}],"poster":"John_Pongthorn","comment_id":"683499","content":"Selected Answer: C\nC \nThis link give reason to support the answer\nhttps://cloud.google.com/bigquery/docs/write-api#overview"},{"timestamp":"1663886340.0","upvote_count":"1","content":"C it is","poster":"clouditis","comment_id":"676597"},{"comment_id":"648490","timestamp":"1660835100.0","upvote_count":"5","poster":"alecuba16","content":"Selected Answer: C\n2022 \nCannot be A there are limits:\n\nTable operations per day 1500 operations \nYour project can make up to 1,500 table operations per table per day, whether the operation appends data to the table or truncates the table. This limit includes the combined total of all load jobs, copy jobs, and query jobs that append to or overwrite a destination table or that use a DML DELETE, INSERT, MERGE, TRUNCATE TABLE, or UPDATE statement to write data to a table.\n\nDML statements count toward partition limits, but aren't limited by them. For more information about DML limits, see Data manipulation language statements.\n\n1.000 updates per hour * 24 = 24000 updates > 1500 daily limit.\n\nhttps://cloud.google.com/bigquery/quotas","comments":[{"content":"I think that you are confusing jobs/operations and DML statement.\nGoogle Cloud do have a quota for the jobs, but not for the DML anymore, so a single operation/job can do more than 1500 updates in 24h, but you can't do more than 1500 jobs/operations in 24h.\nSince there is no DML quotas anymore, the correct answer today is A.","poster":"boulathone","comment_id":"652128","timestamp":"1661502060.0","upvote_count":"2"},{"upvote_count":"2","timestamp":"1666746780.0","poster":"MisuLava","comment_id":"704297","content":"February 11, 2019\nThe following DML quotas have changed:\n\nThe maximum number of combined UPDATE, DELETE, and MERGE statements per day per table has increased from 200 to 1,000.\nThe limit of 10,000 combined UPDATE, DELETE, and MERGE statements per day per project has been removed. There is no longer a project-level limit on DML statements."}]},{"content":"\"ensure that the data is accurate\"\nMeans leverage update statement to update as they are changing.\n\nAnswer A.","comment_id":"633916","timestamp":"1658301900.0","poster":"DataEngineer_WideOps","upvote_count":"1"},{"comments":[{"comments":[{"comment_id":"648488","content":"You cannot do A because it is not a recommended practise:\n\nhttps://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery","upvote_count":"4","poster":"alecuba16","timestamp":"1660834920.0"},{"content":"Provide a link that justifies your answer, bigquery states to avoid frequent updates directly on BQ, for continuous insertion states the usage of steaming insert and insertall methods.","poster":"alecuba16","timestamp":"1660834860.0","comment_id":"648487","upvote_count":"2"}],"poster":"[Removed]","comment_id":"637659","upvote_count":"1","content":"it can be A. the data doesn't have rows that are streamed in using tabledata.insertall\n\nit's historical data that sits in BQ and is updated, not streamed.","timestamp":"1658875800.0"}],"poster":"alecuba16","content":"Selected Answer: C\nCannot be A:\n\nRows that were written to a table recently by using streaming (the tabledata.insertall method or the Storage Write API) cannot be modified with UPDATE, DELETE, or MERGE statements. The recent writes are those that occur within the last 30 minutes. All other rows in the table remain modifiable by using UPDATE, DELETE, or MERGE statements. The streamed data can take up to 90 minutes to become available for copy operations.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language","upvote_count":"4","timestamp":"1656936300.0","comment_id":"626977"},{"comment_id":"621274","timestamp":"1656015300.0","comments":[{"poster":"alecuba16","timestamp":"1660834980.0","upvote_count":"1","content":"Table operations per day 1500 operations \nYour project can make up to 1,500 table operations per table per day, whether the operation appends data to the table or truncates the table. This limit includes the combined total of all load jobs, copy jobs, and query jobs that append to or overwrite a destination table or that use a DML DELETE, INSERT, MERGE, TRUNCATE TABLE, or UPDATE statement to write data to a table.\n\nDML statements count toward partition limits, but aren't limited by them. For more information about DML limits, see Data manipulation language statements.","comment_id":"648489"}],"poster":"cualquiernick","upvote_count":"2","content":"Ans is A, as Bq quota for updates in BQ does not longer have quota limits"},{"upvote_count":"3","content":"I think even though we dont have DML limits now, based on the current document we can go with C.\nhttps://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery","timestamp":"1651766940.0","comment_id":"597381","poster":"tavva_prudhvi"},{"comment_id":"592794","content":"Selected Answer: C\nI'm going for C.\nSure, A works. But leveraging \"UPDATE\" is not efficient in BQ.","poster":"morgan62","comments":[{"timestamp":"1651220040.0","poster":"tavva_prudhvi","content":"We just need to ensure the data is accurate as per the question with the highest performance, they never talked about efficient process!","comment_id":"594313","upvote_count":"1"}],"timestamp":"1651031040.0","upvote_count":"2"},{"poster":"devric","timestamp":"1649270820.0","upvote_count":"5","content":"Selected Answer: C\nOption A is incorrect for the update limits \"Your project can update a table function up to five times every 10 seconds.\" (1800 times per hour) - https://cloud.google.com/bigquery/quotas\n\nThe right option is C","comment_id":"581979"},{"content":"Selected Answer: A\nDML (Data Manupulation Language) is not limited for BigQuery anymore (like it was in the past). You can simply update your table.","comments":[{"poster":"alecuba16","comment_id":"648491","timestamp":"1660835220.0","upvote_count":"2","content":"It is indeed limited:\n\nTable operations per day 1500 operations\nYour project can make up to 1,500 table operations per table per day, whether the operation appends data to the table or truncates the table. This limit includes the combined total of all load jobs, copy jobs, and query jobs that append to or overwrite a destination table or that use a DML DELETE, INSERT, MERGE, TRUNCATE TABLE, or UPDATE statement to write data to a table.\n\nhttps://cloud.google.com/bigquery/quotas"}],"upvote_count":"1","poster":"Jambalaja","comment_id":"547626","timestamp":"1644916980.0"},{"comments":[],"content":"Selected Answer: A\nA\nDML without limits: https://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","comment_id":"506239","upvote_count":"8","timestamp":"1640102100.0","poster":"kishanu"},{"timestamp":"1639873860.0","comment_id":"504536","upvote_count":"5","poster":"hendrixlives","content":"Selected Answer: A\nCurrently the best option is A, since DML is not limited anymore as some others are saying."},{"timestamp":"1628888940.0","comment_id":"424534","content":"answer is C. \nA: limitation of 1500 dml\nB: Partitioning by item not supported\nC: Streaming for near real time\nD: Batch job will not be near real time. Near real time threshold is different for different use case. but a typical near real time is 5-10 secs. Batch job will invalidate this.","upvote_count":"5","comments":[{"poster":"17isprime","comment_id":"476423","upvote_count":"7","timestamp":"1636657080.0","content":"There is no dml limitation anymore, so A is also OK\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery"}],"poster":"safiyu"},{"poster":"raf2121","content":"I will go with C\nA : Not correct, due to constraint of 1500 updates / table / day\nB : Not correct. Same as A, constraint of number of updates to BQ table / day\nC : Streamed data is available for real-time analysis within a few seconds of the first streaming insertion into a table (in the table \"Daily Inventory Movement Table). For the near real-time dashboard , balances are calculated in a view that joins \"Daily Inventory Movement Table \" and \"Historical Inventory Balance Table\" . Night process updates Historical Inventory Balance Table\nD : Not Correct, as it will meet the requirement of near real time","upvote_count":"3","comment_id":"402955","comments":[{"content":"There is no dml limitation anymore, so A is also OK\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","comment_id":"476424","poster":"17isprime","timestamp":"1636657140.0","upvote_count":"3"}],"timestamp":"1625863740.0"},{"comment_id":"396718","upvote_count":"4","timestamp":"1625215980.0","content":"Vote for C","poster":"sumanshu"},{"poster":"daghayeghi","timestamp":"1615510500.0","content":"B:\nin this question asked us to maximize performance, means improvement not only way of doing then B is best choice, because if we use any of method we need partitioning.\n\nhttps://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery","comment_id":"308466","comments":[{"content":"But you can't partition by item. \n\nYou can partition BigQuery tables by:\n\nTime-unit column: Tables are partitioned based on a TIMESTAMP, DATE, or DATETIME column in the table.\nIngestion time: Tables are partitioned based on the timestamp when BigQuery ingests the data.\nInteger range: Tables are partitioned based on an integer column.","comment_id":"421113","comments":[{"poster":"Ral17","upvote_count":"1","content":"Why do you think we can't partition the table by item?","comments":[{"content":"To be able to partition by item, the \"item\" column should be of type integer... and it MAY be, but you should be careful with your assumptions in the exam. We do not know any detail about the identifier for the items (maybe is an integer, maybe is a string...)","comment_id":"504537","timestamp":"1639873980.0","poster":"hendrixlives","upvote_count":"3"}],"comment_id":"440590","timestamp":"1630967160.0"}],"upvote_count":"1","timestamp":"1628327400.0","poster":"tainangao"}],"upvote_count":"1"},{"timestamp":"1614691560.0","content":"B. C & D updated nightly are not far from 'near real-time'","poster":"gabby","comment_id":"302005","upvote_count":"2"},{"timestamp":"1609343040.0","comments":[{"upvote_count":"2","comment_id":"294628","content":"Answer : B","timestamp":"1613771460.0","poster":"funtoosh"}],"poster":"apnu","upvote_count":"2","content":"C. I think it is C, because in A , it says thousands of update per hour, while Big query support only 1500 update per table per day.","comment_id":"255713"},{"poster":"snamburi3","content":"It is C. \nrefer: https://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery","timestamp":"1605737580.0","upvote_count":"2","comment_id":"222292"},{"poster":"Surjit24","upvote_count":"2","timestamp":"1603946100.0","content":"The A best choice other options like partition is only useful when used with right indexes else it will be overhead.","comment_id":"208262"},{"upvote_count":"2","comment_id":"180937","poster":"Diqtator","content":"Not sure how you mean when you say C is correct. BQ itself can't stream anything(?)\n\nI'd go with B","comments":[{"poster":"Diqtator","upvote_count":"2","comment_id":"185550","content":"There is also nothing called BigQuery streaming or BigQuery bulk loader. C&D talks about having a pipeline which is not mentioned in the question. \n\nRealistically you can't use UPDATE for every change that is made over many different tables as the question suggests.\n\nOnly leaves B even though I'm not sure how it serves the purpose of keeping the data up to date.","timestamp":"1600882500.0","comments":[{"timestamp":"1601114400.0","upvote_count":"1","poster":"zxing233","comment_id":"187584","content":"https://cloud.google.com/bigquery/docs/updating-data"},{"upvote_count":"1","poster":"TeaWhyTie","content":"https://cloud.google.com/bigquery/streaming-data-into-bigquery","comment_id":"216336","timestamp":"1604976720.0"}]}],"timestamp":"1600347000.0"},{"poster":"Tanmoyk","upvote_count":"3","comment_id":"176224","comments":[{"comment_id":"176229","poster":"Tanmoyk","timestamp":"1599619380.0","content":"Sorry should be C","upvote_count":"2"}],"content":"I think A is the correct answer as it's need near-real time updates on the inventory , rest all other options are batch and run over night, which cannot produce near-real time inventory details , Option B is no use as it makes no sense to create partition bases on balance.","timestamp":"1599619200.0"},{"timestamp":"1599336900.0","poster":"lawman","upvote_count":"2","comment_id":"174154","content":"From https://cloud.google.com/bigquery/quotas\nMaximum number of partition modifications per column partitioned table — 30,000\n\nSeems like B is correct. However, in the extreme case that if all \"several thousand updates to inventory every hour\" are made to the same item, the 300,000 limit might still be reached.","comments":[{"poster":"kavs","content":"B seems to be right c and d ruled out as night only it is updating..A may be wrong as several thousand of update every hour may exceed 1500 update per table per day. Still confused...","comments":[{"timestamp":"1605659160.0","poster":"kavs","upvote_count":"1","content":"C every one hr update is sufficient so streaming is not required..batch with one hr to store daily movement is ok so every night historic data can b updated ...D looks ok too...","comment_id":"221487"}],"timestamp":"1605625140.0","comment_id":"221154","upvote_count":"2"}]},{"upvote_count":"12","comment_id":"119209","comments":[{"content":"But since it update nightly, even steam ingest data to bigquery, it can't refect the change right away to the dashboard. only increasing cost... I go for D","upvote_count":"2","timestamp":"1601969700.0","comments":[{"timestamp":"1632036060.0","upvote_count":"2","poster":"yoshik","comment_id":"447462","content":"only inventory balance table (historical data) is updated nightly, inventory movement table is updated in real time"},{"poster":"squishy_fishy","comment_id":"463767","timestamp":"1634514000.0","content":"The answer is C. I was thinking D at first, but imagining that Amazon only gives your the inventory updates every hour, it is not possible, so we need real time inventory updates, not every hour.","upvote_count":"1"}],"poster":"zxing233","comment_id":"194090"}],"poster":"jalk","content":"Since BQ doesn't allow more than 1500 updates pr table pr day, A and B are out. Bulk loading (D) is also out since batching the inserts goes against the real time requirement. But C with the more complex querying is at odds with the maximize dashboard performance","timestamp":"1593071340.0"},{"upvote_count":"3","poster":"dambilwa","comment_id":"119029","timestamp":"1593050220.0","content":"Option [C] is most appropriate as streaming would facilitate 'near real time dashboard'. Also, you can partition a table on integers with a range. Wonder how would that work in a streaming scenario"},{"poster":"sh2020","content":"Is either C or Answer D: There is no need to do streaming. Answer is bulk upload. So I am choosing answer D","upvote_count":"3","timestamp":"1592573040.0","comment_id":"113986"},{"comment_id":"113743","upvote_count":"2","timestamp":"1592550780.0","content":"Its B - partitioning will keep query times down and therefore maximise the performance of the dashboard","poster":"Callumr"},{"comment_id":"76744","content":"C.\nThe choices are between B & C. Since B deals with Historical Inventory data, and the dashboard is running off the main inventory table, I am choosing C.","timestamp":"1587358800.0","upvote_count":"1","poster":"Ganshank"},{"upvote_count":"2","comments":[{"poster":"[Removed]","timestamp":"1584944280.0","upvote_count":"1","content":"Selected - C","comment_id":"67191"}],"poster":"[Removed]","content":"not sure of the answer.\nCould be B / C","timestamp":"1584888180.0","comment_id":"66968"},{"content":"why not B?","poster":"Rajokkiyam","upvote_count":"4","comment_id":"66813","timestamp":"1584855420.0"}],"question_text":"You need to create a near real-time inventory dashboard that reads the main inventory tables in your BigQuery data warehouse. Historical inventory data is stored as inventory balances by item and location. You have several thousand updates to inventory every hour. You want to maximize performance of the dashboard and ensure that the data is accurate. What should you do?","isMC":true,"topic":"1","question_images":[],"answer_images":[],"answer_ET":"C","question_id":5,"answers_community":["C (52%)","A (48%)"],"answer":"C","exam_id":11}],"exam":{"numberOfQuestions":319,"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":true,"provider":"Google","id":11,"name":"Professional Data Engineer","isBeta":false},"currentPage":1},"__N_SSP":true}