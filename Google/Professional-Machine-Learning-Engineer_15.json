{"pageProps":{"questions":[{"id":"tDru6RAXL9iCmfoXtNBv","answer":"B","answer_description":"","discussion":[{"upvote_count":"1","timestamp":"1741224420.0","comment_id":"1365657","content":"Selected Answer: A\nScikit-learn generally relies on CPU-based computations and does not natively leverage GPUs for most algorithms.\n\nAnswer A is the best first step to improve training time without sacrificing model performance","poster":"desertlotus1211"},{"upvote_count":"1","content":"Selected Answer: B\nMinimal changes – You can quickly migrate your existing scikit-learn code to Vertex AI Training using CPU instances.\n✅ Vertex AI prebuilt containers already support scikit-learn with CPU (no extra setup needed).\n✅ Lower cost than distributed training or switching to another framework.\n✅ Good for establishing a baseline – Once you see how long it takes on Vertex AI, you can decide if further optimization (like distributed training) is needed.","timestamp":"1738828860.0","comment_id":"1352251","poster":"vini123"},{"comment_id":"1323578","timestamp":"1733668620.0","poster":"lunalongo","upvote_count":"1","content":"Selected Answer: B\nB) The statement asks the FIRST STEP to take. Considering:\n\n- Scikit-learn's limited and non-universal GPU support\n- Higher cost associated with GPU instances\n\nThe first sensible approach would indeed be to first migrate the model to Vertex AI using CPUs to establish a baseline training time. \n\nThis allows for a direct comparison with the existing training setup and helps determine if the improvement from CPU to GPU is necessary."},{"timestamp":"1733253120.0","poster":"rajshiv","upvote_count":"1","content":"Selected Answer: D\nI think it is D. The optimal approach to improve training time in Vertex AI Training is to leverage the parallel processing power of GPUs.","comment_id":"1321510"},{"comment_id":"1240621","content":"Selected Answer: B\nScikit-learn is not intended to be used as a deep-learning framework and it does not provide any GPU support. (Ref: https://stackoverflow.com/questions/41567895/will-scikit-learn-utilize-gpu).\nSo I go with B","poster":"TanTran04","timestamp":"1719906180.0","upvote_count":"2"},{"comment_id":"1234762","upvote_count":"1","content":"You decided to migrate to Vertex AI, If you have a model that requires significant computational resources and doesn't rely heavily on specialized GPU operations (like those in option D), then option B might still be a good choice. However, if your model is computationally intensive or involves complex neural network architectures I would go with D instead of B.","timestamp":"1718994000.0","poster":"AzureDP900"},{"content":"B is correct, because scikit only has CPU support for the following services: \n- prebuilt containers for custom training (this is the case here)\n- prebuilt containers for predictions and explanations \n- Vertex AI Pipelines \n- Vertex AI Workbench user-managed notebooks\nhttps://cloud.google.com/vertex-ai/docs/supported-frameworks-list#scikit-learn_2","comment_id":"1203114","timestamp":"1714221900.0","poster":"AnnaR","upvote_count":"4"},{"content":"Selected Answer: B\nscikit-learn no GPU support.","timestamp":"1709120040.0","upvote_count":"1","poster":"Carlose2108","comment_id":"1161564"},{"content":"Selected Answer: D\nScikit-learn doesn't natively support GPUs for training. However, many scikit-learn algorithms rely on libraries like NumPy and SciPy. These libraries can leverage GPUs if they're available on the system, potentially benefiting scikit-learn models indirectly.","comment_id":"1142371","timestamp":"1707237480.0","upvote_count":"1","poster":"guilhermebutzke"},{"content":"Selected Answer: B\nSK-Learn offers no GPU support. Answer is B!","comment_id":"1115041","timestamp":"1704528300.0","upvote_count":"3","poster":"b1a8fae"},{"timestamp":"1704095220.0","upvote_count":"1","poster":"VMHarry","content":"Selected Answer: D\nGPU helps speeding up training process","comment_id":"1111013"},{"upvote_count":"1","comment_id":"1103512","content":"Why no A?","poster":"vale_76_na_xxx","timestamp":"1703263080.0"},{"upvote_count":"2","content":"B. Train your model using Vertex AI Training with CPUs.\nNo GPUs for ScikitLearn, but parrallelize/distribute training is a good way to increase model building","comment_id":"1092827","poster":"mlx","timestamp":"1702240380.0"}],"question_images":[],"answer_images":[],"unix_timestamp":1702240380,"topic":"1","choices":{"D":"Train your model using Vertex AI Training with GPUs.","A":"Train your model in a distributed mode using multiple Compute Engine VMs.","B":"Train your model using Vertex AI Training with CPUs.","C":"Migrate your model to TensorFlow, and train it using Vertex AI Training."},"url":"https://www.examtopics.com/discussions/google/view/128243-exam-professional-machine-learning-engineer-topic-1-question/","question_id":71,"timestamp":"2023-12-10 21:33:00","exam_id":13,"answers_community":["B (67%)","D (25%)","8%"],"isMC":true,"question_text":"You built a custom ML model using scikit-learn. Training time is taking longer than expected. You decide to migrate your model to Vertex AI Training, and you want to improve the model’s training time. What should you try out first?","answer_ET":"B"},{"id":"Gwu30mI29azCVJMviJZl","question_images":[],"choices":{"A":"Attach an NVIDIA P100 GPU to your deployed model’s instance.","B":"Use a low latency database for the customers’ historic purchase behavior.","D":"Create a materialized view in BigQuery with the necessary data for predictions.","C":"Deploy your model to more instances behind a load balancer to distribute traffic."},"answer_images":[],"unix_timestamp":1704595800,"question_text":"You are an ML engineer at a retail company. You have built a model that predicts a coupon to offer an ecommerce customer at checkout based on the items in their cart. When a customer goes to checkout, your serving pipeline, which is hosted on Google Cloud, joins the customer's existing cart with a row in a BigQuery table that contains the customers' historic purchase behavior and uses that as the model's input. The web team is reporting that your model is returning predictions too slowly to load the coupon offer with the rest of the web page. How should you speed up your model's predictions?","timestamp":"2024-01-07 03:50:00","topic":"1","question_id":72,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/130482-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"B","answer_description":"","discussion":[{"comment_id":"1365660","upvote_count":"1","poster":"desertlotus1211","content":"Selected Answer: B\nYou want to use Bigtable, Firestore, or Memorystore, or maybe ReDIS","timestamp":"1741224540.0"},{"content":"Selected Answer: D\nMaterialized views directly address this bottleneck by pre-computing the join.","timestamp":"1739843100.0","upvote_count":"1","poster":"NamitSehgal","comment_id":"1358080"},{"timestamp":"1738831500.0","comment_id":"1352282","content":"Selected Answer: B\nIf the primary issue is real-time access and speed, Option B is probably the better choice, as low-latency databases are built specifically for that purpose.","upvote_count":"1","poster":"vini123"},{"content":"Selected Answer: D\nKeep everything in BigQuery. Migrating to a fast database is more complex and can potentially introduce challenges.","timestamp":"1733176200.0","upvote_count":"2","poster":"DaleR","comment_id":"1321112","comments":[{"upvote_count":"1","comment_id":"1321512","poster":"rajshiv","timestamp":"1733253300.0","content":"Agree. MV is better."}]},{"timestamp":"1731558480.0","poster":"f084277","content":"Selected Answer: B\nUnclear how an MV would help retrieve a single row any faster. Something like BigTable (a low latency database) would be much faster.","comment_id":"1311724","upvote_count":"2"},{"comment_id":"1262214","poster":"inc_dev_ml_001","content":"Selected Answer: B\nIt says that you have to join the cart data, so you can't use the materialized view because it means that you should materialize the view every time a new cart shows up. So use a low latency DB it's the only way","timestamp":"1723060920.0","upvote_count":"2","comments":[{"comment_id":"1339179","content":"but the cart data is already available in the Big Query. Hence choosing materialized view is a good option, as it can pre-compute the join between the customer's cart and their historical data in BigQuery, reducing the latency of data retrieval.","upvote_count":"1","poster":"Sivaram06","timestamp":"1736605020.0"}]},{"content":"Selected Answer: B\nIn my opinion the materialized view could be the best way but it says that the cart data have to join with historic behaviour so it's impossibile to have all the needed data for the prediction in the materialized view because cart data are not in the database.","timestamp":"1718739840.0","comment_id":"1232590","upvote_count":"2","poster":"inc_dev_ml_001"},{"comments":[{"content":"Sure, but the question asks about SPEED, not cost and effort","upvote_count":"2","timestamp":"1731558540.0","comment_id":"1311725","poster":"f084277"}],"content":"Selected Answer: D\nBoth B and D in theory does reduce latency but B implies that we might need to migrate the database to another low latency database. This migration and setup might incur additional costs and effort.\n\nIn contrast, creating a materialized view seems much more straight forward since there is already a preexisting big query table mentioned in the question.","comment_id":"1212519","upvote_count":"1","timestamp":"1715879520.0","poster":"SausageMuffins"},{"timestamp":"1715764380.0","upvote_count":"1","poster":"Ria_1989","comment_id":"1211846","content":"Coupon to offer an ecommerce customer at checkout based on the items in their cart not the customer historic behaviour. That's creating confusion while choosing B."},{"comments":[{"poster":"pinimichele01","content":"https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction\n\ni'm not sure that bq is the best option, what do you think?","comment_id":"1200514","timestamp":"1713854760.0","upvote_count":"2"}],"upvote_count":"2","comment_id":"1199972","content":"Selected Answer: D\nReduced Join Cost: Joining the customer's cart with their purchase history in BigQuery during each prediction can be slow. A materialized view pre-computes and stores the join results, eliminating the need for repetitive joins and significantly reducing latency.\nTargeted Data Access: Materialized views allow you to specify the exact columns needed for prediction, minimizing data transferred between BigQuery and your serving pipeline.","timestamp":"1713762900.0","poster":"fitri001"},{"content":"Selected Answer: B\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction\n\n\"Analytical data stores such as BigQuery are not engineered for low-latency singleton read operations, where the result is a single row with many columns.\"","timestamp":"1713238860.0","comment_id":"1196313","upvote_count":"4","poster":"gscharly"},{"upvote_count":"3","poster":"guilhermebutzke","content":"Selected Answer: B\nI changed my mind. \n\nB: Im read a lot this page\n\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction\n\nIf the web team is reporting that the model is returning predictions too slowly to load the coupon offer with the rest of the web page, it suggests that the bottleneck might indeed be in the inference process rather than in data retrieval or processing. \nGiven that the model is deployed on Google Cloud, choosing a low-latency database makes it suitable for scenarios where quick access to data is crucial, such as real-time predictions for web applications.\n\nOption D: While pre-aggregating data in BigQuery can improve query speed, it might not be as efficient as a low-latency database for frequently accessed data like customer purchase history.","comment_id":"1153473","timestamp":"1708279680.0"},{"comment_id":"1142398","timestamp":"1707239460.0","comments":[],"poster":"guilhermebutzke","upvote_count":"1","content":"Selected Answer: D\nFirstly, I believe the correct choice should be B. This is supported by a comprehensive Google page discussing methods to minimize real-time prediction latency. In this resource, they don't mention using a BigQuery view but instead suggest precomputing and lookup approaches to minimize prediction time.\n\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction\n\nHowever, I will stick with option D because it's not clear whether option B suggests changing the entire database or just utilizing it as a preliminary step for online prediction."},{"timestamp":"1706611500.0","upvote_count":"2","comment_id":"1135688","content":"Selected Answer: D\nQueries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base tables. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries.","poster":"sonicclasps"},{"upvote_count":"3","comment_id":"1135003","timestamp":"1706535780.0","content":"Selected Answer: D\nD. Create a materialized view in BigQuery with the necessary data for predictions.\n\nHere's why:\n\nCurrent bottleneck: Joining the cart data with the BigQuery table containing historic purchases likely creates the latency bottleneck. Fetching data from BigQuery on every prediction request can be slow.\nMaterialized view: A materialized view pre-computes and stores the join between the cart data and the relevant historic purchase information in BigQuery. This eliminates the need for real-time joins during prediction, significantly reducing latency.\nFaster access: The pre-computed data in the materialized view is readily available within BigQuery, ensuring faster access for your serving pipeline when predicting the coupon offer.\nLower cost: Compared to additional instances or GPU resources, a materialized view can be a more cost-effective solution, especially if prediction requests are frequent.","poster":"ddogg"},{"upvote_count":"1","timestamp":"1704595800.0","content":"Selected Answer: B\nOption B seems most sensible.","comment_id":"1115569","poster":"kalle_balle"}],"answers_community":["B (57%)","D (43%)"],"exam_id":13,"answer":"B"},{"id":"htSeO9WGtRlNbA70nWdJ","timestamp":"2024-01-07 03:56:00","unix_timestamp":1704596160,"answer_ET":"B","answers_community":["B (53%)","C (47%)"],"isMC":true,"answer":"B","topic":"1","answer_images":[],"question_text":"You work for a small company that has deployed an ML model with autoscaling on Vertex AI to serve online predictions in a production environment. The current model receives about 20 prediction requests per hour with an average response time of one second. You have retrained the same model on a new batch of data, and now you are canary testing it, sending ~10% of production traffic to the new model. During this canary test, you notice that prediction requests for your new model are taking between 30 and 180 seconds to complete. What should you do?","exam_id":13,"choices":{"C":"Remove your new model from the production environment. Compare the new model and existing model codes to identify the cause of the performance bottleneck.","B":"Turn off auto-scaling for the online prediction service of your new model. Use manual scaling with one node always available.","D":"Remove your new model from the production environment. For a short trial period, send all incoming prediction requests to BigQuery. Request batch predictions from your new model, and then use the Data Labeling Service to validate your model’s performance before promoting it to production.","A":"Submit a request to raise your project quota to ensure that multiple prediction services can run concurrently."},"url":"https://www.examtopics.com/discussions/google/view/130483-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"discussion":[{"upvote_count":"2","poster":"desertlotus1211","content":"Selected Answer: C\nYou're performing 20 predictions an hour - so scaling isn’t the root issue.\n\nCode issue.","timestamp":"1741224720.0","comment_id":"1365662"},{"content":"Selected Answer: B\nSince the same model is being used and the only change is the data, it's likely that the latency issue is caused by how Vertex AI is scaling the prediction service.","poster":"vini123","timestamp":"1738831860.0","upvote_count":"1","comment_id":"1352283"},{"upvote_count":"1","comment_id":"1338513","content":"Selected Answer: C\nRemoving the new model from production to debug and address the root cause of the latency issue is the most efficient and logical course of action. This ensures minimal disruption to production services and lays the groundwork for a smooth rollout after fixing the bottleneck","poster":"potomeek","timestamp":"1736456280.0"},{"comments":[{"comment_id":"1263737","content":"I was convinced that the machines that are autoscaled by the Vertex AI Endpoint seem to be tied to the endpoint, not the model in which they are deployed.","timestamp":"1723349040.0","poster":"YushiSato","upvote_count":"1"}],"timestamp":"1723348440.0","comment_id":"1263735","poster":"YushiSato","upvote_count":"3","content":"I don't see B as the right answer.\nThe Vertex AI Endpoint cannot scale to 0 for newer version of the model.\n\n> When you configure a DeployedModel, you must set dedicatedResources.minReplicaCount to at least 1. In other words, you cannot configure the DeployedModel to scale to 0 prediction nodes when it is unused.\n\nhttps://cloud.google.com/vertex-ai/docs/general/deployment#scaling"},{"poster":"AnnaR","content":"Selected Answer: B\nB can be effective in controlling resources available to the new model, ensuring that it is not delayed by the autoscaling trying to scale up from 0. \n\nNot A: there is no indication in the description that quota limits cause the slowdown and does not address issue where new model is performing poorly on canary testing. \nNot C : when you pull the new model from prod environment, you could affect end-user experience\nNot D: Same as C plus you rely on batch predictions which does not align with the need for online, real-time predictions in the prod environemnt. Data Labeling Service is more about assessing accuracy and less about resolving latency issues.","timestamp":"1714224000.0","comment_id":"1203129","upvote_count":"2"},{"poster":"pinimichele01","timestamp":"1712489340.0","comment_id":"1190914","upvote_count":"1","comments":[{"upvote_count":"3","content":"the new model has too few requests per hour and therefore scales downs to 0. Which means it has to create the an instance every time it serves a request, and this takes time.\nBy manually setting the number of nodes, the nodes will always be running, whether or not they are serving predictions","comment_id":"1196142","timestamp":"1713201000.0","poster":"pinimichele01"}],"content":"Selected Answer: B\nYou have retrained the same model on a new batch of data"},{"poster":"VipinSingla","content":"Selected Answer: B\nbottleneck seems to be start of node as there are very low number of requests so having one node always available will help in this case.","comment_id":"1180684","timestamp":"1711179000.0","upvote_count":"1"},{"timestamp":"1711014060.0","comments":[{"timestamp":"1733253780.0","poster":"rajshiv","upvote_count":"1","comment_id":"1321518","content":"I also think C. The model performance issue needs to be addressed."}],"comment_id":"1179090","poster":"Aastha_Vashist","upvote_count":"1","content":"Selected Answer: C\nwent with c"},{"comment_id":"1161556","upvote_count":"1","poster":"Carlose2108","content":"Selected Answer: C\nI went C. \nDiagnosing the root cause.","timestamp":"1709119380.0"},{"content":"Selected Answer: C\nChoose C.\n\nThe significant increase in response time from 1 second to between 30 and 180 seconds indicates a performance issue with the new model. Before making any further changes or decisions, it's crucial to identify the root cause of this performance bottleneck. By comparing the code of the new model with the existing model, you can pinpoint any differences that might be causing the slowdown. \nIn A, This may not be the root cause and could incur unnecessary costs without addressing the performance issue. In B,  it doesn't address the underlying issue causing the significant increase in response time observed during canary testing. in D, This would significantly increase latency and hinder real-time predictions, negatively impacting user experience.","timestamp":"1707241020.0","poster":"guilhermebutzke","comment_id":"1142409","upvote_count":"2","comments":[{"timestamp":"1707691620.0","comments":[{"upvote_count":"1","content":"B is still right because\n- Retraining often involves adjustments to hyperparameters or training processes.\n- Changes to data preprocessing steps (e.g., feature scaling, handling missing values) during retraining can change model code and affect model performance.\n- The retraining process itself might have introduced unknown bugs or inefficiencies into the model's deployment pipeline or the code that interacts with the model.","timestamp":"1733670480.0","poster":"lunalongo","comment_id":"1323589"}],"upvote_count":"2","comment_id":"1147709","content":"But in the question it says \"You have retrained the same model on a new batch of data\" it's just the data that changed so no need to check for the code check.","poster":"vaibavi"}]},{"timestamp":"1706612100.0","poster":"sonicclasps","comment_id":"1135695","content":"Selected Answer: B\nsounds to me that the new model has too few requests per hour and therefore scales downs to 0. Which means it has to create the an instance every time it serves a request, and this takes time. \nBy manually setting the number of nodes, the nodes will always be running, whether or not they are serving predictions","upvote_count":"4"},{"comment_id":"1116500","timestamp":"1704705060.0","poster":"b1a8fae","upvote_count":"1","content":"Unsure on this one, but I would go with A.\nB. Turning off auto-scaling is a good measure when dealing with datasets with steep spikes of requests traffic (here we are dealing with avg. 20 request per hour) \"The service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic.\" https://cloud.google.com/blog/products/ai-machine-learning/scaling-machine-learning-predictions\nC. You retrain the SAME model on a different batch of data. It is implied that the code is the same too?\nD. Actual quality of the model is not in question here, but rather the long prediction time per request.\n\nEven if the requests traffic is very low, I can only consider option A: the selected quota cannot deal with the amount of concurrent prediction requests."},{"comment_id":"1115571","comments":[{"upvote_count":"1","poster":"edoo","content":"You only retrained the same model, your code hasn't changed, you won't find anything with C.\nIt's B.","timestamp":"1709807340.0","comment_id":"1167896"}],"poster":"kalle_balle","content":"Selected Answer: C\nOption B or D is completely wrong. Option A to raise the quota might be necessary in some situations but doesn't necessarily deal with the performance issue at the test. Option C seems like the most suitable option.","timestamp":"1704596160.0","upvote_count":"1"}],"answer_description":"","question_id":73},{"id":"9UducixcEpAZHln0awMJ","answer_ET":"A","unix_timestamp":1704596460,"discussion":[{"upvote_count":"1","poster":"vini123","timestamp":"1738832040.0","content":"Selected Answer: A\nBigQuery integration with Vertex AI: BigQuery is fully integrated with Vertex AI, which means you can directly use BigQuery as a data source for Vertex AI managed datasets. By writing a query to preprocess the data and then creating a Vertex AI managed dataset from that query, you can skip extra steps like exporting or converting data into different formats. This is both efficient and leverages the native capabilities of the GCP platform","comment_id":"1352284"},{"timestamp":"1717941660.0","upvote_count":"3","content":"Selected Answer: A\nA) Keep the data in BigQuery and create a new table to avoid latency moving data out of BigQuery","poster":"PhilipKoku","comment_id":"1227359"},{"upvote_count":"1","poster":"nmnm22","timestamp":"1717055460.0","content":"Selected Answer: A\nA seems the correct one","comment_id":"1221452"},{"timestamp":"1713239160.0","poster":"gscharly","upvote_count":"1","comment_id":"1196316","content":"Selected Answer: A\nI go for A:"},{"poster":"shadz10","upvote_count":"1","content":"Selected Answer: A\ncan export directly from big query as vertex ai managed dataset to use train an autoML model","timestamp":"1705262220.0","comment_id":"1122790"},{"content":"A \nBy writing a query that preprocesses the data using BigQuery and creating a new table, you can directly create a Vertex AI managed dataset with the new table as the data source. This approach is efficient because it leverages BigQuery’s powerful data processing capabilities and avoids the need to export data to another format or service. It also simplifies the process by keeping everything within the Google Cloud ecosystem. This makes it easier to manage and monitor your data and model training process.","timestamp":"1704962880.0","comment_id":"1119517","upvote_count":"2","poster":"36bdc1e"},{"comment_id":"1116865","content":"I go for A:","poster":"vale_76_na_xxx","upvote_count":"2","timestamp":"1704737220.0"},{"content":"Selected Answer: A\nForgot to vote","comment_id":"1116684","timestamp":"1704724740.0","upvote_count":"1","poster":"b1a8fae"},{"comment_id":"1116543","content":"A seems the easiest to me: preprocess the data on BigQuery (where the input table is stored) and export directly as Vertex AI managed dataset.","timestamp":"1704710280.0","poster":"b1a8fae","upvote_count":"2"},{"timestamp":"1704596460.0","comments":[{"comment_id":"1311728","content":"The data is already in BigQuery. Preprocess the data in BigQuery. How is Dataflow easier than BigQuery? (question doesn't mention anything about scalability)","upvote_count":"1","timestamp":"1731558840.0","poster":"f084277"},{"content":"small dataset -> no dataflow","comment_id":"1202538","poster":"pinimichele01","timestamp":"1714127880.0","upvote_count":"1"}],"content":"Selected Answer: B\nDataflow seems like the easiest and most scalable way to deal with this issue. Option B.","poster":"kalle_balle","comment_id":"1115575","upvote_count":"1"}],"answers_community":["A (89%)","11%"],"topic":"1","isMC":true,"question_id":74,"answer":"A","answer_images":[],"timestamp":"2024-01-07 04:01:00","exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/130484-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"question_text":"You want to train an AutoML model to predict house prices by using a small public dataset stored in BigQuery. You need to prepare the data and want to use the simplest, most efficient approach. What should you do?","choices":{"A":"Write a query that preprocesses the data by using BigQuery and creates a new table. Create a Vertex AI managed dataset with the new table as the data source.","D":"Use a Vertex AI Workbench notebook instance to preprocess the data by using the pandas library. Export the data as CSV files, and use those files to create a Vertex AI managed dataset.","C":"Write a query that preprocesses the data by using BigQuery. Export the query results as CSV files, and use those files to create a Vertex AI managed dataset.","B":"Use Dataflow to preprocess the data. Write the output in TFRecord format to a Cloud Storage bucket."},"answer_description":""},{"id":"PRLgaQTlFnnF3ec5mcU5","answer_description":"","timestamp":"2024-01-08 14:55:00","answer_ET":"D","question_text":"You developed a Vertex AI ML pipeline that consists of preprocessing and training steps and each set of steps runs on a separate custom Docker image. Your organization uses GitHub and GitHub Actions as CI/CD to run unit and integration tests. You need to automate the model retraining workflow so that it can be initiated both manually and when a new version of the code is merged in the main branch. You want to minimize the steps required to build the workflow while also allowing for maximum flexibility. How should you configure the CI/CD workflow?","answer":"D","isMC":true,"choices":{"D":"Trigger GitHub Actions to run the tests, launch a Cloud Build workflow to build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines.","A":"Trigger a Cloud Build workflow to run tests, build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines.","B":"Trigger GitHub Actions to run the tests, launch a job on Cloud Run to build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines.","C":"Trigger GitHub Actions to run the tests, build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines."},"topic":"1","question_images":[],"unix_timestamp":1704722100,"url":"https://www.examtopics.com/discussions/google/view/130571-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"answer_images":[],"answers_community":["D (55%)","C (39%)","6%"],"question_id":75,"discussion":[{"poster":"pikachu007","timestamp":"1704874140.0","upvote_count":"5","comment_id":"1118352","content":"Selected Answer: C\nConsidering the goal of minimizing steps while allowing for flexibility, option C - \"Trigger GitHub Actions to run the tests, build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines\" appears to be the most straightforward approach. It leverages GitHub Actions for testing and image building, then directly triggers the Vertex AI Pipelines, simplifying the workflow and reducing unnecessary services involved in the process."},{"content":"Selected Answer: C\nSince your team already uses GitHub and GitHub Actions as part of your CI/CD process (including running unit and integration tests), it’s most efficient to extend your existing workflow to also handle the packaging and deployment of your ML pipeline\n\nIt avoids introducing an extra services like (i.e. Run and Build)...","comment_id":"1366111","poster":"desertlotus1211","upvote_count":"1","timestamp":"1741311780.0"},{"comment_id":"1358093","timestamp":"1739846400.0","upvote_count":"1","poster":"NamitSehgal","content":"Selected Answer: D\nCloud Build is a dedicated service for building container images."},{"content":"Selected Answer: D\nOption D is the most suitable answer because it maximizes flexibility, optimizes the image creation process, and integrates well with the Vertex AI Pipelines workflow.","upvote_count":"1","timestamp":"1738833540.0","poster":"vini123","comment_id":"1352290"},{"content":"Selected Answer: C\nC) GitHub Actions can directly build the Docker images, push them to Artifact Registry, and then trigger the Vertex AI pipeline execution. \n*A&D) Add complexity by adding Cloud Build\n*B) Adds Cloud Run for building/pushing Docker images, but GitHub Actions do this.\nSee how:\nhttps://medium.com/@sbkapelner/building-and-pushing-to-artifact-registry-with-github-actions-7027b3e443c1","poster":"lunalongo","comment_id":"1323614","timestamp":"1733672520.0","upvote_count":"2"},{"timestamp":"1732670700.0","poster":"AB_C","comment_id":"1318371","upvote_count":"1","content":"Selected Answer: D\nMaximum flexibility needed. Hence D, not C"},{"timestamp":"1722767700.0","poster":"bfdf9c8","comment_id":"1260620","content":"Selected Answer: A\nThe correct answer is a. I think is tricky because D is posible, but add one step. and we want to minimize the steps.","upvote_count":"2"},{"content":"option D might seem appealing at first, but it adds unnecessary complexity and makes it more challenging to manage the state of your pipeline. Option C, on the other hand, provides a simpler and more straightforward approach to automating your model retraining workflow using GitHub Actions.","upvote_count":"1","poster":"AzureDP900","comment_id":"1234815","timestamp":"1718997300.0"},{"comment_id":"1203134","upvote_count":"4","timestamp":"1714224900.0","poster":"AnnaR","content":"Selected Answer: D\nNot A: does not leverage the integration capabilities of GitHub Actions with GitHub for initial testing, which is more efficient when managing repo triggers and workflows directly from Github. \n\nNot B: Cloud Run for running stateless containers, not for CI/CD tasks like building and pushing images \n\nNot C: building docker images directly in github Actions can encounter limits in terms of build performance and resource availability, esp. for complex images"},{"upvote_count":"1","comment_id":"1199597","poster":"gscharly","timestamp":"1713695040.0","content":"Selected Answer: D\nagree with guilhermebutzke"},{"upvote_count":"1","poster":"fitri001","timestamp":"1713693180.0","content":"Selected Answer: D\nSecurity: GitHub Actions are ideal for running unit and integration tests within the controlled environment of your GitHub repository. This keeps your test code separate from the production pipeline code running in Cloud Build.\nScalability and Resource Management: Cloud Build is a managed service specifically designed for building container images in Google Cloud. It offers better resource management and scalability for building Docker images compared to Cloud Run, which is primarily designed for running stateless containers.\nFlexibility: This configuration allows for independent scaling of test execution (in GitHub Actions) and image building (in Cloud Build). You can modify the workflow files in each platform independently without affecting the other.","comment_id":"1199582","comments":[{"timestamp":"1713693240.0","comment_id":"1199583","content":"A & B. Cloud Run for Image Building: While Cloud Run can build Docker images, it's not its primary function. Cloud Build is a more robust and scalable solution for container image building in Google Cloud.\nC. Building Images in GitHub Actions: GitHub Actions might have limitations on resource allocation and might not be suitable for building complex Docker images, especially if they have large dependencies.","poster":"fitri001","upvote_count":"1"}]},{"upvote_count":"1","content":"Selected Answer: D\ni agree with guilhermebutzke","comment_id":"1190921","poster":"pinimichele01","timestamp":"1712490600.0"},{"timestamp":"1707242340.0","poster":"guilhermebutzke","upvote_count":"2","content":"Selected Answer: D\nChoose D:\n\nGitHub Actions should be used to run tests and initiate the workflow upon code merges. Then, Cloud Build is a suitable service for building Docker images and handling the subsequent steps of pushing the images to Artifact Registry. So, Vertex AI Pipelines can be launched as part of the Cloud Build workflow for model retraining.\n\nIn A Using Cloud Build directly from GitHub Actions would bypass GitHub Actions' capabilities for triggering and testing. In B, Cloud Run for building Docker images can introduce potential compatibility issues with Vertex AI Pipelines. In C,  Skipping Cloud Build for image building limits the workflow's portability and integration with Vertex AI.\n\nhttps://cloud.google.com/vertex-ai/docs/pipelines/introduction\nhttps://medium.com/@cait.ray13/serving-ml-model-using-google-pub-sub-python-f569c46e7eb0","comment_id":"1142427"},{"timestamp":"1707081300.0","poster":"mindriddler","content":"Selected Answer: C\nIt has to be C. Therese no need to use both GH Actions and Cloud Build when GH Actions can do it all by itself","comment_id":"1140533","upvote_count":"2"},{"timestamp":"1705577100.0","content":"Selected Answer: D\nD\nhttps://cloud.google.com/build/docs/building/build-containers\nhttps://cloud.google.com/build/docs/build-push-docker-image","upvote_count":"3","comment_id":"1125775","poster":"shadz10"},{"upvote_count":"2","timestamp":"1704963300.0","comment_id":"1119520","poster":"36bdc1e","content":"The best approach would be Option C.\n\nBy triggering GitHub Actions to run the tests, build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines, you can automate the model retraining workflow. This approach allows for maximum flexibility and minimizes the steps required to build the workflow."},{"content":"Selected Answer: C\nI am torn between C and D. GitHub actions to run the tests is definitely the simplest. Cloud Build allows to access fully managed CI/CD workflow (you could setup the Docker build job), but I figure it would be easier to do it from GitHub actions directly (https://docs.github.com/en/actions/creating-actions/creating-a-docker-container-action) which allows you to use 1 tool less and achieve the same result.","comment_id":"1116666","upvote_count":"2","poster":"b1a8fae","timestamp":"1704723300.0"},{"upvote_count":"2","content":"Selected Answer: D\ni think it's D","comment_id":"1116655","timestamp":"1704722100.0","poster":"BlehMaks"}]}],"exam":{"isBeta":false,"isImplemented":true,"numberOfQuestions":304,"lastUpdated":"11 Apr 2025","id":13,"name":"Professional Machine Learning Engineer","provider":"Google","isMCOnly":true},"currentPage":15},"__N_SSP":true}