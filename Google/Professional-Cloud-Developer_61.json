{"pageProps":{"questions":[{"id":"SvLzNmLmfn7MvXUOXfl6","topic":"1","choices":{"B":"Grant the service account BigQuery Data Editor and BigQuery Data Viewer roles.","A":"Grant the service account BigQuery Data Viewer and BigQuery Job User roles.","C":"Create a view in BigQuery from the SQL query and SELECT* from the view in the CLI.","D":"Create a new dataset in BigQuery, and copy the source table to the new dataset Query the new dataset and table from the CLI."},"question_images":[],"discussion":[{"comment_id":"249660","content":"I think A is the correct one.","timestamp":"1608583140.0","poster":"donchick","upvote_count":"14"},{"upvote_count":"1","poster":"PinkeshExampTopics","comment_id":"1322299","content":"Selected Answer: B\nWhile Data Viewer role only allows read-only access to BigQuery data. It doesn't grant the necessary permissions to execute queries. So, need to give it Data Editor role. Hence, B is the correct answer here.","timestamp":"1733391600.0"},{"comments":[{"timestamp":"1721226360.0","comment_id":"1249686","content":"Let's break down why the other options are less suitable:\n\nB. Grant the service account BigQuery Data Editor and BigQuery Data Viewer roles: While BigQuery Data Editor allows for data modification, it's not strictly necessary for executing queries. Granting BigQuery Data Viewer and BigQuery Job User roles is sufficient.\nC. Create a view in BigQuery from the SQL query and SELECT from the view in the CLI: * Creating a view can simplify queries, but it doesn't address the underlying permission issue. The service account still needs the necessary permissions to access the view and execute queries.\nD. Create a new dataset in BigQuery and copy the source table to the new dataset Query the new dataset and table from the CLI: This approach adds unnecessary complexity and data movement. It doesn't solve the permission issue.","upvote_count":"1","poster":"thewalker"}],"poster":"thewalker","content":"Selected Answer: A\nThe correct answer is A. Grant the service account BigQuery Data Viewer and BigQuery Job User roles.\n\nHere's why:\n\nBigQuery Data Viewer: This role allows the service account to read data from BigQuery tables. This is necessary for the BigQuery CLI to execute the SQL queries.\nBigQuery Job User: This role allows the service account to run queries and jobs in BigQuery. This is essential for the BigQuery CLI to execute the SQL queries and return results.","upvote_count":"2","comment_id":"1249685","timestamp":"1721226360.0"},{"poster":"santoshchauhan","upvote_count":"1","timestamp":"1709866080.0","comment_id":"1168518","content":"Selected Answer: A\nA. Grant the service account BigQuery Data Viewer and BigQuery Job User roles.\n\nThe permission error from the BigQuery CLI suggests that the service account used to execute the queries does not have the necessary permissions. To resolve this, you need to ensure that the service account has the appropriate roles:\n\nBigQuery Data Viewer role: This role allows the service account to read data from BigQuery tables and views. It's necessary for the service account to access and read the dataset against which the queries are being executed.\n\nBigQuery Job User role: This role allows the service account to create and run jobs in BigQuery, including query jobs, which is necessary for executing SQL queries."},{"comment_id":"1149014","timestamp":"1707814080.0","poster":"theseawillclaim","upvote_count":"1","content":"Selected Answer: A\nA is the one. No need to edit data is specified."},{"comment_id":"1011886","poster":"__rajan__","content":"Selected Answer: A\nA is correct.","upvote_count":"1","timestamp":"1695182220.0"},{"timestamp":"1673165520.0","upvote_count":"3","comment_id":"769184","poster":"omermahgoub","content":"The correct answer is Option A. In order to allow the analytics system to execute queries against the BigQuery dataset, the service account must be granted the BigQuery Data Viewer and BigQuery Job User roles. The BigQuery Data Viewer role allows the service account to read data from tables, and the BigQuery Job User role allows the service account to run jobs, which includes executing queries. Option B is not a good solution because the BigQuery Data Editor role allows the service account to modify data in tables, which is not necessary to execute queries. Option C is not a good solution because creating a view in BigQuery and selecting from the view in the CLI will not resolve the permission issue. Option D is not a good solution because creating a new dataset and copying the source table to the new dataset will not resolve the permission issue."},{"poster":"tomato123","upvote_count":"3","timestamp":"1660974660.0","comment_id":"649229","content":"Selected Answer: A\nA is correct"},{"upvote_count":"1","comment_id":"637170","timestamp":"1658812320.0","content":"Selected Answer: A\nA it's correct for the principle of least privilege","poster":"maxdanny"},{"comment_id":"519649","poster":"ParagSanyashiv","timestamp":"1641659400.0","content":"Selected Answer: A\nAccording to the best practice - \"User should have least privilege i.e. only those permissions which are required to perform an operation\" - Option A is Correct.","upvote_count":"1"},{"comment_id":"403004","timestamp":"1625875980.0","upvote_count":"3","content":"Principle of least privilege so A is the answer","poster":"syu31svc"},{"content":"Ans A only \nhttps://cloud.google.com/bigquery/docs/access-control#bigquery\nIn Ans B Bigquery Data Editor is not required.","comment_id":"326285","poster":"cloud_mk","timestamp":"1617335100.0","upvote_count":"4"}],"answer_images":[],"question_text":"Your analytics system executes queries against a BigQuery dataset. The SQL query is executed in batch and passes the contents of a SQL file to the BigQuery\nCLI. Then it redirects the BigQuery CLI output to another process. However, you are getting a permission error from the BigQuery CLI when the queries are executed.\nYou want to resolve the issue. What should you do?","url":"https://www.examtopics.com/discussions/google/view/40500-exam-professional-cloud-developer-topic-1-question-65/","question_id":301,"answers_community":["A (91%)","9%"],"answer":"A","isMC":true,"exam_id":7,"answer_description":"","answer_ET":"A","timestamp":"2020-12-21 21:39:00","unix_timestamp":1608583140},{"id":"NLcoYUpYvricIp6skqid","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/40501-exam-professional-cloud-developer-topic-1-question-66/","question_text":"Your application is running on Compute Engine and is showing sustained failures for a small number of requests. You have narrowed the cause down to a single\nCompute Engine instance, but the instance is unresponsive to SSH.\nWhat should you do next?","choices":{"C":"Delete the machine and create a new one.","D":"Take a snapshot of the disk and attach it to a new machine.","A":"Reboot the machine.","B":"Enable and check the serial port output."},"answer_ET":"B","question_images":[],"exam_id":7,"answers_community":["B (100%)"],"discussion":[{"timestamp":"1608583440.0","content":"Difficult to choose because either B(https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh#debug_with_serial_console) or D(https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh#inspect_vm) is recommended by google. I'd stay with B.","comment_id":"249664","comments":[{"content":"Using the first link you provided: https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh#debug_with_serial_console\n\nWe recommend that you review the logs from the serial console for connection errors. You can access the serial console from your local workstation by using a browser.\n\nEnable read/write access to an instance's serial console, so you can log into the console and troubleshoot problems with the instance. This approach is useful when you cannot log in with SSH, or if the instance has no connection to the network. The serial console remains accessible in both of these situations","timestamp":"1621511940.0","poster":"mastodilu","upvote_count":"4","comment_id":"362121"}],"upvote_count":"5","poster":"donchick"},{"upvote_count":"1","comments":[{"comment_id":"1249691","content":"Let's look at why the other options are less ideal:\n\nA. Reboot the machine: While rebooting might resolve some temporary issues, it doesn't provide any insight into the cause of the problem. If the issue is persistent, rebooting won't solve it.\nC. Delete the machine and create a new one: This is a drastic measure that should be avoided unless absolutely necessary. It involves data loss and reconfiguration, which can be time-consuming and disruptive.\nD. Take a snapshot of the disk and attach it to a new machine: This approach can be useful for recovering data, but it doesn't address the underlying issue causing the instance to be unresponsive.","timestamp":"1721226720.0","upvote_count":"1","poster":"thewalker"}],"comment_id":"1249689","content":"Selected Answer: B\nThe best course of action is B. Enable and check the serial port output.\n\nHere's why:\n\nSerial Port Output: The serial port output provides a log of the instance's boot process and any error messages that might occur during startup. This is a valuable resource for diagnosing issues when an instance is unresponsive to SSH.\nTroubleshooting: By examining the serial port output, you can often identify the root cause of the unresponsiveness, such as a failed boot process, disk errors, or configuration problems.","timestamp":"1721226720.0","poster":"thewalker"},{"poster":"santoshchauhan","upvote_count":"1","comment_id":"1168520","timestamp":"1709866380.0","content":"Selected Answer: B\nB. Enable and check the serial port output.\n\nWhen a Compute Engine instance becomes unresponsive to SSH, one of the best ways to diagnose the issue is to check the serial port output. The serial console can provide valuable information about the state of the instance and any errors that may be occurring at the system level. This is particularly useful when you can't connect via SSH. Google Cloud allows you to enable interactive access to the serial console, which can be a crucial tool for troubleshooting."},{"upvote_count":"1","comment_id":"1012378","poster":"__rajan__","timestamp":"1695217980.0","content":"Selected Answer: B\nB is correct."},{"comments":[{"poster":"omermahgoub","upvote_count":"1","comment_id":"769190","timestamp":"1673165820.0","content":"Option A is not a good solution because rebooting the machine may not resolve the issue that is causing the instance to become unresponsive."}],"poster":"omermahgoub","content":"Option B is correct. According to Google Cloud documentation, if a Compute Engine instance is unresponsive to SSH and you have narrowed the cause down to a single instance, you should enable and check the serial port output. The serial port output is a log of system messages and can help you diagnose the issue causing the instance to become unresponsive. To enable and check the serial port output, you can access the serial console as the root user from your local workstation using a browser. This will allow you to review the logs and potentially identify the cause of the problem.","upvote_count":"4","timestamp":"1673165820.0","comment_id":"769187"},{"comment_id":"649230","poster":"tomato123","upvote_count":"2","timestamp":"1660974660.0","content":"Selected Answer: B\nB is correct"},{"poster":"ParagSanyashiv","comment_id":"519652","content":"Selected Answer: B\nB is correct, because it will be the first step to check the serial port 22 is responsive or not.","upvote_count":"2","timestamp":"1641659520.0"},{"timestamp":"1625876580.0","upvote_count":"3","comment_id":"403012","poster":"syu31svc","content":"I would take B as the answer\n\n\"What should you do next?\"\n\nSSH is port 22 so it makes sense to check for the port"},{"comment_id":"294747","poster":"StelSen","timestamp":"1613791380.0","content":"Option-B is most suitable: In a real world I will try this and even if doesn't work then I will try Option-D which will help me to troubleshoot in details.","upvote_count":"4"}],"unix_timestamp":1608583440,"isMC":true,"topic":"1","timestamp":"2020-12-21 21:44:00","question_id":302,"answer":"B","answer_images":[]},{"id":"HrGwemwHnnStq4RW2U1C","answers_community":["BD (100%)"],"answer_ET":"BD","question_images":[],"question_text":"You configured your Compute Engine instance group to scale automatically according to overall CPU usage. However, your application's response latency increases sharply before the cluster has finished adding up instances. You want to provide a more consistent latency experience for your end users by changing the configuration of the instance group autoscaler.\nWhich two configuration changes should you make? (Choose two.)","unix_timestamp":1605265440,"isMC":true,"answer":"BD","topic":"1","question_id":303,"answer_description":"","choices":{"E":"Remove the health-check for individual VMs in the instance group.","D":"Decrease the target CPU usage for the instance group autoscaler.","B":"Decrease the cool-down period for instances added to the group.","A":"Add the label ג€AUTOSCALEג€ to the instance group template.","C":"Increase the target CPU usage for the instance group autoscaler."},"answer_images":[],"exam_id":7,"url":"https://www.examtopics.com/discussions/google/view/36918-exam-professional-cloud-developer-topic-1-question-67/","timestamp":"2020-11-13 12:04:00","discussion":[{"upvote_count":"16","comments":[{"upvote_count":"5","poster":"fraloca","content":"For me the answer is B and D.\n\"A cool down period value that is significantly longer causing a delay in scaling out\".\nhttps://cloud.google.com/compute/docs/autoscaler#cool_down_period\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cpu#scaling_based_on_cpu_utilization","timestamp":"1624783380.0","comment_id":"253249"}],"comment_id":"249667","poster":"donchick","content":"I'd choose B and D.","timestamp":"1624301340.0"},{"upvote_count":"1","content":"Selected Answer: BD\nB. Decrease the cool-down period for instances added to the group: The cool-down period is the time the autoscaler waits after a new instance is healthy before it collects usage metrics from the instance. A shorter cool-down period allows the autoscaler to react more quickly to changes in load, potentially starting to scale up sooner when there is a sudden increase in traffic.\n\nD. Decrease the target CPU usage for the instance group autoscaler: Lowering the target CPU utilization means that the autoscaler will start adding instances sooner as the CPU usage approaches the lower target. This can help to alleviate the issue where response latency increases sharply because new instances are added before the CPU usage hits a higher threshold.","comment_id":"1168522","poster":"santoshchauhan","timestamp":"1725757320.0"},{"timestamp":"1710914460.0","content":"Selected Answer: BD\nB and D are the best option here.","upvote_count":"1","poster":"__rajan__","comment_id":"1011888"},{"poster":"omermahgoub","comments":[{"timestamp":"1688797140.0","content":"Option A is not a correct solution because adding the label \"AUTOSCALE\" to the instance group template will not affect the configuration of the instance group autoscaler","upvote_count":"1","poster":"omermahgoub","comment_id":"769192"}],"content":"Options B and D are correct. To provide a more consistent latency experience for your end users, you should make the following configuration changes:\n\nOption B: Decrease the cool-down period for instances added to the group. The cool-down period is the time that must pass before the instance group autoscaler can add more instances after it has already added instances to the group. Decreasing the cool-down period can allow the instance group to scale more quickly in response to changes in CPU usage, which may help to reduce latency.\n\nOption D: Decrease the target CPU usage for the instance group autoscaler. The target CPU usage is the average CPU usage that the instance group autoscaler aims to maintain for the group. Decreasing the target CPU usage may allow the instance group to scale down more quickly in response to changes in CPU usage, which may also help to reduce latency.","comment_id":"769191","upvote_count":"4","timestamp":"1688797080.0"},{"comment_id":"649231","content":"Selected Answer: BD\nBD are correct","timestamp":"1676879520.0","poster":"tomato123","upvote_count":"3"},{"timestamp":"1661387760.0","poster":"GCPCloudArchitectUser","content":"Selected Answer: BD\nI would choose these two","comment_id":"555704","upvote_count":"3"},{"content":"Adding label won't solve the issue so A is wrong for sure\nRemoving health check is not recommended so E is wrong as well\nIncrease CPU target is wrong since scaling will take place at a higher usage which is not what we want\n\nB and D are the correct options","poster":"syu31svc","comment_id":"403015","upvote_count":"4","timestamp":"1641781680.0"},{"poster":"whigy","upvote_count":"2","content":"D is more correct than C. If C, the auto-scale up will be further delayed","timestamp":"1620896640.0","comment_id":"218409"}]},{"id":"5oxzPtna04rLWI8LJ13M","answer":"B","topic":"1","timestamp":"2020-12-21 21:57:00","unix_timestamp":1608584220,"answer_ET":"B","isMC":true,"choices":{"C":"Perform a rolling-action with maxHealthy set to 1, maxUnhealthy set to 0.","B":"Perform a rolling-action with maxSurge set to 0, maxUnavailable set to 1","D":"Perform a rolling-action with maxHealthy set to 0, maxUnhealthy set to 1.","A":"Perform a rolling-action with maxSurge set to 1, maxUnavailable set to 0."},"question_id":304,"question_images":[],"answers_community":["B (61%)","A (39%)"],"exam_id":7,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/40503-exam-professional-cloud-developer-topic-1-question-68/","discussion":[{"upvote_count":"22","comments":[{"comment_id":"330878","timestamp":"1617856380.0","content":"\"costs should be minimized and the number of instances should not increase\", maxSurge = 1 will increase the number of instances, so B would be the correct answer","upvote_count":"4","poster":"Ayuewinc"}],"timestamp":"1608584220.0","comment_id":"249671","content":"B(maxSurge = 0, maxUnavailable = 1)","poster":"donchick"},{"poster":"syu31svc","comment_id":"403019","upvote_count":"5","timestamp":"1625877480.0","content":"\"number of instances should not increase\"\n\nB would be correct"},{"comment_id":"1293422","timestamp":"1728122880.0","content":"Selected Answer: B\nmaxSurge = 0, maxUnavailable = 1)","poster":"anshad666","upvote_count":"1"},{"upvote_count":"1","comment_id":"1249697","comments":[{"timestamp":"1721227440.0","upvote_count":"1","comment_id":"1249698","content":"Let's break down why the other options are incorrect:\n\nB. Perform a rolling-action with maxSurge set to 0, maxUnavailable set to 1: This would allow one instance to be taken offline before the new instance is healthy, potentially causing downtime.\nC. Perform a rolling-action with maxHealthy set to 1, maxUnhealthy set to 0: maxHealthy and maxUnhealthy are not valid parameters for rolling updates. They are used for regional instance group managers, not managed instance groups.\nD. Perform a rolling-action with maxHealthy set to 0, maxUnhealthy set to 1: Similar to option C, these parameters are not applicable to managed instance groups.","poster":"thewalker"}],"poster":"thewalker","content":"Selected Answer: A\nThe correct answer is A. Perform a rolling-action with maxSurge set to 1, maxUnavailable set to 0.\n\nHere's why:\n\nRolling Update: A rolling update is the best approach for deploying new versions of your application to a managed instance group while minimizing downtime and ensuring a smooth transition.\nmaxSurge : This parameter controls the maximum number of instances that can be added to the group during the update. Setting maxSurge to 1 means that only one additional instance will be created beyond the target size of the group. This helps to minimize costs by avoiding unnecessary instance creation.\nmaxUnavailable : This parameter controls the maximum number of instances that can be taken offline during the update. Setting maxUnavailable to 0 ensures that no existing instances are taken offline until the newly created instance is healthy. This guarantees that the deployment only continues if the new instance is healthy.","timestamp":"1721227440.0"},{"timestamp":"1709867220.0","comment_id":"1168524","poster":"santoshchauhan","content":"Selected Answer: A\nHere's the reasoning:\n\nmaxSurge: This parameter determines the number of additional instances that can be created above the target size of the instance group during the update. Setting maxSurge to 1 allows the instance group to create one extra instance beyond its target size. This extra instance is used to start a new version of your application, ensuring that there's always a running instance during the update process.\n\nmaxUnavailable: This parameter specifies the number of instances that can be unavailable at any time during the update. Setting maxUnavailable to 0 ensures that there is no reduction in the number of available instances below the target size during the update process.","upvote_count":"2"},{"comment_id":"1079339","poster":"Aeglas","upvote_count":"2","timestamp":"1700836740.0","content":"Selected Answer: B\nmaxSurge controls in a rolling update how many resources can be added above threshold of the MIG (managed instance group),\n while maxUnavailable controls the max number of instances that can be taken offline during update at the same time"},{"upvote_count":"1","content":"Selected Answer: A\nthe correct answer is A i think\nIf you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running.\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_surge","timestamp":"1699218900.0","poster":"Kadhem","comment_id":"1063296"},{"upvote_count":"1","comment_id":"1011890","content":"Selected Answer: A\nOption A is best suited here as if we go with option B it does not ensures that deployment only continue if the new instance is healthy.","poster":"__rajan__","timestamp":"1695182760.0"},{"timestamp":"1680007860.0","poster":"Teraflow","upvote_count":"1","comment_id":"853247","content":"Selected Answer: A\nThe correct answer is A.\n\nPerforming a rolling update with maxSurge set to 1 and maxUnavailable set to 0 ensures that the deployment only continues if the new instance is healthy. The maxSurge parameter ensures that only one new instance is created at a time, while the maxUnavailable parameter ensures that the number of healthy instances does not decrease during the deployment. This will minimize costs by not creating unnecessary instances and will also ensure that the deployment is safe and does not impact the application's availability.\n\nOption B is incorrect because setting maxUnavailable to 1 would mean that one instance will be taken offline at a time, which could impact the application's availability during the deployment.\n\nOptions C and D are incorrect because maxHealthy and maxUnhealthy are not valid parameters for a rolling update.","comments":[{"poster":"closer89","upvote_count":"1","content":"\"costs should be minimized and the number of instances should not increase\"\nits B\nwith maxUnavailable=1, maxSurge=0 - you instance group will be decreased by 1 which is not prohibited","timestamp":"1683484920.0","comment_id":"891631"}]},{"poster":"Foxal","timestamp":"1675344540.0","comment_id":"796050","content":"Selected Answer: A\nthe correct answers is A","upvote_count":"1"},{"content":"the correct answer would be A. Perform a rolling-action with maxSurge set to 1, maxUnavailable set to 0. This will minimize costs by only creating one new instance at a time, and will only continue the deployment if the new instance is healthy, ensuring a consistent latency experience for end users.\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_surge","comment_id":"769198","comments":[{"content":"Option B is not a correct solution because setting maxSurge to 0 and maxUnavailable to 1 will not allow the deployment to continue if the new instance is not healthy. Option C is not a correct solution because maxHealthy and maxUnhealthy are not valid options for rolling-actions. Option D is not a correct solution because setting maxHealthy to 0 and maxUnhealthy to 1 will not allow the deployment to continue if the new instance is not healthy.","poster":"omermahgoub","comment_id":"769199","timestamp":"1673166420.0","upvote_count":"1"}],"poster":"omermahgoub","upvote_count":"2","timestamp":"1673166360.0"},{"content":"Selected Answer: B\nThis is the best site here.\nhttps://tech-lab.sios.jp/archives/18553\nThe site is in Japanese, so please translate and read it.","timestamp":"1665491820.0","upvote_count":"2","comment_id":"692107","poster":"tab02733"},{"timestamp":"1660974720.0","comment_id":"649232","upvote_count":"2","content":"Selected Answer: B\nB is correct","poster":"tomato123"},{"comment_id":"609878","poster":"cloud_enth0325","timestamp":"1654022940.0","upvote_count":"1","content":"Selected Answer: B\nmaxSurge specifies the maximum number (or percentage) of pods above the specified number of replicas (is the maximum number of new pods that will be created at a time) and maxUnavailable is the maximum number of old pods that will be deleted at a time.\n\nmaxSurge = 0 would mean no extra pods with be created."},{"comment_id":"589220","timestamp":"1650531300.0","content":"i'll go with A( number of instances should not increase--- for this number shouldn't increase than target size i think, it doesn't mean no new instances should be created,\nthey are asking in question, like when new instance is being created, so surge >0,)\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_unavailable\n\nIf you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running.","upvote_count":"2","poster":"yogi_508"},{"upvote_count":"2","content":"Selected Answer: B\nAs others suggested, B is the correct option.\n\nI am adding this to highlight the community choice.","comment_id":"578110","poster":"morenocasado","timestamp":"1648625160.0"},{"poster":"GCPCloudArchitectUser","comment_id":"557642","content":"Selected Answer: B\nYes it should be B as question states deployment should stop if it is unhealthy… the only we can happen is to make it to 0 for maxSurge =1","timestamp":"1645997760.0","upvote_count":"1","comments":[{"comments":[{"upvote_count":"1","poster":"GCPCloudArchitectUser","comments":[{"upvote_count":"1","comment_id":"557652","poster":"GCPCloudArchitectUser","comments":[{"timestamp":"1645998300.0","comment_id":"557654","poster":"GCPCloudArchitectUser","content":"That will be B","upvote_count":"1"}],"timestamp":"1645998240.0","content":"Note: If you set both maxSurge and maxUnavailable properties and both properties resolve to 0, the Updater automatically sets maxUnavailable=1, to ensure that the automated update can always proceed."}],"comment_id":"557651","content":"Ok thanks for link reference \nExcerpt","timestamp":"1645998240.0"}],"timestamp":"1645997940.0","comment_id":"557645","content":"I am confused here … it’s either A or B","upvote_count":"1","poster":"GCPCloudArchitectUser"}]},{"timestamp":"1626746820.0","poster":"celia20200410","comment_id":"409969","content":"ANS: A\n\n\nNote: If you set both maxSurge and maxUnavailable properties and both\nproperties resolve to 0, the Updater automatically sets maxUnavailable=1, to\nensure that the automated update can always proceed.\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_surge\nSetting a higher maxSurge value speeds up your update, at the cost of additional instances, which are billed according to the Compute Engine price sheet.","upvote_count":"2"},{"comment_id":"409967","timestamp":"1626746520.0","poster":"celia20200410","upvote_count":"1","content":"ans: a\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_unavailable\nIf you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running."}],"question_text":"You have an application controlled by a managed instance group. When you deploy a new version of the application, costs should be minimized and the number of instances should not increase. You want to ensure that, when each new instance is created, the deployment only continues if the new instance is healthy.\nWhat should you do?","answer_images":[]},{"id":"Jh6VRZpzOteAkiPGy4xo","answer":"B","answer_ET":"B","answer_description":"","unix_timestamp":1624369800,"question_id":305,"isMC":true,"discussion":[{"comment_id":"1249699","comments":[{"poster":"thewalker","timestamp":"1721227740.0","comment_id":"1249700","content":"Let's look at why the other options are less secure:\n\nA. Use HTTP signed URLs to securely provide access to the required resources: HTTP signed URLs are useful for providing temporary access to specific resources, but they don't address the core issue of securely storing and managing service account credentials.\nC. Generate a P12 file from the GCP Console after the instance is deployed and copy the credentials to the host instance before starting the application: This approach requires storing credentials on the instance, which is a security risk. It also introduces manual steps that can be error-prone.\nD. Commit the credential JSON file into your application's source repository and have your CI/CD process package it with the software that is deployed to the instance: This is a major security risk. Storing credentials in source code exposes them to anyone with access to the repository.","upvote_count":"1"}],"timestamp":"1721227740.0","content":"Selected Answer: B\nThe most secure approach is B. Use the instance's service account Application Default Credentials to authenticate to the required resources.\n\nHere's why:\n\nApplication Default Credentials (ADC): ADC is a Google Cloud feature that allows applications running on Google Cloud to automatically authenticate using the service account associated with the instance. This eliminates the need to store credentials directly on the instance, reducing security risks.\nNo Manual Credential Management: ADC handles authentication automatically, eliminating the need to manually manage credentials, which can be error-prone and introduce security vulnerabilities.\nKey Rotation: Google Cloud automatically rotates service account keys, further enhancing security.","poster":"thewalker","upvote_count":"1"},{"poster":"__rajan__","content":"Selected Answer: B\nOption B is Correct: This approach ensures that the credentials are securely managed and automatically provided to the instances when needed.","comment_id":"1011892","upvote_count":"1","timestamp":"1695183000.0"},{"poster":"telp","comment_id":"773201","timestamp":"1673506380.0","upvote_count":"1","content":"Selected Answer: B\nAnswer B because best practice is to not store file with account service information when possible. With compute engine, the account service of the vm can be used to call google api if the roles are added to this account service."},{"upvote_count":"3","poster":"omermahgoub","timestamp":"1673166480.0","comment_id":"769200","content":"B. Use the instance's service account Application Default Credentials to authenticate to the required resources.\n\nUsing the instance's service account Application Default Credentials is the most secure method for distributing credentials to the host instances. This method allows the instance to automatically authenticate with the required resources using the instance's built-in service account, without requiring the credentials to be stored on the instance or transmitted over the network. This eliminates the risk of the credentials being compromised or exposed. Additionally, this method is the most convenient, as it requires no manual steps to set up the credentials on the instance."},{"timestamp":"1660974720.0","comment_id":"649233","content":"Selected Answer: B\nI think B is correct","upvote_count":"2","poster":"tomato123"},{"poster":"cloud_enth0325","content":"Selected Answer: B\nI'm also considering this part -- \"distribute these credentials to the host instances as securely as possible\"\n\nThis falls under B.","upvote_count":"1","timestamp":"1654023300.0","comment_id":"609880"},{"comment_id":"608421","poster":"[Removed]","upvote_count":"1","timestamp":"1653746820.0","content":"Selected Answer: C\nYour application requires service accounts to be authenticated to GCP products via credentials stored on its host Compute Engine virtual machine instances. \nThe application requires the credentials to be stored on the VM instance, so I think the application code points to a file stored in the Instance."},{"poster":"woriheck93","content":"Answer is B\n\nhttps://cloud.google.com/docs/authentication/production#automatically\n\nIf the environment variable GOOGLE_APPLICATION_CREDENTIALS isn't set, ADC uses the service account that is attached to the resource that is running your code.","comment_id":"439450","timestamp":"1630814340.0","upvote_count":"4"},{"comment_id":"403027","poster":"syu31svc","upvote_count":"2","comments":[{"content":"https://cloud.google.com/compute/docs/api/how-tos/authorization:\n\"If you run applications on your Compute Engine instances, application default credentials can get credentials through built-in service accounts\"\n\nAnswer is B not C","timestamp":"1627705020.0","poster":"syu31svc","comment_id":"417698","upvote_count":"4"}],"content":"\"authenticated to GCP\" is the key part of the qn\n\nhttps://cloud.google.com/iam/docs/creating-managing-service-account-keys:\n\"To use a service account from outside of Google Cloud, such as on other platforms or on-premises, you must first establish the identity of the service account\"\n\"You can create service account keys in JSON or PKCS#12 (P12) format. \"\n\nC is the answer","timestamp":"1625878260.0"},{"content":"Only C sounds right","poster":"ralf_cc","upvote_count":"1","comment_id":"388037","timestamp":"1624369800.0"}],"choices":{"A":"Use HTTP signed URLs to securely provide access to the required resources.","D":"Commit the credential JSON file into your application's source repository, and have your CI/CD process package it with the software that is deployed to the instance.","C":"Generate a P12 file from the GCP Console after the instance is deployed, and copy the credentials to the host instance before starting the application.","B":"Use the instance's service account Application Default Credentials to authenticate to the required resources."},"topic":"1","url":"https://www.examtopics.com/discussions/google/view/55845-exam-professional-cloud-developer-topic-1-question-69/","answer_images":[],"question_images":[],"exam_id":7,"question_text":"Your application requires service accounts to be authenticated to GCP products via credentials stored on its host Compute Engine virtual machine instances. You want to distribute these credentials to the host instances as securely as possible.\nWhat should you do?","timestamp":"2021-06-22 15:50:00","answers_community":["B (75%)","13%","13%"]}],"exam":{"name":"Professional Cloud Developer","provider":"Google","isMCOnly":false,"numberOfQuestions":338,"isBeta":false,"isImplemented":true,"lastUpdated":"11 Apr 2025","id":7},"currentPage":61},"__N_SSP":true}