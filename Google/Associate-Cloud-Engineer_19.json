{"pageProps":{"questions":[{"id":"RYfhYK5O8rxTNI4CnSL0","question_images":[],"choices":{"D":"Grant yourself the IAM role of Cloud Spanner Admin.","B":"Configure your Cloud Spanner instance to be multi-regional.","A":"Enable the Cloud Spanner API.","C":"Create a new VPC network with subnetworks in all desired regions."},"timestamp":"2022-04-26 16:05:00","question_id":91,"answer_ET":"A","answer_description":"","exam_id":1,"question_text":"You have just created a new project which will be used to deploy a globally distributed application. You will use Cloud Spanner for data storage. You want to create a Cloud Spanner instance. You want to perform the first step in preparation of creating the instance. What should you do?","answers_community":["A (81%)","Other"],"discussion":[{"content":"A is right\nhttps://cloud.google.com/spanner/docs/getting-started/set-up","comment_id":"615944","upvote_count":"11","poster":"AzureDP900","timestamp":"1655152440.0"},{"timestamp":"1654020780.0","comment_id":"609864","upvote_count":"7","content":"Selected Answer: A\nIf you click on Create instance, the message is show in bottom: Cloud Spanner API for your project has been enabled.","poster":"pfabio"},{"comment_id":"1326795","upvote_count":"1","timestamp":"1734260520.0","poster":"halifax","content":"A<------- is the correct first step. why? Because 99% of cloud deployment is done programmatically using IaC, such as Terraform or Google's own IaC, and for that reason alone \"Enable the Cloud Spanner API\" is a must!!"},{"timestamp":"1727879640.0","comment_id":"1292458","poster":"denno22","upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/spanner/docs/getting-started/set-up"},{"timestamp":"1703871120.0","poster":"Akinzoa","comment_id":"1108951","upvote_count":"1","content":"B looks more like it. \nWhen creating a Cloud Spanner instance, you configure the instance details first before enabling the Cloud Spanner API, if its not enabled by default i.e. if this is the first time you are using Cloud Spanner in your project."},{"timestamp":"1695297420.0","upvote_count":"2","content":"Selected Answer: A\nIts definitely A. Link: https://cloud.google.com/spanner/docs/quickstart-console","poster":"joao_01","comment_id":"1013046"},{"timestamp":"1693965600.0","poster":"Captain1212","content":"Selected Answer: A\nAnswer A is correct before you do anything first you needto enable the API of that particulr service","comment_id":"1000085","upvote_count":"1"},{"poster":"Capability","timestamp":"1673621700.0","comment_id":"774602","content":"Selected Answer: A\nhttps://cloud.google.com/spanner/docs/quickstart-console?_ga=2.68426577.-1975890344.1661276010&_gac=1.161955406.1673625078.Cj0KCQiAn4SeBhCwARIsANeF9DJxfolckwcRZqaOS7Rem2pzXWGmlBaLlxK4hHSe3YZ4DtE5oHHKVMQaArPUEALw_wcB#:~:text=see%20Pricing.-,Before%20you%20begin,Enable%20the%20Cloud%20Spanner%20API,-Create%20an%20instance","upvote_count":"1"},{"upvote_count":"1","content":"A is right\nhttps://cloud.google.com/spanner/docs/quickstart-console?_ga=2.68426577.-1975890344.1661276010&_gac=1.161955406.1673625078.Cj0KCQiAn4SeBhCwARIsANeF9DJxfolckwcRZqaOS7Rem2pzXWGmlBaLlxK4hHSe3YZ4DtE5oHHKVMQaArPUEALw_wcB#:~:text=see%20Pricing.-,Before%20you%20begin,Enable%20the%20Cloud%20Spanner%20API,-Create%20an%20instance","comment_id":"774599","poster":"Capability","timestamp":"1673621640.0"},{"poster":"raaad","comment_id":"746139","upvote_count":"1","content":"Selected Answer: A\nTry the scenario yourself. Its A","timestamp":"1671112080.0"},{"poster":"snkhatri","content":"Selected Answer: A\nA seems right","comment_id":"658464","upvote_count":"1","timestamp":"1662208920.0"},{"timestamp":"1659790080.0","content":"Selected Answer: B\nAnswer must be B , here is why?\nI was confused between A and B but I tested this by creating a new project, when you go to spanner and click on create a spanner instance it automatically enables the API for you and you can all see this activity on the notification panel on the top right along with the visibility of this message clearly on that instance page as well, First it will auto-enable the API and then It will give you an option to select multi-region, Now the questions say your first step that has to be multi-region since enabling the API was done by google automatically and none other options makes sense here.","upvote_count":"3","comment_id":"643386","poster":"bobthebuilder55110","comments":[{"timestamp":"1659790740.0","poster":"bobthebuilder55110","upvote_count":"8","comment_id":"643392","content":"Correction : It's A \nSince it does not specify if we are using command line tool or UI, if you are using command line tool then you will have to enable this."}]},{"comments":[{"content":"This is TRUE. \nVerified this:\n- Go to your GCP project and verify that \"Cloud Spanner API\" is NOT enabled.\n- Go to Cloud Spanner. Click \"Create Instance\".\n- Check back again the \"Cloud Spanner API\" , you will see that status is \"API Enabled\".","poster":"mav3r1ck","timestamp":"1659814260.0","comment_id":"643489","upvote_count":"3"}],"upvote_count":"2","timestamp":"1656063840.0","comment_id":"621543","poster":"mani098","content":"Selected Answer: C\nbecause Api auto enabled when you click create new instance on cloud spanner UI"},{"timestamp":"1653388980.0","comments":[{"comments":[{"comment_id":"674734","poster":"adarsh4503","timestamp":"1663728120.0","content":"If you used CLI then the API would have to be enabled manually.","upvote_count":"2"}],"comment_id":"643388","upvote_count":"1","timestamp":"1659790140.0","content":"How ? I tested this as well and It auto enables the API. Unless this is a new feature by google and the question is old then I am not sure.","poster":"bobthebuilder55110"}],"poster":"Rutu_98","upvote_count":"4","comment_id":"606633","content":"Selected Answer: A\nAnswer : A --> Tested"},{"upvote_count":"2","content":"Selected Answer: A\nEnabling API is the first step","poster":"Random_Mane","comment_id":"595949","timestamp":"1651479240.0"},{"upvote_count":"1","timestamp":"1651250160.0","comment_id":"594595","poster":"jblima","content":"Selected Answer: A\nA - I tested..."},{"content":"A:-tested it need to Enable Api first","upvote_count":"1","timestamp":"1651217220.0","poster":"parag09","comment_id":"594290"},{"content":"shoudnt the ans be enable cloud spanner API.Option A","poster":"sivasan","timestamp":"1650984180.0","comment_id":"592513","upvote_count":"1"},{"upvote_count":"2","timestamp":"1650981900.0","comment_id":"592495","content":"A before anything else","poster":"PAUGURU"}],"answer":"A","answer_images":[],"topic":"1","unix_timestamp":1650981900,"url":"https://www.examtopics.com/discussions/google/view/74600-exam-associate-cloud-engineer-topic-1-question-180/","isMC":true},{"id":"XVk5eya0EEpHI3HgfX5L","answers_community":["C (100%)"],"question_id":92,"topic":"1","unix_timestamp":1662137880,"exam_id":1,"choices":{"C":"Enable the compute googleapis.com API.","B":"Create a VPC network in the project.","D":"Grant yourself the IAM role of Computer Admin.","A":"Create a Cloud Monitoring Workspace."},"answer_description":"","answer_images":[],"answer":"C","url":"https://www.examtopics.com/discussions/google/view/79493-exam-associate-cloud-engineer-topic-1-question-181/","question_text":"You have created a new project in Google Cloud through the gcloud command line interface (CLI) and linked a billing account. You need to create a new Compute\nEngine instance using the CLI. You need to perform the prerequisite steps. What should you do?","isMC":true,"discussion":[{"content":"Selected Answer: C\nnothing can be done before activating the API","comment_id":"670653","upvote_count":"5","poster":"sylva91","timestamp":"1663319880.0"},{"timestamp":"1722000840.0","upvote_count":"2","comment_id":"1255736","content":"agree with C, there are few API enabled by default and the Compute engine API is not a part of them. do not be confused, personally, I thought this API should be part of the basic APIs enabled but it it not.","poster":"yehia2221"},{"poster":"PiperMe","content":"Selected Answer: C\nThe compute.googleapis.com API must be enabled in your project before you can utilize Compute Engine features and issue commands to create instances.","timestamp":"1709324100.0","upvote_count":"3","comment_id":"1163728"},{"comment_id":"783295","comments":[{"upvote_count":"2","poster":"lummy","comment_id":"851914","content":"I believe you can make use of the default vpc","timestamp":"1679904840.0"},{"comment_id":"898017","content":"When you create a new project on the GCP, a default VPC network is automatically created for you.","timestamp":"1684120380.0","comments":[{"timestamp":"1685989980.0","poster":"Kyle1776","upvote_count":"1","content":"Yeah, but who uses the default VPC and CIDR ranges? Technically you could, but its not best practice and RARELY would fit in with a companies existing infrastructure.","comment_id":"915681"}],"poster":"Mike_SG","upvote_count":"2"}],"content":"Why not B? Can you create Compute Engine instance without assigning it it VPC?","upvote_count":"1","poster":"michalmrozik","timestamp":"1674306180.0"},{"timestamp":"1671476280.0","poster":"roaming_panda","upvote_count":"3","content":"api > iam role .i vote for C !!","comment_id":"750170"},{"poster":"zellck","upvote_count":"2","comment_id":"669076","timestamp":"1663163700.0","content":"Selected Answer: C\nC is the obvious answer."},{"poster":"lll_bbb","timestamp":"1662487080.0","upvote_count":"3","comment_id":"661541","content":"Selected Answer: C\napi first"},{"poster":"snkhatri","upvote_count":"2","content":"Selected Answer: C\nC the compute googleapis.com API","comment_id":"658467","timestamp":"1662208980.0"},{"comment_id":"657628","poster":"Nishanth222","timestamp":"1662137880.0","content":"Must be C","upvote_count":"2"}],"question_images":[],"timestamp":"2022-09-02 18:58:00","answer_ET":"C"},{"id":"S9D1Lcf7n0eARLu7xn5i","answers_community":["C (60%)","A (40%)"],"answer":"C","url":"https://www.examtopics.com/discussions/google/view/79790-exam-associate-cloud-engineer-topic-1-question-182/","exam_id":1,"topic":"1","question_text":"Your company has developed a new application that consists of multiple microservices. You want to deploy the application to Google Kubernetes Engine (GKE), and you want to ensure that the cluster can scale as more applications are deployed in the future. You want to avoid manual intervention when each new application is deployed. What should you do?","answer_images":[],"choices":{"A":"Deploy the application on GKE, and add a HorizontalPodAutoscaler to the deployment.","C":"Create a GKE cluster with autoscaling enabled on the node pool. Set a minimum and maximum for the size of the node pool.","D":"Create a separate node pool for each application, and deploy each application to its dedicated node pool.","B":"Deploy the application on GKE, and add a VerticalPodAutoscaler to the deployment."},"isMC":true,"unix_timestamp":1662209040,"question_images":[],"answer_description":"","answer_ET":"C","timestamp":"2022-09-03 14:44:00","discussion":[{"upvote_count":"10","poster":"efar_cloud","timestamp":"1686316380.0","content":"Answer is C\nThe key point is \"ensure that the CLUSTER can scale\"\nA- HorizontalPodAutoscaler - ensures to scale the number of pods\nwhile \nC- Create a GKE cluster with autoscaling enabled on the node pool. Set a minimum and maximum for the size of the node pool.\nensures to scale the number of nodes in the cluster.\n\nSo the answer is C.","comment_id":"919340"},{"timestamp":"1687645560.0","upvote_count":"5","comment_id":"932965","poster":"WendyLC","content":"Selected Answer: C\nC is the right choice... See this for reference https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#fine-tune_gke_autoscaling\n\nA- HorizontalPodAutoscaler - it is best suited for stateless workers that can spin up quickly to react to usage spikes, and shut down gracefully to avoid workload instability."},{"comment_id":"1326810","upvote_count":"1","content":"Selected Answer: C\nMaybe ChatGPT never had the opportunity to get the Google Cloud $300 free tier ;-) \nHorizontalPodAutoscaler and VerticalPodAutoscaler are good for existing pods. \nCan you create a new node with HorizontalPodAutoscaler or VerticalPodAutoscaler?","poster":"halifax","timestamp":"1734262380.0"},{"poster":"denno22","timestamp":"1727937780.0","comment_id":"1292685","upvote_count":"1","content":"Selected Answer: C\nc"},{"timestamp":"1726949100.0","comment_id":"1287448","upvote_count":"1","content":"Selected Answer: A\nA is right ans","poster":"RKS_2021"},{"comment_id":"1255743","upvote_count":"1","timestamp":"1722001440.0","poster":"yehia2221","content":"Answer is C:\nthe HPA is used in for scaling a deployment (an application), but here, the question is asking to scale the cluster when new applications are being added which have different and independent deployments, we have scaling at cluster level, then at deployment level either horizontally or vertically."},{"content":"Selected Answer: A\nIn the context of deploying a new application and ensuring future scalability with minimal manual intervention, focusing on pod scalability is indeed fundamental. This is accurately addressed by option A (Deploy the application on GKE, and add a HorizontalPodAutoscaler to the deployment).\nHowever, it's also important to have node autoscaling enabled (as mentioned in option C) to ensure that the cluster can accommodate the scaling pods. Both pod and node scaling are important for a fully scalable solution, but the immediate focus when deploying a new application is typically on pod configuration and scaling.","comment_id":"1111572","upvote_count":"2","comments":[{"poster":"PiperMe","timestamp":"1709325000.0","content":"This is incorrect. Horizontal Pod Autoscalers scale based on pod-level metrics such as CPU. While useful, HPAs don't directly address the need to add more nodes if the underlying infrastructure is at capacity. The answer is C which provides the most effective and streamlined way to achieve automatic cluster-level scaling in a GKE environment hosting multiple microservices.","comment_id":"1163738","upvote_count":"1"}],"timestamp":"1704166740.0","poster":"Cynthia2023"},{"poster":"MrJkr","comment_id":"915005","upvote_count":"2","timestamp":"1685929740.0","content":"Selected Answer: A\nIts A,\nWhen you first deploy your workload to a Kubernetes cluster, you may not be sure about its resource requirements and how those requirements might change depending on usage patterns, external dependencies, or other factors. Horizontal Pod autoscaling helps to ensure that your workload functions consistently in different situations, and allows you to control costs by only paying for extra capacity when you need it."},{"comment_id":"875324","content":"Selected Answer: A\ni think it is A \"you want to ensure that the cluster can scale as more applications are deployed in the future.\"","timestamp":"1681971060.0","upvote_count":"2","poster":"sabrinakloud"},{"content":"Selected Answer: C\noption C","comment_id":"873486","upvote_count":"2","comments":[],"timestamp":"1681812840.0","poster":"sabrinakloud"},{"comment_id":"862390","poster":"dobberzoon","upvote_count":"1","timestamp":"1680719880.0","content":"Selected Answer: A\nNot knowing how many pods, and like nooneknows said, chatgpt... A is correct."},{"comment_id":"861606","upvote_count":"2","poster":"nooneknows","content":"Chat GPT said A is the Answer!","comments":[{"poster":"Hatem9","content":"Gemini (google AI) said C","comment_id":"1352589","timestamp":"1738868100.0","upvote_count":"1"}],"timestamp":"1680651900.0"},{"timestamp":"1679905020.0","upvote_count":"4","content":"i believe A is the answer, you cant figure out how many nodes you will need in the future...how you gotta set a maximum","comment_id":"851918","poster":"lummy"},{"comment_id":"815298","timestamp":"1676900640.0","poster":"abdelsha","upvote_count":"2","content":"How do you set the maximum number of nodes and you do not know how you app will scale in the future? I think A is more accurate here."},{"comment_id":"759950","timestamp":"1672238160.0","content":"Selected Answer: A\nless manual intervention","upvote_count":"4","poster":"xmh5025"},{"upvote_count":"1","comment_id":"700262","timestamp":"1666296000.0","content":"Selected Answer: C\nC is the right choice... See this for reference https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler","poster":"diasporabro"},{"comment_id":"690116","poster":"Charumathi","content":"Selected Answer: C\nC is the correct answer, you can enable the cluster autoscaling in node pool by specifying min and max node size.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler#adding_a_node_pool_with_autoscaling","timestamp":"1665315540.0","upvote_count":"1"},{"poster":"ale_brd_111","content":"Selected Answer: C\nit's mentioning \"the cluster can scale\" the answer is C","timestamp":"1664531700.0","upvote_count":"3","comment_id":"683447"},{"timestamp":"1663501980.0","poster":"hanskristian","upvote_count":"1","content":"I Think it should be C","comment_id":"672310"},{"comment_id":"671957","content":"Selected Answer: C\nC is my answer. You need to scale the cluster, not the pod.","upvote_count":"4","poster":"zellck","timestamp":"1663471740.0"},{"upvote_count":"2","content":"Selected Answer: A\nA\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler","poster":"manjtrade2","comment_id":"670678","timestamp":"1663322520.0"},{"timestamp":"1663322400.0","poster":"manjtrade2","upvote_count":"2","content":"ITts A\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler","comment_id":"670677"},{"upvote_count":"3","timestamp":"1662209040.0","content":"Selected Answer: C\nC looks fine to me","comment_id":"658469","poster":"snkhatri"}],"question_id":93},{"id":"7Wakwy6TrF4D48XyWV7Z","answer_description":"","answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/74599-exam-associate-cloud-engineer-topic-1-question-183/","answer_images":[],"exam_id":1,"question_images":[],"answer":"C","question_text":"You need to manage a third-party application that will run on a Compute Engine instance. Other Compute Engine instances are already running with default configuration. Application installation files are hosted on Cloud Storage. You need to access these files from the new instance without allowing other virtual machines (VMs) to access these files. What should you do?","discussion":[{"timestamp":"1692890400.0","upvote_count":"9","comment_id":"651339","content":"Selected Answer: C\n\"without allowing other instances\" , the other instances are created with default compute engine service account. So you must create a new independant service account","poster":"VietmanOfficiel"},{"comment_id":"1002187","poster":"scanner2","upvote_count":"3","timestamp":"1725776400.0","content":"Selected Answer: C\nC is correct."},{"upvote_count":"3","poster":"gcpreviewer","comment_id":"680090","content":"Selected Answer: C\nC is the clear choice. Want to create a new service account instead of using the default and grant it permissions in cloud storage. Straightforward C.","timestamp":"1695756000.0"},{"content":"Selected Answer: C\nC is right","comment_id":"670679","upvote_count":"1","poster":"manjtrade2","timestamp":"1694875320.0"},{"upvote_count":"1","poster":"snkhatri","timestamp":"1693745100.0","content":"Selected Answer: C\nC seems right to me","comment_id":"658471"},{"timestamp":"1686687840.0","poster":"AzureDP900","upvote_count":"2","comment_id":"615941","content":"C\nhttps://cloud.google.com/iam/docs/best-practices-for-using-and-managing-service-accounts \nIf an application uses third-party or custom identities and needs to access a resource, such as a BigQuery dataset or a Cloud Storage bucket, it must perform a transition between principals. Because Google Cloud APIs don't recognize third-party or custom identities, the application can't propagate the end-user's identity to BigQuery or Cloud Storage. Instead, the application has to perform the access by using a different Google identity."},{"upvote_count":"1","poster":"KRIV_1","timestamp":"1684832700.0","content":"Although C is the correct answer notice that, as Google recommend, you first need to grant the service account the required permission before attach it to a resource.","comment_id":"605936"},{"timestamp":"1683060360.0","upvote_count":"4","comment_id":"596210","poster":"JelloMan","content":"Selected Answer: C\nC all the way. Restricts access to other VMs since they won’t have the new service account you have associated with your new VM"},{"timestamp":"1682965380.0","content":"Selected Answer: C\nC, other VMs will run as default service account.","upvote_count":"2","comment_id":"595696","poster":"amindbesideitself"},{"poster":"Akash7","upvote_count":"2","comment_id":"592686","content":"C is correct as the other vms have default service accounts.","timestamp":"1682545080.0"},{"comment_id":"592482","timestamp":"1682517000.0","upvote_count":"2","poster":"PAUGURU","content":"Selected Answer: C\nC, using Default account makes the storage visible to other machines"}],"timestamp":"2022-04-26 15:50:00","question_id":94,"isMC":true,"topic":"1","unix_timestamp":1650981000,"answers_community":["C (100%)"],"choices":{"C":"Create a new service account and assign this service account to the new instance. Grant the service account permissions on Cloud Storage.","B":"Create the instance with the default Compute Engine service account. Add metadata to the objects on Cloud Storage that matches the metadata on the new instance.","A":"Create the instance with the default Compute Engine service account. Grant the service account permissions on Cloud Storage.","D":"Create a new service account and assign this service account to the new instance. Add metadata to the objects on Cloud Storage that matches the metadata on the new instance."}},{"id":"dy0hzRWInkBq8HaJDajU","isMC":true,"exam_id":1,"question_id":95,"question_text":"You need to configure optimal data storage for files stored in Cloud Storage for minimal cost. The files are used in a mission-critical analytics pipeline that is used continually. The users are in Boston, MA (United States). What should you do?","answer_description":"","discussion":[{"comments":[{"upvote_count":"2","timestamp":"1659932100.0","poster":"ryumada","comment_id":"643934","content":"At the first point in this documentation says that dual-regional storage is used for business continuity and disaster recovery. Disaster can affect to a regional architecture. I think it's make sense to use dual-regional storage for this case. Also, dual-regional storage is cheaper than multi-regional. \n\nhttps://cloud.google.com/storage/docs/dual-regions#use-dual-region-storage"},{"upvote_count":"4","timestamp":"1659814920.0","comment_id":"643493","comments":[{"timestamp":"1667139360.0","content":"Dual region is expensive than multi-region. (Also mentioned in the documentation: https://cloud.google.com/storage/docs/locations)\nWhen we set objects to be multi-regional, we get to decide/shuffle the data around at will to meet our storage needs. When you take that control away from us, it reduces the flexibility of our systems, making it more expensive to operate.","upvote_count":"6","poster":"Aninina","comment_id":"707851"}],"poster":"mav3r1ck","content":"Keywords: minimal cost and mission-critical\nLooks like people are just looking to be on the cost side. You need to meet both.\n\nIn this case, it needs to be \"dual-region\". This is much cheaper than storage in \"multi-region\" which is obviously not in the choices."}],"upvote_count":"35","comment_id":"606746","timestamp":"1653399840.0","poster":"akshaychavan7","content":"Selected Answer: D\nMission critical is the keyword here which specifies that we need to have a multi-regional backup of the data to survive any regional failures.\nSo option D is the correct choice here."},{"content":"Selected Answer: B\nContinuous access to data means Standard since all of the other options are for infrequently accessed storage (Nearline, Coldline, Archive). Since no other regions are mentioned, single region is best in this case","poster":"JelloMan","upvote_count":"24","comment_id":"596211","comments":[{"upvote_count":"9","content":"And beacuse single region is \"costly-effective\".","poster":"KRIV_1","comment_id":"605939","timestamp":"1653296880.0"}],"timestamp":"1651524540.0"},{"content":"Selected Answer: B\ni go with B keywords:\nStandard storage covers performance \nRegional covers cost and location to users","timestamp":"1738869120.0","comment_id":"1352609","upvote_count":"1","poster":"Hatem9"},{"poster":"kamee15","upvote_count":"2","comment_id":"1336406","content":"Selected Answer: B\nFurther explanation: \nWhy not the other options?\n• A. Regional + Nearline storage class: Nearline storage is designed for infrequently accessed data and would incur retrieval costs and higher latency, which is not suitable for continually used pipelines.\n• C. Dual-regional + Nearline storage class: Dual-regional storage is unnecessary if the use case does not require geo-redundancy, and Nearline storage is unsuitable for frequent access.\n• D. Dual-regional + Standard storage class: While the Standard storage class is appropriate, dual-regional storage adds unnecessary cost for geo-redundancy, which is not required for a region-specific use case.\nConclusion:\nOption B ensures low latency, high performance, and cost-efficiency by storing frequently accessed data in a regional bucket close to the users.","timestamp":"1736001660.0"},{"comment_id":"1336404","upvote_count":"2","timestamp":"1736001480.0","poster":"kamee15","content":"Selected Answer: B\nB. Configure regional storage for the region closest to the users. Configure a Standard storage class.\nReason:\n• The Standard storage class is optimized for frequently accessed data, making it ideal for mission-critical analytics pipelines that are used continually.\n• Regional storage in a region close to the users (e.g., in or near Boston, MA) minimizes latency, providing faster access to the data."},{"content":"Selected Answer: B\nThis is another badly worded question, but I think, I will vote for B. Here is why:\nThe users are in Boston, MA (United States) = a single region \nMission critical = Cloud storage (famous for reliability )\n\nAt minimal cost that is used continually = standard storage class","comment_id":"1326889","upvote_count":"1","timestamp":"1734271200.0","poster":"halifax"},{"poster":"Ciupaz","timestamp":"1729870800.0","content":"Selected Answer: D\nHere's why D is the optimal choice:\n\nDual-regional storage:\n\nProvides high availability across two regions\nIdeal for mission-critical workloads\nMinimizes latency for Boston users by selecting nearby regions\nProvides geographic redundancy\n\nStandard storage class:\n\nOptimal for frequently accessed data (\"used continually\")\nNo additional latency for access\nNo additional retrieval costs\nIdeal for continuously running analytics pipelines","upvote_count":"2","comment_id":"1302912"},{"comment_id":"1292689","content":"Selected Answer: D\nMission-critcal","upvote_count":"1","timestamp":"1727938200.0","poster":"denno22"},{"upvote_count":"1","comment_id":"1288914","timestamp":"1727248200.0","content":"Selected Answer: D\nI will go for D","poster":"Enamfrancis"},{"upvote_count":"3","poster":"iooj","timestamp":"1725999600.0","content":"I need a spaceship, but please make it from the cheapest materials, my mission is very critical","comment_id":"1281776"},{"comment_id":"1153653","content":"Selected Answer: B\nI vote for B because of the keywords: miniaml-cost. The question even specified the single region that the use in, dual region wouldnt match the minimal cost requirement.","poster":"blackBeard33","comments":[{"poster":"kuracpalac","content":"That was my thinking as well","upvote_count":"1","timestamp":"1710334440.0","comment_id":"1172558"}],"timestamp":"1708307280.0","upvote_count":"4"},{"timestamp":"1705143420.0","poster":"sinh","upvote_count":"2","comment_id":"1121528","content":"Selected Answer: D\nmission-critical = dual-regional"},{"content":"Selected Answer: D\nSLA with dual-region is higher than single region, which sounds normal :\nhttps://cloud.google.com/storage/sla\nSo, as this platform is mission-critical, we have to maximize the SLA of the storage, so answer D.","upvote_count":"1","comment_id":"1114324","poster":"c2e9cb4","timestamp":"1704438420.0"},{"content":"Selected Answer: B\nDual-Regional Storage: While dual-regional storage offers higher availability and redundancy by storing data in two geographic locations, it's typically more expensive than regional storage. For mission-critical applications where users are primarily in one geographic area (Boston), the added cost and complexity of dual-regional storage may not be necessary.","poster":"Cynthia2023","timestamp":"1704167460.0","upvote_count":"4","comment_id":"1111575"},{"timestamp":"1700642340.0","comment_id":"1077135","poster":"roronovazoro","upvote_count":"1","comments":[{"content":"C is not correct due to using Nearline Storage.","comment_id":"1172559","timestamp":"1710334500.0","poster":"kuracpalac","upvote_count":"1"}],"content":"Selected Answer: C\ni think C"},{"content":"Selected Answer: B\nThe correct answer is B. Configure regional storage for the region closest to the users. Configure a Standard storage class.\nThe files are used in a mission-critical analytics pipeline that is used continually. This means that the files need to be available quickly and reliably. Regional storage is the best option for this because it ensures that the files are stored in the same region as the users. This will minimize latency and ensure that the files are available quickly.\nThe Standard storage class is the best option for this because it provides a good balance between cost and performance. Nearline storage is a lower-cost option, but it is also slower. Standard storage is a good option for files that are used frequently, but not constantly.\nThe other options are not as good for this scenario. Dual-regional storage is more expensive than regional storage, and it does not provide any performance benefits. Coldline storage is the lowest-cost option, but it is also the slowest.","comment_id":"1061138","upvote_count":"3","timestamp":"1698994920.0","poster":"thanab"},{"poster":"VijKall","timestamp":"1698978840.0","upvote_count":"2","content":"Selected Answer: B\nI will go with B, as both dual-region and regional has 4 9's availability.\nhttps://cloud.google.com/storage/docs/locations","comment_id":"1061033"},{"content":"Selected Answer: B\nCorrect answer is B\nRequirement is low cost and mission critical. It needs low latency which by using region storage will be achieved. \nDual Storage is more expensive then multi regional. There is no need for dual storage in this case. If there will be a zonal failure it can be handled by regional storage.\nUsing Dual storage here will increase cost.","timestamp":"1696341180.0","poster":"SinghAnc","comment_id":"1023996","upvote_count":"3"},{"upvote_count":"4","content":"Selected Answer: B\nB is correct answer. Cloud storage provides 11 9's durability which is safe for mission critical files. Plus the question provides the users location to be a single region, and minimal cost should be there.","comment_id":"1002190","poster":"scanner2","timestamp":"1694154240.0"},{"upvote_count":"4","timestamp":"1693991940.0","comment_id":"1000440","content":"Selected Answer: B\nBecause Google Cloud Storage is 11 Nines durable (99.999999999%) its pretty safe for mission critical data (ref: https://cloud.google.com/blog/products/storage-data-transfer/understanding-cloud-storage-11-9s-durability-target). \nBUT the AVAILABILITY SLA on single region is 99.9% and for dual region its 99.95% (ONLY 0.05% better)... For the additional cost - dual region offers MINIMAL additional availability (ref: https://cloud.google.com/storage/docs/storage-classes#standard)","poster":"NoCrapEva","comments":[{"upvote_count":"1","timestamp":"1709741100.0","comment_id":"1167329","content":"This is the best point brought up by anyone here. The 11 9's makes it both cost effective and great for mission-critical data. Well done.","poster":"PiperMe"}]},{"poster":"gpais","timestamp":"1692791520.0","upvote_count":"3","content":"Selected Answer: B\nhttps://cloud.google.com/storage/docs/locations#location_recommendations","comment_id":"988260"},{"content":"Selected Answer: B\nC is not correct because it would be more expensive than B. Dual-regional storage is designed for data that needs to be available in multiple regions. It is more expensive than regional storage, but it offers higher availability.\n\nIn this case, the files are used in a mission-critical analytics pipeline that is used continually. This means that the files need to be available with low latency. However, the users are only in Boston, MA (United States). Therefore, there is no need for the files to be available in multiple regions.\n\nTherefore, the best option is to configure regional storage for the region closest to the users (us-central1) and configure a Standard storage class. This will be less expensive than dual-regional storage and will still provide the required low latency.","comment_id":"899365","upvote_count":"3","poster":"Mo73w","timestamp":"1684255380.0"},{"poster":"Jelly_Wang","content":"Selected Answer: B\nAnswer is B. If you go with D, you must be a newbie trying to pass the exam and then hopefully get some practice with your next employer. Regional storage doesn't mean no automated failover and failback. It does have automated failover and failback on zonal failure, which is more than enough to cover a single city (Boston). Plus, I work for a provincial government who do business in a single province. We use regional storage. Man if you work for a company that run business in a single city and you go with multi regional storage, finger cross for yourself.","upvote_count":"5","timestamp":"1683922980.0","comment_id":"896232"},{"timestamp":"1682213940.0","upvote_count":"1","poster":"vivekvj","content":"Selected Answer: D\nD. because of mission critical so high availability must be considered as well.","comment_id":"877760"},{"upvote_count":"1","poster":"Technobie","content":"D : \nDual-region storage is a ‘best of both worlds’ solution, as it provides the ability to scale to TB per second (like regional), but also provides a second copy of data in a second region, protecting against regional outages. Similar to regional storage, dual-region storage provides customers with an environment to drive high-throughput analytical workloads by co-locating compute and storage in two regions of their choice.","comment_id":"875297","timestamp":"1681968180.0"},{"poster":"dobberzoon","content":"Selected Answer: D\nMission critical made me opt D instead of B. You want availability.","upvote_count":"2","timestamp":"1680720120.0","comment_id":"862394"},{"poster":"ashtonez","upvote_count":"2","content":"Selected Answer: D\nThe answer is D, key here is we need standard , so B or D, but its a mission critical that must run continuously, so it should be always running, even if there is some region disaster, and the only option to avoid going down on one region disaster is D with dual region standard","comment_id":"834912","timestamp":"1678448820.0"},{"poster":"JC0926","upvote_count":"2","content":"Selected Answer: B\nBoston, MA (United States).","timestamp":"1676860260.0","comment_id":"814752"},{"upvote_count":"1","poster":"nigdyniezapomne","timestamp":"1675803300.0","comment_id":"801445","content":"Selected Answer: D\nstandard & dual-region"},{"poster":"JoniMONI","upvote_count":"2","content":"B. Configure regional storage for the region closest to the users. Configure a Standard storage class.\n\nRegional storage stores data in a specific geographic region, which minimizes latency for users in that region. The Standard storage class is a good option for files that are used frequently, as it provides high availability and performance. Nearline storage is a lower-cost option that is best suited for data that is accessed less frequently. Configuring dual-regional storage and a Nearline storage class would not be appropriate for a mission-critical analytics pipeline that is used continually.","comment_id":"783614","timestamp":"1674323100.0"},{"poster":"robertozga","content":"Answer is B:\nDual-region storage lets users specify two regions within the same continent. The users are in here in one region. The mission-critical app refers to the autoscaling, which can be done in Cloud Spanner.\nPlease refer to: https://cloud.google.com/storage/docs/dual-regions","comment_id":"777853","timestamp":"1673882400.0","upvote_count":"2"},{"comment_id":"777229","timestamp":"1673830920.0","upvote_count":"2","content":"Selected Answer: B\nI think it is B","poster":"Nazz1977"},{"poster":"Sabarno","upvote_count":"1","timestamp":"1673180040.0","comment_id":"769367","content":"Selected Answer: D\nSInce it is a \"Mission Critical State\" we need to have dual region back up. Look for the Key Word, that's it."},{"upvote_count":"1","content":"Selected Answer: D\nkey words: \"mission critical\"","poster":"alex000","timestamp":"1672848000.0","comment_id":"765849"},{"content":"Selected Answer: D\nmission critical","upvote_count":"1","poster":"adtu","comment_id":"761702","timestamp":"1672379040.0"},{"content":"Selected Answer: B\nIt's only one region, I go for B","upvote_count":"2","comment_id":"744526","timestamp":"1670972820.0","poster":"davidsalomon"},{"upvote_count":"2","poster":"Nazz1977","content":"Selected Answer: B\nalso minimal cost","comment_id":"742302","timestamp":"1670809260.0"},{"comment_id":"738569","comments":[{"content":"What did you choose finally?","poster":"AL1123","timestamp":"1680439560.0","upvote_count":"1","comment_id":"858838"}],"upvote_count":"2","content":"How do I know which answer to take. Voting and actual answer never matches. Has anyone passed exam using this question. Should I use comunity answer or most voted. Pls help. I have exam after 2 days","poster":"Priyadev","timestamp":"1670468460.0"},{"comment_id":"737480","content":"Selected Answer: B\nAnswer should be B , as question also mentions \" The users are in Boston, MA (United States)\"","poster":"ankyt9","timestamp":"1670392740.0","upvote_count":"2"},{"comment_id":"732526","timestamp":"1669892580.0","content":"Selected Answer: B\nTricky one. I ended up voting for B after reading the docs: https://cloud.google.com/storage/docs/locations#location_recommendations\n\n\"To maximize performance and lower your total cost of ownership, co-locate your data and compute in the same region(s). Regional and dual-region locations are both suitable for this purpose.\"\n\nWhat's more, Analytics is mentioned as a suitable use case for both regional and dual-region. This, plus \"Lowest data storage cost\" being one of the features in regional tips the scale towards B.","upvote_count":"3","poster":"pepill0"},{"timestamp":"1669477320.0","poster":"AllMaaya","upvote_count":"2","content":"Hope the regional storage( option B) gives multi zone replication.","comment_id":"727619"},{"content":"This is a tricky question. two things to understand from here,\n\n1) minimal cost \n2) Used Continually,\n\nso it should be Regional Standard ( Option B)\n\nMost comments highlighting \"Mission Critical\" as keyword. If you check closely, its \"mission-critical analysis pipeline\", so its definitely B.","upvote_count":"1","timestamp":"1668877020.0","poster":"Viggy1212","comment_id":"722120"},{"upvote_count":"1","comment_id":"715508","timestamp":"1668108780.0","poster":"1209apl","content":"Selected Answer: D\nOption D:\n\"mission-critical\", resources should be available the most time possible, dual-region gives you more availability than regional storage.\n\"Used continually\" = Standard Storage class."},{"poster":"AMIGOY","comment_id":"709675","content":"Selected Answer: B\nThe data is mission critical but it is still redundant across at least 2 zones with single region","upvote_count":"4","timestamp":"1667374140.0"},{"upvote_count":"1","poster":"hiromi","content":"Selected Answer: D\nD is right. \nKeywords:\n- continually -> Standard\n- mission-critical analytics -> dual-regional","timestamp":"1667291160.0","comment_id":"708960"},{"poster":"PKookNN","content":"Selected Answer: D\nI was thinking B until I saw 'mission critical' + cost optimise + constant access = D is the best answer","upvote_count":"1","timestamp":"1666608840.0","comment_id":"702932"},{"timestamp":"1666098300.0","comment_id":"698256","poster":"nosense","upvote_count":"1","content":"Selected Answer: D\nd is correctly"},{"timestamp":"1665051360.0","content":"Selected Answer: D\nMission critical is the keyword so D","comment_id":"687666","upvote_count":"1","poster":"DualCore573"},{"poster":"Cornholio_LMC","timestamp":"1664037120.0","content":"had this question today","comment_id":"678002","upvote_count":"2"},{"poster":"snkhatri","timestamp":"1662209220.0","comment_id":"658474","upvote_count":"2","content":"Selected Answer: B\nB, as there will be multiple zone and standard"},{"poster":"VietmanOfficiel","content":"Selected Answer: B\nRegion contains multi zones so if one zone fail other zones have replicas","timestamp":"1661354640.0","comment_id":"651341","upvote_count":"4"},{"poster":"its_another_samsun","content":"Selected Answer: D\nKey words -- \"minimal cost\", \"mission-critical\", \"continually\"\n\nif u go with B, 2nd condition is not met & also if a region fails 3rd condition also fails\nif u go with D, only thing is there will be slight difference in cost","timestamp":"1660354380.0","comment_id":"646077","upvote_count":"1"},{"content":"Selected Answer: B\nThe data is mission critical but it is still redundant across at least 2 zones with single region storage, so this is sufficient for redundancy , the questions clearly mentions about lowering the cost as well, so B.\nhttps://cloud.google.com/storage/docs/locations","upvote_count":"2","comment_id":"643402","timestamp":"1659792480.0","comments":[{"upvote_count":"1","content":"Correction: D, since that same link talk about why dual-region is great for analytics and that's what we are doing in this questions along with it says \"You need to configure optimal data storage for files stored in Cloud Storage for minimal cost\" so, for minimal cost(it does not say that we want to reduce the cost, it asking the lowest as possible while maintaining the business needs) but it also has to be optimal for business at the same time.","poster":"bobthebuilder55110","comment_id":"643405","timestamp":"1659792900.0"}],"poster":"bobthebuilder55110"},{"content":"Selected Answer: B\nKeyword: minimal cost, used continually","comment_id":"634980","upvote_count":"2","timestamp":"1658467740.0","poster":"Sans_12450"},{"poster":"nginx_aws","timestamp":"1658161860.0","comment_id":"633089","content":"It's B see the scenraio here https://cloud.google.com/architecture/building-and-orchestrating-data-analytics-and-machine-learning-pipeline#objectives","upvote_count":"1"},{"content":"Selected Answer: B\nRegional buckets can tolerate zonal failures and can be used for mission-critical deployments. The actual use case for multi-regional is when you want to reduce latency and move data closer to user.","timestamp":"1657826100.0","comment_id":"631511","upvote_count":"2","poster":"csrazdan"},{"upvote_count":"1","content":"Selected Answer: B\n\"minimal cost\". \"The files are used in pipeline that is used continually\" .\"The users are in Boston, MA (United States)\". \nFor these reasons, i think the correct answer is: B","timestamp":"1657698720.0","comment_id":"630835","poster":"amenur"},{"content":"B and D are correct \n\nB for low cost\nD for mission critical","timestamp":"1657665420.0","comment_id":"630711","poster":"patashish","upvote_count":"1"},{"content":"B for sure...","upvote_count":"1","comment_id":"630225","timestamp":"1657579620.0","poster":"samsonakala"},{"poster":"Roro_Brother","comments":[{"upvote_count":"1","comment_id":"629396","poster":"csrazdan","timestamp":"1657415760.0","content":"But then the requirement of keeping the cost low is not met. B is the right option."}],"comment_id":"626934","content":"Selected Answer: D\nFor sur, it's D beacuse it's a mission critical workload.","upvote_count":"1","timestamp":"1656928680.0"},{"poster":"AzureDP900","timestamp":"1655151600.0","upvote_count":"3","content":"D is the winner for sure :)... one might think about B however it's not satisficing mission critical workloads.","comment_id":"615939"},{"poster":"Tirthankar17","upvote_count":"2","content":"Selected Answer: D\nMission critical means backup is required in case of failure. My Vote is with D.","timestamp":"1654864380.0","comment_id":"614542"},{"upvote_count":"3","timestamp":"1653937260.0","content":"Selected Answer: D\nI go with D - \"mission critical\" sounds like \"must\". \"Costly-effective\" is a \"can\"","comment_id":"609428","poster":"SergijF"},{"comment_id":"609174","timestamp":"1653912360.0","upvote_count":"2","poster":"pluiedust","content":"Selected Answer: D\nMISSION CRITICAL!!!!!!\nIt should be D, not B"},{"upvote_count":"4","content":"Selected Answer: D\nIt says \"mission-critical analytics pipeline\" it means it has to have multi-regional backup, that's the only reason I would go with \"D\".","poster":"GokuPancho","timestamp":"1652384400.0","comment_id":"600792"},{"upvote_count":"4","timestamp":"1650980820.0","content":"Selected Answer: B\nB is correct, only one region for cost and standard for continuous accessed data","poster":"PAUGURU","comment_id":"592479"}],"answer_ET":"B","timestamp":"2022-04-26 15:47:00","topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/74598-exam-associate-cloud-engineer-topic-1-question-184/","answers_community":["B (59%)","D (41%)","1%"],"unix_timestamp":1650980820,"question_images":[],"choices":{"B":"Configure regional storage for the region closest to the users. Configure a Standard storage class.","C":"Configure dual-regional storage for the dual region closest to the users. Configure a Nearline storage class.","A":"Configure regional storage for the region closest to the users. Configure a Nearline storage class.","D":"Configure dual-regional storage for the dual region closest to the users. Configure a Standard storage class."},"answer":"B"}],"exam":{"provider":"Google","numberOfQuestions":285,"lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"Associate Cloud Engineer","id":1,"isImplemented":true,"isBeta":false},"currentPage":19},"__N_SSP":true}