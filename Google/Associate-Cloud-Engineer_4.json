{"pageProps":{"questions":[{"id":"Cn6AEv6S1OOSf1wWAADs","choices":{"D":"Create a Cloud Task to create an image and export it to Cloud Storage.","B":"Create a snapshot schedule for the disk using the desired interval.","C":"Create a cron job to create a new disk from the disk using gcloud.","A":"Create a Cloud Function to create an instance template."},"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/25016-exam-associate-cloud-engineer-topic-1-question-112/","answer_images":[],"answer":"B","topic":"1","question_images":[],"discussion":[{"comment_id":"162491","timestamp":"1613860620.0","upvote_count":"25","poster":"ESP_SAP","content":"Correct Answer (B):\nBest practices for persistent disk snapshots\nYou can create persistent disk snapshots at any time, but you can create snapshots more quickly and with greater reliability if you use the following best practices.\n\nCreating frequent snapshots efficiently\nUse snapshots to manage your data efficiently.\n\nCreate a snapshot of your data on a regular schedule to minimize data loss due to unexpected failure.\n\nImprove performance by eliminating excessive snapshot downloads and by creating an image and reusing it.\n\nSet your snapshot schedule to off-peak hours to reduce snapshot time.\n\nSnapshot frequency limits\nCreating snapshots from persistent disks\nYou can snapshot your disks at most once every 10 minutes. If you want to issue a burst of requests to snapshot your disks, you can issue at most 6 requests in 60 minutes.\n\nIf the limit is exceeded, the operation fails and returns the following error:\n\nhttps://cloud.google.com/compute/docs/disks/snapshot-best-practices"},{"timestamp":"1610035020.0","comments":[{"comment_id":"166127","upvote_count":"3","comments":[{"poster":"Ridhanya","comment_id":"495834","timestamp":"1654587480.0","content":"In snapshot schedule, there is autodelete and you can specify the days after which auto delete can happen","upvote_count":"6"},{"timestamp":"1615142460.0","poster":"Ale1973","content":"Snapshots and disks are independent objects con GCP, you could create a snapshot form disk and then delete the disk, the snapshot will stay in place. Actually, you could use this snapshot to create a new disk, assign to another VM, mount it, and use it (all the information that the original disk had at the time of the snapshot will still be there).","upvote_count":"6","comment_id":"175327"}],"timestamp":"1614278580.0","content":"Question: One cannot delete the old disk when using snapshot, right?","poster":"stepkurniawan"}],"comment_id":"129010","content":"B is correct for this question","poster":"DarioFama23","upvote_count":"21"},{"upvote_count":"3","timestamp":"1732610940.0","content":"2020 is B. Now, 2024, better solution es backup&dr","poster":"ccpmad","comment_id":"1218762"},{"poster":"Ankit_EC_ran","upvote_count":"2","content":"Selected Answer: B\nThe correct answer is B. Automatic snapshot and deletion as per the need.","timestamp":"1726125180.0","comment_id":"1171564"},{"comment_id":"1082029","content":"Selected Answer: B\nB\nthe others make no sense at all","upvote_count":"2","timestamp":"1716848940.0","poster":"kelliot"},{"upvote_count":"1","timestamp":"1715000400.0","comment_id":"1063960","poster":"BAofBK","content":"The correct answer is B"},{"timestamp":"1709830080.0","upvote_count":"1","comment_id":"1001668","poster":"scanner2","content":"Selected Answer: B\ncreate a snapshot schedule to regularly and automatically back up your zonal and regional persistent disks. Use snapshot schedules as a best practice to back up your Compute Engine workloads.\nA snapshot retention policy defines how long you want to keep your snapshots.\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots#retention_policy"},{"timestamp":"1709457660.0","upvote_count":"1","comment_id":"997413","poster":"Captain1212","content":"Selected Answer: B\nB is the correct Answer, as you can create the snapshot as per your requirment"},{"poster":"abirroy","comment_id":"643827","timestamp":"1675805100.0","content":"Selected Answer: B\nCreate a snapshot schedule for the disk using the desired interval.","upvote_count":"2"},{"content":"Selected Answer: B\nSnapshot is a better option because they are incremental and you can configure them to consolidate and delete snapshots that are not required for recovery. Image can also provide this functionality but the image is full backup which is inefficient in cases where the content of the file system is changing frequently.","poster":"csrazdan","timestamp":"1673216340.0","upvote_count":"1","comment_id":"628892"},{"content":"B is right","poster":"AzureDP900","upvote_count":"2","timestamp":"1671833340.0","comment_id":"621271"},{"poster":"haroldbenites","comment_id":"612112","upvote_count":"1","content":"Go for B","timestamp":"1670298900.0"},{"upvote_count":"2","poster":"Rukman","content":"Selected Answer: B\nAns: B","comment_id":"570848","timestamp":"1663554720.0"},{"comment_id":"561917","upvote_count":"4","timestamp":"1662451620.0","poster":"ryzior","content":"say no more:\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots\n\"Use snapshot schedules as a best practice to back up your Compute Engine workloads.\""},{"timestamp":"1652985240.0","comment_id":"482081","poster":"alaahakim","upvote_count":"1","content":"The right Ans is : B"},{"content":"B is correct","timestamp":"1636862340.0","poster":"mcaromit","comment_id":"356808","upvote_count":"1"},{"poster":"[Removed]","comment_id":"320065","timestamp":"1632558300.0","upvote_count":"1","content":"B is correct. Create a snapshot schedule for the disk using the desired interval."},{"poster":"cloud__guru","comment_id":"312430","timestamp":"1631795520.0","content":"B is correct due to snapshots being the perfect solution to the problem question","upvote_count":"1"},{"upvote_count":"2","timestamp":"1629899700.0","content":"B. Create a snapshot schedule for the disk using the desired interval.","comment_id":"299190","poster":"GCP_Student1"},{"timestamp":"1629176940.0","content":"B is Correct","poster":"EABDAJA","upvote_count":"2","comment_id":"292339"},{"poster":"Vikash211982","content":"B is correct","comment_id":"281641","timestamp":"1627875240.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"248728","content":"B 200%","timestamp":"1624192500.0","poster":"Bhagirathi"},{"poster":"hiteshrup","timestamp":"1615382400.0","upvote_count":"3","content":"Answer B is correct. Snapshot can be best option available from listed option\n- Option A is just creating instance template and not taking boot disk backup \n- Option C is to create a disk, but to run this corn job, you need another gcloud resource which is not cost effective and introducing another single failure point. \n- Option D is not a definite not a good cost effective solution.","comment_id":"177063"},{"upvote_count":"2","timestamp":"1615303200.0","content":"Definitely B. That's Google's way of helping you. Why do C or D, which requires a lot of custom coding and configuration when B is pre-built? \n\nA - that just misses the mark. An instnace template have any backup data. Even if it did, it's still a manual way to go, just like C and D. \n\nDefinitely B","poster":"[Removed]","comment_id":"176548"}],"unix_timestamp":1594130220,"answer_ET":"B","question_id":16,"question_text":"You have a workload running on Compute Engine that is critical to your business. You want to ensure that the data on the boot disk of this workload is backed up regularly. You need to be able to restore a backup as quickly as possible in case of disaster. You also want older backups to be cleaned automatically to save on cost. You want to follow Google-recommended practices. What should you do?","timestamp":"2020-07-07 15:57:00","answers_community":["B (100%)"],"exam_id":1,"isMC":true},{"id":"bGvoDIsS5DZEemSuUpug","answer_ET":"B","discussion":[{"upvote_count":"67","comments":[{"comment_id":"165296","comments":[{"comment_id":"165297","upvote_count":"22","timestamp":"1614185700.0","poster":"ESP_SAP","content":"Correct Answer is (B): (Continuation).\nThe table below explains IAM logging roles that an Organization Administrator can grant to the service account used by the dashboard, \nas well as the resource level at which the role is granted: \n\nlogging.viewer Organization Dashboard service account The logging.viewer role permits the service account to read the Admin Activity logs in Cloud Logging.\nbigquery.dataViewer BigQuery dataset Dashboard service account The bigquery.dataViewer role permits the service account used by the dashboard application \nto read the exported Admin Activity logs."}],"upvote_count":"25","poster":"ESP_SAP","content":"Correct Answer is (B): (Continuation).\nScenario: External auditors\nIn this scenario, audit logs for an organization are aggregated and exported to a central sink location. A third-party auditor is granted access several \ntimes a year to review the organization's audit logs. The auditor is not authorized to view PII data in the Admin Activity logs. \n\nDuring normal access, the auditors' Google group is only granted access to view the historic logs stored in BigQuery. If any anomalies are discovered, \nthe group is granted permission to view the actual Cloud Logging Admin Activity logs via the dashboard's elevated access mode. At the end of each audit period,\nthe group's access is then revoked.\n\nData is redacted using Cloud DLP before being made accessible for viewing via the dashboard application.","timestamp":"1614185700.0"}],"timestamp":"1614185520.0","poster":"ESP_SAP","comment_id":"165295","content":"Correct Answer is (B):\n\nBackground\nGoogle Cloud provides Cloud Audit Logs, which is an integral part of Cloud Logging. It consists of two log streams for each project: Admin Activity and Data Access.\nAdmin Activity logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. Admin Activity logs are always enabled. There is no charge for your Admin Activity audit logs.\nData Access logs record API calls that create, modify, or read user-provided data. Data Access audit logs are disabled by default because they can be large.\n\n\nlogging.viewer: The logging.viewer role gives the security admin team the ability to view the Admin Activity logs.\nlogging.privateLogViewer : The logging.privateLogViewer role gives the ability to view the Data Access logs."},{"comment_id":"128639","upvote_count":"17","poster":"DarioFama23","timestamp":"1610003940.0","content":"for me B is the correct answer..","comments":[{"timestamp":"1620721740.0","upvote_count":"8","comment_id":"217210","poster":"Eshkrkrkr","content":"Yes, B is correct because:\n1) Question doesn't ask us to export and store logs for any long period of time.\n2) Custom role with only logging.privateLogEntries.list permission won't let the auditor to access Log Exporer at all (https://cloud.google.com/logging/docs/access-control#console_permissions - Minimal read-only access: logging.logEntries.list)"}]},{"poster":"MANGANDA","content":"Selected Answer: D\nn this scenario, virtual machines in the web-applications project need access to BigQuery datasets in the crm-databases-proj project.","comment_id":"1332323","upvote_count":"1","timestamp":"1735291440.0"},{"comment_id":"1111271","poster":"Cynthia2023","upvote_count":"2","timestamp":"1719837840.0","content":"Selected Answer: B\nThere is no need to export logs to Cloud Storage for the auditor to review them unless there's a specific requirement or preference for reviewing them outside the GCP environment. The Logging service provides the necessary tools for log viewing and querying within the console.\n\nDirecting the auditor to review logs for changes to Cloud IAM policy is part of their duties to ensure that the IAM policies have been correctly managed and modified. This does not require a separate permission as the privateLogViewer role already provides the necessary access."},{"upvote_count":"2","comment_id":"1063979","timestamp":"1715001900.0","poster":"BAofBK","content":"The correct answer is B"},{"content":"No logs in cloud storage since reviewer won't have access to it","comment_id":"1045625","upvote_count":"1","poster":"ziomek666","timestamp":"1713337860.0"},{"content":"Selected Answer: B\n- The Logs Viewer role (roles/logging.viewer) gives you read-only access to Admin Activity, Policy Denied, and System Event audit logs. If you have just this role, you cannot view Data Access audit logs that are in the _Default bucket.\n- The Private Logs Viewer role(roles/logging.privateLogViewer) includes the permissions contained in roles/logging.viewer, plus the ability to read Data Access audit logs in the _Default bucket.\nTherefore, no need to export logs to Cloud storage explicitly, the _Default bucket sink access is already provided from the above role.\nhttps://cloud.google.com/iam/docs/audit-logging#audit_log_permissions","comment_id":"1001673","poster":"scanner2","timestamp":"1709830560.0","upvote_count":"1"},{"comment_id":"997418","poster":"Captain1212","content":"Selected Answer: B\nb is the correect answer","timestamp":"1709457840.0","upvote_count":"1"},{"timestamp":"1708845540.0","upvote_count":"1","content":"B. Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy.","comment_id":"989679","poster":"Neha_Pallavi"},{"upvote_count":"1","content":"This answer is similar to answer choice B, but it suggests creating a custom role for the auditor that includes the \"logging.privateLogEntries.list\" permission. While this would provide the auditor with access to the necessary logs, directing them to also review Cloud IAM policy logs is not relevant to their request. Therefore, this answer is also not correct.","comment_id":"970798","poster":"MilanRajGupta","timestamp":"1706948220.0"},{"upvote_count":"1","content":"Correct Ans: B","timestamp":"1706948100.0","comment_id":"970795","poster":"MilanRajGupta"},{"timestamp":"1687753020.0","content":"I also think B","comment_id":"757109","poster":"anjanc","upvote_count":"1"},{"upvote_count":"1","poster":"AzureDP900","timestamp":"1671833460.0","content":"B is right. Similar practice question in tutorials dojo","comment_id":"621272"},{"comment_id":"606488","content":"Selected Answer: B\nB is correct ans","timestamp":"1669274820.0","poster":"Rutu_98","upvote_count":"1"},{"upvote_count":"3","comment_id":"553197","content":"Selected Answer: B\nB is the correct answer","timestamp":"1661104740.0","poster":"luciorifa"},{"comment_id":"504965","upvote_count":"1","poster":"lazyabhi606","content":"Selected Answer: B\nCorrect Answer is (B)","timestamp":"1655644740.0"},{"content":"Correct Answer is B.","timestamp":"1651980300.0","poster":"maggieli","comment_id":"474124","upvote_count":"1"},{"timestamp":"1649550660.0","content":"A is the correct answer. Exporting logging data to Cloud Storage is ideal, and 'Cloud IAM Policy' is not mentioned in this question.","comment_id":"459816","upvote_count":"3","poster":"ankatsu2010"},{"poster":"mcaromit","upvote_count":"1","comment_id":"356810","timestamp":"1636862400.0","content":"B is correct"},{"upvote_count":"1","content":"B is correct. Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy.","comment_id":"320078","timestamp":"1632559380.0","poster":"[Removed]"},{"comment_id":"319219","content":"answer is B","timestamp":"1632488340.0","poster":"yuvi69","upvote_count":"2"},{"content":"why here cloud storage is mentioned ? they are mentioning only access and why this is coming in the middle","timestamp":"1631756640.0","comment_id":"311973","poster":"Sathya22","upvote_count":"3"},{"content":"B is correct","poster":"EABDAJA","upvote_count":"1","comment_id":"307068","timestamp":"1631261040.0"},{"timestamp":"1629900360.0","poster":"GCP_Student1","upvote_count":"2","comment_id":"299199","content":"B - Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy."},{"upvote_count":"1","timestamp":"1628058840.0","poster":"victory108","comment_id":"283286","content":"B - Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy."},{"content":"A is correct \nroles/logging.privateLogViewer (Private Logs Viewer) includes roles/logging.viewer, plus the ability to read Access Transparency logs and Data Access audit logs. This role applies only to the _Required and _Default buckets.","poster":"Morgan91","timestamp":"1623494640.0","comment_id":"241590","upvote_count":"2"},{"comment_id":"237981","poster":"Bhagirathi","timestamp":"1623128940.0","upvote_count":"1","content":"it is B .\nNo other choices to make."},{"upvote_count":"1","timestamp":"1622632020.0","poster":"Morgan91","content":"for me b is the correct answer \nroles/logging.privateLogViewer (Private Logs Viewer) includes roles/logging.viewer, plus the ability to read Access Transparency logs and Data Access audit logs. This role applies only to the _Required and _Default buckets.","comment_id":"232982"},{"timestamp":"1620647640.0","content":"I think it's B because question asks to audit all Cloud logs, not only Admin or Data logs - all the logs, that means that custom role with logging.privateLogEntries.list is not enogh, what leads us to ask external audior use Log explorer after we assign him/her a roles/logging.privateLogViewer role. Q doest ask us to store log for any particular time period so there is no need in Cloud storage as well.","comments":[{"upvote_count":"3","content":"To support my claim, why it's not D with custom role. https://cloud.google.com/logging/docs/access-control#console_permissions\nThe following table lists the permissions needed to use the Logs Explorer.\nMinimal read-only access: logging.logEntries.list so if we create custom role with just logging.privateLogEntries.list permission the auditor won't be able to access Log Exporer.","comment_id":"217207","timestamp":"1620721620.0","poster":"Eshkrkrkr"}],"comment_id":"216660","poster":"Eshkrkrkr","upvote_count":"2"},{"upvote_count":"2","content":"A\n\nroles/logging.privateLogViewer Private Logs Viewer Provides permissions of the Logs Viewer role and in addition, provides read-only access to log entries in private logs.\n\nroles/iam.securityReviewer Security Reviewer Provides permissions to list all resources and IAM policies on them.\n\nAs stated in the documentation, privateLogViewer can not access Cloud IAM Policy. \nsource: https://cloud.google.com/iam/docs/understanding-roles","poster":"gcper","comment_id":"212942","timestamp":"1620145440.0"},{"upvote_count":"2","comment_id":"192721","timestamp":"1617517380.0","poster":"RockAJ","content":"For me B"},{"timestamp":"1615942440.0","poster":"rezavage","content":"B is the correct answer because:\nroles/logging.privateLogViewer (Private Logs Viewer) includes roles/logging.viewer, plus the ability to read Access Transparency logs and Data Access audit logs. This role applies only to the _Required and _Default buckets.","upvote_count":"1","comment_id":"180594"},{"timestamp":"1615444560.0","content":"Answer is A - https://cloud.google.com/logging/docs/audit","poster":"nff","comment_id":"177460","upvote_count":"1"},{"comment_id":"177066","timestamp":"1615382880.0","upvote_count":"4","poster":"hiteshrup","content":"Answer is B ofcourse. \n\nWe have two roles which can do job for us. \nlogging.viewer: The logging.viewer role gives the security admin team the ability to view the Admin Activity logs.\nlogging.privateLogViewer : The logging.privateLogViewer role gives the ability to view the Data Access logs. \n\nSo keeping privateLogEntries will solve problem. \n\nNow going on second part of problem, accessing the logs. As per google guidelines, auditor might need to have those logs during year and so audit logs for an organization are aggregated and exported to a central sink location. A third-party auditor is granted access several times a year to review the organization's audit logs. \n\nThis export can be BigQuery, Cloud Stroage. However option given us to use Cloud Storage which can also fine-tune by using cold line can be best solution to the problem."},{"upvote_count":"1","content":"Correct answer is A. https://cloud.google.com/iam/docs/job-functions/auditing","comment_id":"171294","poster":"mexblood1","timestamp":"1614601500.0"},{"content":"I come back because I am not sure with the anwers while I am reading them","poster":"SSPC","upvote_count":"2","timestamp":"1614011880.0","comment_id":"163690"},{"poster":"SSPC","upvote_count":"3","content":"B is the correct answer","comment_id":"163618","timestamp":"1614005340.0"},{"content":"roles/logging.privateLogViewer (Private Logs Viewer) includes roles/logging.viewer, plus the ability to read Access Transparency logs and Data Access audit logs. This role applies only to the _Required and _Default buckets.\nA is the correct answer","poster":"Hjameel","comment_id":"158227","comments":[{"comments":[{"content":"roles/logging.privateLogViewer (Private Logs Viewer) includes roles/logging.viewer, plus the ability to read Access Transparency logs and Data Access audit logs. This role applies only to the _Required and _Default buckets.","comment_id":"161871","poster":"AmitKM","timestamp":"1613786400.0","upvote_count":"1"}],"comment_id":"160387","timestamp":"1613608560.0","poster":"cloudguy1","upvote_count":"8","content":"Sure, and how is the external auditor going to view the logs in Cloud Storage with the role roles/logging.privateLogViewer? answer is B, stop confusing people."}],"timestamp":"1613330040.0","upvote_count":"1"},{"comments":[{"upvote_count":"3","timestamp":"1620721200.0","content":"Question doesn't ask us to export and store logs for any long period of time.","comment_id":"217205","poster":"Eshkrkrkr"}],"timestamp":"1613173320.0","poster":"francisco_guerra","upvote_count":"2","content":"Its A\nOnce your log entries have been exported, access to the exported copies is controlled entirely by IAM permissions and roles on the destinations: Cloud Storage, BigQuery, or Pub/Sub.\nhttps://cloud.google.com/logging/docs/access-control#permissions_and_roles\nand roles/logging.privatelogviewer include the permission \"logging.privateLogEntries.lis\"","comment_id":"156746"},{"timestamp":"1611438900.0","content":"Answer is B - why Cloud Storage in the mix","upvote_count":"3","poster":"samvegas","comment_id":"142281"},{"content":"C : https://cloud.google.com/monitoring/audit-logging","poster":"szakaria","upvote_count":"2","comments":[{"poster":"DarioFama23","upvote_count":"3","content":"I can't understand what means the export on Cloud Storage","comment_id":"131491","timestamp":"1610294340.0"}],"timestamp":"1610167320.0","comment_id":"130285"}],"answer":"B","url":"https://www.examtopics.com/discussions/google/view/24973-exam-associate-cloud-engineer-topic-1-question-113/","exam_id":1,"question_id":17,"question_images":[],"unix_timestamp":1594099140,"choices":{"B":"Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy.","C":"Assign the auditor's IAM user to a custom role that has logging.privateLogEntries.list permission. Perform the export of logs to Cloud Storage.","D":"Assign the auditor's IAM user to a custom role that has logging.privateLogEntries.list permission. Direct the auditor to also review the logs for changes to Cloud IAM policy.","A":"Assign the auditor the IAM role roles/logging.privateLogViewer. Perform the export of logs to Cloud Storage."},"topic":"1","isMC":true,"answers_community":["B (90%)","10%"],"timestamp":"2020-07-07 07:19:00","question_text":"You need to assign a Cloud Identity and Access Management (Cloud IAM) role to an external auditor. The auditor needs to have permissions to review your\nGoogle Cloud Platform (GCP) Audit Logs and also to review your Data Access logs. What should you do?","answer_images":[],"answer_description":""},{"id":"QjCwXp9APlhW0ZPUxlsl","exam_id":1,"question_images":[],"unix_timestamp":1594301760,"url":"https://www.examtopics.com/discussions/google/view/25234-exam-associate-cloud-engineer-topic-1-question-114/","discussion":[{"poster":"Verve","upvote_count":"26","content":"Its B.","comment_id":"147602","timestamp":"1612037880.0"},{"content":"The question is to view log past 60 days. B, c, D talks about deleting an object or truncation of table data","comments":[{"comment_id":"178200","upvote_count":"3","timestamp":"1615558800.0","comments":[{"comments":[{"timestamp":"1616271600.0","poster":"[Removed]","content":"Also by default, you have a lot of flexibility when viewing logging in stack driver , to filter and query.","comment_id":"183222","upvote_count":"2","comments":[{"upvote_count":"3","comments":[{"poster":"[Removed]","upvote_count":"3","timestamp":"1617279780.0","content":"Ur correct so minimally is 30 for data access logs https://cloud.google.com/logging/quotas\nthen B is the way to go.","comment_id":"190950"}],"content":"what about minimum retention is 30 days ? is it true ?","timestamp":"1617027840.0","comment_id":"189725","poster":"xtian2900"}]}],"timestamp":"1615559340.0","comment_id":"178214","upvote_count":"4","content":"Also A specifically talks about aggregation","poster":"[Removed]"}],"content":"Answer should be A","poster":"[Removed]"}],"poster":"[Removed]","upvote_count":"11","timestamp":"1615558740.0","comment_id":"178199"},{"comment_id":"1218759","content":"2024, there is not \"Stackdriver Logging Export, but for 2020 it is B","upvote_count":"3","poster":"ccpmad","timestamp":"1732610700.0"},{"upvote_count":"1","timestamp":"1727434260.0","content":"resource.labels.project_id=\"*\" is not a correct query because \"*\" returns 0 records so option A is not a correct answer","comment_id":"1184100","poster":"IshwarChandra"},{"poster":"Cynthia2023","comments":[{"timestamp":"1719838740.0","comment_id":"1111283","poster":"Cynthia2023","content":"In BigQuery, setting an expiration time for tables can be applied in two contexts:\n\nTable Expiration:\n\nWhen you set a table expiration time at the table level, it applies to the entire table. This means that the entire table will be deleted once the specified expiration time has elapsed since the table's creation time.\nPartition Expiration:\n\nFor partitioned tables, you can set a partition expiration time, which applies to individual partitions within the table. Each partition's data will be deleted once the specified expiration time has elapsed since the creation of that specific partition.\nThis is particularly useful for time-series data, like logs, where you might want to only keep recent data and allow older data to be automatically purged.","upvote_count":"2"}],"timestamp":"1719838680.0","comment_id":"1111280","upvote_count":"3","content":"Selected Answer: B\nWhen it comes to log data, you're typically dealing with high-volume time-series data that is partitioned by time (e.g., by day). In such cases, setting a partition expiration is often more appropriate because it ensures that you're continuously retaining a rolling window of log data (for example, the last 60 days' worth) and automatically purging older data, rather than deleting the entire table at once after a certain period."},{"timestamp":"1717771680.0","poster":"Romio2023","comment_id":"1090448","upvote_count":"2","content":"I dont get the options"},{"comment_id":"1082032","content":"Selected Answer: B\nI guess it's B","poster":"kelliot","upvote_count":"2","timestamp":"1716849120.0"},{"upvote_count":"1","timestamp":"1715005920.0","poster":"BAofBK","content":"The correct answer is B","comment_id":"1064054"},{"comment_id":"1001691","upvote_count":"2","poster":"scanner2","content":"Selected Answer: B\nProvides storage of log entries in BigQuery datasets. You can use big data analysis capabilities on the stored logs. Logging sinks stream logging data into BigQuery in small batches, which lets you query data without running a load job.\nYou can set a default table expiration time at the dataset level, or you can set a table's expiration time when the table is created. A table's expiration time is often referred to as \"time to live\" or TTL. When a table expires, it is deleted along with all of the data it contains.\nhttps://cloud.google.com/logging/docs/export/configure_export_v2#overview\nhttps://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time","timestamp":"1709832000.0"},{"poster":"Captain1212","timestamp":"1709458020.0","content":"Selected Answer: B\nB is thecorrect answer, we can use bq to get 60 days logs and analyse","comment_id":"997424","upvote_count":"1"},{"upvote_count":"1","comment_id":"989671","poster":"Neha_Pallavi","timestamp":"1708844760.0","content":"B. Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days."},{"timestamp":"1696033140.0","content":"Selected Answer: B\nhttps://cloud.google.com/architecture/security-log-analytics","upvote_count":"1","poster":"Prat25200607","comment_id":"856451"},{"poster":"sai_learner","content":"All options are wrong , they are talking about deletion after 60 days, but questions asks us to analyse logs of past 60 days","comment_id":"630764","upvote_count":"5","comments":[{"content":"You are absolutely wrong - meaning of \"past 60 days\" is same as \"last 60 days\" in that sentence.","comment_id":"792552","poster":"FeaRoX","timestamp":"1690698000.0","upvote_count":"1"}],"timestamp":"1673590320.0"},{"timestamp":"1671833580.0","content":"B is right for sure","poster":"AzureDP900","upvote_count":"1","comment_id":"621273"},{"timestamp":"1670674440.0","poster":"Tirthankar17","content":"Selected Answer: B\nB is the correct answer.","upvote_count":"2","comment_id":"614480"},{"upvote_count":"6","comment_id":"460903","content":"I believe B is the answer.\n\nAll that matters in this scenario is the logs for the past 60 days. \nWe can use BigQuery to analyze contents so C is incorrect. We need to configure a BQ as the sink for the logs export so we can query and analyze log data in the future. Therefore D is incorrect.\nhttps://cloud.google.com/logging/docs/audit/best-practices#export-best-practices\n\nSince we only care about the logs within 60 days, we can set the expiration time to 60 to retain only the logs within that time frame. Once data is beyond 60 days old, it wouldn't be included in future analyzations. \nhttps://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time","timestamp":"1649741100.0","comments":[{"poster":"ryzior","timestamp":"1662454380.0","upvote_count":"1","content":"I think here we have the case described in details:\nhttps://cloud.google.com/architecture/exporting-stackdriver-logging-for-security-and-access-analytics","comment_id":"561943"}],"poster":"dttncl"},{"timestamp":"1649556180.0","comment_id":"459841","upvote_count":"1","content":"D should be the correct answer. To 'quickly analyze', you need to use BQ, next, you always need access to the logs 'for past 60days'. This means you have to export logs on a daily basis. You don't want to do this job manually right?","poster":"ankatsu2010","comments":[{"poster":"ankatsu2010","timestamp":"1649557260.0","upvote_count":"3","content":"My apologies, B is correct... 'Sink' can route logging data to BQ automatically.","comment_id":"459844"}]},{"timestamp":"1639579260.0","poster":"AD_0525","comment_id":"382662","upvote_count":"3","content":"B is the correct one, option A does not give you the flexibility to analyze."},{"poster":"mcaromit","upvote_count":"2","timestamp":"1636862520.0","comment_id":"356811","content":"B is correct as analysis of the log contents is a key requirement"},{"timestamp":"1634545560.0","content":"Firstly i though that A was correct but when considering \"default\" retention period which is 30 days, i go for B.\nhttps://cloud.google.com/blog/products/it-ops/best-practices-for-working-with-google-cloud-audit-logging\nLog entries are held in Stackdriver Logging for a limited time known as the retention period. After that, the entries are deleted. To keep log entries longer, you need to export them outside of Stackdriver Logging by configuring log sinks.","upvote_count":"6","poster":"tifo16","comment_id":"338024"},{"comments":[{"comment_id":"335223","upvote_count":"1","poster":"Crad","timestamp":"1634191680.0","content":"The output is in JSON so it's still possible to analyze the logs using BigQuery easily as an external source of data."}],"poster":"Crad","timestamp":"1634191560.0","upvote_count":"1","content":"I think it's C.\nIf you set expiration date to 60days on a table then that table will be deleted after 60 days of creation.\nLet's say you created some logs on 31st day - they'll be gone within 29 days and not 60.","comment_id":"335221"},{"timestamp":"1632559440.0","comment_id":"320079","content":"B is correct. Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days.","poster":"[Removed]","upvote_count":"1"},{"timestamp":"1631995620.0","comment_id":"314425","poster":"GCP_Student1","content":"B. Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days.","upvote_count":"4"},{"timestamp":"1631796120.0","content":"The answer is B because that's the simplest solution. In order to view AND analyze logs quickly as question asks, we need it to be in BigQuery","upvote_count":"1","poster":"cloud__guru","comment_id":"312439"},{"content":"B is correct","poster":"EABDAJA","timestamp":"1631197500.0","comment_id":"306547","upvote_count":"1"},{"comment_id":"244200","poster":"JKRowlings","content":"Ans is B. https://cloud.google.com/solutions/exporting-stackdriver-logging-for-security-and-access-analytics","upvote_count":"2","timestamp":"1623715320.0"},{"upvote_count":"1","comment_id":"237982","poster":"Bhagirathi","content":"It is B. that best fits.","timestamp":"1623129120.0"},{"poster":"hiteshrup","upvote_count":"3","comment_id":"177076","timestamp":"1615384080.0","content":"Option B is my Answer as it has all require. Export Logs + Sink + BigQuery for Analytics + Partition Expiration Time. This is best solution for analytic solution. \n\nFor compliance scenario, option C is better choice \n\nOption A is not best fit as minimum retention is 30 days for Data Access Audit logs in StackDriver.\n\nOption D is not require as Sink will do same job as Cloud Scheduler."},{"upvote_count":"2","comments":[{"poster":"vlodia","comment_id":"130641","content":"Well it seems B because \"need to analyze the data\"","upvote_count":"18","timestamp":"1610206680.0"}],"comment_id":"130640","content":"C better than B","timestamp":"1610206560.0","poster":"vlodia"}],"answer":"B","timestamp":"2020-07-09 15:36:00","question_text":"You are managing several Google Cloud Platform (GCP) projects and need access to all logs for the past 60 days. You want to be able to explore and quickly analyze the log contents. You want to follow Google-recommended practices to obtain the combined logs for all projects. What should you do?","answers_community":["B (100%)"],"answer_images":[],"question_id":18,"isMC":true,"topic":"1","answer_description":"","answer_ET":"B","choices":{"C":"Create a Stackdriver Logging Export with a Sink destination to Cloud Storage. Create a lifecycle rule to delete objects after 60 days.","A":"Navigate to Stackdriver Logging and select resource.labels.project_id=\"*\"","D":"Configure a Cloud Scheduler job to read from Stackdriver and store the logs in BigQuery. Configure the table expiration to 60 days.","B":"Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days."}},{"id":"EdYyk9Id32hUp9FMZoGh","answers_community":["A (100%)"],"question_images":[],"answer_images":[],"exam_id":1,"answer_ET":"A","question_text":"You need to reduce GCP service costs for a division of your company using the fewest possible steps. You need to turn off all configured services in an existing\nGCP project. What should you do?","unix_timestamp":1594105560,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/24983-exam-associate-cloud-engineer-topic-1-question-115/","topic":"1","isMC":true,"choices":{"B":"1. Verify that you are assigned the Project Owners IAM role for this project. 2. Switch to the project in the GCP console, locate the resources and delete them.","A":"1. Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID.","D":"1. Verify that you are assigned the Organizational Administrators IAM role for this project. 2. Switch to the project in the GCP console, locate the resources and delete them.","C":"1. Verify that you are assigned the Organizational Administrator IAM role for this project. 2. Locate the project in the GCP console, enter the project ID and then click Shut down."},"timestamp":"2020-07-07 09:06:00","discussion":[{"content":"for me is A the correct answer","upvote_count":"43","timestamp":"1625641560.0","poster":"DarioFama23","comment_id":"128701"},{"content":"A - I reproduced in my project","poster":"shafiqeee1","comment_id":"135305","timestamp":"1626306420.0","upvote_count":"19"},{"poster":"BAofBK","upvote_count":"1","timestamp":"1730911080.0","comment_id":"1064060","content":"The correct answer is A"},{"poster":"scanner2","timestamp":"1725723000.0","upvote_count":"4","comment_id":"1001693","content":"Selected Answer: A\nhttps://cloud.google.com/resource-manager/docs/access-control-proj#permissions\nhttps://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects"},{"timestamp":"1725348480.0","comment_id":"997426","content":"Selected Answer: A\na is the correct answer","upvote_count":"1","poster":"Captain1212"},{"timestamp":"1724559780.0","comment_id":"989656","content":"Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID.","poster":"Neha_Pallavi","upvote_count":"1"},{"content":"Selected Answer: A\nA is the correct answer","poster":"vinodthakur49","upvote_count":"1","timestamp":"1720644420.0","comment_id":"948380"},{"comment_id":"891606","timestamp":"1715103300.0","content":"Selected Answer: A\nAnswer is A\nhttps://support.google.com/googleapi/answer/6251787?hl=en#zippy=%2Cshut-down-a-project","upvote_count":"2","poster":"Shenannigan"},{"timestamp":"1713418620.0","content":"Selected Answer: A\noption A","upvote_count":"1","comment_id":"873297","poster":"sabrinakloud"},{"poster":"sabrinakloud","comment_id":"873296","upvote_count":"1","timestamp":"1713418560.0","content":"i believe it's option A.\n\nroles/owner Owner All Editor permissions and permissions for the following actions:\nManage roles and permissions for a project and all resources within the project.\nSet up billing for a project."},{"poster":"researched_answer_boi","timestamp":"1705819080.0","content":"According to \"https://cloud.google.com/resource-manager/docs/access-control-org#resourcemanager.organizationAdmin\" and \"https://cloud.google.com/resource-manager/docs/access-control-proj#basic_roles\", only the project owner (and project deleter (roles/resourcemanager.projectDeleter)) can delete a project.\nSo, answer A is the technically correct one.","upvote_count":"1","comment_id":"783060"},{"comment_id":"753591","upvote_count":"2","poster":"roaming_panda","content":"Selected Answer: A\nproject not org , A all d way","timestamp":"1703272140.0"},{"content":"A is right \nHint : You need to turn off all configured services in an ***existing GCP project***.\nSo C and D out from selection","timestamp":"1689114600.0","comment_id":"630219","upvote_count":"2","poster":"patashish"},{"upvote_count":"1","poster":"RanjithK","content":"Selected Answer: A\nTried and tested","comment_id":"626270","timestamp":"1688322360.0"},{"upvote_count":"2","content":"correct ans is A","comment_id":"626167","timestamp":"1688305260.0","poster":"keep_it_on"},{"upvote_count":"2","poster":"taiyi078","content":"https://cloud.google.com/run/docs/tutorials/gcloud\n\nClean up\n\nIn the dialog, type the project ID, and then click Shut down to delete the project.","timestamp":"1687897380.0","comment_id":"623491"},{"upvote_count":"2","poster":"AzureDP900","comment_id":"621275","content":"A is right","timestamp":"1687551360.0"},{"content":"Selected Answer: A\nIt's A\nBecause we have to follow the least required permission.\nAlso here given is that they want to reduce the services for a GCP project so the Project Owner role would be sufficient.","comment_id":"606505","timestamp":"1684907940.0","upvote_count":"3","poster":"Rutu_98"},{"upvote_count":"1","comment_id":"599623","timestamp":"1683727140.0","content":"Selected Answer: A\nanswer is A.","poster":"bigbenben"},{"upvote_count":"1","timestamp":"1682591520.0","content":"Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID.","poster":"rikku33","comment_id":"593058"},{"poster":"crisyeb","comment_id":"567621","timestamp":"1678795800.0","upvote_count":"1","content":"Selected Answer: A\nA right"},{"poster":"dishum","comment_id":"526265","timestamp":"1674009720.0","upvote_count":"3","content":"Answer is C\n\nThe question says ' reduce GCP service costs for a division of your company' means the authority person is going to take few more actions on other projects to reduce costs.\nSo the right permission to have is at the organisational level."},{"comment_id":"493284","poster":"rachee","content":"A - https://cloud.google.com/resource-manager/docs/creating-managing-projects","upvote_count":"2","timestamp":"1670088960.0"},{"content":"Selected Answer: A\nThe Right Ans is : A","comment_id":"482084","upvote_count":"2","poster":"alaahakim","timestamp":"1668890280.0"},{"poster":"maggieli","timestamp":"1667885220.0","content":"Correct Answer is A.","comment_id":"474125","upvote_count":"1"},{"poster":"Rothmansua","content":"But.. there is no \"Project Owner\" role. There is a basic \"Owner\" role.\nThere is also no \"Organizational Administrator\" role, there is \"Organization Administrator\" role isntead. And that role doesn't have project modifying permissions.","timestamp":"1665946500.0","comment_id":"463233","upvote_count":"5"},{"comment_id":"427784","timestamp":"1660952760.0","poster":"sudav","content":"A - The primitive Project Owner role provides permissionst to delete project\nhttps://cloud.google.com/iam/docs/understanding-roles#primitive_roles\nYou can shut down projects using the Cloud Console. When you shut down a project, this immediately happens: All billing and traffic serving stops, You lose access to the project, The owners of the project will be notified and can stop the deletion within 30 days, The project will be scheduled to be deleted after 30 days. However, some resources may be deleted much earlier.","upvote_count":"4"},{"content":"Answer should be A, For project shutdown you don't need any roles in org level.Just replicated in my free trial account, where I don't have any org but able to shutdown a project from console.","comment_id":"382664","poster":"AD_0525","timestamp":"1655296980.0","upvote_count":"4"},{"content":"A is correct","upvote_count":"3","timestamp":"1652493840.0","poster":"mcaromit","comment_id":"356813"},{"content":"A Ref: https://cloud.google.com/resource-manager/docs/organization-resource-management#delete-projects","comment_id":"334398","poster":"noreen","upvote_count":"1","timestamp":"1649823060.0"},{"poster":"[Removed]","upvote_count":"2","content":"The confusion is between A and C, because both can dump a project \nI Prefer C because of the wording \nA says 'click Shut down and then enter the project ID'\nC says 'enter the project ID and then click Shut down'\n\nYou cannot click and then enter something. Hence A should be false","comments":[{"timestamp":"1651573320.0","upvote_count":"3","comment_id":"348514","poster":"FunkyTechnician","content":"Entering the project ID is for CONFIRMATION"},{"upvote_count":"1","timestamp":"1650524460.0","comments":[{"upvote_count":"3","poster":"Tez1","content":"In the GUI you actually enter the project Id, for confirmation, before shutting it down. Which is why A looks to be incorrect.","timestamp":"1653381900.0","comment_id":"365432"}],"poster":"Skiro","content":"After you click \"Shut down\" you should enter project ID for confirmation.","comment_id":"340138"}],"timestamp":"1649588340.0","comment_id":"332464"},{"timestamp":"1648443480.0","comment_id":"322381","upvote_count":"2","content":"Ans) A \nTo shut down a project using the Cloud Console:\n\n Open the Settings page (found under IAM & admin) in the Google Cloud Console.\n\n Open the Settings page\n\n Click Select a project.\n\n Select a project you want to delete, and click Open.\n\n Click Shut down.\n\n Enter the Project ID, then click Shut down.\n\nref: https://cloud.google.com/resource-manager/docs/creating-managing-projects#console_4","poster":"Rajusrinivasa"},{"comments":[{"upvote_count":"1","comment_id":"561950","poster":"ryzior","content":"ite behaves different depending on from where you shut it down\nfrom IAM/settings there is shutdown (assuming current project) then provide ID and click shutdown\nfrom Resource Management : you can choose a project from the list and click DELETE then after providing the project ID -> shutdown\nso it is confusing and I guess this is the main concern of the autor","comments":[{"upvote_count":"1","poster":"ryzior","content":"after reading the question I'd say A is for Resource manager scenario and B for IAM/settings (you need to switch the project) since A does not require to switch between the projects it would be my natural choice :)","comment_id":"561956","timestamp":"1678101060.0"}],"timestamp":"1678100640.0"}],"timestamp":"1648205100.0","content":"A is correct. 1. Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID.","poster":"[Removed]","comment_id":"320081","upvote_count":"2"},{"content":"answer is A","comment_id":"319227","timestamp":"1648134300.0","poster":"yuvi69","upvote_count":"1"},{"content":"orgpolicy.policy.get\nresourcemanager.folders.get\nresourcemanager.folders.getIamPolicy\nresourcemanager.folders.list\nresourcemanager.folders.setIamPolicy\nresourcemanager.organizations.get\nresourcemanager.organizations.getIamPolicy\nresourcemanager.organizations.setIamPolicy\nresourcemanager.projects.get\nresourcemanager.projects.getIamPolicy\nresourcemanager.projects.list\nresourcemanager.projects.setIamPolicy\n\norganization admin doesnt have a delete project permission to that role. Ans: A.","comment_id":"306631","upvote_count":"7","timestamp":"1646853720.0","poster":"ShakthiGCP"},{"upvote_count":"1","comment_id":"306194","comments":[{"comment_id":"340962","poster":"BenAji","content":"you enter the ID after the shutdown of any project!!!. Its a way of re-confirming the exact project","upvote_count":"5","timestamp":"1650622800.0"}],"content":"A is not correct, how can you enter the project ID after shut down. C is correct. 1. Verify that you are assigned the Organizational Administrator IAM role for this project. 2. Locate the project in the GCP console, enter the project ID and then click Shut down.","poster":"EABDAJA","timestamp":"1646810400.0"},{"timestamp":"1645807860.0","poster":"GCP_Student1","upvote_count":"3","comment_id":"299228","content":"A. 1. Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID."},{"comment_id":"283285","timestamp":"1643963580.0","upvote_count":"2","poster":"victory108","content":"A - 1. Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID."},{"upvote_count":"1","content":"To me A is the right answer","poster":"rvgcp","timestamp":"1643569380.0","comment_id":"280128"},{"timestamp":"1640813340.0","content":"Just for the record, despite the role the user should have to excecute the shut down, when you are about to do it, you FIRST click Shut down and then enter the ID, you can try it. Only ans that fits is A.","poster":"Liongeek","comment_id":"255168","upvote_count":"6","comments":[{"timestamp":"1640968560.0","comment_id":"256341","upvote_count":"1","content":"This just cements it","poster":"magistrum"}]},{"comments":[{"poster":"rramos96","timestamp":"1640519820.0","upvote_count":"3","comment_id":"252596","comments":[{"poster":"Sadagopan","timestamp":"1640529540.0","content":"https://cloud.google.com/resource-manager/docs/quickstart-organizations\nOrganization admin also can delete (shutdown) a project","comment_id":"252682","upvote_count":"1"}],"content":"Only project owners can shut down or restore projects. ( https://support.google.com/googleapi/answer/6251787 ) so the correct answer is [ A ]"}],"upvote_count":"1","comment_id":"237996","poster":"Bhagirathi","timestamp":"1638948480.0","content":"It is equally fits both seems A & C works.\nSame time, it is equally confusing ...\n something to be clarified how Organization Admin IAM Role can do and not the Project Owner IAM Role?"},{"content":"A or C - it adds to my understanding ..but at last I am confused than getting any clarity.","comment_id":"237987","upvote_count":"1","comments":[{"comment_id":"422991","content":"When are you not confused -- out of questions 1 to 114 you have been confused lol. Didn't this word 'confused' lose any meaning to you","timestamp":"1660167840.0","upvote_count":"1","poster":"babusartop17"}],"poster":"Bhagirathi","timestamp":"1638947700.0"},{"upvote_count":"2","comment_id":"226808","poster":"devscorpio2001","timestamp":"1637773320.0","content":"\"Shutting down a project in the console releases all resources used within the project. Only project owners can shut down or restore projects.\" \nhttps://support.google.com/googleapi/answer/6251787?hl=en#]"},{"comment_id":"221246","poster":"ayj","upvote_count":"3","comments":[{"poster":"JackGlemins","timestamp":"1645235340.0","content":"The same link say: \nThe Organization resource represents an organization and is the root node in the resource hierarchy. The IAM access control policies applied on the Organization resource apply to all projects (and all resources under the project) in that organization.\n\nhttps://support.google.com/googleapi/answer/7053550?hl=en&ref_topic=7014522\n\nI think the organization owner can shutdown a project too","comment_id":"293878","upvote_count":"1"}],"content":"\"Shutting down a project in the console releases all resources used within the project. Only project owners can shut down or restore projects.\" - https://support.google.com/googleapi/answer/6251787?hl=en \n\nSo it's A","timestamp":"1637167320.0"},{"upvote_count":"3","comment_id":"177089","content":"My answer is C. \n\nThe reasoning is a \"division of your company\" because, you can shutdown project for which you are owning but you can't do shutdown / delete project for you are not a owner. To do that, you need to have organizationRoleAdmin\n\nReference: https://cloud.google.com/iam/docs/understanding-custom-roles#organization_role_administrator_role","timestamp":"1631276280.0","comments":[{"comment_id":"179156","upvote_count":"4","timestamp":"1631607900.0","poster":"[Removed]","content":"The answer is A because although it is telling about the cost of the division it is talking about deleting one projet not multiple projects the I would agree it is C"}],"poster":"hiteshrup"},{"content":"Its A, only the project owner can shutdown the project","poster":"Hjameel","upvote_count":"3","timestamp":"1628961600.0","comment_id":"158229"},{"poster":"XRiddlerX","timestamp":"1627551480.0","comments":[{"poster":"Aaduavi11","content":"need full access, if someone can help","timestamp":"1725807720.0","comment_id":"1002592","upvote_count":"1"}],"comment_id":"146393","content":"I have to go with A as the correct answer according to support.google.com\nURL: https://support.google.com/googleapi/answer/6251787?hl=en#\n\nWhat stands out at me is the paragraph \"Shutting down a project stops all billing and traffic serving, shuts down any Google Cloud Platform App Engine applications, and terminates all Compute Engine instances. All project data associated with Google Cloud and Google APIs services becomes inaccessible.\"","upvote_count":"6"}],"answer":"A","question_id":19},{"id":"HxE9nPHbtU5O8FXfBzvm","question_id":20,"topic":"1","question_text":"You are configuring service accounts for an application that spans multiple projects. Virtual machines (VMs) running in the web-applications project need access to BigQuery datasets in crm-databases-proj. You want to follow Google-recommended practices to give access to the service account in the web-applications project. What should you do?","exam_id":1,"isMC":true,"question_images":[],"timestamp":"2020-07-07 16:05:00","unix_timestamp":1594130700,"answer_ET":"D","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/25018-exam-associate-cloud-engineer-topic-1-question-116/","answer_images":[],"answers_community":["D (56%)","C (44%)"],"choices":{"B":"Give ג€project ownerג€ role to crm-databases-proj and the web-applications project.","C":"Give ג€project ownerג€ role to crm-databases-proj and bigquery.dataViewer role to web-applications.","D":"Give bigquery.dataViewer role to crm-databases-proj and appropriate roles to web-applications.","A":"Give ג€project ownerג€ for web-applications appropriate roles to crm-databases-proj."},"answer":"D","discussion":[{"comments":[{"upvote_count":"4","comment_id":"129501","timestamp":"1594189920.0","content":"U re right, D is the correct answee","poster":"DarioFama23"},{"timestamp":"1618292820.0","upvote_count":"6","comment_id":"334431","poster":"tavva_prudhvi","content":"See the option correctly, as the web app needs access to the big query datasets we have to give access to the web app the data viewer role to only read the datasets! Hence, C"},{"upvote_count":"2","content":"Question didn't specify if the required access is Read only or more, its saying \"access\" which could be write permissions as well. I will go with C","comments":[{"content":"It is D because you're right, the question doesn't specify any specific kind of access, however, we need to follow the principle of least-privilege. Hence, we can only assume that read-only access is needed. \n\nbigquery.dataViewer should be assigned to the group of analysts in the crm-databases-proj project. \nhttps://cloud.google.com/bigquery/docs/access-control-examples#read_access_to_data_in_a_different_project","poster":"dttncl","comment_id":"460907","upvote_count":"10","timestamp":"1634017500.0"},{"upvote_count":"6","poster":"[Removed]","content":"U r right, it D. why to give \"project owner\" as stated on C. correct answer is D","comments":[{"timestamp":"1624614180.0","comments":[{"poster":"S_marquez","comment_id":"572506","content":"You can technically give bigquery.dataviewer to crm-databases-proj service account then create a Key and use that key on the VMs, there for making it correct to use D as answer but is way to dumb I would prefer C BUUUUUUT WHY would I give Project Owner to crm-databases-proj? they really do not evaluate your knowladge","upvote_count":"3","timestamp":"1647895680.0"}],"comment_id":"390302","poster":"GCPACE2020","upvote_count":"9","content":"but why giving bigquery.dataViewer to crm-databases-proj. we should give for web-application."}],"timestamp":"1608890280.0","comment_id":"251974"}],"poster":"[Removed]","comment_id":"251973","timestamp":"1608890100.0"}],"timestamp":"1594153080.0","comment_id":"129258","content":"D cuz u just need read for DB at the other project","poster":"ezat","upvote_count":"31"},{"comment_id":"129018","poster":"DarioFama23","content":"C is correct..","upvote_count":"11","timestamp":"1594130700.0","comments":[{"comment_id":"568357","comments":[{"poster":"Romio2023","timestamp":"1702198860.0","content":"I meet BigQuery the first time ever personly","comment_id":"1092359","upvote_count":"2"}],"timestamp":"1647346740.0","poster":"BigQuery","content":"THAT SO DUM","upvote_count":"5"},{"timestamp":"1624614000.0","poster":"GCPACE2020","content":"But why giving project owner role to crm-databases-proj ?","comment_id":"390301","upvote_count":"3"}]},{"poster":"jlocke","content":"Selected Answer: D\nHere’s how to interpret the wording:\n\"Give bigquery.dataViewer role to crm-databases-proj\" means that you update the IAM policy for the crm-databases-proj project (or specifically the BigQuery datasets within that project) so that the service account (which is part of the web-applications project) gets the BigQuery Data Viewer role.","comment_id":"1357175","upvote_count":"1","timestamp":"1739690940.0"},{"poster":"Izzy55555","upvote_count":"1","content":"Selected Answer: C\nI find this question confusing, in Google Cloud IAM, roles are granted to members, not directly to projects. Why are all the options to give roles to projects?","comment_id":"1354933","timestamp":"1739267160.0"},{"poster":"guidbem","comment_id":"1296821","content":"It is D. Tested and approved that you do not need BigQuery permissions on the web-app project to access data on the bq tables stored in the crm-dbs project. You do need bq permissions for the SA on the crm project and compute permissions for the same SA on the web-app project. Then, using this SA on a VM on the web-app server, you can access data from bq on the crm-dbs project","upvote_count":"2","timestamp":"1728810840.0"},{"timestamp":"1724056440.0","content":"Selected Answer: C\nExplanation:\n Least Privilege Principle: This approach adheres to the principle of least privilege by granting the minimum necessary permissions.\n Project Owner: The crm-databases-proj project needs full control to manage its resources.\n bigquery.dataViewer: The web-applications project only needs read access to BigQuery datasets in the crm-databases-proj project.\n\nWhy other options are less suitable:\n\n A: Giving project owner to web-applications provides unnecessary permissions.\n B: Giving project owner to both projects grants excessive permissions.\n D: Giving bigquery.dataViewer to crm-databases-proj is incorrect as this project needs full control over its resources.\n\nBy following option C, you ensure that the web-applications project has the required access to BigQuery datasets without compromising security.","poster":"Namik","upvote_count":"2","comment_id":"1268531"},{"timestamp":"1719307080.0","upvote_count":"1","comment_id":"1236807","content":"Selected Answer: D\nLet's analyze the options:\n\nA & B: Granting \"project owner\" gives excessive permissions, violating the least privilege principle.\nC: Granting \"project owner\" to crm-databases-proj is unnecessary.\nD: Granting \"bigquery.dataViewer\" to crm-databases-proj allows the VM access to datasets and aligns with least privilege. Granting appropriate roles to web-applications secures the web application itself (not shown in this scenario).\nTherefore, option D is the recommended approach.","poster":"nish2288"},{"upvote_count":"2","content":"Project owner role is not required here, so that leaves us with only Option D","comment_id":"1168051","poster":"abhi2704","timestamp":"1709819340.0"},{"comment_id":"1113383","content":"Selected Answer: D\nA, b & c is wrong. Keywords is configuring aervice account. A,b & c concerns user account. Correct answer is D","poster":"Bagibo","timestamp":"1704351240.0","upvote_count":"1"},{"timestamp":"1704122040.0","comment_id":"1111302","upvote_count":"3","comments":[{"comment_id":"1111304","timestamp":"1704122100.0","upvote_count":"2","poster":"Cynthia2023","content":"The ideal approach (not listed in the options) would be:\n\nCreate a service account in the web-applications project specifically for accessing the BigQuery datasets.\nGrant this service account the bigquery.dataViewer role (or another more specific role if different access is needed) on the crm-databases-proj project's BigQuery datasets.\nUse this service account in your VMs in the web-applications project."}],"content":"None of the options is correct. As for D:\nThis option is unclear and potentially misleading. The bigquery.dataViewer role should be assigned specifically to the service account in the web-applications project, not to the crm-databases-proj project.","poster":"Cynthia2023"},{"poster":"thewalker","content":"Damn!\nAll the four options are correct :-D for the question given :-)","timestamp":"1700743680.0","upvote_count":"2","comment_id":"1078438"},{"poster":"BAofBK","timestamp":"1699290720.0","upvote_count":"1","comment_id":"1064088","content":"The correct answer is C"},{"content":"Selected Answer: D\nD is the correct answer, because all other option giveing access to project owner","poster":"Captain1212","timestamp":"1693726500.0","upvote_count":"1","comment_id":"997431"},{"upvote_count":"2","comment_id":"994816","timestamp":"1693461240.0","poster":"SanjeevKumar1983","content":"Corrct Answer is D.\nLets just read the options D this way, then it makes sense\nGive service account the bigquery.dataViewer role to crm-databases-proj and service account the appropriate roles to web-applications."},{"timestamp":"1692437400.0","comment_id":"985085","upvote_count":"1","poster":"Ahmed_Y","content":"Selected Answer: D\nThanks guys for making that clear for me.\nNow simply guys, among all the answers, D is giving to the web-application proj the appropriate role, while giving the crm-databases-proj the least privilege role."},{"comment_id":"973696","poster":"Nxt_007","upvote_count":"1","timestamp":"1691315580.0","content":"Selected Answer: D\nCorrect answer is D\nAs basic roles (including Owner) should not be used in production environment:"},{"timestamp":"1690112520.0","upvote_count":"1","content":"Selected Answer: C\nC is correct..","comment_id":"960398","poster":"yichuan"},{"content":"I dont get the question. It says \"web-applications project need access to BigQuery datasets in crm-databases-proj\"\n\nAnd all you folks stating C or D is the correct one. Why would we want to give those permissions to the DB? When the question clearly states that the web-app is the one that needs access to the DB?","poster":"creativenets","upvote_count":"4","timestamp":"1686757680.0","comment_id":"923356"},{"timestamp":"1686055440.0","poster":"KC_go_reply","comment_id":"916244","content":"Selected Answer: C\nIt says 'web-applications project need access to BigQuery datasets in crm-databases-proj'. Therefore, give web-applications the BigQuery Data Viewer role - not the other way around. Why would crm-databases-proj need this role in this situation?","upvote_count":"4"},{"comment_id":"873305","timestamp":"1681797060.0","upvote_count":"3","poster":"sabrinakloud","content":"Selected Answer: D\nI believe that the correct answer is option D. Although the web application requires the bigquery.dataViewer role, the option D mentions \"appropriate roles to web applications\" and the appropriate role in this case is indeed bigquery.dataViewer. It is not recommended to give the project owner role to crm-databases-proj as it grants too many permissions. Google's best practice is to minimize the number of permissions granted, so option D aligns with this principle."},{"comment_id":"819161","poster":"xaqanik","content":"Selected Answer: D\nwhy we should grant 'project owner' role? we only need to give access for BigQuery\ni choose Answer D","upvote_count":"1","timestamp":"1677154620.0"},{"upvote_count":"2","comment_id":"792243","comments":[{"timestamp":"1682725200.0","poster":"vivekvj","content":"true. none of the options make sense but project owner role for no reason is an absolute NO. So D.","comment_id":"883955","upvote_count":"1"}],"timestamp":"1675041600.0","poster":"jrisl1991","content":"Selected Answer: D\nHonestly none of the answers make full sense to me but Best Practices for Service Accounts basically ask to keep the roles/permissions as low as possible since there are potential bad scenarios where a service account can be used to jeopardize a project, sometimes without leaving traces. Based on this, none of the Project-owner answers would be correct, so I'm going for D."},{"upvote_count":"1","timestamp":"1672328340.0","content":"Selected Answer: D\nD is the best answer.","poster":"SK1990","comment_id":"761206"},{"timestamp":"1670006460.0","poster":"ChristN","content":"Selected Answer: C\nC is the correct answer.\nbigquery.dataViewer role to crm-databases-proj doesn't full fill the requirement. only the web application needs this access.","comment_id":"733992","upvote_count":"2"},{"comment_id":"732712","poster":"fragment137","upvote_count":"9","content":"This question is misleading. the requirements are for the web -application service account to have \"access\" to the BQ Datasets, but it doesn't specify what they need to do. Principal of Least Privilege would lead you to think they just need Viewer. Problem is the answers are also misleading. C gives Project Owner to the dataset project, and -appropriate- permissions to the web-application service account. It seems to indicate that it's giving project owner to the project itself, which makes no sense. D gives dataViewer to the BQ project, which doesn't make sense either because it's the web-application service account that needs access to BQ.\n\nI think this is a poorly worded question and poorly worded answers personally.","timestamp":"1669904940.0"},{"upvote_count":"3","comment_id":"702053","timestamp":"1666519200.0","content":"Selected Answer: C\nAll options are hard to understand.\nOnly C mentions bigquery.dataViewer for web-applications","poster":"Untamables"},{"content":"Selected Answer: D\nNo doubt about this Question!!!\nAnswer is simple and its D. Give bigquery.dataViewer role to crm-databases-proj and appropriate roles to web-applications.\nEND OF THE DISCUSSION NOW!!!","timestamp":"1665586680.0","poster":"AwesomeGCP","upvote_count":"2","comment_id":"693185"},{"upvote_count":"1","timestamp":"1663743420.0","content":"Selected Answer: D\nOption D is correct: as per google best practices you should not give primitive roles to a service account, this already exclude as options the answer A to C that give too many rights to the service account as Project owners.","comment_id":"674867","poster":"ale_brd_111"},{"poster":"theBestStudent","comment_id":"644305","upvote_count":"1","timestamp":"1660010100.0","content":"Selected Answer: D\nThere are some stuff to assume based on the question:\n1- You are configuring Service accounts for projects that span multiple projects, therefore, assume your service account in project X is at least \"accepted\" in project Y. \n2- That being said, make sure the project Y allows SA in project X every time it hits project Y to have the action of having access to big query enabled. Therefore you have to configure that role in project Y (project Y = crm-databases-proj )"},{"content":"Selected Answer: C\nGive ג€project ownerג€ role to crm-databases-proj and bigquery.dataViewer role to web-applications.","poster":"abirroy","timestamp":"1659902280.0","comment_id":"643840","upvote_count":"1"},{"poster":"sonuricky","timestamp":"1659440160.0","content":"C is the right answer","comment_id":"641244","upvote_count":"1"},{"comment_id":"621276","upvote_count":"1","timestamp":"1656015480.0","content":"D is right","poster":"AzureDP900"},{"timestamp":"1648698060.0","upvote_count":"4","comments":[{"upvote_count":"3","poster":"rsuresh27","timestamp":"1651074360.0","comments":[{"poster":"bobthebuilder55110","comment_id":"641512","content":"This does not makes sense, If it is not giving permissions to correct project then simply it will NOT work and what is the point of having it if it will not work. Yes you are giving extra permissions but at the least you are getting the job done in option D you are securing but no Performing the Job, what good would that be ?","upvote_count":"2","timestamp":"1659485940.0"}],"comment_id":"593277","content":"This is the correct way of thinking about it. You never want to give project owner role to a service account. Even though D does not grant the role to the correct project, it is much safer from security standpoint to do with D."}],"comment_id":"578648","poster":"zaxma","content":"Selected Answer: D\nIn my opinion, both C and D are wrong.\nC - you do not need to give the owner role, it has no meaning and over power\nD - the viewer is grant to the wrong project\nThen C still works, but give extra permission\nD does not work, but no damage to the existing security\nGod help me here. I will choose D, just based on instinct"},{"upvote_count":"1","timestamp":"1647957720.0","content":"Selected Answer: D\nAns: D","poster":"Rukman","comment_id":"572980"},{"timestamp":"1647667500.0","upvote_count":"4","comment_id":"570860","poster":"Rukman","content":"Selected Answer: C\nAns: C\n\nBecause providing bigquery.dataViewer role to crm-databases-proj doesn't full fill the requirement. basically, the web application needs this access. So its 'C'","comments":[{"upvote_count":"2","timestamp":"1647957900.0","comment_id":"572984","content":"also google doesn't recommend to provide primitive role for service account!","poster":"Rukman"},{"poster":"bobthebuilder55110","upvote_count":"1","timestamp":"1659486060.0","content":"Where do you see the mention of service account in the option D, it say \"to srm-databases-proj\" NOT service account of that project?","comment_id":"641513"}]},{"poster":"jaffarali","upvote_count":"1","comment_id":"501103","content":"Selected Answer: D\nD would be the perfect choice.","timestamp":"1639462020.0"},{"content":"Selected Answer: C\nhttps://cloud.google.com/bigquery/docs/access-control?hl=en#bigquery.dataViewer","upvote_count":"2","comments":[{"content":"The correct option is \"D\"","upvote_count":"3","timestamp":"1637894580.0","poster":"FernandoJ","comment_id":"487052"}],"comment_id":"487051","poster":"FernandoJ","timestamp":"1637894460.0"},{"comment_id":"483608","timestamp":"1637523540.0","upvote_count":"1","content":"why not A ? give the web app's project owner the required permission to run biq query on crm-database-proj. The scope of the question is about cross project access. It do not ask how crm resources access the big query,","comments":[{"upvote_count":"1","poster":"jabrrJ68w02ond1","comment_id":"484426","timestamp":"1637603160.0","content":"The project Owner is mentioned nowhere in the question itself. It only says that the VM instances need 'some' access to the data in other projects. Following least privilege, you let the VM instances access the data read-only."}],"poster":"GreenTick"},{"upvote_count":"2","content":"Selected Answer: D\nyou just need read DB permission","timestamp":"1637160420.0","poster":"abbottWang","comment_id":"480088"},{"upvote_count":"1","content":"Correct answer must be D .. Give bigquery.dataViewer role to crm-databases-proj and appropriate roles to web-applications.","timestamp":"1634296860.0","comment_id":"462555","poster":"Mandy"},{"upvote_count":"4","timestamp":"1633860900.0","poster":"ankatsu2010","content":"I think \"D\" is trying to say, Add bigquery.dataViewer role (with web-application-proj service account as a principal) to the crm-databases-proj and add some appropriate roles to the web-application service account in the web-application project.\nIf so, \"D\" is achievable...","comment_id":"460009"},{"content":"D - bigquery.dataViewer role provides permissions to read the dataset’s metadata and list tables in the dataset as well as Read data and metadata from the dataset’s tables. This is exactly what we need to fulfil this requirement and follows the least privilege principle.","comment_id":"427785","timestamp":"1629417060.0","upvote_count":"3","poster":"sudav"},{"comments":[{"content":"in other words why give \"bigquery.dataViewer\" role to the Service Account on the web-applications project, while there is no BigQuery in that project, lol.","comment_id":"426655","timestamp":"1629267720.0","upvote_count":"1","poster":"gerhardbl"}],"poster":"gerhardbl","comment_id":"426652","upvote_count":"1","content":"Amazing to see most people don't know how a service account works. The question is intentionally written with the word \"TO\" to add confusion. This is not about giving a \"project.owner Role TO database_project\" because that doesn't make any sense at all. You're not assigning any Roles to the Projects (doh). It is about adding the newly created Service Account to the \"bigquery.dataViewer\" Role ON the \"database_project\".","timestamp":"1629267480.0"},{"timestamp":"1628913420.0","comment_id":"424612","content":"D is correct. Why not C? Because, you no need to provide owner role.","upvote_count":"2","poster":"vvkds"},{"timestamp":"1623761100.0","upvote_count":"3","poster":"AD_0525","content":"D is correct, you just don't need any owner access in any of the projects.","comment_id":"382665"},{"comment_id":"366646","content":"A and B are wrong as they talk about giving Project owner to web-applications which is unnecessary. If we go with D, crm-databases-proj will only have read-only permissions and it cannot write data into that VM so we cannot go with this one which rules that option. So only option left is C which is logical as project owner will have write permission to crm-databases-proj","upvote_count":"5","timestamp":"1621971960.0","poster":"Praveen_N"},{"content":"D is answer","upvote_count":"1","poster":"arsh1916","comment_id":"362201","timestamp":"1621516920.0"},{"content":"D is correct","poster":"mcaromit","upvote_count":"1","comment_id":"356814","timestamp":"1620957900.0"},{"comment_id":"353491","upvote_count":"2","timestamp":"1620628500.0","poster":"ri_unhou119","content":"Why not A?\nI think A is correct","comments":[{"comment_id":"567374","upvote_count":"1","poster":"obeythefist","timestamp":"1647233640.0","content":"A, B, and C all grant Project Owner permission. This is too much! The question asks for \"Best Practice\", which is always going to be the least amount of permission to get the job done."}]},{"comment_id":"325579","upvote_count":"3","timestamp":"1617259620.0","content":"my answer is D \n1.configuring service accounts for an application that spans multiple projects.\n2.web-applications project need access to BigQuery datasets in crm-databases-proj.\n\nAccess control examples\nhttps://cloud.google.com/bigquery/docs/access-control-examples#read_access_to_data_in_a_different_project\n\nRead access to data in a different project","poster":"Jacky_YO"},{"timestamp":"1617193440.0","poster":"ltw71","content":"VMS or Compute Engine in web-applications project need to access the BQ datasets in crm-databases-proj - so the initiation comes FROM web-applications project. This service account (call it SAWEB) would be created in web-applications project and would need roles in it's own project to operate the Compute Engine etc. But it would also need to have permission to access datasets in the crm-databases-proj. So you would add the service account SAWEB which you created in web-applications as an IAM user in the crm-databases-proj and in that project you would grant it the least permissions it needs to get the access. Very doubtful that it would need to be owner and you certainly wouldn't want a service account from one project to be an owner in another project. So the correct answer is D.","upvote_count":"3","comment_id":"325067"},{"content":"C is correct. Give ג€project ownerג€ role to crm-databases-proj and bigquery.dataViewer role to web-applications.","comment_id":"320086","timestamp":"1616669340.0","poster":"[Removed]","upvote_count":"2"},{"content":"Why many guys select C.\nIn this case. We create a service account in crm-databases-proj. And We can use this service account in our application. That all we need to do.\nI vote D","timestamp":"1616656080.0","poster":"pondai","comment_id":"319860","upvote_count":"1"},{"content":"answer is D","timestamp":"1616599920.0","poster":"yuvi69","comment_id":"319271","upvote_count":"1"},{"timestamp":"1616169000.0","comment_id":"314999","poster":"sumanshu","upvote_count":"2","content":"In question it's written ===> project need access to BigQuery datasets in crm-databases-proj.\nSo which type of access required its not mention ==> So answer should be \"Appropriate roles to Database\" . Is not ? which is in 'A'"},{"comment_id":"314462","poster":"GoCloud","timestamp":"1616113740.0","content":"Def D, read the question, the permission needs to be given to the service account.","upvote_count":"3","comments":[{"comment_id":"316136","poster":"user843983409","timestamp":"1616312340.0","upvote_count":"4","comments":[{"upvote_count":"3","timestamp":"1618292700.0","comment_id":"334429","content":"here, we create a service account for the VM's and ask the big query to grant the permission to access the data sets. Then, a big query will assign a data viewer role to the web applications. What will you gain if yoi give data viewer access to crm-databases-proj, so this means big query wants to access from the web app??? Hence, we can go with C.","poster":"tavva_prudhvi"}],"content":"Yes, no need to have a project owner permission for crm-databases-proj .\nIn case of D, service account gets the following permissions:\nFor crm-databases-proj -> dataViewer = enough to access bigquery datasets (No need to give project owner permissions)\nFor web-applications project -> appropriate permissions since service account is in the web-applications project.\nHence, my vote for D"}]},{"comment_id":"301454","content":"C Is correct.\nhttps://cloud.google.com/bigquery/docs/access-control-examples#read_access_to_data_in_a_different_project\n\nAs D is talking about to give dataViewer access to crm-databases-proj which is not relevant because Bigquery is in crm-databases-proj project only. This dataset required to have access from web-applications project so web-applications should have bigquery.dataViewer role","poster":"Sskhan","timestamp":"1614616500.0","upvote_count":"2"},{"timestamp":"1614424260.0","content":"I vote for C too","upvote_count":"2","poster":"hicham","comment_id":"300211"},{"poster":"ShakthiGCP","upvote_count":"2","comment_id":"299957","content":"Answer is C : Dont go with the Answer D because of high 'upvote'. read the answer C carefully and it will make sense. I was thinking Answer D but realized it is wrong. Going with C","timestamp":"1614372540.0"},{"comment_id":"299235","poster":"GCP_Student1","upvote_count":"2","content":"C. Give \"project owner\" role to crm-databases-proj and bigquery.dataViewer role to web-applications.","timestamp":"1614272640.0","comments":[{"poster":"GCPACE2020","content":"But why giving project owner role to crm-databases-proj ??","timestamp":"1624613940.0","upvote_count":"1","comment_id":"390298"}]},{"content":"C is correct, read the question. it states - BigQuery datasets in crm-databases-proj\nso crm-databases-proj is the project owner!","comment_id":"297147","upvote_count":"2","timestamp":"1614055920.0","poster":"nitinz"},{"poster":"DipakKCS","comment_id":"256707","content":"D is correct\nhttps://cloud.google.com/bigquery/docs/access-control-examples#read_access_to_data_in_a_different_project","timestamp":"1609487700.0","comments":[{"timestamp":"1618566600.0","poster":"tavva_prudhvi","content":"It clearly mentioned in the article, that the \"Analyst group is given bigQuery.DataViewer role\" to access the data in a different project. In our case, its the web applications that are accessing the data from the crm-database-proj.","comment_id":"336906","upvote_count":"1"}],"upvote_count":"4"},{"comment_id":"256690","content":"Ans is D.","poster":"piipo","timestamp":"1609483680.0","upvote_count":"1"},{"poster":"Bhagirathi","content":"C is correct. \nNot sure, why some of you choose D?","comment_id":"238059","comments":[{"comment_id":"249931","timestamp":"1608622500.0","upvote_count":"3","poster":"cRobert","content":"Why would you grant project owner on the Database Project?"},{"timestamp":"1609432980.0","upvote_count":"1","poster":"magistrum","comment_id":"256344","content":"Also why will you give bigquery.dataViewer role to web-applications? What does this achieve if your BQ is in the other project"}],"timestamp":"1607417040.0","upvote_count":"5"},{"comment_id":"226354","poster":"Bhagirathi","comments":[{"poster":"neerajgoyal","upvote_count":"2","timestamp":"1614876360.0","comment_id":"303476","content":"True, not sure why most of the answer author mentioned is not matching with what people said here."}],"content":"I really do not understand ...why the solution to these questions differs and most of these are wrong\n and many of us have different understanding and opinions, this adds more confusions who wants to prepare his exam.","timestamp":"1606193460.0","upvote_count":"4"},{"content":"I vote for D","upvote_count":"1","poster":"Siva003","comments":[{"content":"this isn't a political voting, see the answer clearly, C is the one!","poster":"tavva_prudhvi","upvote_count":"2","comment_id":"336907","timestamp":"1618566660.0"}],"comment_id":"214729","timestamp":"1604766420.0"}]}],"exam":{"isBeta":false,"id":1,"isImplemented":true,"lastUpdated":"11 Apr 2025","provider":"Google","numberOfQuestions":285,"isMCOnly":true,"name":"Associate Cloud Engineer"},"currentPage":4},"__N_SSP":true}