{"pageProps":{"questions":[{"id":"LD4ULJ8AtYvklXSw9zb0","choices":{"A":"Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId.","D":"Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId.","B":"Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the Transactionid.","C":"Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the TransactionId."},"answer_images":[],"unix_timestamp":1670850660,"url":"https://www.examtopics.com/discussions/google/view/91188-exam-professional-cloud-developer-topic-1-question-220/","exam_id":7,"answer_ET":"C","discussion":[{"upvote_count":"1","comment_id":"1015725","content":"Selected Answer: C\nC is correct.","poster":"__rajan__","timestamp":"1727179080.0"},{"timestamp":"1723066260.0","content":"Selected Answer: C\nRequirement is to scale globally. Cloud spanner is the best fit. UUID as a transaction ID is good for security purpose...avoids guessing of next transaction ID.","poster":"purushi","comment_id":"974991","upvote_count":"1"},{"content":"Selected Answer: C\nacross the world -> global/multi-region -> spanner\nuuid for primary key","timestamp":"1708436640.0","upvote_count":"2","comment_id":"815299","poster":"Pime13"},{"comment_id":"789711","upvote_count":"1","poster":"omermahgoub","timestamp":"1706367480.0","content":"C. Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the TransactionId. This will ensure that the combination of CustomerId and TransactionId is unique, even as your organization grows and expands across the world. Cloud Spanner is a highly scalable and globally-distributed SQL database, making it well-suited for this use case. Using a UUID for the TransactionId will ensure that it is unique across all regions and customers."},{"upvote_count":"1","timestamp":"1704968040.0","comment_id":"772373","content":"Selected Answer: C\nAnswer C, cloud spanner for multi-region and uui primary key to be sure to be unique","poster":"telp"},{"timestamp":"1703672400.0","upvote_count":"1","content":"Selected Answer: C\nhttps://cloud.google.com/spanner/docs/schema-design#uuid_primary_key\nAnswer C","poster":"TNT87","comment_id":"758363"},{"timestamp":"1702535460.0","content":"Selected Answer: C\noption C","comment_id":"744785","upvote_count":"1","poster":"sharath25"},{"content":"Selected Answer: C\nGlobally available --> Cloud Spanner (multi-region). Cloud SQL is a regional service.","timestamp":"1702405680.0","comment_id":"743161","upvote_count":"1","poster":"x_cath"},{"content":"Selected Answer: C\nC is the answer.","timestamp":"1702386660.0","poster":"zellck","upvote_count":"1","comment_id":"742813"}],"answer_description":"","timestamp":"2022-12-12 14:11:00","question_images":[],"isMC":true,"answer":"C","question_id":136,"question_text":"You work for an organization that manages an online ecommerce website. Your company plans to expand across the world; however, the estore currently serves one specific region. You need to select a SQL database and configure a schema that will scale as your organization grows. You want to create a table that stores all customer transactions and ensure that the customer (CustomerId) and the transaction (TransactionId) are unique. What should you do?","topic":"1","answers_community":["C (100%)"]},{"id":"suQnGtVVPqa1Hj9zq0kS","question_text":"You are monitoring a web application that is written in Go and deployed in Google Kubernetes Engine. You notice an increase in CPU and memory utilization. You need to determine which source code is consuming the most CPU and memory resources. What should you do?","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/91187-exam-professional-cloud-developer-topic-1-question-221/","discussion":[{"comment_id":"1015727","poster":"__rajan__","upvote_count":"1","content":"Selected Answer: B\nB is correct.","timestamp":"1727179200.0"},{"upvote_count":"1","content":"Selected Answer: B\nFocus is to find which function is more CPU and Memory intensive. Flame graphs highlights the memory intensive functions in a graphical way. B is the best answer.","timestamp":"1723067280.0","comment_id":"974996","poster":"purushi"},{"upvote_count":"1","poster":"Pime13","comment_id":"815258","timestamp":"1708435140.0","content":"Selected Answer: B\nhttps://cloud.google.com/profiler/docs"},{"upvote_count":"1","timestamp":"1706366940.0","comment_id":"789703","poster":"omermahgoub","content":"Selected Answer: B\nB. Import the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions.\n\nOption B is the best solution because it involves importing the Cloud Profiler package into the application, initializing the Profiler agent, and reviewing the generated flame graph in the Google Cloud console. This will allow you to identify time-intensive functions and determine which source code is consuming the most CPU and memory resources. The flame graph is a visualization of the call stack and it can be used to identify bottlenecks in the application.\nOption A and C are also related to profiling but they don't exactly focus on identifying time-intensive functions. Option D is not the best option because it would be more complex and less efficient than using a profiler."},{"comment_id":"758362","timestamp":"1703672280.0","content":"Selected Answer: B\nhttps://cloud.google.com/profiler/docs/about-profiler#profiling_agent\nhttps://cloud.google.com/profiler/docs/about-profiler#environment_and_languages\nAnswer B","poster":"TNT87","upvote_count":"2"},{"poster":"sharath25","comment_id":"744786","timestamp":"1702535520.0","content":"Selected Answer: B\noption B","upvote_count":"1"},{"comment_id":"742809","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/profiler/docs/about-profiler\nCloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your production applications. It attributes that information to the source code that generated it, helping you identify the parts of your application that are consuming the most resources, and otherwise illuminating your applications performance characteristics.","poster":"zellck","upvote_count":"1","timestamp":"1702386480.0"}],"topic":"1","choices":{"A":"Download, install, and start the Snapshot Debugger agent in your VM. Take debug snapshots of the functions that take the longest time. Review the call stack frame, and identify the local variables at that level in the stack.","B":"Import the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions.","C":"Import OpenTelemetry and Trace export packages into your application, and create the trace provider.\nReview the latency data for your application on the Trace overview page, and identify where bottlenecks are occurring.","D":"Create a Cloud Logging query that gathers the web application's logs. Write a Python script that calculates the difference between the timestamps from the beginning and the end of the application's longest functions to identity time-intensive functions."},"exam_id":7,"answer_images":[],"unix_timestamp":1670850480,"question_images":[],"timestamp":"2022-12-12 14:08:00","question_id":137,"answer":"B","answers_community":["B (100%)"],"answer_description":"","answer_ET":"B"},{"id":"FjWazk2GGy917QvGS8gB","discussion":[{"upvote_count":"6","comment_id":"871866","content":"Selected Answer: A\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes\nThe kubelet uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.","poster":"closer89","timestamp":"1697468880.0"},{"poster":"Kadhem","timestamp":"1719333960.0","upvote_count":"2","comment_id":"1105477","content":"Selected Answer: A\n\"Sometimes, you have to deal with legacy applications that might require an additional startup time on their first initialization. In such cases, it can be tricky to set up liveness probe parameters without compromising the fast response to deadlocks that motivated such a probe. The trick is to set up a startup probe with the same command\"\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes"},{"upvote_count":"1","timestamp":"1711289040.0","poster":"__rajan__","content":"Selected Answer: A\nA startup probe is a probe that Kubernetes uses to determine if a container has started successfully. If the startup probe fails, Kubernetes will restart the container.","comment_id":"1015729"},{"comment_id":"975017","comments":[{"upvote_count":"1","content":"Startup probes have been enabled by default since v1.19.","comment_id":"1040954","timestamp":"1712855820.0","poster":"flesk"},{"content":"Typo: Likeness probe... -> Liveness probe","poster":"purushi","timestamp":"1707593820.0","upvote_count":"1","comment_id":"977960"}],"upvote_count":"1","timestamp":"1707353520.0","poster":"purushi","content":"Selected Answer: D\nReadiness probe is the right answer. Likeness probe fails if it tries to probe a container not yet ready to serve the traffic. So we need to add readiness probe. There is no such thing called startup probe in kubernetes."},{"timestamp":"1692530400.0","comment_id":"815260","content":"Selected Answer: B\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n\nCaution: Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.","poster":"Pime13","upvote_count":"1"},{"upvote_count":"1","comment_id":"797716","content":"Selected Answer: B\nA liveness probe checks if the container is running as expected, and if not, it restarts it. If the container is slow to launch, it may take some time for it to fully start up and be able to respond to the liveness probe. Increasing the initial delay for the liveness probe can help mitigate this issue by giving the container more time to start up before the probe begins checking its status. This can help reduce the likelihood of false-positive failures during launch.","poster":"mrvergara","timestamp":"1691128500.0"},{"comment_id":"789699","upvote_count":"3","content":"Selected Answer: A\nA. Adding a startup probe is useful for determining when a container has started, but it won't help with the problem of the liveness probe occasionally failing on launch.\n\nB. Increasing the initial delay for the liveness probe might help if the container is taking longer than the delay to start, but it's not a guaranteed solution.\n\nC. Increasing the CPU limit for the container may help if the container is running out of resources, but it may not be necessary if the issue is related to the container's initialization process.\n\nD. A readiness probe can help determine when a container is ready to receive traffic, but it won't help with the problem of the liveness probe occasionally failing on launch.","timestamp":"1690461900.0","poster":"omermahgoub"},{"content":"Selected Answer: D\nTo the people voting B:\nThe question specifically says that the problem occurs sometimes on launch, so how is the solution not a readiness probe?","upvote_count":"1","timestamp":"1689833460.0","comments":[{"upvote_count":"2","poster":"chunker","timestamp":"1689951000.0","comments":[{"timestamp":"1690696320.0","poster":"TNT87","content":"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\nAnswer will remain B","upvote_count":"1","comment_id":"792529"}],"comment_id":"783560","content":"Changing to A:\n\nThe problem is that the liveness probes fires too early, so we need a startup probe to determine when liveness (and potential readiness) probe are valid.\n\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"},{"comment_id":"785237","timestamp":"1690101780.0","comments":[{"poster":"closer89","upvote_count":"1","timestamp":"1697468520.0","comment_id":"871863","content":"Should be A definitely\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes\nSometimes, you have to deal with legacy applications that might require an additional startup time on their first initialization. In such cases, it can be tricky to set up liveness probe parameters without compromising the fast response to deadlocks that motivated such a probe. The trick is to set up a startup probe with the same command, HTTP or TCP check, with a failureThreshold * periodSeconds long enough to cover the worse case startup time","comments":[{"content":"Thanks to the startup probe, the application will have a maximum of 5 minutes (30 * 10 = 300s) to finish its startup. Once the startup probe has succeeded once, the liveness probe takes over to provide a fast response to container deadlocks. If the startup probe never succeeds, the container is killed after 300s and subject to the pod's restartPolicy.","upvote_count":"1","comment_id":"871865","timestamp":"1697468580.0","poster":"closer89"}]}],"upvote_count":"1","content":"provide a link to cement your argument","poster":"TNT87"}],"comment_id":"781987","poster":"chunker"},{"poster":"zellck","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes","comment_id":"751086","upvote_count":"2","timestamp":"1687266720.0"},{"upvote_count":"1","comment_id":"744790","timestamp":"1686717420.0","content":"Selected Answer: B\noption B \nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes","poster":"sharath25"},{"comment_id":"733587","content":"Selected Answer: B\nIncrease the Timeout of the Liveness Probe","timestamp":"1685689320.0","upvote_count":"3","poster":"TNT87"}],"answer":"A","isMC":true,"choices":{"D":"Add a readiness probe.","C":"Increase the CPU limit for the container.","A":"Add a startup probe.","B":"Increase the initial delay for the liveness probe."},"answer_images":[],"topic":"1","answer_description":"","timestamp":"2022-12-02 10:02:00","answers_community":["A (55%)","B (36%)","9%"],"question_id":138,"exam_id":7,"question_images":[],"unix_timestamp":1669971720,"question_text":"You have a container deployed on Google Kubernetes Engine. The container can sometimes be slow to launch, so you have implemented a liveness probe. You notice that the liveness probe occasionally fails on launch. What should you do?","url":"https://www.examtopics.com/discussions/google/view/89759-exam-professional-cloud-developer-topic-1-question-222/","answer_ET":"A"},{"id":"Ei4ETOoyy8BTSCFEjVaS","choices":{"A":"Split traffic between versions using weights.","D":"Use HTTP header-based routing.","B":"Enable the new recommendation feature flag on a single instance.","C":"Mirror traffic to the new version of your application."},"exam_id":7,"question_text":"You work for an organization that manages an ecommerce site. Your application is deployed behind a global HTTP(S) load balancer. You need to test a new product recommendation algorithm. You plan to use A/B testing to determine the new algorithm’s effect on sales in a randomized way. How should you test this feature?","answer_ET":"A","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/89809-exam-professional-cloud-developer-topic-1-question-223/","discussion":[{"comment_id":"1015734","timestamp":"1727179680.0","content":"Selected Answer: A\nSplitting traffic between versions using weights is a common way to implement A/B testing. To do this, you would create two versions of your application, one with the new recommendation algorithm and one without. You would then configure the load balancer to split traffic between the two versions using weights. For example, you could configure the load balancer to send 50% of traffic to the new version and 50% of traffic to the old version.","upvote_count":"1","poster":"__rajan__"},{"poster":"purushi","upvote_count":"1","content":"Selected Answer: A\nSplit traffic is the right answer.","timestamp":"1723071300.0","comment_id":"975019"},{"poster":"closer89","content":"Selected Answer: A\nin a randomized way - so its A, D otherwise","comment_id":"878078","upvote_count":"1","timestamp":"1713862080.0"},{"upvote_count":"1","poster":"Pime13","content":"Selected Answer: A\nhttps://cloud.google.com/traffic-director/docs/advanced-traffic-management#weight-based_traffic_splitting_for_safer_deployments","timestamp":"1708435320.0","comment_id":"815264"},{"comment_id":"744791","upvote_count":"1","poster":"sharath25","timestamp":"1702535820.0","content":"Selected Answer: A\noption A"},{"poster":"zellck","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/load-balancing/docs/https/traffic-management-global#traffic_actions_weight-based_traffic_splitting\nDeploying a new version of an existing production service generally incurs some risk. Even if your tests pass in staging, you probably don't want to subject 100% of your users to the new version immediately. With traffic management, you can define percentage-based traffic splits across multiple backend services.\n\nFor example, you can send 95% of the traffic to the previous version of your service and 5% to the new version of your service. After you've validated that the new production version works as expected, you can gradually shift the percentages until 100% of the traffic reaches the new version of your service. Traffic splitting is typically used for deploying new versions, A/B testing, service migration, and similar processes.","timestamp":"1702261080.0","comment_id":"741369","upvote_count":"1"},{"timestamp":"1701687540.0","poster":"kisswd","content":"Selected Answer: A\nA is the answer","comment_id":"735020","upvote_count":"1"},{"content":"Selected Answer: A\nhttps://cloud.google.com/traffic-director/docs/advanced-traffic-management#weight-based_traffic_splitting_for_safer_deployments","timestamp":"1701554040.0","poster":"TNT87","comment_id":"734082","upvote_count":"2"},{"upvote_count":"2","comment_id":"734079","timestamp":"1701553800.0","poster":"melisargh","content":"Selected Answer: A\nA is the recommended way to test A/B\n\nhttps://cloud.google.com/load-balancing/docs/https/traffic-management-global"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#split_the_traffic_2\nhttps://cloud.google.com/load-balancing/docs/https/traffic-management-global#traffic_actions_weight-based_traffic_splitting","timestamp":"1701553680.0","comment_id":"734078","poster":"TNT87"}],"question_id":139,"unix_timestamp":1670017680,"topic":"1","answers_community":["A (100%)"],"answer":"A","timestamp":"2022-12-02 22:48:00","answer_images":[],"answer_description":"","isMC":true},{"id":"PP75ZUNIyDNRANYeA8Vt","timestamp":"2022-12-04 12:05:00","answer_description":"","question_text":"You plan to deploy a new application revision with a Deployment resource to Google Kubernetes Engine (GKE) in production. The container might not work correctly. You want to minimize risk in case there are issues after deploying the revision. You want to follow Google-recommended best practices. What should you do?","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/89950-exam-professional-cloud-developer-topic-1-question-224/","answer":"A","exam_id":7,"answers_community":["A (100%)"],"discussion":[{"content":"Selected Answer: A\nBy performing a rolling update with a PDB of 80%, you can ensure that at least 80% of the Pods are always available during the deployment. This will minimize the risk of downtime in case there are issues with the new revision.","timestamp":"1727180160.0","upvote_count":"2","comment_id":"1015740","poster":"__rajan__"},{"timestamp":"1708435620.0","comment_id":"815271","upvote_count":"2","poster":"Pime13","content":"Selected Answer: A\nhttps://kubernetes.io/docs/tasks/run-application/configure-pdb/#identify-an-application-to-protect"},{"poster":"mrvergara","content":"Selected Answer: A\nA rolling update with a PodDisruptionBudget (PDB) of 80% helps to minimize the risk of issues after deploying a new revision to a production environment in GKE. The PDB specifies the number of pods in a deployment that must remain available during an update, ensuring that there is sufficient capacity to handle any increase in traffic or demand. By setting a PDB of 80%, you ensure that at least 80% of the pods are available during the update, reducing the risk of disruption to your application. This is a recommended best practice by Google for deploying updates to production environments in GKE.","timestamp":"1707033780.0","comment_id":"797723","upvote_count":"1"},{"timestamp":"1703671080.0","comment_id":"758345","upvote_count":"1","poster":"TNT87","content":"Selected Answer: A\nhttps://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/\nhttps://cloud.google.com/blog/products/containers-kubernetes/ensuring-reliability-and-uptime-for-your-gke-cluster\nAnswer A"},{"poster":"zellck","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/blog/products/containers-kubernetes/ensuring-reliability-and-uptime-for-your-gke-cluster\nSetting PodDisruptionBudget ensures that your workloads have a sufficient number of replicas, even during maintenance. Using the PDB, you can define a number (or percentage) of pods that can be terminated, even if terminating them brings the current replica count below the desired value. With PDB configured, Kubernetes will drain a node following the configured disruption schedule. New pods will be deployed on other available nodes. This approach ensures Kubernetes schedules workloads in an optimal way while controlling the disruption based on the PDB configuration.","timestamp":"1702040880.0","upvote_count":"1","comment_id":"739083"},{"upvote_count":"1","timestamp":"1701687900.0","content":"Selected Answer: A\nhttps://blog.knoldus.com/how-to-avoid-outages-in-your-kubernetes-cluster-using-pdb/","comment_id":"735025","poster":"kisswd"}],"topic":"1","isMC":true,"answer_ET":"A","question_id":140,"answer_images":[],"choices":{"B":"Perform a rolling update with a HorizontalPodAutoscaler scale-down policy value of 0.","D":"Convert the Deployment to a StatefulSet, and perform a rolling update with a HorizontalPodAutoscaler scale-down policy value of 0.","C":"Convert the Deployment to a StatefulSet, and perform a rolling update with a PodDisruptionBudget of 80%.","A":"Perform a rolling update with a PodDisruptionBudget of 80%."},"unix_timestamp":1670151900}],"exam":{"isBeta":false,"numberOfQuestions":338,"lastUpdated":"11 Apr 2025","isMCOnly":false,"isImplemented":true,"name":"Professional Cloud Developer","id":7,"provider":"Google"},"currentPage":28},"__N_SSP":true}