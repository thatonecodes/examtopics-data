{"pageProps":{"questions":[{"id":"HXsT2s8dQKH4sQBfSQVe","exam_id":2,"question_text":"You work for an online retail company. Your company collects customer purchase data in CSV files and pushes them to Cloud Storage every 10 minutes. The data needs to be transformed and loaded into BigQuery for analysis. The transformation involves cleaning the data, removing duplicates, and enriching it with product information from a separate table in BigQuery. You need to implement a low-overhead solution that initiates data processing as soon as the files are loaded into Cloud Storage. What should you do?","answer_images":[],"answer_description":"","question_images":[],"answers_community":["C (100%)"],"isMC":true,"topic":"1","question_id":41,"discussion":[{"comment_id":"1366600","content":"Selected Answer: C\ndataflow with pub/sub is lowest overhead for real-time","upvote_count":"1","timestamp":"1741443900.0","poster":"n2183712847"},{"content":"Selected Answer: C\nThe best solution is C. Dataflow with Pub/Sub notifications. Dataflow offers low-overhead, real-time streaming, ideal for immediate processing of files as they land in Cloud Storage, and handles the required transformations efficiently before loading into BigQuery. Option A (Composer/Dataproc) is too heavyweight and costly for frequent, small batches. Option B (Composer Hourly Batch) is too slow, introducing delays and data staleness. Option D (Data Fusion/Cloud Run) is more complex and potentially higher overhead than a simpler Dataflow pipeline for this streaming use case.","upvote_count":"2","timestamp":"1740669780.0","comment_id":"1362583","poster":"n2183712847"}],"choices":{"C":"Use Dataflow to implement a streaming pipeline using an OBJECT_FINALIZE notification from Pub/Sub to read the data from Cloud Storage, perform the transformations, and write the data to BigQuery.","A":"Use Cloud Composer sensors to detect files loading in Cloud Storage. Create a Dataproc cluster, and use a Composer task to execute a job on the cluster to process and load the data into BigQuery.","D":"Create a Cloud Data Fusion job to process and load the data from Cloud Storage into BigQuery. Create an OBJECT_FINALI ZE notification in Pub/Sub, and trigger a Cloud Run function to start the Cloud Data Fusion job as soon as new files are loaded.","B":"Schedule a direct acyclic graph (DAG) in Cloud Composer to run hourly to batch load the data from Cloud Storage to BigQuery, and process the data in BigQuery using SQL."},"unix_timestamp":1740669780,"answer":"C","answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/157205-exam-associate-data-practitioner-topic-1-question-46/","timestamp":"2025-02-27 16:23:00"},{"id":"5EpYxW9mLSnFMEtpVw7k","question_text":"You work for a home insurance company. You are frequently asked to create and save risk reports with charts for specific areas using a publicly available storm event dataset. You want to be able to quickly create and re-run risk reports when new data becomes available. What should you do?","question_id":42,"url":"https://www.examtopics.com/discussions/google/view/157203-exam-associate-data-practitioner-topic-1-question-47/","unix_timestamp":1740669300,"choices":{"C":"Reference and query the storm event dataset using SQL in BigQuery Studio. Export the results to Google Sheets, and use cell data in the worksheets to create charts.","D":"Reference and query the storm event dataset using SQL in a Colab Enterprise notebook. Display the table results and document with Markdown, and use Matplotlib to create charts.","A":"Export the storm event dataset as a CSV file. Import the file to Google Sheets, and use cell data in the worksheets to create charts.","B":"Copy the storm event dataset into your BigQuery project. Use BigQuery Studio to query and visualize the data in Looker Studio."},"discussion":[{"content":"Selected Answer: B\nbigquery studio + looker studio for visualization and making \"reports\" in looker studio.","timestamp":"1741443960.0","poster":"n2183712847","upvote_count":"1","comment_id":"1366601"},{"content":"Selected Answer: B\nWhile Option D offers flexibility and control, it's more technical and less focused on quick and easy report generation for a business user. Options A and C are too manual and less scalable for the stated requirements of frequent report generation with new data.\n\nFinal Answer: B","timestamp":"1740669300.0","poster":"n2183712847","upvote_count":"1","comment_id":"1362579"}],"topic":"1","answer_ET":"B","answer_description":"","question_images":[],"answer":"B","isMC":true,"exam_id":2,"timestamp":"2025-02-27 16:15:00","answers_community":["B (100%)"],"answer_images":[]},{"id":"kWunxADNlFFLEw9utCsn","answer_description":"","exam_id":2,"question_id":43,"question_text":"Your company currently uses an on-premises network file system (NFS) and is migrating data to Google Cloud. You want to be able to control how much bandwidth is used by the data migration while capturing detailed reporting on the migration status. What should you do?","question_images":[],"answers_community":["C (100%)"],"answer":"C","unix_timestamp":1740668760,"discussion":[{"poster":"n2183712847","comment_id":"1366602","upvote_count":"1","timestamp":"1741444020.0","content":"Selected Answer: C\nSTS can control bandwidth used by data migration."},{"timestamp":"1740668760.0","content":"Selected Answer: C\nStorage Transfer Service is the most appropriate solution because it directly addresses both requirements: bandwidth control and detailed reporting, specifically for data migration to Google Cloud Storage from sources like NFS.","upvote_count":"1","comment_id":"1362574","poster":"n2183712847"}],"isMC":true,"choices":{"A":"Use a Transfer Appliance.","C":"Use Storage Transfer Service.","B":"Use Cloud Storage FUSE.","D":"Use gcloud storage commands."},"timestamp":"2025-02-27 16:06:00","answer_ET":"C","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/157200-exam-associate-data-practitioner-topic-1-question-48/","topic":"1"},{"id":"LLA9VOEoSkG1eoyrz0bK","url":"https://www.examtopics.com/discussions/google/view/155630-exam-associate-data-practitioner-topic-1-question-49/","isMC":true,"unix_timestamp":1738122060,"discussion":[{"comment_id":"1366603","content":"Selected Answer: D\n\"run against company database\" would be D","poster":"n2183712847","upvote_count":"1","timestamp":"1741444080.0"},{"comment_id":"1362569","upvote_count":"1","timestamp":"1740668100.0","content":"Selected Answer: D\nCustom fields (Option D) are specifically designed to allow users without Develop permissions to create new fields that generate SQL and run against the database. This is the direct answer to the question. Table calculations (Option C) are a common point of confusion, but they operate on the results of a query, not the underlying database. LookML changes (Option A) require Develop permissions, and Looker Studio (Option B) is a different tool.","poster":"n2183712847"},{"content":"Selected Answer: D\nuse custom to meet the requirement here, table calculations are used for building calculations on the fly in looker","poster":"A4M","comment_id":"1358170","timestamp":"1739864460.0","upvote_count":"2"},{"content":"Selected Answer: C\nCreate a table calculation from the field picker in Looker, and add it to your report.","timestamp":"1738122060.0","comment_id":"1348369","poster":"a_vi","upvote_count":"1"}],"choices":{"C":"Create a table calculation from the field picker in Looker, and add it to your report.","B":"Create a calculated field using the Add a field option in Looker Studio, and add it to your report.","D":"Create a custom field from the field picker in Looker, and add it to your report.","A":"Create a new field in the LookML layer, refresh your report, and select your new field from the field picker."},"question_images":[],"topic":"1","timestamp":"2025-01-29 04:41:00","answer_ET":"D","question_id":44,"answers_community":["D (80%)","C (20%)"],"answer":"D","answer_description":"","question_text":"You are a Looker analyst. You need to add a new field to your Looker report that generates SQL that will run against your company's database. You do not have the Develop permission. What should you do?","answer_images":[],"exam_id":2},{"id":"v8I5YfwQKg5dtDVMrWP8","answer_ET":"B","isMC":true,"question_id":45,"url":"https://www.examtopics.com/discussions/google/view/157022-exam-associate-data-practitioner-topic-1-question-5/","discussion":[{"timestamp":"1741236960.0","poster":"n2183712847","content":"Selected Answer: B\nThe best option is B. Cloud Storage lifecycle rule. Option B is best because lifecycle rules are built-in, automatic, and efficient for Cloud Storage object management like deletion based on age. Option A (Cloud Scheduler + Cloud Run) is incorrect because it's overly complex using two services when one built-in feature exists. Option C (Dataflow batch) is incorrect because Dataflow is overkill for simple deletion and more costly. Option D (Cloud Run daily) is incorrect because it's still more complex than lifecycle rules and daily frequency is likely unnecessary. Therefore, Option B, lifecycle rule, is the simplest, most efficient, and cost-effective solution.","comment_id":"1365747","upvote_count":"1"},{"comment_id":"1362237","content":"Selected Answer: B\nSimplest solution. All solutions can be made to work.","timestamp":"1740594480.0","poster":"rich_maverick","upvote_count":"1"},{"content":"Selected Answer: B\nThe most effective solution.","poster":"jatinbhatia2055","timestamp":"1740306900.0","comment_id":"1360463","upvote_count":"2"}],"answers_community":["B (100%)"],"choices":{"D":"Create a Cloud Run function that runs daily and deletes files older than seven days.","A":"Set up a Cloud Scheduler job that invokes a weekly Cloud Run function to delete files older than seven days.","C":"Develop a batch process using Dataflow that runs weekly and deletes files based on their age.","B":"Configure a Cloud Storage lifecycle rule that automatically deletes objects older than seven days."},"question_images":[],"unix_timestamp":1740306900,"answer_description":"","topic":"1","question_text":"You manage a Cloud Storage bucket that stores temporary files created during data processing. These temporary files are only needed for seven days, after which they are no longer needed. To reduce storage costs and keep your bucket organized, you want to automatically delete these files once they are older than seven days. What should you do?","timestamp":"2025-02-23 11:35:00","exam_id":2,"answer_images":[],"answer":"B"}],"exam":{"numberOfQuestions":72,"name":"Associate Data Practitioner","isBeta":false,"isImplemented":true,"isMCOnly":true,"id":2,"provider":"Google","lastUpdated":"11 Apr 2025"},"currentPage":9},"__N_SSP":true}