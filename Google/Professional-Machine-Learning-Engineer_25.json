{"pageProps":{"questions":[{"id":"xrcp46dvzoRw7ksmoEgY","answer_description":"","topic":"1","question_id":121,"answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/130543-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (58%)","A (27%)","Other"],"isMC":true,"discussion":[{"comment_id":"1357192","upvote_count":"1","poster":"NamitSehgal","content":"Selected Answer: D\nA (BigQuery ML Regression): A simple regression model is not designed for time series forecasting. It wouldn't capture the temporal dependencies in the data and wouldn't be able to effectively predict future bed usage based on past trends.\n\nD Forecasting Task","timestamp":"1739696700.0"},{"comment_id":"1345902","timestamp":"1737697500.0","upvote_count":"1","poster":"Hrishikesh1992","content":"Selected Answer: B\nWe have only 360 rows of data, I went with it because we require statistical model here rather than ML models."},{"timestamp":"1733245860.0","content":"Selected Answer: A\nA is the best option because:\n- BigQuery ML allows model build/training within BigQuery using SQL\n- This is a regression model, not a timeseries forecast; no ARIMA (B) fit! \n- Data transfer to Vertex AI (C, D) and usage of AutoML not needed \n- AutoML is better for larger datasets, BigQuery ML works for 365 rows","poster":"lunalongo","upvote_count":"2","comment_id":"1321458"},{"content":"Selected Answer: D\n'Vertex AI AutoML Forecasting' == for forecasting time series data","comment_id":"1256575","timestamp":"1722137940.0","poster":"forport","upvote_count":"3"},{"poster":"VinaoSilva","content":"Selected Answer: D\n\"You want to predict how many beds will be needed for patients each day\" = Forecasting","timestamp":"1719671340.0","upvote_count":"2","comment_id":"1239328"},{"upvote_count":"1","poster":"dija123","content":"Selected Answer: D\nTrain a Vertex AI AutoML Forecasting model","comment_id":"1238579","timestamp":"1719555480.0"},{"comment_id":"1231992","upvote_count":"1","poster":"info_appsatori","timestamp":"1718638020.0","content":"Selected Answer: A\nIDK, i going with A, because its maximize the speed of development and testing. Also in question it says: You need to create a model that uses the \"\"\"relationship\"\" between the number of surgeries scheduled and beds used. = linear regression problem."},{"poster":"b2aaace","upvote_count":"1","timestamp":"1714137540.0","comments":[{"upvote_count":"1","content":"\"You want to predict how many beds will be needed for patients each day in advance based on the scheduled surgeries.\"","comment_id":"1203536","timestamp":"1714303320.0","poster":"pinimichele01"}],"content":"Selected Answer: C\nI don't think this is a time series forecasting problem. The question clearly states that we should predict the number of beds based on the number of scheduled surgeries. this is a simple linear regression problem.","comment_id":"1202612"},{"upvote_count":"2","comments":[{"comment_id":"1198280","poster":"fitri001","upvote_count":"2","timestamp":"1713493320.0","content":"A. BigQuery ML regression: While BigQuery ML offers quick model building, a regression model might not capture the time-series aspect of daily bed occupancy. Daily bed occupancy might have trends or seasonality which a plain regression model wouldn't capture.\nB. BigQuery ML ARIMA: ARIMA models are specifically for stationary time series data, and hospital bed occupancy might not always be stationary (e.g., holiday season might lead to higher occupancy). Additionally, ARIMA models typically don't incorporate additional features like the number of scheduled surgeries.\nC. Vertex AI AutoML Regression: Similar to option A, a regression model might not capture the time series aspect. While Vertex AI offers AutoML regression, using a solution designed for time series forecasting is more suitable here."}],"comment_id":"1198279","timestamp":"1713493320.0","poster":"fitri001","content":"Selected Answer: D\nVertex AI AutoML Forecasting: This option leverages Vertex AI's AutoML capabilities for time series forecasting. It automatically explores different model types and hyperparameters to find the best fit for your data. This can significantly speed up model development compared to building a model from scratch.\nDate as time variable, surgeries as covariate: This approach acknowledges the time-series nature of bed occupancy with \"date\" as the time series variable. It also incorporates the \"number of scheduled surgeries\" as a covariate, allowing the model to learn the relationship between surgeries and bed usage."},{"poster":"pinimichele01","comments":[{"upvote_count":"1","poster":"pinimichele01","timestamp":"1713263760.0","comment_id":"1196524","content":"not b: ARIMA does not use number of scheduled surgeries, and it is stated that the prediction must be based on that variable"}],"timestamp":"1712555760.0","comment_id":"1191411","upvote_count":"1","content":"Selected Answer: D\nbest suited"},{"timestamp":"1710248940.0","upvote_count":"1","content":"Selected Answer: B\nI went with B.","comment_id":"1171709","poster":"CHARLIE2108"},{"upvote_count":"1","timestamp":"1706685180.0","comment_id":"1136480","poster":"sonicclasps","content":"Selected Answer: D\nbest suited, and treats the input as a time series, unlike A"},{"comment_id":"1134189","timestamp":"1706454000.0","poster":"Yan_X","upvote_count":"2","content":"Selected Answer: D\nD, as B doesn't mention the 'number of scheduled surgeries'."},{"upvote_count":"2","comment_id":"1124124","timestamp":"1705403460.0","poster":"shadz10","content":"Selected Answer: D\nD is correct I believe","comments":[{"comment_id":"1125287","content":"https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/overview","upvote_count":"1","poster":"shadz10","timestamp":"1705523340.0"}]},{"comment_id":"1123464","upvote_count":"4","timestamp":"1705330740.0","content":"Selected Answer: A\nA.\nUsing BigQuery to comply requirement of speed of development.\nARIMA does not use number of scheduled surgeries, and it is stated that the prediction must be based on that variable. So it must be A. LR model on BQ using scheduled surgeries, day of the week, etc, as predictors.","poster":"b1a8fae"},{"content":"365 days of data may be insufficient for big query I’m going with C","timestamp":"1704839040.0","poster":"shadz10","comment_id":"1117889","upvote_count":"3","comments":[{"content":"D* not C Forecasting models are well-suited for predicting future values based on past trends, making them ideal for the goal of predicting bed occupancy for upcoming days. Dataset is too small for bigquery.","poster":"shadz10","upvote_count":"2","timestamp":"1704950640.0","comment_id":"1119379"}]},{"content":"Selected Answer: B\nUsing B instead of D as it requires speed of development.","comments":[{"comment_id":"1124122","poster":"shadz10","timestamp":"1705403400.0","content":"While ARIMA models are commonly used for time series forecasting, they are more suitable for univariate time series data and might require additional manual intervention for feature engineering.\n\nIn this case, we have multiple variables such as the number of scheduled surgeries, the number of beds occupied, and the date. AutoML Forecasting in option D is designed to handle multivariate time series data, and it automates much of the modeling process, including feature selection and hyperparameter tuning. This can potentially result in a faster and more efficient development and testing process compared to manually implementing and tuning an ARIMA model.","upvote_count":"2"}],"timestamp":"1704683280.0","upvote_count":"1","poster":"kalle_balle","comment_id":"1116359"}],"question_images":[],"answer_images":[],"unix_timestamp":1704683280,"choices":{"D":"Create a Vertex AI tabular dataset. Train a Vertex AI AutoML Forecasting model, with number of beds as the target variable, number of scheduled surgeries as a covariate and date as the time variable.","A":"Create a BigQuery table. Use BigQuery ML to build a regression model, with number of beds as the target variable, and number of scheduled surgeries and date features (such as day of week) as the predictors.","C":"Create a Vertex AI tabular dataset. Train an AutoML regression model, with number of beds as the target variable, and number of scheduled minor surgeries and date features (such as day of the week) as the predictors.","B":"Create a BigQuery table. Use BigQuery ML to build an ARIMA model, with number of beds as the target variable, and date as the time variable."},"answer":"D","question_text":"You work for a hospital that wants to optimize how it schedules operations. You need to create a model that uses the relationship between the number of surgeries scheduled and beds used. You want to predict how many beds will be needed for patients each day in advance based on the scheduled surgeries. You have one year of data for the hospital organized in 365 rows.\n\nThe data includes the following variables for each day:\n• Number of scheduled surgeries\n• Number of beds occupied\n• Date\n\nYou want to maximize the speed of model development and testing. What should you do?","exam_id":13,"timestamp":"2024-01-08 04:08:00"},{"id":"vEkyFYPZCQyNpHE8Efyi","answer_ET":"A","discussion":[{"content":"Selected Answer: A\nThe correct answer is:\n\nA. Use the Kubeflow Pipelines SDK to implement the pipeline. Use the BigQueryJobOp component to run the preprocessing script and the CustomTrainingJobOp component to launch a Vertex AI training job.\n\nKubeflow Pipelines (KFP) is a good choice for orchestrating training pipelines, especially since the requirement is to minimize model development and training time.\n\nThe BigQueryJobOp component is appropriate because the data preprocessing is already performed in BigQuery using SQL scripts. Using BigQueryJobOp avoids unnecessary additional processing layers.\n\nThe CustomTrainingJobOp component allows launching a Vertex AI training job, which aligns with the need for scalable and managed model training.","timestamp":"1743479340.0","poster":"batevv","comment_id":"1416093","upvote_count":"1"},{"upvote_count":"2","comment_id":"1321461","poster":"lunalongo","timestamp":"1733246880.0","content":"Selected Answer: C\nC is the best option because: \n- TFX is designed for ML pipelines, reducing custom code needs and development time and training time as the statement requires\n- ExampleGen with the BQ executor eliminate data export needs; \n- Trainer component seamlessly integrates with Vertex AI, leveraging its managed infrastructure for training, further reducing development and operational overhead.\n\nA & B uses Kubeflow Pipelines, which would mean more development time and code customization (your model is in TensorFlow); \n\nD puts preprocessing inside input_fn, which is generally less efficient for large datasets and complex transformations."},{"upvote_count":"1","content":"Selected Answer: D\n\"If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX.\"\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline\n\nGoogle recommends TFX for large amount of structured data. Use input_fn for the Tensorflow model as it will output a tf.data.Dataset object.\n\nNote: As it is not mentionned that we are working with terabytes of data, Kubeflow is a viable option and i would choose answer A but i'll stick to google's recommendations","timestamp":"1724851680.0","poster":"tdum76000","comment_id":"1274080"},{"poster":"forport","content":"Selected Answer: C\nOption C is the most suitable because TFX provides a comprehensive MLOps framework, seamlessly integrating data ingestion, preprocessing, and model training, while also offering strong support for Vertex AI, making it the most efficient solution for the given use case.","upvote_count":"1","comment_id":"1263322","timestamp":"1723267980.0"},{"poster":"AK2020","content":"Selected Answer: C\nC. Use the TensorFlow Extended SDK to implement the pipeline. Use the ExampleGen component with the BigQuery executor to ingest the data, the Transform component to preprocess the data, and the Trainer component to launch a Vertex AI training job.","timestamp":"1722654180.0","comment_id":"1260116","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nI go with A\nKubeflow Pipelines SDK: supports machine learning and includes components specifically for tasks like data preprocessing, model training, and validation.\n\nBigQueryJobOp: enabling you to preprocess data using SQL scripts efficiently within BigQuery.","poster":"TanTran04","timestamp":"1720440300.0","comment_id":"1244322"},{"content":"Selected Answer: C\nExample Gen directly ingest data from BigQuery and the transform component makes it more efficient than using an input fn.\n\nI chose C over A and B because kubeflow pipelines is more sophisticated and requires more setup and effort because of it's customizability.","comment_id":"1213705","poster":"SausageMuffins","timestamp":"1716113820.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1713623820.0","comment_id":"1199192","poster":"gscharly","content":"Selected Answer: A\nagree with guilhermebutzke"},{"poster":"pinimichele01","timestamp":"1713010080.0","comment_id":"1194942","upvote_count":"1","content":"Selected Answer: A\nagree with guilhermebutzke"},{"poster":"Shark0","timestamp":"1712332680.0","comment_id":"1189994","content":"Selected Answer: C\nGiven the requirement to minimize model development and training time while creating a training pipeline for a wide and deep model trained on datasets preprocessed using a SQL script in BigQuery, the most suitable option is:\n\nC. Use the TensorFlow Extended SDK to implement the pipeline. Use the ExampleGen component with the BigQuery executor to ingest the data, the Transform component to preprocess the data, and the Trainer component to launch a Vertex AI training job.\n\nThis option leverages TensorFlow Extended (TFX), which is designed for scalable and production-ready machine learning pipelines. The ExampleGen component with the BigQuery executor efficiently ingests data from BigQuery. The Transform component applies preprocessing steps to the data, and the Trainer component launches a Vertex AI training job, minimizing the time and effort required for model development and training.","upvote_count":"1"},{"timestamp":"1709392800.0","poster":"Carlose2108","upvote_count":"1","comment_id":"1164177","content":"Why not C?"},{"upvote_count":"3","content":"My Answer: A\nAccording with this documentation:\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/overview\n\nA: CORRECT: BigQueryJobOp for running the existing preprocessing script that already resides there, CustomTrainingJobOp for launching custom training jobs on Vertex AI, which aligns with the requirement of using the pre-trained TensorFlow model.\n\nB: Not Correct: While DataflowPythonJobOp can be used for preprocessingthis increasing development time compared to the simpler BigQueryJobOp approach.\n\nC and D: Not Correct: While possible, using the TensorFlow Extended SDK with its components introduces unnecessary complexity for this specific scenario. For example, why use ExampleGen? Implementing preprocessing within the model's input_fn is generally not recommended due to potential efficiency drawbacks and training-serving skew issues.","comment_id":"1153503","timestamp":"1708282560.0","poster":"guilhermebutzke"},{"upvote_count":"2","content":"Selected Answer: A\nD is wrong. Google doesn't recommend to use input_fn for preprocessing\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices#preprocessing_options_summary","poster":"BlehMaks","comment_id":"1130819","timestamp":"1706110740.0"},{"content":"Selected Answer: D\nAddressing Limitations of Other Options:\n\nKubeflow Pipelines (A and B): While Kubeflow offers flexibility, it might require more setup and configuration, potentially increasing development time compared to TFX's integrated approach.\nSeparate Preprocessing (C): Using a separate Transform component for preprocessing can add complexity and potential overheads, especially for instance-level transformations that can often be directly integrated within the model's input pipeline.","upvote_count":"1","timestamp":"1705120140.0","comment_id":"1121293","poster":"pikachu007"}],"choices":{"D":"Use the TensorFlow Extended SDK to implement the pipeline Implement the preprocessing steps as part of the input_fn of the model. Use the ExampleGen component with the BigQuery executor to ingest the data and the Trainer component to launch a Vertex AI training job.","A":"Use the Kubeflow Pipelines SDK to implement the pipeline. Use the BigQueryJobOp component to run the preprocessing script and the CustomTrainingJobOp component to launch a Vertex AI training job.","C":"Use the TensorFlow Extended SDK to implement the pipeline Use the ExampleGen component with the BigQuery executor to ingest the data the Transform component to preprocess the data, and the Trainer component to launch a Vertex AI training job.","B":"Use the Kubeflow Pipelines SDK to implement the pipeline. Use the DataflowPythonJobOp component to preprocess the data and the CustomTrainingJobOp component to launch a Vertex AI training job."},"isMC":true,"answer_description":"","topic":"1","timestamp":"2024-01-13 05:29:00","answer_images":[],"question_text":"You recently developed a wide and deep model in TensorFlow. You generated training datasets using a SQL script that preprocessed raw data in BigQuery by performing instance-level transformations of the data. You need to create a training pipeline to retrain the model on a weekly basis. The trained model will be used to generate daily recommendations. You want to minimize model development and training time. How should you develop the training pipeline?","exam_id":13,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/131028-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1705120140,"answer":"A","question_id":122,"answers_community":["A (43%)","C (43%)","14%"]},{"id":"4YsUhSMQypF8HvC9hMnv","question_text":"You are training a custom language model for your company using a large dataset. You plan to use the Reduction Server strategy on Vertex AI. You need to configure the worker pools of the distributed training job. What should you do?","answer_ET":"B","topic":"1","question_images":[],"discussion":[{"content":"Selected Answer: B\nReduction server strategy:\n1. Only GPUs\n2. You do not use GPUs for the Reduction Server nodes. \n\nhttps://cloud.google.com/vertex-ai/docs/training/distributed-training","upvote_count":"1","timestamp":"1733862780.0","comment_id":"1324741","poster":"Pau1234"},{"content":"Selected Answer: B\nB is the right answer because:\n- Reduction Server strategy is generally implemented with GPUs, not TPUs.\n- First 2 pools' replicas perform model training; need GPUs for faster processing\n- Container image should contain your custom training code.\n- 3rd pool contains reduction server, no GPU is needed here; prioritize network bandwidth instead!","upvote_count":"1","poster":"lunalongo","comment_id":"1321469","timestamp":"1733247420.0"},{"comment_id":"1287456","poster":"wences","timestamp":"1726951920.0","content":"Selected Answer: B\nThe real reason for answer B is the custom model, which means it was not suited well for TPU","upvote_count":"1"},{"poster":"fitri001","upvote_count":"2","timestamp":"1713493680.0","comment_id":"1198284","comments":[{"timestamp":"1713493740.0","upvote_count":"1","comment_id":"1198285","content":"A. GPUs for Reduction Server: Reduction Server itself doesn't require or benefit from GPUs. It focuses on communication and reduction of gradients. It's better to use a CPU-based machine type for the third pool.\nC. TPUs instead of GPUs: While TPUs can be used for training some language models, Reduction Server specifically works with GPUs using the NCCL library. Configure your first two pools with GPUs for your training code.\nD. TPUs in Reduction Server pool: Similar to option A, Reduction Server doesn't benefit from TPUs. It's best to use a CPU with high bandwidth for the third pool.","poster":"fitri001"}],"content":"Selected Answer: B\nGPUs for Training: Configure the first two worker pools with GPUs to leverage the hardware acceleration capabilities for your custom language model training code.\nReduction Server without GPUs: The third worker pool should use the reductionserver container image. This image is pre-configured for Reduction Server functionality and doesn't require GPUs.\nHigh-Bandwidth CPU: Choose a machine type with high bandwidth for the third pool since Reduction Server focuses on communication and gradient reduction."},{"content":"Selected Answer: B\nhttps://cloud.google.com/blog/topics/developers-practitioners/optimize-training-performance-reduction-server-vertex-ai\n\nIn this article, we introduce Reduction Server, a new Vertex AI feature that optimizes bandwidth and latency of multi-node distributed training on NVIDIA GPUs for synchronous data parallel algorithms.","comment_id":"1191416","poster":"pinimichele01","upvote_count":"1","timestamp":"1712557500.0"},{"content":"Selected Answer: B\nTPUs are not supported for reductionserver so B","poster":"shadz10","upvote_count":"3","comment_id":"1123039","timestamp":"1705291500.0"},{"poster":"winston9","timestamp":"1705137960.0","upvote_count":"2","content":"Selected Answer: B\nbandwidth is important for the reduction server","comment_id":"1121453"},{"timestamp":"1705120320.0","upvote_count":"2","poster":"pikachu007","content":"Selected Answer: B\nWorker Pools 1 and 2:\nThese pools are responsible for the actual model training tasks.\nThey require GPUs (or TPUs, if applicable to your model) to accelerate model computations.\nThey run the container image containing your training code.\nWorker Pool 3:\nThis pool is dedicated to the reduction server.\nIt doesn't require accelerators (GPUs or TPUs) for gradient aggregation.\nPrioritize machines with high network bandwidth to optimize gradient exchange.\nUse the specific reductionserver","comment_id":"1121294"}],"unix_timestamp":1705120320,"timestamp":"2024-01-13 05:32:00","question_id":123,"answer":"B","answers_community":["B (100%)"],"exam_id":13,"answer_images":[],"answer_description":"","choices":{"A":"Configure the machines of the first two worker pools to have GPUs, and to use a container image where your training code runs. Configure the third worker pool to have GPUs, and use the reductionserver container image.","D":"Configure the machines of the first two pools to have TPUs, and to use a container image where your training code runs. Configure the third pool to have TPUs, and use the reductionserver container image.","C":"Configure the machines of the first two worker pools to have TPUs and to use a container image where your training code runs. Configure the third worker pool without accelerators, and use the reductionserver container image without accelerators, and choose a machine type that prioritizes bandwidth.","B":"Configure the machines of the first two worker pools to have GPUs and to use a container image where your training code runs. Configure the third worker pool to use the reductionserver container image without accelerators, and choose a machine type that prioritizes bandwidth."},"url":"https://www.examtopics.com/discussions/google/view/131029-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true},{"id":"nq7e9sB2HmKDpWj2RJo0","answers_community":["D (53%)","B (27%)","10%","10%"],"unix_timestamp":1623093240,"question_text":"You have deployed multiple versions of an image classification model on AI Platform. You want to monitor the performance of the model versions over time. How should you perform this comparison?","answer_ET":"D","choices":{"C":"Compare the receiver operating characteristic (ROC) curve for each model using the What-If Tool.","A":"Compare the loss performance for each model on a held-out dataset.","B":"Compare the loss performance for each model on the validation data.","D":"Compare the mean average precision across the models using the Continuous Evaluation feature."},"answer":"D","timestamp":"2021-06-07 21:14:00","isMC":true,"exam_id":13,"question_images":[],"topic":"1","discussion":[{"content":"Answer is D","upvote_count":"13","comment_id":"384472","poster":"chohan","timestamp":"1623969060.0"},{"comment_id":"441647","upvote_count":"6","poster":"Danny2021","content":"D is correct. Choose the feature / capability GCP provides is always a good bet. :)","timestamp":"1631135040.0"},{"comment_id":"1300171","content":"Selected Answer: D\n[B] Compare the loss performance for each model on the validation data.\n--> Not validation data but testing data","upvote_count":"1","poster":"jkkim_jt","timestamp":"1729371240.0"},{"timestamp":"1719467520.0","poster":"bludw","comments":[{"timestamp":"1741492800.0","comment_id":"1367004","content":"true. I guess people chose B because the official study guide said so but in my view that's obviously wrong.","upvote_count":"1","poster":"RyanTan"}],"content":"Selected Answer: A\nThe answer is A. I am not sure why people choose B vs A as you may overfit your validation set. And you are using your held-out set really rare == no option to overfit.","upvote_count":"3","comment_id":"1237895"},{"timestamp":"1717560360.0","upvote_count":"1","poster":"Wookjae","content":"Continuous Evaluation feature is deprecated.","comments":[{"comment_id":"1224826","upvote_count":"1","content":"so is the what if tool","poster":"Goosemoose","timestamp":"1717604220.0"}],"comment_id":"1224495"},{"upvote_count":"3","comments":[{"content":"you minimise loss DURING TRAINING to get the best model. you don't use it for performance monitoring of a deployed model","upvote_count":"2","comment_id":"1323350","poster":"joqu","timestamp":"1733630700.0"}],"content":"Selected Answer: B\nIn the official study guide, this was the explanation given for answer B : \n\"The image classification model is a deep learning model. You minimize the loss of deep learning models to get the best model. So comparing loss performance for each model on validation data is the correct answer.\"","timestamp":"1717252500.0","comment_id":"1222691","poster":"saadci"},{"content":"Selected Answer: D\nD - because you are using a Google provided feature.\nremember in this exam its important to always choose the google services over anything else","upvote_count":"4","timestamp":"1700059680.0","comment_id":"1071552","poster":"Sum_Sum"},{"timestamp":"1696502940.0","upvote_count":"3","comment_id":"1025547","poster":"claude2046","content":"mAP is for object detection, so the answer should be B"},{"content":"Selected Answer: D\nWent with D, using continuous evaluation feature seems correct to me.","poster":"Liting","comment_id":"945702","upvote_count":"1","timestamp":"1688735340.0"},{"content":"Selected Answer: D\nI choose by myself D. But as I read the post here https://www.v7labs.com/blog/mean-average-precision, I was not sure about D. \nIt wrote mAP is commonly used for object detection or instance segmentation tasks. \nValidation Dataset in GCP context: not trained dataset and not seen dataset","poster":"SamuelTsch","timestamp":"1688715180.0","upvote_count":"1","comment_id":"945438"},{"content":"Selected Answer: D\nD. Compare the mean average precision across the models using the Continuous Evaluation feature\nhttps://cloud.google.com/vertex-ai/docs/evaluation/introduction\nVertex AI provides model evaluation metrics, such as precision and recall, to help you determine the performance of your models...\nVertex AI supports evaluation of the following model types:\nAuPRC: The area under the precision-recall (PR) curve, also referred to as average precision. This value ranges from zero to one, where a higher value indicates a higher-quality model.","poster":"Voyager2","timestamp":"1685465100.0","upvote_count":"1","comment_id":"910437"},{"comment_id":"892697","content":"Selected Answer: D\nWent with D","poster":"M25","upvote_count":"1","timestamp":"1683608400.0"},{"content":"Selected Answer: B\nI go for B. Option D is good when we are already in production","upvote_count":"1","timestamp":"1681871040.0","poster":"lucaluca1982","comment_id":"874238"},{"comments":[{"upvote_count":"3","content":"Please, How? B is not monitoring. It is a validation. The definition of monitoring states:\n\"observe and check the progress or quality of (something) over a period of time\"\nSo it is a continuous process. Each option A,B,C are just one time check, not monitoring.","poster":"Jarek7","comment_id":"893441","timestamp":"1683667560.0"}],"poster":"prakashkumar1234","content":"o monitor the performance of the model versions over time, you should compare the loss performance for each model on the validation data. Therefore, option B is the correct answer.","timestamp":"1679422320.0","comment_id":"846226","upvote_count":"1"},{"content":"Selected Answer: B\nThe best option to monitor the performance of multiple versions of an image classification model on AI Platform over time is to compare the loss performance for each model on the validation data.\n\nOption B is the best approach because comparing the loss performance of each model on the validation data is a common method to monitor machine learning model performance over time. The validation data is a subset of the data that is not used for model training, but is used to evaluate its performance during training and to compare different versions of the model. By comparing the loss performance of each model on the same validation data, you can determine which version of the model has better performance.","upvote_count":"4","poster":"Fatiy","comment_id":"824846","timestamp":"1677591060.0"},{"content":"Selected Answer: D\nIf you have multiple model versions in a single model and have created an evaluation job for each one, you can view a chart comparing the mean average precision of the model versions over time","upvote_count":"1","timestamp":"1675731180.0","comment_id":"800406","poster":"enghabeth"},{"poster":"guilhermebutzke","comment_id":"794244","timestamp":"1675173600.0","upvote_count":"3","content":"Guys, I not sure about the answer D ... And maybe you could help me in my arguments. \n\nI think choose loss to compare the model performance is better than see for metrics. For example, when can build an image model classification that has good precision metrics, because the class in unbalanced, but the loss could be terrible because of kind of loss choose that penalizes classes. \n\nso, losses are better than metrics to available models, and the answer is in A or B.\n\nI thought that the A could be the answer because I see validation as a part of the training process. So, If we want to test the model performance over time, we have to use new data, which I suppose to be the held-out data."},{"timestamp":"1671140220.0","content":"Selected Answer: D\nans: D","poster":"wish0035","upvote_count":"1","comment_id":"746532"},{"content":"Selected Answer: D\nSince you want to monitor the performance of the model versions *over time*, use the Continuous Evaluation feature, so D","comment_id":"725687","poster":"EFIGO","upvote_count":"1","timestamp":"1669281720.0"},{"content":"Answer : D\nhttps://cloud.google.com/ai-platform/prediction/docs/continuous-evaluation#how_it_works","comment_id":"721798","poster":"vakati","timestamp":"1668838800.0","upvote_count":"3"},{"comment_id":"647196","timestamp":"1660567740.0","upvote_count":"1","content":"Selected Answer: D\nCorrect answer is \"D\"","poster":"GCP72"},{"upvote_count":"1","comment_id":"643212","content":"Selected Answer: D\nD is correct","poster":"sachinxshrivastav","timestamp":"1659765000.0"},{"comments":[{"poster":"Mohamed_Mossad","timestamp":"1657538280.0","comment_id":"629959","content":"I was wrong , D is the best option\nYou have deployed multiple versions of an image classification model on Al Platform. You want to\nmonitor the performance of the model versions overtime. How should you perform this\ncomparison","upvote_count":"1"}],"poster":"Mohamed_Mossad","timestamp":"1655067360.0","content":"Selected Answer: C\nD maybe wrong as mean average precision metric for object detection not image classification , A,B are near to each other , I will vote for C","upvote_count":"3","comment_id":"615495"},{"poster":"sally_1310","timestamp":"1645027020.0","upvote_count":"1","comment_id":"548735","content":"Vote for B. \nIf the question didn't mention about image classification model I'll choose D"},{"timestamp":"1643334780.0","comment_id":"534263","content":"Selected Answer: D\nshould be D","upvote_count":"2","poster":"xiaoF"},{"content":"Answer: D\nhttps://cloud.google.com/ai-platform/prediction/docs/continuous-evaluation/view-metrics","upvote_count":"2","comment_id":"517718","timestamp":"1641406380.0","poster":"josh2022"},{"upvote_count":"1","poster":"pddddd","timestamp":"1632733500.0","comment_id":"452286","comments":[{"content":"what about what-if-tool ?","comment_id":"456567","upvote_count":"1","timestamp":"1633263600.0","poster":"mouhannad"}],"content":"D is not GA."},{"comment_id":"437414","content":"I think the right answer is B\nOption D seems way too overkill and it focuses more on providing continual feedback on how the same model is performing over time.","comments":[{"comment_id":"569954","content":"This is the same model. It says you deployed different versions of it on AI Platform. That's precisely what you have to do to use the continuous evaluation tool: deploy your model as a model version, and then create an evaluation job for that version.","upvote_count":"1","poster":"giaZ","timestamp":"1647535140.0"}],"upvote_count":"1","poster":"Swagluke","timestamp":"1630521660.0"},{"timestamp":"1627493820.0","comment_id":"416354","upvote_count":"3","poster":"sensev","content":"Also agree answer is D, see: https://cloud.google.com/ai-platform/prediction/docs/continuous-evaluation"},{"upvote_count":"4","comment_id":"377009","poster":"inder0007","content":"The correct answer is D","timestamp":"1623093240.0"}],"url":"https://www.examtopics.com/discussions/google/view/54840-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":"","question_id":124,"answer_images":[]},{"id":"CIhja4fzMILMGsJx9gv1","topic":"1","question_id":125,"answers_community":["B (100%)"],"answer_ET":"B","answer_images":[],"isMC":true,"question_images":[],"timestamp":"2024-01-13 05:36:00","discussion":[{"comments":[{"timestamp":"1729305000.0","comment_id":"1198289","poster":"fitri001","upvote_count":"2","content":"A. Data Validation: While data validation is important, it doesn't guarantee consistent preprocessing logic. You need to ensure the same transformations are applied.\nC. Share Code with End Users: Sharing code with end-users might not be ideal, especially if it requires specific libraries or configurations for execution outside of the pipeline.\nD. Batching and Dataflow: Batching real-time requests for Dataflow processing might introduce latency and defeat the purpose of real-time inference."}],"upvote_count":"1","comment_id":"1198287","timestamp":"1729305000.0","poster":"fitri001","content":"Selected Answer: B\nRefactored Transformation Code: By refactoring the transformation code from the batch pipeline, you can create a reusable module that performs the same preprocessing steps.\nSame Code in Endpoint: Utilize the refactored code within your real-time inference endpoint. This ensures the data is preprocessed identically to how it was preprocessed during training."},{"content":"Selected Answer: B\nagree with guilhermebutzke","timestamp":"1728821460.0","poster":"pinimichele01","comment_id":"1194945","upvote_count":"1"},{"content":"Selected Answer: B\nMy Answer B:\n\nB. This option ensures that the preprocessing logic used during training, which has already been validated and tested, is applied consistently during real-time inference. By making the transformation code reusable outside of the batch pipeline and utilizing it in the endpoint, you ensure that the same preprocessing steps are applied to incoming data during inference, thus maintaining consistency between training and serving.\n\nA:  While data validation is essential, it only ensures the format. It doesn't guarantee consistent preprocessing logic between training and serving.\n\nC: Sharing code with end-users might not be desirable for security or maintainability reasons.\n\nD: Batching introduces latency and might not be suitable for real-time needs. Additionally, using the entire Dataflow pipeline might be inefficient for individual requests.","comment_id":"1153583","upvote_count":"3","timestamp":"1724009040.0","poster":"guilhermebutzke"},{"content":"Selected Answer: B\nThe transformation logic code in the serving_fn function defines the serving interface of your SavedModel for online prediction. If you implement the same transformations that were used for preparing training data in the transformation logic code of the serving_fn function, it ensures that the same transformations are applied to new prediction data points when they're served.\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices","comment_id":"1125327","poster":"shadz10","timestamp":"1721243580.0","upvote_count":"3"},{"upvote_count":"1","comment_id":"1121295","timestamp":"1720838160.0","poster":"pikachu007","content":"Selected Answer: B\nA. Data validation: While essential, it doesn't guarantee consistency if the preprocessing logic itself differs between pipeline and endpoint.\nC. Sharing code with end users: This shifts the preprocessing burden to end users, potentially leading to inconsistencies and errors, and isn't feasible for real-time inference.\nD. Batching real-time requests: This introduces latency and might not align with real-time requirements, as users expect immediate responses."}],"question_text":"You have trained a model by using data that was preprocessed in a batch Dataflow pipeline. Your use case requires real-time inference. You want to ensure that the data preprocessing logic is applied consistently between training and serving. What should you do?","exam_id":13,"choices":{"B":"Refactor the transformation code in the batch data pipeline so that it can be used outside of the pipeline. Use the same code in the endpoint.","C":"Refactor the transformation code in the batch data pipeline so that it can be used outside of the pipeline. Share this code with the end users of the endpoint.","A":"Perform data validation to ensure that the input data to the pipeline is the same format as the input data to the endpoint.","D":"Batch the real-time requests by using a time window and then use the Dataflow pipeline to preprocess the batched requests. Send the preprocessed requests to the endpoint."},"unix_timestamp":1705120560,"url":"https://www.examtopics.com/discussions/google/view/131030-exam-professional-machine-learning-engineer-topic-1-question/","answer":"B","answer_description":""}],"exam":{"provider":"Google","lastUpdated":"11 Apr 2025","isBeta":false,"name":"Professional Machine Learning Engineer","id":13,"isMCOnly":true,"numberOfQuestions":304,"isImplemented":true},"currentPage":25},"__N_SSP":true}