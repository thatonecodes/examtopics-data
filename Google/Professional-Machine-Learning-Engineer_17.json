{"pageProps":{"questions":[{"id":"9PgPXYMCnb42LRo5vkQU","url":"https://www.examtopics.com/discussions/google/view/130588-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"choices":{"D":"Configure a n1-standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use MultiWorkerMirroredStrategy to train the model.","A":"Configure a v3-8 TPU VM. SSH into the VM to train and debug the model.","C":"Configure a n1 -standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use ParameterServerStraregv to train the model.","B":"Configure a v3-8 TPU node. Use Cloud Shell to SSH into the Host VM to train and debug the model."},"answer":"D","answer_images":[],"unix_timestamp":1704729060,"timestamp":"2024-01-08 16:51:00","discussion":[{"content":"Selected Answer: D\nAnswer D.\n- GPUs are more accurate at complex numerical calculations than TPUs.\n- MultiWorkerMirroredStrategy will train on multiple machines.","poster":"5091a99","timestamp":"1741235760.0","upvote_count":"1","comment_id":"1365727"},{"timestamp":"1739847540.0","upvote_count":"1","poster":"NamitSehgal","comment_id":"1358099","content":"Selected Answer: A\ncomplex TensorFlow models use TPUs"},{"comment_id":"1302623","upvote_count":"1","timestamp":"1729796460.0","poster":"JDpmle2024","content":"How would D be correct:\n\nD. Configure a n1-standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use MultiWorkerMirroredStrategy to train the model.\n\nThis is a single VM. The MultiWorkerMirroredStrategy is for multiple VMs. \n\nBased on this, choosing A."},{"upvote_count":"2","content":"MultiWorkerMirroredStrategy is for multiple workers, each with one or more GPUs. For a single worker/vm with multiple GPUs it would be MirroredStrategy, so D is definitely wrong.\nC is wrong as that is a totally unrelated concept, B is probably wrong as it's much less convenient than using a terminal (B vs A is tough call, but A replicates their existing setup most closely)","poster":"baimus","timestamp":"1726037880.0","comment_id":"1282000"},{"comment_id":"1234896","content":"Option D Configure a n1-standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use MultiWorkerMirroredStrategy to train the model. is indeed a correct answer.\nMultiWorkerMirroredStrategy: This strategy allows you to distribute your training process across multiple machines (in this case, the 4 NVIDIA P100 GPUs) while maintaining synchronization between them.\nNVIDIA P100 GPUs: These high-performance GPUs are well-suited for computationally intensive tasks like deep learning model training.","timestamp":"1719000240.0","poster":"AzureDP900","upvote_count":"2"},{"upvote_count":"3","timestamp":"1718741040.0","content":"Selected Answer: A\nIt says \"state-of-art\" and TPU is more recent than GPU. No need to log using Cloud Shell into VM and there's no mention about cost. So TPU + SSH directly into VM could be the choice.","poster":"inc_dev_ml_001","comment_id":"1232592"},{"comment_id":"1199612","upvote_count":"2","content":"Selected Answer: D\nDebugging Ease: SSHing into a VM provides a familiar environment for researchers to use familiar debugging tools within the VM for their complex TensorFlow models. This maintains ease of debugging compared to TPUs which require special considerations.\nFaster Training: Utilizing 4 NVIDIA P100 GPUs within the VM leverages parallel processing capabilities to significantly accelerate training compared to a CPU-only VM.","poster":"fitri001","timestamp":"1713696720.0"},{"upvote_count":"2","comment_id":"1190924","content":"Selected Answer: D\nthe need to balance ease of debugging and reduce training time","timestamp":"1712491440.0","poster":"pinimichele01"},{"content":"Selected Answer: D\nMy choice is D. \n\nWhile TPUs offer faster training, they can be less convenient for debugging due to limitations in tooling and visualization, such as the lack of support for some debuggers and limited visualization options. \n\nComparing options C and D, MultiWorkerMirroredStrategy uses synchronous distributed training across multiple workers, making it easier to inspect intermediate states and variables during debugging. In contrast, ParameterServerStraregv utilizes asynchronous multi-machine training, which can be less intuitive to debug. However, it's important to note that ParameterServerStraregv might be more efficient for training extremely large models. Therefore, considering the specific need for ease of debugging in this scenario, MultiWorkerMirroredStrategy appears to be the more suitable choice.","timestamp":"1707256620.0","poster":"guilhermebutzke","comment_id":"1142774","upvote_count":"2"},{"upvote_count":"4","poster":"pikachu007","timestamp":"1704874860.0","content":"Selected Answer: D\nGiven the need to balance ease of debugging and reduce training time for complex models in TensorFlow, option D - \"Configure an n1-standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use MultiWorkerMirroredStrategy to train the model\" appears to be more suitable. This setup utilizes NVIDIA P100 GPUs for computational power and employs MultiWorkerMirroredStrategy, which can distribute the workload across GPUs efficiently, potentially reducing training time while maintaining a relatively straightforward environment for debugging.","comment_id":"1118368"},{"comment_id":"1116738","timestamp":"1704729060.0","upvote_count":"1","poster":"b1a8fae","content":"Selected Answer: D\nD.\n\nCannot be B, because node architecture make it difficult to debug: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu-node-arch\n\nWhile TPUs are faster than GPUs for certain scenarios, and never slower, they are less easy to debug. Parallelizing the training across different workers (GPUs) using MultiWorkerMirroredStrategy makes most sense to me."}],"question_text":"You work with a team of researchers to develop state-of-the-art algorithms for financial analysis. Your team develops and debugs complex models in TensorFlow. You want to maintain the ease of debugging while also reducing the model training time. How should you set up your training environment?","answer_description":"","answers_community":["D (75%)","A (25%)"],"topic":"1","answer_ET":"D","isMC":true,"exam_id":13,"question_id":81},{"id":"blFHPUNMTIoIRKEALdRF","unix_timestamp":1704729480,"answers_community":["D (100%)"],"question_id":82,"exam_id":13,"isMC":true,"topic":"1","choices":{"D":"1. Create an experiment in Vertex AI Experiments.\n2. Create a Vertex AI pipeline with a custom model training job as part of the pipeline. Configure the pipeline’s parameters to include those you are investigating.\n3. Submit multiple runs to the same experiment, using different values for the parameters.","C":"1. Create a Vertex AI Workbench notebook for each of the different input datasets.\n2. In each notebook, run different local training jobs with different combinations of the max tree depth and optimizer learning rate parameters.\n3. After each notebook finishes, append the results to a BigQuery table.","A":"1. Use BigQueryML to create a boosted tree regressor, and use the hyperparameter tuning capability.\n2. Configure the hyperparameter syntax to select different input datasets: max tree depths, and optimizer learning rates. Choose the grid search option.","B":"1. Create a Vertex AI pipeline with a custom model training job as part of the pipeline. Configure the pipeline’s parameters to include those you are investigating.\n2. In the custom training step, use the Bayesian optimization method with F1 score as the target to maximize."},"answer":"D","answer_description":"","timestamp":"2024-01-08 16:58:00","question_images":[],"answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/130590-exam-professional-machine-learning-engineer-topic-1-question/","answer_images":[],"discussion":[{"content":"Selected Answer: D\nVertex AI Experiments: This service allows you to group and track different pipeline runs associated with the same experiment. This facilitates comparing runs with various parameter combinations.\nVertex AI Pipelines: Pipelines enable you to define a workflow for training your model. You can include a custom training step within the pipeline and configure its parameters as needed. This ensures reproducibility as all runs follow the same defined workflow.\nSubmitting multiple runs: By submitting multiple pipeline runs to the same experiment with different parameter values, you can efficiently explore various configurations and track their performance metrics like F1 score, training time, and model complexity within Vertex AI Experiments.","poster":"fitri001","comments":[{"comment_id":"1199868","timestamp":"1729554720.0","upvote_count":"1","content":"A. BigQuery ML: BigQuery ML doesn't offer functionalities like Vertex AI Pipelines for building and managing workflows. It also lacks experiment tracking capabilities.\nC. Vertex AI Workbench notebooks: While Vertex AI Workbench provides notebooks for running training jobs, this approach wouldn't be reproducible. Each notebook would be a separate entity, making it difficult to track runs and manage different parameter combinations.","poster":"fitri001"}],"comment_id":"1199867","timestamp":"1729554720.0","upvote_count":"3"},{"content":"Selected Answer: D\nVertex AI Experiment was created to compare runs.","poster":"pinimichele01","timestamp":"1728302820.0","upvote_count":"1","comment_id":"1190925"},{"poster":"36bdc1e","comment_id":"1121830","timestamp":"1720879560.0","upvote_count":"2","content":"D\nThe best option for investigating the tradeoffs between different parameter combinations is to create an experiment in Vertex AI Experiments,"},{"poster":"BlehMaks","content":"Selected Answer: D\nVertex AI Experiment was created to compare runs.\nA is incorrect because you can't create a boosted tree using BigQueryML \nhttps://cloud.google.com/bigquery/docs/bqml-introduction#supported_models","timestamp":"1720798080.0","upvote_count":"1","comment_id":"1120960"},{"comment_id":"1118372","timestamp":"1720592760.0","content":"Selected Answer: D\nGiven the objective of investigating parameter tradeoffs while ensuring reproducibility and tracking, option D - \"Create an experiment in Vertex AI Experiments and submit multiple runs to the same experiment, using different values for the parameters\" seems to be the most suitable. This approach provides a structured and trackable environment within Vertex AI Experiments, allowing multiple runs with varied parameters to be monitored for F1 score, training times, and potentially model complexity, enabling a comprehensive analysis of parameter combinations' tradeoffs.","upvote_count":"1","poster":"pikachu007"},{"comment_id":"1116889","poster":"vale_76_na_xxx","upvote_count":"1","timestamp":"1720456680.0","content":"I go with D : https://cloud.google.com/vertex-ai/docs/evaluation/introduction#tabular"},{"upvote_count":"1","timestamp":"1720447080.0","poster":"b1a8fae","content":"Selected Answer: D\nYou want to investigate tradeoffs between different parameter combinations and track all runs on the same platform -> clearly D. Vertex AI experiments etcetera.","comment_id":"1116742"}],"question_text":"You created an ML pipeline with multiple input parameters. You want to investigate the tradeoffs between different parameter combinations. The parameter options are\n• Input dataset\n• Max tree depth of the boosted tree regressor\n• Optimizer learning rate\n\nYou need to compare the pipeline performance of the different parameter combinations measured in F1 score, time to train, and model complexity. You want your approach to be reproducible, and track all pipeline runs on the same platform. What should you do?"},{"id":"17ahEILh1F368G5wCDhU","topic":"1","unix_timestamp":1704730620,"isMC":true,"discussion":[{"poster":"8619d79","upvote_count":"1","timestamp":"1739019660.0","comment_id":"1353404","content":"Selected Answer: D\nThis approach acknowledges that the skew might be due to a mismatch between the training data and the current production data distribution. By waiting for sufficient new production traffic, you can collect a more representative dataset that reflects the current state of the production environment. Retraining the model on this new data ensures that the model is better aligned with the production data distribution, which should resolve the skew."},{"comment_id":"1323317","upvote_count":"1","poster":"lunalongo","content":"Selected Answer: C\nC) - The model is adapting to the changing data distribution in production\n- Disabling alerts temporarily gives model a chance to adjust to new data \n*A) would hide/mask the skew; B) doesn't make sense because the monitoring job already uses the most recently trained data, it's just different from production data; D) is reactive and short term solution","timestamp":"1733621940.0"},{"poster":"AzureDP900","upvote_count":"1","content":"C. Temporarily disable the alert. Enable the alert again after a sufficient amount of new production traffic has passed through the Vertex AI endpoint.\n\n\nHere's why:\n\n\n\nYou've already retrained the model with more recent training data and deployed it back to the Vertex AI endpoint, but the alert persists.\nThis suggests that the model is still adapting to the changing data distribution in production.\nTemporarily disabling the alert will give the model a chance to adjust to the new data distribution before the monitoring job starts firing alerts again.\nOnce enough new traffic has passed through, you can re-enable the alert and continue monitoring the model's performance.","comment_id":"1234916","timestamp":"1719001440.0"},{"upvote_count":"2","poster":"info_appsatori","content":"Selected Answer: B\nThe baseline is calculated when you create a Vertex AI Model Monitoring job, and is only recalculated if you update the training dataset for the job.","timestamp":"1718631240.0","comment_id":"1231951"},{"timestamp":"1715048340.0","content":"Is B actually the correct answer? According to the documentation, training-serving skew detection can only be enabled if the original training data is available. Furthermore, the baseline is automatically recalculated when the training data is updated. \n\nSo does this question imply that the model is trained on data without updating the original training-dataset? If so then B is clearly correct. If they updated the training dataset with new data and then retrained the model then the model monitoring job's baseline should automatically have been recalculated. I see no other valid answers in that case?","upvote_count":"2","comment_id":"1207682","poster":"SahandJ"},{"timestamp":"1713005100.0","content":"Selected Answer: B\nThis option can help align the baseline distribution of the model monitoring job with the current distribution of the production data, and eliminate the false positive alerts.","poster":"pinimichele01","upvote_count":"1","comment_id":"1194889"},{"upvote_count":"3","content":"B\nThis option can help align the baseline distribution of the model monitoring job with the current distribution of the production data, and eliminate the false positive alerts.","timestamp":"1705162260.0","comment_id":"1121835","poster":"36bdc1e"},{"poster":"BlehMaks","upvote_count":"3","timestamp":"1705081680.0","content":"Selected Answer: B\nthe cause of the issue could be that the developer forgot to switch their monitoring job to the latest training dataset and the monitoring job still compares prod data with old training dataset and they of course have a skew","comment_id":"1120975"},{"timestamp":"1704967680.0","content":"Selected Answer: B\nB. Update the model monitoring job to use the more recent training data that was used to retrain the model:\n\nThis option directly aligns the model monitoring with the recently retrained model and ensures that the monitoring job reflects the characteristics of the latest training data.","poster":"pikachu007","upvote_count":"1","comment_id":"1119586"},{"poster":"b1a8fae","comment_id":"1116751","content":"Selected Answer: D\nA. Changing the sampling rate affects not training skew but cost efficiency: https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#considerations\nB. The model monitoring job is already using the most recent data to detect skew.\nC&D are the same, except for D being more specific, so I would tend towards D.","timestamp":"1704730620.0","upvote_count":"1"}],"question_text":"You received a training-serving skew alert from a Vertex AI Model Monitoring job running in production. You retrained the model with more recent training data, and deployed it back to the Vertex AI endpoint, but you are still receiving the same alert. What should you do?","answer":"B","exam_id":13,"question_images":[],"timestamp":"2024-01-08 17:17:00","answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/130592-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"A":"Update the model monitoring job to use a lower sampling rate.","C":"Temporarily disable the alert. Enable the alert again after a sufficient amount of new production traffic has passed through the Vertex AI endpoint.","D":"Temporarily disable the alert until the model can be retrained again on newer training data. Retrain the model again after a sufficient amount of new production traffic has passed through the Vertex AI endpoint.","B":"Update the model monitoring job to use the more recent training data that was used to retrain the model."},"answer_images":[],"answer_description":"","question_id":83,"answers_community":["B (70%)","D (20%)","10%"]},{"id":"lY38ddzCdvDhVlXNMzBX","answer":"D","answer_description":"","question_id":84,"timestamp":"2024-01-08 18:23:00","choices":{"C":"Use the features and the feature attributions for monitoring. Set a monitoring-frequency value that is lower than the default.","B":"Use the features for monitoring. Set a prediction-sampling-rate value that is closer to 1 than 0.","A":"Use the features for monitoring. Set a monitoring-frequency value that is higher than the default.","D":"Use the features and the feature attributions for monitoring. Set a prediction-sampling-rate value that is closer to 0 than 1."},"url":"https://www.examtopics.com/discussions/google/view/130611-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"D","exam_id":13,"question_text":"You developed a custom model by using Vertex AI to forecast the sales of your company’s products based on historical transactional data. You anticipate changes in the feature distributions and the correlations between the features in the near future. You also expect to receive a large volume of prediction requests. You plan to use Vertex AI Model Monitoring for drift detection and you want to minimize the cost. What should you do?","answers_community":["D (100%)"],"unix_timestamp":1704734580,"question_images":[],"isMC":true,"discussion":[{"timestamp":"1729555740.0","upvote_count":"3","content":"Selected Answer: D\nFeature and Feature Attribution Monitoring: Since you anticipate changes in feature distributions and correlations, monitoring both features and their attributions provides a more comprehensive view of potential drift. Feature attributions explain how each feature contributes to the model's predictions. Monitoring them helps identify if these contributions are changing as expected.\nLower Prediction Sampling Rate: This reduces the cost associated with Vertex AI Model Monitoring. The sampling rate determines the percentage of prediction requests used for monitoring calculations. A lower rate reduces the number of predictions analyzed, lowering monitoring costs. However, it's important to strike a balance between cost and having enough data for drift detection.","comment_id":"1199874","poster":"fitri001"},{"upvote_count":"3","poster":"BlehMaks","comment_id":"1121116","content":"Selected Answer: D\nif we expect a large volume of prediction requests then pick D. if we expect the changes to be infrequent then C\nhttps://cloud.google.com/vertex-ai/docs/model-monitoring/overview#considerations","timestamp":"1720810860.0"},{"comment_id":"1119588","poster":"pikachu007","content":"Selected Answer: D\nGiven the need to minimize costs while addressing changes in feature distributions and correlations, option D - \"Use the features and the feature attributions for monitoring. Set a prediction-sampling-rate value that is closer to 0 than 1\" seems to be a reasonable choice. This option allows monitoring both features and feature attributions, offering insights into changes in feature importance, while the lower prediction-sampling-rate helps manage costs by monitoring a subset of predictions. It's a trade-off between cost efficiency and the need for effective drift detection","upvote_count":"2","timestamp":"1720685400.0"},{"timestamp":"1720452180.0","content":"Selected Answer: D\nNot A. because higher monitoring frequency, higher cost.\nNot B. because higher prediction request sample rate, higher cost.\nBetween the remaining 2, better to lower the prediction request sample rate so only a small fraction of the latest data is evaluated for drift, also because lots of data are expected so a small perecentage should suffice to detect drift.","comment_id":"1116836","upvote_count":"2","poster":"b1a8fae"}],"topic":"1","answer_images":[]},{"id":"iNzxWNPiCSGzzN7IxyEF","answer_description":"","discussion":[{"upvote_count":"5","poster":"shadz10","comment_id":"1122842","timestamp":"1720983840.0","content":"Selected Answer: B\nB - Creating a custom container without CPR adds additional complexity. i.e. write model server write dockerfile and also build and upload image. Where as using a CPR only requires writing a predictor and using vertex SDK to build image.\nhttps://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines"},{"poster":"desertlotus1211","timestamp":"1741360380.0","upvote_count":"1","comment_id":"1366307","content":"Selected Answer: A\nyou want to minimize code... all other you need code..."},{"poster":"bobjr","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines","comment_id":"1225724","timestamp":"1733517360.0"},{"timestamp":"1729506960.0","comment_id":"1199603","content":"Selected Answer: B\nagree with shadz10","poster":"gscharly","upvote_count":"1"},{"content":"Selected Answer: C\nMy choose: C\n\nOption C ensures that the scikit-learn model is properly packaged, deployed, and integrated with Vertex AI services while minimizing the need for additional code beyond what is necessary for customizing the serving function.\n\nOption B is not considered correct because it suggests wrapping the scikit-learn model in a custom prediction routine (CPR), which might not be the most suitable approach for deploying scikit-learn models on Vertex AI.\n\nOptions A and D using InstanceConfig, that is limited for preprocessing. Uploading the container without a serving function won't work.","poster":"guilhermebutzke","comment_id":"1142849","upvote_count":"1","timestamp":"1722981660.0"},{"comment_id":"1119596","content":"Selected Answer: D\nConsidering the goal of minimizing additional code and complexity, option D - \"Create a custom container for your scikit-learn model, upload your model and custom container to Vertex AI Model Registry, deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job that uses the instanceConfig.instanceType setting to transform your input data\" seems to be a more straightforward and efficient approach. It involves customizing the container for the scikit-learn model, leveraging the Vertex AI Model Registry, and utilizing the specified instance type for batch prediction without introducing unnecessary complexity like custom prediction routines.","upvote_count":"1","poster":"pikachu007","timestamp":"1720685700.0"},{"upvote_count":"4","content":"Selected Answer: B\nI go with B:\n\n“Custom prediction routines (CPR) lets you build custom containers with pre/post processing code easily, without dealing with the details of setting up an HTTP server or building a container from scratch.” (https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines). This alone makes B preferable to C and D, provided lack of complex model architecture.\n\nRegarding A, pre-built containers only allow serving predictions, but not preprocessing of data (https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#use_a_prebuilt_container). B thus remains the most likely option.","comment_id":"1118747","timestamp":"1720617840.0","poster":"b1a8fae"}],"choices":{"D":"1. Create a custom container for your scikit learn model.\n2. Upload your model and custom container to Vertex AI Model Registry.\n3. Deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job that uses the instanceConfig.instanceType setting to transform your input data.","B":"1. Wrap your model in a custom prediction routine (CPR). and build a container image from the CPR local model.\n2. Upload your scikit learn model container to Vertex AI Model Registry.\n3. Deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job","C":"1. Create a custom container for your scikit learn model.\n2. Define a custom serving function for your model.\n3. Upload your model and custom container to Vertex AI Model Registry.\n4. Deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job.","A":"1. Upload your model to the Vertex AI Model Registry by using a prebuilt scikit-ieam prediction container.\n2. Deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job that uses the instanceConfig.instanceType setting to transform your input data."},"topic":"1","isMC":true,"answer_ET":"B","timestamp":"2024-01-10 16:24:00","answer":"B","unix_timestamp":1704900240,"answers_community":["B (79%)","7%","7%"],"question_id":85,"exam_id":13,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/130797-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"question_text":"You have recently trained a scikit-learn model that you plan to deploy on Vertex AI. This model will support both online and batch prediction. You need to preprocess input data for model inference. You want to package the model for deployment while minimizing additional code. What should you do?"}],"exam":{"numberOfQuestions":304,"isBeta":false,"provider":"Google","id":13,"isMCOnly":true,"lastUpdated":"11 Apr 2025","isImplemented":true,"name":"Professional Machine Learning Engineer"},"currentPage":17},"__N_SSP":true}