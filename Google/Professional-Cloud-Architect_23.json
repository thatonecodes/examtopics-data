{"pageProps":{"questions":[{"id":"8IaBATWYoKakJqBrb5XT","answer":"C","unix_timestamp":1724778540,"answer_images":[],"question_text":"You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed the Cloud Monitoring agent and configured the autoscaling policy as follows:\n\n//IMG//\n\n\nYou observe that the application does not scale under high load. You want to resolve this. What should you do?","topic":"1","question_id":111,"url":"https://www.examtopics.com/discussions/google/view/146572-exam-professional-cloud-architect-topic-1-question-199/","answers_community":["C (100%)"],"choices":{"A":"Change the Target type to DELTA_PER_MINUTE.","C":"Change the filter to metric.label.state = ‘used’.","D":"Change the filter to metric.label.state = ‘free’ and the Target utilization to 20.","B":"Change the Metric identifier to agent.googleapis.com/memory/bytes_used."},"answer_description":"","answer_ET":"C","timestamp":"2024-08-27 19:09:00","exam_id":4,"isMC":true,"discussion":[{"poster":"Nick89GR","content":"I am confused since this question comes in contrast with the answer in question 190. Anyone knows what is the real answer? I would expected used+buffered+cached+slab Which means the sum of all these.","upvote_count":"10","timestamp":"1730115000.0","comments":[{"comment_id":"1411182","upvote_count":"1","content":"yes..it is confusing. I used Claude.ai to clarify my doubt. \n- By monitoring metric.label.state = 'free' memory, you're tracking available memory instead of used memory\n- Setting the target to 20% means autoscaling will trigger when free memory drops below 20% (equivalent to total usage exceeding 80%)\n- This approach accurately captures all memory usage regardless of how it's being utilized by the system","timestamp":"1743139560.0","poster":"Piddi"}],"comment_id":"1303936"},{"comment_id":"1273595","content":"C. Change the filter to metric.label.state = ‘used’. The current filter is set up with multiple AND conditions, which means it's looking for a metric that simultaneously has all these states: 'used', 'buffered', 'cached', and 'slab'. This is logically impossible, as a memory location can't be in multiple states at once. Therefore, the filter will never match any metrics, and the autoscaling policy won't trigger.","poster":"JamesKarianis","timestamp":"1724778540.0","upvote_count":"9"},{"content":"Selected Answer: C\nExplanation:\n • Issue with the Current Filter:\nThe filter you configured uses multiple conditions with the AND operator for states (‘used’, ‘buffered’, ‘cached’, ‘slab’). However, each time series reported by the Cloud Monitoring agent includes a single state label value. This means no single data point can satisfy all these conditions simultaneously.\n • Correcting the Filter:\nTo monitor total memory usage accurately, you should only focus on the “used” memory state. Changing the filter to just metric.label.state = 'used' ensures that the autoscaler is correctly evaluating the actual used memory percentage. Once the used memory exceeds the target (80%), the autoscaler will trigger scaling.\n\nThus, modifying the filter as in option C resolves the issue.","upvote_count":"2","timestamp":"1742453640.0","comment_id":"1400919","poster":"frank_tsai_tech"},{"comment_id":"1333268","upvote_count":"3","timestamp":"1735438320.0","content":"Selected Answer: C\nAutoscale based on memory usage\nTo configure autoscaling based on the percent of used memory, specify the percent_used metric provided by the memory Ops Agent metrics. You should filter the metric by state to use only the used memory state. If you do not specify the filter, then the autoscaler takes the sum of memory usage by all memory states labeled as buffered, cached, free, slab, and used.","poster":"JaquiMB"},{"content":"Selected Answer: C\nas it is clearly indicated in the public documentation https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#gcloud_5\nyou have change the filter to metric.label.state=\"used\"","upvote_count":"9","poster":"tangac","timestamp":"1724840580.0","comment_id":"1273972"}],"question_images":["https://img.examtopics.com/professional-cloud-architect/image1.png"]},{"id":"e83yxWBRsWKIuMoNEfVF","question_id":112,"choices":{"D":"Stream data into Google Cloud Datastore","A":"Load data into Google BigQuery","B":"Insert data into Google Cloud SQL","C":"Put flat files into Google Cloud Storage"},"answer":"A","unix_timestamp":1571849040,"exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/7080-exam-professional-cloud-architect-topic-1-question-2/","answer_description":"","question_text":"Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24hrs a day. Your business analysts have experience only with using a SQL interface.\nHow should you store the data to optimize it for ease of analysis?","answer_ET":"A","isMC":true,"answers_community":["A (97%)","3%"],"answer_images":[],"timestamp":"2019-10-23 18:44:00","question_images":[],"discussion":[{"upvote_count":"36","comment_id":"16994","poster":"Eroc","content":"This question could go either way for A or B. But Big Query was designed with this in mind, according to numerous Google presentation and videos. Cloud Datastore is a NoSQL database (https://cloud.google.com/datastore/docs/concepts/overview)\nCloud Storage does not have an SQL interface. The previous two sentences eliminate options C and D. So I'd pick \"A\".","comments":[{"poster":"tartar","timestamp":"1596617820.0","comment_id":"151025","content":"A is ok","upvote_count":"16"},{"poster":"0xE8D4A51000","comments":[{"comment_id":"698800","comments":[{"content":"Also cloudsql cannot handle multi peta byte data whereas biq query can. please correct me if i'm wrong.","upvote_count":"1","timestamp":"1735227900.0","poster":"b9be167","comment_id":"1331979"}],"content":"Also the question does not say whether the data is relational or not. So we cannot assume it is only relational. Therefore, for maximum flexibility BQ is the correct option also. Note that Cloud SQL storage capacity is now at 64TB","upvote_count":"6","timestamp":"1666166700.0","poster":"0xE8D4A51000"}],"timestamp":"1666166580.0","upvote_count":"1","comment_id":"698798","content":"IMHO, it should be A only. The reason is that they want to perform analysis on the data and BigQuery excels in that over Cloud SQL. You can run SQL queries in both but I BigQuery has better analytical tools. It can do ad-hoc analysis like Cloud SQL using Cloud Standard SQL and it can do geo-spatial and ML analysis via its Cloud Standard SQL interface."},{"timestamp":"1665970320.0","content":"Cloud SQL does not scale to that magnitude also Cloud SQL is not meant for OLAP\nAnswer is BigQuery","comment_id":"696730","poster":"zr79","upvote_count":"4"},{"timestamp":"1642319340.0","comment_id":"524720","content":"B is not correct because Cloud SQL storage limit doesn't fit the requirement.","poster":"kinghin","upvote_count":"13"}],"timestamp":"1726851960.0"},{"timestamp":"1589057940.0","content":"I'll go with A because BQ (and BT) are usually meant for analytics.\nB isn't correct because Cloud SQL does not scale to that volume.\nC isn't correct because Cloud Storage does not provide a standard SQL mechanism.\nD could be right but it sounds off because of the analytics requirement.","comment_id":"86231","poster":"clouddude","upvote_count":"14"},{"upvote_count":"1","timestamp":"1741600020.0","comment_id":"1379934","content":"Selected Answer: A\nCloud SQL supports TB not PB of data","poster":"Sib09"},{"poster":"RTATAVAR","upvote_count":"1","content":"Selected Answer: A\noptimize the ease of Analysis ... Any data that can be analyzed is using Big Query \nSo Option A will suit the requirement","comment_id":"1364769","timestamp":"1741069500.0"},{"poster":"lks68","comment_id":"1352641","content":"Selected Answer: A\nCloudSQL cannot handle petabytes but tens of terabytes only.","upvote_count":"1","timestamp":"1738872480.0"},{"poster":"szagarella","timestamp":"1738845420.0","content":"Selected Answer: A\nCloud SQL is limited to 30TB of data. A is correct.","upvote_count":"1","comment_id":"1352374"},{"content":"Selected Answer: A\nA or B but as it said it is for an analyst, A is better","timestamp":"1738064160.0","upvote_count":"1","comment_id":"1347866","poster":"hpf97"},{"timestamp":"1735498020.0","poster":"JonathanSJ","comment_id":"1333671","content":"Selected Answer: A\nA is the answer.","upvote_count":"1"},{"comment_id":"1332343","content":"Selected Answer: A\nI'll go A as BQ has compatible usage of SQL","upvote_count":"1","timestamp":"1735293960.0","poster":"XiaobinJiang"},{"poster":"devenderpraksh","comment_id":"1315020","upvote_count":"1","timestamp":"1732072560.0","content":"Answer: A. Load data into Google BigQuery\nThis is the correct choice because it's the only option that meets all requirements:\n\nCan handle multi-petabyte scale\nProvides the required SQL interface for analysts\nEnsures 24/7 availability\nOptimized for analytical queries\nServerless and automatically scales"},{"upvote_count":"1","content":"Selected Answer: A\nThe best answer here is A. Load data into Google BigQuery.\n\nHere's why:\n\n1. Designed for large datasets: BigQuery is a serverless, highly scalable, and cost-effective multicloud data warehouse designed specifically for analyzing massive datasets. Petabyte-scale data is exactly what it excels at.\n\n2. SQL interface: Your analysts are already familiar with SQL, and BigQuery uses standard SQL, making the transition easy and minimizing the learning curve.\n\n3. High availability: BigQuery offers high availability with built-in redundancy and replication.\n\n4. Performance: BigQuery is optimized for analytical queries and can handle complex queries across massive datasets very efficiently.","poster":"Ekramy_Elnaggar","comment_id":"1309557","timestamp":"1731260640.0"},{"comment_id":"1288877","upvote_count":"1","timestamp":"1727237940.0","poster":"dev_evening","content":"Selected Answer: A\nA, BigQuery is more suitable for analysis. While Cloud SQL can work, it's more generic"},{"comments":[{"content":"Hence Option A","upvote_count":"2","timestamp":"1630989600.0","comment_id":"440698","poster":"amxexam"}],"timestamp":"1726907400.0","comment_id":"430443","poster":"amxexam","upvote_count":"2","content":"Let's go with option elimination\nA. Load data into Google BigQuery\n>>Big Query = Analytic + SQL (Ease of using SQL) Storage hence the solution\nB. Insert data into Google Cloud SQL\n>> Yes you can SQL query with your own applicaiton console compared to BigQuery SQL console, and 24 hrs avalablity but you won't have 1-2 sec response on petabytes of data, as you can do in GCP BigQuery partitioned and clustered tables.\nC. Put flat files into Google Cloud Storage\n>>The requirement is for analytics and SQL querying of data. You can store it in the flat file but will need to use GCP BigQuery to do that\nD. Stream data into Google Cloud Datastore\n>> Only dealing with storage problems does not address analytics and SQL querying"},{"poster":"i_am_robot","upvote_count":"2","comment_id":"747751","content":"A. Load data into Google BigQuery\n\nBigQuery is a fully managed, cloud-native data warehousing solution that makes it easy to analyze large and complex datasets. It is optimized for analyzing large amounts of data quickly, and can handle petabyte-scale datasets with ease. It also has a SQL-like interface that is familiar to business analysts, making it easy for them to query and analyze the data. Additionally, BigQuery is highly scalable and can handle high query concurrency, making it a good choice for storing data that must be available 24/7.\n\nOption B, inserting data into Google Cloud SQL, is not a good choice for a multi-petabyte dataset because Cloud SQL is not designed to handle such large volumes of data. Option C, putting flat files into Cloud Storage, is also not a good choice because it is not optimized for querying and analyzing data. Option D, streaming data into Cloud Datastore, is not a good choice because Cloud Datastore is a NoSQL database and does not have a SQL-like interface.","timestamp":"1726907400.0"},{"timestamp":"1726907400.0","poster":"omermahgoub","upvote_count":"2","content":"A. Load data into Google BigQuery\n\nTo optimize the storage of the multi-petabyte data set for ease of analysis by business analysts who have experience only with using a SQL interface, you should load the data into Google BigQuery. BigQuery is a fully-managed, cloud-native data warehouse that allows you to perform fast SQL queries on large amounts of data. By loading the data into BigQuery, you can provide your business analysts with a familiar SQL interface for querying the data, making it easier for them to analyze the data set.\n\nOther options, such as inserting data into Google Cloud SQL, putting flat files into Google Cloud Storage, or streaming data into Google Cloud Datastore, may not provide the necessary SQL interface or query performance for efficient analysis of the data set.","comment_id":"751966"},{"comment_id":"1199823","content":"A is correct","upvote_count":"1","poster":"juanlopezcervero","timestamp":"1713731640.0"},{"content":"A is ok","upvote_count":"1","comment_id":"1160304","poster":"sanjeevisubhash","timestamp":"1709018400.0"},{"poster":"lisabisa","comment_id":"1150714","timestamp":"1707965040.0","upvote_count":"1","content":"A BigQuery formula is similar to SQL.\nB Google Cloud SQL cannot handle multiple petabyte data.\nD Google Cloud Datastore is NoSQL."},{"poster":"hzaoui","timestamp":"1704802980.0","content":"Selected Answer: A\nBigQuery","comment_id":"1117462","upvote_count":"2"},{"upvote_count":"2","poster":"yas_cloud","comment_id":"1105812","timestamp":"1703578440.0","content":"B doesn’t fit the bill as cloud SQL is good for data up to 30 TB. I would go with option A."},{"comment_id":"1091285","timestamp":"1702062780.0","content":"I got with A\nBigQuery is a serverless, highly scalable data warehouse designed for analytics:\n\nHigh-performance querying: BigQuery allows large datasets to be queried quickly and efficiently, making it ideal for business analysts who need to analyze data frequently.\nSQL compatibility: BigQuery uses a standard SQL interface, allowing business analysts to leverage their existing SQL skills without needing to learn new tools or languages.\n24/7 availability: BigQuery offers 99.95% availability, ensuring that your data is accessible to your business analysts whenever they need it.","poster":"sam422","upvote_count":"1"},{"upvote_count":"2","comment_id":"1010308","poster":"ChinaSailor","timestamp":"1695016320.0","content":"Selected Answer: A\nBQ the correct tool"},{"timestamp":"1692086640.0","content":"Selected Answer: A\nMulti petabyte and SQL interface => BigQuery","upvote_count":"2","comment_id":"981407","poster":"RaviRS"},{"timestamp":"1677124140.0","content":"Selected Answer: A\nA is the correct one","upvote_count":"1","comment_id":"818763","poster":"Kicod"},{"content":"Selected Answer: A\nA - The question states that the data set must be available 24hrs a day and that your business analysts have experience only with using a SQL interface.\nLoading data into Google BigQuery will allow your business analysts to access the data using a SQL interface. It will also allow the data to be available 24hrs a day.","comment_id":"789697","poster":"simonab23","upvote_count":"2","timestamp":"1674830580.0"},{"timestamp":"1674830460.0","comment_id":"789692","upvote_count":"2","content":"A - The question states that the data set must be available 24hrs a day and that your business analysts have experience only with using a SQL interface.\nLoading data into Google BigQuery will allow your business analysts to access the data using a SQL interface. It will also allow the data to be available 24hrs a day.","poster":"simonab23"},{"poster":"cooljayforever","content":"Selected Answer: A\nBigquery for data analytics","comment_id":"774169","upvote_count":"1","timestamp":"1673593140.0"},{"timestamp":"1672474440.0","poster":"AShrujit","content":"A for me","upvote_count":"1","comment_id":"762548"},{"poster":"Jaldhi24","content":"Selected Answer: A\nA is the correct one.","timestamp":"1671898680.0","comment_id":"754999","upvote_count":"1"},{"comment_id":"751393","timestamp":"1671564600.0","content":"A is the answer, with keywords, migrate multi-petabyte dataset, data should be available 24 hours, business analyst with SQL experience, optimize for analysis. BigQuery is the right answer.","poster":"examch","upvote_count":"1"},{"comment_id":"741389","content":"A is the correct one.","upvote_count":"1","poster":"GCP_Student1","timestamp":"1670729400.0"},{"comment_id":"721826","content":"Selected Answer: A\nBigQuery is the multi petabyte data ware house solution provided with SQL support. Hence A is correct","upvote_count":"1","timestamp":"1668841800.0","poster":"AniketD"},{"poster":"andreavale","timestamp":"1667553840.0","upvote_count":"1","content":"Selected Answer: A\nA it's ok","comment_id":"711020"},{"poster":"ckorbet","comment_id":"708275","upvote_count":"1","timestamp":"1667203620.0","content":"Selected Answer: A\nA is the correct one"},{"content":"Selected Answer: A\nIMHO, it should be A only. The reason is that they want to perform analysis on the data and BigQuery excels in that over Cloud SQL. You can run SQL queries in both but I BigQuery has better analytical tools. It can do ad-hoc analysis like Cloud SQL using Cloud Standard SQL and it can do geo-spatial and ML analysis via its Cloud Standard SQL interface.\nAlso the question does not say whether the data is relational or not. So we cannot assume it is only relational. Therefore, for maximum flexibility BQ is the correct option also. Note that Cloud SQL storage capacity is now at 64TB so it can't handle multi-PB of data. The answer is A only.","poster":"0xE8D4A51000","comment_id":"698802","timestamp":"1666166820.0","upvote_count":"1"},{"content":"Bigquery is right option A is right","poster":"AzureDP900","upvote_count":"1","timestamp":"1665948900.0","comment_id":"696475"},{"content":"Selected Answer: A\nA. Load data into Google BigQuery","comment_id":"693063","poster":"minmin2020","upvote_count":"1","timestamp":"1665577020.0"},{"poster":"Swet169","content":"A. \nD. isn't correct as this not a streaming data so no need for Datastore","comment_id":"680461","timestamp":"1664260740.0","upvote_count":"1"},{"poster":"gee1979","comment_id":"660168","content":"Selected Answer: A\nA...multi-petabyte...analysts have experience only with using a SQL interface.\n\nA. Load data into Google BigQuery","upvote_count":"1","timestamp":"1662380460.0"},{"upvote_count":"1","comment_id":"658990","timestamp":"1662270720.0","poster":"Amargcp","content":"A is the Answer. BQ supports petabytes of data and standard SQL can be used for extracting data."},{"content":"Selected Answer: A\nBigTable has SQL Like queries, even though is non-sql. Also supports petabytes of data and specially for analytics.","poster":"alexandercamachop","timestamp":"1662172140.0","upvote_count":"1","comment_id":"657987"},{"comments":[{"timestamp":"1665948960.0","poster":"AzureDP900","content":"Big Query is option A, You might mistakenly selected as B","comment_id":"696476","upvote_count":"1"}],"upvote_count":"1","poster":"abirroy","timestamp":"1660161540.0","comment_id":"645140","content":"Selected Answer: B\nBig Query"},{"upvote_count":"1","timestamp":"1658535600.0","comment_id":"635386","poster":"backhand","content":"vote A\nkey word: multi-petabyte data set, analysts, SQL"},{"comment_id":"623130","content":"Selected Answer: A\nmulti Po + SQL => BigQuery","poster":"nicoueron","upvote_count":"1","timestamp":"1656315720.0"},{"upvote_count":"1","timestamp":"1653010200.0","comment_id":"604208","poster":"elaineshi","content":"Datastore supports SQL query syntax as well, yet Datastore is not preferred for data analytics use case."},{"upvote_count":"2","content":"A and B are SQL databases. \nB could have been. But the max size of the database is too small. \nCloud SQL storage is limited to a maximum of 30,720GB. (0.03072PetaBytes) \nhttps://cloud.netapp.com/blog/gcp-cvo-blg-google-cloud-sql-pricing-and-limits-a-cheat-sheet\nA is the only valid option.","comment_id":"588164","timestamp":"1650372540.0","poster":"Nirca"},{"content":"Selected Answer: A\nBigQuery - SQL and Petabytes of data for analysis.","upvote_count":"2","comment_id":"537847","poster":"anjuagrawal","timestamp":"1643719560.0"},{"comment_id":"517035","content":"Selected Answer: A\nBecause they want to query the data set","timestamp":"1641336840.0","poster":"Moss2011","upvote_count":"2"},{"upvote_count":"1","content":"A- is correct\nB- is not because of the size\nCloud SQL for MySQL has a limit of 10,000 tables for an instance. Too many tables can significantly impact the performance of a Cloud SQL instance. Instances that exceed this limit are not covered by the SLA. When a table size reaches 16 TB, the maximum size for Linux partitions, more data files cannot be added to it.\nMySQL instances \nUp to 64 TB, depending on the machine type. \nPostgreSQL and SQL Server instances\nUp to 64 TB, depending on whether the instance has dedicated or shared vCPUs.","timestamp":"1640568720.0","poster":"Atnafu","comment_id":"509941"},{"poster":"haroldbenites","upvote_count":"1","comment_id":"493205","content":"Go for A","timestamp":"1638546060.0"},{"poster":"nqthien041292","content":"Selected Answer: A\nVote A","comment_id":"482590","timestamp":"1637417340.0","upvote_count":"1"},{"content":"B seems to be closer, however Cloud SQL can't handle multi peta bytes of dataset, hence option A is the correct answer. Also requirement of Analytics makes option A as a perfect choice.","upvote_count":"1","comment_id":"473349","timestamp":"1636190100.0","poster":"vincy2202"},{"timestamp":"1635982620.0","upvote_count":"2","content":"A is correct","comment_id":"472327","poster":"exam_war"},{"upvote_count":"2","content":"A.is right for sure, Same time Option.C also make sense. Bigquery can query data from Cloud Store with out having store data into Big Query. BQ can query flat files ( in CSV format) stored in cloud storage. Infact this is most economical way.","comment_id":"470190","timestamp":"1635589680.0","poster":"FERIN_02"},{"poster":"fwfw","timestamp":"1634382300.0","content":"AAA analytics+SQL+PBData","upvote_count":"2","comment_id":"463020"},{"content":"Option A is perfect for the combo Analysis+sql","comment_id":"457425","poster":"mum_lalitha0508","upvote_count":"2","timestamp":"1633398300.0"},{"comment_id":"432395","upvote_count":"2","timestamp":"1629987060.0","poster":"babuu2021","content":"A is good."},{"poster":"bala786","upvote_count":"2","comment_id":"400435","content":"Option A is correct, for Analysis we can select BigQuery","timestamp":"1625628780.0"},{"timestamp":"1624681560.0","poster":"aviratna","content":"A is correct as it support SQL query which can be used by Business Analysts and it will support Peta Byte of data for analysis","comment_id":"390918","upvote_count":"2"},{"poster":"victory108","content":"A. Load data into Google BigQuery","upvote_count":"2","timestamp":"1621316640.0","comment_id":"360097"},{"timestamp":"1621298580.0","poster":"Amber25","content":"Answer - A. Bigquery.\n\nBecause Database needs multi patabyte and SQL supports.","upvote_count":"2","comment_id":"359921"},{"upvote_count":"2","poster":"un","comment_id":"351218","timestamp":"1620319800.0","content":"A is correct"},{"timestamp":"1617598740.0","comment_id":"328377","content":"ans is A","upvote_count":"2","poster":"jacob_f"},{"upvote_count":"2","timestamp":"1617454200.0","content":"A is correct","comment_id":"327359","poster":"AjayChakiat"},{"upvote_count":"1","content":"A is ok","poster":"lynx256","timestamp":"1617083640.0","comment_id":"324012"},{"upvote_count":"1","content":"Answer is A","comment_id":"323954","timestamp":"1617078360.0","poster":"Ausias18"},{"comment_id":"297137","content":"A is ok, there are three reasons. 1) using dataset 2) we can set expiration time for dataset in cloud bigquery 3) bigquery support query syntax in standard SQL","poster":"dinhphan695","upvote_count":"3","timestamp":"1614053340.0"},{"upvote_count":"2","content":"Undoubtedly \"A\" is the correct answer.\nConsidering volume of data, availability and SQL like accessibility, what best suites the purpose other than BigQuery.","comment_id":"289991","timestamp":"1613274660.0","poster":"Joyjit_Deb"},{"upvote_count":"1","timestamp":"1609121340.0","comment_id":"253772","content":"A is correct based on the first description of BigQuery :\nAnalyze petabytes of data using ANSI SQL at blazing-fast speeds, with zero operational overhead.\nMeets petabyte scale, 24 hour availability and SQL access.","poster":"NeoFer"},{"comment_id":"239130","timestamp":"1607513100.0","poster":"bigclouds","upvote_count":"1","content":"A is the correct answer."},{"timestamp":"1602886380.0","comment_id":"201241","content":"Answer is straight forward A","upvote_count":"1","poster":"zathrin"},{"comment_id":"198859","timestamp":"1602545100.0","content":"Cloud SQL cannot support multi-petabyte Option a correct","poster":"Aru23","upvote_count":"1"},{"poster":"AshokC","comment_id":"188039","content":"A is right","upvote_count":"1","timestamp":"1601167080.0"},{"poster":"gkdinesh","comment_id":"174546","content":"A is correct","upvote_count":"1","timestamp":"1599395520.0"},{"poster":"ESP_SAP","timestamp":"1599074520.0","comment_id":"172214","upvote_count":"1","content":"Correct Answer is (A):\n\nhttps://cloud.google.com/bigquery/docs\n\nBigQuery is Google Cloud's fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of data in near real time. With BigQuery, there's no infrastructure to set up or manage, letting you focus on finding meaningful insights using standard SQL and taking advantage of flexible pricing models across on-demand and flat-rate options."},{"comment_id":"159356","timestamp":"1597597800.0","poster":"kdharma","content":"A only","upvote_count":"1"},{"content":"Correct is A","poster":"AS007","upvote_count":"1","comment_id":"133386","timestamp":"1594602540.0"},{"upvote_count":"1","timestamp":"1594289220.0","content":"No doubt.Its Big query only.so Answer is A","poster":"RM07","comment_id":"130478"},{"timestamp":"1592845140.0","upvote_count":"1","content":"Yeah A is correct","comment_id":"116623","poster":"mlantonis"},{"content":"A, for sure.\n...for multi-petabyte analysis.","timestamp":"1591765380.0","poster":"gfhbox0083","comment_id":"106487","upvote_count":"1"},{"comment_id":"100782","timestamp":"1591100220.0","content":"A is the correct answer","poster":"Nirms","upvote_count":"1"},{"content":"A is the correct answer","timestamp":"1590760440.0","poster":"Ziegler","upvote_count":"1","comment_id":"98256"},{"poster":"Javed","timestamp":"1590269700.0","content":"Answer A","upvote_count":"2","comment_id":"94581"},{"content":"A is correct. B is wrong only because Cloud SQL supports up to 30TB DB size only.","comment_id":"91970","timestamp":"1589882040.0","upvote_count":"1","poster":"huangmeiguai"},{"poster":"laksg","timestamp":"1589668980.0","content":"Answer : A , key words kere { SQL , lot of data , Analytics) . If its for Backup or storage , Cloud storage will work.","upvote_count":"2","comment_id":"90185"},{"timestamp":"1588537080.0","comment_id":"83234","content":"A is the correct anwer","upvote_count":"3","poster":"gcp_aws"},{"upvote_count":"2","poster":"Fouad","comment_id":"59049","timestamp":"1583364480.0","content":"Answer:A"},{"content":"answer: A","upvote_count":"3","poster":"2g","comment_id":"44686","timestamp":"1580389140.0"},{"timestamp":"1578767520.0","comment_id":"37800","poster":"AWS56","content":"Yes, Only Google BigQuery would support this load.... I would agree with A","upvote_count":"4"},{"timestamp":"1575415740.0","poster":"Shariq","comment_id":"26404","content":"Cloud SQL cannot support multi-petabyte data","upvote_count":"5"}],"topic":"1"},{"id":"wEMLVMwT1XVtUXCQilUI","answer":"C","choices":{"B":"Google Cloud SQL","C":"Google Cloud Bigtable","D":"Google Cloud Storage","A":"Google BigQuery"},"question_images":[],"answer_description":"","question_id":113,"exam_id":4,"answer_ET":"C","answer_images":[],"discussion":[{"comment_id":"398368","comments":[{"poster":"khadar","upvote_count":"4","content":"I too got this question in 10-09-22 exam with similar option and result is pass","comment_id":"665216","timestamp":"1662795180.0"}],"content":"C. Google Cloud Bigtable","poster":"victory108","upvote_count":"12","timestamp":"1625405280.0"},{"content":"Selected Answer: C\nCloud Bigtable is right solution and correct database choice, which provides high throughput, low latency and scalability for time series data such as this case.","upvote_count":"1","poster":"FabPanda","comment_id":"1344734","timestamp":"1737548880.0"},{"comment_id":"1309636","poster":"Ekramy_Elnaggar","content":"Selected Answer: C\n1. High Write Throughput: Bigtable excels at handling high-volume write operations, which is crucial for your application receiving data from 50,000 sensors sending 10 readings per second.\n2. Low Latency: Bigtable offers very low latency for read operations, essential for real-time charting and data visualization.\n3. Time-Series Data: Bigtable is well-suited for storing and querying time-series data, like your weather sensor readings with timestamps.\n4. Scalability: Bigtable can handle massive amounts of data and scale seamlessly as your application grows.","upvote_count":"3","timestamp":"1731272340.0"},{"poster":"lisabisa","timestamp":"1708543380.0","comment_id":"1155810","content":"Bigtable - NoSQL, high-throughput, low-latency, making it suitable for storing time-series data from sensors","upvote_count":"2"},{"content":"Selected Answer: C\nBigTable is NoSQL for IoT","comment_id":"847558","upvote_count":"2","timestamp":"1679522520.0","poster":"alekonko"},{"upvote_count":"1","content":"C is correct","timestamp":"1677815880.0","comment_id":"827643","poster":"sivaamum"},{"content":"To optimize the performance of an accurate, real-time, weather-charting application that receives data from 50,000 sensors sending 10 readings per second, it would be most appropriate to store the data in a distributed, horizontally scalable, NoSQL database such as Google Cloud Bigtable\nOther options, such as Google BigQuery, Google Cloud SQL, and Google Cloud Storage, may not be as well-suited for handling high volumes of real-time data and may not provide the same level of performance and scalability as Google Cloud Bigtable.","comment_id":"750681","poster":"omermahgoub","timestamp":"1671526260.0","upvote_count":"3"},{"poster":"Bry_040706","comment_id":"727308","upvote_count":"1","timestamp":"1669443120.0","content":"C. Bigtable, IoT data."},{"poster":"AniketD","timestamp":"1668850260.0","comment_id":"721900","content":"Selected Answer: C\nC Bigtable","upvote_count":"1"},{"poster":"Mahmoud_E","upvote_count":"1","timestamp":"1666459560.0","content":"Selected Answer: C\nC bigtable right answer","comment_id":"701686"},{"comment_id":"696759","poster":"zr79","upvote_count":"4","timestamp":"1665975600.0","content":"real-time, IoT, time series and huge writes are some of the keywords to look after for Bigtable"},{"comment_id":"693697","poster":"minmin2020","upvote_count":"1","timestamp":"1665646080.0","content":"Selected Answer: C\nC. Google Cloud Bigtable"},{"comment_id":"674182","upvote_count":"1","poster":"holerina","timestamp":"1663678620.0","content":"C big table for IOT data"},{"comment_id":"651997","poster":"abirroy","content":"Selected Answer: C\nGoogle Cloud Bigtable","timestamp":"1661480220.0","upvote_count":"1"},{"poster":"Dhiraj03","upvote_count":"2","content":"Keyword - Timestamp - Big table","timestamp":"1655460660.0","comment_id":"617661"},{"content":"Go for c","poster":"szanio","comment_id":"612223","upvote_count":"1","timestamp":"1654498140.0"},{"poster":"Nirca","comment_id":"588493","upvote_count":"1","timestamp":"1650440820.0","content":"Selected Answer: C\nC. Google Cloud Bigtable is the Best Practice option"},{"content":"Ans c - when ever thier is input from IOT devices across and time series data which is huge go for big table in gcp","upvote_count":"2","comment_id":"551511","poster":"belly265","comments":[{"upvote_count":"1","comment_id":"696521","content":"Big Table is right choice, hence C is correct","poster":"AzureDP900","timestamp":"1665951420.0"}],"timestamp":"1645328160.0"},{"comment_id":"528180","timestamp":"1642659480.0","poster":"Narinder","content":"Google Cloud Big Table is best for the use-case to store the time-series data, so C is correct","upvote_count":"1"},{"timestamp":"1640769900.0","upvote_count":"2","poster":"OrangeTiger","content":"I choose C.\nA Big Query Seems good.But keyword 'IOT' is here.\nB. Google Cloud SQL does'nt work this case.\nD. Google Cloud Storage does'nt suppourt realtime analytics.","comment_id":"512004"},{"timestamp":"1638551280.0","poster":"haroldbenites","upvote_count":"2","comment_id":"493268","content":"Go for C."},{"timestamp":"1638015480.0","upvote_count":"1","comment_id":"488114","poster":"vincy2202","content":"Answer is C"},{"content":"C: Bigtable is suitable for IoT sensor data and it has low latency which will provide the performance for weather app","comment_id":"391086","poster":"aviratna","timestamp":"1624699980.0","upvote_count":"2"},{"upvote_count":"3","content":"Bigtable is ideal for IoT kind of data. Answer is C.","comment_id":"348275","timestamp":"1620018120.0","poster":"Koushick"},{"comment_id":"344062","poster":"JohnWick2020","upvote_count":"2","timestamp":"1619537280.0","content":"Answer is C - Cloud Bigtable, which is the default option for IoT and streaming use cases."},{"upvote_count":"2","poster":"Umer24","timestamp":"1611423240.0","comment_id":"274765","content":"New Question:4 (Part 2 of 2)\n(C). 1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more space. 2. Deploy Memcached to reduce CPU load. 3. Change the instance type to 32-core machine type to reduce replication lag.\n(D). 1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more space. 2. Deploy Memcached to reduce CPU load. 3. Create a Stackdriver alert for replication lag, and change the instance type to a 32-core machine type to reduce replication lag."},{"comment_id":"273767","timestamp":"1611322920.0","content":"New Question:7\nAll Compute Engine instance in your VPC should be able to connect to an Active Directory server on specific ports. Any other traffic emerging from your instances is not allowed. You want to enforce this using VPC firewall rules. How should you configure the firewall rules?\nA. Create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.\nB. Create an egress rule with priority 100 to deny all traffic for all instance. Create another egress rule with priority 1000 to allow the Active Directory traffic for all instances.\nC. Create an egress rule with priority 1000 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 100 to block all traffic for all instances.\nD. Create an egress rule with priority 100 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 1000 to block all traffic for all instances.","comments":[{"content":"A is the ans.\nFor C no implied deny egress rule in GCP, only implied allow egress and implied deny ingress","upvote_count":"3","poster":"tzKhalil","comment_id":"332408","timestamp":"1618045440.0"}],"upvote_count":"2","poster":"Umer24"},{"timestamp":"1611322440.0","content":"You need to setup Microsoft SQL Server on GCP. Management requires that there’s no downtime in case of a data center outage in any of the zones with a GCP region. What should you do?\nA. Configure a Cloud SQL instance with high availability enabled.\nB. Configure a Cloud Spanner instance with a regional instance configuration.\nC. Set up SQL Server on Compute Engine, using Always on Availability Groups using windows Failover Clustering. Place nodes in different subnets.\nD. Set up SQL Server Always On Availability Groups using Windows Failover Clustering. Place nodes in different zones.","comments":[{"timestamp":"1618045320.0","comment_id":"332407","upvote_count":"2","poster":"tzKhalil","content":"A is the ans"}],"comment_id":"273756","poster":"Umer24","upvote_count":"2"},{"comments":[{"poster":"tzKhalil","content":"A is the ans","comment_id":"332403","upvote_count":"2","timestamp":"1618044900.0"}],"content":"New Question:2 (Part 1 of 2)\nYou have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to operate in production. You want to monitor and maximize machine utilization. You also want to reliably deploy a new versions of the application. Which set of steps should you take?\n(A). Perform the following: 1. Create a managed instance group with f1-micro type machines. 2. Use a startup script to clone the repository. Check out the production branch, install the dependencies, and start the Python app. 3. Restart the instances to automatically deploy new production releases.\n(B). Perform the following: 1. Create a managed instance group with n 1-standard-1 type machines. 2. Build a Compute Engine image from the production branch that contains all the dependencies and automatically starts the Python app.\n3. Rebuild the Compute Engine image, and update the instance template to deploy new production releases.","poster":"Umer24","timestamp":"1611321840.0","upvote_count":"3","comment_id":"273750"},{"comment_id":"273745","upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1619537220.0","content":"Answer for this question is D (Cloud Data Prep).","poster":"JohnWick2020","comment_id":"344061"},{"content":"A is the ans","comment_id":"332402","upvote_count":"1","poster":"tzKhalil","timestamp":"1618044840.0"}],"timestamp":"1611321420.0","content":"New Question:1\nFor this question, refer to the TerramEarth case study. A new architecture that writes all incoming data to bigQuery has been introduced. You notice that the data is dirty, and want to ensure data quality on an automated daily basis while managing cost. What should you do?\nA. Set up a streaming Cloud Dataflow job, receiving data by the ingestion process. Clean the data in a Cloud Dataflow pipeline.\nB. Create a Cloud Function that reads dta from BigQuery and clean it. \nC. Create a SQL statement on the data in BigQuery, and save it as a view. Run a view daily, and save the result to a new table.\nD. Use Cloud Dataprep and configure the BigQuery tables as the source. Schedule a daily job to clean the data.","poster":"Umer24"},{"content":"C, for sure.\nReal Time","comment_id":"105937","comments":[{"content":"and IOT","timestamp":"1591863120.0","comment_id":"107537","upvote_count":"2","poster":"gfhbox0083"}],"upvote_count":"4","poster":"gfhbox0083","timestamp":"1591702380.0"},{"comments":[{"comment_id":"304031","upvote_count":"1","content":"Ans is C, IoT = Bigtable","poster":"nitinz","timestamp":"1614923400.0"}],"content":"C is right","poster":"Ziegler","timestamp":"1591482720.0","upvote_count":"4","comment_id":"104184"}],"answers_community":["C (100%)"],"unix_timestamp":1591482720,"isMC":true,"question_text":"You want to optimize the performance of an accurate, real-time, weather-charting application. The data comes from 50,000 sensors sending 10 readings a second, in the format of a timestamp and sensor reading.\nWhere should you store the data?","timestamp":"2020-06-07 00:32:00","url":"https://www.examtopics.com/discussions/google/view/22386-exam-professional-cloud-architect-topic-1-question-20/","topic":"1"},{"id":"6arnJyblzQ9FYTDN7w88","timestamp":"2025-03-17 06:55:00","url":"https://www.examtopics.com/discussions/google/view/169213-exam-professional-cloud-architect-topic-1-question-200/","discussion":[{"timestamp":"1743951840.0","content":"Selected Answer: B\nAnswer is B, and I think the question is duplicated","comment_id":"1558292","upvote_count":"1","poster":"samsonakala"},{"content":"Selected Answer: B\nExplanation:\n• VPC Service Controls help you create a security perimeter around your Google Cloud services (like BigQuery) to reduce the risk of data exfiltration, even if credentials are compromised or insiders attempt to overshare data.\n• Private Google Access for on-premises hosts ensures that on-premises systems can securely access Google APIs and services without going over the public internet, further reducing exposure.\n\nTogether, these measures address the security team’s concerns by limiting external data leakage from BigQuery while allowing secure, private connectivity.","upvote_count":"1","poster":"frank_tsai_tech","comment_id":"1400954","timestamp":"1742461080.0"},{"poster":"yokoyan","content":"Selected Answer: B\n(Gemini's Answer)\nAnswer: B\nComment: VPC Service Controls establishes a security perimeter to mitigate data exfiltration risks, while Private Google Access enables private connectivity from on-premises, enhancing overall security. Combining these effectively addresses the security team's concerns about data leakage.","upvote_count":"1","timestamp":"1742190900.0","comment_id":"1399542"}],"answer_description":"","isMC":true,"answer":"B","question_text":"Your company has a Google Cloud project that uses BigOuery for data warehousing. The VPN tunnel between the on-premises environment and Google Cloud is configured with Cloud VPN. Your security team wants to avoid data exfiltration by malicious insiders, compromised code, and accidental oversharing. What should you do?","choices":{"A":"Configure Private Service Connect.","B":"Configure VPC Service Controls and configure Private Google Access for on-promises hosts.","D":"Configure Private Google Access.","C":"Create a service account, grant the BigQuery JobUser role and Storage Object Viewer role to the service account, and remove all other Identity and Access Management (IAM) access from the project."},"answers_community":["B (100%)"],"question_images":[],"exam_id":4,"answer_ET":"B","answer_images":[],"unix_timestamp":1742190900,"topic":"1","question_id":114},{"id":"wfQbZsKuEzggXXmiRxim","url":"https://www.examtopics.com/discussions/google/view/7128-exam-professional-cloud-architect-topic-1-question-21/","question_text":"Your company's user-feedback portal comprises a standard LAMP stack replicated across two zones. It is deployed in the us-central1 region and uses autoscaled managed instance groups on all layers, except the database. Currently, only a small group of select customers have access to the portal. The portal meets a\n99,99% availability SLA under these conditions. However next quarter, your company will be making the portal available to all users, including unauthenticated users. You need to develop a resiliency testing strategy to ensure the system maintains the SLA once they introduce additional user load.\nWhat should you do?","answer_images":[],"topic":"1","isMC":true,"question_images":[],"discussion":[{"comments":[{"poster":"rockstar9622","comment_id":"40630","content":"I agree with @jcmoranp, B) is correct for more info - https://cloud.google.com/solutions/scalable-and-resilient-apps#test_your_resilience","upvote_count":"20","timestamp":"1579446780.0"},{"content":"Isn't A superior in one way. It will demonstrate that the app is regionally redundant by demonstrating it can survive the loss of an entire zone. B only demonstrates the app is zonally redundant and can lose a random instance here and there within individual zones which is not that resilient. Thoughts?","comment_id":"477121","comments":[{"poster":"0xE8D4A51000","comment_id":"699023","content":"No. It is only terminating the service in ONE zone. B caters for terminating the service in both zones randomly. You want to be able to test resiliency when either zone has an outage.","timestamp":"1666182600.0","upvote_count":"7"}],"upvote_count":"8","poster":"AWSPro24","timestamp":"1636743360.0"}],"timestamp":"1572115440.0","upvote_count":"94","poster":"jcmoranp","comment_id":"17679","content":"resilience test is not about load, is about terminate resources and service not affected. Think it's B. The best for resilience in to introduce chaos in the infraestructure"},{"content":"Will go with A. Reason: \n1. SLA in question is about the Availability (The portal meets a\n99,99% availability SLA under these conditions.) therefore maintaining SLA means Availability.\n2. Its a user-feedback portal and type of user input is going to be similar or same (A is capturing the user input and replaying it).\n\nWhy not B: \nThe infrastructure is using MIG (Instances created using templates) most likely to be used with Health Check and killing random VMs cannot test the availability (neither affect the availability as health check will immediately kill the effected Instances and create the other one.)\nWhy not D:\nSLA is about Availability not reliability or scaling. (As all of it does work hand to hand but still major focus should be on availability.)\n\n--- IF AGREE PLEASE UP VOTE TO MAKE IT CLEAR FOR THE OTHERS --- Thank you.","comments":[{"timestamp":"1660725900.0","poster":"RitwickKumar","upvote_count":"10","content":"Only problem with A is that it says \"replay captured user load\". We are not testing for the incoming unpredictable load due to the inclusion of unauthenticated users and something that we haven't captured earlier.\n\nOption B covers breadth and depth for the desired SLA.","comment_id":"647991","comments":[{"comment_id":"650370","poster":"jay9114","content":"What does \"replay captured user load\" mean?","timestamp":"1661188680.0","upvote_count":"3"}]},{"content":"valuable input in terms of 'availability'. did you select this answer in exam too?","poster":"bolu","upvote_count":"1","timestamp":"1609700520.0","comment_id":"258787"},{"comment_id":"440803","content":"We are talking about resilience testing where as SLA is an argument of the system.","comments":[{"comment_id":"440806","poster":"amxexam","upvote_count":"1","content":"And resilience means the capacity to recover from failure.","timestamp":"1631003580.0"}],"poster":"amxexam","timestamp":"1631003520.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1636999560.0","poster":"AWSPro24","content":"A ensures the app can withstand the loss of a whole Zone which I think is important as well.","comment_id":"478905"},{"poster":"bargou","timestamp":"1706731080.0","content":"also, it's mentionned that there is only one DB, if randomly we kill the DB, all the system will be KO","upvote_count":"1","comment_id":"1137084"}],"comment_id":"225183","timestamp":"1606073160.0","upvote_count":"62","poster":"OSNG"},{"content":"Selected Answer: C\nI'm a bit puzzled by none selecting C. Why shouldn't C be the best options. Aren't ABD only partially fulfilling the requirements, whilst C marks them all? Real users to test (most realistic), allows stress testing by increasing load gradually, randomly shuts resources so includes Cahos Eng. Any feedback here please?","comment_id":"1411820","poster":"scialappa","upvote_count":"1","timestamp":"1743273120.0"},{"upvote_count":"1","poster":"david_tay","comment_id":"1359618","timestamp":"1740110760.0","content":"Selected Answer: A\nAnswer is likely A as Gemini said so, and some others here agree on the logic."},{"content":"Selected Answer: B\nTo perform resilience test and auto scale testing B is correct choice.","timestamp":"1737549420.0","comment_id":"1344738","poster":"FabPanda","upvote_count":"1"},{"poster":"JonathanSJ","upvote_count":"1","content":"Selected Answer: B\nI will go for B","timestamp":"1735505460.0","comment_id":"1333722"},{"content":"Selected Answer: B\n1. Synthetic Load Generation: Creating synthetic user input allows you to simulate a wide range of user behavior and load patterns, including spikes and sustained high traffic. This helps you test the system's ability to scale and handle unexpected loads.\n2. Autoscaling Validation: By replaying the synthetic load, you can verify that the autoscaling logic is working correctly across all layers of the LAMP stack. This ensures that the system can dynamically adjust resources to meet demand.\n3. Chaos Engineering: Introducing chaos by terminating random resources simulates real-world failures and helps you test the system's resilience to unexpected disruptions. This is crucial for maintaining the 99.99% availability SLA.\n4. Controlled Environment: This approach allows you to conduct testing in a controlled environment without impacting real users. You can gradually increase the load and introduce chaos in a measured way to identify weaknesses and improve resilience.","upvote_count":"2","poster":"Ekramy_Elnaggar","timestamp":"1731310020.0","comment_id":"1309878"},{"comment_id":"1272507","timestamp":"1724657340.0","upvote_count":"1","poster":"potorange","content":"Selected Answer: A\nA: requirements states \"all layers\" and \"resiliency testing\""},{"timestamp":"1716661140.0","upvote_count":"1","poster":"Robert0","comment_id":"1218503","content":"Selected Answer: A\nI would go with A.\nThis solution test autoscale policy of each layer (not only one as option B refers).\nAlso, it propose a regional shutdown. This is a very good test commonly requested if your application is geo-redundant. In crontast, option B propose random termination of resources, not a bad practice but a little bit vague that can be implemented terrible wrong (for example you do not kill the interesting services or you kill the same service in both regions, thus generating a blackout)"},{"content":"Selected Answer: B\nWe need to do 1. load testing and 2. reliability test ( failover redundency )\n B does both\nA only tests one zone\nC impacts real user experience\nD 200% not necessary","upvote_count":"4","poster":"lisabisa","comment_id":"1155831","timestamp":"1708546380.0"},{"content":"Selected Answer: A\nYou need to develop a resiliency testing strategy to ensure the system maintains the SLA once they introduce additional user load.\n\nNeed to maintain SLA of 99.9% means multiple zones, resilience means fault tolerance. Teminating all resources in one zone is also creating a chaos.","upvote_count":"1","timestamp":"1696414080.0","poster":"AdityaGupta","comment_id":"1024641"},{"timestamp":"1692719580.0","poster":"heretolearnazure","comment_id":"987580","upvote_count":"1","content":"B is correct"},{"content":"Selected Answer: D\nOption D is the best resiliency testing strategy in this scenario as it ensures that the system is tested with actual user data, takes into account the expected increase in user load, and ensures that the system is adequately scaled to handle the anticipated load.","poster":"VaraSrinvas","comments":[{"poster":"didek1986","comment_id":"984982","content":"Do not agree. B is 100% correct","upvote_count":"1","timestamp":"1692422220.0"},{"timestamp":"1695587100.0","upvote_count":"1","content":"But this would assume that the user load will not change; plus, the current application is visible only to a small group of select customers - this is the current production setup. The deployment should be prepared for all existing users plus unauthenticated users, and the load increase is unknown, so testing for 200% of \"expected load\" is very ambiguous.","poster":"jrisl1991","comment_id":"1016158"}],"timestamp":"1686711660.0","upvote_count":"2","comment_id":"922697"},{"content":"Selected Answer: B\nB. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce ג€chaosג€ to the system by terminating random resources on both zones.\n\nBy creating synthetic random user input and replaying the load, you can simulate the expected increased user traffic and trigger the autoscale logic on different layers of the application. Introducing chaos to the system by terminating random resources in both zones helps test the resiliency and redundancy of the system under stress. This strategy will help ensure that the system can maintain the 99.99% availability SLA when subjected to additional user load.","comment_id":"873391","upvote_count":"5","poster":"JC0926","timestamp":"1681803960.0"},{"upvote_count":"2","poster":"telp","timestamp":"1679648160.0","content":"Selected Answer: B\nchaos == test resilience for google","comment_id":"849129"},{"upvote_count":"2","poster":"Deb2293","timestamp":"1677203460.0","comment_id":"820008","content":"Selected Answer: B\nThis is chaos engineering used by Netflix. https://netflixtechblog.com/tagged/chaos-engineering"},{"timestamp":"1673286060.0","comment_id":"770704","poster":"roaming_panda","upvote_count":"4","content":"Selected Answer: B\nchaos == checking resilience"},{"content":"right thought ,","upvote_count":"1","comment_id":"747078","timestamp":"1671188280.0","poster":"holerina"},{"timestamp":"1668851220.0","comment_id":"721918","content":"Selected Answer: B\nB is correct; Using synthetic/random input is recommended. Chaos Engineering/Symian Army from Netflix is one of the proven mechanism to test the resilience of the application.","upvote_count":"2","poster":"AniketD"},{"upvote_count":"2","poster":"megumin","timestamp":"1668077220.0","comment_id":"715152","content":"Selected Answer: B\nB is ok"},{"comment_id":"699024","content":"Selected Answer: B\nB caters for terminating the service in both zones randomly. You want to be able to test resiliency when either zone has an outage.","timestamp":"1666182660.0","poster":"0xE8D4A51000","upvote_count":"3"},{"comment_id":"696761","upvote_count":"1","timestamp":"1665976140.0","content":"chaos engineering is the buzzword to look after \nAnswer is B","poster":"zr79"},{"timestamp":"1665646920.0","upvote_count":"1","content":"Selected Answer: B\nB. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce ג€chaosג€ to the system by terminating random resources on both zones","comment_id":"693710","poster":"minmin2020","comments":[{"upvote_count":"2","poster":"AzureDP900","content":"B is right","comment_id":"696524","timestamp":"1665951540.0"}]},{"content":"Selected Answer: B\nQuestion say : \"You need to develop a resiliency testing strategy\" so introduce Chaos Engineering is the best option in a testing process. I choose B","timestamp":"1660719480.0","poster":"[Removed]","upvote_count":"5","comment_id":"647958"},{"content":"Target is to maintain 99.99% availability.\nLAMP - Linux, Apache, MySQL, and PHP\nOption B synthetic load is created unless autoscaling is triggered in atleast one layer. We aren't necessarily testing autoscaling on all layers. So I would rule out B.\n+ Incase of a Zonal failure, Availability is affected. We better test Zonal failure instead of chaos termination.\n\nI would go with Option A instead.","comment_id":"645968","poster":"AMohanty","timestamp":"1660321020.0","upvote_count":"1"},{"timestamp":"1653379620.0","content":"Selected Answer: B\nComparing scenarios from A and B it looks like A is a specific scenario which could be covered by chaos strategy : shut down whole zone. Why not? chaos engineering is not just about shutting down singe instances or interfaces, right?\nSo I'd go with B.","poster":"ryzior","upvote_count":"2","comment_id":"606559"},{"comment_id":"600996","poster":"Gini","content":"Selected Answer: B\nI go for B as it \"create synthetic random user input\" which can reveal potential faults. Option A may not cover all possible input since it has only existing users input.","timestamp":"1652424780.0","upvote_count":"2"},{"content":"Selected Answer: B\nCheck out my eviler pust for detailed explanation.","timestamp":"1651937580.0","comments":[{"upvote_count":"1","comment_id":"598183","poster":"amxexam","content":"typo elier post for detailed explation.","timestamp":"1651937640.0"}],"poster":"amxexam","comment_id":"598182","upvote_count":"1"},{"timestamp":"1651933980.0","poster":"kapara","content":"Selected Answer: B\nB is the correct answer. why in the hell will I need to take down the whole zone for this question senario?","comment_id":"598150","upvote_count":"2"},{"poster":"wilwong","comment_id":"598033","content":"Selected Answer: B\nMIG, It is deployed in the us-central1 region and uses autoscaled managed instance groups on all layers","upvote_count":"1","timestamp":"1651912080.0"},{"poster":"Nirca","comment_id":"588562","content":"Selected Answer: A\nI will go for A : \n1. Tt is a situation were the system is in production so we can capture real load traffic. \n2. A is also triggering \"All layers\", including the Database. The database must be analyzed because it was stated as \"not auto-scaled\"","upvote_count":"3","timestamp":"1650450780.0"},{"poster":"pawel_ski","comment_id":"573532","content":"Selected Answer: A\nWill go with A. Reason:\n1. SLA in question is about the Availability (The portal meets a\n99,99% availability SLA under these conditions.) therefore maintaining SLA means Availability.\n2. Its a user-feedback portal and type of user input is going to be similar or same (A is capturing the user input and replaying it).","upvote_count":"3","timestamp":"1648030140.0"},{"poster":"belly265","upvote_count":"3","content":"Ans b -best for resilience is to go for Chaos testing","timestamp":"1645328340.0","comment_id":"551514"},{"poster":"pakilodi","timestamp":"1638697860.0","content":"Selected Answer: B\nVote B. A and D cannot be the answer for one thing: Capture user input data. Doing this, it will break privacy regulament and so one you can be in legal situation. only for testing resiliency.","comment_id":"494201","upvote_count":"3"},{"comment_id":"489476","upvote_count":"2","timestamp":"1638142020.0","poster":"duocnh","content":"Selected Answer: B\nvote B"},{"poster":"vincy2202","content":"B seems to be the correct answer. \nTo test \"Resiliency\", we need to introduce intermittent failures & \"chaos\" is the best way to do so. \nhttps://cloud.google.com/architecture/scalable-and-resilient-apps#test_your_resilience","upvote_count":"4","comment_id":"483782","timestamp":"1637544540.0"},{"poster":"MaxNRG","content":"B – create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce chaos in the system by terminating random resources on both zones.","comments":[{"poster":"MaxNRG","upvote_count":"3","content":"Quite interesting Q, when A and B are the candidates.\n“D” won’t work since it doesn’t test resiliency (outage of resources), it’s completely focused on scalability.\n“B” most accuratly approaches real-world scenario. Random input for multiple users (registered and not) and random outage of a resource in both zones (when system scaled). Such chaos methodology is practiced on Cloud solutions. Check this page about Chaos Monkey resiliency test tool at Netflix. “B” basically fits all Q’s requirements.\nAlso read about Google’s Scalability and Resiliency Designs, this page doesn’t focus on testability but very closely describe system design of this Q.\n“A” tests more hypothetic case, even more about Disaster Recovery, when whole zone gets down. Since, this test doesn’t change test data / scenario – it is too artificial for finding bottlenecks, performance bugs, etc.","comment_id":"467030","comments":[{"poster":"jay9114","upvote_count":"1","content":"Good explanation","timestamp":"1661203320.0","comment_id":"650434"}],"timestamp":"1635091140.0"}],"comment_id":"467029","upvote_count":"3","timestamp":"1635091140.0"},{"upvote_count":"5","content":"Spliting answer into 2 as the website wont allow posting big answers.\n\nThe requirement is reliance testing (recovery from failure )while maintaining 99.9 availability (SLA)\n\nLet's go with elimination.\n\nA. Capture existing users input, and replay captured user load until autoscale is triggered on all layers. At the same time, terminate all resources in one of the zones\n>> We are overkilling resilience testing by entering the boundary of DR hence will eliminate this option.\n\nB. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce ג€chaosג€ to the system by terminating random resources on both zones\n>> This option takes care of the requirement of testing. Is a patter used by org like Netflix chaos monkey).","timestamp":"1631004480.0","comment_id":"440814","poster":"amxexam","comments":[{"comment_id":"440815","timestamp":"1631004540.0","poster":"amxexam","content":"C. Expose the new system to a larger group of users, and increase group size each day until autoscale logic is triggered on all layers. At the same time, terminate random resources on both zones\n>> Next quarter your user going to increase, but how are you going to get more users to test, this option is not feasible hence eliminating. \n\nD. Capture existing users input, and replay captured user load until resource utilization crosses 80%. Also, derive estimated number of users based on existing userג€™s usage of the app, and deploy enough resources to handle 200% of expected load\n>> The failure is not necessary from CPU utilization, even the hardware can fail under load. Blanket provisioning is not the way to go hence eliminating.\n\nHence B","upvote_count":"6"}]},{"timestamp":"1627192740.0","poster":"DreamerK","comment_id":"413635","upvote_count":"2","content":"I would go for A since A achieves the greatest resiliency:\n1. Autoscaling is tested at all layers. It will be a disaster if any of the layer couldn't be scaled out properly in real spiking workloads. In this case, B only tests at least one.\n2. We should expect a failure at zonal level to ensure higher resiliency. B only shuts down random resource, which cannot test the resiliency when full zone goes down."},{"comment_id":"391088","upvote_count":"2","content":"B: For test simulate the load and Chaos test is important part of testing which will handle failure scenario at different level","timestamp":"1624700220.0","poster":"aviratna"},{"content":"B. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce \"chaos\" to the system by terminating random resources on both zones","poster":"victory108","upvote_count":"2","comment_id":"360244","timestamp":"1621325820.0"},{"content":"I will go with B","comment_id":"353929","timestamp":"1620664920.0","upvote_count":"2","poster":"un"},{"timestamp":"1617081180.0","upvote_count":"3","content":"Answer is B","comment_id":"323990","poster":"Ausias18"},{"upvote_count":"3","content":"IMO both B and A should be done in the real world. None of them is sufficient alone. \nBut for the Exam - B is more comprehensiv than A.\nSo - I'll go with B.","comment_id":"320136","timestamp":"1616672820.0","poster":"lynx256"},{"content":"A vs B\nA. “until autoscale is triggered on all layers”, “all resources in one of the zones”\nB. “until autoscale logic is triggered on at least one layer”, “introduce \"chaos\"”\nWe “need to develop a resiliency testing strategy”. So we are supposed to test the worst case which can may happen. \nI choose A.","timestamp":"1616010720.0","comment_id":"313561","upvote_count":"2","poster":"pawel_ski"},{"upvote_count":"3","content":"It cannot be answer D as the question states \"available to all users\". It pretty seems like opening the app to the public and means you have no clues on what will be the expectations for a 100% load. Hence you cannot test a 200% load.\n\nB fits better as it ensures autoscalling is properly configured by triggering it and health checks are configured by terminating ressources and see them being recreated","timestamp":"1614238440.0","comment_id":"298840","poster":"Alekshar"},{"upvote_count":"1","timestamp":"1611785460.0","comment_id":"278103","content":"The reasoning for D may be following.\nThe goal is to \"maintain the SLA once they introduce additional user load\".\nNOT to prepare for random service failure. A, B, C all test by introducing failure NOT focusing on increasing load on a working system.","poster":"Rothmansua"},{"comment_id":"274970","timestamp":"1611459300.0","content":"C is out. Because it is not a test but a production change.\nA vs B: A want to shutdown \"ALL\" resources while B shutdown some randomly. That is the way to go.\nD: sounds nothing wrong but it is about load test. and the number 80% and 200% is arguable.\nI choose B.","upvote_count":"2","poster":"bnlcnd"},{"comment_id":"267471","upvote_count":"2","timestamp":"1610663640.0","poster":"alii","content":"I am still not sure what's the majority opinion on this. With D problem is how are you sure about 200% and 80% numbers. where these number came from, who said 80% of CPU will be a good measure to prove something?\n\nA seems correct in the sense that one of your zone is down, you are getting a lot of load but still one zone able to sustain with autoscaling capability."},{"upvote_count":"3","timestamp":"1609936380.0","content":"Test about maximum resiliency is only possible for a outage of a zone. So A should be the answer. B is about creating chaos which is good but will not take care of the resiliency and SLA in the forefront. D is about performance.","poster":"Arimaverick","comment_id":"260998"},{"comment_id":"260451","timestamp":"1609868580.0","poster":"mwilbert","upvote_count":"2","content":"B seems correct in the first place, but if you replay real user load in a production system, how are you going to distinguish real data from test data (assuming users ever update anything)? So I don't like any of the answers that involve doing replays into production. Synthetic test data doesn't have that problem."},{"poster":"Surjit24","content":"Pingpong going on this","timestamp":"1607365260.0","comment_id":"237560","upvote_count":"1"},{"content":"Should be A.\nResilience test is effective if involve all layers, this exclude B.\nD impose to deploy more VM than necessary, why? and don't test autoscale.\nC works in production so it isn't a test.","poster":"occupatissimo","timestamp":"1606466760.0","comment_id":"228927","upvote_count":"6"},{"poster":"nimso","upvote_count":"2","comment_id":"206512","timestamp":"1603738080.0","content":"I'll go with B"},{"poster":"dan80","content":"it clear that B is incorrect because of the \"chaos\" word, this is more close to AWS Netflix team destroying Production but not with GCP. Answer D is correct - to estimate load and size it 2.5 times is best practice.","upvote_count":"2","timestamp":"1602450660.0","comment_id":"197970"},{"poster":"awadheshk","comment_id":"195051","content":"Correct answers can be A, B and D. But when it comes to maintain SLA .. remember that we need to have quantifiable data (reference from SRE) .. So option D is right .. as per me.","timestamp":"1602061920.0","upvote_count":"1"},{"comment_id":"187709","poster":"Viba","upvote_count":"1","timestamp":"1601129580.0","content":"D looks correct after reading the excerpt - \nCompute Engine virtual machines or GKE clusters typically take time to scale up, because new nodes need to be created and initialized. Therefore, it might be necessary to maintain a minimum set of resources, even if there is no traffic."},{"content":"D is proven a wrong answer by Pokemon Go launch.\nThe 200% is a figure plug from the air (and no scientific basis that it is 100% address the SLA requirements)\n\nI think B is the right answer.","timestamp":"1601096640.0","upvote_count":"2","poster":"VedaSW","comment_id":"187441"},{"comment_id":"187021","content":"A. It tests more than what chaos does: 100% of a zone. The load exists only for the auto-scaling. B only tests a subset of what A tests. You'd do B in *production*, you'd only do A during testing.","upvote_count":"1","poster":"kimberjdaw","timestamp":"1601042520.0"},{"comment_id":"179454","upvote_count":"3","poster":"AshokC","timestamp":"1600104960.0","content":"B is right answer.\nCreate synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce \"chaos\" to the system by terminating random resources on both zones.\n\nResilience - the ability of the system to manage the \"Chaos\" in terms of failure/unavailability, etc."},{"content":"B is correct","comment_id":"167996","upvote_count":"3","timestamp":"1598580180.0","poster":"Kabiliravi"},{"timestamp":"1597929240.0","poster":"wiqi","comment_id":"162245","content":"I think its B, since the question is more about resiliency and not autoscaling...\nAutoscaling is already working well.","upvote_count":"2"},{"content":"Answer is clearly D ,The Qn Mentions test need to check 99,99% availability SLA ,Also given is there are two zones , hence to meet Availability you need 200% resource deployment to cater for zonal outages ,Also its good practice to simulate load based on existing user data","timestamp":"1596347220.0","upvote_count":"1","comment_id":"148919","poster":"kaush"},{"upvote_count":"1","comment_id":"120340","content":"I believe it's \"A\"\nThere is already a test performed with select number of users so we have data about user behavior therefore I don't see the point of creating an additional synthetic user data. \n\n.. and to test resiliency it's best to perform a zonal failure (and it can also be considered as a chaos)","poster":"cetanx","timestamp":"1593161460.0"},{"poster":"HectorLeon2099","comment_id":"117839","upvote_count":"3","content":"I'll go with B. Option D is not cost effective","timestamp":"1592946000.0"},{"timestamp":"1592891220.0","upvote_count":"3","comment_id":"117084","poster":"mlantonis","content":"B is the correct. We need to introduce chaos for resiliency."},{"comment_id":"115081","poster":"Pupina","timestamp":"1592694120.0","upvote_count":"4","content":"Go with B. Chaos Monkey strategy to probe resilience"},{"content":"B is the right answer.","timestamp":"1592630880.0","upvote_count":"4","comment_id":"114444","poster":"Tushant"},{"content":"Right option is B. Resiliency test need chaos testing by shutting some VMs in one or both zone, sometimes complete zone also.","comment_id":"112955","upvote_count":"2","timestamp":"1592459640.0","poster":"kban"},{"poster":"devnull10","comment_id":"108767","timestamp":"1591973220.0","content":"A: This seems correct to me - we're talking about resilience, not scalability, therefore replaying user input and taking out a zone seems reasonable. It's extremely unlikely to lose multiple zones simultaneously, so I would prefer this.\nB: Picking random components in random zones doesn't test for a full zone going down.\nC: Is effectively testing in prod - a big no.\nD: if you deployed enough resource to handle 200% expected load then it's just going to autoscale back anyway.","upvote_count":"1"},{"poster":"devnull10","content":"A: This seems correct to me - we're talking about resilience, not scalability, therefore replaying user input and taking out a zone seems reasonable. It's extremely unlikely to lose multiple zones simultaneously, so I would prefer this.\nB: Picking random components in random zones doesn't test for a full zone going down.\nC: Is effectively testing in prod - a big no.\nD: if you deployed enough resource to handle 200% expected load then it's just going to autoscale back anyway.","comment_id":"108766","upvote_count":"1","timestamp":"1591973040.0"},{"poster":"Nirms","content":"B is the correct answer","comment_id":"100858","timestamp":"1591104960.0","upvote_count":"4"},{"comment_id":"98326","content":"B is the correct answer because the question talks about resiliency of the even from unauthenticated (unknown) user.","timestamp":"1590766080.0","poster":"Ziegler","upvote_count":"3"},{"upvote_count":"3","comment_id":"97300","content":"Final Decision to go with Option B","poster":"AD2AD4","timestamp":"1590646620.0"},{"comment_id":"95306","upvote_count":"1","content":"I think A is correct. The 99.95% availability has been achieved using the 2-Zone design, so any test should involve simulating a zone failure. Incremental replay of user activity would simulate the expected additional workload and shows at one point in load (number of users) that autoscale is triggered , which would help to configure the autoscaling parameters.","timestamp":"1590398820.0","poster":"wazza88"},{"comment_id":"93355","content":"D (Correct answer) - Deploy changes to a small subset of users before rolling out to production. This is the practice in Canary deployment. The bug slip into production may be caused by the discrepancy between test/staging and production environments or testing data. With Canary deployment or Canary test, you have the ability to test code with live data at any time, you increase the chance discovering the bug earlier and reduced the risk bring the bug into production with minimums impact and down time by rolling back quickly\n\nC - Increase the load on your test and staging environments. Increase the load in your test and staging environment may or may not help to discover the bugs. Also, the question did not say what kind of load level. In some situation with same environment and same set of test data, just increase load won't help to discover the performance bug so you need Canary test/deployment.","upvote_count":"1","poster":"misho","timestamp":"1590060960.0"},{"comment_id":"86296","timestamp":"1589072880.0","content":"I will go with B.\nIt sounds like there is a gap between \"today\" and next quarter which would allow time for testing.\nA does not seem reasonable because there isn't that much input.\nB sounds reasonable because you can generate huge amounts of activity and also inject failure.\nC doesn't sound reasonable because you risk a bad user experience without having done basic load testing.\nD sounds ok but doesn't handle failure injection.","poster":"clouddude","upvote_count":"5"},{"timestamp":"1588796160.0","poster":"gcp_aws","upvote_count":"2","content":"B is the correct answer","comment_id":"84716"},{"content":"I think answer is B.\n\nResilience - ability of the system to manage the \"Chaos\" in terms of failure/unavailability, etc.\n\nOption D: I will not deploy ENOUGH resources to handle 200% of expected traffic in advance and pay for it.\n\nCloud is all about Dynamic scaling, no need to allocate in advance, this is feature missing in data center and migrating to cloud.","comment_id":"61796","poster":"Rathish","upvote_count":"6","timestamp":"1583855520.0"},{"upvote_count":"5","timestamp":"1583575980.0","content":"I also think B, because the application will be extended to unauthenticated users and their usage behaviour may not be the same as existing users","comment_id":"60250","poster":"rickywck"},{"poster":"Jeysolomon","comment_id":"57776","upvote_count":"5","timestamp":"1583180640.0","comments":[{"content":"if you shutdown everything in one zone, you're just testing DR and not individual components. B looks more approriate","upvote_count":"3","poster":"[Removed]","timestamp":"1596047340.0","comment_id":"146754"}],"content":"I think the Answer is A - This tests all individual components failure, zonal failure and at the same time autoscale in all applicable layers."},{"comment_id":"44714","content":"answer: B","timestamp":"1580390340.0","upvote_count":"7","poster":"2g"},{"poster":"natpilot","timestamp":"1580022600.0","content":"B is correct","comment_id":"42794","upvote_count":"9"},{"timestamp":"1578771000.0","poster":"AWS56","comment_id":"37823","content":"B is the one","upvote_count":"8"},{"comment_id":"31798","timestamp":"1577032140.0","poster":"suryalsp","upvote_count":"9","content":"https://www.linkedin.com/pulse/resilience-testing-cloud-archana-agarwal-pmi-acp. \nAns is B"},{"poster":"VenkatGCP1","upvote_count":"2","content":"I think the question is to maintain SLA, B looks like introducing Chaos Monkey, so I believe D is correct.","comment_id":"22420","timestamp":"1574080680.0"},{"timestamp":"1571952480.0","upvote_count":"4","poster":"Eroc","content":"To test resilience we should set the circuitry to its maximum processing capacity. Terminating resources would prevent that. So options A and B are incorrect. The question says that they want to test before exposing it to mroe users, so C is incorrect. Leaving D as correct. D also tests beyond the expected usage, which is a great idea because surges do occur.","comment_id":"17268"},{"poster":"KouShikyou","content":"Can D test resilience? I think we need to terminate some resource.","comment_id":"17161","timestamp":"1571918040.0","comments":[{"poster":"tartar","content":"B is ok","comment_id":"151653","timestamp":"1596686820.0","upvote_count":"5"},{"timestamp":"1614877080.0","poster":"nitinz","upvote_count":"1","content":"B, key take away is resiliency. Netflix made Chaos Monkey just for resiliency testing.","comment_id":"303482"}],"upvote_count":"4"}],"answer_ET":"B","answers_community":["B (78%)","A (17%)","3%"],"answer_description":"","exam_id":4,"answer":"B","choices":{"B":"Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce ג€chaosג€ to the system by terminating random resources on both zones","D":"Capture existing users input, and replay captured user load until resource utilization crosses 80%. Also, derive estimated number of users based on existing user's usage of the app, and deploy enough resources to handle 200% of expected load","C":"Expose the new system to a larger group of users, and increase group size each day until autoscale logic is triggered on all layers. At the same time, terminate random resources on both zones","A":"Capture existing users input, and replay captured user load until autoscale is triggered on all layers. At the same time, terminate all resources in one of the zones"},"unix_timestamp":1571918040,"timestamp":"2019-10-24 13:54:00","question_id":115}],"exam":{"numberOfQuestions":279,"id":4,"lastUpdated":"11 Apr 2025","isBeta":false,"isMCOnly":false,"isImplemented":true,"provider":"Google","name":"Professional Cloud Architect"},"currentPage":23},"__N_SSP":true}