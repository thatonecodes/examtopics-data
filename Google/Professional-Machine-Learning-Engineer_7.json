{"pageProps":{"questions":[{"id":"2eOh51fHlXAEdQImg7EZ","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/91964-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (100%)"],"question_images":["https://img.examtopics.com/professional-machine-learning-engineer/image1.png"],"exam_id":13,"discussion":[{"timestamp":"1687287120.0","upvote_count":"7","content":"Selected Answer: D\nparallel reads, parallel processing, and prefetch is needed here","poster":"pshemol","comment_id":"751461"},{"upvote_count":"3","content":"Selected Answer: D\nOptimizing the data pipeline with parallel reads, processing, and prefetching can significantly improve training speed on TPUs by reducing I/O wait times. This approach utilizes the TPU's capabilities more effectively and avoids extra costs associated with hardware upgrades.","poster":"fitri001","comment_id":"1201700","comments":[{"poster":"fitri001","comments":[{"content":"(C) can be part of the preprocessing step, but it likely won't address the core issue if the bottleneck is related to how data is being fed into the training process.","upvote_count":"1","poster":"fitri001","timestamp":"1729822500.0","comment_id":"1201703"}],"comment_id":"1201701","timestamp":"1729822500.0","upvote_count":"1","content":"A. Moving to a different TPU version (v3) and increasing the batch size might improve training speed, but it's an expensive solution without a guarantee of the most efficient outcome.\nB. Switching to GPUs (V100) also increases costs and may not be optimized for your specific workload."}],"timestamp":"1729822500.0"},{"comment_id":"893012","timestamp":"1699534260.0","content":"Selected Answer: D\nWent with D","poster":"M25","upvote_count":"1"},{"comment_id":"831887","upvote_count":"2","poster":"TNT87","timestamp":"1694083260.0","content":"Selected Answer: D\nBased on the profile, it appears that the Compute time is relatively low compared to the HostToDevice and DeviceToHost time. This suggests that the data transfer between the host (CPU) and the TPU device is a bottleneck. Therefore, the best action to decrease training time in a cost-efficient way would be to reduce the amount of data transferred between the host and the device."},{"content":"Selected Answer: D\nD\n- https://www.tensorflow.org/guide/data_performance","poster":"hiromi","upvote_count":"4","comment_id":"752833","timestamp":"1687380600.0"},{"timestamp":"1687047600.0","poster":"mymy9418","content":"Selected Answer: D\ni didn't see v3 has any benefit than v2\nhttps://cloud.google.com/tpu/docs/system-architecture-tpu-vm#performance_benefits_of_tpu_v3_over_v2","comment_id":"748551","upvote_count":"1"}],"topic":"1","timestamp":"2022-12-18 03:20:00","isMC":true,"unix_timestamp":1671330000,"answer_description":"","answer":"D","question_id":31,"choices":{"D":"Rewrite your input function using parallel reads, parallel processing, and prefetch.","B":"Move from Cloud TPU v2 to 8 NVIDIA V100 GPUs and increase batch size.","C":"Rewrite your input function to resize and reshape the input images.","A":"Move from Cloud TPU v2 to Cloud TPU v3 and increase batch size."},"question_text":"You are training an object detection model using a Cloud TPU v2. Training time is taking longer than expected. Based on this simplified trace obtained with a Cloud TPU profile, what action should you take to decrease training time in a cost-efficient way?\n\n//IMG//","answer_ET":"D"},{"id":"vYtZVBnFX4o2UXD5InBP","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/92402-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["C (100%)"],"question_images":[],"exam_id":13,"discussion":[{"comments":[{"comment_id":"1202405","timestamp":"1729922940.0","upvote_count":"1","content":"if B nominate mode instead of mean?","poster":"pinimichele01"}],"upvote_count":"4","poster":"fitri001","comment_id":"1201707","content":"Selected Answer: C\nMinimizes Bias: Removing rows (A) with missing data can introduce bias if the missingness is not random.expand_more Upsampling the remaining data (A) might not address the underlying cause of missing values.\nUnsuitable for Categorical Features: Replacing with the mean (B) only works for numerical features.\nTransparency and Model Interpretation: A placeholder category (C) explicitly acknowledges the missing data and avoids introducing assumptions during model training. It also improves model interpretability.\nValidation Set Contamination (D): Moving rows with missing values to the validation set (D) contaminates the validation data and hinders its ability to assess model performance on unseen data.\nUsing a placeholder category creates a separate category for missing values, allowing the model to handle them explicitly. This approach is particularly suitable for categorical features with a relatively small percentage of missing values (like 5% in this case).","timestamp":"1729822680.0"},{"content":"Selected Answer: C\nhttp://webcache.googleusercontent.com/search?q=cache:FzNjYfqNEZ0J:https://towardsdatascience.com/missing-values-dont-drop-them-f01b1d8ff557&hl=de&gl=de&strip=1&vwsrc=0\n\nSee also #62, #123","timestamp":"1699535940.0","comment_id":"893041","comments":[{"poster":"M25","upvote_count":"1","comment_id":"893045","timestamp":"1699536240.0","content":"Also, tab \"Forecasting\":\n\"For forecasting models, null values are imputed from the surrounding data. (There is no option to leave a null value as null.) If you would prefer to control the way null values are imputed, you can impute them explicitly. The best values to use might depend on your data and your business problem.\nMissing rows (for example, no row for a specific date, with a data granularity of daily) are allowed, but Vertex AI does not impute values for the missing data. Because missing rows can decrease model quality, you should avoid missing rows where possible. For example, if a row is missing because sales quantity for that day was zero, add a row for that day and explicitly set sales data to 0.\"\nhttps://cloud.google.com/vertex-ai/docs/datasets/data-types-tabular#null-values"}],"upvote_count":"1","poster":"M25"},{"timestamp":"1694083140.0","comment_id":"831886","content":"Selected Answer: C\nC. Replace the missing values with a placeholder category indicating a missing value.\n\nThis approach is often referred to as \"imputing\" missing values, and it is a common technique for dealing with missing data in categorical features. By using a placeholder category, you explicitly indicate that the value is missing, rather than assuming that the missing value is a particular category. This can help to minimize bias in downstream analyses, as it does not introduce any assumptions about the missing data that could bias your results.","upvote_count":"2","poster":"TNT87"},{"upvote_count":"3","comment_id":"820327","timestamp":"1692864420.0","content":"Selected Answer: C\nWhen handling missing values in a categorical feature, replacing the missing values with a placeholder category indicating a missing value, as described in option C, is the most appropriate solution in order to minimize bias that could result from the missing values. This approach allows the algorithm to treat missing values as a separate category, avoiding the risk of any assumptions being made about the missing values.\nOption A, removing the rows with missing values and upsampling the dataset by 5%, can lead to a loss of valuable data and can also introduce bias into the data. This approach can lead to overrepresentation of certain classes and underrepresentation of others.\n\nOption B, replacing the missing values with the feature's mean, is not appropriate for categorical features as there is no meaningful average value for categorical features.\n\nOption D, moving the rows with missing values to the validation dataset, is not a good solution. This approach may introduce bias into the validation dataset and can lead to overfitting.","poster":"shankalman717"},{"upvote_count":"1","comment_id":"819661","poster":"ailiba","timestamp":"1692812880.0","content":"I am not really understanding the concept of C. What information should the model learn from that missing value category?"},{"upvote_count":"2","poster":"jdeix","comments":[{"content":"It is categorical field, you can replace with median or mode not with mean","comment_id":"798674","timestamp":"1691214360.0","upvote_count":"2","poster":"rayban3981"}],"timestamp":"1690265820.0","comment_id":"787393","content":"If you want to minimize the bias, why do not you use mean?"},{"poster":"ares81","timestamp":"1688373300.0","comment_id":"764438","upvote_count":"1","content":"Selected Answer: C\nC, for me."},{"upvote_count":"2","comment_id":"754042","content":"C looks correct. We should replace the values with the a placeholder","poster":"hargur","timestamp":"1687502580.0"},{"timestamp":"1687382400.0","content":"Selected Answer: C\nC (not sure)","comment_id":"752854","poster":"hiromi","upvote_count":"1"}],"topic":"1","timestamp":"2022-12-22 00:20:00","isMC":true,"unix_timestamp":1671664800,"answer_description":"","answer":"C","question_id":32,"choices":{"D":"Move the rows with missing values to your validation dataset.","C":"Replace the missing values with a placeholder category indicating a missing value.","A":"Remove the rows with missing values, and upsample your dataset by 5%.","B":"Replace the missing values with the feature’s mean."},"question_text":"While performing exploratory data analysis on a dataset, you find that an important categorical feature has 5% null values. You want to minimize the bias that could result from the missing values. How should you handle the missing values?","answer_ET":"C"},{"id":"49mgb42S3ljJMpTUlSZv","question_text":"You are an ML engineer on an agricultural research team working on a crop disease detection tool to detect leaf rust spots in images of crops to determine the presence of a disease. These spots, which can vary in shape and size, are correlated to the severity of the disease. You want to develop a solution that predicts the presence and severity of the disease with high accuracy. What should you do?","timestamp":"2022-12-21 11:15:00","question_images":[],"isMC":true,"choices":{"B":"Develop an image segmentation ML model to locate the boundaries of the rust spots.","C":"Develop a template matching algorithm using traditional computer vision libraries.","D":"Develop an image classification ML model to predict the presence of the disease.","A":"Create an object detection model that can localize the rust spots."},"topic":"1","unix_timestamp":1671617700,"question_id":33,"url":"https://www.examtopics.com/discussions/google/view/92320-exam-professional-machine-learning-engineer-topic-1-question/","answer_images":[],"exam_id":13,"answer_ET":"B","answer":"B","answers_community":["B (86%)","14%"],"answer_description":"","discussion":[{"content":"Selected Answer: B\nNot D because Classification can't predict the severity for that we need Segmentation","comment_id":"759582","timestamp":"1687934580.0","poster":"Nayak8","upvote_count":"8"},{"content":"Selected Answer: B\nRust Spot Location and Size: Object detection (A) primarily focuses on identifying and bounding the location of objects.expand_more While it can detect the presence of rust spots, it wouldn't capture the variations in size and shape that correlate with disease severity.\nDetailed Boundaries: Image classification (D) would only predict the presence or absence of the disease based on the entire image. It wouldn't provide details about the location or extent of the rust spots.\nTemplate matching (C) with traditional libraries might be computationally expensive and struggle with the variability in spot shapes and sizes.","upvote_count":"2","poster":"fitri001","timestamp":"1729822980.0","comment_id":"1201710"},{"poster":"julliet","upvote_count":"2","comment_id":"901632","content":"Selected Answer: B\nonly B gets the severity here","timestamp":"1700373960.0"},{"upvote_count":"2","timestamp":"1699537320.0","poster":"M25","comment_id":"893052","content":"Selected Answer: B\nObject Detection [Option A] and Image Segmentation [Option B]:\nhttps://www.oreilly.com/library/view/practical-machine-learning/9781098102357/ch04.html\nImage Recognition [Option D]:\nhttps://www.oreilly.com/library/view/practical-machine-learning/9781098102357/ch03.html#image_vision"},{"upvote_count":"2","comments":[{"content":"Your reasoning is correct, but the headline says: \"crop disease detection tool to detect leaf rust spots in images of crops to determine the presence of a disease\". So I understand that from the output after processing the images is Disease/No Disease. Which I guess could be achieved with Classification.","upvote_count":"2","comments":[{"comment_id":"1162508","content":"\"You want to develop a solution that predicts the presence and severity of the disease with high accuracy.\"\nTherefore, detection only is not the best solution as it is not reliable as to the size of the detected object (rust spot)","poster":"LFavero","timestamp":"1724925840.0","upvote_count":"1"}],"comment_id":"1004420","timestamp":"1710135120.0","poster":"andresvelasco"}],"timestamp":"1694083080.0","content":"Selected Answer: B\nB. Develop an image segmentation ML model to locate the boundaries of the rust spots.\n\nAn image segmentation model is well-suited for this task because it can identify the exact location and shape of the rust spots in the image, which is critical for determining the severity of the disease. Once the rust spots have been identified, other algorithms can be used to analyze the data and predict the severity of the disease. Object detection models are another option, but they may not be as accurate as image segmentation models when it comes to identifying the exact boundaries of the rust spots. Template matching algorithms using traditional computer vision libraries are generally not as accurate as ML models when it comes to image analysis.","poster":"TNT87","comment_id":"831884"},{"poster":"q2ng","timestamp":"1688128140.0","comment_id":"762031","upvote_count":"3","content":"Selected Answer: B\nthe shape of the spot is quite important for the severity of the disease, and image segmentation could help us to determine it in a more granular manner. And it is often used in the healthcare industry, for getting the shapes of all the cancerous cells"},{"timestamp":"1688084220.0","content":"Selected Answer: B\nAnswer B","upvote_count":"2","comment_id":"761614","poster":"Abhijat"},{"upvote_count":"4","content":"Selected Answer: B\nTo determine severity of the disease, boundary of rust spots should be determined - for size/ shape etc.","comment_id":"760076","timestamp":"1687962060.0","poster":"Dataspire"},{"comment_id":"752858","poster":"hiromi","content":"Selected Answer: D\nD should works","upvote_count":"3","timestamp":"1687382700.0"},{"upvote_count":"1","timestamp":"1687335300.0","content":"Selected Answer: D\nI think D","poster":"MithunDesai","comment_id":"752125"}]},{"id":"4PQloeHQq7ayjKsv27HT","unix_timestamp":1671665460,"question_images":[],"answers_community":["B (100%)"],"answer_ET":"B","choices":{"A":"Move the Jupyter notebook to a Notebooks instance on the largest N2 machine type, and schedule the execution of the steps in the Notebooks instance using Cloud Scheduler.","D":"Extract the steps contained in the Jupyter notebook as Python scripts, wrap each script in an Apache Airflow BashOperator, and run the resulting directed acyclic graph (DAG) in Cloud Composer.","B":"Write the code as a TensorFlow Extended (TFX) pipeline orchestrated with Vertex AI Pipelines. Use standard TFX components for data validation and model analysis, and use Vertex AI Pipelines for model retraining.","C":"Rewrite the steps in the Jupyter notebook as an Apache Spark job, and schedule the execution of the job on ephemeral Dataproc clusters using Cloud Scheduler."},"question_id":34,"url":"https://www.examtopics.com/discussions/google/view/92404-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"topic":"1","timestamp":"2022-12-22 00:31:00","question_text":"You have been asked to productionize a proof-of-concept ML model built using Keras. The model was trained in a Jupyter notebook on a data scientist’s local machine. The notebook contains a cell that performs data validation and a cell that performs model analysis. You need to orchestrate the steps contained in the notebook and automate the execution of these steps for weekly retraining. You expect much more training data in the future. You want your solution to take advantage of managed services while minimizing cost. What should you do?","discussion":[{"timestamp":"1731161580.0","comment_id":"893076","upvote_count":"3","content":"Selected Answer: B\nWent with B","poster":"M25"},{"poster":"Antmal","timestamp":"1729200660.0","content":"Selected Answer: B\nI believe it B. Write the code as a TensorFlow Extended (TFX) pipeline orchestrated with Vertex AI Pipelines. Use standard TFX components for data validation and model analysis, and use Vertex AI Pipelines for model retraining. Because :\n \n- Solution A is not scalable and will be expensive to run. It also does not take advantage of managed services.\n\n Solution C is more scalable than option A, but it is still not as scalable as using TFX and Vertex AI Pipelines. It also does not take advantage of managed services.\n\n- Solution D is the most flexible, but it is also the most complex. It requires more knowledge of Apache Airflow and is more difficult to manage.\n\nOverall, the best solution to productionize the proof-of-concept ML model is to use TFX and Vertex AI Pipelines. This solution is scalable, reliable, and easy to manage. It also takes advantage of managed services, which can help to reduce costs.","comment_id":"873114","upvote_count":"4"},{"upvote_count":"3","timestamp":"1725705360.0","content":"Selected Answer: B\nB. Write the code as a TensorFlow Extended (TFX) pipeline orchestrated with Vertex AI Pipelines. Use standard TFX components for data validation and model analysis, and use Vertex AI Pipelines for model retraining.\n\nThe reason for this choice is that TFX and Vertex AI Pipelines provide a scalable and cost-effective solution for productionizing machine learning models. TFX is an end-to-end ML platform for building scalable and repeatable ML workflows, while Vertex AI Pipelines provides a fully managed service for orchestrating ML workflows at scale. By using TFX and Vertex AI Pipelines, you can automate the execution of the steps contained in the Jupyter notebook, and schedule the pipeline for weekly retraining. This approach also takes advantage of managed services, which helps to minimize cost.","poster":"TNT87","comment_id":"831880"},{"timestamp":"1719995580.0","content":"Selected Answer: B\nAll the others look really wrong, so B.","upvote_count":"2","comment_id":"764433","poster":"ares81"},{"upvote_count":"2","content":"Selected Answer: B\nB (not sure)","comment_id":"752866","timestamp":"1719005460.0","poster":"hiromi"}],"answer_images":[],"isMC":true,"answer":"B","answer_description":""},{"id":"6JtFUQK4xUMEUYa6Jum0","answer_images":[],"discussion":[{"upvote_count":"26","comment_id":"382936","timestamp":"1623796500.0","poster":"chohan","content":"Should be C\nhttps://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/"},{"timestamp":"1622918520.0","poster":"inder0007","comment_id":"375391","content":"increasing the size of the network will make the overfitting situation worse","upvote_count":"7"},{"comment_id":"1320986","content":"Selected Answer: C\nC is the best answers. You cannot increase neurons as the model is too complex already and cannot generalize!","poster":"chibuzorrr","upvote_count":"1","timestamp":"1733150760.0"},{"comment_id":"1085222","content":"Selected Answer: C\nVoted C","timestamp":"1701436140.0","upvote_count":"1","poster":"fragkris"},{"upvote_count":"3","timestamp":"1699971120.0","content":"Selected Answer: C\nA,B have very specific numbers which doesn't gurantee success \nC is best \nD - increases the size - which is not helping with overfitting","poster":"Sum_Sum","comment_id":"1070454"},{"poster":"harithacML","comment_id":"946894","content":"Selected Answer: C\nReq: make model resilient \n\nA. Apply a dropout parameter of 0.2, and decrease the learning rate by a factor of 10. : Might / might not work . But may not find optimal parameter set since it uses random values \nB. Apply a L2 regularization parameter of 0.4, and decrease the learning rate by a factor of 10. : Might / might not work . But may not find optimal parameter set since it uses random values \nC. Run a hyperparameter tuning job on AI Platform to optimize for the L2 regularization and dropout parameters. : l2 and dropout are regularisation method which would work. Let AI find the optimal solution on how extend these parameters should regularise. Yes this would work.\nD. Run a hyperparameter tuning job on AI Platform to optimize for the learning rate, and increase the number of neurons by a factor of 2 : AIplatform would do but adding neurons would make network nore complex. So we can eliminate this option.","upvote_count":"3","timestamp":"1688880240.0"},{"comment_id":"902418","content":"Selected Answer: C\nIt should be C as regularization (L1/L2), early stopping and drop out are some of the ways in deep learning to handle overfitting. Other options have specific values which may or may not solve overfitting as it depends on specific use case.","timestamp":"1684568880.0","poster":"ashu381","upvote_count":"1"},{"upvote_count":"2","poster":"M25","content":"Selected Answer: C\nWent with C","comment_id":"892688","timestamp":"1683608220.0"},{"content":"Selected Answer: C\nANS: C\n\nA and B are random values, why they choose that values?\nD could increase even more overfitting since you're using a more complex model.","poster":"wish0035","timestamp":"1671138720.0","upvote_count":"2","comment_id":"746507"},{"upvote_count":"1","comment_id":"725243","content":"Selected Answer: C\nWe don't know the optimum values for the parameters, so we need to run a hyperparameter tuning job; L2 regularization and dropout parameters are great ways to avoid overfitting.\nSo C is the answer","poster":"EFIGO","timestamp":"1669218900.0"},{"comment_id":"647182","content":"Selected Answer: C\nCorrect answer is \"C\"","poster":"GCP72","timestamp":"1660566720.0","upvote_count":"1"},{"poster":"Mohamed_Mossad","timestamp":"1655041740.0","content":"Selected Answer: C\n- by options eliminations C,D are better than A,D (more automated , scalable)\n- between C,D C is better as in D \"and increase the number of neurons by a factor of 2\" will make matters worse and increase overfitting","comments":[{"comment_id":"626677","content":"also in A,D mainly learning rate has no direct relation with overfitting","upvote_count":"1","timestamp":"1656869700.0","poster":"Mohamed_Mossad"}],"upvote_count":"1","comment_id":"615352"},{"poster":"morgan62","timestamp":"1649238300.0","content":"Selected Answer: C\nC for sure","comment_id":"581735","upvote_count":"2"},{"poster":"giaZ","content":"Selected Answer: C\nBest practice is to let a AI Platform tool run the tuning to optimize hyperparameters. Why should I trust values in answers A or B?? Plus L2 regularization and dropout are the way to go here.","upvote_count":"2","timestamp":"1646331480.0","comment_id":"560276"},{"upvote_count":"2","content":"Selected Answer: C\nCommunity vote","comment_id":"557800","timestamp":"1646016180.0","poster":"caohieu04"},{"upvote_count":"3","content":"Selected Answer: C\nit is the logical ans","timestamp":"1642995120.0","comment_id":"531032","poster":"wences"},{"content":"Selected Answer: C\nregularization and dropout","timestamp":"1642586460.0","poster":"stefant","upvote_count":"3","comment_id":"527385"},{"upvote_count":"2","comment_id":"518713","timestamp":"1641522420.0","poster":"NamitSehgal","content":"Increasing Neurons or layers / network will increase overfitting, it is good for under fitting. C should be fine."},{"timestamp":"1641020760.0","comment_id":"514407","upvote_count":"2","content":"Vote for C","poster":"kingback123"},{"content":"decreasing learning rate will help if loss was oscillating and failed to converge. That rules out option A and B.","comment_id":"500083","poster":"ashii007","timestamp":"1639321740.0","upvote_count":"1"},{"comment_id":"464229","poster":"mousseUwU","content":"It's C my friends","timestamp":"1634576940.0","upvote_count":"2"},{"upvote_count":"2","poster":"george_ognyanov","content":"Another vote for answer C\n\nFor me answer A is only partially correct. Although, while dropout layers come to mind when talking about overfitting, decreasing the learning rate by a factor of 10 is \n1) too much of an increase for one single change, having in mind learning rates are between 0.01 and 0.0001 and \n2) we might actually have to increase the learning rate and not decrease it. \n\nSince the learning rate is the speed with which we move down the loss gradient, decreasing it will make this rate slower and force the model to go though more epochs than necessary thus memorizing (overfitting) and not generalizing. \n\nAnother thing that points me to answer C is the fact dropout layers are not the ultimate solution. We might be happy with the predictive power of all our features (thus we dont need to drop neurons and features) and want to minimize the effect of some features with L2 (whilst keeping all the features ).","comment_id":"459630","timestamp":"1633783620.0"},{"timestamp":"1631584080.0","comment_id":"444273","poster":"Y2Data","upvote_count":"1","comments":[{"timestamp":"1633833780.0","content":"How do you know dropout rate =0.2 and learning rate / 10 will work though? You can't know the optimal values in advance so you have to try different values using an hyperparameter tuning job, so C is correct","upvote_count":"6","comment_id":"459847","poster":"ms_lemon"}],"content":"Should be A.\n1. It's a DNN, so there are more than 2 layers. An overfit with more layers usually means many layers of it is not necessary.\n2. Dropout works brilliantly in most case.\n3. L2 regularization is fine, but the parameter of it shouldn't be something to optimize with.\n4. Same goes with the learning rate. The question didn't mention anything about if the model is converging fast or slow, so learning rate shouldn't be considered here.\n\nTo sum up, I choose A since it's the only one I'm sure would work in some extent."}],"answer_ET":"C","unix_timestamp":1622663760,"answers_community":["C (100%)"],"question_text":"You have trained a deep neural network model on Google Cloud. The model has low loss on the training data, but is performing worse on the validation data. You want the model to be resilient to overfitting. Which strategy should you use when retraining the model?","exam_id":13,"answer":"C","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/54300-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"question_id":35,"choices":{"B":"Apply a L2 regularization parameter of 0.4, and decrease the learning rate by a factor of 10.","D":"Run a hyperparameter tuning job on AI Platform to optimize for the learning rate, and increase the number of neurons by a factor of 2.","A":"Apply a dropout parameter of 0.2, and decrease the learning rate by a factor of 10.","C":"Run a hyperparameter tuning job on AI Platform to optimize for the L2 regularization and dropout parameters."},"isMC":true,"topic":"1","timestamp":"2021-06-02 21:56:00"}],"exam":{"isMCOnly":true,"name":"Professional Machine Learning Engineer","isImplemented":true,"numberOfQuestions":304,"id":13,"lastUpdated":"11 Apr 2025","provider":"Google","isBeta":false},"currentPage":7},"__N_SSP":true}