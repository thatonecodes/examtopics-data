{"pageProps":{"questions":[{"id":"GqykAho9gPn2pHObrVDA","exam_id":11,"answer":"A","question_id":81,"timestamp":"2022-09-02 19:44:00","answer_description":"","choices":{"D":"Preprocess all data using Dataflow. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any further transformations on the input data.","A":"When creating your model, use BigQuery's TRANSFORM clause to define preprocessing steps. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.","C":"Use a BigQuery view to define your preprocessing logic. When creating your model, use the view as your model training data. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.","B":"When creating your model, use BigQuery's TRANSFORM clause to define preprocessing steps. Before requesting predictions, use a saved query to transform your raw input data, and then use ML.EVALUATE."},"isMC":true,"unix_timestamp":1662140640,"url":"https://www.examtopics.com/discussions/google/view/79520-exam-professional-data-engineer-topic-1-question-171/","answer_ET":"A","answers_community":["A (90%)","5%"],"question_text":"You work for a large real estate firm and are preparing 6 TB of home sales data to be used for machine learning. You will use SQL to transform the data and use\nBigQuery ML to create a machine learning model. You plan to use the model for predictions against a raw dataset that has not been transformed. How should you set up your workflow in order to prevent skew at prediction time?","answer_images":[],"topic":"1","discussion":[{"upvote_count":"15","poster":"AWSandeep","timestamp":"1662140640.0","content":"Selected Answer: A\nA. When creating your model, use BigQuery's TRANSFORM clause to define preprocessing steps. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.\n\nUsing the TRANSFORM clause, you can specify all preprocessing during model creation. The preprocessing is automatically applied during the prediction and evaluation phases of machine learning.\n\nReference: https://cloud.google.com/bigquery-ml/docs/bigqueryml-transform","comment_id":"657672"},{"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/bigquery-ml/docs/bigqueryml-transform\nUsing the TRANSFORM clause, you can specify all preprocessing during model creation. The preprocessing is automatically applied during the prediction and evaluation phases of machine learning","comment_id":"730504","poster":"zellck","upvote_count":"6","timestamp":"1669732080.0"},{"upvote_count":"1","content":"Selected Answer: C\nC. Use a BigQuery view to define your preprocessing logic. When creating your model, use the view as your model training data. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.\n\nExplanation:\nPreventing Data Skew:\n\nTraining-serving skew occurs when the transformations applied to training data are not identically applied to prediction data. Using a BigQuery view ensures consistent preprocessing for both training and prediction.\nAdvantages of BigQuery Views:\n\nViews encapsulate preprocessing logic, ensuring that the same transformations are applied whenever the view is queried.\nBy referencing the view during both training and prediction, you eliminate the need for manual transformations and the risk of discrepancies.","timestamp":"1735747740.0","comment_id":"1335225","poster":"f74ca0c"},{"timestamp":"1730107020.0","content":"Selected Answer: A\nA","upvote_count":"1","comment_id":"1303899","poster":"SamuelTsch"},{"timestamp":"1720143780.0","content":"Selected Answer: B\nThe key to preventing skew in machine learning models is to ensure that the same data preprocessing steps are applied consistently to both the training data and the prediction data. In option B, the TRANSFORM clause in BigQuery ML is used to define preprocessing steps during model creation, and a saved query is used to apply the same transformations to the raw input data before making predictions. This ensures consistency and prevents skew. The ML.EVALUATE function is then used to evaluate the model’s performance on the transformed prediction data. This is the recommended workflow","poster":"Lenifia","comment_id":"1242398","upvote_count":"2"},{"upvote_count":"1","timestamp":"1705177380.0","comment_id":"1122025","content":"Selected Answer: A\nOption A","poster":"Matt_108"},{"content":"Selected Answer: A\nA is correct answer if we use TRANSFORM clause in BigQuery no need to use any transform while evaluating and predicting https://cloud.google.com/bigquery/docs/bigqueryml-transform","poster":"Prudvi3266","upvote_count":"3","comment_id":"876413","timestamp":"1682074020.0"},{"comment_id":"781911","upvote_count":"2","content":"Selected Answer: A\nA is the correct answer","poster":"Kvk117","timestamp":"1674193860.0"},{"timestamp":"1671188820.0","upvote_count":"3","content":"Selected Answer: A\nProblem: Skew\n\nOne thing that I overlooked when answering previously is that B, C does not address skew. When we preprocess our training data, we need to save our scaled factors somewhere, and when performing predictions on our test data, we need to use the scaling factors of our training data to predict the results.\n\nML.EVALUATE already incorporates preprocessing steps for our test data using the saved scaled factors.","comment_id":"747084","poster":"jkhong"},{"upvote_count":"1","poster":"GCPSharon","content":"Selected Answer: C\nStew prediction time by remove the preprocessing!","timestamp":"1666839720.0","comment_id":"705169"},{"comment_id":"664220","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery-ml/docs/bigqueryml-transform\nAns A","poster":"TNT87","upvote_count":"4","timestamp":"1662698580.0"},{"upvote_count":"2","timestamp":"1662165960.0","comment_id":"657934","content":"Selected Answer: A\nThis query's nested SELECT statement and FROM clause are the same as those in the CREATE MODEL query. Because the TRANSFORM clause is used in training, you don't need to specify the specific columns and transformations. They are automatically restored.\n\n\nReference: https://cloud.google.com/bigquery-ml/docs/bigqueryml-transform","poster":"ducc"}],"question_images":[]},{"id":"s0uJzNpsD9mCnzw4QTzD","answer_images":[],"answer_ET":"D","answer_description":"","topic":"1","answer":"D","exam_id":11,"choices":{"B":"Use a fixed window with a duration of 30 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow().plusDelayOf (Duration.standardSeconds(5))","A":"Use a fixed window with a duration of 5 seconds. Emit results by setting the following trigger: AfterProcessingTime.pastFirstElementInPane().plusDelayOf (Duration.standardSeconds(30))","D":"Use a sliding window with a duration of 30 seconds and a period of 5 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow ()","C":"Use a sliding window with a duration of 5 seconds. Emit results by setting the following trigger: AfterProcessingTime.pastFirstElementInPane().plusDelayOf (Duration.standardSeconds(30))"},"discussion":[{"comment_id":"961974","content":"Selected Answer: D\nOption D: Sliding Window: Since you need to compute a moving average of the past 30 seconds' worth of data every 5 seconds, a sliding window is appropriate. A sliding window allows overlapping intervals and is well-suited for computing rolling aggregates.\n\nWindow Duration: The window duration should be set to 30 seconds to cover the required 30 seconds' worth of data for the moving average calculation.\n\nWindow Period: The window period or sliding interval should be set to 5 seconds to move the window every 5 seconds and recalculate the moving average with the latest data.\n\nTrigger: The trigger should be set to AfterWatermark.pastEndOfWindow() to emit the computed moving average results when the watermark advances past the end of the window. This ensures that all data within the window is considered before emitting the result.","upvote_count":"9","timestamp":"1690226820.0","poster":"vamgcp"},{"comment_id":"657674","upvote_count":"7","content":"Selected Answer: D\nD. Use a sliding window with a duration of 30 seconds and a period of 5 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow ()\nReveal Solution","timestamp":"1662140880.0","poster":"AWSandeep"},{"timestamp":"1718422200.0","poster":"Anudeep58","comment_id":"1230759","content":"Selected Answer: D\nOption D is the correct configuration because it uses a sliding window of 30 seconds with a period of 5 seconds, ensuring that the moving average is computed every 5 seconds based on the past 30 seconds of data. The trigger AfterWatermark.pastEndOfWindow() ensures timely and accurate results are emitted as the watermark progresses.","upvote_count":"1"},{"poster":"Kimich","upvote_count":"2","comment_id":"1085214","content":"AfterWatermark is an essential triggering condition in Dataflow that allows computations to be triggered based on event time rather than processing time. Then eliminate A&C. Comparing B&D, B will generate outcome every 30 seconds which is not what we want\n\nD. Using a sliding window with a duration of 30 seconds and a period of 5 seconds, and setting the trigger as AfterWatermark.pastEndOfWindow(), is a sliding window that generates results every 5 seconds, and each result includes data from the past 30 seconds. In other words, every 5 seconds, you get the average value of the most recent 30 seconds' data, and there is a 5-second overlap between these windows. This is what we want.","timestamp":"1701434940.0"},{"comment_id":"730497","upvote_count":"2","timestamp":"1669731840.0","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#hopping-windows\nYou set the following windows with the Apache Beam SDK or Dataflow SQL streaming extensions:\nHopping windows (called sliding windows in Apache Beam)\n\nA hopping window represents a consistent time interval in the data stream. Hopping windows can overlap, whereas tumbling windows are disjoint.\n\nFor example, a hopping window can start every thirty seconds and capture one minute of data. The frequency with which hopping windows begin is called the period. This example has a one-minute window and thirty-second period.","poster":"zellck"},{"timestamp":"1662628800.0","comment_id":"663421","content":"Selected Answer: D\nMoving average ——> sliding window","poster":"pluiedust","upvote_count":"4"}],"unix_timestamp":1662140880,"question_text":"You are analyzing the price of a company's stock. Every 5 seconds, you need to compute a moving average of the past 30 seconds' worth of data. You are reading data from Pub/Sub and using DataFlow to conduct the analysis. How should you set up your windowed pipeline?","answers_community":["D (100%)"],"question_images":[],"question_id":82,"timestamp":"2022-09-02 19:48:00","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/79521-exam-professional-data-engineer-topic-1-question-172/"},{"id":"DVX30FjZqxxyTl7Otf9L","answer_images":[],"answer_ET":"D","answer_description":"","topic":"1","answer":"D","exam_id":11,"choices":{"B":"Schedule a Cloud Function to run hourly, pulling all available messages from the Pub/Sub topic and performing the necessary aggregations.","C":"Schedule a batch Dataflow job to run hourly, pulling all available messages from the Pub/Sub topic and performing the necessary aggregations.","D":"Create a streaming Dataflow job that reads continually from the Pub/Sub topic and performs the necessary aggregations using tumbling windows.","A":"Create a Cloud Function to perform the necessary data processing that executes using the Pub/Sub trigger every time a new message is published to the topic."},"discussion":[{"poster":"Atnafu","timestamp":"1671223800.0","content":"D\n\nTUMBLE=> fixed windows.\nHOP=> sliding windows. \nSESSION=> session windows.","comment_id":"747583","upvote_count":"11"},{"upvote_count":"7","timestamp":"1677276360.0","comment_id":"820990","content":"why not c ? as data is arriving hourly why we can use batch processing rather than streaming with 1 hour fixed window?","poster":"musumusu","comments":[{"timestamp":"1681467060.0","content":"\"you need to be able to aggregate events across disjoint hourly intervals\" does not means data is arriving hourly. however, it's tricky! Answer D","comment_id":"870112","upvote_count":"2","poster":"MrMone"},{"content":"I second your question. Noone who suggests Dataflow streaming (D) has given an explanation why an hourly batch job is insufficient.","upvote_count":"2","comment_id":"1056759","poster":"ga8our","timestamp":"1698576240.0"},{"timestamp":"1698576120.0","poster":"ga8our","upvote_count":"2","content":"I second your question. Noone who suggests C has given an explanation why an hourly batch job is insufficient.","comment_id":"1056755"}]},{"timestamp":"1727845620.0","upvote_count":"4","comment_id":"1292216","content":"Selected Answer: D\nJust to provide clarity to people asking \"why not C\" - the source is a pub/sub. Pub/Sub has a limit of 10 MB or 1000 messages for a single batch publish request, which means that batch dataflow will not necessarily be able to retrieve all messages. If the question had said \"there will always be less than 1000 messages and less than 10mb\", only then would batch be acceptable.","poster":"baimus"},{"timestamp":"1725511860.0","content":"Selected Answer: D\nThe question asks for future scalability for large volumes of events, its better to go with streaming dataflow job.","comment_id":"1278661","poster":"mayankazyour","upvote_count":"1"},{"comment_id":"1076576","timestamp":"1700594400.0","upvote_count":"2","poster":"emmylou","content":"I just do not understand why this needs to be streamed. I understand that there might be a slight delay using batch processing but there is no indication this is critical data. Can someone please provide your thinking?"},{"poster":"vamgcp","content":"We can use TUMBLE(1 HOUR) to create hourly windows, where each window contains events from a specific hour.","comment_id":"961965","timestamp":"1690226400.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1690226220.0","comment_id":"961961","content":"Selected Answer: D\nOption D : A streaming Dataflow job is the best way to process and load data from Pub/Sub to BigQuery in real time. This is because streaming Dataflow jobs can scale to handle large volumes of data, and they can perform aggregations using tumbling windows.","poster":"vamgcp"},{"timestamp":"1669731360.0","comment_id":"730492","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#tumbling-windows","poster":"zellck","upvote_count":"3"},{"comment_id":"696706","upvote_count":"2","timestamp":"1665967380.0","content":"Selected Answer: D\nAnswer D\nTumbling Windows = Fixed Windows","poster":"devaid"},{"poster":"TNT87","upvote_count":"2","comment_id":"675950","content":"Selected Answer: D\nAnswer D","timestamp":"1663839960.0"},{"timestamp":"1662141060.0","poster":"AWSandeep","content":"Selected Answer: D\nD. Create a streaming Dataflow job that reads continually from the Pub/Sub topic and performs the necessary aggregations using tumbling windows.\n\nA tumbling window represents a consistent, disjoint time interval in the data stream.\n\nReference:\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#tumbling-windows","comment_id":"657679","upvote_count":"2"}],"unix_timestamp":1662141060,"question_text":"You are designing a pipeline that publishes application events to a Pub/Sub topic. Although message ordering is not important, you need to be able to aggregate events across disjoint hourly intervals before loading the results to BigQuery for analysis. What technology should you use to process and load this data to\nBigQuery while ensuring that it will scale with large volumes of events?","answers_community":["D (100%)"],"question_images":[],"question_id":83,"timestamp":"2022-09-02 19:51:00","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/79524-exam-professional-data-engineer-topic-1-question-173/"},{"id":"FlWRsHC3I2J1wyd4lkWe","question_text":"You work for a large financial institution that is planning to use Dialogflow to create a chatbot for the company's mobile app. You have reviewed old chat logs and tagged each conversation for intent based on each customer's stated intention for contacting customer service. About 70% of customer requests are simple requests that are solved within 10 intents. The remaining 30% of inquiries require much longer, more complicated requests. Which intents should you automate first?","question_images":[],"exam_id":11,"choices":{"B":"Automate the more complicated requests first because those require more of the agents' time.","C":"Automate a blend of the shortest and longest intents to be representative of all intents.","A":"Automate the 10 intents that cover 70% of the requests so that live agents can handle more complicated requests.","D":"Automate intents in places where common words such as 'payment' appear only once so the software isn't confused."},"url":"https://www.examtopics.com/discussions/google/view/80144-exam-professional-data-engineer-topic-1-question-174/","answer":"A","discussion":[{"poster":"MaxNRG","upvote_count":"3","timestamp":"1718906820.0","content":"Selected Answer: A\nThis is the best approach because it follows the Pareto principle (80/20 rule). By automating the most common 10 intents that address 70% of customer requests, you free up the live agents to focus their time and effort on the more complex 30% of requests that likely require human insight/judgement. Automating the simpler high-volume requests first allows the chatbot to handle those easily, efficiently routing only the trickier cases to agents. This makes the best use of automation for high-volume simple cases and human expertise for lower-volume complex issues.","comment_id":"1101914"},{"content":"Selected Answer: A\nOption A : : By automating the intents that cover a significant majority (70%) of customer requests, you target the areas with the highest volume of interactions. This helps reduce the load on live agents, enabling them to focus on more complicated and time-consuming inquiries that require their expertise.","comment_id":"961957","poster":"vamgcp","timestamp":"1706130660.0","upvote_count":"1"},{"comment_id":"923425","content":"Selected Answer: A\nA is the answer.","upvote_count":"1","poster":"Takshashila","timestamp":"1702582320.0"},{"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/dialogflow/cx/docs/concept/agent-design#build-iteratively\nIf your agent will be large or complex, start by building a dialog that only addresses the top level requests. Once the basic structure is established, iterate on the conversation paths to ensure you're covering all of the possible routes an end-user may take.","upvote_count":"4","comment_id":"730494","poster":"zellck","timestamp":"1685362740.0"},{"content":"Correct answer: A\n\nAs it states in the documentation: \"If your agent will be large or complex, start by building a dialog that only addresses the top level requests. Once the basic structure is established, iterate on the conversation paths to ensure you're covering all of the possible routes an end-user may take.\" (https://cloud.google.com/dialogflow/cx/docs/concept/agent-design#build-iteratively)\n\nTherefore, you should initally automate the 70 % of the requests that are simpler before automating the more complicated ones.","poster":"SMASL","timestamp":"1679129700.0","comment_id":"672048","upvote_count":"4"},{"poster":"AWSandeep","timestamp":"1677970200.0","comment_id":"659534","content":"Selected Answer: A\nA. Automate the 10 intents that cover 70% of the requests so that live agents can handle more complicated requests.","upvote_count":"1"}],"timestamp":"2022-09-04 22:50:00","answer_description":"","answers_community":["A (100%)"],"isMC":true,"answer_images":[],"topic":"1","unix_timestamp":1662324600,"question_id":84,"answer_ET":"A"},{"id":"tMSPTZvjJNmbk83m8s0r","choices":{"B":"Shard the data by customer ID.","A":"Denormalize the data.","D":"Partition the data by transaction date.","C":"Materialize the dimensional data in views."},"isMC":true,"answer_images":[],"topic":"1","answers_community":["D (88%)","12%"],"answer_description":"","unix_timestamp":1662138600,"answer":"D","discussion":[{"poster":"waiebdi","timestamp":"1691867520.0","comments":[{"content":"\"Agree with you, denormalize usually increases storage, which may lead to an increase in cost. As for speeding up the query without increasing storage costs, another method is to partition the data by transaction date.\"","poster":"Kimich","timestamp":"1717236540.0","comment_id":"1085189","upvote_count":"1"}],"comment_id":"806823","content":"Selected Answer: D\nD is the right answer because it does not increase storage costs.\nA is not correct because denormalization typically increases the amount of storage needed.","upvote_count":"14"},{"timestamp":"1718356680.0","upvote_count":"2","comment_id":"1096396","content":"Bro, you are playing with words now. Gotta read the question fully.","poster":"Aman47"},{"poster":"philv","timestamp":"1711375980.0","comment_id":"1016779","content":"Some might say that Star schema is already denormalized, but it is considered relationnal (hence kind of normalized) from Google's perspective:\n\n\"BigQuery performs best when your data is denormalized. Rather than preserving a relational schema such as a star or snowflake schema, denormalize your data and take advantage of nested and repeated columns. Nested and repeated columns can maintain relationships without the performance impact of preserving a relational (normalized) schema.\"\n\nI would go for A\n\nhttps://cloud.google.com/bigquery/docs/nested-repeated#when_to_use_nested_and_repeated_columns","upvote_count":"1","comments":[{"timestamp":"1713356340.0","content":"Changed my mind to D because of the \"without increasing storage costs\" part.","poster":"philv","upvote_count":"1","comment_id":"1045981"}]},{"poster":"vamgcp","upvote_count":"1","timestamp":"1706130480.0","content":"Selected Answer: D\nOption D - BigQuery supports partitioned tables, where the data is divided into smaller, manageable portions based on a chosen column (e.g., transaction date). By partitioning the data based on the transaction date, BigQuery can efficiently query only the relevant partitions that contain data for the past 30 days, reducing the amount of data that needs to be scanned.Partitioning does not increase storage costs. It organizes existing data in a more structured manner, allowing for better query performance without any additional storage expenses.","comment_id":"961952"},{"poster":"WillemHendr","comment_id":"918037","upvote_count":"2","content":"A is not a bad idea, but this questions is written around \"please partition first on date\", which is common best practice. The \"storage\" remark is hinting on we are not going to 'explode' the data for the sake of performance.","timestamp":"1702032600.0"},{"comment_id":"740186","timestamp":"1686312720.0","upvote_count":"4","content":"Selected Answer: A\nI think better option is [A] considering GCP Documentation: https://cloud.google.com/bigquery/docs/migration/schema-data-overview#denormalization \"BigQuery supports both star and snowflake schemas, but its native schema representation is neither of those two. It uses nested and repeated fields instead for a more natural representation of the data ..... Changing your schema to use nested and repeated fields is an excellent evolutionary choice. It reduces the number of joins required for your queries, and it aligns your schema with the BigQuery internal data representation. Internally, BigQuery organizes data using the Dremel model and stores it in a columnar storage format called Capacitor.\"","poster":"pcadolini"},{"timestamp":"1685362020.0","poster":"zellck","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables\n A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.","comment_id":"730483","upvote_count":"3"},{"upvote_count":"4","content":"Selected Answer: D\nA sneaky question. \n[D] Yes - Since data is queried with date criteria, partitioning by transaction date will surely speed it up without further cost.\n[A] Yes? - Star schema is a denormalized model but as user Reall01 pointed out, the option to use nested and repeated fields can be considered a further denormalization. And if the model hasn't frequently changing dimensions, this kind of denormalization will result in increased performance, according to https://cloud.google.com/bigquery/docs/loading-data#loading_denormalized_nested_and_repeated_data :\n\"In some circumstances, denormalizing your data and using nested and repeated fields doesn't result in increased performance. Avoid denormalization in these use cases:\n - You have a star schema with frequently changing dimensions\"\n\nI guess that the person who added this question, had in mind [D] as a correct answer. If the questioner had all the aforementioned under consideration, would state clearly if there are frequently changing dimensions in the schema.","poster":"NicolasN","comment_id":"712230","timestamp":"1683358860.0"},{"timestamp":"1681824720.0","comment_id":"698266","content":"Selected Answer: D\nStar schema is supported by Big Query but is not the most efficient form, if you should design a schema from scratch google recommend to use nested and repeated fields.\n\nIn this case, you already have done a migration of the schema and data, so it sounds good and with less effort to do partitioning by transaction date than to redesign the schema.\n\nAnd other aspect to consider is that this is a data warehouse, so is sure that there is an ETL process and if you change the schema you must adapt the ETL process.\n\nI vote for D.","upvote_count":"1","poster":"josrojgra"},{"comment_id":"686538","timestamp":"1680664080.0","upvote_count":"2","poster":"devaid","content":"Selected Answer: D\nStar schema is not denormalized itself, but this assumes you already have moved ur data to big query, because you are querying. So, as BQ is not relational, the data already have been denormalized. I go with D."},{"comment_id":"667867","timestamp":"1678710300.0","upvote_count":"1","content":"I think Denormalizing here means ,using big queries native data representation and that is using nested and repeated columns .Thats is the best practice in GCP \nhttps://cloud.google.com/bigquery/docs/nested-repeated#example","poster":"learner2610"},{"content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/migration/schema-data-overview#migrating_data_and_schema_from_on-premises_to_bigquery\n\nStar schema. This is a denormalized model, where a fact table collects metrics such as order amount, discount, and quantity, along with a group of keys. These keys belong to dimension tables such as customer, supplier, region, and so on. Graphically, the model resembles a star, with the fact table in the center surrounded by dimension tables.\n\nStar schema is already denormalized so partition makes more sense going with D","comment_id":"660500","upvote_count":"2","poster":"[Removed]","timestamp":"1678052040.0","comments":[{"upvote_count":"2","comment_id":"673656","comments":[{"comment_id":"932841","timestamp":"1703449980.0","content":"In some circumstances, denormalizing your data and using nested and repeated fields doesn't result in increased performance. For example, star schemas are typically optimized schemas for analytics, and as a result, performance might not be significantly different if you attempt to denormalize further.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-nested","poster":"GabyB","upvote_count":"1"},{"timestamp":"1683357480.0","poster":"NicolasN","comment_id":"712223","upvote_count":"2","content":"You bring up a valid point. According to denormalization best practices, there is a critical info missing in order to decide whether further denormalization with nested and repeated fields could help, if there are frequently changing dimensions. Here's a quote from https://cloud.google.com/bigquery/docs/loading-data#loading_denormalized_nested_and_repeated_data :\n\"In some circumstances, denormalizing your data and using nested and repeated fields doesn't result in increased performance. Avoid denormalization in these use cases:\n- You have a star schema with frequently changing dimensions.\""}],"timestamp":"1679269800.0","content":"If you drill down within that link and land at: https://cloud.google.com/architecture/bigquery-data-warehouse it mentions “ In some cases, you might want to use nested and repeated fields to denormalize your data.” under schema design. Feels like a poorly written question since all depends on what context you take things in as “denormalization”","poster":"Reall01"}]},{"timestamp":"1677787020.0","comment_id":"657685","content":"D. Partition the data by transaction date.\n\nStar schema is already denormalized.","poster":"AWSandeep","upvote_count":"3"},{"content":"Selected Answer: D\nshould be D, not A","upvote_count":"2","comment_id":"657636","poster":"PhuocT","timestamp":"1677784200.0"}],"exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/79498-exam-professional-data-engineer-topic-1-question-175/","answer_ET":"D","question_id":85,"question_text":"Your company is implementing a data warehouse using BigQuery, and you have been tasked with designing the data model. You move your on-premises sales data warehouse with a star data schema to BigQuery but notice performance issues when querying the data of the past 30 days. Based on Google's recommended practices, what should you do to speed up the query without increasing storage costs?","question_images":[],"timestamp":"2022-09-02 19:10:00"}],"exam":{"id":11,"name":"Professional Data Engineer","isMCOnly":true,"isBeta":false,"provider":"Google","lastUpdated":"11 Apr 2025","numberOfQuestions":319,"isImplemented":true},"currentPage":17},"__N_SSP":true}