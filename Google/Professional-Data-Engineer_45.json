{"pageProps":{"questions":[{"id":"PLRMHiMzLbQ7fhLKw8EE","question_images":[],"isMC":true,"discussion":[{"poster":"raaad","comment_id":"1115441","timestamp":"1720292640.0","upvote_count":"10","content":"Selected Answer: D\n- New Key Creation: A new Cloud KMS key ensures a secure replacement for the compromised one.\n- New Bucket: A separate bucket prevents potential conflicts with existing objects and configurations.\n- Default CMEK: Setting the new key as default enforces encryption for all objects in the bucket, reducing the risk of unencrypted data.\n- Copy Without Key Specification: Copying objects without specifying a key leverages the default key, simplifying the process and ensuring consistent encryption.\n- Old Key Deletion: After copying, the compromised key can be safely deleted."},{"poster":"rahulvin","upvote_count":"8","content":"Selected Answer: D\nWrong:\nA - rotating external key doesn't trigger re-encryption of data already in GCS: https://cloud.google.com/kms/docs/rotate-key#rotate-external-coordinated\nC - Setting key during copy doesn't take care of objects that are later uploaded to the bucket, that will still use the default key","comment_id":"1109955","timestamp":"1719771480.0"},{"timestamp":"1743256860.0","content":"Selected Answer: C\nIf no key is specified, and the bucket's default CMEK key is used, there's a risk that some objects might fall back to Google-managed encryption, especially if misconfigured","comment_id":"1411721","poster":"desertlotus1211","upvote_count":"1"},{"comment_id":"1156062","timestamp":"1724289060.0","content":"Selected Answer: D\nOption D","poster":"JyoGCP","upvote_count":"1"},{"upvote_count":"3","comment_id":"1153573","poster":"ML6","content":"Selected Answer: D\nThe correct answer is D. Rotating the key does not seem to re-encrypt:\n\nIn the event that a key is compromised, regular rotation (!!) limits the number of actual messages vulnerable to compromise (!!).\nIf you suspect that a key version is compromised, disable it and revoke access to it as soon as possible.\nSource: https://cloud.google.com/kms/docs/key-rotation#why_rotate_keys","comments":[{"upvote_count":"3","poster":"ML6","content":"Note: When you rotate a key, data encrypted with previous key versions is not automatically re-encrypted with the new key version. You can learn more about re-encrypting data.\nSource: https://cloud.google.com/kms/docs/key-rotation#how_often_to_rotate_keys","timestamp":"1724008380.0","comment_id":"1153575"}],"timestamp":"1724008260.0"},{"timestamp":"1721844120.0","content":"I don't understand why only Matt select A\n\nhttps://cloud.google.com/sdk/gcloud/reference/kms/keys/update\n\nThis seems to do the job, am I wrong ?","poster":"Medmah","comment_id":"1131093","upvote_count":"2"},{"poster":"Matt_108","comment_id":"1121928","content":"Selected Answer: A\nDefinitely A","comments":[{"poster":"ML6","timestamp":"1724008200.0","content":"Rotating does not mean you re-encrypt data.","upvote_count":"1","comment_id":"1153572"}],"upvote_count":"1","timestamp":"1720885380.0"}],"url":"https://www.examtopics.com/discussions/google/view/129911-exam-professional-data-engineer-topic-1-question-298/","exam_id":11,"unix_timestamp":1703967480,"answer":"D","question_id":221,"answers_community":["D (92%)","4%"],"question_text":"One of your encryption keys stored in Cloud Key Management Service (Cloud KMS) was exposed. You need to re- encrypt all of your CMEK-protected Cloud Storage data that used that key, and then delete the compromised key. You also want to reduce the risk of objects getting written without customer-managed encryption key (CMEK) protection in the future. What should you do?","answer_description":"","timestamp":"2023-12-30 21:18:00","answer_images":[],"topic":"1","answer_ET":"D","choices":{"A":"Rotate the Cloud KMS key version. Continue to use the same Cloud Storage bucket.","C":"Create a new Cloud KMS key. Create a new Cloud Storage bucket. Copy all objects from the old bucket to the new one bucket while specifying the new Cloud KMS key in the copy command.","B":"Create a new Cloud KMS key. Set the default CMEK key on the existing Cloud Storage bucket to the new one.","D":"Create a new Cloud KMS key. Create a new Cloud Storage bucket configured to use the new key as the default CMEK key. Copy all objects from the old bucket to the new bucket without specifying a key."}},{"id":"y68jgKBBJ6cztmnLixMQ","choices":{"B":"1. Create a Cloud Storage bucket in the US multi-region.\n2. Run the Dataproc cluster in a zone in the us-central1 region, reading data from the US multi-region bucket.\n3. In case of a regional failure, redeploy the Dataproc cluster to the us-central2 region and continue reading from the same bucket.","A":"1. Create two regional Cloud Storage buckets, one in the us-central1 region and one in the us-south1 region.\n2. Have the upstream process write data to the us-central1 bucket. Use the Storage Transfer Service to copy data hourly from the us-central1 bucket to the us-south1 bucket.\n3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in that region.\n4. In case of regional failure, redeploy your Dataproc clusters to the us-south1 region and read from the bucket in that region instead.","C":"1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.\n2. Enable turbo replication.\n3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the us-south1 region.\n4. In case of a regional failure, redeploy your Dataproc cluster to the us-south1 region and continue reading from the same bucket.","D":"1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.\n2. Enable turbo replication.\n3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the same region.\n4. In case of a regional failure, redeploy the Dataproc clusters to the us-south1 region and read from the same bucket."},"answer_description":"","answer_images":[],"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/google/view/130328-exam-professional-data-engineer-topic-1-question-299/","answer":"D","question_id":222,"isMC":true,"topic":"1","unix_timestamp":1704375240,"exam_id":11,"question_images":[],"discussion":[{"poster":"raaad","comment_id":"1115434","upvote_count":"6","content":"Selected Answer: D\n- Rapid Replication: Turbo replication ensures near-real-time data synchronization between regions, achieving an RPO of 15 minutes or less.\n- Minimal Latency: Dataproc clusters can read from the bucket in the same region, minimizing data transfer latency and optimizing performance.\n- Disaster Recovery: In case of regional failure, Dataproc clusters can seamlessly redeploy to the other region and continue reading from the same bucket, ensuring business continuity.","timestamp":"1720292220.0"},{"upvote_count":"1","content":"Selected Answer: D\nOption D","timestamp":"1724289600.0","comment_id":"1156075","poster":"JyoGCP"},{"comment_id":"1121930","timestamp":"1720885500.0","content":"Selected Answer: D\nOption D, answers all needs from the request","upvote_count":"2","poster":"Matt_108"},{"upvote_count":"3","content":"Selected Answer: D\nD. \n1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.\n2. Enable turbo replication.\n3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the same region.\n4. In case of a regional failure, redeploy the Dataproc clusters to the us-south1 region and read from the same bucket.","poster":"scaenruy","comment_id":"1113705","timestamp":"1720092840.0"}],"answer_ET":"D","question_text":"You have an upstream process that writes data to Cloud Storage. This data is then read by an Apache Spark job that runs on Dataproc. These jobs are run in the us-central1 region, but the data could be stored anywhere in the United States. You need to have a recovery process in place in case of a catastrophic single region failure. You need an approach with a maximum of 15 minutes of data loss (RPO=15 mins). You want to ensure that there is minimal latency when reading the data. What should you do?","timestamp":"2024-01-04 14:34:00"},{"id":"VReDrphhkQzeI8QqrPZr","choices":{"C":"Normalize the master patient-record table into the patient table and the visits table, and create other necessary tables to avoid self-join.","B":"Shard the tables into smaller ones based on date ranges, and only generate reports with prespecified date ranges.","D":"Partition the table into smaller tables, with one for each clinic. Run queries against the smaller table pairs, and use unions for consolidated reports.","A":"Add capacity (memory and disk space) to the database server by the order of 200."},"answer_images":[],"topic":"1","question_id":223,"exam_id":11,"timestamp":"2020-03-15 08:14:00","answer":"C","unix_timestamp":1584256440,"question_text":"You designed a database for patient records as a pilot project to cover a few hundred patients in three clinics. Your design used a single database table to represent all patients and their visits, and you used self-joins to generate reports. The server resource utilization was at 50%. Since then, the scope of the project has expanded. The database must now store 100 times more patient records. You can no longer run the reports, because they either take too long or they encounter errors with insufficient compute resources. How should you adjust the database design?","answer_description":"","discussion":[{"content":"C is correct because this option provides the least amount of inconvenience over using pre-specified date ranges or one table per clinic while also increasing performance due to avoiding self-joins. \nA is not correct because adding additional compute resources is not a recommended way to resolve database schema problems.\nB is not correct because this will reduce the functionality of the database and make running reports more difficult.\nD is not correct because this will likely increase the number of tables so much that it will be more difficult to generate reports vs. the correct option. \nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#explicit-alias-visibility","poster":"MaxNRG","comment_id":"473987","upvote_count":"7","timestamp":"1636306620.0","comments":[{"comment_id":"1332101","content":"Why are we assuming the database in question is BigQuery? There are several other RDBMS options in GCP. ..\nAlso, why was the original db pushed to prod without being normalized first? Typically, the normalized db is released to prod. When the data set becomes larger, you would add partitioning.\nThe scenario being presented is unrealistic.","poster":"gord_nat","timestamp":"1735242660.0","upvote_count":"1"}]},{"upvote_count":"6","content":"A is incorrect because adding space won't solve the problem of query performance.\nB is incorrect because there is nothing related to the report generation which is specified and sharding tables on date ranges is not a good option as it will create many tables.\nC is CORRECT because the statement says \"the scope of the project has expanded. The database must now store 100 times more patient records\". As the data increases there would be difficulty in managing the tables and querying it. Hence creating different table is correct as per the need.\nD is Incorrect as it Partitions on each clinic. We have to adjust the database design so that it performs optimally when generating reports.\nAlso nothing is specified for generation of reports in the required statement.","comment_id":"280556","timestamp":"1612099440.0","poster":"balseron99"},{"content":"Selected Answer: C\nanswer is C, the problem here is the self-join (avoid self-join if possible) on a Denormalized table. So the solution is to Normalize","poster":"Ahamada","timestamp":"1740605880.0","comment_id":"1362312","upvote_count":"1"},{"timestamp":"1736781960.0","poster":"cqrm3n","content":"Selected Answer: C\nNormalizing the database into separate Patients and Visits tables, along with creating other necessary tables, is the best solution for handling the increased data size while ensuring efficient query performance and maintainability. This approach addresses the root problem instead of applying temporary fixes.","comment_id":"1339951","upvote_count":"1"},{"comment_id":"1300757","upvote_count":"1","content":"Selected Answer: C\nC is the most suitable solution for this situation. It provides a better way for scalability and monitoring. B has a constraint on predefined date range, which is usually not suitable for reporting.","timestamp":"1729485660.0","poster":"SamuelTsch"},{"content":"Selected Answer: C\nNormalization is a technique used to organize data in a relational database to reduce data redundancy and improve data integrity. Breaking the patient records into separate tables (patient and visits) and eliminating self-joins will make the database more scalable and improve query performance. It also helps maintain data integrity and makes it easier to manage large datasets efficiently.\n\nOptions A, B, and D may provide some benefits in specific cases, but for a scenario where the project scope has expanded significantly and there are performance issues with self-joins, normalization (Option C) is the most robust and scalable solution.","poster":"rocky48","timestamp":"1698956640.0","upvote_count":"4","comment_id":"1060866"},{"content":"Selected Answer: C\nNormalization is a technique used to organize data in a relational database to reduce data redundancy and improve data integrity. Breaking the patient records into separate tables (patient and visits) and eliminating self-joins will make the database more scalable and improve query performance. It also helps maintain data integrity and makes it easier to manage large datasets efficiently.\n\nOptions A, B, and D may provide some benefits in specific cases, but for a scenario where the project scope has expanded significantly and there are performance issues with self-joins, normalization (Option C) is the most robust and scalable solution.","upvote_count":"3","comment_id":"1050463","timestamp":"1697971800.0","poster":"rtcpost"},{"content":"Selected Answer: C\n\"100 times more patient records\"immediately brings to create a patient dimensional table to save space on disk if a generical relational database is mentioned.","timestamp":"1684501680.0","upvote_count":"1","poster":"vaga1","comment_id":"901955"},{"upvote_count":"1","content":"C - https://cloud.google.com/bigquery/docs/best-practices-performance-patterns","timestamp":"1678819500.0","comment_id":"839145","poster":"maurilio_cardoso_multiedro"},{"comment_id":"835649","upvote_count":"1","timestamp":"1678507980.0","poster":"bha11111","content":"Selected Answer: C\nC- This is correct have verified from different sources"},{"poster":"Morock","comment_id":"810161","upvote_count":"1","timestamp":"1676510760.0","content":"Selected Answer: C\nShould be C. Basic ER design..."},{"upvote_count":"1","content":"c - is the correct one.","timestamp":"1673327580.0","poster":"GCPpro","comment_id":"771070"},{"comment_id":"767528","timestamp":"1673001780.0","upvote_count":"1","poster":"testoneAZ","content":"C should be the correct answer"},{"poster":"Brillianttyagi","timestamp":"1671904380.0","upvote_count":"1","comment_id":"755064","content":"Selected Answer: C\nC- Is the correct answer!"},{"upvote_count":"2","comment_id":"559254","poster":"Arkon88","timestamp":"1646208660.0","content":"Selected Answer: C\nC - based on Google documentation, self-join is an anti-pattern: \nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns"},{"upvote_count":"1","comment_id":"544481","content":"Selected Answer: C\nCorrect","poster":"ch1nczyk","timestamp":"1644492900.0"},{"content":"correct answer -> Normalize the master patient-record table into the patient table and the visits table, and create other necessary tables to avoid self-join.\n\nAvoid self-join at all cost because that's what google says.\n\nReference:\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns","timestamp":"1642792380.0","poster":"samdhimal","upvote_count":"3","comment_id":"529322","comments":[{"comment_id":"784786","content":"Normalizing the database design will help to minimize data redundancy and improve the efficiency of the queries. By separating the patient and visit information into separate tables, the database will be able to handle the increased number of records and generate reports more efficiently, because the self-joins will no longer be required.\n\nOption A is not a good solution because adding more capacity to the server will not address the underlying problem of the database design, and it may not be sufficient to handle the increased data volume.\n\nOption B is not a good solution because it limits the flexibility of the queries and reports, and it may not be sufficient to handle the increased data volume.\n\nOption D is not a good solution because partitioning the table into smaller tables may lead to data redundancy and it may not be sufficient to handle the increased data volume.","poster":"samdhimal","timestamp":"1674430920.0","upvote_count":"3"}]},{"content":"Ans: C","comment_id":"462012","upvote_count":"2","timestamp":"1634214420.0","poster":"anji007"},{"upvote_count":"3","content":"Vote for C. I read this question in 'google 'sample question' of PDE.","timestamp":"1625653320.0","poster":"sumanshu","comment_id":"400757"},{"timestamp":"1616644920.0","upvote_count":"2","poster":"lbhhoya82","comment_id":"319763","content":"Correct: C"},{"timestamp":"1614520080.0","poster":"sid091","content":"C is correct . Self Joins increases the overhead","comment_id":"300805","upvote_count":"1"},{"timestamp":"1612625760.0","poster":"naga","content":"Correct C","comment_id":"284916","upvote_count":"1"},{"timestamp":"1605628440.0","upvote_count":"2","comment_id":"221205","content":"It should be C, to avoid self-join","poster":"luatnc"},{"content":"the answer is B. C is not correct because you are normalizing the data. Normalizing will decrease the read performance. Denormalization is better. However, in this case we are sharding the tables based on date ranges and hence all the sharded tables will have a unique data which will be easier to read and get report from. It will work faster too.","poster":"Radhika7983","timestamp":"1603846080.0","comment_id":"207369","comments":[{"timestamp":"1622027100.0","comment_id":"367054","content":"Self-joins appears to be the culprit, not sure how your solution will avoid the self-joins even if we shard the tables?","upvote_count":"1","poster":"Anirkent"}],"upvote_count":"3"},{"poster":"Athanasia","timestamp":"1603610580.0","comment_id":"205492","content":"Answer is C","upvote_count":"1"},{"poster":"ajay1709","timestamp":"1601536080.0","comment_id":"190794","upvote_count":"1","content":"D\nThe recommended best practice is to use date/timestamp/datetime partitioned tables instead of date-sharded tables","comments":[{"content":"'D' does not look correct here, It says partitioned based on Clinic.(which means still we need to perform self-join to in individual table to get data of Patient and it visits because it's in the same table) Let's suppose Data will grow furthermore for the individual clinic, again performance issue will come. So I belive best option here is Normalize the tables and create different table i.e. one for patient record, other for their visits etc...which avoids the Self-Joins.","poster":"sumanshu","timestamp":"1624390080.0","comment_id":"388237","upvote_count":"1"}]},{"upvote_count":"2","timestamp":"1597853520.0","poster":"atnafu2020","comment_id":"161647","content":"C\nself joins in BigQuery are inefficient — the larger the “smaller” table becomes, the more data needs to be shipped between nodes."},{"timestamp":"1597685040.0","upvote_count":"2","comment_id":"160227","poster":"Ravivarma4786","content":"It should be C - Normalize eliminates data redundant"},{"upvote_count":"3","timestamp":"1597654500.0","content":"Option C: As per Google Documentation, Bigquery will perform bad in case of Self-Joins. So, need to avoid the Self-joins as much as possible.","poster":"PRABHUKKARTHI","comment_id":"159785"},{"poster":"lokeee","comment_id":"149746","content":"Agree C","upvote_count":"1","timestamp":"1596457260.0"}],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/16635-exam-professional-data-engineer-topic-1-question-3/","question_images":[],"answers_community":["C (100%)"],"answer_ET":"C"},{"id":"FQRyl5uuKuKozBBqQIWu","answers_community":["B (57%)","D (42%)","1%"],"answer_ET":"B","answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/16655-exam-professional-data-engineer-topic-1-question-30/","question_images":[],"unix_timestamp":1584273360,"exam_id":11,"timestamp":"2020-03-15 12:56:00","choices":{"A":"Add a node to the MySQL cluster and build an OLAP cube there.","B":"Use an ETL tool to load the data from MySQL into Google BigQuery.","D":"Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc.","C":"Connect an on-premises Apache Hadoop cluster to MySQL and perform ETL."},"question_id":224,"isMC":true,"question_text":"Your company's customer and order databases are often under heavy load. This makes performing analytics against them difficult without harming operations.\nThe databases are in a MySQL cluster, with nightly backups taken using mysqldump. You want to perform analytics with minimal impact on operations. What should you do?","answer":"B","discussion":[{"content":"It is a GOOGLE exam. The answer won't be on-premise or OLAP cubes even if it is the easiest. The answer is B","comment_id":"237801","poster":"HectorLeon2099","timestamp":"1607393640.0","upvote_count":"114","comments":[{"content":"choose dataproc over hadoop cluster\nchose bigquery over all..\n\nthere is no special customer requirement that gonna drive us to hadoop or dataproc.","upvote_count":"10","comment_id":"531431","comments":[{"comments":[{"comment_id":"867116","poster":"ThorstenStaerk","comments":[{"upvote_count":"2","comment_id":"900170","content":"I agree that B sounds like running ETL directly on the database. It doesn't say anything explicitly about using dumps.\n\nHowever, by leveraging the Dataproc JDBC Connector, one can perform various operations such as querying, joining, filtering, and aggregating data from your SQL databases within your Dataproc jobs. This can be particularly useful when you want to combine data from multiple sources or perform complex data transformations before processing the data further.\n\nSo with D, you can run your analysis from a separate cloud-sql instance created from the dump and without affecting the production database.","poster":"cetanx","timestamp":"1684328820.0"}],"content":"So, you are saying that B takes the backup data from the nightly dumps? How can you be sure?","timestamp":"1681203900.0","upvote_count":"2"}],"poster":"cetanx","upvote_count":"2","content":"Answer - B\nmysql dump: This utility creates a logical backup and a flat file containing the SQL statements that can be run again to bring back the database to the state when this file was created. So this file can easily be processed by an ETL tool and loaded into BQ.","timestamp":"1674576840.0","comment_id":"786684"}],"poster":"Tanzu","timestamp":"1643039760.0"},{"upvote_count":"2","comment_id":"1155144","poster":"Preetmehta1234","timestamp":"1708479720.0","content":"That’s so true! This should be the first logic for elimination"}]},{"timestamp":"1585303320.0","content":"Answer: D\nDescription: Easy and it won’t affect processing","comment_id":"68560","comments":[{"comment_id":"120514","poster":"dambilwa","content":"Agreed- Option[D] is most appropriate in this scenario","upvote_count":"6","timestamp":"1593173880.0"},{"upvote_count":"6","poster":"StefanoG","comment_id":"443862","timestamp":"1631524500.0","content":"So I vote for B"},{"upvote_count":"15","comments":[{"content":"but how about the impact on operation? D seems better match.","timestamp":"1611447240.0","poster":"g2000","comment_id":"274923","upvote_count":"2"},{"upvote_count":"4","content":"I also think the answer is D, because on B it is not written that the source is the backup but (directly) MYSQL. So wit this solution we add requests on MySQL and so, mpacting the operations-","poster":"StefanoG","timestamp":"1631105100.0","comment_id":"441460"},{"upvote_count":"1","poster":"hellofrnds","comments":[{"comment_id":"470142","timestamp":"1635581400.0","content":"The link titles \"Genomics analysis with Hail, BIGQUERY, and Data Proc\", the solution describes the use of Bigquery to do analytics","upvote_count":"1","poster":"sergio6"}],"content":"\" Dataproc makes open source data and analytics processing fast, easy, and more secure in the cloud \". Please refer this google link.\nhttps://cloud.google.com/blog/products/data-analytics/genomics-data-analytics-with-cloud-pt2","comment_id":"459632","timestamp":"1633784160.0"},{"comment_id":"392866","poster":"ralf_cc","timestamp":"1624880520.0","content":"ETL in B will drain the MySQL cluster resource, hence impacting the operations.","upvote_count":"7"}],"content":"I think it is B and not D:\n1) There are no info regarding date freshness required for analytics. So nightly backup might be not enough as a source because it will only provide info one tie a day.\n2) Dataproc is recommended as easiest way for migration of hadoop processes. SO no reason to use Dataproc for designing a new analytics processes.\n3) The solution is really very limited if you will extend it in the future and add new data sources or create new aggregate tables. Where they should be created?\n4) There is no info on which version is on prem MySQL database (I am not an expert in MySql) but I can imagine there might be compartibility issue for backup / restore between different releases","timestamp":"1609674300.0","comment_id":"258428","poster":"Alexej_123"},{"upvote_count":"10","poster":"StefanoG","content":"Google Cloud Dataproc is not an analytic tool","comment_id":"443861","timestamp":"1631524440.0"},{"timestamp":"1634220840.0","comment_id":"462115","content":"27 up vote for a wrong ans!!\nWhy do you need dataproc for MySQL dump?!","poster":"[Removed]","upvote_count":"31"}],"upvote_count":"41","poster":"[Removed]"},{"comment_id":"1400184","poster":"willyunger","content":"Selected Answer: D\nOption D has no impact on operations, uses backups which are already there. Option B with ETL could impact MySQL performance.","upvote_count":"1","timestamp":"1742310180.0"},{"timestamp":"1737904380.0","upvote_count":"1","content":"I think it's B, today BigQuery has multiple connectors that can allow an easy connection to external data sources without impacting the database itself, even if the database was in a SQL instance, MySQL, Federated queries could be used. In my opinion it's B","comment_id":"1346968","poster":"Juanesdelacruz97"},{"upvote_count":"1","poster":"Augustax","comment_id":"1340094","content":"Selected Answer: D\nSince the question mentions the nightly backup, why we cannot use it? ETL reduces the impact of the source system but still some impacts. D doesn't add any additional impact.","timestamp":"1736813400.0"},{"poster":"meh_33","timestamp":"1723437780.0","content":"Believe me all questions were from Exam topic all were there yesterday in exam. But yes dont go with starting questions mainly focus questions after 200 and latest questions are at last page.","comments":[{"upvote_count":"1","timestamp":"1723803120.0","content":"How you accessed questions after question number 70 it is asking for pro subscription ?","poster":"Gayatri147","comment_id":"1266958"}],"upvote_count":"1","comment_id":"1264473"},{"timestamp":"1715922480.0","comment_id":"1212705","poster":"mark1223jkh","content":"Answer B:\nI don't know why people are choosing D. It is two steps, first cloudsql and then dataproc, a lot of overhead. BigQuery is just perfect fit.","upvote_count":"1"},{"timestamp":"1709709360.0","upvote_count":"1","poster":"0725f1f","comment_id":"1166975","content":"Selected Answer: D\nThis won’t affect processing"},{"timestamp":"1700733120.0","upvote_count":"2","comment_id":"1078305","poster":"TVH_Data_Engineer","content":"Selected Answer: B\nBased on these considerations, option B is likely the best approach. By using an ETL tool to load data from MySQL into Google BigQuery, you're leveraging BigQuery's strengths in handling large-scale analytics workloads without impacting the performance of the operational databases. This option provides a clear separation of operational and analytical workloads and takes advantage of BigQuery's fast analytics capabilities."},{"poster":"axantroff","timestamp":"1700577420.0","upvote_count":"1","comment_id":"1076376","content":"Selected Answer: B\nDo not spend much time on in - just B"},{"upvote_count":"2","poster":"rocky48","timestamp":"1699297620.0","content":"Selected Answer: B\nAnswer is B - Use an ETL tool to load the data from MySQL into Google BigQuery.\n\n* Google BigQuery is a serverless, highly scalable data warehouse that can handle large-scale analytics workloads without impacting your MySQL cluster's performance.\n* Using an ETL (Extract, Transform, Load) tool to transfer data from MySQL to BigQuery allows you to maintain a separate analytics environment, ensuring that your operational database remains unaffected.\n\nOption C (connecting an on-premises Apache Hadoop cluster to MySQL and performing ETL) introduces complexity and may not be as scalable as a cloud-based solution.\n\nOption D (mounting backups to Google Cloud SQL and processing the data using Google Cloud Dataproc) could be an option for historical data analysis but might not be the best choice for real-time analytics while the MySQL cluster is under heavy load. Additionally, the backups need to be restored and processed, which might introduce some delay.","comment_id":"1064181"},{"content":"It's GOOGLE exam where choosing the GCP service shall be first preference.\nNow notice the problem statement \"perform analytics with minimal impact on operations\"\nBigQuery is right option for analytic as well as Cloud SQL does provide easy export to GCS where we can query from BigQuery without loading into BQ to save storage cost.","comment_id":"1053164","timestamp":"1698181320.0","poster":"mk_choudhary","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: B\nB. Use an ETL tool to load the data from MySQL into Google BigQuery.\n* Google BigQuery is a serverless, highly scalable data warehouse that can handle large-scale analytics workloads without impacting your MySQL cluster's performance.\n* Using an ETL (Extract, Transform, Load) tool to transfer data from MySQL to BigQuery allows you to maintain a separate analytics environment, ensuring that your operational database remains unaffected.\n\nOption C (connecting an on-premises Apache Hadoop cluster to MySQL and performing ETL) introduces complexity and may not be as scalable as a cloud-based solution.\n\nOption D (mounting backups to Google Cloud SQL and processing the data using Google Cloud Dataproc) could be an option for historical data analysis but might not be the best choice for real-time analytics while the MySQL cluster is under heavy load. Additionally, the backups need to be restored and processed, which might introduce some delay.","poster":"rtcpost","comment_id":"1050541","timestamp":"1697977020.0"},{"poster":"melligeri","comment_id":"1049205","content":"Selected Answer: B\nThe question clearly says there is load on MYSQL already so doing analytics on it is bad idea. Its bad to run analytics on MYSQL but still a better option to run etl with it to load it to BigQuery.","timestamp":"1697851980.0","upvote_count":"1"},{"timestamp":"1696666980.0","upvote_count":"2","content":"B. Use an ETL tool to load the data from MySQL into Google BigQuery. This way, analytics is entirely separated from the operational database, and BigQuery is well-suited for large-scale analytics.","poster":"imran79","comment_id":"1027196"},{"upvote_count":"2","timestamp":"1696421700.0","poster":"emmylou","comment_id":"1024730","comments":[{"poster":"Fotofilico","timestamp":"1696946460.0","comment_id":"1039580","content":"thanks! :3","upvote_count":"1"}],"content":"The correct answer is to build a read replica :-) but since we can't do that then migrating to BigQuery will have to suffice."},{"upvote_count":"1","comment_id":"1019088","poster":"Nirca","content":"Selected Answer: D\nAnswer is Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc.\n\nA: OLAP on MySQL performs poorly.\nB: ETL consumes lot of MySQL resources, to read the data, as per question MySQL is under pressure already.\nC: Similar to B.\nD: By mounting backup can avoid reading from MySQL, data freshness is not an issue as per the question (and is not mention in the question).","timestamp":"1695835020.0"},{"upvote_count":"1","content":"Wouldn't the correct answer be to create read replica and do analytics off of that?","comment_id":"1013312","timestamp":"1695317460.0","poster":"emmylou"},{"timestamp":"1693982820.0","content":"Selected Answer: D\nAnswer is Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc. \n\nA: OLAP on MySQL performs poorly.\nB: ETL consumes lot of MySQL resources, to read the data, as per question MySQL is under pressure already.\nC: Similar to B.\nD: By mounting backup can avoid reading from MySQL, data freshness is not an issue as per the question (and is not mention in the question). \n\nReference:\nhttps://cloud.google.com/blog/products/data-analytics/genomics-data-analytics-with-cloud-pt2","upvote_count":"1","comment_id":"1000305","poster":"boraxer1"},{"poster":"FP77","comment_id":"965435","timestamp":"1690535520.0","comments":[{"poster":"FP77","timestamp":"1690536060.0","comment_id":"965439","upvote_count":"1","content":"I meant hadoop cluster"}],"content":"Selected Answer: B\nWhy overcomplicate things by using Dataproc? I choose B","upvote_count":"3"},{"poster":"theseawillclaim","comment_id":"954258","timestamp":"1689600960.0","content":"I think it might be \"C\" because \"B\" mentions a random ETL tool, while C uses a more GCP-specific solution.\nHowever, terrible question.","upvote_count":"1"},{"poster":"Jarek7","timestamp":"1683045480.0","upvote_count":"3","content":"Stupid answer options. I'd mount a backup and use Dataflow to import into BQ.","comment_id":"887693"},{"upvote_count":"2","timestamp":"1681209780.0","poster":"izekc","comment_id":"867204","content":"Selected Answer: D\nD. minimal impact is the key hit"},{"content":"Answer - B\nsqldump > JSON > BigQuery\nhttps://stackoverflow.com/questions/41774233/best-practice-to-migrate-data-from-mysql-to-bigquery","poster":"khinohar0605","timestamp":"1679806440.0","comment_id":"850731","upvote_count":"1"},{"upvote_count":"5","comment_id":"847604","timestamp":"1679528880.0","poster":"luks_skywalker","content":"Between B and D, the others don't make sense. To me, since the question explicitly talks about the nighly backup, this should come up somewhere in the answer. Having that in mind, option B seems to imply the ETL tool will connect directly to the database, which would further increase the load on the MySQL DB. Option D, however, is explicit in saying that we'll be using the backup. True, Dataproc is weird here, but it could just mean that Dataproc will process the backup and stream data to BQ. So, I'll vote D."},{"content":"Selected Answer: B\nVoting B.\nBigQuery is the recommended for Analytics.\nThe ETL won't have a huge impact. After the first load it's only deltas.","comment_id":"842315","poster":"juliobs","upvote_count":"1","timestamp":"1679088840.0"},{"content":"ANS B.\nOption D suggests mounting the backups to Google Cloud SQL and then processing the data using Google Cloud Dataproc. However, this approach may not be the most efficient or cost-effective since the backups are taken nightly and may not reflect the most recent data. Also, mounting backups to Cloud SQL may incur additional costs and may not provide the necessary scalability for performing analytics on large datasets. Instead, using an ETL tool to load the data from MySQL into Google BigQuery provides a more scalable and cost-effective solution for performing analytics with minimal impact on operations.","timestamp":"1678636560.0","upvote_count":"1","comment_id":"837179","poster":"elitedea"},{"content":"Selected Answer: D\nKey hints: current heavy load on MySQL; need minimal impact to MySQL operation; existing nightly backup.\nSince the argument is between B and D.\nB: ETL will impact MySQL operation, and what ETL tool from MYSQL to BQ? it could be Dataproc/Spark.\nD: Using nightly backup has no impact to MySQL operation, Cloud SQL is a perfect managed service to load MySQL backup, Dataproc/spark can perform ETL/Analytics, if not, send data to BQ as the final destination.","timestamp":"1678482420.0","upvote_count":"2","poster":"SuperVee","comment_id":"835435"},{"timestamp":"1675000680.0","comments":[{"comments":[{"timestamp":"1690185780.0","comment_id":"961311","upvote_count":"1","content":"The ETL could happen using the dump, but B says load from the database which implies reading directly from the database.","poster":"Mathew106"}],"timestamp":"1676357520.0","content":"Since they take an overnight dump, the ETL process can happen using the dump. Moreover, google only recommends to use Dataproc when one wants to migrate their existing on-prem Hadoop/Spark workloads with minimal effort.","comment_id":"808156","upvote_count":"1","poster":"niketd"}],"comment_id":"791689","content":"While I do not have a solid answer, I'd like to offer a different way to look at the problem.\nMaybe we do not need to provide the analytical platform in the solution, after all, they did not mention what kind of analytics they used to use previously.\n\nMaybe B and D are just targets, where data will be held.\nWhat am trying to say is, just cause you processed the data via dataprocs, doesn't mean you can't write it to bigquery or any other analytical tool. Hell you can even analyze it with Spark if you want (not the best idea but if you want sure.) \n\nI cannot seem to accept B as the answer, it will definitely violate the \"doesn't affect operations\" criteria. \n\nbut I am not experienced enough to say for sure.","upvote_count":"3","poster":"Lestrang"},{"upvote_count":"2","comments":[{"content":"Noone said the analytics will be run in SQL, but even then, you could do Spark SQL.","upvote_count":"1","poster":"Mathew106","comment_id":"961313","timestamp":"1690185840.0"}],"content":"How could you say D is the answer? Dataproc is used to easily create, manage, and scale Hadoop and Spark clusters in GCP. this is SQL...\n\nAnswer is B","timestamp":"1673556780.0","poster":"desertlotus1211","comment_id":"773901"},{"upvote_count":"2","content":"Selected Answer: D\nMinimal impact on operations is key.","comment_id":"764771","poster":"noonting","timestamp":"1672757580.0"},{"content":"Selected Answer: D\nnone of the option is really good ....\nif need to choose - I vote on D to avoid impact on source system\noption B (as all other options) will add load into mySQL to extract data from db directly - so rejected","timestamp":"1672261080.0","comment_id":"760331","upvote_count":"2","poster":"Jackalski"},{"comment_id":"759453","timestamp":"1672210080.0","upvote_count":"1","content":"Selected Answer: B\nits B...","poster":"PrashantGupta1616"},{"upvote_count":"3","comment_id":"757385","poster":"PrashantGupta1616","timestamp":"1672056900.0","content":"Selected Answer: B\nBigquery is most suitable for analytical purposes and the question is asking about 'minimal impact' on current DB"},{"content":"Selected Answer: D\nD makes more sense to me as in B ETL consumes lot of MySQL resources, to read the data, as per question MySQL is under pressure already,\nIn ques they are also talking about the mysql dump then dump can be used to process data or do analytics as we don't have to increase workload.\n\"They don't give name of ETL tool in B so it can also be Dataproc and in D There is also dataproc and in D we are using google services like Cloud sql and dataproc and we can also use bigquery although it is not mention in the ques but they also not denied that we can not use bigquery in option D.\nSo thing about it it a situation to confuse all of us by giving bigquery in option and analytics in question.","poster":"Brillianttyagi","comment_id":"755889","upvote_count":"2","timestamp":"1671991560.0"},{"timestamp":"1671470460.0","content":"Its B. Close your eyes and choose BigQuery for analytical processing. If operations are not to be impacted, choose either a file extract from the source system or a jdbc connection to run a batch load to GCS/BQ directly in a window where operations are least impacted. Processing delta data in a nightly batch wont take more than 15-20 mins even if the changes dealt with are a million for a given window","upvote_count":"2","poster":"Krish6488","comment_id":"750075"},{"upvote_count":"2","timestamp":"1671021240.0","comment_id":"745062","poster":"Rodolfo_Marcos","content":"Selected Answer: D\nD of course"},{"comment_id":"736888","timestamp":"1670336340.0","poster":"odacir","content":"Selected Answer: B\nThe only option viable is B. Analytics in google World it's BigQuery. The only thing is the heavy load of moving data from on premise to BQ, but that is only once as a big task, after that it's only small chunks and all the analytical power will be out of the DB...","upvote_count":"1"},{"comments":[{"poster":"odacir","timestamp":"1670336160.0","comment_id":"736885","content":"I think the same as you, it's overload the system, then it's better to use backups. But I'm not .","upvote_count":"1"},{"upvote_count":"2","content":"Option D seems logical but i'm not sure","comment_id":"722188","poster":"Jay_Krish","timestamp":"1668885420.0"}],"comment_id":"722186","content":"What about performance issues of choosing option B when you read heavily from the source DB to process data and ingest into BigQuery?\nAs there's a nightly backup already taken every night won't this be an easier solution","poster":"Jay_Krish","timestamp":"1668885240.0","upvote_count":"4"},{"content":"Selected Answer: B\nIts BigQuery because this is Googles data warehouse (storage + query and analysis)","comment_id":"721290","poster":"assU2","timestamp":"1668776400.0","upvote_count":"1"},{"poster":"kennyloo","comment_id":"700311","upvote_count":"1","content":"B - BigQuery is all the while for Analytics purpose","timestamp":"1666301280.0"},{"comment_id":"689242","poster":"maxdataengineer","content":"Selected Answer: B\nDiscarded\nA -> You are not even using cloud services in this solution\nC -> You are not even using cloud services in this solution.\nD -> Dataproc is an ETL tool, not an analytical one.\n\nB -> BEST CHOICE. BigQuery is a warehouse for analytical workloads, and MySQL is for transactional workloads. You take the data from MySQL to BigQuery apply the necessary transformations and make the analysis there.","timestamp":"1665228540.0","upvote_count":"4"},{"timestamp":"1665185880.0","comment_id":"688943","poster":"devaid","content":"Selected Answer: B\nAnswer: B. D don't make sense to me, cloud sql is not designed for analytics, and B mention the usage of an ETL tool to store clean and transformed data (ready for analysis) to a datawarehouse that is Big Query.","upvote_count":"2"},{"upvote_count":"3","comment_id":"686257","timestamp":"1664895060.0","content":"Selected Answer: B\nB : Main task is analystics(Data Analysing) and avoid any operation impact (Transactional operation MySQL on cloud SQL)\nD seems correct but if we consider the sentence carefully\n\"Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc.\" ???? What is destination? if it specifies \"Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc to Big Query\", I suppose it is the best choice.\n\nA abd C rule out\n , Nevertheless we are taking exam DataEngineer and Bigquery is data warehouse for analystics task","poster":"John_Pongthorn"},{"upvote_count":"2","timestamp":"1661233080.0","comments":[{"poster":"ducc","content":"You want to perform analytics with minimal impact on operations","comment_id":"650601","timestamp":"1661233080.0","upvote_count":"1"}],"comment_id":"650600","content":"Selected Answer: B\nI think a lot, B or D, and I go with B\nBecause D generates a lot of operation (management) overhead.","poster":"ducc"},{"comment_id":"648262","poster":"t11","upvote_count":"1","content":"It has to be B. \n\nGoogle's analytic platform is BQ centric, not Dataproc/Spark.","timestamp":"1660792320.0"},{"poster":"changsu","timestamp":"1657817640.0","content":"Selected Answer: D\nB or D, but B has impact on MySQL, so D.","comment_id":"631467","upvote_count":"2"},{"upvote_count":"1","content":"Try to understand the hint. It is mentioned that every night there is a backup, which means you can use this backup for analytics. So D is correct, cause you can load this backup to cloud sql for analytics.","poster":"thapliyal","timestamp":"1656249420.0","comment_id":"622568"},{"timestamp":"1655229000.0","content":"Selected Answer: B\nA is correct, unless you are not a google partner and you want to spend money and time on infra.\nC and D are also correct, if you are a Hadoop master and you still want to be on a local environment for C and for both answers you are just solving the ETL part.\nB is the correct answer since you are performing the ETL and using a specialized analytic tool (BigQuery) for which is the main issue of this question (perform analytics without having an impact on the operations).","upvote_count":"3","comment_id":"616337","poster":"Preemptible_cerebrus"},{"upvote_count":"1","content":"Selected Answer: B\nFor SQL & Analytics Bigquery","poster":"nexus1_","comment_id":"615550","timestamp":"1655079420.0"},{"upvote_count":"1","timestamp":"1654698900.0","poster":"FrankT2L","comment_id":"613336","content":"Selected Answer: C\nperform analytics with minimal impact on operations"},{"comment_id":"599578","upvote_count":"1","poster":"alecuba16","content":"IT can be D if you consider DATAPROC as the result of run a DATA FUSION PIPELINE (it runs over dataproc cluster).\n\nOtherwise it will be B, but B has an impact on the MYSQL load.","timestamp":"1652185020.0"},{"upvote_count":"1","timestamp":"1651143780.0","content":"Selected Answer: B\n....\n...........................................................................................................","comment_id":"593761","poster":"Ritzy7"},{"comment_id":"590498","poster":"msaqib934","content":"A: OLAP on MySQL performs poorly.\nB: ETL consumes lot of MySQL resources, to read the data, as per question MySQL is under pressure already.\nC: Similar to B.\nD: By mounting backup can avoid reading from MySQL, data freshness is not an issue as per the question (and is not mention in the question).","timestamp":"1650704640.0","upvote_count":"4"},{"poster":"Didine_22","timestamp":"1650614460.0","comment_id":"589813","upvote_count":"1","content":"Selected Answer: B\nAnswer is B"},{"comment_id":"588606","timestamp":"1650454860.0","upvote_count":"1","content":"Selected Answer: D\nI'm between B and D.\n\nD Is 0 load for the operating mysql, because you are loading already done backup and processing it using your spark analytics in dataproc. \n\nBut B will have a big impact in production , but will be the best option for future OLAP analysis over the past data. The problem with that option is that it doesn't specify if the ETL is performed daily, hourly , or in low load periods, so the MYSQL server will be overloaded meanwhile the ETL is loading the data to bigquery.","poster":"alecuba16"},{"content":"Selected Answer: D\nMinimal impact is from backup","poster":"CedricLP","timestamp":"1649835900.0","comment_id":"585089","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: B\nThe answer is B","timestamp":"1649360520.0","poster":"bilel_tlily","comment_id":"582589"},{"poster":"GCPCloudArchitectUser","content":"Selected Answer: B\nI am confused why people are considering Dataproc w/ for transformation.. data flow should be the answer","upvote_count":"1","timestamp":"1648983240.0","comment_id":"580220","comments":[{"upvote_count":"1","timestamp":"1652184960.0","poster":"alecuba16","comment_id":"599577","content":"Dataproc is used when you use DATA FUSION (CDAP), it uses dataproc clusters to perform the analytics."}]},{"comment_id":"568393","timestamp":"1647349560.0","upvote_count":"1","content":"Selected Answer: B\nBigquery seems right service here","poster":"OmJanmeda"},{"comment_id":"563553","timestamp":"1646775180.0","upvote_count":"1","poster":"Rakane","content":"The correct answer is B : you need to migrate data from your mysql db on-premise to DWH BQ through an ETL (DATAFLOW, DATAFUSION, DATAPREP) \nD: you will mount the backup on CloudSQL then process Data by Dataproc, but What's about analytics?"},{"comment_id":"560023","content":"Selected Answer: B\nB. Use an ETL tool to load the data from MySQL into Google BigQuery\n\nHere we need to reduce the load of my sql, so etl and analytical workloads need to be done in separate places thus migration to big query is valid, OLAP cube is not valid as it is a lotally separate topic, analysis can be done without it plus on huge amount of data this approach will not work(and big query can scale up). \n\nDataproc is for hadoop clusters migrations so Mysql workloads do not make sence(agree with alexej_123)","timestamp":"1646307600.0","upvote_count":"1","poster":"Arkon88"},{"timestamp":"1645460580.0","comment_id":"553005","upvote_count":"1","content":"Selected Answer: D\nI vote for B because uses dumps and not access to DB\nand because BigG saids that with Dataproc Hub now You can analyze the data\nhttps://cloud.google.com/blog/products/data-analytics/run-data-science-scale-dataproc-and-apache-spark","poster":"BigDataBB"},{"content":"Selected Answer: B\nFor analytics use BigQuery","upvote_count":"1","poster":"fraloca","timestamp":"1644776340.0","comment_id":"546628"},{"content":"Selected Answer: B\nit's obvious!","poster":"ionescuandrei","upvote_count":"2","comment_id":"538592","timestamp":"1643798580.0"},{"upvote_count":"1","content":"So far I haven't learned much but if there's one thing I've learned then that would be \"no matter what the question is in the exam if you are asked what service to use for Data Analytics then the answer is always Big Query.\"\n\n***BigQuery is Google Cloud's fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of data in near real time.***\n\nReference:\nhttps://cloud.google.com/bigquery/docs","timestamp":"1642980240.0","comment_id":"530904","poster":"samdhimal"},{"poster":"exnaniantwort","comment_id":"529783","content":"Selected Answer: B\nSegregate OLTP and OLAP DB to avoid impact of analytic query is the reason why data warehouse exists.\nHence we need a DW (BigQuery). Adding a node to MySQL cluster will still just use up the MySQL resource to process, so not A.\nThe question already mention the MySQL clusters OFTEN experience heavy load, but not just occuring nightly. There is nothing to deal with the backup. Not D.","upvote_count":"1","timestamp":"1642851660.0"},{"timestamp":"1642796280.0","content":"Selected Answer: B\nfor Analytics BQ should be used.","upvote_count":"1","poster":"evesmary","comment_id":"529356"},{"upvote_count":"3","content":"Selected Answer: D\n1. D: There is a mention of a backup that is already taken nightly which can be stored in Google Cloud and then DataProc can be performed using a suited Spark library whenever analysis needs to be performed\n2. Not A: Adding a single node wouldn't be helping much in building an OLAP cube. Even even if the node has high CPU and memory, it is mentioned that a nightly backup is taken and the databases are under heavy load\n3. Not B: Bigquery is an expensive location to store DB, when there is an alternative (Eg: Cloud storage in option D with DataProc). As there is an ETL tool involved in this option too, it is no different from option D, except for the usage of BigQuery instead of Cloud and DataProc (For ETL)\n4. Not C: The load is offline now and there is no mention of any on-premise cluster in question (which makes it completely invalid due to it's high human intervention as opposed to serverless in GCP)","timestamp":"1642107600.0","poster":"sraakesh95","comment_id":"523129"},{"upvote_count":"1","timestamp":"1639902060.0","poster":"kishanu","comment_id":"504721","content":"B seems to be the closest one.\nThough D could be one of them, where Hive queries might run on Cloud Data proc clusters. However, in B we have BIG QUERY which has an upper hand in Analytics when compared with Dataproc-Hive queries."},{"upvote_count":"5","poster":"hendrixlives","timestamp":"1639719540.0","content":"Selected Answer: D\nI think that D (using existing dumps and dataproc) is the only option that does not add extra load to an already busy system. In B it seems MySQL is directly the source for the ETL into BigQuery, and that would have impact on the operations.","comment_id":"503377"},{"timestamp":"1639651020.0","content":"Selected Answer: D\nIf the answer were B, why would the question included the phrase \"nightly backups taken using mysqldump\"?\n\nAlso, we have to find a solution \"with minimal impact on operations\".\nETL on MySQL does impact on operations.\nIf there is not a backup, ETL might be a choice; however, we already have a backup.\nWe need to make use of this backup (at least in this question; otherwise, this question does not make sense).\n\nFurthermore, Dataproc can be used for analytics (see the official document: https://cloud.google.com/dataproc)\n\nIt seems the answer is D.","comment_id":"502827","upvote_count":"2","poster":"ramen_lover"},{"content":"B, BigQuery for analytics","timestamp":"1636829940.0","comment_id":"477693","poster":"MaxNRG","upvote_count":"4"},{"poster":"JayZeeLee","timestamp":"1635879060.0","content":"B. \nYes, D works. But the end product is focused on ANALYTICS, therefore BigQuery is a better fit than D in this case.","comment_id":"471780","upvote_count":"2"},{"poster":"gcp_k","timestamp":"1635034320.0","comment_id":"466780","content":"Not selecting B since they are already generating mysqldump. If we already have backups, why connect ETL to MySQL server AGAIN? This seems to be redundant and harm the Ops of the existing MySQL cluster.\n\nChoosing D - Use already generated backups to do analytics.","upvote_count":"1","comments":[{"poster":"sergio6","timestamp":"1635580560.0","upvote_count":"3","comment_id":"470137","content":"We don't need to do ETL on the MySQL server, we can use mysqldump. So B."}]},{"upvote_count":"1","comment_id":"466023","timestamp":"1634885400.0","poster":"mbkim","content":"If the client's database is on-prem and the back-ups are being done on-prem, and analytics can be defined as anything from simple sql to big data analytics to ml, then an on-prem analytics solution might actually work best for this case."},{"comments":[{"poster":"Chelseajcole","content":"I go with B. It says it is difficult to do the analytic job, what should you do? Bigquery is the answer","comment_id":"465792","upvote_count":"1","timestamp":"1634836620.0"}],"poster":"Chelseajcole","content":"Wouldn't the answer should add one more option which says read replica?","upvote_count":"1","timestamp":"1634086500.0","comment_id":"461317"},{"poster":"anji007","content":"Ans: D\nA: OLAP on MySQL performs poorly.\nB: ETL consumes lot of MySQL resources, to read the data, as per question MySQL is under pressure already.\nC: Similar to B.\nD: By mounting backup can avoid reading from MySQL, data freshness is not an issue as per the question (and is not mention in the question).","upvote_count":"2","timestamp":"1634054220.0","comment_id":"461145"},{"content":"going with B as they r looking for analytics database access ..and we dont want to move to on-premsie ( so no to c)","poster":"Vandy10","upvote_count":"1","timestamp":"1633454340.0","comment_id":"457833"},{"comment_id":"453743","content":"The majority of the debate between B and D is about B will have some impact on the operation. Not sure if the question has been updated from \"No impact\" to \"minimal impact\" already so the answer now is clear: B","timestamp":"1632879720.0","upvote_count":"1","poster":"Chelseajcole"},{"timestamp":"1627949040.0","upvote_count":"3","poster":"Meuter","comment_id":"418983","content":"Answer B. mysqldump can generate CVS files, those fields could be imported directly into BigQuery to do the analytics, even without loading them into Storage. this way we don't impact operations and can do analytics","comments":[{"timestamp":"1631329080.0","comment_id":"442787","poster":"kubosuke","content":"I think so, but B doesn't say that \"use mysqldump\" clearly, so if it really do \"ETL job\" without using dump file, it might be affect performance, so I'll vote for D.","upvote_count":"2"}]},{"upvote_count":"7","poster":"gcp_learner","content":"The correct answer if B because BigQuery is Google’s core Analytics service ie Data Warehouse. The question suggests the on-prem MySQL database can’t support both transactional and analytic workloads. So by the principle of separation of concerns and since this is a Google exam, BigQuery is the obvious choice - ETL data there and perform the analytics from there","comment_id":"395450","timestamp":"1625110740.0"},{"content":"Confused with 'B' and 'D'\nbut 'D' looks okay for this case.\nbecause if we go with 'B' i.e. Load data into BIGQUERY, (for anlaysis it's okay) - But even to Load, (it's not mentioned whether we load from dump or from directlly SQL) - So if we load from SQL directly - it will put immpact on SQL and already databases are under very LOAD. So better not to touch SQL data for anlytics,...go with backup availab;e","upvote_count":"4","poster":"sumanshu","comment_id":"391405","timestamp":"1624725360.0"},{"comments":[{"timestamp":"1625744160.0","poster":"sumanshu","content":"Then D","upvote_count":"1","comment_id":"401858"}],"content":"Answer: B\nmysqldump can be ETL'ed and loaded to Bigquery, it is the right process, not from the live database. of course if load the mysqldump to a Cloud SQL then used as external table for BigQuery also works, but I think dataproc would be overkill (in Selection D)","timestamp":"1624240920.0","upvote_count":"3","poster":"moonlightbeamer","comment_id":"386692"},{"upvote_count":"2","content":"I would choose D. \nI thought it should be B originally, but the question said \"with nightly backups taken using mysqldump.\", which means performing the ETL job would require more operation cost than mounting the backups to CloudSQL.","timestamp":"1620714780.0","comment_id":"354392","poster":"jasper_430"},{"timestamp":"1615302180.0","comments":[{"comment_id":"309543","timestamp":"1615623840.0","content":"If there is no info about something, dont assume your own conditions... The only requirement is to perform analytics without impact on operations. If you build an ETL, you are impacting operations. If you use existing workload (backups) to build your ETL (Dataproc can be used as ETL) you are not impacting operations","poster":"razerlg","upvote_count":"4"}],"comment_id":"306493","poster":"daghayeghi","upvote_count":"2","content":"I think it is B and not D:\n1) There are no info regarding date freshness required for analytics. So nightly backup might be not enough as a source because it will only provide info one tie a day.\n2) Dataproc is recommended as easiest way for migration of hadoop processes. SO no reason to use Dataproc for designing a new analytics processes.\n3) The solution is really very limited if you will extend it in the future and add new data sources or create new aggregate tables. Where they should be created?\n4) There is no info on which version is on prem MySQL database (I am not an expert in MySql) but I can imagine there might be compartibility issue for backup / restore between different releases"},{"poster":"elenamatay","upvote_count":"7","content":"I'm quite confused, the most voted answer is D but the question clearly says \"you want to PERFORM ANALYTICS with minimal impact (...)\". Cloud Dataproc could help us process the data, but that's not doing analysis on it. I think the only option that serves for analysis is B (BigQuery).","timestamp":"1614866520.0","comment_id":"303361"},{"upvote_count":"2","comments":[{"timestamp":"1624724880.0","content":"On-Premises Really? We are moving towards cloud and 'C' says On-premises.","comment_id":"391403","poster":"sumanshu","upvote_count":"3"}],"timestamp":"1612634940.0","comment_id":"285010","content":"Correct C","poster":"naga"},{"content":"It should be D . because in B and C , we are making connection with SQL, which will further degrade performance. Option A will impact operation , if we add new Node. so D looks more appropriate because we already have the backup file available, we can copy them to cloud SQl and then run ETL.","timestamp":"1610090400.0","upvote_count":"3","poster":"apnu","comment_id":"262371"},{"comment_id":"243363","content":"A should be the answer to have Read transactions shifted to replica set which can be updated real time.","timestamp":"1607931600.0","upvote_count":"1","poster":"NamitSehgal"},{"upvote_count":"3","poster":"Radhika7983","content":"B is also right as big query is used for analytics. However, we already have mysqldumps and the same can be used to import in another database in cloud sql. This can be then processed using cloud data proc for analytics. \n\nIf we use option B which is ETL to load into bog query we will have to go with cloud data flow and also extract the data from mysql which will impact the operations. \nI think D is right. If option D was not given, B would have been right.","comment_id":"220407","timestamp":"1605538380.0"},{"comment_id":"209213","poster":"Surjit24","upvote_count":"1","content":"A is the best choice option D will also impact network.","timestamp":"1604056200.0"},{"timestamp":"1602046800.0","comment_id":"194860","upvote_count":"3","poster":"Tanmoyk","content":"I would go with D, reasons are following \n1. Don't want to increase load in the existing DB , so will spin up a Cluster in Cloud SQL with the dump.\n2. Will use dataflow to read data from SQL and then I may put this to BigQuery ( the question didn't mentioned that I want to analysis in the Cloud SQL only , I will use it as Source and Sink the Data in BigQuery or BigTable as required)"},{"content":"Prefer B...\nThough it is not 100% proper because BigQuery is NoSQL, so the \"impact\" might be not that minor. But still it makes the most sense to me in all the options.","poster":"KennneK","comments":[{"upvote_count":"2","content":"BigQuery is NOT NoSQL","poster":"fire558787","timestamp":"1629127020.0","comment_id":"425868"}],"comment_id":"171746","upvote_count":"3","timestamp":"1599024720.0"},{"timestamp":"1597881240.0","upvote_count":"1","poster":"atnafu2020","comment_id":"161868","content":"D\nBackups, recovery, and high availability while creating instances and managed scalablity with only selecting regions."},{"poster":"tprashanth","content":"C.\nGoal is to perform analytics with minimal impact on operations. So introducing Google Cloud(if already not exists) to the firm is a very huge initiative whereas using an existing infrastructure, in this case, Hadoop cluster is the best option. It is best for any kind of analytics, not just OLAP.","upvote_count":"3","comment_id":"133280","timestamp":"1594591980.0"},{"upvote_count":"2","content":"C\nLocal HDFS storage is a good option if you have workloads that involve heavy I/O. For example, you have a lot of partitioned writes. It is a good option if you also have I/O workloads that are especially sensitive to latency","timestamp":"1594123440.0","poster":"saurabhsingh92","comment_id":"128895"},{"poster":"VishalB","content":"A, Add a read-replica (Extra Node) for performing analytics","timestamp":"1593952800.0","comment_id":"126845","upvote_count":"1"},{"comment_id":"126138","content":"D is best answer. Goal is to minimize analytics query load on live OLTP operations. \n\nA. Add a node to the MySQL cluster and build an OLAP cube there. \nThis will not help. Cluster nodes will have identical data.\nB. Use an ETL tool to load the data from MySQL into Google BigQuery.\nThis approach still puts a query load on SQL server.\nC. Connect an on-premises Apache Hadoop cluster to MySQL and perform ETL.\nThis approach also puts a query load on SQL server.\nhttps://www.examtopics.com/exams/google/professional-data-engineer/view/6/#\nD. Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc.\nIn this approach there is read query impact on production SQL instances","upvote_count":"12","poster":"dg63","timestamp":"1593865680.0"},{"comment_id":"115913","poster":"nasirdec","upvote_count":"5","timestamp":"1592783100.0","content":"Should be B"},{"content":"Sure Option C is incorrect.\nOption A 55% ( SQL Dump cannot contain Views, So Spinning a node makes sense)\nOption B 45% ( BiqQuery -> Analytics)","upvote_count":"3","timestamp":"1584813300.0","poster":"Rajokkiyam","comment_id":"66617"},{"upvote_count":"2","comments":[{"comment_id":"66254","content":"D is best option without any impact on operation. Using the nightly dumps.","poster":"[Removed]","timestamp":"1584705780.0","upvote_count":"16"}],"content":"Confused between A/B.","poster":"[Removed]","timestamp":"1584705360.0","comment_id":"66252"},{"comment_id":"64242","upvote_count":"1","poster":"jvg637","content":"feel like 'A' would be cheapest and fastest to implement solution.","timestamp":"1584273360.0"}],"topic":"1"},{"id":"BiPYKSvx7wrmWAmawOp1","discussion":[{"comment_id":"1238272","timestamp":"1719503940.0","upvote_count":"5","content":"Selected Answer: D\nMinimize cost. https://cloud.google.com/alloydb?hl=en\n\nAlloyDB offers superior performance, 4x faster than standard PostgreSQL for transactional workloads. That does not come without cost.","poster":"8ad5266"},{"upvote_count":"1","timestamp":"1739269380.0","poster":"mednoun","comment_id":"1354945","content":"Selected Answer: B\nThe question specifies that the analytical needs need to reside in a single database. This can't be done using Cloud SQL. The database that supports all of that is AlloyDB that's why I will go with the B answer."},{"content":"Selected Answer: B\n\"support analytics needs\" -> columnar storage -> AlloyDB","timestamp":"1738672500.0","comment_id":"1351344","poster":"plum21","upvote_count":"1"},{"poster":"juliorevk","content":"Selected Answer: D\nCloud SQL natively supports PostgreSQL\nAlloyDB for PostgreSQL is a great option if you're specifically looking for high performance in both transactional and analytical workloads. However, it might be more complex and costly than Cloud SQL","timestamp":"1737823620.0","comment_id":"1346547","upvote_count":"1"},{"poster":"joelcaro","content":"Selected Answer: B\nB\nAlloyDB es la mejor opción para modernizar el entorno, mantener compatibilidad con PostgreSQL y manejar tanto cargas transaccionales como analíticas en un único sistema, minimizando costos y complejidad.","upvote_count":"2","timestamp":"1734470640.0","comment_id":"1328161"},{"comment_id":"1294706","upvote_count":"3","timestamp":"1728390120.0","poster":"baimus","content":"Selected Answer: B\nIn real life clearly how performant it needed to be would be a massive factor. AlloyDB is more expensive (see https://cloud.google.com/alloydb/pricing, vs https://cloud.google.com/sql/pricing), but when they say \"minimise cost\" is that per query, or is it per year assuming similar instance size. There's no way for us to know, we have to guess. I'm guessing AlloyDB, as the question seem to be telegraphing that, but it could just as easily be CloudSQL postgres based on the cheaper costs. We simply cannot know."},{"upvote_count":"4","content":"Selected Answer: B\nBecause AlloyDB is optimised for hybrid transactional and analytical processing (HTAP), meaning you can run both transactional workloads and analytics on the same database with excellent performance.","comment_id":"1246868","poster":"Antmal","timestamp":"1720799460.0"},{"content":"Selected Answer: B\nAlloyDB","comment_id":"1246441","poster":"Anudeep58","upvote_count":"2","timestamp":"1720753740.0"},{"comment_id":"1236980","poster":"finixd","content":"Selected Answer: B\nIt's a little complicated, considering it says minimize costs (Cloud SQL) and run transactional workloads and support analytics needs (AlloyDB). I consider B. because you can minimize costs in the long-term instead of doing it immediately with possible extra costs in the long-term. Think about it","timestamp":"1719331920.0","upvote_count":"2"},{"upvote_count":"3","comment_id":"1228116","content":"Selected Answer: D\nAlloyDB is for large scale and more expensive. We want to minimize cost and complexity, so the answer is D.","poster":"extraego","timestamp":"1718054580.0"},{"comments":[{"timestamp":"1716964080.0","upvote_count":"2","content":"Sorry its D. Migrate your PostgreSQL database to Cloud SQL for PostgreSQL.","comment_id":"1220728","poster":"virat_kohli"}],"timestamp":"1716963900.0","poster":"virat_kohli","comment_id":"1220726","content":"Selected Answer: B\nB. Migrate your workloads to AlloyDB for PostgreSQL.","upvote_count":"2"},{"upvote_count":"4","comment_id":"1184577","content":"Selected Answer: D\nThey currently have transactional data stored on-premises in a PostgreSQL database and they want to modernize their database that supports transactional workloads and analytics .If they select cloud Sql (postgreSQL) it will minimize the cost and complexity.\nand for analytics purpose they can create federated queries over cloudSql(postgreSql)\nhttps://cloud.google.com/bigquery/docs/federated-queries-intro\nThis approach will minimze the cost","timestamp":"1711604040.0","poster":"omkarr24"},{"comment_id":"1181058","comments":[{"upvote_count":"2","timestamp":"1713948540.0","comment_id":"1201211","poster":"MissK1371","content":"so answer D?"}],"upvote_count":"1","content":"Selected Answer: B\nB - minimize cost\n\nCloud SQL for PostgreSQL: Generally less expensive than AlloyDB, especially for smaller deployments.\nAlloyDB: Can be significantly more expensive due to its advanced features and high performance capabilities.","timestamp":"1711214940.0","poster":"Izzyt99"},{"poster":"MaxNRG","upvote_count":"4","content":"Selected Answer: D\nminimize cost and complexity","timestamp":"1709469660.0","comment_id":"1164778"},{"content":"Selected Answer: D\nConsidering the cost factor, I'll go with D. \nIf \"minimize cost\" is not present in the question, then I'd go with 'B' AlloyDB. \n\nCloud SQL for PostgreSQL: Generally less expensive than AlloyDB.\nAlloyDB: Can be significantly more expensive due to its advanced features and high performance capabilities.","timestamp":"1708577280.0","poster":"JyoGCP","comment_id":"1156130","upvote_count":"4"},{"content":"Selected Answer: B\nAlloyDB for PostgreSQL is a fully managed, PostgreSQL-compatible database service that's designed for your most demanding workloads, including hybrid transactional and analytical processing.\nref: https://cloud.google.com/alloydb/docs/overview","upvote_count":"4","comment_id":"1145593","poster":"valbru","timestamp":"1707492300.0"},{"content":"Selected Answer: D\nhttps://cloud.google.com/alloydb#all-features\nThe requirement is to minimize cost and complexity. Cloud SQL would be the best choice.","poster":"datapassionate","upvote_count":"4","comment_id":"1134977","timestamp":"1706534580.0"},{"poster":"Vaisnavi","upvote_count":"2","content":"Selected Answer: D\nDatabase Migration Service makes it easier for you to migrate your data to Google Cloud. This service helps you lift and shift your PostgreSQL workloads into Cloud SQL.","timestamp":"1705643820.0","comment_id":"1126464"},{"content":"Selected Answer: B\n- AlloyDB is a fully managed, PostgreSQL-compatible database service with industry-leading performance.","comments":[{"content":"Why not D","poster":"AllenChen123","timestamp":"1705825200.0","upvote_count":"1","comment_id":"1127704"}],"comment_id":"1115431","poster":"raaad","upvote_count":"4","timestamp":"1704574260.0"},{"comment_id":"1113637","timestamp":"1704369060.0","poster":"scaenruy","content":"Selected Answer: B\nB. Migrate your workloads to AlloyDB for PostgreSQL.","upvote_count":"1"}],"answers_community":["B (50%)","D (50%)"],"timestamp":"2024-01-04 12:51:00","url":"https://www.examtopics.com/discussions/google/view/130318-exam-professional-data-engineer-topic-1-question-300/","exam_id":11,"answer":"B","answer_description":"","question_id":225,"question_images":[],"choices":{"C":"Migrate to BigQuery to optimize analytics.","D":"Migrate your PostgreSQL database to Cloud SQL for PostgreSQL.","B":"Migrate your workloads to AlloyDB for PostgreSQL.","A":"Migrate and modernize your database with Cloud Spanner."},"question_text":"You currently have transactional data stored on-premises in a PostgreSQL database. To modernize your data environment, you want to run transactional workloads and support analytics needs with a single database. You need to move to Google Cloud without changing database management systems, and minimize cost and complexity. What should you do?","answer_ET":"B","answer_images":[],"unix_timestamp":1704369060,"topic":"1","isMC":true}],"exam":{"isBeta":false,"id":11,"lastUpdated":"11 Apr 2025","isMCOnly":true,"isImplemented":true,"numberOfQuestions":319,"name":"Professional Data Engineer","provider":"Google"},"currentPage":45},"__N_SSP":true}