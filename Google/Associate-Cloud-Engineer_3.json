{"pageProps":{"questions":[{"id":"bjp5cr9YlUWuTysHahld","question_id":11,"question_images":[],"answer_images":[],"answer_ET":"A","topic":"1","timestamp":"2020-06-06 17:06:00","answer":"A","discussion":[{"content":"If you need something for long-running, non- restartable jobs you dont use preemptible VMs\n\nThink answer is D.","poster":"Polok","comment_id":"103881","upvote_count":"72","timestamp":"1591455960.0"},{"timestamp":"1618046880.0","comments":[{"content":"A is not correct because you can't add a GPU node to an existing GKE cluster \n\nLimitations\nBefore using GPUs on GKE, keep in mind the following limitations:\n\nYou cannot add GPUs to existing node pools.\nGPU nodes cannot be live migrated during maintenance events.\nGPUs are only supported with general-purpose N1 machine types.\nGPUs are not supported in Windows Server node pools\n\nREF: https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#limitations\n\nSo the answer should be D","comments":[{"comment_id":"493269","timestamp":"1638551340.0","upvote_count":"5","comments":[{"comment_id":"495764","poster":"Ridhanya","comments":[{"poster":"wjtb","timestamp":"1653994560.0","upvote_count":"10","content":"https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning\nNode auto-provisioning creates node pools based on the following information:\n\nCPU, memory and ephemeral storage resource requests.\nGPU requests\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations.","comment_id":"609676"}],"upvote_count":"3","timestamp":"1638864420.0","content":"but node pools are homogenous, so how can we be sure that option A will create a GPU node pool"}],"content":"Your reference says existing \"node pools\" not GKE cluster. Auto-provisioning creates new \"node pools\": https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning","poster":"rachee"}],"upvote_count":"12","comment_id":"484236","poster":"kimharsh","timestamp":"1637587500.0"},{"content":"I do agree A is the answer. Since this is for infrequent needs, autoscaling in letter D is not cost effective as it will always run min. of 1 instance. If we need to infrequently use a cluster, the nodes should be able to adjust based on the current need.\n\n\"With node auto-provisioning, new node pools are created and deleted automatically.\" https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning","upvote_count":"7","timestamp":"1634014020.0","comments":[{"upvote_count":"1","poster":"ngeorgiev2","timestamp":"1708598340.0","content":"If the answer was \"Create auto-provisioning node pool\" or demand is not about GPU resources I'll agree with A too, but there is a limitation about existing node pools and GPU, so enabling of auto-provisioning will not create GPU nodes. Need to create separate GPU pool then enable auto-provisioning for it.","comment_id":"1156304"}],"comment_id":"460893","poster":"dttncl"},{"upvote_count":"3","timestamp":"1627119540.0","poster":"kyo","content":"I think using NAP is the correct answer.\n→Node Auto Provisioning (NAP a.k.a., Nodepool Auto Provisioning)\nThere is an introduction of NAP described below on the blog.\n\n>The above recommendations optimize for cost. NAP, for instance, reduces costs by taking down nodes during underutilized periods.\n\nhttps://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster","comment_id":"413072"},{"poster":"JCH760310","upvote_count":"7","timestamp":"1640132640.0","comment_id":"506543","content":"they \"require GPUs\" - so after checking in Udemy practice tests there is similar question there. And the D answer seems to be the best fit for our scenario here.\n\n\"This option is the most optimal solution for the requirement. Rather than recreating all nodes, you create a new node pool with GPU enabled. You then modify the pod specification to target particular GPU types by adding node selector to your workload's Pod specification. You still have a single cluster, so you pay Kubernetes cluster management fee for just one cluster, thus minimizing the cost.\" Still better option than creating new GKE cluster with GPUs.\nRef: https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\nRef: https://cloud.google.com/kubernetes-engine/pricing"}],"content":"Incorrect options are \nB. VerticalPodAutscaler scales PODS based on the app you deploy.\n For handle infrequently GPU access, you need infrequently GPU nodes \n VerticalAutscaler Pod deployed on a non GPU node it useless, \n [We cant have the node always have GPU for infrequent requests]\nC. Preemptible VMs cant last long \nD. For infrequent access, you don't want to have a permanent homogenous cluster.\n\n\nThe correct option is \"A\"\nauto-provisioning = Attaches and deletes node pools to cluster based on the requirements.\nHence creating a GPU node pool, and auto-scaling would be better\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning","upvote_count":"25","poster":"[Removed]","comment_id":"332423"},{"timestamp":"1733656740.0","content":"Selected Answer: D\nno one is talking about non-restartable jobs , it should be D then","comment_id":"1323503","upvote_count":"1","poster":"Moin23"},{"timestamp":"1726797420.0","poster":"RKS_2021","content":"Selected Answer: A\nChanging the answer","upvote_count":"1","comment_id":"1286636"},{"comment_id":"1286635","upvote_count":"2","poster":"RKS_2021","content":"Selected Answer: D\nD is right answer","timestamp":"1726796940.0"},{"poster":"Timfdklfajlksdjlakf","content":"Selected Answer: A\nIt's A. Shit gets only auto-provisioned when your devs actually deploy something that requires a GPU. It doesn't run permanently by default thus saves costs since it only gets provisioned when neededn.","comment_id":"1274620","upvote_count":"1","timestamp":"1724948580.0"},{"poster":"Timfdklfajlksdjlakf","timestamp":"1724486040.0","upvote_count":"1","comment_id":"1271579","content":"Selected Answer: A\nA is correct. If the application requires a GPU then auto-provisioning will provision a vm with a GPU"},{"comment_id":"1218776","poster":"ccpmad","upvote_count":"1","timestamp":"1716706740.0","content":"Selected Answer: D\n\"Enable node auto-provisioning\" with GPU will not works due to limitation \"You cannot add GPUs to existing node pools\""},{"poster":"Sheqos","upvote_count":"1","comment_id":"1176460","timestamp":"1710764580.0","content":"Selected Answer: D\nSelected Answer: D"},{"content":"Selected Answer: D\nYou are not able to add GPUs to existing node pools. This significantly impacts the viability of option A.\n\nMy reasoning for D: A dedicated GPU node pool allows configuring those nodes with specific instance types, disk sizes, etc., ensuring the best fit for the long-running jobs. While it incurs some cost even with a minimum size of 1, it might still be more cost-efficient than full auto-provisioning if the jobs are infrequent but require a predictable baseline capacity. Separating GPU and non-GPU workloads can improve resource scheduling and prevent potential conflicts.","comment_id":"1166662","timestamp":"1709662200.0","poster":"PiperMe","upvote_count":"1"},{"comment_id":"1156255","poster":"ngeorgiev2","content":"In my opinion, more sense has A, but then i read again and again the answer - \"Enable node auto-provisioning\" with GPU will not works due to limitation \"You cannot add GPUs to existing node pools\". If \"A\" was like \"Create GPU node pool with enabled auto-provisioning\" this will be correct answer, in in our case should be D","upvote_count":"2","timestamp":"1708592520.0"},{"timestamp":"1705028220.0","upvote_count":"1","content":"The most cost-effective option for your scenario would be **C. Create a node pool with preemptible VMs and GPUs attached to those VMs**.\n\nPreemptible VMs are Google Cloud's excess compute capacity. They are up to 80% cheaper than regular instances, making them a cost-effective choice for fault-tolerant workloads that do not require continuous availability³. \n\nHowever, please note that preemptible VMs are subject to availability and can be preempted if Google Cloud requires access to those resources, but they will be a good choice if the jobs can tolerate occasional preemptions³.\n\nWhile options A, B, and D could also be used in certain scenarios, they may not provide the same level of cost-effectiveness for long-running, non-restartable jobs that require GPUs⁵. Always consider the nature of your workloads and their tolerance for interruptions when choosing the right solution.","poster":"JB28","comment_id":"1120397"},{"poster":"kaby1987","content":"Selected Answer: D\nThe ans is D,since they require gpu","comment_id":"1111429","upvote_count":"1","timestamp":"1704142560.0"},{"comment_id":"1108362","content":"Selected Answer: D\nThe correct option is \"D\"","timestamp":"1703828040.0","upvote_count":"1","poster":"yash_1199"},{"poster":"ogerber","upvote_count":"2","content":"Selected Answer: A\nIt is A \nNode auto-provisioning creates node pools based on the following information:\n\nCPU, memory, and ephemeral storage resource requests.\nGPU requests.\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning","comment_id":"1088705","timestamp":"1701800160.0"},{"content":"Selected Answer: A\nFor me is A","upvote_count":"1","poster":"kelliot","timestamp":"1701130920.0","comment_id":"1082025"},{"timestamp":"1701088440.0","comments":[{"upvote_count":"1","timestamp":"1703624880.0","poster":"sinceronny","comment_id":"1106384","content":".... any reason?"}],"upvote_count":"1","comment_id":"1081517","content":"Selected Answer: D\nThink answer is D.","poster":"vipinnn00980"},{"comment_id":"1078386","content":"Selected Answer: A\nBest option is A: https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning#how-it-works","poster":"thewalker","timestamp":"1700740260.0","upvote_count":"1"},{"content":"I go with D","timestamp":"1699281360.0","upvote_count":"1","comment_id":"1063930","poster":"BAofBK"},{"comment_id":"1052772","upvote_count":"1","poster":"ezzar","timestamp":"1698147300.0","content":"Selected Answer: A\nBy default, GKE uses the E2 machine series unless any of the following conditions apply:\n\nThe workload requests a feature that is not available in the E2 machine series. For example, if a GPU is requested by the workload, the N1 machine series is used for the new node pool.\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning"},{"comment_id":"1025330","timestamp":"1696487760.0","poster":"ekta25","content":"D. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1.","upvote_count":"1"},{"comment_id":"1023571","upvote_count":"1","poster":"raxt","timestamp":"1696303440.0","content":"Selected Answer: D\nOption A, which seems to be the most voted so far, is incorrect in my opinion.\nFrom Google's docs: \"You cannot add GPUs to existing node pools.\"\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus#limitations\n\nSo, answer A implies that GPUs are already used in the current cluster that needs to be autoscaled."},{"upvote_count":"1","comment_id":"1021891","poster":"gloria216","content":"The correct answer should be A","timestamp":"1696115400.0"},{"content":"Selected Answer: C\nwhy cant be C?","upvote_count":"2","poster":"smilyluv","comment_id":"1004314","timestamp":"1694381640.0"},{"poster":"scanner2","content":"Selected Answer: A\nnode auto-provisioning works in Standard Google Kubernetes Engine (GKE) clusters. Node auto-provisioning automatically manages and scales a set of node pools on the user's behalf. With node auto-provisioning, GKE automatically creates and deletes node pools.\nNode auto-provisioning can be configured by using a YAML configuration file. Single or Multiple settings can be specified in a single config file. Some advanced configurations can only be specified by using a configuration file.\nSets resource limits for CPU, memory and GPU. Node auto-provisioning will not create a node if the total size of the cluster exceeds the specified resource limits.\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning","upvote_count":"1","timestamp":"1694096340.0","comment_id":"1001642"},{"comment_id":"1001491","content":"Selected Answer: D\nThe correct option is \"D\"","poster":"ExamsFR","timestamp":"1694087280.0","upvote_count":"1"},{"timestamp":"1693724700.0","poster":"Captain1212","content":"Selected Answer: A\nA seems more correct","comment_id":"997394","upvote_count":"1"},{"upvote_count":"1","content":"It is A \n\"How node auto-provisioning works\nNode auto-provisioning is a mechanism of the cluster autoscaler, which only scales existing node pools. With node auto-provisioning enabled, the cluster autoscaler can create node pools automatically based on the specifications of unschedulable Pods.\"\nreference: https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning","poster":"Ahmed_Y","comment_id":"985033","timestamp":"1692429600.0"},{"poster":"sthapit","content":"C is the answer.","comment_id":"977170","upvote_count":"1","timestamp":"1691633460.0"},{"comment_id":"952715","content":"I believe the correct option is A. Note auto-provisioning creates node pools based on the following; CPU, memory and ephemeral storage, GPU requests, pending pods ( node affinities and label selectors, node taints and toleration). - https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning","timestamp":"1689458160.0","upvote_count":"1","poster":"chiNelo"},{"comment_id":"894307","content":"Selected Answer: A\nD works for most part except for \"infrequently\". With option D you keep node running all the time which is no necessary. with A, node pool is created and deleted automatically based on workload. Auto-provisioning enable auto scaling as well, which you can specify min number of instance. so A include D, but more than D.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning","timestamp":"1683746040.0","upvote_count":"2","poster":"Jelly_Wang"},{"upvote_count":"1","poster":"vivekvj","timestamp":"1682641740.0","comment_id":"883120","content":"Selected Answer: A\nI thought D but read about Node auto provisioning feature of GKE and got convinced A is correct."},{"timestamp":"1681728900.0","content":"Selected Answer: A\nto minimize the cost","poster":"sabrinakloud","comment_id":"872624","upvote_count":"1"},{"poster":"Nazz1977","comment_id":"797597","upvote_count":"1","timestamp":"1675483560.0","content":"Selected Answer: D\nI think it is D\n\nCreating an autoscaling GPU node pool:\nTo take the best, most cost-effective advantage of GPUs on GKE, and to take advantage of cluster autoscaling, we recommend creating separate GPU node pools in your clusters.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus#gpu_pool"},{"upvote_count":"4","content":"Selected Answer: A\nI'll go with A - https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning?hl=es_419#operation.\n\nYou can request GPUs in autoprovisioning. Besides, creating a node pool of instances with GPUs would mean that the GPUs will be there after the data scientists jobs are done, not very costly efficient to me.","timestamp":"1675039020.0","poster":"jrisl1991","comment_id":"792206"},{"poster":"A84-64","comment_id":"778741","upvote_count":"1","timestamp":"1673947920.0","content":"Selected Answer: D\nD is the right answer. \nA is wrong because the GKE cluster doesn't use the GPU.\nC is wrong as the question stated \"non-restartable\" load."},{"upvote_count":"1","comment_id":"767889","poster":"juss4friendz","content":"Selected Answer: A\nA is correct.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning","timestamp":"1673022480.0"},{"comment_id":"734867","upvote_count":"1","timestamp":"1670133420.0","poster":"ArjunKennedy","content":"This question requires more english skills it seems. It says \"cluster that you manage\". hence this automatically removes B,C and D as options. As the Scientists are already using this solution. And we just need to minimize cost."},{"comment_id":"729643","timestamp":"1669673100.0","content":"minimum size of 1 will ensure long running, non restartable jobs are safe to go on. so i would go with D.","upvote_count":"1","poster":"mvk2022"},{"comment_id":"717122","poster":"Kopy","content":"Selected Answer: D\nCreating an autoscaling GPU node pool\n\nTo take the best, most cost-effective advantage of GPUs on GKE, and to take advantage of cluster autoscaling, we recommend creating separate GPU node pools in your clusters.\n\nD: When you add a GPU node pool to an existing cluster that already runs a non-GPU node pool, GKE automatically taints the GPU nodes with the following node taint:","timestamp":"1668321660.0","upvote_count":"1"},{"timestamp":"1668122460.0","comment_id":"715612","poster":"d_ella2001","content":"Selected Answer: D\nD. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1","upvote_count":"1"},{"upvote_count":"1","poster":"mattcl","content":"it is A) https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning","comment_id":"713427","timestamp":"1667873880.0"},{"poster":"Untamables","upvote_count":"1","content":"Selected Answer: A\nVote A","timestamp":"1666487640.0","comment_id":"701845"},{"comment_id":"697754","upvote_count":"1","content":"I'll go with A.\nNode auto-provisioning creates node pools based on the following information:\n\nCPU, memory and ephemeral storage resource requests.\nGPU requests\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and","poster":"diasporabro","timestamp":"1666050000.0"},{"upvote_count":"1","comment_id":"693798","poster":"anolive","content":"Selected Answer: A\nNode auto-provisioning automatically manages a set of node pools on the user's behalf. Without node auto-provisioning, GKE starts new nodes only from user-created node pools. With node auto-provisioning, new node pools are created and deleted automatically.","timestamp":"1665657540.0"},{"upvote_count":"1","poster":"AwesomeGCP","comment_id":"692729","timestamp":"1665554700.0","content":"D. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1."},{"upvote_count":"1","content":"A is the correct answer,\n\nNode auto-provisioning creates node pools based on the following information:\n\nCPU, memory and ephemeral storage resource requests.\nGPU requests\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations.\n\nGPU requests:\nGPU availability in GKE\nIn GKE Autopilot and Standard, you can attach GPU hardware to nodes in your clusters, and then allocate GPU resources to containerized workloads running on those nodes. You can use these accelerators to perform resource-intensive tasks, such as the following:\n\nMachine learning (ML) inference and training\nLarge-scale data processing","poster":"Charumathi","comment_id":"688898","timestamp":"1665175500.0"},{"poster":"theBestStudent","upvote_count":"1","comment_id":"643530","content":"Selected Answer: A\nNode auto-provisioning creates node pools based on the following information:\n\nCPU, memory and ephemeral storage resource requests.\nGPU requests\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning#operation","timestamp":"1659820140.0"},{"upvote_count":"2","content":"Selected Answer: A\nBased on this, you can request GPU with node auto-aprovisioning. it's A\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning?hl=es_419#operation","comment_id":"637716","poster":"jeffangel28","timestamp":"1658884500.0"},{"comment_id":"626265","content":"Go with D","timestamp":"1656785400.0","upvote_count":"1","poster":"RanjithK"},{"comment_id":"621265","upvote_count":"1","poster":"AzureDP900","content":"D is my answer .. long-running, non- restartable jobs is key to answer this question","timestamp":"1656014520.0"},{"timestamp":"1654704480.0","upvote_count":"1","poster":"Tirthankar17","content":"Selected Answer: D\nD is the correct answer","comment_id":"613369"},{"poster":"haroldbenites","content":"Go for D\nThe key word is “GPU”","timestamp":"1654449840.0","comment_id":"611970","upvote_count":"1"},{"timestamp":"1653511020.0","comment_id":"607380","content":"Selected Answer: A\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning","poster":"peugeotdude","upvote_count":"4"},{"timestamp":"1653463980.0","content":"Selected Answer: D\nCreating an autoscaling GPU node pool\n\nTo take the best, most cost-effective advantage of GPUs on GKE, and to take advantage of cluster autoscaling, we recommend creating separate GPU node pools in your clusters.\n\nWhen you add a GPU node pool to an existing cluster that already runs a non-GPU node pool, GKE automatically taints the GPU nodes with the following node taint:\n\nKey: nvidia.com/gpu\nEffect: NoSchedule\nNote: If a GPU node pool is added to a cluster where all the existing node pools are GPU node pools, or if you are creating a new cluster with a GPU attached default pool, the above taint will not be added to the GPU nodes. The taint will also not be added to the existing GPU nodes retrospectively when a non-GPU node pool is added afterwards.\nAdditionally, GKE automatically applies the corresponding tolerations to Pods requesting GPUs by running the ExtendedResourceToleration admission controller.\n\nThis causes only Pods requesting GPUs to be scheduled on GPU nodes, which enables more efficient autoscaling: your GPU nodes can quickly scale down if there are not enough Pods requesting GPUs.","upvote_count":"3","comments":[{"timestamp":"1673527320.0","content":"But D states \"minimum size of 1\" and we need to minimize costs. Since the scientists only infrequently use the cluster, we are wasting money keeping a GPU node always online","poster":"David_C_90","comment_id":"773444","upvote_count":"1"}],"poster":"LaxmanTiwari","comment_id":"607106"},{"upvote_count":"1","timestamp":"1653162240.0","content":"I would go for D as it is non- restartable jobs which is not perfect with preemtive node","poster":"nhadi82","comment_id":"605008"},{"content":"Selected Answer: D\nRef: https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools","timestamp":"1652569380.0","upvote_count":"1","comment_id":"601821","poster":"pfabio"},{"upvote_count":"1","comment_id":"595130","timestamp":"1651333080.0","poster":"hiranfilho","content":"Selected Answer: D\ncould never be C as preemptible is only for restartable workloads so answer D makes perfect sense"},{"comment_id":"593263","timestamp":"1651073340.0","content":"Selected Answer: A\nThe correct answer is A. After eliminating the obvious wrong choices, you are left with A and D. D does not minimize cost as it keeps an instance running. Moreover, it does not make sense to keep an instance running for something that is \"infrequent\". A makes the most sense.","poster":"rsuresh27","upvote_count":"4"},{"content":"Selected Answer: A\nA- Node auto-provisioning creates node pools based on the following information:\n\nCPU, memory and ephemeral storage resource requests.\nGPU requests","upvote_count":"2","poster":"bobbyMG","comment_id":"583447","timestamp":"1649530200.0"},{"timestamp":"1646942220.0","comment_id":"565011","poster":"nshah68","upvote_count":"3","content":"Here is my take after reading the \"Overview\" & \"Operation\" section on https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning\n\nNode auto-provisioning automatically manages a set of node pools on the user's behalf. Without node auto-provisioning, GKE starts new nodes only from user-created node pools. With node auto-provisioning, new node pools are created and deleted automatically.\n\nNode auto-provisioning creates node pools based on the following information:\nCPU, memory and ephemeral storage resource requests.\nGPU requests\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations.\n\nI thought D at first as well but after some research I believe A is more straight forward and cost effective"},{"content":"Selected Answer: A\nhttps://stackoverflow.com/questions/58939075/auto-provisioning-not-creating-new-node-pool\n\nI'm a cloud newb, but in my understanding auto-provisioning can handle deployments of new node pools based on GPU requests. \nFor \"infrequent access\" this might be the least expensive solution.","timestamp":"1645268280.0","comment_id":"550854","upvote_count":"1","poster":"oracle111"},{"content":"Selected Answer: D\nD is right. Check the official GCP best practise answer here:\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus#gpu_pool","upvote_count":"2","poster":"Raz0r","comment_id":"532979","timestamp":"1643208840.0"},{"timestamp":"1641419280.0","poster":"kped21","comment_id":"517827","content":"D - correct, C says preemptible and non-restartable.","upvote_count":"1"},{"content":"Answer is A. Clearly stated on https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#create\nExtract follows:\n\n\"Creating an autoscaling GPU node pool\nTo take the best, most cost-effective advantage of GPUs on GKE, and to take advantage of cluster autoscaling, we recommend ****creating separate GPU node pools in your clusters****.\"\n\nIt then also explains how the pools are differentiated by the auto-scaler in order to \"taint\" PODs scheduling.","timestamp":"1640124960.0","upvote_count":"3","poster":"mchaconr","comment_id":"506480"},{"poster":"jaffarali","comment_id":"501090","upvote_count":"1","content":"D is the right option. For long running, non restartable jobs preemtible is not correct fit.","timestamp":"1639460640.0"},{"timestamp":"1639391040.0","poster":"wh1t4k3r","content":"I did vote for D at first, but A is making more sense after some reading:\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning\nAuto provisioning will provide nodes with GPU if requested by the workload.","upvote_count":"2","comment_id":"500522"},{"upvote_count":"1","poster":"sharmamohitkr700","timestamp":"1638788940.0","content":"Selected Answer: D\nFor long-running and non-restartable jobs preemptible VMs can be a good choice.","comment_id":"495075"},{"timestamp":"1637416320.0","upvote_count":"2","content":"D looks like the correct one for me.","comment_id":"482575","poster":"FernandoJ"},{"poster":"waqas","content":"Dears.....D is almost correct but the issue is we want to minimize cost which is A.(NAP) hence A is final answer.","comment_id":"468797","timestamp":"1635358260.0","upvote_count":"3"},{"comment_id":"427772","content":"D - A node pool is a group of nodes within a cluster that all have the same configuration. Our requirement is GPUs, so we create a node pool with GPU enabled and have the scientist’s applications deployed to the cluster and use this node pool. At the same time, you want to minimize cost so you start with 1 instance and scale up as needed. It is important to note that the scale down needs to take into consideration if there are any running jobs otherwise the scale down may terminate the nonrestartable job.","poster":"sudav","timestamp":"1629415080.0","upvote_count":"1"},{"poster":"gerhardbl","timestamp":"1629215280.0","comment_id":"426412","content":"A seems correct. C is clearly not ideal because the jobs cannot handle preemption. D is clearly not ideal because 1 instance with GPUs running all the time for an incidental job is too expensive. A is a very brief answer but it is correct that Node Auto Provisioning can add a Node Pool of appropriate GPU-equipped instances if Pods are requested that require GPU nodes; it is the only answer that is not incorrect so to say.","upvote_count":"2"},{"comment_id":"417682","timestamp":"1627701240.0","content":"D is correct \nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus#gpu_pool","poster":"YAS007","upvote_count":"1"},{"timestamp":"1620956220.0","upvote_count":"3","comment_id":"356789","poster":"mcaromit","content":"D is correct"},{"content":"it says \"non-restartable job\", ANS is defintely letter D. by enabling auto scaling with max 1 instances, you can ensure that there is always 1 instance running.","comments":[{"upvote_count":"2","timestamp":"1620572460.0","comment_id":"353132","poster":"lxgywil","content":"Since the usage is infrequent that doesn't like the most optimal solution price-wise."}],"poster":"EJJ","upvote_count":"1","comment_id":"324036","timestamp":"1617085560.0"},{"content":"C\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms","comment_id":"323344","timestamp":"1617014640.0","poster":"khi","upvote_count":"2","comments":[{"comment_id":"323345","content":"D is correct.","poster":"khi","timestamp":"1617014760.0","upvote_count":"1"},{"timestamp":"1621115700.0","comment_id":"358207","poster":"Ashii","upvote_count":"1","content":"non-restartable, has to be D"}]},{"poster":"[Removed]","timestamp":"1616667540.0","comment_id":"320052","content":"D is correct. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1.","upvote_count":"2"},{"upvote_count":"1","poster":"cloud__guru","timestamp":"1615904520.0","comment_id":"312419","content":"Definitely D is the answer. You shouldn't use preemptible VMs for long running jobs that can't be restarted. Thanks a lot examtopics"},{"comment_id":"307595","poster":"Vic1043","upvote_count":"1","timestamp":"1615437180.0","content":"ANS-D\n\nsome long-running, non- restartable jobs\n\nnot possible with preemptible VMs"},{"comment_id":"307474","timestamp":"1615416540.0","content":"D. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1.","upvote_count":"2","poster":"GCP_Student1"},{"content":"D is correct.","upvote_count":"1","comment_id":"306178","poster":"EABDAJA","timestamp":"1615272120.0"},{"content":"Answer is D . Spent more time to look into Option A since someone confused .\nI choose - D . Check how to get a GPUs and set autoscale enabled in this link https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#gpu_pool","timestamp":"1614795600.0","comment_id":"302801","upvote_count":"1","poster":"ShakthiGCP"},{"timestamp":"1614479160.0","comment_id":"300560","upvote_count":"5","comments":[{"upvote_count":"1","comment_id":"300562","timestamp":"1614479460.0","poster":"JackGlemins","comments":[{"comment_id":"300564","timestamp":"1614479580.0","upvote_count":"2","content":"Key word: \"infrequently needs to use\". You dont need create a pool of instance with GPU. D is wrong","poster":"JackGlemins"}],"content":"B- Wrong. Do Nothing about the question\nC- preemptible instances cannot be automatically restarted. They require GPUs for some long-running, non- restartable jobs.\nD- Is right but is more expensive. You want to minimize cost."},{"upvote_count":"1","poster":"JackGlemins","content":"Overview\nNode auto-provisioning automatically manages a set of node pools on the user's behalf. Without node auto-provisioning, GKE considers starting new nodes only from the set of user created node pools. With node auto-provisioning, new node pools can be created and deleted automatically.","comment_id":"300565","timestamp":"1614479700.0"}],"poster":"JackGlemins","content":"A is the correct. https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning\n\nNode auto-provisioning creates node pools based on the following information:\n\nCPU, memory and ephemeral storage resource requests.\nGPU requests\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations."},{"timestamp":"1613549040.0","upvote_count":"1","content":"D is the correct answer. Preemptible vms are restartable during maintaince","comment_id":"292361","poster":"prasadjblin"},{"poster":"victory108","timestamp":"1611479460.0","upvote_count":"1","content":"D - Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1.","comment_id":"275135"},{"timestamp":"1609496280.0","upvote_count":"1","comment_id":"256775","content":"During maintenance events, preemptible instances with GPUs are preempted by default and cannot be automatically restarted. If you want to recreate your instances after they have been preempted, use a managed instance group. Managed instance groups recreate your instances if the vCPU, memory, and GPU resources are available.\nIf you want a warning before your instance is preempted, or want to configure your instance to automatically restart after a maintenance event, use a non-preemptible instance with a GPU. For non-preemptible instances with GPUs, Google provides one hour advance notice before preemption. Hence Answer is C","poster":"vara3dk"},{"timestamp":"1608835680.0","poster":"PhilipAWS","upvote_count":"1","content":"According to info in below urls, answer is C ONLY... Whoever wants to bet, let's have....\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms\n\nSorry, this time GOPINATHM WON :)","comment_id":"251721"},{"timestamp":"1605697080.0","comment_id":"221807","poster":"swatititame","upvote_count":"2","content":"• D. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1."},{"timestamp":"1602742560.0","content":"D. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1.","poster":"glam","upvote_count":"5","comment_id":"200314"},{"upvote_count":"1","comment_id":"192266","timestamp":"1601729640.0","poster":"RockAJ","content":"Ans is D"},{"comment_id":"191040","timestamp":"1601562540.0","poster":"Ravihonnagiri","upvote_count":"1","content":"NON-RESTARTABLE JOBS - We cannot use preemptible ones since they essentially support jobs that can be restarted. Answer would be D"},{"timestamp":"1600847880.0","upvote_count":"1","comment_id":"185108","comments":[{"poster":"mohdafiuddin","content":"The description you have given is correct, but the question says these are not restartable jobs.\nSo Option C is not right.\n\nYou can definitely use preemptible VMs with GKE but with the caveat that the jobs you are running are fault tolerant, etc.","timestamp":"1609177500.0","comment_id":"254233","upvote_count":"2"},{"comment_id":"302807","upvote_count":"2","poster":"ShakthiGCP","timestamp":"1614796020.0","content":"Answer cannot be C. The maximum duration for long running Preempt Node is 24 hours. And they should be able to restart if it is stopped. In this 2 point itself it faile. The question is about long running and non restartable. Hope you got it."}],"content":"You can use preemptible VMs in your GKE clusters or node pools to run batch or fault-tolerant jobs that are less sensitive to the ephemeral, non-guaranteed nature of preemptible VMs.\nyou can use preemitive vms for GKE\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms","poster":"GopinathM"},{"poster":"hiteshrup","upvote_count":"3","content":"Correct Answer is D: \n\nKeywords to look for : GPU, Minimize Cost, Non-Restartable, long-running jobs that fit into 1 node pool. \n\nOption C may have choice but preemptive instance will not best fit for long running non-restartable jobs","comment_id":"177044","timestamp":"1599733860.0"},{"comments":[{"timestamp":"1599489900.0","poster":"Ale1973","comment_id":"175277","upvote_count":"5","content":"Right, C is not correct, but yes, you could use preemtible nodes in a GKE node pool. \nIN THIS SCENARIO (where you need long-running, non- restartable jobs) you couldn't use preemptible VMs in your node pools."}],"content":"preemtible VM is not for GKE, it is for Compute Engine, so C is not correct","poster":"stepkurniawan","timestamp":"1598372580.0","comment_id":"166123","upvote_count":"2"},{"poster":"professor","upvote_count":"4","comment_id":"120522","timestamp":"1593174300.0","content":"Ans is D\n\nKeywords are long-running, non- restartable jobs"},{"timestamp":"1591722660.0","poster":"mlantonis","upvote_count":"2","content":"You cannot use preemptible VMs in this scenario. D is correct.","comment_id":"106150"},{"comment_id":"105269","upvote_count":"4","poster":"dan80","content":"D is correct - C is not good because of VMs - different then GKE","timestamp":"1591624260.0"}],"choices":{"C":"Create a node pool with preemptible VMs and GPUs attached to those VMs.","A":"Enable node auto-provisioning on the GKE cluster.","B":"Create a VerticalPodAutscaler for those workloads.","D":"Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1."},"url":"https://www.examtopics.com/discussions/google/view/22312-exam-associate-cloud-engineer-topic-1-question-108/","isMC":true,"unix_timestamp":1591455960,"answer_description":"","answers_community":["A (57%)","D (39%)","3%"],"exam_id":1,"question_text":"A team of data scientists infrequently needs to use a Google Kubernetes Engine (GKE) cluster that you manage. They require GPUs for some long-running, non- restartable jobs. You want to minimize cost. What should you do?"},{"id":"Ebh5PGd95gdhrWO2WUH3","url":"https://www.examtopics.com/discussions/google/view/24144-exam-associate-cloud-engineer-topic-1-question-109/","question_id":12,"choices":{"D":"Ask each employee to create a Google account using self signup. Require that each employee use their company email address and password.","A":"Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.","C":"Export users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console.","B":"Use the cloud Identity APIs and write a script to synchronize users to Cloud Identity."},"timestamp":"2020-06-26 14:07:00","isMC":true,"topic":"1","answers_community":["A (100%)"],"answer":"A","answer_images":[],"question_text":"Your organization has user identities in Active Directory. Your organization wants to use Active Directory as their source of truth for identities. Your organization wants to have full control over the Google accounts used by employees for all Google services, including your Google Cloud Platform (GCP) organization. What should you do?","question_images":[],"exam_id":1,"discussion":[{"timestamp":"1624709220.0","poster":"professor","upvote_count":"30","content":"Ans is A\n\nhttps://tools.google.com/dlpage/dirsync/","comment_id":"120506"},{"comment_id":"162415","timestamp":"1629483000.0","poster":"ESP_SAP","upvote_count":"22","content":"Correct Answer (A):\n\nDirectory Sync\nGoogle Cloud Directory Sync enables administrators to synchronize users, groups and other data from an Active Directory/LDAP service to their Google Cloud domain directory\n\nhttps://tools.google.com/dlpage/dirsync/"},{"timestamp":"1730904300.0","upvote_count":"1","comment_id":"1063945","poster":"BAofBK","content":"The correct answer is A"},{"timestamp":"1725719100.0","content":"Selected Answer: A\nhttps://support.google.com/a/answer/106368?hl=en\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-synchronizing-user-accounts\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction","poster":"scanner2","comment_id":"1001647","upvote_count":"2"},{"content":"Selected Answer: A\nA is the correct answer as it help you to sybchronize users","timestamp":"1725347400.0","poster":"Captain1212","comment_id":"997400","upvote_count":"1"},{"poster":"Neha_Pallavi","timestamp":"1724565600.0","upvote_count":"1","content":"A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.","comment_id":"989702"},{"poster":"Nazz1977","upvote_count":"1","content":"Selected Answer: A\nA....is right","comment_id":"734506","timestamp":"1701614340.0"},{"content":"A is right, this is part of Tutorials Dojo practice test","poster":"AzureDP900","timestamp":"1687550640.0","upvote_count":"2","comment_id":"621267"},{"content":"Go for A","timestamp":"1685986320.0","upvote_count":"1","poster":"haroldbenites","comment_id":"611976"},{"comment_id":"567590","content":"Selected Answer: A\nA is correct","poster":"crisyeb","upvote_count":"1","timestamp":"1678791240.0"},{"content":"Selected Answer: A\nA is right","poster":"LeonNip","upvote_count":"3","timestamp":"1673160720.0","comment_id":"519354"},{"poster":"alaahakim","content":"Selected Answer: A\nA is Correct","upvote_count":"4","comment_id":"482078","timestamp":"1668889860.0"},{"comment_id":"356791","content":"A is correct","timestamp":"1652492280.0","poster":"mcaromit","upvote_count":"3"},{"comment_id":"320054","upvote_count":"1","timestamp":"1648203540.0","poster":"[Removed]","content":"A is correct. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity."},{"comment_id":"298599","upvote_count":"3","content":"• A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.","poster":"GCP_Student1","timestamp":"1645741980.0"},{"comment_id":"226801","upvote_count":"1","poster":"devscorpio2001","timestamp":"1637771880.0","content":"This is A , you can use Google Cloud Sync"},{"content":"• A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.","upvote_count":"1","poster":"swatititame","timestamp":"1637580120.0","comment_id":"224910"},{"comment_id":"192269","content":"A also for me!","poster":"RockAJ","upvote_count":"1","timestamp":"1633266000.0"},{"content":"Straight Answer: A\n\nGoogle has Cloud Directory Sync service to link LDAP in GCP","timestamp":"1631269920.0","poster":"hiteshrup","comment_id":"177045","upvote_count":"2"},{"upvote_count":"2","comment_id":"161639","timestamp":"1629389220.0","content":"A is correct for me","poster":"SSPC"},{"content":"A it is","timestamp":"1627667400.0","poster":"Verve","upvote_count":"2","comment_id":"147585"}],"answer_description":"","answer_ET":"A","unix_timestamp":1593173220},{"id":"zJmxWVdxMQMUT2yGHTjQ","url":"https://www.examtopics.com/discussions/google/view/74372-exam-associate-cloud-engineer-topic-1-question-11-discussion/","isMC":true,"topic":"1","answer_description":"","answer_images":[],"discussion":[{"poster":"Buruguduystunstugudunstuy","timestamp":"1672347300.0","comment_id":"761484","content":"Selected Answer: A\nThe correct answer is Option A - Deployment Manager. Deployment Manager is a configuration management tool that allows you to define and deploy a set of resources, including Compute Engine VMs, in a declarative manner. You can use it to specify the exact specifications of your VMs in a configuration file, and Deployment Manager will create and manage those VMs for you. Deployment Manager is recommended by Google as a way to automate and manage the deployment of resources on the Google Cloud Platform.\n\nhttps://cloud.google.com/deployment-manager/docs/","upvote_count":"22"},{"timestamp":"1698211200.0","comment_id":"1053467","poster":"shreymath9999","upvote_count":"9","content":"The question says - \"Dynamic way of provision VMs on Compute Engine\" which both the Managed Instance group and the Deployment Manager does, but for different purposes. So here the answer can be more of Deployment Manager as it covers the scope of question, while the Managed Instance group can also dynamically provision VMs based on configuration file but only for auto-healing or horizontal scaling purposes, the answer could have been C if the question was asked as \"Dynamic way of provision VMs on Compute Engine for horizontal scaling/auto healing\""},{"content":"A - due to provisioning VMs on Compute Engine - exact specifications will be in a dedicated configuration file.","poster":"harsh5kalsait","upvote_count":"1","timestamp":"1723363320.0","comment_id":"1263906"},{"content":"Correct Answer: C\nReference:\nhttps://cloud.google.com/compute/docs/instances/","timestamp":"1716484620.0","comment_id":"1216797","poster":"subha.elumalai","upvote_count":"1"},{"upvote_count":"1","content":"The correct answer is A","poster":"BAofBK","timestamp":"1699219920.0","comment_id":"1063307"},{"upvote_count":"1","comments":[{"poster":"Nikki2424","content":"Why does C make no sense?","comment_id":"1209345","timestamp":"1715339940.0","upvote_count":"1"}],"comment_id":"1053095","timestamp":"1698172680.0","poster":"guicane","content":"Selected Answer: A\nC makes no sense, it's A"},{"timestamp":"1697014860.0","content":"A deployment manager","upvote_count":"1","comment_id":"1040402","poster":"Evan7557"},{"upvote_count":"1","content":"Yes, deployment manager is the right answer,it helps you to configure resourres as per your file","comment_id":"996175","timestamp":"1693582080.0","poster":"Captain1212"},{"upvote_count":"4","content":"Selected Answer: C\nI would go with C. A has a broader scope, but C is the correct answer for VM's.","comment_id":"968897","poster":"mmierke","timestamp":"1690879860.0"},{"comment_id":"963882","upvote_count":"1","poster":"Rkraj","content":"Selected Answer: A\ncorrect is A","timestamp":"1690384920.0"},{"content":"Selected Answer: A\nThe correct answer is : A","timestamp":"1689850020.0","upvote_count":"1","poster":"ExamsFR","comment_id":"957404"},{"comment_id":"956696","timestamp":"1689774360.0","content":"The correct answer is:\n\nA. Deployment Manager","upvote_count":"1","poster":"rosh199"},{"upvote_count":"2","poster":"AdamCaster","comment_id":"869275","timestamp":"1681380900.0","content":"Selected Answer: A\nA for sure"},{"comments":[{"timestamp":"1680925740.0","upvote_count":"5","comment_id":"864385","poster":"sakdip66","content":"Managed Instance Group (option C) and Unmanaged Instance Group (option D) are Compute Engine features that allow you to group related VM instances and manage them as a single entity. However, they do not provide a dynamic way of provisioning VMs based on a configuration file like Deployment Manager does."}],"poster":"esqandares","content":"any one can share why is C","upvote_count":"4","timestamp":"1680145800.0","comment_id":"855234"},{"poster":"23_7k","timestamp":"1673243760.0","upvote_count":"2","content":"Hi which answer we need to select? from discussion or website answer? could you please tell me?","comment_id":"770065","comments":[{"poster":"InigoGutierrez","content":"Discussion is normally the correct one","timestamp":"1675679340.0","upvote_count":"2","comment_id":"799614"}]},{"comment_id":"736538","timestamp":"1670304600.0","poster":"ninjaasmoke","upvote_count":"1","content":"A.\nExplained here: https://cloud.google.com/deployment-manager/docs/configuration/create-basic-configuration"},{"poster":"ChristN","timestamp":"1669806600.0","upvote_count":"2","comment_id":"731368","content":"A is the answer. We are talking about a dedicated config file. https://cloud.google.com/deployment-manager/docs"},{"timestamp":"1669711140.0","comment_id":"730119","content":"Selected Answer: A\nA is okay","poster":"romega2","upvote_count":"2"},{"upvote_count":"1","comment_id":"701992","content":"A. deployment manager, \ninstance template / group is wrong","timestamp":"1666511820.0","poster":"leogor"},{"timestamp":"1664378460.0","upvote_count":"1","content":"Hello, if possible, please email me the questions at alex.reznicek@live.com. I have access to only a limited number as well. Thank you.","comment_id":"681852","poster":"alexander_reznicek"},{"upvote_count":"4","poster":"RAVI321","comment_id":"649006","comments":[{"poster":"Chris10X","upvote_count":"3","comment_id":"652578","timestamp":"1661606100.0","comments":[{"comments":[{"upvote_count":"2","content":"i need it too","timestamp":"1662200520.0","poster":"Pushpen_27","comment_id":"658332"}],"timestamp":"1661834100.0","comment_id":"653814","content":"Hey Chris10X \ncan you share the doc of the whole 198 questions please?\ni can share you the email id once you respond here","poster":"v_15","upvote_count":"1"},{"comment_id":"660025","content":"Can you please share for me as well all this questions it would be really helpful\nThanks in advance","upvote_count":"1","poster":"Revupvn","timestamp":"1662372480.0"},{"poster":"RAVI321","comment_id":"660999","upvote_count":"1","timestamp":"1662453360.0","content":"Bro can pls share all those questions it will really helpfull"},{"content":"hi Chris Can you please send them to 15000616693@162.com","comment_id":"692203","poster":"Irenia111","upvote_count":"1","timestamp":"1665500040.0"},{"timestamp":"1685953500.0","comment_id":"915216","content":"Hi Chris can you please share the complete list of questions to samruddhijagtap23@gmail.com. I have my exam on 7th june. Thank you in advance!","poster":"[Removed]","upvote_count":"1"}],"content":"I do... yes"},{"upvote_count":"1","poster":"isatemelci","comment_id":"665557","timestamp":"1662829860.0","content":"Could you please share with me? tmlc@duck.com"},{"timestamp":"1662833820.0","poster":"Letahrgicbeagle","content":"Did anyone get that from Chris?","upvote_count":"1","comment_id":"665596"}],"timestamp":"1660920300.0","content":"Hey guys anybody have the whole 197 question of this site"},{"content":"Selected Answer: A\nA correct answer","upvote_count":"1","timestamp":"1659835320.0","poster":"12234","comment_id":"643559"},{"content":"Answer is A","timestamp":"1656739620.0","poster":"RanjithK","comment_id":"625946","upvote_count":"1"},{"comment_id":"625221","upvote_count":"4","poster":"orious","content":"Selected Answer: A\nA is correct.\nManaged Instance Groups don't support Configuration file in order to provision VM instances.","timestamp":"1656588480.0"},{"comment_id":"620630","poster":"AzureDP900","upvote_count":"1","content":"Deployment Manager is same like AWS Cloud Formation.. A is right answer","timestamp":"1655931240.0"},{"poster":"alex000","comment_id":"607890","content":"Selected Answer: A\nhttps://cloud.google.com/deployment-manager/docs/fundamentals?hl=en&_ga=2.254774664.-1447792053.1630588214","timestamp":"1653624240.0","upvote_count":"3"},{"upvote_count":"1","timestamp":"1653211620.0","comment_id":"605313","content":"Go for A","poster":"haroldbenites"},{"upvote_count":"2","comment_id":"599092","timestamp":"1652108340.0","poster":"Krmnpi","content":"Selected Answer: A\nIt should be A"},{"content":"Selected Answer: A\nManaged instance groups use instance templates. C is not correct. The correct answer is A.","upvote_count":"2","comment_id":"593087","timestamp":"1651057140.0","poster":"rsuresh27"},{"comment_id":"592570","upvote_count":"1","content":"The answer should be A","poster":"Chile","timestamp":"1650990900.0"},{"content":"pls see in this article https://cloud.google.com/deployment-manager/docs/configuration","comments":[{"content":"So you're saying that \"B\" is correct?","poster":"darkroomie","upvote_count":"1","comment_id":"681985","timestamp":"1664386500.0"}],"comment_id":"592379","upvote_count":"2","timestamp":"1650974220.0","poster":"DesmoMike74"},{"poster":"DesmoMike74","upvote_count":"1","comment_id":"592375","timestamp":"1650974040.0","content":"It should be A it's mentioned a configuration file"},{"poster":"DanielB96","upvote_count":"2","comment_id":"592289","content":"Selected Answer: A\nConfiguration File is the key.\nhttps://cloud.google.com/deployment-manager/docs","timestamp":"1650967140.0"},{"upvote_count":"1","timestamp":"1650965280.0","poster":"mshenoyb","comment_id":"592272","content":"I agree, it should be A - Deployment Manager - \nhttps://cloud.google.com/deployment-manager/docs/configuration"},{"upvote_count":"1","content":"Selected Answer: A\nI think it's 1 bcs of Configuration file.","comment_id":"592142","poster":"Karim_2454","timestamp":"1650955020.0"},{"comment_id":"591763","timestamp":"1650896160.0","poster":"RishiRawal","content":"It should be A Deployment Manager, Since Question is talking about Configuration File","upvote_count":"2"},{"upvote_count":"2","timestamp":"1650837420.0","poster":"Nihaaa","content":"Shouldn't it be A, since there is the mention of the config file?","comment_id":"591246"}],"answer":"A","question_id":13,"answers_community":["A (92%)","8%"],"timestamp":"2022-04-24 23:57:00","exam_id":1,"question_images":[],"unix_timestamp":1650837420,"answer_ET":"A","choices":{"D":"Unmanaged Instance Group","B":"Cloud Composer","C":"Managed Instance Group","A":"Deployment Manager"},"question_text":"You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices. Which method should you use?"},{"id":"8UiGYk4u30ib2DySEjv0","unix_timestamp":1591787640,"question_id":14,"answer_ET":"A","isMC":true,"question_images":[],"discussion":[{"comment_id":"176541","timestamp":"1631193120.0","content":"A - correct. Best practice is to create a new project for each environment, such as production and testing. There are no routes between VPCs in these projects by default, so that satisfies the requirement by the security team. \nB. Nope. not best practice and allows communication. \nC. While this is best practice to create a new project for a different environment, it explicitly breaks the security team's rule of having no path between environments by nature of the shared VPC. The shared VPC allows entities in both VPCs to communicate as if they were in the same VPC. That's definitely wrong. \nD. One - not best practice to replicate in the setup in that project. Two - why do they suddenly need the project editor rule? Just a bad answer. Wrong.","poster":"[Removed]","upvote_count":"41"},{"comment_id":"106740","timestamp":"1623323640.0","upvote_count":"35","comments":[{"content":"Correct answer!","upvote_count":"4","poster":"pYWORLD","timestamp":"1659073500.0","comment_id":"416657"}],"poster":"poogcp","content":"Correct answer is A."},{"content":"Selected Answer: A\nA\nGoogle's best practices says \"create a new project for each environment.","upvote_count":"2","timestamp":"1732753560.0","poster":"kelliot","comment_id":"1082026"},{"comment_id":"1063949","poster":"BAofBK","upvote_count":"1","timestamp":"1730904600.0","content":"The correct answer is A"},{"timestamp":"1725719820.0","upvote_count":"1","poster":"scanner2","content":"Selected Answer: A\nAccording to Google recommended practices, you should create a separate project for different environments (dev, test, and prod). Also, the question has forbidden the existence of these environments so shared VPC cannot be used.","comment_id":"1001656"},{"content":"Selected Answer: A\nA is the correct answer, as it satisy tyhe requirement of security team , no commiunicatiojn , as option c allows coummnication","poster":"Captain1212","upvote_count":"1","timestamp":"1725347520.0","comment_id":"997405"},{"timestamp":"1724565720.0","comment_id":"989703","upvote_count":"1","poster":"Neha_Pallavi","content":"A. Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment."},{"upvote_count":"1","comment_id":"697762","timestamp":"1697588100.0","poster":"diasporabro","content":"Selected Answer: A\nSatisfies requirements by the security team"},{"upvote_count":"1","poster":"anolive","timestamp":"1697193780.0","content":"Selected Answer: A\nmake sense","comment_id":"693800"},{"poster":"alexandercamachop","content":"Selected Answer: A\nA is definitely the answer.","timestamp":"1691729880.0","upvote_count":"1","comment_id":"645268"},{"comment_id":"643533","content":"https://cloud.google.com/architecture/framework/system-design/resource-management#decouple","upvote_count":"1","poster":"theBestStudent","timestamp":"1691357520.0"},{"comment_id":"633824","upvote_count":"1","timestamp":"1689826920.0","poster":"andreherwanto","content":"Selected Answer: A\nCorrect answer is A."},{"content":"Selected Answer: C\nTechnically we should create a new VPC if the network is not shared. Creating resources in a new project even within a new subnet will not separate unless firewall rules are not explicitly denying the traffic. The best answer is to create a shared VPC where DEV and PROD are service projects. \nMy Answer is: C","upvote_count":"1","timestamp":"1688846520.0","comment_id":"628891","comments":[{"poster":"theBestStudent","timestamp":"1691357040.0","comment_id":"643532","upvote_count":"3","content":"Why you want to share environments? they should be isolated. Therefore Answer should be A."}],"poster":"csrazdan"},{"poster":"haroldbenites","content":"Go for A\nI thought that the correct answer was the C , but the question did not say to communicate both environments.","comment_id":"612075","timestamp":"1686010560.0","upvote_count":"1"},{"comment_id":"570846","upvote_count":"1","timestamp":"1679199660.0","poster":"Rukman","content":"Selected Answer: A\nAns: A\nAgreed!"},{"poster":"JieHeng","upvote_count":"6","content":"Should be A\nit's a best practice \"to have one project per application per environment.\" - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#project-structure","timestamp":"1656074520.0","comment_id":"389596"},{"timestamp":"1653052860.0","content":"A answer","upvote_count":"1","comment_id":"362199","poster":"arsh1916"},{"poster":"mcaromit","timestamp":"1652492460.0","content":"A is correct","comment_id":"356793","upvote_count":"1"},{"upvote_count":"1","poster":"EJJ","comment_id":"324056","timestamp":"1648623300.0","content":"i will go with A"},{"poster":"[Removed]","timestamp":"1648203720.0","comment_id":"320060","content":"A is correct. Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment.","upvote_count":"1"},{"timestamp":"1646808660.0","content":"C is correct","upvote_count":"2","poster":"EABDAJA","comment_id":"306182"},{"timestamp":"1645744800.0","poster":"GCP_Student1","comment_id":"298621","content":"A. Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment.","upvote_count":"2"},{"poster":"piipo","timestamp":"1641019080.0","content":"Ans is A.","upvote_count":"1","comment_id":"256686"},{"content":"Guys please be aware of Google recommended practices, yes A is right but it's not recommended in this case . Use a shared vpc with a single project as the host, these are the fine margins for passing the exam","comment_id":"244469","comments":[{"poster":"ShakthiGCP","timestamp":"1646332500.0","comment_id":"302814","upvote_count":"4","content":"\"The security team has forbidden the existence of network routes between these 2 environments \" . Forbidden means preventing access across environment and why u want to link those network using shared VPC ???"},{"comment_id":"302651","poster":"DickDastardly","timestamp":"1646315520.0","upvote_count":"1","content":"From https://cloud.google.com/solutions/best-practices-vpc-design\n\nCreate a shared services VPC if multiple VPC networks need access to common resources but not each other\nA VPC network provides a full mesh of global reachability. For this reason, shared services and continuous integration pipelines residing in the same VPC network don't require special consideration when it comes to connectivity—they are inherently reachable. Shared VPC extends this concept, allowing shared services to reside in an isolated project while providing connectivity to other services or consumers.\n\nThis points to C"},{"timestamp":"1640157660.0","comment_id":"249924","upvote_count":"1","poster":"cRobert","content":"Why would you use a shared VPC?"}],"poster":"nightflyer","upvote_count":"1","timestamp":"1639563780.0"},{"poster":"RockAJ","upvote_count":"1","content":"Ans is A","comment_id":"192271","timestamp":"1633266300.0"},{"comment_id":"177050","content":"The answer is A only. Google never recommend you to share Dev and Prod resources to be shared. For Production env you should have a new project and a new set of rules. This can be more efficient by using Deployment Manager use, however, that is not a case. So sticking with Answer A. None other will satisfy any condition. \n\nThough Shared VPC can achieve things which are asked that is not the recommended approach and same time, we should have Cloud SQL and Compute Engine to be deployed on prod project eventually to make the boundary between Prod and Dev. So option C should be avoided","upvote_count":"4","poster":"hiteshrup","timestamp":"1631270220.0"},{"poster":"ESP_SAP","timestamp":"1629777540.0","upvote_count":"4","content":"Correct Answer is (C):\n\nShared VPC overview\nShared VPC allows an organization to connect resources from multiple projects to a common Virtual Private Cloud (VPC) network, so that they can communicate with each other securely and efficiently using internal IPs from that network. When you use Shared VPC, you designate a project as a host project and attach one or more other service projects to it. The VPC networks in the host project are called Shared VPC networks. Eligible resources from service projects can use subnets in the Shared VPC network.\n\nhttps://cloud.google.com/vpc/docs/shared-vpc","comment_id":"164852","comments":[{"poster":"ESP_SAP","timestamp":"1694358060.0","upvote_count":"1","content":"Correction.\nCorrect Answer is (A).\nExplicitly they asking not sharing the resources and use the GCP best Practices.\nGCP Best practices indicate create a new project for every environment (DEV, QA, PRD).","comment_id":"665491"},{"upvote_count":"2","comment_id":"319150","timestamp":"1648129080.0","content":"we cannot use shared VPC because it provides network routes between those two projects which violates the question.","poster":"yuvi69"},{"upvote_count":"2","comment_id":"178168","timestamp":"1631443980.0","content":"It is recommended the projects are isolated why we are talking about sharing the resources?","poster":"[Removed]"},{"poster":"bachor","content":"But \"The security team has forbidden the existence of network routes between these 2 environments\" hence there shouldn't be any communication between development and production. I would say the correct answer is A","timestamp":"1630345440.0","upvote_count":"6","comment_id":"170229"}]},{"timestamp":"1624867740.0","content":"Seems answer is C: Shared VPC which allows replicate","comment_id":"121724","upvote_count":"1","poster":"raffiq","comments":[{"timestamp":"1625392680.0","poster":"jskumar","content":"According to Google recommendation we should create separate project for each environment . Hence option A looks good.","upvote_count":"6","comment_id":"126063"}]},{"poster":"professor","content":"Ans is A\n\ninstances communicate in Shared VPC","comment_id":"120510","upvote_count":"2","timestamp":"1624709640.0"}],"exam_id":1,"answer":"A","answer_images":[],"answer_description":"","answers_community":["A (90%)","10%"],"timestamp":"2020-06-10 13:14:00","url":"https://www.examtopics.com/discussions/google/view/22733-exam-associate-cloud-engineer-topic-1-question-110/","topic":"1","choices":{"D":"Ask the security team to grant you the Project Editor role in an existing production project used by another division of your company. Once they grant you that role, replicate the setup you have in the development environment in that project.","B":"Create a new production subnet in the existing VPC and a new production Cloud SQL instance in your existing project, and deploy your application using those resources.","C":"Create a new project, modify your existing VPC to be a Shared VPC, share that VPC with your new project, and replicate the setup you have in the development environment in that new project in the Shared VPC.","A":"Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment."},"question_text":"You have successfully created a development environment in a project for an application. This application uses Compute Engine and Cloud SQL. Now you need to create a production environment for this application. The security team has forbidden the existence of network routes between these 2 environments and has asked you to follow Google-recommended practices. What should you do?"},{"id":"es4UadSSOu57WvsxdOVb","exam_id":1,"timestamp":"2020-06-05 17:42:00","answer_description":"","topic":"1","answer_ET":"C","isMC":true,"question_images":[],"answer":"C","choices":{"D":"Create a temporary account for the auditor in Cloud Identity, and give that account the Security Reviewer role on the project.","C":"Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project.","A":"Ask the auditor for their Google account, and give them the Viewer role on the project.","B":"Ask the auditor for their Google account, and give them the Security Reviewer role on the project."},"unix_timestamp":1591371720,"answers_community":["C (77%)","14%","9%"],"question_id":15,"discussion":[{"upvote_count":"52","timestamp":"1591623660.0","poster":"dan80","comment_id":"105261","comments":[{"content":"This guy is right!","comment_id":"125088","upvote_count":"7","timestamp":"1593698280.0","poster":"spudleymcdudley"}],"content":"C - https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors"},{"content":"Correct Answer is (C):\n\nroles/viewer Read access to all resources. Get and list access for all resources.\n\nUsing primitive roles\nThe following table lists the primitive roles that you can grant to access a project, the description of what the role does, and the permissions bundled within that role. Avoid using primitive roles except when absolutely necessary. These roles are very powerful, and include a large number of permissions across all Google Cloud services. For more details on when you should use primitive roles, see the Identity and Access Management FAQ.\n\nIAM predefined roles are much more granular, and allow you to carefully manage the set of permissions that your users have access to. See Understanding Roles for a list of roles that can be granted at the project level. Creating custom roles can further increase the control you have over user permissions.\n\nhttps://cloud.google.com/resource-manager/docs/access-control-proj#using_primitive_roles","poster":"ESP_SAP","comment_id":"162483","timestamp":"1597954860.0","upvote_count":"21"},{"timestamp":"1719164940.0","upvote_count":"2","content":"Selected Answer: C\nthe key word is \"organisation Policy called Domain Restricted sharing.\" his external google account wont work","poster":"kayceeec","comment_id":"1235937"},{"comment_id":"1171563","timestamp":"1710234660.0","content":"Selected Answer: C\nCORRECT ANSWER IS C","upvote_count":"1","poster":"Ankit_EC_ran"},{"upvote_count":"3","poster":"ogerber","timestamp":"1701801960.0","comment_id":"1088729","content":"Selected Answer: C\nDomain Restricted Sharing: Since your organization has the Domain Restricted Sharing policy enabled, sharing resources with accounts outside your Cloud Identity domain isn't allowed. Therefore, options A and B, which involve using the auditor's Google account, aren't feasible."},{"poster":"kelliot","comment_id":"1082028","timestamp":"1701131220.0","upvote_count":"1","content":"C, without doubt"},{"poster":"thewalker","comment_id":"1078402","timestamp":"1700741340.0","content":"Selected Answer: D\nD\n\nAs per the documentation, Security Reviewer is more narrow role than the basic Viewer role: \nhttps://cloud.google.com/iam/docs/understanding-roles#iam.securityReviewer\nhttps://cloud.google.com/iam/docs/understanding-roles#viewer","upvote_count":"2"},{"content":"Selected Answer: C\nIt could be A, But C is more practical and you don't have to give the auditor extra 3 seconds of work, and yourself for deleting him after he finishes","comment_id":"1074733","poster":"Rahaf99","upvote_count":"3","timestamp":"1700411460.0"},{"content":"The correct answer is C","poster":"BAofBK","comment_id":"1063955","timestamp":"1699282440.0","upvote_count":"1"},{"comment_id":"1001662","content":"Selected Answer: C\nThe Resource Manager provides a domain restriction constraint that can be used in organization policies to limit resource sharing based on domain or organization resource. This constraint allows you to restrict the set of identities that are allowed to be used in Identity and Access Management policies.\nOrganization policies can use this constraint to limit resource sharing to identities that belong to a particular organization resource.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains","upvote_count":"1","timestamp":"1694097780.0","poster":"scanner2"},{"comment_id":"997411","poster":"Captain1212","timestamp":"1693725540.0","upvote_count":"1","content":"Selected Answer: C\nC is more correct"},{"timestamp":"1692941340.0","poster":"Neha_Pallavi","comment_id":"989685","content":"Correct Answer is (C):","upvote_count":"1"},{"poster":"WendyLC","timestamp":"1686505860.0","upvote_count":"1","comment_id":"920893","content":"Selected Answer: C\nCorrect Answer is (C): \n\nAnswer A is wrong because we can't use the the auditor Google account, security team has enabled the Organization Policy specifying only one Cloud Identity domain."},{"content":"Selected Answer: C\nAnswer is definitely C\nPlease review this as it seems to be looked over in the other comments\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains\n(a google account that isn't part of the domain will not work unless you specifically allow exceptions at the project level and that was not defined in the answers)","comment_id":"891589","poster":"Shenannigan","upvote_count":"1","timestamp":"1683480000.0"},{"content":"Selected Answer: C\ni believe it is C","timestamp":"1681730460.0","poster":"sabrinakloud","upvote_count":"1","comment_id":"872637"},{"comment_id":"773003","poster":"thaliath","upvote_count":"1","timestamp":"1673484420.0","content":"Correct answer is C. A is not correct. You can not ask someone to create a personal google account. He/she has no obligation to do so"},{"poster":"alex000","upvote_count":"2","comment_id":"768613","timestamp":"1673100720.0","content":"Selected Answer: A\nFrom: https://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors\n\n\"The organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application.\n\nDuring normal access, the auditors' Google group is only granted access to view the historic logs stored in BigQuery. If any anomalies are discovered, the group is granted permission to view the actual Cloud Logging Admin Activity logs via the dashboard's elevated access mode. At the end of each audit period, the group's access is then revoked.\""},{"comments":[{"poster":"jrisl1991","timestamp":"1675039680.0","comment_id":"792214","content":"Honestly that seems to me a terrible reasoning. Nowhere it says \"ask the auditor to give you their Google account\" ¯\\_(ツ)_/¯.\n\nI think it's C, because that same scenario says \"The organization creates a Google group for these external auditors and adds the current auditor to the group.\". Plus, that specific scenario talks about logs in BigQuery, and in the question they are asking to review all the resources.","upvote_count":"3"}],"content":"Selected Answer: A\nCorrect Answer is (A)\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors\nnowhere I see \"temporary account\"","upvote_count":"1","timestamp":"1672819620.0","poster":"alex000","comment_id":"765368"},{"upvote_count":"1","timestamp":"1660194000.0","comment_id":"645270","poster":"alexandercamachop","content":"Selected Answer: C\nC - https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors"},{"poster":"abirroy","timestamp":"1659900180.0","comment_id":"643826","upvote_count":"1","content":"Selected Answer: C\nCreate a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project."},{"content":"C is right","timestamp":"1656014880.0","poster":"AzureDP900","comment_id":"621270","upvote_count":"1"},{"upvote_count":"1","timestamp":"1654477560.0","content":"Go for C","comment_id":"612086","poster":"haroldbenites"},{"poster":"Rukman","content":"Selected Answer: C\nAns: C","upvote_count":"1","timestamp":"1647663960.0","comment_id":"570847"},{"timestamp":"1626378360.0","content":"C is right","poster":"Cthakker","comment_id":"407408","upvote_count":"2"},{"timestamp":"1623082200.0","poster":"nana1995","upvote_count":"1","comment_id":"376895","comments":[{"comment_id":"379492","timestamp":"1623392220.0","poster":"happyBoo","upvote_count":"6","content":"roles/iam.securityReviewer will give access to view IAM roles in addition, which is not required."}],"content":"Why is not D?"},{"poster":"mcaromit","timestamp":"1620957000.0","comment_id":"356803","content":"C is correct","upvote_count":"1"},{"comment_id":"341071","upvote_count":"3","timestamp":"1619095860.0","poster":"meh009","content":"Easy - C\n\nDomain Retricted Sharing is on ( restrict the set of identities that are allowed to be used in Identity and Access Management policies) which takes out A/B of the equation. \n\nEasy choice between C/D"},{"content":"C is correct. Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project.","upvote_count":"1","comment_id":"320063","poster":"[Removed]","timestamp":"1616667900.0"},{"timestamp":"1616593440.0","content":"option C is correct. because viewer role grants viewing all the resources in the project whereas securityreviewer role only grants permission to list the resources\nhttps://cloud.google.com/iam/docs/understanding-roles#iam-roles","poster":"yuvi69","upvote_count":"1","comment_id":"319156"},{"upvote_count":"1","timestamp":"1615590180.0","poster":"SSunny","content":"C- https://cloud.google.com/iam/docs/understanding-custom-roles\n\nThe IAM Security Reviewer role (roles/iam.securityReviewer) enables the ability to view custom roles but not administer them.","comment_id":"309260"},{"comment_id":"307486","upvote_count":"2","content":"C - Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project.","poster":"GCP_Student1","timestamp":"1615418160.0"},{"upvote_count":"1","timestamp":"1615306740.0","poster":"EABDAJA","content":"Answer C","comment_id":"306545"},{"comment_id":"283276","content":"C - Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project.","upvote_count":"1","timestamp":"1612426020.0","poster":"victory108"},{"poster":"nherrerab","content":"C is Correct.","upvote_count":"1","comment_id":"267585","timestamp":"1610680500.0"},{"comment_id":"221231","content":"Security Reviewer:\nProvides permissions to list all resources and IAM policies on them.\n\nViewer: Permissions for read-only actions that do not affect state, such as viewing (but not modifying) existing resources or data.\n\nThink C makes sense then","upvote_count":"4","poster":"ayj","timestamp":"1605630360.0"},{"content":"For sure it's C","comment_id":"216946","upvote_count":"1","timestamp":"1605042180.0","poster":"Yamac"},{"comment_id":"176545","poster":"[Removed]","timestamp":"1599657360.0","content":"A and B are just bad. A temporary account is much safer. What if they forgot to revoke access once the auditor is done? Yikes. \n\nC - seems to grant only the necessary permissions. \nD - close, but with the ability to view Cloud IAM policies as spudleymcdudley pointed out, it's over-permissive when the need is to just view project resources. \n\nanswer: C","upvote_count":"3"},{"poster":"rehma017","upvote_count":"3","comment_id":"103874","timestamp":"1591455420.0","content":"D for sure - C could work technically, but its better at an ops level to be able to list all auditor accounts with the Security Reviewer role","comments":[{"content":"This is incorrect, security review only relates to IAM - \"Provides permissions to list all resources and Cloud IAM policies on them\" no god for seeing everything in a project","timestamp":"1593698220.0","poster":"spudleymcdudley","upvote_count":"2","comment_id":"125087"}]},{"comments":[{"timestamp":"1591455240.0","poster":"SIX","comment_id":"103870","content":"Sorry, I mean C","upvote_count":"9"}],"content":"definitely D\nhttps://cloud.google.com/iam/docs/understanding-roles","upvote_count":"3","poster":"SIX","timestamp":"1591371720.0","comment_id":"103245"}],"question_text":"Your management has asked an external auditor to review all the resources in a specific project. The security team has enabled the Organization Policy called\nDomain Restricted Sharing on the organization node by specifying only your Cloud Identity domain. You want the auditor to only be able to view, but not modify, the resources in that project. What should you do?","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/22213-exam-associate-cloud-engineer-topic-1-question-111/"}],"exam":{"isMCOnly":true,"isBeta":false,"isImplemented":true,"id":1,"numberOfQuestions":285,"name":"Associate Cloud Engineer","provider":"Google","lastUpdated":"11 Apr 2025"},"currentPage":3},"__N_SSP":true}