{"pageProps":{"questions":[{"id":"2ZTFCzyxArmS1HMkNPlg","answer":"AD","answer_description":"","question_id":46,"url":"https://www.examtopics.com/discussions/google/view/16281-exam-professional-data-engineer-topic-1-question-14/","answer_ET":"AD","choices":{"E":"You already have labels for which samples are mutated and which are normal in the database.","D":"You expect future mutations to have similar features to the mutated samples in the database.","B":"There are roughly equal occurrences of both normal and mutated samples in the database.","A":"There are very few occurrences of mutations relative to normal samples.","C":"You expect future mutations to have different features from the mutated samples in the database."},"answers_community":["AD (47%)","AC (46%)","7%"],"question_text":"You want to use a database of information about tissue samples to classify future tissue samples as either normal or mutated. You are evaluating an unsupervised anomaly detection method for classifying the tissue samples. Which two characteristic support this method? (Choose two.)","topic":"1","answer_images":[],"exam_id":11,"discussion":[{"content":"I think that AD makes more sense. D is the explanation you gave. In the rest, A makes more sense, in any anomaly detection algorithm it is assumed a priori that you have much more \"normal\" samples than mutated ones, so that you can model normal patterns and detect patterns that are \"off\" that normal pattern. For that you will always need the no. of normal samples to be much bigger than the no. of mutated samples.","comments":[{"upvote_count":"1","content":"as per chatGPT, it can be different (C) - thats how unsupervised anomaly detection works - as long as they are different than \"normal' tissues , they would be detected","comment_id":"1326075","timestamp":"1734084180.0","poster":"AmitK121981"},{"upvote_count":"22","comment_id":"494902","content":"Guys its A & C.\nAnomaly detection has two basic assumptions: \n->Anomalies only occur very rarely in the data. (a)\n->Their features differ from the normal instances significantly. (c)\n\nlink -> https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1#:~:text=Unsupervised%20Anomaly%20Detection%20for%20Univariate%20%26%20Multivariate%20Data.&text=Anomaly%20detection%20has%20two%20basic,from%20the%20normal%20instances%20significantly.","timestamp":"1638766500.0","comments":[{"content":"I don't agree on C. Anomaly detection assumes \"Their features differ from the NORMAL INSTANCES significantly\" and in the C option you have: \n\"You expect future mutations to have different features from the MUTATED SAMPLES IN THE DATABASE\". \n\nIMHO Answer D fits better: \"D. You expect future mutations to have similar features to the mutated samples in the database.\" - in other words: Expect future anomalies to be similar to the anomalies that we already have in database","comment_id":"502480","timestamp":"1639601940.0","poster":"szefco","upvote_count":"27"}],"poster":"BigQuery"}],"poster":"jvg637","timestamp":"1584271680.0","comment_id":"64229","upvote_count":"73"},{"content":"A instead of B:\n\"anomaly detection (also outlier detection[1]) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data","timestamp":"1583947440.0","comment_id":"62579","upvote_count":"21","poster":"jvg637"},{"content":"Selected Answer: AC\nUnsupervised anomaly detection. Characteristics are few anomalies and future mutations different. So A and C.","upvote_count":"1","poster":"Parandhaman_Margan","comment_id":"1398847","timestamp":"1742046180.0"},{"timestamp":"1737832320.0","comment_id":"1346613","poster":"LP_PDE","upvote_count":"2","content":"Selected Answer: AC\nAC. Answer C. Unsupervised anomaly detection methods are particularly useful when you don't have labeled examples of the anomalies you're trying to detect. Why not D, if future mutations are similar to existing ones, a supervised model trained on labeled examples of known mutations would likely be more accurate in classifying new samples."},{"upvote_count":"1","poster":"cqrm3n","comment_id":"1342431","content":"Selected Answer: AC\nThe answer should be A and C.\n\nUnsupervised anomaly detection is useful when labels are unavailable, or when anomalies are rare and distinct.\n\nHence A is definitely correct because anomaly detection excels when anomalies are rare compared to normal data.\n\nI think C is correct because by adding new mutation data that is similar to the existing mutation data, the model will learn in a broader sense on what constitutes to 'mutation', and it leads to a better generalization. If the new data is too similar to the existing mutation data (answer D), the model might overfit to those specific examples. However, the new data should still share some fundamental characteristic to the existing data so that the model can recognize them as belonging to the same anomaly category.","timestamp":"1737180780.0"},{"content":"Selected Answer: AC\nI think it's A&C. For an anomaly detection model, the ratio of normal vs abnormal is expected to be high. 'C' because the model is expected to be adaptive meaning the model detects the abnormal features that can be different from the abnormal features currently being trained on.","poster":"kumar34","upvote_count":"1","timestamp":"1735081620.0","comment_id":"1331265"},{"poster":"SamuelTsch","content":"Selected Answer: AC\nThe keyword is unsupervised anomaly detection. So A is correct. We think and should ensure the majority of data represents 'normal'. Unsupervised methods are good for detecting unknown patterns. Thus C could be correct.","upvote_count":"1","comment_id":"1301180","timestamp":"1729530240.0","comments":[{"comment_id":"1301201","poster":"SamuelTsch","upvote_count":"1","content":"I correct my answer. AD should be better. Unsupervised method is usually used for grouping the data. So, if the future mutations have similar features to the mutated samples, our trained model should group it into anomalies even though no label exists.","timestamp":"1729531500.0"}]},{"content":"Selected Answer: AD\nAD: to use unsupervised anomaly detection the anomalies a) must be rare b) they must differ from the NORMAL. So...\nA: mutated samples must be scarce compared to normal tissue.\nD: yes, we expect the future mutated samples to have similar features to the mutated samples currently in the database.\nWhy not C? If I train my model with mutated samples with specific characteristics, I do not expect it to find different mutations. In the future, when new mutations appear, I would retrain my model including those new samples.","comment_id":"503327","timestamp":"1727155320.0","upvote_count":"4","poster":"hendrixlives"},{"poster":"MaxNRG","comment_id":"528611","upvote_count":"4","timestamp":"1727155320.0","content":"Selected Answer: AD\nAnomaly detection has two basic assumptions:\n*Anomalies only occur very rarely in the data.\n*Their features differ from the normal instances significantly.\nAnomaly detection involves identifying rare data instances (anomalies) that come from a different class or distribution than the majority (which are simply called “normal” instances). Given a training set of only normal data, the semi-supervised anomaly detection task is to identify anomalies in the future. Good solutions to this task have applications in fraud and intrusion detection.\nThe unsupervised anomaly detection task is different: Given unlabeled, mostly-normal data, identify the anomalies among them.\nhttps://www.science.gov/topicpages/u/unsupervised+anomaly+detection\nA because “Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal”, B is for Supervised anomaly detection https://en.wikipedia.org/wiki/Anomaly_detection"},{"comment_id":"719602","content":"Selected Answer: AD\nA - anomaly detection is used for detecting rare events, meaning it is expected that there are much less of those than of normal ones.\nD - you expect the future mutations to be similar to the mutations you already have, so that you can detect them (pattern recognition)","timestamp":"1727155320.0","upvote_count":"2","poster":"gudiking"},{"poster":"jkhong","comment_id":"741533","upvote_count":"4","timestamp":"1727155320.0","content":"Selected Answer: AD\nA makes sense\n\nC and D compares future mutations to mutated samples in database\n\nThe question is pretty badly worded… If we were to run a full unsupervised anomaly detection over the entire dataset, C and D will be true, since some future mutations may be similar to current mutations and some will be significantly different to current mutations.\n\nThe question is suggesting \"labelling\" tissue samples using unsupervised anomaly detection, and subsequently using the labels with a supervised algorithm to classify future samples. If this interpretation of the question is correct, then D makes sense"},{"comment_id":"768244","timestamp":"1727155320.0","upvote_count":"2","content":"Selected Answer: AD\nThe answer should be AD.\n\nA, anomaly should have a little amount, if there are many samples then we should do classification instead, because unsupervised will give a lot of false positive.\n\nD, the future anomaly should be of the same distribution as present anomaly! or else our anomaly detection will not be generalize to the future feature.","poster":"korntewin"},{"comment_id":"799159","poster":"samdhimal","content":"A. There are very few occurrences of mutations relative to normal samples. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying rare events or anomalies in large amounts of data. By training the algorithm on the normal tissue samples in the database, it can then identify new tissue samples that have different features from the normal samples and classify them as mutated.\n\nD. You expect future mutations to have similar features to the mutated samples in the database. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying patterns or anomalies in the data. By training the algorithm on the mutated tissue samples in the database, it can then identify new tissue samples that have similar features and classify them as mutated.","timestamp":"1727155320.0","upvote_count":"2"},{"timestamp":"1727155260.0","upvote_count":"5","comment_id":"949537","content":"Selected Answer: AD\nD should be correct. You expect future samples will correlate with the training samples. That's the whole point of learning procedure. If you do not expect that they have similar features, then why would you use features in the training samples in the first place? A is also correct, since anomaly labels would be seen rarely.","poster":"azmiozgen"},{"comment_id":"1062185","timestamp":"1727155260.0","poster":"rocky48","content":"Selected Answer: AD\nA. There are very few occurrences of mutations relative to normal samples. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying rare events or anomalies in large amounts of data. By training the algorithm on the normal tissue samples in the database, it can then identify new tissue samples that have different features from the normal samples and classify them as mutated.\n\nD. You expect future mutations to have similar features to the mutated samples in the database. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying patterns or anomalies in the data. By training the algorithm on the mutated tissue samples in the database, it can then identify new tissue samples that have similar features and classify them as mutated.","upvote_count":"2"},{"content":"Selected Answer: AC\nA. There are very few occurrences of mutations relative to normal samples.\n\nAnomaly detection is well-suited for situations where anomalies (in this case, mutations) are rare compared to the normal cases. When the dataset is highly imbalanced, with far fewer mutated samples than normal samples, anomaly detection can be used to identify these rare cases as outliers or anomalies.\nC. You expect future mutations to have different features from the mutated samples in the database.\n\nUnsupervised anomaly detection works under the assumption that anomalies (mutations) will differ significantly from the majority of the data (normal samples). If future mutations are expected to exhibit different features, this method can help detect those anomalies as deviations from the normal samples.","timestamp":"1723101240.0","upvote_count":"2","comment_id":"1262377","poster":"Nittin"},{"poster":"iooj","timestamp":"1722361020.0","content":"Selected Answer: AC\nA. There are very few occurrences of mutations relative to normal samples.\nAnomaly detection is particularly useful in scenarios where anomalies (mutations, in this case) are rare compared to normal instances. This aligns with the nature of anomaly detection, which focuses on identifying rare events that deviate significantly from the majority (normal) data.\n\nC. You expect future mutations to have different features from the mutated samples in the database.\n\nUnsupervised anomaly detection methods do not rely on prior knowledge of anomalies. They work on the assumption that anomalies will be different from normal instances in a significant way. If future mutations have different features from known mutations, it supports using an unsupervised method as it can detect novel anomalies not seen during training","comment_id":"1258323","upvote_count":"3"}],"isMC":true,"question_images":[],"unix_timestamp":1583947440,"timestamp":"2020-03-11 18:24:00"},{"id":"KClYOSxvXPt7U5s4OyDS","question_images":[],"choices":{"D":"The original order identification number from the sales system, which is a monotonically increasing integer","A":"The current epoch time","B":"A concatenation of the product name and the current epoch time","C":"A random universally unique identifier number (version 4 UUID)"},"exam_id":11,"timestamp":"2022-09-05 10:23:00","question_text":"You need to create a new transaction table in Cloud Spanner that stores product sales data. You are deciding what to use as a primary key. From a performance perspective, which strategy should you choose?","answer_images":[],"answer":"C","topic":"1","answers_community":["C (92%)","8%"],"url":"https://www.examtopics.com/discussions/google/view/80270-exam-professional-data-engineer-topic-1-question-140/","unix_timestamp":1662366180,"answer_ET":"C","isMC":true,"question_id":47,"answer_description":"","discussion":[{"upvote_count":"9","comments":[{"upvote_count":"1","poster":"AzureDP900","content":"Agree with C","comment_id":"762729","timestamp":"1688137380.0"}],"timestamp":"1678722360.0","poster":"Remi2021","comment_id":"668094","content":"Selected Answer: C\nAccording to the documentation:\nUse a Universally Unique Identifier (UUID)\nYou can use a Universally Unique Identifier (UUID) as defined by RFC 4122 as the primary key. Version 4 UUID is recommended, because it uses random values in the bit sequence. Version 1 UUID stores the timestamp in the high order bits and is not recommended.\n\nhttps://cloud.google.com/spanner/docs/schema-design"},{"comment_id":"1015439","poster":"barnac1es","timestamp":"1711255380.0","upvote_count":"4","content":"Selected Answer: C\nFor a transaction table in Cloud Spanner that stores product sales data, from a performance perspective, it is generally recommended to choose a primary key that allows for even distribution of data across nodes and minimizes hotspots. Therefore, option C, which suggests using a random universally unique identifier number (version 4 UUID), is the preferred choice."},{"content":"Selected Answer: C\nFor a RDB I would choice D.\n\nBut for Google Spanner, Google says: \nhttps://cloud.google.com/spanner/docs/schema-and-data-model#:~:text=monotonically%20increasing%20integer","upvote_count":"1","poster":"arien_chen","comment_id":"985590","timestamp":"1708418580.0"},{"poster":"vaga1","comment_id":"893157","timestamp":"1699546080.0","content":"Selected Answer: C\nB might work if you say timestamp instead than epoch. PK of sales should contain the exact purchase date or timestamp, not the time when the transaction was processed. I personally associate the term epoch in this context to the process timestamp instead than the purchase timestamp.","upvote_count":"2"},{"timestamp":"1694583720.0","comment_id":"837731","comments":[{"poster":"NickNtaken","timestamp":"1732413240.0","comment_id":"1217116","upvote_count":"1","content":"Agreed. Additionally, using the product name can lead to unbalanced distribution if some products are sold more frequently than others."}],"upvote_count":"2","poster":"midgoo","content":"Selected Answer: C\nB may cause error if same product ID came at the same time (same id + same epoch)\nSo C is the correct answer here"},{"timestamp":"1687255740.0","content":"Selected Answer: C\nA and D are invalid because they monotonically increases.\nB would work, but in terms of pure performance UUID 4 is the fastest because it virtually will not cause hotspots","poster":"jkhong","comment_id":"750832","upvote_count":"2"},{"upvote_count":"3","poster":"odacir","comment_id":"739033","timestamp":"1686220380.0","content":"Selected Answer: C\nA and D are not valid, because they monotonically increase.\nC avoid hotspots for sure, but It's nor relate with querys. So for writing performance it's perfect that the reason for chose this:  “You need to create a new transaction table in Cloud Spanner that stores product sales data”. They only ask you to store product data, its a writing ops.\nIf the question had spoken about query the info or hard performance read, the best option would be B, because it has the balance of writing/reading best practices.\nThere are a few disadvantages to using a UUID:\n\n They are slightly large, using 16 bytes or more. Other options for primary keys don't use this much storage.\n They carry no information about the record. For example, a primary key of SingerId and AlbumId has an inherent meaning, while a UUID does not.\n You lose locality between records that are related, which is why using a UUID eliminates hotspots.\n\n\nhttps://cloud.google.com/spanner/docs/schema-design#uuid_primary_key"},{"upvote_count":"1","content":"Selected Answer: C\nC. A random universally unique identifier number (version 4 UUID)\n\nFrom https://cloud.google.com/spanner/docs/schema-and-data-model\n\n\nThere are techniques that can spread the load across multiple servers and avoid hotspots:\n\nHash the key and store it in a column. Use the hash column (or the hash column and the unique key columns together) as the primary key.\nSwap the order of the columns in the primary key.\nUse a Universally Unique Identifier (UUID). Version 4 UUID is recommended, because it uses random values in the high-order bits. Don't use a UUID algorithm (such as version 1 UUID) that stores the timestamp in the high order bits.\nBit-reverse sequential values.","poster":"YorelNation","comment_id":"660028","timestamp":"1678018140.0"},{"comments":[{"comment_id":"1334445","poster":"LP_PDE","upvote_count":"1","content":"Potential Skew: If there are a limited number of product names, this could still lead to uneven data distribution and potential hotspots.\nIncreased Key Size: Concatenating strings can result in larger primary keys, which can slightly impact storage and performance.","timestamp":"1735597020.0"}],"upvote_count":"2","content":"Selected Answer: B\nAnswer should be B as in all the other options hotspotting is possible. According to proper schema design guideline.. \nSchema design best practice #1: Do not choose a column whose value monotonically increases or decreases as the first key part for a high write rate table.\n\nSupporting link: \nhttps://cloud.google.com/spanner/docs/schema-design#primary-key-prevent-hotspots","poster":"jsree236","comment_id":"659916","timestamp":"1678011780.0"}]},{"id":"tDhEVLfpeDVCUfo6hOA0","question_text":"Data Analysts in your company have the Cloud IAM Owner role assigned to them in their projects to allow them to work with multiple GCP products in their projects. Your organization requires that all BigQuery data access logs be retained for 6 months. You need to ensure that only audit personnel in your company can access the data access logs for all projects. What should you do?","timestamp":"2020-03-22 08:58:00","isMC":true,"answers_community":["D (100%)"],"question_id":48,"unix_timestamp":1584863880,"question_images":[],"exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/17225-exam-professional-data-engineer-topic-1-question-141/","choices":{"B":"Export the data access logs via a project-level export sink to a Cloud Storage bucket in the Data Analysts' projects. Restrict access to the Cloud Storage bucket.","D":"Export the data access logs via an aggregated export sink to a Cloud Storage bucket in a newly created project for audit logs. Restrict access to the project that contains the exported logs.","C":"Export the data access logs via a project-level export sink to a Cloud Storage bucket in a newly created projects for audit logs. Restrict access to the project with the exported logs.","A":"Enable data access logs in each Data Analyst's project. Restrict access to Stackdriver Logging via Cloud IAM roles."},"discussion":[{"upvote_count":"30","comment_id":"186071","timestamp":"1616589600.0","content":"Answer D is correct. Aggregated log sink will create a single sink for all projects, the destination can be a google cloud storage, pub/sub topic, bigquery table or a cloud logging bucket. without aggregated sink this will be required to be done for each project individually which will be cumbersome.\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks","comments":[{"poster":"AzureDP900","upvote_count":"1","timestamp":"1688137500.0","content":"D is right","comment_id":"762730"}],"poster":"SteelWarrior"},{"comments":[{"poster":"daghayeghi","comment_id":"309190","content":"https://cloud.google.com/iam/docs/job-functions/auditing#scenario_operational_monitoring","upvote_count":"3","timestamp":"1631474580.0"},{"content":"The above link shows BigQuery as a sink for aggregated exports and not Cloud Storage.","timestamp":"1610521020.0","poster":"Rajuuu","comment_id":"133655","upvote_count":"3"}],"upvote_count":"12","poster":"[Removed]","comment_id":"68134","timestamp":"1601035440.0","content":"Correct: D\nhttps://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors"},{"comment_id":"1183091","poster":"pbtpratik","upvote_count":"1","content":"D is the correct ans","timestamp":"1727326980.0"},{"upvote_count":"1","timestamp":"1711255620.0","comment_id":"1015440","content":"Selected Answer: D\nD. \nHere's why this option is recommended:\nAggregated Export Sink: By using an aggregated export sink, you can consolidate data access logs from multiple projects into a single location. This simplifies log management and retention policies.\nNewly Created Project for Audit Logs: Creating a dedicated project for audit logs allows you to centralize access control and manage logs separately from individual Data Analyst projects.\nAccess Restriction: By restricting access to the project containing the exported logs, you ensure that only authorized audit personnel have access to the logs while preventing Data Analysts from accessing them.","poster":"barnac1es"},{"upvote_count":"1","timestamp":"1694584320.0","content":"Selected Answer: D\nTo create the Log Router, at step 3 to define the logs (Source), we can include logs from many projects (aggregated)","poster":"midgoo","comment_id":"837735"},{"comment_id":"732641","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks\nAggregated sinks combine and route log entries from the Google Cloud resources contained by an organization or folder. For instance, you might aggregate and route audit log entries from all the folders contained by an organization to a Cloud Storage bucket.","timestamp":"1685618040.0","upvote_count":"2","poster":"zellck"},{"upvote_count":"1","comment_id":"593500","content":"D is correct","poster":"dffffff","timestamp":"1666927740.0"},{"upvote_count":"4","timestamp":"1657382940.0","content":"Selected Answer: D\nD: https://cloud.google.com/logging/docs/export/aggregated_exports\nYou can create an aggregated export sink that can export log entries from all the projects, folders, and billing accounts of an organization. As an example, you might use this feature to export audit log entries from an organization's projects to a central location.","comment_id":"520387","poster":"MaxNRG"},{"timestamp":"1648769700.0","poster":"Chelseajcole","comment_id":"455183","upvote_count":"4","content":"The auditor needs to audit data analyst's behaviors (how they access multiple projects in BQ ). So, the key is, multiple projects. According to Google doc project-level sinks:\nhttps://cloud.google.com/logging/docs/export/configure_export_v2\nHowever, the Cloud Console can only create or view sinks in Cloud projects. To create sinks in organizations, folders, or billing accounts using the gcloud command-line tool or Cloud Logging API, see Aggregated sinks.\n\nObviously, the auditor needs to check all projects accessed by data analyst which is not project-level, a higher level like folder or organization level, this can only be done via the aggregate sink.\n\nSo D is the answer."},{"upvote_count":"6","content":"A - eliminated , because logs needs to be retained for 6 months (So, some storage require)\nB - eliminated, because if we store in same project then, Data Analyst can also access (But in question it's mention, ONLY audit personnel needs access)\nC - Wrong (No need to restrict project as well as logs separately) - wording does not look okay.\nD - Correct (If we restrict the project, then all resources get restricted)\n\nVote for D","comment_id":"398275","comments":[{"comment_id":"398350","content":"Option 'C' - I guess said - restrict access to the project with the exported logs. (i.e. restrict access of that project from where we took logs) - If I am not wrong... Thus it's INCORRECT","comments":[{"timestamp":"1658853840.0","upvote_count":"1","comment_id":"533150","content":"Sinks are different from Aggregate Sinks, refer https://cloud.google.com/logging/docs/export/configure_export_v2#api","poster":"at99"}],"upvote_count":"2","poster":"sumanshu","timestamp":"1641308760.0"}],"poster":"sumanshu","timestamp":"1641303600.0"},{"upvote_count":"3","poster":"septiandy","content":"what is the difference between C and D? I think it's same.","comment_id":"341007","timestamp":"1634901900.0","comments":[{"comment_id":"982755","upvote_count":"1","content":"I think the key difference is that D talks about aggregated sinks.","timestamp":"1708107240.0","poster":"FP77"}]},{"upvote_count":"3","timestamp":"1613999820.0","poster":"haroldbenites","comment_id":"163549","content":"D is correct"},{"content":"D is correct answer, refer below link for more information.","poster":"saurabh1805","upvote_count":"3","comment_id":"160971","timestamp":"1613668440.0"},{"timestamp":"1610382660.0","upvote_count":"5","poster":"VishalB","content":"Ans : D\n Aggregated Exports, which allows you to set up a sink at the Cloud IAM organization or folder level, and export logs from all the projects inside the organization or folder.","comment_id":"132157"},{"comment_id":"66859","upvote_count":"4","timestamp":"1600754280.0","content":"Answer D","poster":"[Removed]"}],"answer_ET":"D","topic":"1","answer_description":"","answer_images":[],"answer":"D"},{"id":"OeGu9WkJBirN5dpwC7eM","exam_id":11,"question_id":49,"choices":{"C":"Create a log export for each project, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric","B":"Create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project","A":"Create a Cloud Monitoring dashboard based on the BigQuery metric query/scanned_bytes","D":"Create an aggregated log export at the organization level, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric"},"answer_ET":"B","question_text":"Each analytics team in your organization is running BigQuery jobs in their own projects. You want to enable each team to monitor slot usage within their projects.\nWhat should you do?","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/81914-exam-professional-data-engineer-topic-1-question-142/","question_images":[],"topic":"1","answer":"B","answer_images":[],"timestamp":"2022-09-13 04:43:00","unix_timestamp":1663036980,"discussion":[{"upvote_count":"1","comment_id":"1183092","poster":"pbtpratik","content":"B is correct answer","timestamp":"1727327040.0"},{"comment_id":"1099794","content":"Selected Answer: B\nViewing project and reservation slot usage in Stackdriver Monitoring\nInformation is available from the \"Slots Allocated\" metric in Stackdriver Monitoring. This metric information includes a per-reservation and per-job breakdown of slot usage. The information can also be visualized by using the custom charts metric explorer.\nhttps://cloud.google.com/bigquery/docs/reservations-monitoring\nhttps://cloud.google.com/monitoring/api/metrics_gcp","upvote_count":"2","poster":"MaxNRG","timestamp":"1718714460.0","comments":[{"timestamp":"1737907980.0","content":"this is stols/allocated, not slots/allocated_for_project","comment_id":"1346995","poster":"keisoes","upvote_count":"1"}]},{"poster":"barnac1es","upvote_count":"3","content":"Selected Answer: B\nThe slots/allocated_for_project metric provides information about the number of slots allocated to each project. It directly reflects the slot usage, making it a relevant and accurate metric for monitoring slot allocation within each project.\n\nOptions A, C, and D involve log exports and custom metrics, but they may not be as straightforward or provide the same level of detail as the built-in metric slots/allocated_for_project:","comment_id":"1015443","timestamp":"1711255860.0"},{"timestamp":"1710938580.0","comment_id":"1012220","content":"The naming is quite misleading in this case, but it actually seems from the documentation that slots/allocated_for_project indicates the \"slots used by project\", in which case answer B is correct: https://cloud.google.com/monitoring/api/metrics_gcp#:~:text=slots/allocated_for_project%20GA%0ASlots%20used%20by%20project","upvote_count":"2","poster":"ckanaar"},{"content":"Selected Answer: D\nB slots/allocated_for_project will give you the total number of slots allocated to each project, but it will not tell you how many slots are actually being used.\n\nThe purpose to monitor 'slot usgae' is for billing. 'slot/allocated' means nothing.\nOption D is better than B.\n\nAnd, the question mention 'Each analytics team in organization', so it should be 'organization level'.","timestamp":"1708423620.0","poster":"arien_chen","upvote_count":"1","comment_id":"985634"},{"timestamp":"1695522300.0","comment_id":"848944","poster":"midgoo","upvote_count":"1","content":"Selected Answer: D\nIf 'usage' = how the slots are being used, D is the corret answer\nIf 'usage' = how the slots are being allocated, B is the correct answer\n\nI think in this question, usage = how the slots are being used"},{"poster":"musumusu","upvote_count":"1","content":"Answer B, \nWhy not D, aggregated log export is good but it will generate all the details which is large in size and costly too. you dont need all the information. It can break data privacy. so look for B because this much is asked only. Normally, i make such errors alot.","timestamp":"1692300900.0","comment_id":"812403"},{"content":"Selected Answer: B\nThe correct answer is B. You should create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project.\n\nThis metric represents the number of BigQuery slots allocated for a project. By creating a Cloud Monitoring dashboard based on this metric, you can monitor the slot usage within each project in your organization. This will allow each team to monitor their own slot usage and ensure that they are not exceeding their allocated quota.\n\nOption A is incorrect because the query/scanned_bytes metric represents the number of bytes scanned by BigQuery queries, not the slot usage.\n\nOption C is incorrect because it involves creating a log export for each project and using a custom metric based on the totalSlotMs field. While this may be a valid way to monitor slot usage, it is more complex than simply using the slots/allocated_for_project metric.\n\nOption D is also incorrect because it involves creating an aggregated log export at the organization level, which is not necessary for monitoring slot usage within individual projects.","poster":"saurabhsingh4k","upvote_count":"4","comment_id":"751867","timestamp":"1687314060.0"},{"content":"vote for B","comment_id":"675993","timestamp":"1679489280.0","upvote_count":"2","poster":"dn_mohammed_data"},{"poster":"John_Pongthorn","comment_id":"667600","upvote_count":"4","content":"B ,the another is related to the question as well.\nhttps://cloud.google.com/bigquery/docs/reservations-monitoring#viewing-slot-usage","timestamp":"1678682940.0"},{"comments":[{"upvote_count":"1","comment_id":"762731","content":"B. Create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project","timestamp":"1688137560.0","poster":"AzureDP900"}],"content":"Selected Answer: B\nB the below is related to the question.\nhttps://cloud.google.com/blog/topics/developers-practitioners/monitoring-bigquery-reservations-and-slot-utilization-information_schema","timestamp":"1678682580.0","comment_id":"667597","poster":"John_Pongthorn","upvote_count":"4"}],"isMC":true,"answers_community":["B (87%)","13%"]},{"id":"EFnFmWVRFrXyp8Q46oVY","url":"https://www.examtopics.com/discussions/google/view/79678-exam-professional-data-engineer-topic-1-question-143/","choices":{"C":"Stop the Cloud Dataflow pipeline with the Cancel option. Create a new Cloud Dataflow job with the updated code","B":"Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to a new unique job name","D":"Stop the Cloud Dataflow pipeline with the Drain option. Create a new Cloud Dataflow job with the updated code","A":"Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to the existing job name"},"answer_ET":"D","topic":"1","isMC":true,"unix_timestamp":1662180240,"discussion":[{"upvote_count":"15","poster":"odacir","content":"Selected Answer: D\nIt's D. → Your engineers have a new version of the pipeline with a different windowing algorithm and triggering strategy. \nNew version is mayor changes. Stop and drain and then launch the new code is a lot is the safer way. \nWe recommend that you attempt only smaller changes to your pipeline's windowing, such as changing the duration of fixed- or sliding-time windows. Making major changes to windowing or triggers, like changing the windowing algorithm, might have unpredictable results on your pipeline output.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing","comment_id":"739061","timestamp":"1686221280.0","comments":[{"upvote_count":"2","comment_id":"745375","poster":"maggieee","content":"Since updating the job as in A does a compatibility check, wouldn't you want to try that first? Then if the compatibility check fails then you proceed to drain current pipeline and then launch new pipeline (Answer D)?\n\nAs in A would be correct answer, then if compatibility check fails, you proceed to D. \n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#CCheck","timestamp":"1686760860.0","comments":[{"poster":"ckanaar","timestamp":"1711034280.0","comment_id":"1013102","upvote_count":"1","content":"You're right in your reasoning, but since the documentation specifically uses this example for stopping and draining, it's safe to assume that the compatibility check will always fail with these adjustments. Therefore, we can go straight to D. \n\nFurthermore, answer A doesn't state: \"Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to the existing name, if the compatibility check fails, THEN proceed to stopping the pipeline with the drain option\", so in itself it is not the right answer if the check fails."}]}]},{"content":"Selected Answer: D\nD seems the right way to go","timestamp":"1719745260.0","comment_id":"1109734","poster":"patitonav","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nOption A is the first approach to try, as it allows for an in-flight update with minimal disruption. However, if the changes in the new version of the pipeline are not compatible with an in-flight update (due to significant changes in windowing or triggering), then option D should be used. The Drain option ensures a graceful shutdown of the existing pipeline, reducing the risk of data loss, and then a new job can be started with the updated code.","comment_id":"1104475","poster":"TVH_Data_Engineer","timestamp":"1719203580.0"},{"upvote_count":"1","comment_id":"1099828","timestamp":"1718718000.0","content":"Selected Answer: D\nA is not an option as \"You want to ensure that no data is lost during the update. \": \nMaking major changes to windowing or triggers, like changing the windowing algorithm, might have unpredictable results on your pipeline output.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#change_windowing","poster":"MaxNRG"},{"timestamp":"1711256760.0","comment_id":"1015447","content":"Selected Answer: D\nDrain Option: The \"Drain\" option allows the existing Dataflow job to complete processing of any in-flight data before stopping the job. This ensures that no data is lost during the transition to the new version.\nCreate a New Job: After draining the existing job, you create a new Cloud Dataflow job with the updated code. This new job starts fresh and continues processing data from where the old job left off.\n\nOption A (updating the inflight pipeline with the --update option) may not guarantee no data loss, as the update could disrupt the existing job's operation and potentially cause data loss.\n\nOption B (updating the inflight pipeline with the --update option and a new job name) is similar to option A and may not provide data loss guarantees.\n\nOption C (stopping the pipeline with the Cancel option and creating a new job) will abruptly stop the existing job without draining, potentially leading to data loss.","upvote_count":"1","poster":"barnac1es"},{"comment_id":"963660","poster":"knith66","content":"Look D after seeing some docs. please check the below link https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline","upvote_count":"1","timestamp":"1706275680.0"},{"upvote_count":"1","comment_id":"963297","content":"Selected Answer: D\nI will go with option D - If you want to minimize the impact of the update, then option A is the best option. However, if you are not concerned about a temporary interruption in processing, then option D is also a valid option. Option Pros Cons\nA Does not stop the pipeline, so no data is lost. Requires you to create a new version of the pipeline.\nB Creates a new job with the updated code, so you do not have to update the running pipeline. Can lead to data loss if the new job does not process all of the data that was in the running pipeline.\nC Stops the pipeline and drains any data that is currently in flight, so no data is lost. Causes a temporary interruption in processing.","poster":"vamgcp","timestamp":"1706245860.0"},{"upvote_count":"3","timestamp":"1694587320.0","poster":"midgoo","content":"Selected Answer: D\nA is not recommeded for major changes in pipeline.","comment_id":"837774"},{"upvote_count":"1","timestamp":"1692301740.0","comments":[{"poster":"musumusu","comment_id":"820886","upvote_count":"3","content":"Answer D: as per latest documents 02/2023 google has removed update flag.","timestamp":"1692899700.0"}],"content":"Answer A:\n```gcloud dataflow jobs update <JOB_ID> --update <GCS_PATH_TO_UPDATED_PIPELINE> --region <REGION>```\n--update flag does not miss any data and you can execute this command even yourpipeline is running. Its safe any fast, you can continuously make some change and update this command. no problem.\nStop and Drain, is required when you want to test the pipeline and stop it without losing the data.","comment_id":"812424","poster":"musumusu"},{"comment_id":"753198","content":"Selected Answer: D\nagree with odacir","poster":"jkhong","timestamp":"1687423920.0","upvote_count":"4"},{"comment_id":"734224","timestamp":"1685763480.0","content":"Selected Answer: A\nvote A\nD: drain doesn't mention about update dataflow job just stop and preserve data\nA: replace existing job and preserve data\n(When you update your job, the Dataflow service performs a compatibility check between your currently-running job and your potential replacement job. The compatibility check ensures that things like intermediate state information and buffered data can be transferred from your prior job to your replacement job.)\n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline","upvote_count":"2","poster":"hauhau"},{"comment_id":"732634","timestamp":"1685617680.0","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Launching\nTo update your job, launch a new job to replace the ongoing job. When you launch your replacement job, set the following pipeline options to perform the update process in addition to the job's regular options:\n- Pass the --update option.\n- Set the --jobName option in PipelineOptions to the same name as the job you want to update.","poster":"zellck","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1686221340.0","poster":"odacir","comment_id":"739063","content":"Are mayor changes. It's not safe to update. I vote D."}]},{"content":"D\nA-is not because The Dataflow service retains the job name, but runs the replacement job with an updated Job ID. \nDescription:\nWhen you update a job on the Dataflow service, you replace the existing job with a new job that runs your updated pipeline code. The Dataflow service retains the job name, but runs the replacement job with an updated Job ID. This process can cause downtime while the existing job stops, the compatibility check runs, and the new job starts.'\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python:~:text=When%20you%20update%20a,has%20the%20following%20transforms%3A\nD is correct\nDrain ->clone -> update -> run","comment_id":"727307","upvote_count":"1","poster":"Atnafu","comments":[{"upvote_count":"1","content":"Changed my mind to A\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python_2:~:text=Set%20the%20%2D%2Djob_name,%2D%2Dtransform_name_mapping%20option.","poster":"Atnafu","comment_id":"727324","timestamp":"1685075520.0"}],"timestamp":"1685074320.0"},{"poster":"drunk_goat82","content":"Selected Answer: D\nChanging windowing algorithm may break the pipeline.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing","upvote_count":"3","timestamp":"1684938000.0","comment_id":"726036"},{"timestamp":"1684813980.0","upvote_count":"1","content":"Selected Answer: A\nNo, do not drain the current job.","poster":"ovokpus","comment_id":"724918"},{"timestamp":"1684555500.0","comments":[{"timestamp":"1684555680.0","comment_id":"722422","poster":"dish11dish","content":"If the pipeline was batch then ans would been A","upvote_count":"1"}],"poster":"dish11dish","comment_id":"722420","upvote_count":"1","content":"Selected Answer: D\nin this scenario pipline is streaming pipline with windowing algorithm and triggering strategy changes to new one without loss of data,so better to go with Drain option as it fullfile all precondition described in scenario which is :-\n1.streaming\n2.code changes with windowing algorithm and triggering strategy to new way\n3.no loss of data during update \n\nReferances:-\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain\nDrain a job. This method applies only to streaming pipelines. Draining a job enables the Dataflow service to finish processing the buffered data while simultaneously ceasing the ingestion of new data. For more information, see Draining a job."},{"upvote_count":"3","timestamp":"1683465300.0","poster":"Mcloudgirl","content":"D: They want to preserve data and updates might not be predictable.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing","comment_id":"713132"},{"poster":"cloudmon","timestamp":"1683401940.0","content":"Selected Answer: A\nIt's A (https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#UpdateMechanics)\nD would stop the pipeline, leading to loss of new data that would be sent into the pipeline before you start the new pipeline.","upvote_count":"3","comment_id":"712665"},{"comment_id":"704848","timestamp":"1682525160.0","upvote_count":"3","content":"The google documentation did mention that if the windowing or triggering algorithm changes, you might have unpredictable results. This question did mention change in windowing and triggering algorithm. \nSo A might lead to unpredictable results. \nSafe bet is D. \nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline","poster":"Azlijaffar"},{"comment_id":"696605","content":"Selected Answer: D\nD of course. Also you can only update for minor changes on windowing/triggering. Question say a different strategy.","timestamp":"1681682220.0","upvote_count":"4","poster":"devaid"},{"comment_id":"693778","comments":[{"poster":"Duckjai","comment_id":"694577","content":"Agree. It is A","timestamp":"1681457460.0","upvote_count":"1"}],"poster":"josrojgra","upvote_count":"4","timestamp":"1681379460.0","content":"Selected Answer: A\nThe answer is A.\nThe question says \"update\", and on the documentation (https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#UpdateMechanics) says explicitly on the second paragraph this: \nThe replacement job preserves any intermediate state data from the prior job, as well as any buffered data records or metadata currently \"in-flight\" from the prior job. For example, some records in your pipeline might be buffered while waiting for a window to resolve.\n\nSo the update process prevent data loss."},{"comment_id":"675994","upvote_count":"2","timestamp":"1679489400.0","content":"D : drain to ensure that there is no data loss during the code update","poster":"dn_mohammed_data"},{"comment_id":"666277","timestamp":"1678559400.0","upvote_count":"2","poster":"MounicaN","content":"Selected Answer: D\ndrain for graceful shutdown"},{"timestamp":"1678089540.0","comment_id":"660858","poster":"sanjithj","content":"Selected Answer: D\nDrain option is the correct one","upvote_count":"3"},{"poster":"damaldon","content":"Drain is the only option","timestamp":"1677849240.0","upvote_count":"1","comment_id":"658372"},{"poster":"ducc","timestamp":"1677825840.0","upvote_count":"3","content":"Selected Answer: D\nIf you want to prevent data loss as you bring down your streaming pipelines, the best option is to drain your job.","comment_id":"658073"}],"question_text":"You are operating a streaming Cloud Dataflow pipeline. Your engineers have a new version of the pipeline with a different windowing algorithm and triggering strategy. You want to update the running pipeline with the new version. You want to ensure that no data is lost during the update. What should you do?","answer":"D","answer_description":"","answer_images":[],"question_id":50,"answers_community":["D (80%)","A (20%)"],"question_images":[],"timestamp":"2022-09-03 06:44:00","exam_id":11}],"exam":{"lastUpdated":"11 Apr 2025","isImplemented":true,"id":11,"name":"Professional Data Engineer","isBeta":false,"isMCOnly":true,"numberOfQuestions":319,"provider":"Google"},"currentPage":10},"__N_SSP":true}