{"pageProps":{"questions":[{"id":"ryDnsUehlmWji58vyaY2","answers_community":["D (100%)"],"timestamp":"2024-01-13 15:30:00","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/131093-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1705156200,"choices":{"A":"Deploy the model to BigQuery ML by using CREATE MODEL with the BOOSTED_TREE_REGRESSOR statement, and invoke the BigQuery API from the microservice.","B":"Build a Flask-based app. Package the app in a custom container on Vertex AI, and deploy it to Vertex AI Endpoints.","C":"Build a Flask-based app. Package the app in a Docker image, and deploy it to Google Kubernetes Engine in Autopilot mode.","D":"Use a prebuilt XGBoost Vertex container to create a model, and deploy it to Vertex AI Endpoints."},"answer_description":"","topic":"1","question_text":"You recently trained an XGBoost model on tabular data. You plan to expose the model for internal use as an HTTP microservice. After deployment, you expect a small number of incoming requests. You want to productionize the model with the least amount of effort and latency. What should you do?","answer_ET":"D","answer":"D","question_images":[],"answer_images":[],"discussion":[{"timestamp":"1705156200.0","upvote_count":"7","content":"Selected Answer: D\nPrebuilt Container: It eliminates the need to build and manage a custom container, reducing development time and complexity.\nVertex AI Endpoints: It provides a managed serving infrastructure with low latency and high availability, optimizing performance for predictions.\nMinimal Effort: It involves simple steps of creating a Vertex model and deploying it to an endpoint, streamlining the process.","poster":"pikachu007","comment_id":"1121735"},{"content":"Selected Answer: D\nBit lost here. I would discard buiding a Flask app since that is the opposite of \"minimum effort\". Between A and D, I guess a prebuilt container (D) involves less effort, but I am not 100% confident.","poster":"b1a8fae","upvote_count":"5","comment_id":"1128583","timestamp":"1705925700.0"},{"content":"Option D is correct : \nUsing a prebuilt XGBoost Vertex container (Option D) is the most straightforward approach. This container is specifically designed for running XGBoost models in production environments and can be easily deployed to Vertex AI Endpoints. This will allow you to expose your model as an HTTP microservice with minimal additional work.","upvote_count":"1","comment_id":"1242787","timestamp":"1720184760.0","poster":"AzureDP900"},{"content":"Selected Answer: D\nPackage the Model: Use a library like xgboost-server to create a minimal server for your XGBoost model. This package helps convert your model into a format suitable for serving predictions through an HTTP endpoint.\nDeploy to Cloud Functions: Deploy the packaged model server as a Cloud Function on Google Cloud Platform (GCP). Cloud Functions are serverless, lightweight execution environments ideal for event-driven applications like microservices.\nConfigure Trigger: Set up an HTTP trigger for your Cloud Function, allowing it to be invoked through HTTP requests.","poster":"fitri001","upvote_count":"2","comment_id":"1196894","timestamp":"1713312720.0"}],"exam_id":13,"question_id":176},{"id":"89dJ7X1l4afT0SiQf6rb","topic":"1","url":"https://www.examtopics.com/discussions/google/view/131094-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"question_id":177,"answer_ET":"C","answer_images":[],"answer":"C","question_images":[],"discussion":[{"timestamp":"1705993860.0","comment_id":"1129259","upvote_count":"6","poster":"daidai75","content":"Selected Answer: C\nThe answer is C, to use Translation Hub\n1.Accuracy maximization: AutoML Translation uses machine learning to train a translation model on your specific data, which can lead to higher accuracy compared to generic translation models.\n2.Minimal operational overhead: AutoML Translation handles the training and deployment of the translation model, reducing the need for manual intervention.\n3.Evaluation and correction: The solution includes human reviewers to evaluate and correct any incorrect translations, ensuring high quality."},{"timestamp":"1741117500.0","comment_id":"1365089","poster":"Wuthuong1234","upvote_count":"1","content":"Selected Answer: C\nMy first instinct was to go for A, but after reading through the question in detail, I think the right answer is C.\nIt is mentioned that we are dealing with instructions for scientific products. The implication is that the instructions will therefore use very complicated and niche language, which the Natural Language API will most likely struggle to understand properly. AutoML Translations is meant to be for these types of tasks where the language is very domain-specific: https://cloud.google.com/translate/docs/advanced/automl-beginner"},{"comment_id":"1320658","upvote_count":"2","content":"Selected Answer: A\nA - It uses the most managed services which reduces operational overhead. Translation API has good translations that's improving as Google improves its translation services.","poster":"juliorevk","timestamp":"1733078040.0"},{"poster":"DaleR","timestamp":"1732681380.0","content":"Selected Answer: B\nYou want to minimize operational overhead.","comment_id":"1318421","upvote_count":"1"},{"comment_id":"1242802","timestamp":"1720185300.0","poster":"AzureDP900","upvote_count":"1","content":"Using AutoML Translation (Option C) allows you to train a model on your data, which can be used for translation. You can then configure a Translation Hub project to manage the translation process and use human reviewers to evaluate any incorrect translations.\nIt is scalable solution that maximizes accuracy and minimizes operational overhead."},{"content":"Selected Answer: C\nif we assume there is training data available (source-target language pairs) then I would go with C.","poster":"gscharly","comment_id":"1199247","timestamp":"1713630060.0","upvote_count":"1"},{"content":"Selected Answer: C\nOption A: Cloud Functions are suitable for simple tasks. This approach wouldn't leverage machine learning for improved translations and lacks features like model evaluation and retraining.\nOption B: Vertex AI pipelines with AutoML Translation training can be powerful, but it might be overkill for this scenario. Additionally, retraining based on a predetermined data skew might not be necessary if human review is effective at catching and correcting errors.\nOption D: While fine-tuning a pre-trained model with Vertex AI custom training offers flexibility, it requires more expertise and ongoing maintenance compared to the simpler approach of using AutoML Translation.","comment_id":"1196895","timestamp":"1713313200.0","poster":"fitri001","upvote_count":"3"},{"poster":"b2aaace","content":"Answer A\n\nIt is the only option that makes sense all over. I would go for C if the first sentence was \nnot there \"Use AutoML Translation\". you can't use autoML because there is no training data.","comment_id":"1194935","timestamp":"1713009540.0","upvote_count":"4"},{"timestamp":"1712996040.0","comments":[{"timestamp":"1712996040.0","comment_id":"1194790","content":"Why not B: Retraining the model upon data skew detection can become cumbersome and impact translation speed. Translation Hub offers a more streamlined approach for managing model updates.","poster":"omermahgoub","upvote_count":"1"}],"poster":"omermahgoub","upvote_count":"2","content":"Selected Answer: C\nC: Use AutoML Translation with Translation Hub. Here's why:\n1. Scalability: \n - AutoML Translation: This simplifies model training without extensive manual configuration. \n- Translation Hub: Centrally stores and manages your translation models, facilitating deployment and reuse across various applications, promoting scalability for your 15 target languages.\n2. Accuracy and Evaluation:\n- AutoML Translation: while pre-trained models might not be perfect, AutoML Translation lets you fine-tune the model with your specific scientific domain data (instruction manuals) to improve accuracy.\n- Human Review and Iteration: This allows for evaluation and correction of any inaccurate translations, improving overall quality. This is crucial for technical documents like instruction manuals.","comment_id":"1194789"},{"comment_id":"1192594","upvote_count":"1","content":"Selected Answer: C\nTranslation Hub can manage translation workloads at scale and also integrate human feedback where required.","timestamp":"1712711400.0","poster":"emsherff"},{"timestamp":"1710184020.0","content":"So what is the deal? pikachu007 authors the question, adds C as suggested answer and then vote for B?","upvote_count":"2","comment_id":"1171256","poster":"edoo"},{"upvote_count":"1","poster":"Sunny_M","comment_id":"1156843","content":"Selected Answer: B\nAgree with pikachu007, I think there is no point in using ML once the manual(human) mode is added.","timestamp":"1708654740.0"},{"upvote_count":"3","poster":"b1a8fae","timestamp":"1705926540.0","content":"Selected Answer: C\nTranslation Hub is a service that allows you to manage and automate your translation workflows on Google Cloud. You can use Translation Hub to upload the documents to a Cloud Storage bucket, select the source and target languages, and apply the trained model to translate the documents. You can use human reviewers to improve the quality and accuracy of the translations, and provide feedback to the ML model.","comment_id":"1128592"},{"timestamp":"1705156440.0","content":"Selected Answer: B\nOption A: While Cloud Functions provide automation, the Cloud Translation API uses generic models that might not be as accurate for domain-specific content, potentially leading to more human corrections.\nOption C: Translation Hub offers collaboration features but lacks automated model training and pipeline orchestration, requiring more manual effort.\nOption D: Vertex AI custom training jobs provide flexibility but require more expertise and effort compared to AutoML Translation, and the pre-trained model might not be as well-suited for the specific domain.","comment_id":"1121741","poster":"pikachu007","upvote_count":"2"}],"question_text":"You work for an international manufacturing organization that ships scientific products all over the world. Instruction manuals for these products need to be translated to 15 different languages. Your organization’s leadership team wants to start using machine learning to reduce the cost of manual human translations and increase translation speed. You need to implement a scalable solution that maximizes accuracy and minimizes operational overhead. You also want to include a process to evaluate and fix incorrect translations. What should you do?","answer_description":"","timestamp":"2024-01-13 15:34:00","unix_timestamp":1705156440,"choices":{"B":"Create a Vertex AI pipeline that processes the documents launches, an AutoML Translation training job, evaluates the translations and deploys the model to a Vertex AI endpoint with autoscaling and model monitoring. When there is a predetermined skew between training and live data, re-trigger the pipeline with the latest data.","D":"Use Vertex AI custom training jobs to fine-tune a state-of-the-art open source pretrained model with your data. Deploy the model to a Vertex AI endpoint with autoscaling and model monitoring. When there is a predetermined skew between the training and live data, configure a trigger to run another training job with the latest data.","A":"Create a workflow using Cloud Function triggers. Configure a Cloud Function that is triggered when documents are uploaded to an input Cloud Storage bucket. Configure another Cloud Function that translates the documents using the Cloud Translation API, and saves the translations to an output Cloud Storage bucket. Use human reviewers to evaluate the incorrect translations.","C":"Use AutoML Translation to train a model. Configure a Translation Hub project, and use the trained model to translate the documents. Use human reviewers to evaluate the incorrect translations."},"isMC":true,"answers_community":["C (74%)","B (17%)","9%"]},{"id":"z5mDOk8QpnhleZyOoAkO","url":"https://www.examtopics.com/discussions/google/view/131096-exam-professional-machine-learning-engineer-topic-1-question/","question_images":["https://img.examtopics.com/professional-machine-learning-engineer/image5.png"],"discussion":[{"poster":"AzureDP900","timestamp":"1720185960.0","comment_id":"1242805","upvote_count":"3","content":"Option C is right because: \n1)Exposing individual models as Vertex AI Endpoints (Option C) allows for version tracking, which is essential for maintaining consistency across different workflows.\n2)Using Cloud Run to orchestrate the workflow (Option C) enables you to scale down to zero and minimize compute resource utilization.\n3)You want to deploy your application while ensuring version control for each individual model and the overall workflow."},{"comment_id":"1199252","poster":"gscharly","upvote_count":"1","content":"Selected Answer: C\nB,D not correct since BQ is not the best approach.\nA would require more manual work","timestamp":"1713630180.0"},{"comment_id":"1152244","poster":"guilhermebutzke","upvote_count":"4","timestamp":"1708121220.0","content":"My Answer: C\n\nB and D: Not Correct: Big query is not the best approach to trach versions of model. \n\nA and C: Looking for “ensuring version control for each individual mode” (endpoints), and “be able to scale down to zero”, “minimize the compute resource utilization and the manual effort required to manage this solution”, I think to use Cloud Run could be the best option for those cases.\n\nhttps://www.youtube.com/watch?v=nhwYc4StHIc&ab_channel=GoogleCloudTech"},{"content":"Selected Answer: C\nOption A: A custom container endpoint for orchestration adds complexity and management overhead.\nOption B: Loading model files directly into a custom container endpoint can lead to versioning challenges and potential conflicts if models are shared across workflows.\nOption D: Using BigQuery for model versioning is not its primary function and might introduce complexities in model loading and management.","timestamp":"1705156560.0","upvote_count":"4","poster":"pikachu007","comment_id":"1121744"}],"unix_timestamp":1705156560,"timestamp":"2024-01-13 15:36:00","question_id":178,"topic":"1","choices":{"B":"Create a custom container endpoint for the workflow that loads each model’s individual files Track the versions of each individual model in BigQuery.","A":"Expose each individual model as an endpoint in Vertex AI Endpoints. Create a custom container endpoint to orchestrate the workflow.","C":"Expose each individual model as an endpoint in Vertex AI Endpoints. Use Cloud Run to orchestrate the workflow.","D":"Load each model’s individual files into Cloud Run. Use Cloud Run to orchestrate the workflow. Track the versions of each individual model in BigQuery."},"answer_ET":"C","answers_community":["C (100%)"],"answer_description":"","question_text":"You have developed an application that uses a chain of multiple scikit-learn models to predict the optimal price for your company’s products. The workflow logic is shown in the diagram. Members of your team use the individual models in other solution workflows. You want to deploy this workflow while ensuring version control for each individual model and the overall workflow. Your application needs to be able to scale down to zero. You want to minimize the compute resource utilization and the manual effort required to manage this solution. What should you do?\n\n//IMG//","answer":"C","isMC":true,"exam_id":13,"answer_images":[]},{"id":"SbVJzpxMEfoO2lsL8dEa","answers_community":["D (94%)","6%"],"isMC":true,"timestamp":"2021-06-02 23:00:00","url":"https://www.examtopics.com/discussions/google/view/54316-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1622667600,"choices":{"C":"Apache Flink","A":"Dataflow","D":"Cloud Data Fusion","B":"Dataprep"},"topic":"1","answer_description":"","question_text":"You are responsible for building a unified analytics environment across a variety of on-premises data marts. Your company is experiencing data quality and security challenges when integrating data across the servers, caused by the use of a wide range of disconnected tools and temporary solutions. You need a fully managed, cloud-native data integration service that will lower the total cost of work and reduce repetitive work. Some members on your team prefer a codeless interface for building Extract, Transform, Load (ETL) process. Which service should you use?","answer_ET":"D","answer":"D","question_images":[],"answer_images":[],"discussion":[{"timestamp":"1638486000.0","poster":"[Removed]","upvote_count":"14","content":"D. correct.\nReference: https://cloud.google.com/data-fusion","comment_id":"373056"},{"timestamp":"1733498160.0","content":"Selected Answer: D\nD) Cloud Data Function","upvote_count":"1","poster":"PhilipKoku","comment_id":"1225529"},{"upvote_count":"2","timestamp":"1728912900.0","poster":"pinimichele01","content":"Selected Answer: D\ncodeless interface -> D","comment_id":"1195539"},{"upvote_count":"1","poster":"Sum_Sum","content":"Selected Answer: D\nD is correct","timestamp":"1715780040.0","comment_id":"1071581"},{"timestamp":"1704632880.0","comment_id":"945583","content":"Selected Answer: D\nI think D is correct.","poster":"SamuelTsch","upvote_count":"1"},{"content":"Selected Answer: D\nWent with D","timestamp":"1699513260.0","poster":"M25","upvote_count":"1","comment_id":"892703"},{"upvote_count":"1","content":"Selected Answer: B\nAnswer is B","poster":"FDS1993","timestamp":"1693988400.0","comment_id":"830718"},{"upvote_count":"2","comment_id":"824949","content":"Selected Answer: D\nCloud Data Fusion is a fully managed, cloud-native data integration service provided by Google Cloud Platform. It is designed to simplify the process of building and managing ETL pipelines across a variety of data sources and targets.","poster":"Fatiy","timestamp":"1693227660.0"},{"content":"Selected Answer: D\n\"codeless interface\" ==> Data Fusion","timestamp":"1684916280.0","poster":"EFIGO","upvote_count":"3","comment_id":"725732"},{"upvote_count":"1","timestamp":"1676472840.0","content":"Selected Answer: D\nCorrect answer is \"D\"","comment_id":"647204","poster":"GCP72"},{"content":"Selected Answer: D\nD is correct as it is codeless","comment_id":"633668","upvote_count":"1","timestamp":"1674151200.0","poster":"capt2101akash"},{"upvote_count":"1","timestamp":"1670887380.0","comment_id":"615501","poster":"Mohamed_Mossad","content":"Selected Answer: D\nhttps://cloud.google.com/data-fusion/docs/concepts/overview#using_the_code-free_web_ui"},{"poster":"morgan62","comment_id":"582242","content":"Selected Answer: D\nD without any doubt","upvote_count":"2","timestamp":"1665125700.0"},{"timestamp":"1658966760.0","content":"D.\nDatafusion is more designed for data ingestion from one source to another one, with few transformation. Dataprep is more designed for data preparation (as its name means), data cleaning, new column creation, splitting column. Dataprep also provide insight of the data for helping you in your recipes.","poster":"xiaoF","comment_id":"534271","upvote_count":"3"},{"upvote_count":"2","comment_id":"467476","content":"D. Dataprep would also work but Data Fusion is better suited.\n(See https://stackoverflow.com/questions/58175386/can-google-data-fusion-make-the-same-data-cleaning-than-dataprep)","poster":"majejim435","timestamp":"1650893760.0"},{"comment_id":"464777","upvote_count":"4","timestamp":"1650391620.0","content":"D is correct\n\nVisual point-and-click interface enabling code-free deployment of ETL/ELT data pipelines and Operate high-volumes of data pipelines periodically\n\nsource: https://cloud.google.com/data-fusion#all-features","poster":"mousseUwU"},{"poster":"raintree","comment_id":"443340","upvote_count":"2","timestamp":"1647077580.0","content":"B. Dataprep makes use of Apache beam, which can process streaming and batch, and thus prevent training-serving skew."}],"exam_id":13,"question_id":179},{"id":"SCL2GoPPZh0ew5bSleEE","unix_timestamp":1705411380,"question_images":[],"answer_ET":"B","answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/131304-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["C (50%)","B (50%)"],"discussion":[{"poster":"gscharly","content":"Selected Answer: C\nlog_time_series_metrics requires setting Tensorboard: https://cloud.google.com/vertex-ai/docs/experiments/log-data\n\nassign_input_artifacts can be used to track input data: https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/get_started_with_vertex_experiments.ipynb","upvote_count":"6","comment_id":"1198366","timestamp":"1713508020.0"},{"poster":"Dirtie_Sinkie","timestamp":"1726651260.0","comment_id":"1285663","upvote_count":"1","content":"Selected Answer: C\nC sounds more correct"},{"upvote_count":"3","comments":[{"content":"In A, there's a typo in log_merrics (should be log_metrics). Therefore A is incorrect.","poster":"rajshiv","upvote_count":"1","comment_id":"1320974","timestamp":"1733148360.0"}],"comment_id":"1253415","poster":"tungdeptraiqua","content":"Selected Answer: B\nA and B are the same","timestamp":"1721709240.0"},{"upvote_count":"4","poster":"fitri001","comment_id":"1203029","timestamp":"1714208100.0","comments":[{"timestamp":"1714208100.0","upvote_count":"2","comment_id":"1203030","poster":"fitri001","content":"Option A: It lacks the functionality to log preprocessed data (no log_time_series_metrics).\nOption C and D: While TensorBoard can be used for visualization, it's not directly related to logging data within Vertex AI Experiments.\n\npen_spark\nexclamation Additionally, assign_input_artifact isn't the correct method for logging time series data"}],"content":"Selected Answer: B\nVertex AI Experiment and ML Metadata: This is the foundation for tracking experiments and artifacts within Vertex AI.expand_more Creating an experiment allows you to group related runs and log data associated with those runs. ML Metadata helps manage the lineage of data and models used in your experiments.expand_more\n\nLogging Data:\n\nlog_time_series_metrics: This function is specifically designed for tracking time-series data, making it suitable for logging the preprocessed multivariate time series data in your experiment.\nlog_metrics: This function is appropriate for logging loss values during model training. It can handle numerical values like loss efficiently.\nBy combining these techniques, you can effectively track both the preprocessed data (time series) and the training performance metrics (loss values) within your Vertex AI Experiment."},{"timestamp":"1712996460.0","content":"Selected Answer: B\nWhy B?\n1. Experiment Creation: Vertex AI SDK establishes a context for grouping your training runs and facilitates experiment management.\n2. By setting up Vertex ML Metadata (only can be done when creating an experiment with the Vertex AI SDK), you enable tracking of artifacts and metrics associated with each experiment run.\n3. log_time_series_metrics function is well-suited for tracking the preprocessed multivariate time series data associated with each experiment run. This allows you to analyze how preprocessing impacts model performance.","upvote_count":"2","comment_id":"1194797","poster":"omermahgoub"},{"poster":"Yan_X","timestamp":"1711707240.0","upvote_count":"2","comment_id":"1185348","content":"Selected Answer: B\nB\nThe assign_input_artifacts method is used to associate input artifacts with an experiment, that is not used for log time series and labels.\nA and B is just with a minor typo (metric vs merric), so select B."},{"comment_id":"1152257","timestamp":"1708122720.0","upvote_count":"2","content":"Selected Answer: C\nMy Answer: C \n\nassign_input_artifact method is a method to Vertex Ai Experiment to track the preprocessed data while log_time_series_metrics is a function of Vertex AI TensorBoard to log metrics along time. \n\nlook:\n\nhttps://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/build_model_experimentation_lineage_with_prebuild_code.ipynb\n\nhttps://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_local_trained_models.ipynb","poster":"guilhermebutzke"},{"timestamp":"1705929900.0","content":"Selected Answer: C\nC.\nTensorboard for experimentation and comparison of different model runs.\nassign_input_artifacts to track preprocessed data, since it links artifacts as inputs to the execution. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Execution#google_cloud_aiplatform_Execution_assign_input_artifacts\nUsing log_time_series_metrics would make sense if what we were doing is logging a metric, which we aren't when we track the preprocessed data not yet ran by the model.","poster":"b1a8fae","upvote_count":"2","comment_id":"1128672"}],"question_id":180,"answer":"C","choices":{"A":"1. Use the Vertex AI SDK to create an experiment and set up Vertex ML Metadata.\n2. Use the log_time_series_metrics function to track the preprocessed data, and use the log_merrics function to log loss values.","C":"1. Create a Vertex AI TensorBoard instance and use the Vertex AI SDK to create an experiment and associate the TensorBoard instance.\n2. Use the assign_input_artifact method to track the preprocessed data and use the log_time_series_metrics function to log loss values.","D":"1. Create a Vertex AI TensorBoard instance, and use the Vertex AI SDK to create an experiment and associate the TensorBoard instance.\n2. Use the log_time_series_metrics function to track the preprocessed data, and use the log_metrics function to log loss values.","B":"1. Use the Vertex AI SDK to create an experiment and set up Vertex ML Metadata.\n2. Use the log_time_series_metrics function to track the preprocessed data, and use the log_metrics function to log loss values."},"exam_id":13,"topic":"1","timestamp":"2024-01-16 14:23:00","question_text":"You are developing a model to predict whether a failure will occur in a critical machine part. You have a dataset consisting of a multivariate time series and labels indicating whether the machine part failed. You recently started experimenting with a few different preprocessing and modeling approaches in a Vertex AI Workbench notebook. You want to log data and track artifacts from each run. How should you set up your experiments?","isMC":true}],"exam":{"isImplemented":true,"isBeta":false,"id":13,"isMCOnly":true,"name":"Professional Machine Learning Engineer","numberOfQuestions":304,"provider":"Google","lastUpdated":"11 Apr 2025"},"currentPage":36},"__N_SSP":true}