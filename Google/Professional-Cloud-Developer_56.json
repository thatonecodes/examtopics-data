{"pageProps":{"questions":[{"id":"cyOl2YFL4ka2lgR9x7Ux","isMC":true,"answer":"C","question_images":[],"discussion":[{"comment_id":"107873","content":"The answer is C : the default service account use by cloud function is service-PROJECT_NUMBER@gcf-admin-robot.iam.gserviceaccount.com (cf. https://cloud.google.com/functions/docs/concepts/iam#troubleshooting_permission_errors)","timestamp":"1607707620.0","comments":[{"comment_id":"216182","upvote_count":"2","poster":"saurabh1805","content":"Yes correct answer.","timestamp":"1620582120.0"}],"poster":"[Removed]","upvote_count":"17"},{"poster":"santoshchauhan","content":"Selected Answer: C\nC. Grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket.\n\nThe error \"403 Forbidden\" typically indicates a permissions issue. When a Google Cloud Function tries to access a resource in another project (in this case, a Cloud Storage bucket in project B), it does so using its associated service account. By default, this service account is PROJECT_ID@gcf-admin-robot.iam.gserviceaccount.com where PROJECT_ID is the ID of the project where the Cloud Function is running (project A).","timestamp":"1725736620.0","upvote_count":"1","comment_id":"1168365"},{"comment_id":"1011113","content":"Selected Answer: C\nCorrect : C","timestamp":"1710844140.0","poster":"__rajan__","upvote_count":"1"},{"timestamp":"1688709780.0","poster":"omermahgoub","comments":[{"poster":"omermahgoub","upvote_count":"1","timestamp":"1688709780.0","comment_id":"768342","content":"Option B would not work because the roles/iam.serviceAccountUser role does not grant any permissions to access Cloud Storage. Option D would not solve the problem, as the Cloud Storage API is already enabled in both projects by default."}],"comment_id":"768341","upvote_count":"1","content":"C. Grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket.\n\nIn order for the Cloud Functions code running in project A to write to a Cloud Storage bucket in project B, the service account that is used to execute the code needs to be granted the appropriate permissions. In this case, you should grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket in project B. This will allow the code to write objects to the bucket. Option A would not work because it is the service account, not your user account, that needs to be granted permissions."},{"comment_id":"649203","poster":"tomato123","upvote_count":"3","timestamp":"1676878800.0","content":"Selected Answer: C\nC is correct"},{"timestamp":"1657271100.0","content":"Selected Answer: C\nAnswer is C","comment_id":"519459","poster":"ParagSanyashiv","upvote_count":"2"},{"content":"Appeared exam 26/10","timestamp":"1650946620.0","upvote_count":"2","poster":"trungtran","comments":[{"timestamp":"1651398600.0","comment_id":"471167","upvote_count":"2","poster":"KevT94","content":"How about the other question ? Does it appear also ?"}],"comment_id":"467787"},{"comment_id":"397209","timestamp":"1641179280.0","content":"https://cloud.google.com/functions/docs/troubleshooting:\n\"The Cloud Functions service uses the Cloud Functions Service Agent service account (service-<PROJECT_NUMBER>@gcf-admin-robot.iam.gserviceaccount.com) when performing administrative actions on your project. By default this account is assigned the Cloud Functions cloudfunctions.serviceAgent role. This role is required for Cloud Pub/Sub, IAM, Cloud Storage and Firebase integrations. If you have changed the role for this service account, deployment fails.\"\n\nAnswer is C","upvote_count":"3","poster":"syu31svc"},{"upvote_count":"1","timestamp":"1640868480.0","comment_id":"394604","poster":"kernel1973","content":"Answer is C.\nservice-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com is a google-managed SA."},{"content":"defenitely C","timestamp":"1635740220.0","upvote_count":"1","comment_id":"346532","poster":"kubosuke"},{"content":"Seems there is no correct answer here... The correct answer should be grant add service account used by cloud function as a member to target bucket with roles/storage.objectCreator role","comments":[{"upvote_count":"1","timestamp":"1706361360.0","poster":"samuelmorher","content":"The correct answer is the C but like you say, is not the best. To leave the default account is a bad procedure. The best answer must be \"Create a new service account and assing it to the cloud build, and grant the object creator permission to that account\".","comment_id":"964639"}],"timestamp":"1607617440.0","comment_id":"106912","upvote_count":"3","poster":"emmet"}],"answer_ET":"C","timestamp":"2020-06-10 16:24:00","question_text":"Your code is running on Cloud Functions in project A. It is supposed to write an object in a Cloud Storage bucket owned by project B. However, the write call is failing with the error \"403 Forbidden\".\nWhat should you do to correct the problem?","question_id":276,"url":"https://www.examtopics.com/discussions/google/view/22768-exam-professional-cloud-developer-topic-1-question-42/","answers_community":["C (100%)"],"exam_id":7,"choices":{"B":"Grant your user account the roles/iam.serviceAccountUser role for the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account.","D":"Enable the Cloud Storage API in project B.","C":"Grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket.","A":"Grant your user account the roles/storage.objectCreator role for the Cloud Storage bucket."},"unix_timestamp":1591799040,"answer_images":[],"answer_description":"","topic":"1"},{"id":"M6B2XTsJlLGa4ZxN6JGy","answer_images":[],"answer_ET":"A","unix_timestamp":1604951220,"exam_id":7,"answers_community":["A (39%)","C (33%)","B (28%)"],"discussion":[{"poster":"saurabh1805","comments":[{"content":"A is wrong because appengine is single region: https://cloud.google.com/appengine/docs/locations. For me the correct answer is C, compute engine with instance Group.","comment_id":"251807","upvote_count":"3","poster":"fraloca","comments":[{"content":"One of reqs is \"Move to serverless architecture to facilitate elastic scaling\". I vote for A.","timestamp":"1609624200.0","poster":"donchick","upvote_count":"7","comments":[{"comment_id":"297858","content":"A is correct","upvote_count":"3","poster":"StelSen","timestamp":"1614131520.0"}],"comment_id":"257945"}],"timestamp":"1608849060.0"}],"comment_id":"216189","content":"A is correct answer here App engine, as app engine flexible support .net","timestamp":"1604951220.0","upvote_count":"11"},{"content":"Selected Answer: A\nApp egnine , even its regional","upvote_count":"1","comment_id":"1296751","timestamp":"1728801720.0","poster":"anshad666"},{"content":"Selected Answer: C\nfor me its compute engine cluster","poster":"anshad666","comment_id":"1273731","upvote_count":"1","timestamp":"1724807400.0"},{"upvote_count":"1","timestamp":"1721207340.0","content":"Selected Answer: B\nThe best answer here is B. Use Cloud Functions for autoscaling.\n\nHere's why:\n\nServerless Architecture: Cloud Functions is a serverless platform, meaning you don't need to manage servers or infrastructure. This aligns with the technical requirement to move to a serverless architecture for elastic scaling.\nAutoscaling: Cloud Functions automatically scales based on demand, ensuring that your auth service can handle spikes in traffic without performance degradation. This addresses the business requirement of increasing the number of concurrent users.\nCost-Effective: Serverless platforms like Cloud Functions are cost-effective because you only pay for the resources you use. This aligns with the business requirement of reducing infrastructure management time and cost.\nImproved Reliability: Cloud Functions is a highly reliable platform, reducing the risk of downtime and improving the user experience. This addresses the business requirement of ensuring a consistent experience for users.","poster":"thewalker","comment_id":"1249479","comments":[{"content":"Why other options are less suitable:\n\nA. Use App Engine for autoscaling: While App Engine is a good option for autoscaling, it's not as ideal for a specific service like authentication. Cloud Functions is more lightweight and focused on individual functions.\nC. Use a Compute Engine cluster for the service: This approach requires more manual management and configuration, making it less efficient and cost-effective than Cloud Functions.\nD. Use a dedicated Compute Engine virtual machine instance for the service: This is the least scalable option and doesn't address the issue of intermittent load.\nIn summary: Cloud Functions provides the best combination of autoscaling, serverless architecture, cost-effectiveness, and reliability to address HipLocal's challenges with their .NET-based auth service.","comment_id":"1249480","timestamp":"1721207340.0","poster":"thewalker","upvote_count":"1"}]},{"comment_id":"1168370","poster":"santoshchauhan","timestamp":"1709847060.0","content":"Selected Answer: B\nB. Use Cloud Functions for autoscaling: Cloud Functions is a serverless execution environment that automatically scales based on the load. It is well-suited for applications with intermittent or unpredictable traffic patterns, such as HipLocal's auth service. The serverless nature of Cloud Functions also reduces infrastructure management overhead, aligning with the business requirement to reduce management time and cost. However, it's important to note that the runtime support for .NET in Cloud Functions is limited, and a migration or use of an alternative supported runtime might be necessary.","upvote_count":"2"},{"timestamp":"1700912280.0","poster":"Aeglas","upvote_count":"1","content":"Selected Answer: A\nA, since AppEngine can be deployed in several regions with a Load Balancer in front as demonstrated by Google. This will make the deployment serverless as requested, sticking to .net framework.","comment_id":"1079970"},{"comment_id":"1074119","content":"Selected Answer: A\nServerless architecture -> App Engine","poster":"wanrltw","upvote_count":"1","timestamp":"1700321760.0"},{"timestamp":"1699511940.0","poster":"xiaofeng_0226","content":"Vote for B","upvote_count":"1","comment_id":"1066175"},{"content":"Selected Answer: B\nB is correct.","upvote_count":"2","comment_id":"1015414","poster":"__rajan__","timestamp":"1695518400.0"},{"comments":[{"comment_id":"1074117","timestamp":"1700321640.0","content":"Cloud Functions isn't an optimal solution for an entire app.","upvote_count":"1","poster":"wanrltw"}],"poster":"__rajan__","timestamp":"1695113160.0","content":"Selected Answer: C\nOption C is correct as AppEngine is regional.","comment_id":"1011121","upvote_count":"1"},{"upvote_count":"1","poster":"minagmaxwell","comment_id":"950257","content":"Selected Answer: A\nA is correct. Don't get thrown off by app engine being regional.\nGoogle has demonstracted you can deploy to multiple regions by creating multiple project and throwing a load balancer in front. Check their demo toward the end:\nhttps://www.youtube.com/watch?v=JCvzUTmKakQ&ab_channel=GoogleCloudTech","timestamp":"1689208620.0"},{"comment_id":"815020","content":"I vote A. App Engine cannot run in multi-region. But we can create multiple projects for supporting different region app engine.","timestamp":"1676883060.0","upvote_count":"1","poster":"edward_zhang"},{"timestamp":"1675341840.0","comment_id":"796020","content":"Selected Answer: C\nC is correct, A option is for regional solutions","poster":"Foxal","upvote_count":"1"},{"timestamp":"1673610480.0","comment_id":"774407","poster":"telp","content":"Selected Answer: C\nC is correct because app engine is regional only so it not answer the need of global application","upvote_count":"1"},{"upvote_count":"1","poster":"tomato123","content":"Selected Answer: A\nA is correct","comment_id":"649204","timestamp":"1660974060.0"},{"timestamp":"1660950960.0","comment_id":"649112","upvote_count":"1","poster":"kinoko1330","content":"Selected Answer: A\nA for \"serverless architecture to facilitate elastic scaling\""},{"comment_id":"637317","poster":"nehaxlpb","timestamp":"1658829420.0","content":"Selected Answer: A\nA is correct answer here App engine, as app engine flexible support .net","upvote_count":"1"},{"comments":[{"comment_id":"641905","timestamp":"1659532200.0","poster":"akshaychavan7","upvote_count":"1","content":"Also, note that \"Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud\", which is given in the existing technical environment. \nConsidering this, it is advisable that instead of using a single VM we can create a cluster of such VMs and distribute the load among them."}],"upvote_count":"2","timestamp":"1656994380.0","poster":"akshaychavan7","comment_id":"627239","content":"Selected Answer: C\nInitially, I had picked option A considering the flexible request support. However, it imposes the limitation of a single region which is not expected in the case study. So, the only suitable option here is to use Compute Engine Cluster."},{"timestamp":"1642841400.0","comment_id":"529700","poster":"gfr892","upvote_count":"4","content":"App Engine is regional. C is correct, because is the only multi-regional solution"},{"content":"https://cloud.google.com/appengine/docs/flexible/dotnet/how-instances-are-managed:\n\"Automatic scaling creates instances based on request rate, response latencies, and other application metrics. You can specify thresholds for each of these metrics, as well as a minimum number instances to keep running at all times.\"\n\nA is the answer","comment_id":"397223","upvote_count":"4","timestamp":"1625275140.0","poster":"syu31svc"}],"isMC":true,"answer":"A","choices":{"C":"Use a Compute Engine cluster for the service.","A":"Use App Engine for autoscaling.","D":"Use a dedicated Compute Engine virtual machine instance for the service.","B":"Use Cloud Functions for autoscaling."},"question_images":[],"topic":"1","answer_description":"","question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nCompany Overview -\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\nExecutive Statement -\nWe are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.\n\nSolution Concept -\nHipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.\n\nExisting Technical Environment -\nHipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:\n* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.\n* State is stored in a single instance MySQL database in GCP.\n* Data is exported to an on-premises Teradata/Vertica data warehouse.\n* Data analytics is performed in an on-premises Hadoop environment.\n* The application has no logging.\n* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.\n\nBusiness Requirements -\nHipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:\n* Expand availability of the application to new regions.\n* Increase the number of concurrent users that can be supported.\n* Ensure a consistent experience for users when they travel to different regions.\n* Obtain user activity metrics to better understand how to monetize their product.\n* Ensure compliance with regulations in the new regions (for example, GDPR).\n* Reduce infrastructure management time and cost.\n* Adopt the Google-recommended practices for cloud computing.\n\nTechnical Requirements -\n* The application and backend must provide usage metrics and monitoring.\n* APIs require strong authentication and authorization.\n* Logging must be increased, and data should be stored in a cloud analytics platform.\n* Move to serverless architecture to facilitate elastic scaling.\n* Provide authorized access to internal apps in a secure manner.\nHipLocal's .net-based auth service fails under intermittent load.\nWhat should they do?","timestamp":"2020-11-09 20:47:00","question_id":277,"url":"https://www.examtopics.com/discussions/google/view/36596-exam-professional-cloud-developer-topic-1-question-43/"},{"id":"qP7SDujIXRgfuLY6RZbz","answers_community":["B (63%)","D (25%)","13%"],"unix_timestamp":1642842300,"exam_id":7,"answer_ET":"B","topic":"1","url":"https://www.examtopics.com/discussions/google/view/70408-exam-professional-cloud-developer-topic-1-question-44/","timestamp":"2022-01-22 10:05:00","choices":{"B":"Install the Cloud Logging agent on the virtual machines.","C":"Install the Cloud Monitoring agent on the virtual machines.","A":"Take frequent snapshots of the virtual machines.","D":"Use Cloud Trace to look for performance bottlenecks."},"answer_description":"","question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nCompany Overview -\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\nExecutive Statement -\nWe are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.\n\nSolution Concept -\nHipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.\n\nExisting Technical Environment -\nHipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:\n* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.\n* State is stored in a single instance MySQL database in GCP.\n* Data is exported to an on-premises Teradata/Vertica data warehouse.\n* Data analytics is performed in an on-premises Hadoop environment.\n* The application has no logging.\n* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.\n\nBusiness Requirements -\nHipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:\n* Expand availability of the application to new regions.\n* Increase the number of concurrent users that can be supported.\n* Ensure a consistent experience for users when they travel to different regions.\n* Obtain user activity metrics to better understand how to monetize their product.\n* Ensure compliance with regulations in the new regions (for example, GDPR).\n* Reduce infrastructure management time and cost.\n* Adopt the Google-recommended practices for cloud computing.\n\nTechnical Requirements -\n* The application and backend must provide usage metrics and monitoring.\n* APIs require strong authentication and authorization.\n* Logging must be increased, and data should be stored in a cloud analytics platform.\n* Move to serverless architecture to facilitate elastic scaling.\n* Provide authorized access to internal apps in a secure manner.\nHipLocal's APIs are having occasional application failures. They want to collect application information specifically to troubleshoot the issue. What should they do?","question_id":278,"isMC":true,"question_images":[],"discussion":[{"timestamp":"1648480140.0","upvote_count":"6","comments":[{"comment_id":"603594","poster":"dishum","timestamp":"1652933580.0","upvote_count":"1","content":"No.. C is the right answer"}],"poster":"dishum","comment_id":"576923","content":"B is the correct answer"},{"poster":"PinkeshExampTopics","timestamp":"1733320620.0","content":"Selected Answer: C\nCloud Monitoring provides a wide range of metrics, including CPU, memory, network, and disk usage, which can help identify performance bottlenecks and resource constraints.\n\nCloud Logging is primarily for collecting and analyzing logs. While it can provide some insights into application behavior, it's not as comprehensive as Cloud Monitoring.","upvote_count":"1","comment_id":"1321918"},{"poster":"thewalker","upvote_count":"1","comments":[{"comment_id":"1249487","timestamp":"1721207700.0","content":"Let's break down why the other options are less suitable:\n\nA. Take frequent snapshots of the virtual machines: Snapshots are useful for backups and disaster recovery, but they don't provide real-time insights into application performance or errors.\nB. Install the Cloud Logging agent on the virtual machines: Cloud Logging is essential for collecting logs, but it doesn't provide the same level of detailed tracing information as Cloud Trace. Logs can help identify errors, but they don't show the flow of requests or pinpoint performance issues.\nC. Install the Cloud Monitoring agent on the virtual machines: Cloud Monitoring is great for collecting metrics and setting up alerts, but it doesn't provide the detailed tracing information needed for troubleshooting application failures.","poster":"thewalker","upvote_count":"1"}],"comment_id":"1249486","content":"Selected Answer: D\nThe best answer is D. Use Cloud Trace to look for performance bottlenecks.\n\nHere's why:\n\nCloud Trace for Troubleshooting: Cloud Trace is a distributed tracing tool that helps you understand the flow of requests through your application. It can pinpoint performance bottlenecks, identify slow calls, and highlight potential areas of failure. This is exactly what HipLocal needs to troubleshoot their occasional application failures.","timestamp":"1721207700.0"},{"timestamp":"1709847240.0","poster":"santoshchauhan","comment_id":"1168373","content":"Selected Answer: B\nB. Install the Cloud Logging agent on the virtual machines.\n\nTo troubleshoot occasional application failures, HipLocal needs to collect detailed logs that provide insights into what's happening within their applications. Installing the Cloud Logging agent on the virtual machines is the most direct approach to achieving this. The Cloud Logging agent will collect logs from various applications and system components running on the VMs, which can be invaluable for diagnosing issues.","upvote_count":"1"},{"timestamp":"1695113340.0","content":"Selected Answer: B\nI would go with B.","poster":"__rajan__","upvote_count":"1","comment_id":"1011123"},{"content":"C is the correct answer","comment_id":"959371","upvote_count":"1","poster":"Google","timestamp":"1690016460.0"},{"comment_id":"926680","content":"Selected Answer: B\nB.\nA: No. This is for VM boot failed or other not related to API. The question didn't mention VM failed, nor the scenario.\nB: Yes.\nC: No. Monitoring agent is for metrics collection, such as memory. Not related to API.\nD: No. If the question stated something like the API works perfectly but slow, then this would be valid.","timestamp":"1687091520.0","poster":"zanhsieh","upvote_count":"1"},{"comment_id":"770557","timestamp":"1673277120.0","content":"Selected Answer: B\nThey don't have logging so need to add logging agent so we can have logs to study. Tracr is for latency issue and it's not the issue here.","poster":"telp","upvote_count":"1"},{"timestamp":"1660974120.0","upvote_count":"1","comment_id":"649205","content":"Selected Answer: D\nD is correct","poster":"tomato123"},{"comment_id":"641912","timestamp":"1659532620.0","poster":"akshaychavan7","comments":[{"upvote_count":"1","timestamp":"1660125960.0","comment_id":"644901","content":"I will still go for option B here, as Trace is majorly used for finding out performance bottlenecks which is not specified in the problem statement.","poster":"akshaychavan7"}],"content":"I might be wrong here, but Cloud Trace is also kind of suitable for this use case, isn't it?\nReference - https://cloud.google.com/trace\n\nFast, automatic issue detection\nTrace continuously gathers and analyzes trace data from your project to automatically identify recent changes to your application's performance. These latency distributions, available through the Analysis Reports feature, can be compared over time or versions, and Cloud Trace will automatically alert you if it detects a significant shift in your app's latency profile.","upvote_count":"1"},{"content":"Selected Answer: B\nYep they don’t have any logging so it should be B","upvote_count":"1","poster":"GCPCloudArchitectUser","timestamp":"1645994100.0","comment_id":"557607"},{"timestamp":"1642842300.0","content":"B is correct too.","upvote_count":"2","poster":"gfr892","comment_id":"529710"}],"answer_images":[],"answer":"B"},{"id":"RnuIv0Cxn4Y8HyablANo","question_id":279,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/36600-exam-professional-cloud-developer-topic-1-question-45/","exam_id":7,"answers_community":["A (53%)","B (47%)"],"question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nCompany Overview -\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\nExecutive Statement -\nWe are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.\n\nSolution Concept -\nHipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.\n\nExisting Technical Environment -\nHipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:\n* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.\n* State is stored in a single instance MySQL database in GCP.\n* Data is exported to an on-premises Teradata/Vertica data warehouse.\n* Data analytics is performed in an on-premises Hadoop environment.\n* The application has no logging.\n* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.\n\nBusiness Requirements -\nHipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:\n* Expand availability of the application to new regions.\n* Increase the number of concurrent users that can be supported.\n* Ensure a consistent experience for users when they travel to different regions.\n* Obtain user activity metrics to better understand how to monetize their product.\n* Ensure compliance with regulations in the new regions (for example, GDPR).\n* Reduce infrastructure management time and cost.\n* Adopt the Google-recommended practices for cloud computing.\n\nTechnical Requirements -\n* The application and backend must provide usage metrics and monitoring.\n* APIs require strong authentication and authorization.\n* Logging must be increased, and data should be stored in a cloud analytics platform.\n* Move to serverless architecture to facilitate elastic scaling.\n* Provide authorized access to internal apps in a secure manner.\nHipLocal has connected their Hadoop infrastructure to GCP using Cloud Interconnect in order to query data stored on persistent disks.\nWhich IP strategy should they use?","discussion":[{"timestamp":"1733320980.0","upvote_count":"1","comment_id":"1321923","poster":"PinkeshExampTopics","content":"Selected Answer: B\nCreate an auto mode subnet is the most suitable IP strategy for connecting HipLocal's on-premises Hadoop infrastructure to GCP using Cloud Interconnect.\n\nManual subnets require more manual configuration and can be more complex to manage, especially as the infrastructure grows."},{"poster":"anshad666","timestamp":"1728801840.0","comments":[{"poster":"EMPERBACH","comment_id":"1402106","upvote_count":"1","timestamp":"1742702280.0","content":"Correct, same idea with you about overlapping CIDR due to auto mode -> eliminate B"}],"comment_id":"1296752","content":"Selected Answer: A\nince Cloud Interconnect is used, HipLocal is likely setting up a hybrid cloud environment. This requires careful planning of IP ranges to avoid conflicts and ensure smooth communication between on-premises infrastructure and the cloud, which manual subnets provide.","upvote_count":"2"},{"upvote_count":"1","poster":"thewalker","timestamp":"1721208120.0","comments":[{"content":"Why other options are less suitable:\n\nA. Create manual subnets: Manual subnets require you to manually assign IP addresses to instances, which can be time-consuming and error-prone, especially in a dynamic environment.\nC. Create multiple peered VPCs: Peering VPCs is useful for connecting different VPCs, but it's not necessary for connecting your Hadoop infrastructure to GCP using Cloud Interconnect.\nD. Provision a single instance for NAT: While NAT can be used for outbound connectivity, it's not the most efficient or secure approach for connecting your Hadoop infrastructure to GCP.\nIn summary: Auto mode subnets provide the most efficient and manageable IP address strategy for HipLocal's hybrid environment, simplifying IP address management and ensuring seamless connectivity between their Hadoop infrastructure and GCP.","comment_id":"1249492","poster":"thewalker","upvote_count":"1","timestamp":"1721208120.0"}],"comment_id":"1249491","content":"Selected Answer: B\nThe best answer here is B. Create an auto mode subnet.\n\nHere's why:\n\nAuto Mode Subnets: Auto mode subnets automatically assign internal IP addresses to instances within the subnet. This simplifies IP address management and eliminates the need for manual configuration.\nCloud Interconnect: Cloud Interconnect provides a dedicated, high-bandwidth connection between your on-premises network and Google Cloud. This allows for efficient data transfer between your Hadoop infrastructure and the persistent disks in GCP.\nSimplified Management: Auto mode subnets make it easier to manage IP addresses, especially when dealing with a hybrid environment like HipLocal's."},{"content":"Selected Answer: A\nManual Subnets:\nControl Over IP Addressing: Creating manual subnets (also known as custom mode VPCs) provides precise control over IP addressing and subnet creation. This ensures that the IP ranges do not overlap and can be managed to meet specific requirements of the Hadoop infrastructure and Cloud Interconnect setup.\nSubnet Management: With manual subnets, HipLocal can create subnets that are optimised for their data traffic patterns and usage requirements, which is crucial for performance and efficient utilisation of network resources.\nIntegration: Custom mode VPCs allow for better integration with on-premises networks through Cloud Interconnect, ensuring a seamless and efficient network setup.\nWhy Not the B?\nAuto mode subnets automatically create subnets in each region with pre-defined IP ranges. This lack of control over IP address assignment and network segmentation is not suitable for complex and specific networking requirements like those of HipLocal’s Hadoop infrastructure.","poster":"d_ella2001","timestamp":"1720854360.0","upvote_count":"1","comment_id":"1247160"},{"upvote_count":"1","content":"Selected Answer: B\nB. Create an auto mode subnet.\n\nWhen integrating Hadoop infrastructure with Google Cloud Platform (GCP) via Cloud Interconnect and querying data stored on persistent disks, the IP strategy should simplify network management while ensuring efficient and secure data access. Creating an auto mode subnet in their VPC is a suitable approach for this scenario.","comment_id":"1168375","timestamp":"1709847420.0","poster":"santoshchauhan"},{"upvote_count":"1","poster":"Bessa24","timestamp":"1708947540.0","content":"Selected Answer: B\nB is correct","comment_id":"1159635"},{"timestamp":"1695113760.0","comment_id":"1011125","poster":"__rajan__","content":"Selected Answer: B\nFor simplicity and ease of management, an auto mode subnet (option B) could be a good choice.","upvote_count":"1"},{"poster":"tomato123","timestamp":"1660974120.0","upvote_count":"1","content":"Selected Answer: A\nA is correct","comment_id":"649206"},{"content":"Selected Answer: A\nA - Need to take control of the IP assignment thru manual subnet especially when establishing the connectivity between on-prem/cloud","comment_id":"647982","poster":"bk7","upvote_count":"4","timestamp":"1660723860.0"},{"timestamp":"1659533640.0","comment_id":"641925","upvote_count":"2","poster":"akshaychavan7","content":"Selected Answer: B\nI will go with auto mode subnet creation as it will automatically create a subnet inside each region. Moreover, one of the business requirements states that 'Reduce infrastructure management time and cost.'. Thus, with auto mode subnet we avoid infrastructure management."},{"content":"https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-data\n\nI would take A based on the 2nd figure given in the link","upvote_count":"1","poster":"syu31svc","timestamp":"1625275920.0","comment_id":"397236"},{"timestamp":"1604952060.0","content":"A is correct answer here.","upvote_count":"1","comment_id":"216198","poster":"saurabh1805"}],"question_images":[],"unix_timestamp":1604952060,"timestamp":"2020-11-09 21:01:00","answer_ET":"A","answer_images":[],"choices":{"A":"Create manual subnets.","D":"Provision a single instance for NAT.","B":"Create an auto mode subnet.","C":"Create multiple peered VPCs."},"answer_description":"","isMC":true,"answer":"A"},{"id":"bSAvMldr2trl8f0wqYMi","answer_images":[],"question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nCompany Overview -\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\nExecutive Statement -\nWe are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.\n\nSolution Concept -\nHipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.\n\nExisting Technical Environment -\nHipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:\n* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.\n* State is stored in a single instance MySQL database in GCP.\n* Data is exported to an on-premises Teradata/Vertica data warehouse.\n* Data analytics is performed in an on-premises Hadoop environment.\n* The application has no logging.\n* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.\n\nBusiness Requirements -\nHipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:\n* Expand availability of the application to new regions.\n* Increase the number of concurrent users that can be supported.\n* Ensure a consistent experience for users when they travel to different regions.\n* Obtain user activity metrics to better understand how to monetize their product.\n* Ensure compliance with regulations in the new regions (for example, GDPR).\n* Reduce infrastructure management time and cost.\n* Adopt the Google-recommended practices for cloud computing.\n\nTechnical Requirements -\n* The application and backend must provide usage metrics and monitoring.\n* APIs require strong authentication and authorization.\n* Logging must be increased, and data should be stored in a cloud analytics platform.\n* Move to serverless architecture to facilitate elastic scaling.\n* Provide authorized access to internal apps in a secure manner.\nWhich service should HipLocal use to enable access to internal apps?","question_images":[],"timestamp":"2020-11-09 21:12:00","topic":"1","url":"https://www.examtopics.com/discussions/google/view/36601-exam-professional-cloud-developer-topic-1-question-46/","choices":{"B":"Cloud Armor","D":"Cloud Identity-Aware Proxy","A":"Cloud VPN","C":"Virtual Private Cloud"},"answer_description":"","isMC":true,"question_id":280,"unix_timestamp":1604952720,"answers_community":["D (100%)"],"discussion":[{"poster":"MickeyRourke","comment_id":"256554","timestamp":"1640997420.0","content":"I think it should be D .","upvote_count":"7"},{"poster":"hongminhcbg","content":"Selected Answer: D\nD is correct","comment_id":"1409712","upvote_count":"1","timestamp":"1742832600.0"},{"poster":"__rajan__","content":"Selected Answer: D\nCloud IAP works by verifying user identity and context of the request to determine if a user should be allowed to access the application. It provides secure application-level access control and does not require a traditional VPN connection.","comment_id":"1011134","upvote_count":"1","timestamp":"1726736940.0"},{"comment_id":"649207","content":"Selected Answer: D\nD is correct","timestamp":"1692510180.0","upvote_count":"1","poster":"tomato123"},{"poster":"dishum","timestamp":"1680016680.0","comments":[{"timestamp":"1680016740.0","upvote_count":"1","content":"I think it is A, not D","comment_id":"576931","poster":"dishum"}],"upvote_count":"1","content":"D is correct","comment_id":"576930"},{"content":"https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview\nhttps://cloud.google.com/armor\n\nD is the answer for sure","upvote_count":"2","comment_id":"397253","poster":"syu31svc","timestamp":"1656814680.0"},{"upvote_count":"1","content":"D.\nhttps://cloud.google.com/iap/docs/concepts-overview","comment_id":"394631","timestamp":"1656588660.0","poster":"kernel1973"},{"upvote_count":"4","timestamp":"1636488720.0","poster":"saurabh1805","comment_id":"216200","content":"if internal app mens app hosted on-prem then option A seems to be correct one"}],"answer_ET":"D","answer":"D","exam_id":7}],"exam":{"provider":"Google","lastUpdated":"11 Apr 2025","isMCOnly":false,"isBeta":false,"isImplemented":true,"numberOfQuestions":338,"name":"Professional Cloud Developer","id":7},"currentPage":56},"__N_SSP":true}