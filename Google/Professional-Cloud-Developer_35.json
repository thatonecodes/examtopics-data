{"pageProps":{"questions":[{"id":"78fesdx3VE8kjKaPmDgq","answer_description":"","question_text":"You work for a financial services company that has a container-first approach. Your team develops microservices applications. A Cloud Build pipeline creates the container image, runs regression tests, and publishes the image to Artifact Registry. You need to ensure that only containers that have passed the regression tests are deployed to Google Kubernetes Engine (GKE) clusters. You have already enabled Binary Authorization on the GKE clusters. What should you do next?","question_images":[],"topic":"1","question_id":171,"discussion":[{"upvote_count":"5","poster":"JonathanSJ","comment_id":"1140705","comments":[{"content":"With option D the cloud build step could looks like:\n\n - name: 'gcr.io/cloud-builders/gcloud'\n entrypoint: 'bash'\n args: [ '-c', 'gcloud container binauthz create-signature --artifact-url gcr.io/<PROJECT_ID>/<IMAGE_NAME>:signed --attestor <ATTESTOR_NAME> --keyversion <KEY_VERSION> --project <PROJECT_ID>' ]","upvote_count":"1","timestamp":"1707114000.0","poster":"JonathanSJ","comment_id":"1140708"}],"content":"Selected Answer: D\nI will go for D.\n\nD: The next step, after enable Binary Auth, is creating an attestor and a policy and then configure the attestation step in the cloud build pipeline.\n\nNot A because when you use kritis to sign an image you must provide the private key file from the attestor. And for that you must save the private key when you create the attestor for it later use. Its more complicated.\n\nNot C because pod security standard level to restricted don't enforce the use of signed images.","timestamp":"1707113880.0"},{"upvote_count":"1","poster":"mrgarfield","content":"Selected Answer: D\nsimpler thatn using Kritis","timestamp":"1725781140.0","comment_id":"1280261"},{"comment_id":"1224781","poster":"pico","timestamp":"1717598820.0","upvote_count":"4","content":"Selected Answer: D\nhttps://cloud.google.com/binary-authorization/docs/cloud-build\n\nAttestation Creation: The key difference is that you don't necessarily need to use Kritis Signer within Cloud Build to create the attestation. The Cloud Build documentation shows how you can use the gcloud beta container binauthz attestations create command directly within your Cloud Build steps to generate the attestation."},{"upvote_count":"2","poster":"__rajan__","timestamp":"1695611940.0","comment_id":"1016344","content":"Selected Answer: A\nA is correct."},{"upvote_count":"2","timestamp":"1691523780.0","comment_id":"975979","poster":"purushi","content":"Selected Answer: A\nI go with A since it is detailed and more specific about Kritis digital signature."},{"poster":"zanhsieh","upvote_count":"2","comment_id":"920896","content":"Selected Answer: A\nA. For folks wonder what differences between Kritis Signer and Voucher Server Voucher Client, I asked Google Bard about it. Bard stated Kritis Signer is a command-line tools, whereas Voucher Server Voucher Client is a web-based tool. I then tried to verify that with Google search and Google image search (search \"voucher server voucher client\" then click Images). It seems Bard report correctly. Someone even wrote a Kritis Signer integrated pipeline with terraform (https://xebia.com/blog/how-to-automate-the-kritis-signer-on-google-cloud-platform/) . \nAlso, yes, both Kritis Signer and Voucher Server Voucher Client have Google official documentations. However, if you look carefully on Voucher Server Voucher Client Google official doc, they use curl to the Voucher Server address, which indirectly prove Vouch Server Vouch Client is a web-based tool.","timestamp":"1686506100.0"},{"timestamp":"1681673460.0","upvote_count":"1","comments":[{"upvote_count":"2","poster":"closer89","timestamp":"1682445300.0","content":"its D definitely\nhttps://cloud.google.com/binary-authorization/docs/cloud-build","comment_id":"880705"}],"content":"Selected Answer: C\nquestion is not about checking vulnerabilities.\nits not A. The Kritis Signer is a command-line utility to check whether an image violates the policy on security vulnerabilities.\nits not a voucher too.","comment_id":"872093","poster":"closer89"},{"timestamp":"1677746280.0","content":"took my exam yesterday (01-03-2023) and this question was there","upvote_count":"3","poster":"Pime13","comment_id":"826660"},{"comment_id":"822442","timestamp":"1677418740.0","content":"info on voucher server: https://cloud.google.com/binary-authorization/docs/creating-attestations-voucher","poster":"Pime13","upvote_count":"2"},{"comment_id":"822428","poster":"Pime13","timestamp":"1677418620.0","upvote_count":"2","content":"Selected Answer: A\nKritis Signer is an open source command-line tool that can create Binary Authorization attestations based on a policy that you configure. You can also use Kritis Signer to create attestations after checking an image for vulnerabilities identified by Container Analysis.\n\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis"},{"upvote_count":"2","comments":[{"poster":"mrvergara","content":"Kritis Signer is a component of the Kritis project, which is an open-source implementation of Binary Authorization for Kubernetes. Kritis Signer is used to sign container images and create attestations, which verify that the image meets the criteria specified in a Binary Authorization policy. These attestations can be used to enforce that only authorized containers are deployed in a cluster, providing an additional layer of security for your containerized applications.","timestamp":"1675517160.0","upvote_count":"2","comment_id":"797986"}],"poster":"mrvergara","content":"Selected Answer: A\nBinary Authorization in GKE provides a way to enforce that only verified container images are deployed in a cluster. In this scenario, to ensure that only containers that have passed the regression tests are deployed, you would create an attestor and a policy in Binary Authorization, and use Kritis Signer to create an attestation for the container image after it has passed the tests. The attestation verifies that the image meets the policy's criteria and is authorized to be deployed. This provides a secure and automated way to enforce that only containers that have passed the required tests are deployed in the cluster.","timestamp":"1675517160.0","comment_id":"797985"},{"upvote_count":"3","poster":"TNT87","timestamp":"1675208340.0","comment_id":"794718","content":"Selected Answer: A\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis"}],"url":"https://www.examtopics.com/discussions/google/view/97495-exam-professional-cloud-developer-topic-1-question-252/","timestamp":"2023-02-01 00:39:00","answer_images":[],"isMC":true,"exam_id":7,"choices":{"A":"Create an attestor and a policy. After a container image has successfully passed the regression tests, use Cloud Build to run Kritis Signer to create an attestation for the container image.","B":"Deploy Voucher Server and Voucher Client components. After a container image has successfully passed the regression tests, run Voucher Client as a step in the Cloud Build pipeline.","D":"Create an attestor and a policy. Create an attestation for the container images that have passed the regression tests as a step in the Cloud Build pipeline.","C":"Set the Pod Security Standard level to Restricted for the relevant namespaces. Use Cloud Build to digitally sign the container images that have passed the regression tests."},"unix_timestamp":1675208340,"answer_ET":"A","answers_community":["A (54%)","D (42%)","4%"],"answer":"A"},{"id":"InDyWZPYNAvdONJ2uj3c","choices":{"A":"Enable Binary Authorization, and configure it to attest that no vulnerabilities exist in a container image.","B":"Upload the built container images to your Docker Hub instance, and scan them for vulnerabilities.","C":"Enable the Container Scanning API in Artifact Registry, and scan the built container images for vulnerabilities.","D":"Add Artifact Registry to your Aqua Security instance, and scan the built container images for vulnerabilities."},"url":"https://www.examtopics.com/discussions/google/view/97492-exam-professional-cloud-developer-topic-1-question-253/","answer_description":"","topic":"1","question_text":"You are reviewing and updating your Cloud Build steps to adhere to best practices. Currently, your build steps include:\n\n1. Pull the source code from a source repository.\n2. Build a container image\n3. Upload the built image to Artifact Registry.\n\nYou need to add a step to perform a vulnerability scan of the built container image, and you want the results of the scan to be available to your deployment pipeline running in Google Cloud. You want to minimize changes that could disrupt other teamsâ€™ processes. What should you do?","unix_timestamp":1675207680,"isMC":true,"answer_images":[],"discussion":[{"poster":"wanrltw","content":"Selected Answer: A\nI'm not so sure about C because the task is to add a STEP to our Cloud Build pipeline to perform the vulnerability scan, whereas C implies more doing the job via Cloud Console. Why would we enable the Container Scanning API in Artifact Registry every time we run the pipeline?\n\nThis scenario is similar to what we have in question #252. I'd go with A:\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis","upvote_count":"1","comment_id":"1099551","timestamp":"1718693640.0"},{"upvote_count":"1","timestamp":"1711347240.0","content":"Selected Answer: C\nC is correct.","poster":"__rajan__","comment_id":"1016425"},{"timestamp":"1707428760.0","upvote_count":"1","poster":"purushi","content":"Selected Answer: C\nC is right. Requirement is to perform a vulnerability scan of the built container image.\nC states Enable the Container Scanning API in Artifact Registry, and scan the built container images for vulnerabilities. Further steps for better security would be to follow option A.","comment_id":"975980"},{"poster":"Pime13","comment_id":"822456","content":"Selected Answer: C\ni choose C","timestamp":"1693050060.0","upvote_count":"2"},{"comment_id":"797991","poster":"mrvergara","content":"Selected Answer: C\nEnabling the Container Scanning API in Artifact Registry and scanning the built container images is a best practice because it allows you to perform security scans within the same environment where the built images are stored. This helps minimize the changes that could disrupt other teams' processes, as the images are already in Artifact Registry, and the scanning results can be easily accessed by the deployment pipeline in Google Cloud. Additionally, the Container Scanning API integrates with Google Cloud security and governance tools, allowing you to enforce security policies and manage vulnerabilities in a centralized and automated way.","upvote_count":"2","timestamp":"1691148600.0"},{"upvote_count":"1","timestamp":"1690838880.0","content":"Selected Answer: C\nhttps://cloud.google.com/container-analysis/docs/automated-scanning-howto#view_the_image_vulnerabilities","poster":"TNT87","comment_id":"794711"}],"question_images":[],"answer":"C","question_id":172,"exam_id":7,"answer_ET":"C","answers_community":["C (88%)","13%"],"timestamp":"2023-02-01 00:28:00"},{"id":"D9DE0B2X60YEWQVQK5pZ","timestamp":"2023-02-01 14:14:00","isMC":true,"question_text":"You are developing an online gaming platform as a microservices application on Google Kubernetes Engine (GKE). Users on social media are complaining about long loading times for certain URL requests to the application. You need to investigate performance bottlenecks in the application and identify which HTTP requests have a significantly high latency span in user requests. What should you do?","discussion":[{"poster":"drewtest1234","timestamp":"1740302400.0","comment_id":"1360448","content":"Selected Answer: C\nCloud Trace is built for latency analysis.","upvote_count":"1"},{"upvote_count":"1","poster":"alpha_canary","comment_id":"1192615","content":"Selected Answer: C\nhttps://cloud.google.com/trace/docs/setup/python-ot","timestamp":"1728525840.0"},{"poster":"__rajan__","upvote_count":"1","comment_id":"1016434","content":"Selected Answer: C\nThis approach allows you to update your application code to send traces to Trace for inspection and analysis. You can then create an analysis report on Trace to analyze user requests. This will help you identify which HTTP requests have a significantly high latency span in user requests, which seems to be the main concern according to the complaints from users on social media.","timestamp":"1711348500.0"},{"content":"Selected Answer: C\nThere is no best fit other than C here.","comment_id":"976001","timestamp":"1707429480.0","poster":"purushi","upvote_count":"1"},{"timestamp":"1698323400.0","poster":"Writer","content":"Selected Answer: C\nThis is the best way to investigate performance bottlenecks in a microservices application. By using OpenTelemetry, you can collect traces from all of your microservices and analyze them in Trace. This will allow you to identify which requests are taking the longest and where the bottlenecks are occurring.","upvote_count":"3","comment_id":"881609"},{"comment_id":"826659","timestamp":"1693636680.0","content":"took my exam yesterday (01-03-2023) and this question was there","poster":"Pime13","upvote_count":"2"},{"poster":"Pime13","content":"Selected Answer: C\ncorrecting my choice","timestamp":"1693198500.0","comment_id":"824477","upvote_count":"1"},{"comments":[],"content":"Selected Answer: A\nquestion clearly says: performance botlenecks and which step is having latency ---> Cloud Trace\nhttps://cloud.google.com/trace/docs/overview","comment_id":"813976","poster":"Pime13","upvote_count":"1","timestamp":"1692435240.0"},{"content":"Selected Answer: C\nhttps://cloud.google.com/trace/docs/setup#when-to-instrument","upvote_count":"1","comment_id":"799185","poster":"TNT87","timestamp":"1691262780.0"},{"poster":"mrvergara","comment_id":"797997","content":"Selected Answer: C\nInstrumenting your microservices with the OpenTelemetry tracing package, updating your application code to send traces to Trace for inspection and analysis, and creating an analysis report on Trace would be the recommended solution for investigating performance bottlenecks in the application and identifying HTTP requests with high latency. This would allow you to visualize and analyze the complete request-response cycle and identify specific parts of the application that might be contributing to long loading times.","upvote_count":"1","timestamp":"1691149020.0"},{"comment_id":"795146","upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/stackdriver/docs/solutions/gke/workload-metrics","poster":"TNT87","timestamp":"1690888440.0"}],"unix_timestamp":1675257240,"exam_id":7,"topic":"1","question_images":[],"answer_description":"","answer":"C","question_id":173,"answers_community":["C (83%)","A (17%)"],"choices":{"A":"Configure GKE workload metrics using kubectl. Select all Pods to send their metrics to Cloud Monitoring. Create a custom dashboard of application metrics in Cloud Monitoring to determine performance bottlenecks of your GKE cluster.","C":"Instrument your microservices by installing the OpenTelemetry tracing package. Update your application code to send traces to Trace for inspection and analysis. Create an analysis report on Trace to analyze user requests.","B":"Update your microservices to log HTTP request methods and URL paths to STDOUT. Use the logs router to send container logs to Cloud Logging. Create filters in Cloud Logging to evaluate the latency of user requests across different methods and URL paths.","D":"Install tcpdump on your GKE nodes. Run tcpdump to capture network traffic over an extended period of time to collect data. Analyze the data files using Wireshark to determine the cause of high latency."},"answer_ET":"C","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/97547-exam-professional-cloud-developer-topic-1-question-254/"},{"id":"k5RZh1VYY6wzKg3mIKlD","answer":"C","timestamp":"2023-02-01 00:04:00","isMC":true,"question_images":[],"discussion":[{"upvote_count":"1","timestamp":"1722832080.0","content":"Selected Answer: C\nI will go for C.","comment_id":"1140713","poster":"JonathanSJ"},{"comments":[{"poster":"purushi","content":"Option D is closer but we cannot instantiate the mutliple requests with different IP addresses.","upvote_count":"1","timestamp":"1707431580.0","comment_id":"976031"}],"poster":"purushi","comment_id":"976030","upvote_count":"1","content":"Selected Answer: C\nRequirements are very clear. Load testing with concurrent users/threads + Multiple source origins/IP address, C is the best choice.","timestamp":"1707431460.0"},{"timestamp":"1699278360.0","comment_id":"890714","content":"Selected Answer: D\nIt's normal to launch some compute engine from cloud shell.\nI think we can increase right load by increasing Compute Engine which do load-test from Cloud Shell step by step.\nCan the test from GKE cover the condition which is \"from multiple source IP addresses.\".","upvote_count":"1","comments":[{"poster":"NewComer200","timestamp":"1699278660.0","content":"I feel that load testing from the Compute Engine is more accurate than load testing from the Pod.\nI'm not sure what to think.\nIs it possible that GKE's engress is a bottleneck and load testing is not possible?","comment_id":"890719","upvote_count":"1"}],"poster":"NewComer200"},{"content":"Selected Answer: C\nOption D, which involves starting several instances of a load testing framework container on Cloud Shell, may not be a recommended approach for several reasons:\n\nCloud Shell is a shell environment for managing resources hosted on Google Cloud and does not provide a scalable infrastructure for running load tests.\n\nStarting several instances of a container on Cloud Shell is not a highly available or scalable solution for load testing, and may not provide sufficient parallelism or control over the source IP addresses of the traffic.\n\nUsing a private Google Kubernetes Engine cluster to deploy a distributed load testing framework allows for scaling up the load testing by deploying additional Pods, which can provide more control over the number of concurrent users and the source IP addresses of the traffic, and can provide a more robust and scalable infrastructure for load testing.","upvote_count":"3","poster":"mrvergara","comment_id":"798010","comments":[{"poster":"NewComer200","content":"\"Cloud Shell is a shell environment for managing resources hosted on Google Cloud and does not provide a scalable infrastructure for running load tests.\"\nI agree with this explanation.\nBut\n\"Starting several instances of a container on Cloud Shell is not a highly available or scalable solution for load testing, and may not provide sufficient parallelism or control over the source IP addresses of the traffic.\"\nI can't agree with this explanation.\nPlease teach me where this explanation is written.\nIt's normal to launch some compute engine from cloud shell.\nI think we can increase right load by increasing Compute Engine which do load-test from Cloud Shell step by step.\nCan the test from GKE cover the condition which is \"from multiple source IP addresses.\".\nI think this question's answer is D.","upvote_count":"1","timestamp":"1699278300.0","comment_id":"890713"}],"timestamp":"1691149920.0"},{"comments":[{"upvote_count":"1","timestamp":"1690888560.0","comment_id":"795150","content":"https://cloud.google.com/run/docs/about-load-testing","poster":"TNT87"}],"upvote_count":"1","content":"Selected Answer: D\nnope Answer is D....not C","poster":"TNT87","timestamp":"1690882200.0","comment_id":"795047"},{"poster":"TNT87","comment_id":"794701","timestamp":"1690837440.0","content":"Selected Answer: C\nTo deploy the load testing tasks, you do the following:\nDeploy a load testing master.\nDeploy a group of load testing workers. With these load testing workers, you can create a substantial amount of traffic for testing purposes.\nhttps://cloud.google.com/run/docs/about-load-testing\nhttps://cloud.google.com/architecture/distributed-load-testing-using-gke#build_the_container_image\nAnswer","comments":[{"upvote_count":"1","timestamp":"1690949700.0","comment_id":"795805","poster":"TNT87","content":"This tutorial explains how to use Google Kubernetes Engine (GKE) to deploy a distributed load testing framework that uses multiple containers to create traffic for a simple REST-based API. This tutorial load-tests a web application deployed to App Engine that exposes REST-style endpoints to respond to incoming HTTP POST requests."}],"upvote_count":"1"}],"choices":{"A":"Create an image that has cURL installed, and configure cURL to run a test plan. Deploy the image in a managed instance group, and run one instance of the image for each VM.","B":"Create an image that has cURL installed, and configure cURL to run a test plan. Deploy the image in an unmanaged instance group, and run one instance of the image for each VM.","C":"Deploy a distributed load testing framework on a private Google Kubernetes Engine cluster. Deploy additional Pods as needed to initiate more traffic and support the number of concurrent users.","D":"Download the container image of a distributed load testing framework on Cloud Shell. Sequentially start several instances of the container on Cloud Shell to increase the load on the API."},"exam_id":7,"topic":"1","answer_description":"","answer_ET":"C","question_text":"You need to load-test a set of REST API endpoints that are deployed to Cloud Run. The API responds to HTTP POST requests. Your load tests must meet the following requirements:\nâ€¢ Load is initiated from multiple parallel threads.\nâ€¢ User traffic to the API originates from multiple source IP addresses.\nâ€¢ Load can be scaled up using additional test instances.\n\nYou want to follow Google-recommended best practices. How should you configure the load testing?","answers_community":["C (75%)","D (25%)"],"question_id":174,"answer_images":[],"unix_timestamp":1675206240,"url":"https://www.examtopics.com/discussions/google/view/97491-exam-professional-cloud-developer-topic-1-question-255/"},{"id":"0Nsg89rhw9jv8eEa5PoL","answer":"D","isMC":true,"timestamp":"2023-04-23 07:25:00","question_images":[],"discussion":[{"content":"Selected Answer: D\nI will go for D.\n\nOption C sounds good, but as the service account only have the Storage Object Viewer role it can't generate signed URLs for files from bucket because need storage.object.get, then it's incorrect.","timestamp":"1722832620.0","poster":"JonathanSJ","comment_id":"1140714","upvote_count":"1"},{"poster":"__rajan__","comment_id":"1016446","content":"Selected Answer: D\nThis approach allows you to secure your Cloud Storage bucket by enforcing public access prevention, which prevents data from being accidentally shared with the public. By creating and updating the Cloud Run service to use a user-managed service account, you can ensure that only this service has access to the bucket. Granting the Storage Object Viewer IAM role on the bucket to the service account allows the service to read objects stored in the bucket.","upvote_count":"1","timestamp":"1711349280.0"},{"upvote_count":"2","content":"Selected Answer: D\nD is right.\n1) Create service account with role of viewing the objects under Cloud storage bucket\n2) Create policies to prevent public access to the bucket.\n\nA and C: Neither of these are close to the solution.\nB is somewhat closer but the statement \"Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account\" is wrong since we need to create service account for the application and not for VM.","timestamp":"1707432000.0","comment_id":"976033","poster":"purushi"},{"content":"Selected Answer: D\nmost secure and efficient way to give the application Identity and Access Management (IAM) permission to access the images in the bucket.","upvote_count":"2","poster":"Writer","timestamp":"1698322980.0","comment_id":"881605"}],"choices":{"B":"Enforce public access prevention for the desired bucket. Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account.","A":"Enforce signed URLs for the desired bucket. Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account.","C":"Enforce signed URLs for the desired bucket. Create and update the Cloud Run service to use a user-managed service account. Grant the Storage Object Viewer IAM role on the bucket to the service account.","D":"Enforce public access prevention for the desired bucket. Create and update the Cloud Run service to use a user-managed service account. Grant the Storage Object Viewer IAM role on the bucket to the service account."},"exam_id":7,"topic":"1","answer_description":"","answer_ET":"D","question_text":"Your team is creating a serverless web application on Cloud Run. The application needs to access images stored in a private Cloud Storage bucket. You want to give the application Identity and Access Management (IAM) permission to access the images in the bucket, while also securing the services using Google-recommended best practices. What should you do?","answers_community":["D (100%)"],"question_id":175,"answer_images":[],"unix_timestamp":1682227500,"url":"https://www.examtopics.com/discussions/google/view/107095-exam-professional-cloud-developer-topic-1-question-256/"}],"exam":{"numberOfQuestions":338,"provider":"Google","isMCOnly":false,"isImplemented":true,"id":7,"name":"Professional Cloud Developer","lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":35},"__N_SSP":true}