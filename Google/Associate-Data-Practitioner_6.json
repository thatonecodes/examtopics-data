{"pageProps":{"questions":[{"id":"X1W6HNeovD5SmEJQs2GD","topic":"1","question_id":26,"unix_timestamp":1740674640,"answers_community":["D (100%)"],"discussion":[{"content":"Selected Answer: D\nThe best solution for creating fixed BigQuery budgets per department is D. Separate projects per department, separate reservations per department, and project assignments to departmental reservations. This approach is optimal because separate reservations per department create fixed, predictable costs for each department. Assigning departmental projects to their respective reservations ensures that each department's query costs are contained within their allocated budget. Options A, B, and C fail to create fixed departmental budgets. Option A (Quotas) is for resource limits, not budgets. Options B and C (Single Reservation) provide overall cost predictability but don't enable departmental budget control. Therefore, Option D is the only option that effectively addresses the requirement of fixed departmental budgets.","poster":"n2183712847","comment_id":"1362617","timestamp":"1740674640.0","upvote_count":"1"}],"answer":"D","answer_ET":"D","question_images":[],"answer_description":"","isMC":true,"choices":{"A":"Create a custom quota for each analyst in BigQuery.","C":"Assign each analyst to a separate project associated with their department. Create a single reservation by using BigQuery editions. Assign all projects to the reservation.","B":"Create a single reservation by using BigQuery editions. Assign all analysts to the reservation.","D":"Assign each analyst to a separate project associated with their department. Create a single reservation for each department by using BigQuery editions. Create assignments for each project in the appropriate reservation."},"timestamp":"2025-02-27 17:44:00","question_text":"Your organization has several datasets in their data warehouse in BigQuery. Several analyst teams in different departments use the datasets to run queries. Your organization is concerned about the variability of their monthly BigQuery costs. You need to identify a solution that creates a fixed budget for costs associated with the queries run by each department. What should you do?","url":"https://www.examtopics.com/discussions/google/view/157221-exam-associate-data-practitioner-topic-1-question-32/","exam_id":2,"answer_images":[]},{"id":"Io7U1OaUSEzAqGuL2a4v","url":"https://www.examtopics.com/discussions/google/view/157220-exam-associate-data-practitioner-topic-1-question-33/","discussion":[{"comment_id":"1362615","timestamp":"1740674160.0","upvote_count":"1","content":"Selected Answer: D\nread the question, it says offloading read traffic from the primary database, meaning there would be a read-replica.\n\nThe best solution for improving Cloud SQL read performance by offloading traffic while minimizing effort and cost is D. Enable automatic backups and create a read replica. Read replicas are specifically designed to offload read traffic with minimal effort in Cloud SQL. Option C (Larger Instance) scales up the primary, not offloading, and can be more expensive. Option B (Memorystore) requires more application code changes and cache management, increasing effort. Option A (Cloud CDN) is less directly applicable to database read offloading and requires significant application changes, making it higher effort and less effective for this purpose. Therefore, Option D provides the most direct, efficient, and cost-effective way to improve read performance by offloading read traffic from your Cloud SQL database.","poster":"n2183712847"}],"isMC":true,"choices":{"A":"Use Cloud CDN to cache frequently accessed data.","B":"Store frequently accessed data in a Memorystore instance.","D":"Enable automatic backups, and create a read replica of the Cloud SQL instance.","C":"Migrate the database to a larger Cloud SQL instance."},"unix_timestamp":1740674160,"answers_community":["D (100%)"],"answer":"D","topic":"1","answer_ET":"D","exam_id":2,"question_id":27,"answer_images":[],"timestamp":"2025-02-27 17:36:00","question_text":"You manage a web application that stores data in a Cloud SQL database. You need to improve the read performance of the application by offloading read traffic from the primary database instance. You want to implement a solution that minimizes effort and cost. What should you do?","question_images":[],"answer_description":""},{"id":"i4QNZ2qpzHVVGhXbLhom","url":"https://www.examtopics.com/discussions/google/view/157198-exam-associate-data-practitioner-topic-1-question-34/","topic":"1","timestamp":"2025-02-27 15:46:00","question_images":[],"answers_community":["A (100%)"],"answer_images":[],"question_id":28,"discussion":[{"upvote_count":"1","comment_id":"1362610","poster":"n2183712847","timestamp":"1740673500.0","content":"Selected Answer: A\n500 TB in a few days with 1gbps bandwidth would require option A. multiple Transfer Appliances"}],"answer_ET":"A","question_text":"Your organization plans to move their on-premises environment to Google Cloud. Your organization’s network bandwidth is less than 1 Gbps. You need to move over 500 ТВ of data to Cloud Storage securely, and only have a few days to move the data. What should you do?","answer":"A","isMC":true,"unix_timestamp":1740667560,"answer_description":"","exam_id":2,"choices":{"C":"Connect to Google Cloud using VPN. Use the gcloud storage command to move the data to Cloud Storage.","D":"Connect to Google Cloud using Dedicated Interconnect. Use the gcloud storage command to move the data to Cloud Storage.","A":"Request multiple Transfer Appliances, copy the data to the appliances, and ship the appliances back to Google Cloud to upload the data to Cloud Storage.","B":"Connect to Google Cloud using VPN. Use Storage Transfer Service to move the data to Cloud Storage."}},{"id":"SHrHxWOO9ATqbcvpX9Sn","unix_timestamp":1740673320,"question_text":"Your organization uses a BigQuery table that is partitioned by ingestion time. You need to remove data that is older than one year to reduce your organization’s storage costs. You want to use the most efficient approach while minimizing cost. What should you do?","answer_ET":"D","choices":{"D":"Set the table partition expiration period to one year using the ALTER TABLE statement in SQL.","A":"Create a scheduled query that periodically runs an update statement in SQL that sets the “deleted\" column to “yes” for data that is more than one year old. Create a view that filters out rows that have been marked deleted.","C":"Require users to specify a partition filter using the alter table statement in SQL.","B":"Create a view that filters out rows that are older than one year."},"answer_images":[],"timestamp":"2025-02-27 17:22:00","exam_id":2,"answers_community":["D (100%)"],"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/google/view/157218-exam-associate-data-practitioner-topic-1-question-35/","question_images":[],"question_id":29,"answer":"D","isMC":true,"discussion":[{"timestamp":"1741443240.0","content":"Selected Answer: D\nD. is the only one that has a partition expiration, meaning deletion. the other answer choices just filter views.","comment_id":"1366589","poster":"n2183712847","upvote_count":"2"},{"upvote_count":"2","poster":"n2183712847","comment_id":"1362608","timestamp":"1740673320.0","content":"Selected Answer: D\nThe most efficient and cost-effective solution is D. Set the table partition expiration period to one year using the ALTER TABLE statement in SQL. Partition expiration is a built-in BigQuery feature specifically designed to automatically delete old partitions, directly reducing storage costs and requiring minimal effort. Options A and B (soft delete and views) do not reduce storage costs; they only filter or mark data without removing it. Option C (requiring partition filters) is about query optimization, not data removal or storage cost reduction. Therefore, Option D is the only option that directly and effectively addresses the requirement of removing old data to minimize storage costs in a BigQuery partitioned table."}]},{"id":"rIu8G6kdrUMUsXi8j3Fp","url":"https://www.examtopics.com/discussions/google/view/157216-exam-associate-data-practitioner-topic-1-question-36/","answer":"B","question_images":[],"discussion":[{"timestamp":"1740673200.0","poster":"n2183712847","comment_id":"1362606","upvote_count":"1","content":"Selected Answer: B\nThe best solution is B. Use Dataform workflows. Dataform is uniquely suited because it is specifically designed for programmatic data transformations using only SQL and has native, robust Git integration for version control. Option A (Cloud Data Fusion) is a visual ETL tool, not SQL-centric, and has limited Git integration. Option C (Dataflow) is code-based (Java/Python), not SQL-based. Option D (Cloud Composer) is an orchestration tool, not a transformation tool, and DAGs are defined in Python, not SQL. Therefore, Option B, Dataform workflows, is the only option that fully satisfies both requirements."}],"choices":{"B":"Use Dataform workflows.","C":"Use Dataflow pipelines.","D":"Use Cloud Composer operators.","A":"Use Cloud Data Fusion pipelines."},"answers_community":["B (100%)"],"unix_timestamp":1740673200,"isMC":true,"answer_images":[],"answer_description":"","question_text":"Your company is migrating their batch transformation pipelines to Google Cloud. You need to choose a solution that supports programmatic transformations using only SQL. You also want the technology to support Git integration for version control of your pipelines. What should you do?","timestamp":"2025-02-27 17:20:00","topic":"1","answer_ET":"B","exam_id":2,"question_id":30}],"exam":{"numberOfQuestions":72,"isBeta":false,"name":"Associate Data Practitioner","isMCOnly":true,"provider":"Google","isImplemented":true,"lastUpdated":"11 Apr 2025","id":2},"currentPage":6},"__N_SSP":true}