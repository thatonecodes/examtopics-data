{"pageProps":{"questions":[{"id":"nKlm0KnRdEpkIzt0Zi6o","isMC":true,"exam_id":13,"answers_community":["A (100%)"],"unix_timestamp":1624749600,"question_id":246,"answer_images":[],"question_text":"As the lead ML Engineer for your company, you are responsible for building ML models to digitize scanned customer forms. You have developed a TensorFlow model that converts the scanned images into text and stores them in Cloud Storage. You need to use your ML model on the aggregated data collected at the end of each day with minimal manual intervention. What should you do?","answer_description":"","answer_ET":"A","question_images":[],"topic":"1","discussion":[{"timestamp":"1640568000.0","upvote_count":"29","comment_id":"391604","content":"Use the model at the end of the day => Not D, C. \nMinimize manual intervention => not B\nAns: A","poster":"Paul_Dirac"},{"comment_id":"1225646","poster":"PhilipKoku","content":"Selected Answer: A\nA) This a batch prediction using AI Platform","timestamp":"1733511600.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1180464","timestamp":"1727049360.0","poster":"Arthurious","content":"Selected Answer: A\nA is the most efficient"},{"comment_id":"1071842","content":"Selected Answer: A\nA is the only way","upvote_count":"1","poster":"Sum_Sum","timestamp":"1715796600.0"},{"comment_id":"892728","poster":"M25","content":"Selected Answer: A\nWent with A","upvote_count":"1","timestamp":"1699513620.0"},{"comment_id":"766638","timestamp":"1688555220.0","poster":"ares81","content":"Selected Answer: A\nThere is only A, for me.","upvote_count":"1"},{"content":"Selected Answer: A\nBecause aggregated data can be sent at the end of the day for batch prediction and AI platform is managed so satisfy minimal intervention requirement\nNot B as violates minimal intervention requirement\nNot C and D as real-time or online inference is not needed since data is aggregated at the end of the day","comment_id":"753433","upvote_count":"3","poster":"koakande","timestamp":"1687440540.0"},{"timestamp":"1686403980.0","poster":"hiromi","content":"Selected Answer: A\nYou need to use your ML model on the aggregated data collected at the end of each day with minimal manual intervention.","comment_id":"741071","upvote_count":"1"},{"comment_id":"722393","content":"A.\nhttps://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/#:~:text=Vertex%20AI%20Batch%20Prediction%20provides,to%20GCS%20or%20BigQuery%2C%20respectively.","poster":"seifou","upvote_count":"1","timestamp":"1684551120.0"},{"content":"Selected Answer: A\n\"You need to use your ML model on the aggregated data\" that means we need the batch prediction feature in AI platform","poster":"Mohamed_Mossad","timestamp":"1670613000.0","comment_id":"614117","upvote_count":"1"},{"poster":"ggorzki","comment_id":"527834","timestamp":"1658249940.0","upvote_count":"3","content":"Selected Answer: A\nA\nhttps://cloud.google.com/ai-platform/prediction/docs/batch-predict"},{"content":"Another vote for A. Technically, through the right lens D could be correct as well, but what tipped me towards A was batch vs online predictions and the need for less manual work.","poster":"george_ognyanov","timestamp":"1649161560.0","upvote_count":"3","comment_id":"457693"},{"comment_id":"446200","upvote_count":"2","poster":"Y2Data","timestamp":"1647477720.0","content":"https://cloud.google.com/ai-platform/prediction/docs/batch-predict"}],"choices":{"A":"Use the batch prediction functionality of AI Platform.","B":"Create a serving pipeline in Compute Engine for prediction.","C":"Use Cloud Functions for prediction each time a new data point is ingested.","D":"Deploy the model on AI Platform and create a version of it for online inference."},"url":"https://www.examtopics.com/discussions/google/view/56134-exam-professional-machine-learning-engineer-topic-1-question/","answer":"A","timestamp":"2021-06-27 01:20:00"},{"id":"KXNwX9OXW3fM8FeQOWNi","answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/55570-exam-professional-machine-learning-engineer-topic-1-question/","answer":"A","question_text":"You recently joined an enterprise-scale company that has thousands of datasets. You know that there are accurate descriptions for each table in BigQuery, and you are searching for the proper BigQuery table to use for a model you are building on AI Platform. How should you find the data that you need?","discussion":[{"content":"Should be A\nhttps://cloud.google.com/data-catalog/docs/concepts/overview","comment_id":"384856","timestamp":"1624022640.0","upvote_count":"19","poster":"chohan"},{"timestamp":"1649951280.0","comment_id":"585882","poster":"mmona19","upvote_count":"7","content":"Selected Answer: A\nwho is providing these answers?? Its clearly A. most of the answers are incorrect here."},{"upvote_count":"1","content":"Selected Answer: A\nA is the right one","comment_id":"1308166","timestamp":"1730927700.0","poster":"louisaok"},{"content":"Selected Answer: A\nA) Data Catalog","timestamp":"1717693260.0","comment_id":"1225647","poster":"PhilipKoku","upvote_count":"1"},{"timestamp":"1701767400.0","comment_id":"1088311","content":"Selected Answer: A\nA without hesitation.","poster":"fragkris","upvote_count":"1"},{"content":"Selected Answer: A\nA is the only way","timestamp":"1700079060.0","upvote_count":"1","poster":"Sum_Sum","comment_id":"1071844"},{"timestamp":"1688755080.0","upvote_count":"1","poster":"SamuelTsch","comment_id":"945935","content":"Selected Answer: A\nA should be correct"},{"timestamp":"1683608820.0","upvote_count":"2","poster":"M25","content":"Selected Answer: A\nWent with A","comment_id":"892729"},{"comment_id":"551775","timestamp":"1645361280.0","upvote_count":"1","poster":"TheGrew","content":"Selected Answer: A\nAnother vote for A by me."},{"upvote_count":"3","timestamp":"1641283440.0","poster":"NamitSehgal","comment_id":"516331","content":"Selected Answer: A\nA should be the way to go for large datasets\n--This is also good but it is legacy way of checking:-\nNFORMATION_SCHEMA contains these views for table metadata: TABLES and TABLE_OPTIONS for metadata about tables. COLUMNS and COLUMN_FIELD_PATHS for metadata about columns and fields. PARTITIONS for metadata about table partitions (Preview)"},{"timestamp":"1640037060.0","poster":"JobQ","comment_id":"505687","upvote_count":"1","content":"I vote A"},{"content":"Another vote for answer A from me.","upvote_count":"1","comment_id":"457694","timestamp":"1633436820.0","poster":"george_ognyanov"}],"unix_timestamp":1624022640,"exam_id":13,"choices":{"C":"Maintain a lookup table in BigQuery that maps the table descriptions to the table ID. Query the lookup table to find the correct table ID for the data that you need.","B":"Tag each of your model and version resources on AI Platform with the name of the BigQuery table that was used for training.","A":"Use Data Catalog to search the BigQuery datasets by using keywords in the table description.","D":"Execute a query in BigQuery to retrieve all the existing table names in your project using the INFORMATION_SCHEMA metadata tables that are native to BigQuery. Use the result o find the table that you need."},"answer_images":[],"timestamp":"2021-06-18 15:24:00","question_images":[],"answer_ET":"A","topic":"1","question_id":247,"answers_community":["A (100%)"]},{"id":"Pglah43xrcETcG2GNhvb","timestamp":"2021-06-27 03:10:00","answer":"B","discussion":[{"content":"Ans: B (Ref: https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9)\n(C) High correlation doesn't mean leakage. The question may suggest target leakage and the defining point of this leakage is the availability of data after the target is available.(https://www.kaggle.com/dansbecker/data-leakage)","comment_id":"391657","poster":"Paul_Dirac","comments":[{"upvote_count":"6","comment_id":"946996","content":"This ref doesn't explain WHY we should use NCV in this case - it just explains HOW to use NCV when dealing with time series.\nCross-validation, including nested cross-validation, is a powerful tool for model evaluation and hyperparameter tuning, but it does NOT DIRECTLY ADDRESS data leakage. Data leakage refers to a situation where information from the test dataset leaks into the training dataset, causing the model to have an unrealistically high performance. Nested cross-validation can indeed help provide a more accurate estimation of the model's performance on unseen data, but IT DOESN'T SOLVE the underlying issue of data leakage if it's already present.","poster":"Jarek7","timestamp":"1688890740.0"}],"timestamp":"1624756200.0","upvote_count":"27"},{"comment_id":"830597","timestamp":"1678085340.0","content":"Selected Answer: C\nC: this is correct choice 1000000000%\nThis is data leakage issue on training data\nhttps://cloud.google.com/automl-tables/docs/train#analyze\nThe question is from this content.\nIf a column's Correlation with Target value is high, make sure that is expected, and not an indication of target leakage.\n\nLet 's explain on my owner way, sometime the feature used on training data use value to calculate something from target value unintentionally, it result in high correlation with each other. \nfor instance , you predict stock price by using moving average, MACD , RSI despite the fact that 3 features have been calculated from price (target).","poster":"John_Pongthorn","upvote_count":"8","comments":[{"upvote_count":"2","content":"I agree. Besides, when a CV is done randomly (not split by the time point) it can make things worse.","poster":"black_scissors","comment_id":"912707","timestamp":"1685699940.0"}]},{"content":"Selected Answer: B\nGemini Explanation: Nested Cross-validation: Nested cross-validation is a robust technique to detect and mitigate data leakage. It involves two loops of cross-validation:\nInner loop: Tunes hyperparameters and performs model selection.\nOuter loop: Evaluates the model's performance on unseen data, giving you a more realistic estimate of how well your model generalizes.\nWhy not C : C. Address data leakage by removing features highly correlated with the target value: While highly correlated features can sometimes be a sign of leakage, they might also be genuinely informative features. Removing them without proper analysis might hurt your model's performance.","poster":"Sivaram06","timestamp":"1736346180.0","comment_id":"1337969","upvote_count":"1"},{"content":"Selected Answer: B\nAs per the PMLE cert book the answer is B. Since the model is performing well with training data, it is a case of data leakage. Cross‐validation is one of the strategies to overcome data leakage.","timestamp":"1732913400.0","poster":"Pau1234","comments":[{"comment_id":"1334866","upvote_count":"1","content":"The book mentions cross-validation.. NOT 'nested cross-validation' (page 34), however this answer is better than C. you want to remove value that are NOT correlated versus correlated as in answer C. ;)","timestamp":"1735659300.0","poster":"desertlotus1211"}],"comment_id":"1319917","upvote_count":"2"},{"content":"Select answer: C. --reason--- While B (nested cross-validation) helps improve the evaluation process and prevents over-optimistic performance estimates, it doesn't tackle the root cause of data leakage. Data leakage is often caused by features that are too closely tied to the target—in this case, the unusually high AUC suggests that the model is gaining unfair information.","upvote_count":"2","poster":"Foxy2021","timestamp":"1728982620.0","comment_id":"1298155"},{"content":"Selected Answer: B\nB is the correct option","comment_id":"1236804","poster":"chirag2506","upvote_count":"1","timestamp":"1719306780.0"},{"upvote_count":"1","comment_id":"1225655","timestamp":"1717693800.0","poster":"PhilipKoku","content":"Selected Answer: C\nC) Is the best answer"},{"poster":"girgu","timestamp":"1716742020.0","comment_id":"1219061","upvote_count":"1","content":"Selected Answer: C\nNested cross validation will not work for time series data. Time series data require the expanding widow training data set. Seems most likely the issue is high correlation in columns."},{"poster":"AnnaR","upvote_count":"2","comment_id":"1202682","timestamp":"1714146240.0","content":"B: correct. \nconsidering c, but why should we remove a feature of highly predictive nature?? for me, this does not explain the problem of overfitting... a highly predictive feature is also useful for good performance evaluated on the test set. \n--> Decide for B!"},{"upvote_count":"1","comment_id":"1199506","content":"Selected Answer: B\nagree with Paul_Dirac","timestamp":"1713684540.0","poster":"gscharly"},{"timestamp":"1703843760.0","comment_id":"1108549","content":"Selected Answer: B\nI initially went with B- however after reading this: https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ I think C is right. Quoted from the link: \"Nested cross-validation is an approach to model hyperparameter optimization and model selection that attempts to overcome the problem of overfitting the training dataset.\". Overfitting is exactly our problem here. Correlated features in the dataset may be a sign of data leakage, but they are not necessarily.","upvote_count":"1","poster":"b1a8fae"},{"upvote_count":"1","content":"Selected Answer: B\nI think its B. GPT4 makes a good argument about C:\n While this is a valid approach to handling data leakage, it might not be sufficient if the leakage is due to reasons other than high correlation, such as temporal leakage in time-series data.","comment_id":"1071849","timestamp":"1700079300.0","poster":"Sum_Sum"},{"upvote_count":"1","timestamp":"1694588340.0","content":"Selected Answer: A\nOption A: This option is a reasonable choice. Switching to a less complex algorithm can help reduce overfitting, and using k-fold cross-validation can provide a better estimate of how well the model will generalize to unseen data. It's essential to ensure that the high performance isn't solely due to overfitting.","comment_id":"1006298","poster":"pico","comments":[{"content":"Option B: Nested cross-validation is primarily used to estimate model performance accurately and select the best model hyperparameters. While it's a good practice, it doesn't directly address the overfitting issue. It helps prevent over-optimistic model performance estimates but doesn't necessarily fix the overfitting problem.\n\nOption C: Removing features highly correlated with the target value can be a valid step in feature selection or preprocessing. However, it doesn't directly address the overfitting issue or explain why the model is performing exceptionally well on the training data. It's a separate step from mitigating overfitting.\n\nOption D: This option is incorrect. Tuning hyperparameters should aim to improve model performance on the validation set, not reduce it. \n\nIn summary, the most appropriate next step is Option A:","upvote_count":"2","timestamp":"1694588400.0","poster":"pico","comment_id":"1006299"}]},{"content":"Selected Answer: B\nB: If splits are done chronologically(as it is always advised), Nested CV should work\nC: High correlation with target means we have to check if this is strong explanatory power or data leakage. dropping the features won't help us distinguish in those cases but may help reveal independence contribution of remaining features","comment_id":"989309","poster":"atlas_lyon","upvote_count":"1","timestamp":"1692891840.0"},{"comment_id":"975892","content":"Selected Answer: B\nOption C is a good step to avoid overfitting, but it's not necessarily the best approach to address data leakage.\n\nData leakage occurs when information from the validation or test data leaks into the training data, leading to overly optimistic performance metrics. In time-series data, it's important to avoid using future information to predict past events.\n\nRemoving features highly correlated with the target value may help to reduce overfitting, but it does not necessarily address data leakage.\n\nTherefore, applying nested cross-validation during model training is a better approach to address data leakage in this scenario.","poster":"tavva_prudhvi","upvote_count":"2","timestamp":"1691516460.0"},{"timestamp":"1688890620.0","comment_id":"946993","upvote_count":"1","content":"Selected Answer: C\nhttps://towardsdatascience.com/avoiding-data-leakage-in-timeseries-101-25ea13fcb15f\nDirectly says: \"Dive straight into the MVP, cross-validate later!\" \nMVP stands for Minimum Viable Product","poster":"Jarek7"},{"timestamp":"1688737260.0","content":"Selected Answer: B\nAgree with Paul_Dirac. Also it is recommended to use nested-cross-validation to avoid data leakage in time series data.","upvote_count":"1","comment_id":"945730","poster":"Liting"},{"comment_id":"912708","timestamp":"1685700060.0","upvote_count":"1","content":"Selected Answer: C\nThere can be a feature causing data leakage which might have been overlooked. In addition, when cross-validation is done randomly, the leakage can be even bigger.","poster":"black_scissors"},{"poster":"M25","content":"Selected Answer: B\nWent with B","comment_id":"892730","timestamp":"1683608880.0","upvote_count":"1"},{"content":"Selected Answer: B\nB\nI agree with Paul_Dirac","timestamp":"1671361740.0","upvote_count":"2","comment_id":"748771","poster":"hiromi"},{"timestamp":"1654797540.0","poster":"Mohamed_Mossad","content":"Selected Answer: B\n\"You haven't explored using any sophisticated algorithms or spent any time on hyperparameter tuning\" so we have a base /default hyperparameters estimator so overfitting is quite not possible , so it is a leakage problem , by inspection C is wrong , so it will be B","comment_id":"614140","upvote_count":"1"},{"comment_id":"599848","upvote_count":"3","content":"Selected Answer: B\nQuite tricky but through elimination, correct answer is B. Model overfitting doesn't apply here as we can't tell if a model is overfitting by just looking at training data results.","timestamp":"1652237220.0","poster":"David_ml"},{"poster":"Celia20210714","timestamp":"1626666720.0","upvote_count":"2","comment_id":"409287","comments":[{"timestamp":"1627655160.0","upvote_count":"5","comment_id":"417470","content":"D is incorrect since they mentioned that the initial model without sophisticated algorithm (e.g. model architecture) and without parameter tuning already achieves 99% accuracy. This suggests data leakage, and especially so since they mentioned it is time-series data, which suggest incorrect data split for train and evaluation.","poster":"sensev"},{"upvote_count":"3","timestamp":"1627776300.0","comment_id":"418052","content":"No. We won't be able to know whether the model is overfitting just by looking at the training set alone.","poster":"Paul_Dirac"}],"content":"ANS: D \n (AUC ROC) value of 99% for training data after just a few experiments\n>> overfitting"}],"topic":"1","question_text":"You started working on a classification problem with time series data and achieved an area under the receiver operating characteristic curve (AUC ROC) value of\n99% for training data after just a few experiments. You haven't explored using any sophisticated algorithms or spent any time on hyperparameter tuning. What should your next step be to identify and fix the problem?","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/56140-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"B","isMC":true,"question_images":[],"unix_timestamp":1624756200,"exam_id":13,"choices":{"C":"Address data leakage by removing features highly correlated with the target value.","B":"Address data leakage by applying nested cross-validation during model training.","A":"Address the model overfitting by using a less complex algorithm.","D":"Address the model overfitting by tuning the hyperparameters to reduce the AUC ROC value."},"answer_images":[],"question_id":248,"answers_community":["B (58%)","C (39%)","3%"]},{"id":"apQ8PUuIaKR42iTPtaSZ","isMC":true,"topic":"1","answer":"C","question_images":[],"answers_community":["C (63%)","B (38%)"],"exam_id":13,"unix_timestamp":1624023720,"question_text":"You work for an online travel agency that also sells advertising placements on its website to other companies. You have been asked to predict the most relevant web banner that a user should see next. Security is important to your company. The model latency requirements are 300ms@p99, the inventory is thousands of web banners, and your exploratory analysis has shown that navigation context is a good predictor. You want to Implement the simplest solution. How should you configure the prediction pipeline?","answer_description":"","question_id":249,"answer_images":[],"answer_ET":"C","discussion":[{"upvote_count":"13","comment_id":"418055","poster":"Paul_Dirac","content":"Security => not A.\nB: doesn't handle processing with banner inventory.\nD: deployment on GKE is less simple than on AI Platform. Besides, MemoryStore is in-memory while banners are stored persistently.\nAns: C","comments":[{"timestamp":"1714057200.0","poster":"pinimichele01","comment_id":"1202082","upvote_count":"2","content":"B: doesn't handle processing with banner inventory ---> not true..."}],"timestamp":"1627777860.0"},{"content":"ANS: C\nGAE + IAP\nhttps://medium.com/google-cloud/secure-cloud-run-cloud-functions-and-app-engine-with-api-key-73c57bededd1\n\nBigtable at low latency\nhttps://cloud.google.com/bigtable#section-2","timestamp":"1626664500.0","upvote_count":"8","poster":"Celia20210714","comment_id":"409282"},{"timestamp":"1732571400.0","content":"Selected Answer: B\nB - right answer","comment_id":"1317783","upvote_count":"1","poster":"AB_C"},{"upvote_count":"2","poster":"ccb23cc","comment_id":"1234270","timestamp":"1718959560.0","content":"Selected Answer: C\nThey affirm that navigation context is a good predictor for your model. Therefore you need to be able to perform the prediction and write the new context (if you get more data you will get a better model) and read (to use it for your prediction).\nOn one hand, BigQuery is a OLAP method so for writings and readings could take it around 2 seconds.\nOn the other hand, BigTable is a OLTP method and can make writings and readings in about 9 milliseconds \nConclusion: As one of the requerements is that the latency requirements have to be below 300ms your only choice is using BigTable\n\nhttps://galvarado.com.mx/post/comparaci%C3%B3n-de-bases-de-datos-en-google-cloud-datastore-vs-bigtable-vs-cloud-sql-vs-spanner-vs-bigquery/"},{"timestamp":"1717693980.0","comment_id":"1225657","content":"Selected Answer: C\nC) Big Table for low latency","upvote_count":"2","poster":"PhilipKoku"},{"comment_id":"1202686","content":"Selected Answer: B\nWas torn between B and C, but decided for B, because the question states how we should configure the PREDICTION pipeline! \nSince the exploratory analysis already identified navigation context as good predictor, the focus should be on the prediction model itself.","poster":"AnnaR","timestamp":"1714146660.0","upvote_count":"4"},{"timestamp":"1713684660.0","content":"Selected Answer: C\nagree with Paul_Dirac","comment_id":"1199507","upvote_count":"2","poster":"gscharly"},{"timestamp":"1710360420.0","upvote_count":"3","content":"look at Q80","comment_id":"1172873","poster":"rightcd"},{"timestamp":"1700079600.0","upvote_count":"3","poster":"Sum_Sum","content":"Selected Answer: B\nI was torn between B and C.\nBut I really don't see the need for a DB","comment_id":"1071853"},{"timestamp":"1700045220.0","content":"Selected Answer: B\nEmbed the client on the website, deploy the gateway on App Engine, and then deploy the model on AI Platform Prediction.","comment_id":"1071332","poster":"Mickey321","upvote_count":"1"},{"comment_id":"951201","content":"Selected Answer: B\nsecuirity (gateway) + Simplest(ai, not DB)","upvote_count":"1","poster":"harithacML","timestamp":"1689309900.0"},{"upvote_count":"2","poster":"Liting","timestamp":"1688737320.0","comment_id":"945731","content":"Selected Answer: C\nBigtable is recommended for storage in the case scenario."},{"comment_id":"941972","timestamp":"1688397540.0","poster":"tavva_prudhvi","upvote_count":"6","content":"Selected Answer: C\nB is also a possible solution, but it does not include a database for storing and retrieving the user's navigation context. This means that every time a user visits a page, the gateway would need to query the website to retrieve the navigation context, which could be slow and inefficient. By using Cloud Bigtable to store the navigation context, the gateway can quickly retrieve the context from the database and pass it to the model for prediction. This makes the overall prediction pipeline more efficient and scalable. Therefore, C is a better option compared to B."},{"content":"Selected Answer: B\nB is correct, C introduces computational overhead, unnecessarily increasing serving latency.","comment_id":"930171","timestamp":"1687411140.0","upvote_count":"1","poster":"friedi"},{"upvote_count":"2","content":"Selected Answer: C\nC. Embed the client on the website, deploy the gateway on App Engine, deploy the database on Cloud Bigtable for writing and for reading the user's navigation context, and then deploy the model on AI Platform Prediction\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#choosing_a_nosql_database\nTypical use cases for Bigtable are:\n* Ad prediction that leverages dynamically aggregated values over all ad requests and historical data.","timestamp":"1685950620.0","comment_id":"915176","poster":"Voyager2"},{"upvote_count":"2","content":"Selected Answer: C\nBigtable is a massively scalable NoSQL database service engineered for high throughput and for low-latency workloads. It can handle petabytes of data, with millions of reads and writes per second at a latency that's on the order of milliseconds.\n\nTypical use cases for Bigtable are:\n\nFraud detection that leverages dynamically aggregated values. Applications in Fintech and Adtech are usually subject to heavy reads and writes.\nAd prediction that leverages dynamically aggregated values over all ad requests and historical data.\nBooking recommendation based on the overall customer base's recent bookings.","poster":"CloudKida","comment_id":"892898","timestamp":"1683617820.0"},{"poster":"M25","content":"Selected Answer: C\nWent with C","upvote_count":"2","comment_id":"892731","timestamp":"1683608880.0"},{"content":"Selected Answer: B\nThe volume is too low for a Bigtable scenario","timestamp":"1681139160.0","poster":"fredcaram","comment_id":"866374","upvote_count":"1"},{"content":"Selected Answer: B\nB.\nsimplest solution","comment_id":"856982","upvote_count":"1","poster":"SergioRubiano","timestamp":"1680262500.0"},{"timestamp":"1680013860.0","upvote_count":"1","comment_id":"853346","poster":"alejandroverger","comments":[{"timestamp":"1680687780.0","comment_id":"861941","poster":"kucuk_kagan","content":"gpt answer","upvote_count":"2"}],"content":"Selected Answer: B\nB. Embed the client on the website, deploy the gateway on App Engine, and then deploy the model on AI Platform Prediction.\n\nFor the simplest solution, you can embed the client on the website to collect user data and send it to a gateway deployed on App Engine. App Engine provides a scalable and cost-effective solution to handle web requests. Then, you can deploy your model on AI Platform Prediction, which can handle the required latency (300ms@p99) and provides a managed solution for serving machine learning models.\n\nOption A might not provide the necessary security by directly accessing AI Platform Prediction from the client side. Options C and D introduce additional complexity by adding a database layer (Cloud Bigtable and Memorystore, respectively) that is not necessary for the simplest solution, as you can use the navigation context directly from the client."},{"content":"Selected Answer: B\nSee kaan1234's response for a fuller solution, but for me a database is not necessary here as the reference data is lightweight and the prediction calculation requires just the user's browsing context - no realtime aggregation is required to calculate model features.","comment_id":"823519","timestamp":"1677493080.0","upvote_count":"1","comments":[{"comment_id":"959286","upvote_count":"1","timestamp":"1690007220.0","content":"The reference data is lightweight, but it still needs to be stored somewhere. The prediction calculation requires just the user's browsing context, but the browsing context needs to be stored in the database before it can be used for prediction.","poster":"tavva_prudhvi"}],"poster":"BenMS"},{"content":"Selected Answer: B\nOption C could also be a valid solution. It involves deploying the database on Cloud Bigtable or Memorystore for reading and writing the user's navigation context, which could improve latency and make data storage more efficient. However, it introduces additional complexity to the system, which may not be necessary for the current problem at hand, especially if the inventory of web banners is not very large.\n\nUsing AI Platform Prediction and App Engine is a simpler solution that can still meet the latency and inventory requirements while keeping the system architecture relatively simple.\n\nSo, both B and C could be valid options, but B is a simpler solution for this specific problem statement, where the focus is on implementing the simplest solution.","poster":"shankalman717","upvote_count":"1","timestamp":"1677047700.0","comment_id":"817535"},{"content":"Selected Answer: B\nB. Embed the client on the website, deploy the gateway on App Engine, and then deploy the model on AI Platform Prediction.\n\nThe simplest solution for configuring the prediction pipeline in this scenario is to embed the client on the website, deploy the gateway on App Engine, and then deploy the model on AI Platform Prediction.\n\nAI Platform Prediction is a managed service for deploying and serving machine learning models with a low latency requirement. It can handle thousands of web banners and provides the necessary infrastructure to run and manage models with the required latency.\n\nApp Engine is a managed serverless platform that can host web applications and APIs, making it an excellent choice for deploying the gateway. The gateway can handle user requests and route them to the deployed model on AI Platform Prediction.\n\nSince navigation context is a good predictor, a simple approach is to use a REST API to send the user's navigation context to the prediction pipeline for inference. The response with the predicted web banner can be returned to the client through the gateway.","timestamp":"1677047520.0","upvote_count":"2","comments":[{"poster":"tavva_prudhvi","upvote_count":"1","timestamp":"1690007160.0","content":"Where will you store the user's navigation context?","comment_id":"959284"}],"comment_id":"817534","poster":"shankalman717"},{"comment_id":"776736","poster":"John_Pongthorn","content":"Selected Answer: C\nC: the key of this question is Database to store input/output data for ML Prediction with low latency.\nA ,B they didn't refer to the database\nD , Memorystore is not suitable for writing and for reading the user's navigation context\n\nSo it left you with C","upvote_count":"2","timestamp":"1673796480.0"},{"poster":"u_phoria","comment_id":"622642","content":"Selected Answer: C\nB is almost feasible. There is nothing I see in the question that requires banner inventory to be part of the prediction flow - presumably the output of the model would simply be a banner ID of some kind, to be used to fetch and serve the predicted banner separately. If model input was simply the in-session navigation context, this could be tracked client-side and passed to the serving endpoint as needed.\n\nC wins for me only because persisting navigation context information allows it to be captured and used between visits / sessions (and because it's simpler than D, as pointed out by others).","timestamp":"1656256080.0","upvote_count":"1"},{"comment_id":"613437","timestamp":"1654711200.0","poster":"Mohamed_Mossad","content":"Selected Answer: C\nsolution is between C,D but in D deploy the model on Google Kubernetes Engine will need you to wrap the model inside some sort of backend to handle requests for example flask , but question mentioned \"simplest solution\" so AI Platform is much more faster to deploy so I will vote for C","upvote_count":"2"},{"comment_id":"527855","content":"Selected Answer: C\nBigtable to get the context at low latency, AIP for simple solution","upvote_count":"5","poster":"ggorzki","timestamp":"1642620960.0"},{"comment_id":"516403","timestamp":"1641288660.0","poster":"NamitSehgal","upvote_count":"4","content":"C\nThe values in Bigtable are used as input features to invoke AI Platform models for prediction."},{"comments":[{"timestamp":"1635963540.0","poster":"kfrd","upvote_count":"3","content":"That's right, but GKE doesn't satisfy the simplicity requirement.","comment_id":"472190"}],"content":"is not memorystore faster than bigtable ?? it required 300ms","timestamp":"1626350100.0","upvote_count":"1","comment_id":"407042","poster":"nunzio144"},{"poster":"chohan","comment_id":"384863","content":"I think it's C","upvote_count":"5","timestamp":"1624023720.0"}],"url":"https://www.examtopics.com/discussions/google/view/55571-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"C":"Embed the client on the website, deploy the gateway on App Engine, deploy the database on Cloud Bigtable for writing and for reading the user's navigation context, and then deploy the model on AI Platform Prediction.","B":"Embed the client on the website, deploy the gateway on App Engine, and then deploy the model on AI Platform Prediction.","A":"Embed the client on the website, and then deploy the model on AI Platform Prediction.","D":"Embed the client on the website, deploy the gateway on App Engine, deploy the database on Memorystore for writing and for reading the user's navigation context, and then deploy the model on Google Kubernetes Engine."},"timestamp":"2021-06-18 15:42:00"},{"id":"fFBiVHtEszUBm8D5IS3G","answer_description":"","discussion":[{"comment_id":"375292","content":"the answer is A","timestamp":"1622907600.0","upvote_count":"25","poster":"gcp2021go"},{"upvote_count":"10","content":"A, because AI platform supported all the frameworks mentioned. And Kubeflow is not managed service in GCP. https://cloud.google.com/ai-platform/training/docs/getting-started-pytorch","timestamp":"1627104840.0","comment_id":"412998","poster":"guruguru"},{"comment_id":"1300014","timestamp":"1729342440.0","upvote_count":"3","poster":"jkkim_jt","content":"I think \"custom container\" should be written in capital letters like \"AI Platform Custom Container\". It is one of the features of AI Platform. It is a proper noun not a general term."},{"content":"Selected Answer: A\nA. Now it's called Vertex AI","comment_id":"1244336","upvote_count":"4","poster":"Yorko","timestamp":"1720443180.0"},{"comment_id":"1224871","timestamp":"1717606980.0","content":"Selected Answer: A\nThe best approach is option A: Use AI Platform custom containers. It provides flexibility, scalability, and support for various frameworks, making it an ideal choice for your team’s needs.","upvote_count":"1","poster":"PhilipKoku"},{"timestamp":"1701429600.0","content":"Selected Answer: A\nChose A","comment_id":"1085167","poster":"fragkris","upvote_count":"1"},{"comment_id":"1070401","upvote_count":"2","poster":"Sum_Sum","timestamp":"1699968540.0","content":"Selected Answer: A\nA is the only Google managed service solution \nB,C - are not managed \nD- is a 3rd party"},{"timestamp":"1683608040.0","upvote_count":"2","comment_id":"892678","content":"Selected Answer: A\nWent with A","poster":"M25"},{"comments":[{"content":"D is incorrect, this is more far from a managed service based solution.","comment_id":"830667","timestamp":"1678092420.0","upvote_count":"2","poster":"tavva_prudhvi"}],"upvote_count":"1","timestamp":"1673528880.0","content":"The answer must be D as nowhere in the question has GCP been mention https://aadityachapagain.com/2020/09/distributed-training-with-slurm-on-gcp/","comment_id":"773462","poster":"Antmal"},{"content":"The Answer is D. As no where in the answer has GCP been mentioned. https://aadityachapagain.com/2020/09/distributed-training-with-slurm-on-gcp/","upvote_count":"1","comment_id":"773460","timestamp":"1673528820.0","poster":"Antmal"},{"timestamp":"1673001960.0","poster":"ares81","comment_id":"767529","content":"Selected Answer: A\nIt's A","upvote_count":"2"},{"comment_id":"739282","content":"Selected Answer: D\nHere the question is on workload management not on supporting frameworks slurm is a managed solution for workloads","timestamp":"1670518200.0","poster":"Moulichintakunta","upvote_count":"1"},{"timestamp":"1669211400.0","content":"Selected Answer: A\nNow it's Vertex AI (instead of AI Platform), but it's the best solution, no need to do anything more complicated","poster":"EFIGO","comment_id":"725174","upvote_count":"4"},{"upvote_count":"3","comment_id":"708332","content":"A - Vertex AI now","timestamp":"1667210580.0","poster":"abhi0706"},{"timestamp":"1660565280.0","content":"Selected Answer: A\nCorrect answer is \"A\"","comment_id":"647166","upvote_count":"1","poster":"GCP72"},{"comment_id":"557779","timestamp":"1646015160.0","poster":"caohieu04","upvote_count":"2","content":"Selected Answer: A\nA is correct"},{"poster":"vinit1101","content":"Selected Answer: A\nthe answer is A","timestamp":"1642851180.0","upvote_count":"2","comment_id":"529778"},{"comment_id":"518684","content":"A AI Platform should be correct\nYou can build pipelines in Airflow, Kubeflow, Dataflow but they need to be managed over AI platform or Vertex AI.","timestamp":"1641516420.0","poster":"NamitSehgal","upvote_count":"2"},{"content":"=New Question5=\nYou are building a linear model with over 100 input features, all with values between -1 and 1. You suspect that many features are non-informative. You want to remove the non-informative features from your model while keeping the informative ones in their original form. Which technique should you use? \n\nA. Use Principal Component Analysis to eliminate the least informative features.\nB. Use L 1 regularization to reduce the coefficients of uninformative features to 0. \nC. After building your model, use Shapley values to determine which features are the most informative. \nD. Use an iterative dropout technique to identify which features do not degrade the model when removed","timestamp":"1640183280.0","comment_id":"507156","upvote_count":"2","poster":"MisterHairy","comments":[{"content":"B - L1 regularization is useful for selective reduction to zero","timestamp":"1641118620.0","poster":"wviv","comment_id":"514924","upvote_count":"7"},{"upvote_count":"1","poster":"wences","comment_id":"542785","content":"Ans is C","timestamp":"1644281400.0"},{"timestamp":"1640183520.0","comment_id":"507164","upvote_count":"1","content":"Answer?","poster":"MisterHairy"},{"poster":"giaZ","content":"Best ans is B for how the question is posed. Sampled Shapley is a method for evaluating features attributions, but it's recommended especially for non-differentiable models..\nhttps://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview#sampled-shapley\nPlus, you'd need to build the model first, calculate those attributions, remove the features with low attributions, and re-train the model with fewer features..","timestamp":"1647422940.0","upvote_count":"2","comment_id":"568958"},{"timestamp":"1649232780.0","comment_id":"581692","content":"Ans is B. Ridge (L1) regularization pruning can be aggressive though.\nA - we need \"keeping the informative ones in their original form\"\nC - Shapley values here basically is checking the R2 statistic, which is not so useful in this situation. The right approach should be the Shapley regression.\nD - Something like backward elimination might remove informative features without properly testing all combinations.","poster":"eeah","upvote_count":"1"}]},{"upvote_count":"1","timestamp":"1639319940.0","poster":"ashii007","content":"Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. It provides no native support for machine learning framework. \nWhy is D still marked as correct answer, when it is clearly a wrong answer and other comments are in agreement.","comment_id":"500062"},{"comment_id":"495040","upvote_count":"1","timestamp":"1638786180.0","content":"My option is A. It requires a manged service. AI Platform is the one. \nhttps://cloud.google.com/ai-platform/prediction/docs/use-custom-container","poster":"alphard"},{"timestamp":"1634556900.0","poster":"mousseUwU","comment_id":"464090","upvote_count":"1","content":"Probably A"},{"poster":"gcper","upvote_count":"1","timestamp":"1631338020.0","content":"A\n\nIt is the only one that is a managed service and allows for custom containers","comment_id":"442825"},{"upvote_count":"3","poster":"Celia20210714","timestamp":"1626647640.0","comment_id":"409191","content":"ANS: A\n\nhttps://cloud.google.com/ai-platform/training/docs/containers-overview#advantages_of_custom_containers\nUse the ML framework of your choice. If you can't find an AI Platform Training runtime version that supports the ML framework you want to use, then you can build a custom container that installs your chosen framework and use it to run jobs on AI Platform Training."},{"content":"B - Kubeflow that can integrate data scientists into one managing platform","poster":"omar_bh","upvote_count":"1","comments":[{"comment_id":"441494","timestamp":"1631109600.0","upvote_count":"2","poster":"q4exam","content":"Kubeflow is not managed platform, AI Platform or Vertex AI is managed Kubeflow"},{"upvote_count":"3","poster":"sensev","timestamp":"1627052640.0","content":"I dont think B is correct, since the question mentioned some data sciensts using other frameworks than Tensorflow (PyTorch, theanos etc) - thus training with only TF Jobs on Kubeflow is incorrect.","comment_id":"412620"}],"timestamp":"1626438060.0","comment_id":"407854"},{"poster":"SparkExpedition","timestamp":"1626143520.0","upvote_count":"1","comments":[],"content":"I think it is D as the problem is about submitting jobs","comment_id":"405072"},{"content":"A - Vertex AI now","poster":"ralf_cc","upvote_count":"4","comment_id":"401792","timestamp":"1625740680.0"}],"topic":"1","answer_images":[],"question_text":"You manage a team of data scientists who use a cloud-based backend system to submit training jobs. This system has become very difficult to administer, and you want to use a managed service instead. The data scientists you work with use many different frameworks, including Keras, PyTorch, theano, Scikit-learn, and custom libraries. What should you do?","question_id":250,"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/54653-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"A","unix_timestamp":1622907600,"answer":"A","timestamp":"2021-06-05 17:40:00","exam_id":13,"answers_community":["A (95%)","5%"],"choices":{"A":"Use the AI Platform custom containers feature to receive training jobs using any framework.","C":"Create a library of VM images on Compute Engine, and publish these images on a centralized repository.","B":"Configure Kubeflow to run on Google Kubernetes Engine and receive training jobs through TF Job.","D":"Set up Slurm workload manager to receive jobs that can be scheduled to run on your cloud infrastructure."}}],"exam":{"provider":"Google","lastUpdated":"11 Apr 2025","isBeta":false,"isImplemented":true,"numberOfQuestions":304,"id":13,"isMCOnly":true,"name":"Professional Machine Learning Engineer"},"currentPage":50},"__N_SSP":true}