{"pageProps":{"questions":[{"id":"n03uaMNDRadsvpBGkYng","question_id":21,"timestamp":"2020-03-15 04:23:00","answer":"D","question_text":"You are designing a data processing pipeline. The pipeline must be able to scale automatically as load increases. Messages must be processed at least once and must be ordered within windows of 1 hour. How should you design the solution?","choices":{"B":"Use Apache Kafka for message ingestion and use Cloud Dataflow for streaming analysis.","C":"Use Cloud Pub/Sub for message ingestion and Cloud Dataproc for streaming analysis.","A":"Use Apache Kafka for message ingestion and use Cloud Dataproc for streaming analysis.","D":"Use Cloud Pub/Sub for message ingestion and Cloud Dataflow for streaming analysis."},"isMC":true,"answers_community":["D (100%)"],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/16629-exam-professional-data-engineer-topic-1-question-117/","question_images":[],"exam_id":11,"unix_timestamp":1584242580,"answer_images":[],"answer_description":"","answer_ET":"D","discussion":[{"content":"Answer should be D","upvote_count":"27","timestamp":"1615778580.0","comment_id":"64133","poster":"madhu1171"},{"poster":"[Removed]","comment_id":"66918","timestamp":"1616412300.0","content":"Answer - D","upvote_count":"14"},{"poster":"grshankar9","content":"Selected Answer: D\nKafka is recommended over Pub/Sub only when there is requirement of high throughput, complex streaming, more flexibility with customization and fine-tuning configurations or when application spans multiple cloud providers and requires more flexibility in deployment across different platforms.","upvote_count":"1","comment_id":"1342793","timestamp":"1737241260.0"},{"timestamp":"1722862920.0","poster":"NeoNitin","content":"Data proc is serverbased \nDataflow is serverless which is used to run pipelines which uses apache framework in the background. Just\nneed to mention the number of workers needed.\n\nso question saying we need scale automatically . so dataproc eliminate ho gaya \nnow Dataflow is correct , pub/sub is recommended for this scenario. D","upvote_count":"1","comment_id":"973042"},{"upvote_count":"2","content":"Selected Answer: D\ngoogle's preferred choice","poster":"dconesoko","comment_id":"760075","timestamp":"1703780400.0"},{"content":"D is the answer.","comment_id":"735845","upvote_count":"1","timestamp":"1701771840.0","poster":"zellck"},{"upvote_count":"1","poster":"Pime13","comment_id":"627787","timestamp":"1688632080.0","content":"Selected Answer: D\nAnswer should be D"},{"upvote_count":"1","comment_id":"592507","timestamp":"1682519400.0","content":"Selected Answer: D\nIt cannot be C because Dataproc is more suitable for Hadoop jobs.","poster":"VictorBa"},{"content":"Selected Answer: D\nPub/Sub + Dataflow","timestamp":"1673033640.0","comment_id":"518530","poster":"medeis_jar","upvote_count":"1"},{"upvote_count":"4","content":"Selected Answer: D\nD: Pub/Sub + Dataflow\nhttps://cloud.google.com/solutions/stream-analytics/\nhttps://cloud.google.com/blog/products/data-analytics/streaming-analytics-now-simpler-more-cost-effective-cloud-dataflow","poster":"MaxNRG","timestamp":"1672940700.0","comment_id":"517699"},{"upvote_count":"3","timestamp":"1671413880.0","poster":"hendrixlives","comment_id":"504568","content":"Selected Answer: D\nD: \"at least once and must be ordered within windows\" means Pub/Sub (at least once) with Dataflow (windows)."},{"timestamp":"1669441740.0","poster":"JG123","content":"Correct: D","comment_id":"487096","upvote_count":"3"},{"poster":"Chelseajcole","upvote_count":"8","content":"rule of thumb: If you see Kafka and Pub/Sub, always go with Pub/Sub in Google exam","comments":[{"upvote_count":"7","content":"Careful doing that: I got a question where you had to choose between Kafka and Pub/Sub... and the solution required to be able to replay all messages without time limit. So no Pub/Sub there.\nThis being a Google cert does not mean that they always force Google solutions.","comment_id":"505220","poster":"hendrixlives","timestamp":"1671513480.0"}],"comment_id":"455046","timestamp":"1664557440.0"},{"comment_id":"421967","timestamp":"1660027380.0","upvote_count":"2","content":"Answer is D","poster":"sandipk91"},{"content":"https://cloud.google.com/architecture/migrating-from-kafka-to-pubsub#comparing_features","timestamp":"1657060260.0","comment_id":"399495","poster":"awssp12345","upvote_count":"2"},{"comment_id":"397140","timestamp":"1656791460.0","poster":"sumanshu","upvote_count":"3","content":"Vote for D\n\nScaling - Dataflow.\nDelivery of confimed atleast 1 message - Pub/Sub"},{"poster":"Sush12","comment_id":"288478","upvote_count":"2","content":"Answer is D","timestamp":"1644604200.0"},{"timestamp":"1636534980.0","comment_id":"216471","upvote_count":"5","content":"Indeed the correct answer is Option D. \nAgain, not sure why Exam topic answer is deliberately chosen for a wrong answer, for such simple question.","comments":[{"upvote_count":"1","timestamp":"1667513040.0","content":"To make us think of each question while studying, not just trying to memorize answers :) it looks that \"correct\" answers are chosen randomly :)","comment_id":"472307","poster":"szefco"}],"poster":"Alasmindas"},{"timestamp":"1636471200.0","comment_id":"216033","upvote_count":"3","poster":"Cloud_Enthusiast","content":"D for GCP native solution"},{"timestamp":"1629567840.0","poster":"haroldbenites","comment_id":"163100","content":"D is correct","upvote_count":"3"},{"upvote_count":"3","content":"Answer D…Pub/Sub and Cloud Dataflow are native Google solution...","poster":"Rajuuu","comment_id":"131897","timestamp":"1625979060.0"},{"comment_id":"127450","upvote_count":"4","timestamp":"1625548080.0","content":"Answer is D..","poster":"Rajuuu"},{"poster":"[Removed]","content":"Answer: D\nDescription: Dataflow has autoscaling feature and pubsub is best solution","comment_id":"68785","upvote_count":"8","timestamp":"1616907960.0"},{"comment_id":"67013","upvote_count":"6","content":"Answer D","poster":"Rajokkiyam","timestamp":"1616430900.0"}]},{"id":"rC7tM88fnZ6WqRquVRjW","question_id":22,"answer":"B","choices":{"D":"Create a table for each department. Assign the department leads the role of Editor, and assign the data analysts the role of Viewer on the project the table is in.","A":"Create a dataset for each department. Assign the department leads the role of OWNER, and assign the data analysts the role of WRITER on their dataset.","B":"Create a dataset for each department. Assign the department leads the role of WRITER, and assign the data analysts the role of READER on their dataset.","C":"Create a table for each department. Assign the department leads the role of Owner, and assign the data analysts the role of Editor on the project the table is in."},"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/79462-exam-professional-data-engineer-topic-1-question-118/","topic":"1","answers_community":["B (68%)","D (32%)"],"discussion":[{"upvote_count":"12","content":"Old question. It's done using IAM nowadays: bigquery.dataEditor and bigquery.dataViewer","poster":"juliobs","timestamp":"1679231160.0","comment_id":"843763"},{"timestamp":"1662207180.0","upvote_count":"9","poster":"AWSandeep","comment_id":"658424","content":"Selected Answer: B\nB. Create a dataset for each department. Assign the department leads the role of WRITER, and assign the data analysts the role of READER on their dataset."},{"timestamp":"1729712400.0","upvote_count":"2","content":"Selected Answer: D\nNo writer, reader role","poster":"SamuelTsch","comment_id":"1302183"},{"upvote_count":"2","content":"Selected Answer: D\nWRITER role is not there in roles of BigQuery table/dataset","poster":"mothkuri","timestamp":"1709655180.0","comment_id":"1166601"},{"upvote_count":"2","timestamp":"1703146500.0","content":"Answer : D. There is no role called WRITER or READER as preliminary role.","poster":"Kalai_1","comment_id":"1102256"},{"content":"Selected Answer: B\nboth C & D violate the principle of least privilege.\nA talks about OWNER and WRITER roles, and the analyst doesn't need a writer role.\nSo we're left with B.","upvote_count":"2","timestamp":"1685556120.0","poster":"forepick","comment_id":"911481"},{"upvote_count":"1","comment_id":"874719","poster":"Joane_","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery","timestamp":"1681911360.0"},{"timestamp":"1678670940.0","upvote_count":"2","poster":"midgoo","content":"Selected Answer: B\nB - Lead needs to have the role to create tables and also Analyst only need to read","comment_id":"837523"},{"comment_id":"820769","content":"Answer B:\nWhy not D, mentioned in question: Data lead will create tables in dataset. Imagine, other department leads are creating unnecessory tables in shared dataset and you are struggling to find your tables as everyday there are some new tables. Headache right ? better to give them seperate dataset and do whatever you want in that dataset.","timestamp":"1677259260.0","upvote_count":"5","poster":"musumusu"},{"content":"Vote B, both BD can fullfill the job requirement but B is on dataset level and D on project level. \"By default, granting access to a project also grants access to datasets within it.\" D may issue unnecessary accesses to other content in the project.","poster":"xj_kevin","comment_id":"803571","timestamp":"1675966500.0","upvote_count":"3"},{"comment_id":"781789","content":"Interestingly enough - I know believe the answer is A...\nDeleting is not the same as modify...","upvote_count":"1","poster":"desertlotus1211","timestamp":"1674180960.0"},{"comment_id":"781779","comments":[{"content":"I apologize - I thought B said Owner...\nThis questions makes no sense now...","timestamp":"1674180480.0","upvote_count":"1","poster":"desertlotus1211","comment_id":"781784"}],"content":"Answer is B: https://cloud.google.com/bigquery/docs/access-control\nThe question ask for the lead to be able to: \nCREATE, UPDATE, and SHARE with the team...\n\nBigQuery Data Owner can do that\n(roles/bigquery.dataOwner)\nWhen applied to a table or view, this role provides permissions to:\n\nRead and update data and metadata for the table or view.\nShare the table or view.\nDelete the table or view.\n\nEditor cannot do that.\n\nThoughts?","poster":"desertlotus1211","timestamp":"1674180180.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1670440740.0","content":"Selected Answer: D\nIt's D, because this is an outdated question, before IAM you cannot set Editor to a dataset; but the best practice is: Create a dataset for each department. Assign the department leads the role of EDITOR(NOT OWNER), and assign the data analysts the role of READER on their dataset.","comment_id":"738275","comments":[{"comment_id":"747138","comments":[{"timestamp":"1672211160.0","upvote_count":"1","comment_id":"759462","poster":"Wonka87","content":"and to supplement why does it need viewer role on the project the table is in?"}],"poster":"jkhong","content":"Dude, I know there are updates to IAM, but the key point of the question is to have the leads have table creation and update roles... So they already need roles at the dataset level and hence C and D is out. We wouldn't be able to memorise all the roles, but clearly we cannot provide access on a table level...","timestamp":"1671192120.0","upvote_count":"7"}],"poster":"odacir"},{"content":"Wow B is an answer \nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles#dataset-basic-roles","poster":"Atnafu","comment_id":"716495","timestamp":"1668235080.0","upvote_count":"4"},{"timestamp":"1667226660.0","comment_id":"708490","poster":"MisuLava","upvote_count":"2","content":"Selected Answer: D\nIt CANNOT BE B BECAUSE OF : \n\n\n\nCaution: BigQuery's dataset-level basic roles existed prior to the introduction of IAM. We recommend that you minimize the use of basic roles. In production environments, don't grant basic roles unless there is no alternative. Instead, use predefined IAM roles.\n\nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles","comments":[{"comment_id":"781776","upvote_count":"1","content":"Ummm owner is a predefined role\nhttps://cloud.google.com/bigquery/docs/access-control\nBigQuery Data Owner \n(roles/bigquery.dataOwner)","timestamp":"1674180000.0","poster":"desertlotus1211"}]},{"upvote_count":"4","content":"Selected Answer: B\nI vote B because C and D says that the role is on the project that the table is in, this mean that the role is at the project level that implies that: \nIf you create a dataset in a project that contains any editors, BigQuery grants those users the bigquery.dataEditor predefined role for the new dataset. (from https://cloud.google.com/bigquery/docs/access-control-basic-roles#project-basic-roles)\n\nA can't not be because the analysts, in this case, can access the data.\n\nB grant to the leads update their datasets, that's mean create tables, and the analysts only read their datasets.","poster":"josrojgra","comment_id":"693733","timestamp":"1665648780.0"},{"comment_id":"691857","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles\n\nCaution: BigQuery's dataset-level basic roles existed prior to the introduction of IAM. We recommend that you minimize the use of basic roles. In production environments, don't grant basic roles unless there is no alternative. Instead, use predefined IAM roles.","poster":"VipinSingla","timestamp":"1665472620.0","upvote_count":"1"},{"comment_id":"659854","poster":"nwk","comments":[{"comment_id":"663103","content":"Sorry but you are wrong. There is WRITER and READER role for dataset see them in this documentation. I was also confused at the beginning:\nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles","timestamp":"1662611700.0","poster":"Remi2021","upvote_count":"6"}],"content":"Vote D, there is only Viewer, Editor and Owner roles for BQ\nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles","timestamp":"1662362820.0","upvote_count":"5"},{"comment_id":"658059","poster":"ducc","timestamp":"1662179160.0","upvote_count":"4","content":"Selected Answer: B\nB is correct"},{"comment_id":"657547","poster":"damaldon","timestamp":"1662132120.0","upvote_count":"3","content":"Ans.B\nLeads needs to create tables, meaning they need a dataset and analysts only need read permissions"}],"question_text":"You need to set access to BigQuery for different departments within your company. Your solution should comply with the following requirements:\n✑ Each department should have access only to their data.\n✑ Each department will have one or more leads who need to be able to create and update tables and provide them to their team.\n✑ Each department has data analysts who need to be able to query but not modify data.\nHow should you set access to the data in BigQuery?","exam_id":11,"unix_timestamp":1662132120,"answer_images":[],"timestamp":"2022-09-02 17:22:00","isMC":true,"question_images":[],"answer_description":""},{"id":"hS8Y1WJEKJnYrrQuby8X","timestamp":"2020-03-22 12:41:00","isMC":true,"answers_community":["A (92%)","8%"],"question_images":[],"answer_images":[],"question_text":"You operate a database that stores stock trades and an application that retrieves average stock price for a given company over an adjustable window of time. The data is stored in Cloud Bigtable where the datetime of the stock trade is the beginning of the row key. Your application has thousands of concurrent users, and you notice that performance is starting to degrade as more stocks are added. What should you do to improve the performance of your application?","choices":{"B":"Change the row key syntax in your Cloud Bigtable table to begin with a random number per second.","D":"Use Cloud Dataflow to write a summary of each day's stock trades to an Avro file on Cloud Storage. Update your application to read from Cloud Storage and Cloud Bigtable to compute the responses.","C":"Change the data pipeline to use BigQuery for storing stock trades, and update your application.","A":"Change the row key syntax in your Cloud Bigtable table to begin with the stock symbol."},"answer_description":"","topic":"1","answer_ET":"A","discussion":[{"upvote_count":"41","poster":"[Removed]","timestamp":"1585379520.0","comment_id":"68787","content":"Answer: A\nDescription: Timestamp at starting of rowkey causes bottleneck issues"},{"content":"Stock symbol will be similar for most of the records, so it's better to start with random number.. Answer should be B","comments":[{"content":"You never use something called random number in bigtable rowkey because it gives you no use in querying possibilities, since we can't run sql querys in bigtable we should not randomise rowkeys in bigtable. \nDon't confuse the above point with the hotspot logic, both are different if you think so.\n\nAnd another thing is, what you said can be good choice if we are using cloud spanner and trying to comeup with primary key situation, since there we can always run sql query.\n\nI think you got the point now.","upvote_count":"13","comment_id":"476195","poster":"Abhi16820","timestamp":"1636633440.0"},{"timestamp":"1588140060.0","content":"I agree with u","comment_id":"81139","upvote_count":"3","poster":"taepyung"},{"comment_id":"294820","timestamp":"1613805600.0","comments":[{"upvote_count":"3","poster":"Yonghai","timestamp":"1640577660.0","content":"for a given company, the data poits starts with the same stock symbol. The dataset is not distrubuted. It is not a good option.","comment_id":"510001"}],"upvote_count":"6","content":"it can start with stock symbol concated with timestamp can be a good row key design","poster":"karthik89"}],"comment_id":"73540","timestamp":"1586668380.0","upvote_count":"13","poster":"kichukonr"},{"timestamp":"1734294540.0","poster":"clouditis","comment_id":"1327049","upvote_count":"1","content":"Selected Answer: B\nthe most plausible option to pick here is B, A can introduce hot-spotting"},{"poster":"Vineet_Mor","comment_id":"1271224","upvote_count":"2","timestamp":"1724407980.0","content":"B is correct, By introducing a random number or a hash at the beginning of the row key, you distribute the writes and reads more evenly across the Bigtable cluster, thereby improving performance under heavy load.\n\nWHY NOT A?\nThis might still cause hotspots if certain stocks are more popular than others. It could lead to uneven load distribution, which wouldn't solve the performance degradation problem."},{"upvote_count":"2","poster":"Sofiia98","comment_id":"1116033","timestamp":"1704647700.0","content":"Selected Answer: A\nAnswer is A."},{"comment_id":"820774","upvote_count":"12","content":"Answer A:\nTrick to remember: Row-key adjustment always be like in decending order. \n#<<Least value>>#<<Lesser value>>\nFor example: \n1. #<<Earth>>#<<continents>>#<<countries>>#<<cities>> and so on.. \n2. #<<Stock>>#<<users>>#timestamp.. \nin 99% cases timestamp will be in the end, as its smallest division...","poster":"musumusu","comments":[{"upvote_count":"1","poster":"piyush7777","timestamp":"1692202260.0","comment_id":"982752","content":"Awesome!"}],"timestamp":"1677259620.0"},{"upvote_count":"6","timestamp":"1670235420.0","comment_id":"735838","poster":"zellck","comments":[{"comment_id":"762443","timestamp":"1672454820.0","upvote_count":"1","poster":"AzureDP900","content":"I agree with you . A is right"}],"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys\nIt's important to create a row key that makes it possible to retrieve a well-defined range of rows. Otherwise, your query requires a table scan, which is much slower than retrieving specific rows.\n\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys-avoid\nSome types of row keys can make it difficult to query your data, and some result in poor performance. This section describes some types of row keys that you should avoid using in Bigtable.\n- Row keys that start with a timestamp. This pattern causes sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, precede it with a high-cardinality value like a user ID to avoid hotspots."},{"content":"Selected Answer: A\nA: https://cloud.google.com/bigtable/docs/schema-design-time-series#prefer_rows_to_column_versions","upvote_count":"4","poster":"MaxNRG","comment_id":"517703","timestamp":"1641405000.0"},{"upvote_count":"1","comment_id":"487101","timestamp":"1637906340.0","poster":"JG123","content":"Correct: A"},{"timestamp":"1636507440.0","upvote_count":"1","comment_id":"475153","poster":"JayZeeLee","content":"A and B would both work, since both would distribute the work. This question is not framed properly."},{"poster":"sumanshu","upvote_count":"3","comment_id":"397159","timestamp":"1625258040.0","content":"Vote for A"},{"comment_id":"302712","poster":"Jay3244","content":"Option A.\nBelow document explains \nHaving EXCHANGE and SYMBOL in the leading positions in the row key will naturally distribute activity.\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series","upvote_count":"5","timestamp":"1614786240.0"},{"upvote_count":"2","comment_id":"222885","timestamp":"1605799020.0","content":"I think A","poster":"arghya13"},{"upvote_count":"1","content":"Catch here is current Rowley starts with timestamp which should not be in the starting or end position so symbolmshould be prefixed before timestamp","timestamp":"1605696300.0","poster":"kavs","comment_id":"221791"},{"comment_id":"216039","timestamp":"1604935560.0","content":"A is correct..A Good ROW KEY has to be an ID followed by timestamp. Stock symbol in this case works as an ID","poster":"Cloud_Enthusiast","upvote_count":"6"},{"poster":"kino2020","comment_id":"189675","timestamp":"1601384220.0","upvote_count":"2","content":"A.\nYou can find an example in Google's introductory guide.\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series?hl=ja#financial_market_data"},{"content":"I think A would be best practice. Adding random numbers as start of rowkey doesn't help with troubleshooting","comment_id":"181383","timestamp":"1600405320.0","upvote_count":"3","poster":"Diqtator"},{"upvote_count":"1","poster":"Tanmoyk","timestamp":"1600056720.0","comment_id":"179059","content":"B should be the answer as adding random numbers in the beginning of the rowkey will distributes data across multiple nodes"},{"comment_id":"163109","poster":"haroldbenites","upvote_count":"1","timestamp":"1598032800.0","comments":[{"content":"Aggree with promotion field and salting. But, there is no constant. A stock symbol is a unique series of letters assigned to security for trading purposes.","upvote_count":"2","timestamp":"1598138580.0","poster":"atnafu2020","comment_id":"163988"},{"poster":"daghayeghi","upvote_count":"1","comment_id":"293392","comments":[{"comment_id":"293394","timestamp":"1613655480.0","content":", then A is correct.","upvote_count":"1","poster":"daghayeghi"}],"timestamp":"1613655420.0","content":"A is correct:\nYou deny your sentence, User ID means Stock Symbol, then B is correct."}],"content":"B is correct.\nA is incorrect. The docuemntation don´t recommend constants in the row key because the balance is not efficiente. The are 2 methods to avoid hotspoting. Promotion Field (use a UserId BEFORE the timestamp) and Salting (use timestamp-hash divided by 3 y put it before the timestamp)"},{"poster":"Rajuuu","content":"Answer is A ..Prefix the timestamp of the rowdy to avoid hot spotting.","comment_id":"131895","upvote_count":"3","timestamp":"1594442400.0"},{"poster":"VishalB","comment_id":"129165","upvote_count":"6","content":"A,\nPromote a Stock Symbol to Row Key at the start, it will would distribute the load across different nodes","timestamp":"1594143480.0"},{"upvote_count":"5","timestamp":"1584895020.0","content":"Answer A","poster":"Rajokkiyam","comment_id":"67015"},{"poster":"[Removed]","timestamp":"1584877260.0","content":"Should be A","comment_id":"66925","upvote_count":"6"}],"exam_id":11,"unix_timestamp":1584877260,"question_id":23,"answer":"A","url":"https://www.examtopics.com/discussions/google/view/17244-exam-professional-data-engineer-topic-1-question-119/"},{"id":"0yCG69yBzIkXaaGeWygg","choices":{"C":"Put each client's BigQuery dataset into a different table.","F":"Use the appropriate identity and access management (IAM) roles for each client's users.","B":"Load data into a different dataset for each client.","E":"Only allow a service account to access the datasets.","D":"Restrict a client's dataset to approved users.","A":"Load data into different partitions."},"question_id":24,"unix_timestamp":1584259620,"answer_images":[],"question_text":"Your company handles data processing for a number of different clients. Each client prefers to use their own suite of analytics tools, with some allowing direct query access via Google BigQuery. You need to secure the data so that clients cannot see each other's data. You want to ensure appropriate access to the data.\nWhich three steps should you take? (Choose three.)","question_images":[],"isMC":true,"answer_ET":"BDF","exam_id":11,"topic":"1","answer":"BDF","timestamp":"2020-03-15 09:07:00","answers_community":["BDF (85%)","BEF (15%)"],"discussion":[{"timestamp":"1597684680.0","content":"My vota also goes for B,D,F","poster":"saurabh1805","upvote_count":"13","comment_id":"160222"},{"comments":[{"upvote_count":"5","poster":"awssp12345","timestamp":"1625239860.0","content":"yes, that is precisely why we need to eliminate E.","comment_id":"397001"}],"comment_id":"330760","poster":"sumanshu","content":"Some voted for 'E' i.e. E. Only allow a service account to access the datasets.\nNot sure why ?\n\nif we gave access ONLY to service account - Does not it mean - we need to access BigQuery using Some Code (by mentioning Service account credentials there) OR using some other resource like VM)\nIn this case - i think person can't even access the Big Query Service via UI (if we give access only to Service account). Correct me if there is option on UI as well","timestamp":"1617836580.0","upvote_count":"9"},{"timestamp":"1737172500.0","comment_id":"1342415","content":"Selected Answer: BDF\nA is wrong because partitions do not provide access boundaries between the clients. All partitions within a table are accessible to anyone with access to the table.\nC is wrong because tables within the same dataset share the same access controls.\nE is wrong because service accounts are typically used for automated or backend processes, not client-specific access.","poster":"cqrm3n","upvote_count":"1"},{"poster":"Nirca","comment_id":"771344","comments":[{"comment_id":"817523","timestamp":"1677046380.0","content":"For C. What if thinking about that there are tables by clients? such as customer_clients_a table and giving IAM from each table to users??..","upvote_count":"1","poster":"jin0"}],"content":"Selected Answer: BDF\nB, D, F!\nC - is technically wrong . tables are being logically stored in a single dataset.\nA - Partitioning data is for improving performance. once you SQL (select) the table, you can not control the data being selected for the developer.","timestamp":"1727154840.0","upvote_count":"2"},{"poster":"samdhimal","comments":[{"upvote_count":"1","comment_id":"1318607","poster":"certs4pk","timestamp":"1732707600.0","content":"so, we r assuming there is no 'common' data shared by different clients! if yes, will B still ba correct option?"}],"timestamp":"1727154840.0","upvote_count":"4","comment_id":"799145","content":"Selected Answer: BDF\nB. Load data into a different dataset for each client.\nD. Restrict a client's dataset to approved users.\nF. Use the appropriate identity and access management (IAM) roles for each client's users.\n\nBy loading each client's data into a separate dataset, you ensure that each client's data is isolated from the data of other clients. Restricting access to each client's dataset to only approved users, as specified in D, further enhances data security by ensuring that only authorized users can access the data. By using appropriate IAM roles for each client's users, as specified in F, you can grant different levels of access to different clients and their users, ensuring that each client has only the level of access required for their specific needs."},{"timestamp":"1727154840.0","poster":"suku2","content":"Selected Answer: BDF\nB. Load data into a different dataset for each client.\nD. Restrict a client's dataset to approved users.\nF. Use the appropriate identity and access management (IAM) roles for each client's users.","upvote_count":"3","comment_id":"1008518"},{"upvote_count":"5","timestamp":"1727154840.0","comment_id":"1050482","poster":"rtcpost","content":"Selected Answer: BDF\nB. Load data into a different dataset for each client: Organize the data into separate datasets for each client. This ensures data isolation and simplifies access control.\n\nD. Restrict a client's dataset to approved users: Implement access controls by specifying which users or groups are allowed to access each client's dataset. This restricts data access to approved users only.\n\nF. Use the appropriate identity and access management (IAM) roles for each client's users: Assign IAM roles based on client-specific requirements to manage permissions effectively. IAM roles help control access at a more granular level, allowing you to tailor access to specific users or groups within each client's dataset.\n\nThese steps ensure that each client's data is separated, and access is controlled based on client-specific requirements. Options A, C, and E, while important in other contexts, are not sufficient on their own to ensure client data isolation and access control in a multi-client environment."},{"poster":"philli1011","comment_id":"1131588","content":"My Vote is BDF.\nI was thinking BEF but the question shows that the Big Query warehouse will be accessed by both direct users and other applications, as preferred by each customer.","upvote_count":"1","timestamp":"1706180940.0"},{"comment_id":"1131565","content":"Selected Answer: BDF\nagreed B,D,F","poster":"SoloLeveling","upvote_count":"1","timestamp":"1706178240.0"},{"comment_id":"1065086","upvote_count":"1","poster":"RT_G","timestamp":"1699382520.0","content":"Selected Answer: BDF\nAgree with others"},{"upvote_count":"2","poster":"imran79","comment_id":"1027022","timestamp":"1696646400.0","content":"the answers are B, D, and F.\nTo ensure that clients cannot see each other's data and have appropriate access, you would want to:\n\n Segregate the data by client.\n Restrict access to each client's data.\n Use proper identity and access management techniques."},{"poster":"Chi_Wang","comment_id":"1006657","upvote_count":"2","timestamp":"1694611020.0","content":"Selected Answer: BDF\nB,D,F is the answer"},{"poster":"elitedea","comment_id":"836436","content":"BDF is right","timestamp":"1678562220.0","upvote_count":"4"},{"comment_id":"771342","upvote_count":"1","poster":"Nirca","content":"B, D, F! \nC - is technically wrong . tables are being logically stored in a single dataset. \n A - Partitioning data is for improving performance. once you SQL (select) the table, you can not control the data being selected for the developer.","timestamp":"1673350680.0"},{"timestamp":"1672195200.0","comment_id":"759288","content":"Please why is DEF not correct?","upvote_count":"2","poster":"DeeData"},{"content":"Selected Answer: BDF\nAgree BDF","poster":"Kyr0","timestamp":"1672049280.0","comment_id":"757336","upvote_count":"1"},{"content":"Selected Answer: BEF\nWhy no E? E has a lot a sense to me, they have external analytical tools, and the best practice is to give access to external trow service account, and not throw user level.","upvote_count":"4","comments":[{"comment_id":"754044","upvote_count":"2","timestamp":"1671784980.0","content":"Yes, I also wonder why not E instead of D","comments":[{"upvote_count":"1","poster":"VincentMenzel","comment_id":"968095","content":"Because the client might want a mixture of SAs and user accounts. Maybe they have a Big Data Team that wants to run queries and access the data with their account. Also SAs do not help with segregating the data","timestamp":"1690806300.0"}],"poster":"ler_mp"},{"poster":"cloudyy","content":"I hesitated over this too, but the question talks about direct access query so that's the reason for not choosing E.","timestamp":"1677441420.0","upvote_count":"4","comment_id":"822902"}],"poster":"odacir","comment_id":"740942","timestamp":"1670672880.0"},{"poster":"lalli117","content":"agreed, B,D,F","timestamp":"1663006740.0","comment_id":"667327","upvote_count":"1"},{"poster":"hui521","upvote_count":"1","comment_id":"649899","timestamp":"1661109900.0","comments":[{"timestamp":"1665618540.0","upvote_count":"3","poster":"Whoswho","content":"\"only a(one) service account to access all datasets\".which is wrong because we're actually trying to implement access control on clients datasets and data.","comment_id":"693455"},{"timestamp":"1669637460.0","upvote_count":"1","content":"because of this: \"with some allowing direct query access via Google BigQuery\"","poster":"sfsdeniso","comment_id":"729065"}],"content":"why is C not correct? Can someone explain?"},{"content":"These questions rock!! They helped me pass the exam on the first attempt!! Make sure you study all of these questions carefully with their associated documentation.","upvote_count":"5","poster":"jackjackington","comment_id":"604972","timestamp":"1653155460.0"},{"upvote_count":"1","content":"Selected Answer: BDF\ncorrect","timestamp":"1650428040.0","poster":"gamer5005","comment_id":"588432"},{"poster":"Arkon88","comment_id":"559372","timestamp":"1646222400.0","content":"Selected Answer: BDF\nagreed B,D,F","upvote_count":"1"},{"poster":"anji007","timestamp":"1634220540.0","comment_id":"462114","content":"Ans: B, D and F.","upvote_count":"4"},{"comment_id":"346745","poster":"prasioso","content":"I agree with B, D, F. But could someone explain how A (loading different partitions) and C (different tables) would violate the stated requirements?","timestamp":"1619861760.0","upvote_count":"2","comments":[{"upvote_count":"1","content":"I'm not so sure that C would violate the requirements anymore now that you can set access controls on the table level. I don't think that was possible when this question was created.\n\nAnswer A just isn't relevant to the goal. Partitioning data in BigQuery is more about improving performance and is not used for access controls.","comments":[{"timestamp":"1621876920.0","poster":"thesysops","upvote_count":"1","comment_id":"365761","content":"Actually, C still would probably be wrong because it doesn't say anything about setting the table level access which would still be required split customer's data by table."}],"poster":"thesysops","timestamp":"1621876740.0","comment_id":"365753"},{"poster":"sergio6","timestamp":"1634486040.0","upvote_count":"4","comment_id":"463609","content":"How could you put a dataset into a table ? A dataset is a container of tables"}]},{"content":"ans BDF","upvote_count":"1","comment_id":"327992","poster":"userd83","timestamp":"1617542640.0"},{"content":"Correct : B,D,F","upvote_count":"1","poster":"lbhhoya82","timestamp":"1617059040.0","comment_id":"323856"},{"upvote_count":"2","timestamp":"1614520680.0","content":"Should be B,D and F .","poster":"sid091","comment_id":"300817"},{"comment_id":"284946","poster":"naga","timestamp":"1612628280.0","upvote_count":"3","content":"Correct BDF"},{"timestamp":"1605087720.0","upvote_count":"2","comments":[{"upvote_count":"1","poster":"godot","content":"You need multiple service accounts not one","comment_id":"569842","timestamp":"1647530400.0"},{"timestamp":"1611073020.0","poster":"abunasar786","content":"Service account is associated with a service usually","upvote_count":"4","comment_id":"271355"}],"comment_id":"217184","poster":"Radhika7983","content":"According to me the answer is B, D AND E. \nWhen an identity calls a Google Cloud API, BigQuery requires that the identity has the appropriate permissions to use the resource. You can grant permissions by granting roles to a user, a group, or a service account.\n\nThe question is about securing the data so that clients cannot see each other's data. A service account is a Google Account that is associated with your Google Cloud project"},{"upvote_count":"4","comment_id":"193840","timestamp":"1601946720.0","content":"agreed b, d, f","poster":"Darlee"},{"poster":"atnafu2020","content":"B,D,E\nE-To grant access to a BigQuery resource, assign one or more roles to a user, group, or service account. You can grant access at the following BigQuery resource levels:\norganization or Google Cloud project level\ndataset level\ntable(beta) or view level\nF-Cloud project level\nWhen you assign roles at the organization and project level, you provide permission to run BigQuery jobs or to access all of a project's BigQuery resources. These users already have project-level permission. The question is how to restrict them?","comments":[{"comment_id":"569841","content":"it s refering to \"A service account\", one would need several services accounts to allow access segregation...","upvote_count":"1","timestamp":"1647530340.0","poster":"godot"}],"upvote_count":"3","comment_id":"161767","timestamp":"1597865820.0"},{"upvote_count":"5","poster":"Archy","comment_id":"150653","timestamp":"1596565740.0","content":"B,D,F :)"}],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/16644-exam-professional-data-engineer-topic-1-question-12/"},{"id":"RAml3VhCpCpt15Kp1mse","timestamp":"2020-03-22 12:46:00","isMC":true,"answers_community":["B (91%)","9%"],"answer_images":[],"question_text":"You are operating a Cloud Dataflow streaming pipeline. The pipeline aggregates events from a Cloud Pub/Sub subscription source, within a window, and sinks the resulting aggregation to a Cloud Storage bucket. The source has consistent throughput. You want to monitor an alert on behavior of the pipeline with Cloud\nStackdriver to ensure that it is processing data. Which Stackdriver alerts should you create?","question_images":[],"choices":{"C":"An alert based on a decrease of instance/storage/used_bytes for the source and a rate of change increase of subscription/ num_undelivered_messages for the destination","B":"An alert based on an increase of subscription/num_undelivered_messages for the source and a rate of change decrease of instance/storage/ used_bytes for the destination","A":"An alert based on a decrease of subscription/num_undelivered_messages for the source and a rate of change increase of instance/storage/ used_bytes for the destination","D":"An alert based on an increase of instance/storage/used_bytes for the source and a rate of change decrease of subscription/ num_undelivered_messages for the destination"},"topic":"1","answer_description":"","answer_ET":"B","discussion":[{"content":"You would want to get alerted only if Pipeline fails & not if it is running fine. I think Option [B] is correct, because in event of Pipeline failure :\n1) subscription/ num_undelivered_messages would pile up at a constant rate as the source has consistent throughput\n2) instance/storage/ used_bytes will get closer to zero. Hence need to monitor it's rate of change","upvote_count":"28","comments":[{"timestamp":"1641113160.0","poster":"Barniyah","comment_id":"124771","upvote_count":"5","content":"Yes, you are right, it should be B:\nThank you"},{"comment_id":"500464","upvote_count":"1","poster":"marioferrulli","content":"Why would the instance/storage/used_bytes get closer to zero? If there's an error at a certain point, wouldn't we just see that the used_bytes remain constant while the num_undelivered_messages increases? I don't get why the destination's used bytes should decrease.","timestamp":"1686638100.0","comments":[{"poster":"baubaumiaomiao","timestamp":"1687183080.0","content":"\"If there's an error at a certain point, wouldn't we just see that the used_bytes remain constant while the num_undelivered_messages increases?\"\nIt's the rate of change, not the absolute value","upvote_count":"2","comment_id":"504975"},{"content":"\"rate of change decrease of instance/storage/ used_bytes\" - if rate of instance/storage/ used_bytes decreases that means less data is written - so something is wrong with the pipeline.\nIt's not used bytes that decreases - it's rate of change decreases. \nExample: if everything works fine your pipeline writes 5MB/s to the sink. If it decreases to 0.1MB/s it means something is wrong","upvote_count":"6","poster":"szefco","comment_id":"504187","timestamp":"1687082760.0"}]}],"poster":"dambilwa","timestamp":"1640530680.0","comment_id":"120548"},{"timestamp":"1632303960.0","comment_id":"66927","upvote_count":"21","content":"Correct - B","poster":"[Removed]"},{"comment_id":"1399490","poster":"desertlotus1211","timestamp":"1742174220.0","content":"Selected Answer: A\nIt should be Answer A. You want to see in being processed versus looking for a bottleneck.","upvote_count":"1"},{"upvote_count":"15","content":"Selected Answer: B\nFor those who may get confuse at the start by the term 'subscription/num_undelivered_messages', it is not a division. It is the full path of the metric. So we should just read it as 'num_undelivered_messages'. The same for 'used_bytes'.\n\nSo if we see the source have more backlog (more num_undelivered_messages), or the destination ultilization going down, that is the indicator of something going wrong","timestamp":"1726185840.0","comments":[{"poster":"desertlotus1211","content":"the Answer is A. You want to see it working.","comment_id":"1399491","upvote_count":"1","comments":[{"comment_id":"1399492","upvote_count":"1","poster":"desertlotus1211","content":"you're looking for evidence that it's working:\n' that it is processing data....'","timestamp":"1742174340.0"}],"timestamp":"1742174280.0"},{"comment_id":"919342","content":"great explanation thanks !","timestamp":"1733757240.0","poster":"kryzo","upvote_count":"2"}],"comment_id":"837541","poster":"midgoo"},{"content":"Answer B:\nTrick: In stackdriver always put Alert for Subscriber + CPU\nSubscriber - num of undelivered message INCREASE alert\nCPU - Instance or storage DECREASE alert. \nMake sense right !","timestamp":"1724513580.0","poster":"musumusu","upvote_count":"3","comment_id":"820782"},{"upvote_count":"1","comment_id":"770242","timestamp":"1720513680.0","comments":[{"comment_id":"781796","poster":"desertlotus1211","upvote_count":"2","content":"look here: https://cloud.google.com/monitoring/api/metrics_gcp\n\ninstance/storage/used_bytes GA\nStorage used.","timestamp":"1721434980.0"}],"content":"Nobody seems to pay attention to instance/storage/used_bytes. I only find this metric for Spanner. \nhttps://cloud.google.com/monitoring/api/metrics_gcp#gcp-spanner\n\nWhile Dataflow processes and stores everything in Cloud Storage, Spanner could only be the source.\nhttps://cloud.google.com/spanner/docs/change-streams\n\nAlso, if it is either A or B, the instance/storage/used_bytes metric does not make sense for the destination, which is Cloud Storage.\n\nCan anyone help me understand?","poster":"atlan"},{"timestamp":"1719948240.0","content":"B. An alert based on an increase of subscription/num_undelivered_messages for the source and a rate of change decrease of instance/storage/ used_bytes for the destination","poster":"AzureDP900","upvote_count":"1","comment_id":"764009"},{"content":"B is right","timestamp":"1719708600.0","poster":"AzureDP900","comment_id":"762445","upvote_count":"1"},{"comment_id":"760799","timestamp":"1719644400.0","poster":"Catweazle1983","upvote_count":"1","content":"Selected Answer: A\nAn alert based on a decrease of subscription/num_undelivered_messages for the source and a rate of change increase of instance/storage/ used_bytes for the destination\n\n10 subscriptions / 1 undelivered messages = 10\n10 subscriptions / 5 undelivered messages = 2\nYou clearly want to be alerted when the number of undelivered messages increases. The ratio then decreases. In my example from 10 to 2.","comments":[{"comment_id":"844237","timestamp":"1726771620.0","content":"subscription/num_undelivered_messages is a path, not a division.","poster":"squishy_fishy","upvote_count":"1"}]},{"poster":"zellck","timestamp":"1717574880.0","upvote_count":"3","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog\nMonitor message backlog\nTo ensure that your subscribers are keeping up with the flow of messages, create a dashboard. The dashboard can show the following backlog metrics, aggregated by resource, for all your subscriptions:\n- Unacknowledged messages (subscription/num_undelivered_messages) to see the number of unacknowledged messages.","comment_id":"735832"},{"content":"Selected Answer: B\nIncrease subscription/num delivered message\ndecrease instance/storage/used bytes","timestamp":"1708371900.0","upvote_count":"1","comment_id":"649065","poster":"A1000"},{"content":"Selected Answer: B\nCorrect - B","comment_id":"627785","poster":"Pime13","timestamp":"1704536820.0","upvote_count":"1"},{"content":"Correct: B","poster":"JG123","comment_id":"487104","timestamp":"1685073660.0","upvote_count":"2"},{"upvote_count":"2","content":"isn't B and C are same.","comment_id":"476198","timestamp":"1683800880.0","poster":"Abhi16820"},{"timestamp":"1683674820.0","poster":"JayZeeLee","upvote_count":"3","comment_id":"475154","content":"B.\nIt's useful to monitor the source that keeps sending data while the destination that doesn't take anything in."},{"content":"The answer is B. \nsubscription/num_undelivered_messages: the number of messages that subscribers haven't processed https://cloud.google.com/pubsub/docs/monitoring#monitoring_forwarded_undeliverable_messages","comment_id":"458737","upvote_count":"2","timestamp":"1680870540.0","poster":"squishy_fishy"},{"comments":[{"timestamp":"1696263420.0","content":"yes is misleading:\nthe metric \"subscription/num_undelivered_messages\" is just the path of the API URL\n\nactions.googleapis.com/...subscription/num_undelivered_messages\n\nref: https://cloud.google.com/monitoring/api/metrics_gcp#pubsub/subscription/num_undelivered_messages","upvote_count":"3","comment_id":"579934","poster":"910"}],"timestamp":"1678637220.0","content":"Silly question: what is subscription/ num_undelivered_messages, it is divided by? or per subscription per num_undelivered_messages?","comment_id":"443512","poster":"squishy_fishy","upvote_count":"2"},{"comment_id":"397161","poster":"sumanshu","timestamp":"1672699080.0","upvote_count":"3","content":"Looks B"},{"poster":"gcper","timestamp":"1662052080.0","comment_id":"301558","upvote_count":"5","content":"A\n\n\"An alert based on a decrease of subscription/num_undelivered_messages for the source\"\nThe more we have undelivered messages, the worse. Thus we want to be alerted when the ratio goes down as the denominator goes up.\n\n\"A rate of change increase of instance/storage/used_bytes for the destination\"\nAn increase in the rate of change of how much we are storing per instance storage."},{"upvote_count":"5","content":"The correct answer is Option B : increase of subscription/num_undelivered_messages and decrease of instance/storage/ used_bytes, reason as follows:-\n- The first question we should ask is - why do we want to monitor things - this his very subject, one can say - we want to monitor to check - if everything is running \"OK\" or we want to monitor things to check if everything is running \"NOT OK\" .\n\nGenerally, we would go with the second point - i.e.- we want to monitor things - to check what is NOT OK. if everything works fine - may be we should monitor.\n\nGoing with that logic - Option B standouts - i.e.- the more we have undelivered messages in subscriber and less we have data in the sync (cloud storage) - means things are not OK and that why we want to monitor it .\n\nAs mentioned - this approach is subject and different people may have different approach in deciding why we monitor","timestamp":"1652175120.0","poster":"Alasmindas","comment_id":"216556"},{"poster":"Surjit24","comment_id":"210148","content":"Change it has to B.","upvote_count":"3","timestamp":"1651346100.0"},{"content":"It should be A.","upvote_count":"1","timestamp":"1651246380.0","comment_id":"208740","poster":"Surjit24"},{"comment_id":"179060","upvote_count":"3","poster":"Tanmoyk","timestamp":"1647238620.0","content":"Answer : B , need to create alert when pipeline data processing is not as per the expectation also need to monitor the storage"},{"poster":"haroldbenites","upvote_count":"1","timestamp":"1645473900.0","comments":[{"content":"Sorry. A is correct . subscription/num_undelivered_messages is for the Source.","poster":"haroldbenites","comment_id":"163111","upvote_count":"2","timestamp":"1645474080.0"}],"comment_id":"163110","content":"D is correct"},{"comments":[{"content":"no. You don't need to be alerted when its working fine","comments":[{"comment_id":"124770","content":"Yes, you are right, it should be B:\nI thought there was a trick in the question, Sorry & Thank you","timestamp":"1641113100.0","upvote_count":"6","poster":"Barniyah"}],"poster":"Callumr","upvote_count":"9","timestamp":"1640000220.0","comment_id":"114653"},{"poster":"Rajuuu","upvote_count":"4","content":"Descrese in undelivered message indicates the consumer is picking the data correctly with increase in processing used_bytes.\nAnswer is B","comment_id":"133067","timestamp":"1642012500.0"}],"comment_id":"103583","content":"There is a BIG TRICK here .\nyour mission is to monitor that the pipieline (IS PROCESSING THE DATA) , and not to monitor errors \nSo, The decrease of subscription/num_undelivered_messages for the source, and the increase of instance/storage/ used_bytes for the destination tells you clearly that this pipeline is working fine .\nThe answer is A :","upvote_count":"6","timestamp":"1638775440.0","poster":"Barniyah"},{"poster":"[Removed]","content":"Answer: B\nDescription: Increase in number of undelivered messages shows that the messages are not getting subscribed.","upvote_count":"5","comment_id":"68789","timestamp":"1632806100.0"},{"content":"Shouldnt the answer be A. With Increase in Undelivered message the Factor should decrease and we need to monitor it.","poster":"Rajokkiyam","upvote_count":"1","comment_id":"67018","timestamp":"1632321600.0","comments":[{"timestamp":"1632978660.0","comment_id":"69727","upvote_count":"5","content":"Answer B.","comments":[{"poster":"Unmesh93","content":"I think it should be A, as you mentioned earlier factor would decrease with increase in undelivered message in case of failure of pipeline.","comment_id":"128759","timestamp":"1641553140.0","upvote_count":"1"},{"comment_id":"128761","content":"I think it should be A, as you mentioned earlier factor would decrease with increase in undelivered message in case of failure of pipeline.","timestamp":"1641553200.0","upvote_count":"1","poster":"Unmesh93"}],"poster":"Rajokkiyam"}]}],"exam_id":11,"unix_timestamp":1584877560,"question_id":25,"answer":"B","url":"https://www.examtopics.com/discussions/google/view/17245-exam-professional-data-engineer-topic-1-question-120/"}],"exam":{"isImplemented":true,"name":"Professional Data Engineer","id":11,"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":319,"isBeta":false,"provider":"Google"},"currentPage":5},"__N_SSP":true}