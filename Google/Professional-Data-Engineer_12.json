{"pageProps":{"questions":[{"id":"LehFVeRIc4Cpmnm8kO7y","choices":{"B":"Create SQL views for each team in the same dataset in which the data resides, and assign the users/groups data viewer access to the SQL views","A":"Assign the users/groups data viewer access at the table level for each table","C":"Create authorized views for each team in the same dataset in which the data resides, and assign the users/groups data viewer access to the authorized views","D":"Create authorized views for each team in datasets created for each team. Assign the authorized views data viewer access to the dataset in which the data resides. Assign the users/groups data viewer access to the datasets in which the authorized views reside"},"topic":"1","answer_description":"","discussion":[{"poster":"someshsehgal","comment_id":"286585","upvote_count":"41","timestamp":"1612846740.0","content":"Correct A: A . Now it is feasible to provide table level access to user by allowing user to query single table and no other table will be visible to user in same dataset.","comments":[{"poster":"BigDataBB","comment_id":"1192905","content":"the request says \"team membership\", so access depends on the team and not the user","timestamp":"1712747280.0","upvote_count":"1"},{"upvote_count":"2","content":"A is not at all possible","poster":"Shiv_am","comment_id":"428755","comments":[{"poster":"squishy_fishy","content":"It is possible for about a year now. https://cloud.google.com/bigquery/docs/table-access-controls-intro#example_use_case","timestamp":"1633699680.0","upvote_count":"8","comment_id":"459248"}],"timestamp":"1629556260.0"},{"poster":"alecuba16","content":"The problem is that option A has a lot of work for the DevOps, meanwhile option D is easier to manage. The view is like having a shortcut to the same data, but with different permissions","timestamp":"1658410380.0","comments":[{"comment_id":"910111","timestamp":"1685439240.0","upvote_count":"3","content":"According to Chat GPT, it is also D.\nAnd it explains why it shouldn't be \"A\" as;\n\nGranularity: While you can assign access permissions at the table level, it doesn't allow for fine-grained access control. For example, if you want to restrict access to certain columns or rows within a table based on user or group, table-level permissions would not be sufficient.\n\nScalability: In organizations with many tables and users, managing permissions at the table level can quickly become unwieldy. You would need to individually set permissions for each user for each table, which can be time-consuming and error-prone.\n\nSecurity: Table-level permissions expose the entire table to a user or a group. If the data in the table changes over time, users might get access to data they shouldn't see. With authorized views, you have more control over what data is exposed.\n\nMaintenance: If the structure of your data changes (for instance, if tables are added or removed, or if the schema of a table changes), you would need to manually update the permissions for each affected table.","poster":"cetanx"}],"comment_id":"634610","upvote_count":"2"},{"poster":"jits1984","upvote_count":"11","content":"Should still be D.\n\nQuestion states - \"They should only see certain tables based on their team membership\"\n\nOption A states - Assign the users/groups data viewer access at the table level for each table\n\nWith A, everyone will see every table. Hence D.","timestamp":"1635411780.0","comment_id":"469133"}]},{"poster":"madhu1171","timestamp":"1584289140.0","content":"D should be the answer","comments":[{"timestamp":"1633699620.0","content":"There is only one dataset mentioned in the question here. \"You have migrated all of your data into tables in a dataset\"","upvote_count":"3","poster":"squishy_fishy","comment_id":"459246"},{"poster":"ducc","content":"It is updated, now A is correct","comment_id":"652374","timestamp":"1661555160.0","upvote_count":"1"}],"upvote_count":"27","comment_id":"64368"},{"upvote_count":"1","content":"Selected Answer: D\nThe question was created at the time when it was not possible to share data on table level (dataset was the only option). At that time D was possible only. Right now A is feasible as well.","comment_id":"1350361","timestamp":"1738494660.0","poster":"plum21"},{"poster":"LP_PDE","content":"Selected Answer: C\nAuthorized views provide a centralized way to manage access. You define the data each team can see in a view and then grant access to that view. This is much easier to maintain and update than managing permissions on individual tables.\nWhy not D? - Option D suggests creating separate datasets for each team and using authorized views within those datasets. This adds unnecessary complexity and overhead.\nYou would need to manage multiple datasets.You would need to grant the authorized views access to the original dataset.","upvote_count":"1","comment_id":"1334478","timestamp":"1735601160.0"},{"poster":"SamuelTsch","comment_id":"1303373","timestamp":"1729966500.0","content":"Selected Answer: A\nTable level access could be done in bigquery.","upvote_count":"2"},{"content":"Selected Answer: D\nRecommended approach","upvote_count":"1","poster":"JamesKarianis","timestamp":"1723538280.0","comment_id":"1265064"},{"comment_id":"1224105","timestamp":"1717501740.0","content":"Selected Answer: D\nShould be D.","poster":"dsyouness","upvote_count":"2"},{"poster":"MaxNRG","comment_id":"1099864","timestamp":"1702917420.0","upvote_count":"4","content":"Selected Answer: D\nhttps://cloud.google.com/solutions/migration/dw2bq/dw-bq-data-governance\nWhen you create the view, it must be created in a dataset separate from the source data queried by the view. Because you can assign access controls only at the dataset level, if the view is created in the same dataset as the source data, your users would have access to both the view and the data.\nhttps://cloud.google.com/bigquery/docs/authorized-views\nThis approach aligns with the Google Cloud best practices for data governance, ensuring that users can only access the data intended for them without having direct access to the source tables. Authorized views serve as a secure interface to the underlying data, and by placing these views in separate datasets per team, you can manage permissions effectively at the dataset level."},{"timestamp":"1700596920.0","upvote_count":"2","content":"but the question said that all data are copied into one dataset. so it should be C","poster":"lokiinaction","comment_id":"1076617"},{"poster":"spicebits","upvote_count":"2","timestamp":"1699446300.0","comment_id":"1065615","content":"A is the best answer for security as stated in the documentation - https://cloud.google.com/bigquery/docs/row-level-security-intro#comparison_of_authorized_views_row-level_security_and_separate_tables"},{"content":"A is a better fit than D for this case","timestamp":"1695554520.0","poster":"EsaP","upvote_count":"1","comment_id":"1015705"},{"poster":"barnac1es","content":"Selected Answer: C\nAuthorized Views: Authorized views in BigQuery allow you to control access to specific rows and columns within a table. This means you can create views for each team that restrict access to only the data relevant to that team.\nSingle Dataset: Keeping all the authorized views and the underlying data in the same dataset simplifies management and access control. It avoids the need to create multiple datasets, making the permission management process more straightforward.\n\nOption A (assigning data viewer access at the table level) would not provide the granularity you need, as it would allow users to see all tables in the dataset. This does not align with the requirement to restrict access based on team membership.","comment_id":"1015473","upvote_count":"1","timestamp":"1695527520.0"},{"poster":"arien_chen","timestamp":"1692522540.0","comment_id":"985675","upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/share-access-views#:~:text=the%20source%20data.-,Authorized%20views,-should%20be%20created\n\nFor best practice, Option D is bettern than others."},{"content":"Selected Answer: A\n[A] is correct if it is for individual table\nHowever, in practice we normally do [C] as most of the time, the view is a JOIN of a few tables or a subset of the table (some columns removed)","timestamp":"1679637660.0","poster":"midgoo","comment_id":"849007","upvote_count":"2"},{"timestamp":"1676715900.0","comment_id":"812854","upvote_count":"1","content":"Answer A, Trick here is, if question is not asking for data level Access such as some rows or columns, don't go for authorized view in that case i would go for C. If it's Table level request only in question, then A is simple answer","poster":"musumusu"},{"timestamp":"1669897980.0","upvote_count":"2","poster":"zellck","comment_id":"732602","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/bigquery/docs/control-access-to-resources-iam#grant_access_to_a_table_or_view"},{"timestamp":"1668855600.0","poster":"gudiking","comment_id":"721951","upvote_count":"2","content":"Selected Answer: A\nA - table level access control now exists: https://cloud.google.com/bigquery/docs/table-access-controls-intro#example_use_case"},{"comment_id":"715994","content":"A.\nPlease see:\nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro#example_use_case","poster":"Transscend","upvote_count":"1","timestamp":"1668165240.0"},{"content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro\nDon't think too much ,there is nothing to do with view, the question refer to table obviousely.\nAssume that User see certain table so he can see everything in such a table","upvote_count":"2","timestamp":"1663943160.0","poster":"John_Pongthorn","comment_id":"677241"},{"poster":"John_Pongthorn","comments":[{"comment_id":"1014055","content":"finally after tens of comments, i see one that explains and makes sense","upvote_count":"1","timestamp":"1695382920.0","poster":"exnaniantwort"}],"comment_id":"667051","timestamp":"1662990900.0","content":"Selected Answer: A\nIt has nothing to do with authorize view because of the following \nAuthorized views make use of query results but this question emphasise on Table level \nhttps://cloud.google.com/bigquery/docs/authorized-views\nAn authorized view lets you share query results with particular users and groups without giving them access to the underlying source data.","upvote_count":"4"},{"timestamp":"1661898120.0","poster":"ducc","upvote_count":"2","comment_id":"654470","content":"Selected Answer: A\nVote for A"},{"timestamp":"1660815480.0","content":"C can not be correct because: \"authorized views should be created in a different dataset from the source data\".","poster":"soichirokawa","upvote_count":"1","comment_id":"648376"},{"poster":"Remi2021","upvote_count":"2","comments":[{"poster":"alecuba16","comment_id":"634616","timestamp":"1658410680.0","content":"Thinking about A and D, now I have realised that both A and D are the same more or less in terms of changes, but D has a big drawback, if the table changes, the view has to change.","upvote_count":"1"}],"content":"D should be the answer. When we set rights on the dataset level then all tables are affected. Imagine having hundreds of tables how difficult would it be to assign permissions directly.","timestamp":"1657187640.0","comment_id":"628308"},{"timestamp":"1644599520.0","poster":"kped21","comment_id":"545422","upvote_count":"3","content":"A - as per latest\nBigQuery table ACL lets you set table-level permissions on resources like tables and views. Table-level permissions determine the users, groups, and service accounts that can access a table or view. You can give a user access to specific tables or views without giving the user access to the complete dataset. For example, grant the role BigQuery Data Viewer (roles/bigquery.dataViewer) to a user to let that user query just the table or view without full dataset access."},{"upvote_count":"3","comment_id":"519572","content":"Selected Answer: A\nA, since we just deal with permission to data and not give access to a subset of data.","poster":"medeis_jar","timestamp":"1641651060.0"},{"poster":"kishanu","content":"I would go with D.\nThough A seems to be feasible, there could be numerous tables under a dataset, and to provide access to the users could be a cumbersome process. Instead, create datasets for multiple groups(so-called teams), and assign access policies to them.","timestamp":"1640656500.0","upvote_count":"3","comment_id":"510729"},{"timestamp":"1639223940.0","poster":"AACHB","comment_id":"499374","upvote_count":"1","content":"Answer is A : it is now possible to assign roles at the table/view level"},{"timestamp":"1638489180.0","poster":"Pupina","upvote_count":"3","comment_id":"492850","content":"Tables are in one dataset but the autorized view can be in a different new dataset. One for each user simplifying the number of binding policies used or all the views in one dataset. Both eschemas has pros and cons. Need to understand whtat this question is wanting us to answer. Cannot be A because it is not a best practice. I prefer D over than C because reduce administration but the on the other hand the views cannot be re-used."},{"timestamp":"1627458600.0","comment_id":"416060","content":"Back then D, now A. Question is, if the exam is already up to date.","upvote_count":"14","poster":"hdmi_switch"},{"poster":"timolo","upvote_count":"2","content":"A is correct \nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro#permissions","comment_id":"393290","timestamp":"1624920120.0"},{"comment_id":"294523","timestamp":"1613759040.0","content":"D:\nmain point is that you can't give table level access for users and when you give access to a Dataset, user have access to all of data in it. then you should create new Datasets and view for each group and give view access for each of groups.","comments":[],"poster":"daghayeghi","upvote_count":"3"},{"poster":"apnu","comment_id":"258584","upvote_count":"5","content":"both A and D are correct , now we have option to share the table with user and group. in this case as billiability is not a concern so I would go with A.","timestamp":"1609685580.0"},{"comment_id":"251436","content":"A . we have now the feasibility to provide table level access to user with acl permission bigquery.dataviewer. it will allow user to query single table and no other table willl be visible to user in same dataset.","upvote_count":"3","timestamp":"1608783840.0","comments":[{"timestamp":"1635996840.0","poster":"butaut","upvote_count":"1","content":"It will allow user to query a single table, but other tables will be visible in the same dataset. I’ve tried this by myself. So A is not an option","comment_id":"472375"},{"timestamp":"1621569060.0","upvote_count":"1","content":"A is correct under table - level access capability","poster":"senura96","comment_id":"362618"}],"poster":"ashuchip"},{"timestamp":"1607552220.0","content":"looks like A\nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro#example_use_case","poster":"VM_GCP","upvote_count":"2","comment_id":"239544"},{"upvote_count":"3","comment_id":"232097","timestamp":"1606843020.0","content":"THe tables were saved in only 1 dATASET so the answer is C not D because D is for many datasets \n\" You have migrated all of your data into tables in a dataset. \" This mains clear that is only one. Answer D but now may be A its possible too because i are able to gice access at table level.","poster":"federicohi"},{"content":"think its A because both author views is usefull if you need con constraint access to some columns or rows of table not for all table","poster":"federicohi","comment_id":"223823","upvote_count":"2","timestamp":"1605891540.0"},{"timestamp":"1602052500.0","content":"The question is not what to do with table privileges, but what to do with user privileges? It says.\nI could do that with an A.\n----------------------------------------------\nComparison with authorized views\nBigQuery also provides access using authorized views. An authorized view lets you share query results with particular users and groups without giving them access to the underlying tables. Authorized view access is always read-only.\n----------------------------------------------\nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro#authorized_views\n\nAs it says above, it also says how it compares to approved views. It can be done, but if it strays from the definition of requirements, then it becomes a suggestion. In case you're wondering, this is a beta service with access to the tables and does not guarantee an SLA.\nWho would want to build that?","comment_id":"194920","upvote_count":"1","poster":"kino2020"},{"poster":"vakati","upvote_count":"3","comment_id":"183396","timestamp":"1600656180.0","content":"A. seems relevant now"},{"poster":"haroldbenites","content":"C is correct","comment_id":"163570","timestamp":"1598096820.0","upvote_count":"2"},{"content":"D looks correct","upvote_count":"4","poster":"clouditis","comment_id":"152769","timestamp":"1596841620.0"},{"content":"A \nDescription Big query now supports access st Table level.\nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro","comments":[{"timestamp":"1598115780.0","poster":"ayush955","comment_id":"163771","upvote_count":"2","content":"its in beta"}],"comment_id":"129421","timestamp":"1594179300.0","upvote_count":"4","poster":"Unmesh93"},{"timestamp":"1593370980.0","upvote_count":"6","poster":"norwayping","comment_id":"122098","content":"Sorry, it should be D"},{"timestamp":"1593369480.0","comment_id":"122089","content":"C is right. \nD the views are not used as the viewer access is on the dataset not views","upvote_count":"1","poster":"norwayping"},{"comment_id":"118516","content":"Option [D] - has to be the Answer","poster":"dambilwa","upvote_count":"6","timestamp":"1593011640.0"},{"content":"Answer D","timestamp":"1585873560.0","poster":"Rajokkiyam","comment_id":"70570","upvote_count":"6"},{"upvote_count":"7","timestamp":"1585411500.0","content":"Answer: D\nDescription: Authorized views are used to solve access related issues","poster":"[Removed]","comment_id":"68908"},{"upvote_count":"7","timestamp":"1584862800.0","content":"Should be D.\nhttps://cloud.google.com/bigquery/docs/share-access-views#introduction","poster":"[Removed]","comment_id":"66853"},{"comments":[{"comment_id":"175435","content":"yes, should be D:\nhttps://cloud.google.com/bigquery/docs/share-access-views\nCreate a separate dataset to store your view\nAfter creating your source dataset you create a new dataset to store the view you will share with your data analysts. This view will have access to the data in the source dataset. Your data analysts will have access to the view, not the source data.","poster":"mAbreu","upvote_count":"2","timestamp":"1599517440.0"}],"upvote_count":"9","content":"Yes, should be D:\n\nhttps://cloud.google.com/bigquery/docs/share-access-views","poster":"rickywck","timestamp":"1584669540.0","comment_id":"66151"}],"question_text":"You are migrating your data warehouse to BigQuery. You have migrated all of your data into tables in a dataset. Multiple users from your organization will be using the data. They should only see certain tables based on their team membership. How should you set user permissions?","answers_community":["A (66%)","D (28%)","6%"],"url":"https://www.examtopics.com/discussions/google/view/16678-exam-professional-data-engineer-topic-1-question-149/","question_images":[],"answer_ET":"A","answer_images":[],"exam_id":11,"unix_timestamp":1584289140,"answer":"A","question_id":56,"isMC":true,"timestamp":"2020-03-15 17:19:00"},{"id":"GUeHUade0lnCPJ0q3qca","timestamp":"2020-03-16 09:37:00","choices":{"C":"Load the original message to Google Cloud SQL, and export the table every hour to BigQuery via streaming inserts.","A":"Re-write the application to load accumulated data every 2 minutes.","B":"Convert the streaming insert code to batch load for individual messages.","D":"Estimate the average latency for data availability after streaming inserts, and always run queries after waiting twice as long."},"question_id":57,"question_images":[],"question_text":"You need to store and analyze social media postings in Google BigQuery at a rate of 10,000 messages per minute in near real-time. Initially, design the application to use streaming inserts for individual postings. Your application also performs data aggregations right after the streaming inserts. You discover that the queries after streaming inserts do not exhibit strong consistency, and reports from the queries might miss in-flight data. How can you adjust your application design?","url":"https://www.examtopics.com/discussions/google/view/16723-exam-professional-data-engineer-topic-1-question-15/","discussion":[{"comment_id":"616873","content":"Selected Answer: D\nAnswer: D. The only that describe a way to resolve the problem, with buffering the data. \n\n(the question is possible old, the best approach would be Pub/Sub + Dataflow Streaming + Bigquery for streaming data instead near-real time)","poster":"noob_master","timestamp":"1727155560.0","upvote_count":"8"},{"content":"B. Streams data into BigQuery one record at a time without needing to run a load job: https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll\nInstead of using a job to load data into BigQuery, you can choose to stream your data into BigQuery one record at a time by using the tabledata.insertAll method. This approach enables querying data without the delay of running a load job:\nhttps://cloud.google.com/bigquery/streaming-data-into-bigquery\nThe BigQuery Storage Write API is a unified data-ingestion API for BigQuery. It combines the functionality of streaming ingestion and batch loading into a single high-performance API. You can use the Storage Write API to stream records into BigQuery that become available for query as they are written, or to batch process an arbitrarily large number of records and commit them in a single atomic operation.\nCommitted mode. Records are available for reading immediately as you write them to the stream. Use this mode for streaming workloads that need minimal read latency.\nhttps://cloud.google.com/bigquery/docs/write-api","upvote_count":"7","timestamp":"1636394940.0","comments":[{"poster":"Abhi16820","upvote_count":"1","content":"IN THIS ALSO BIGQUERY HAS A BUFFER WHICH IT TAKES SLOWLY ANS INSERTS INTO REAL THING, WHAT YOU SAID IS HELPFULL IN REMOVING THE APPLICATION PART","comment_id":"481244","comments":[{"content":"could you please argue?","timestamp":"1639605840.0","upvote_count":"1","poster":"MarcoDipa","comment_id":"502500"}],"timestamp":"1637284080.0"}],"poster":"MaxNRG","comment_id":"474426"},{"poster":"Rav761","timestamp":"1735126440.0","content":"Selected Answer: A\nTo address the issue of strong consistency and ensure your reports do not miss in-flight data after streaming inserts, you should re-write the application to load accumulated data every 2 minutes (option A).\n\nHere's why:\n\nBy accumulating and loading data in 2-minute intervals, you can balance between real-time data processing and ensuring data consistency.\n\nThis approach allows you to process the data in manageable batches, reducing the likelihood of inconsistencies that might occur with individual streaming inserts.\n\nIt maintains a near real-time analysis capability while allowing enough time for all in-flight data to be captured and accurately represented in your reports.\n\nThis adjustment should help improve the reliability of your data analysis and reporting.","upvote_count":"2","comment_id":"1331528"},{"timestamp":"1732862280.0","upvote_count":"1","poster":"imrane1995","content":"Selected Answer: A\nAccumulating data and loading it periodically (e.g., every 2 minutes) via batch inserts ensures strong consistency for queries. Batch loads in BigQuery allow you to avoid the latency issues inherent to streaming inserts and guarantee data availability for queries.","comment_id":"1319572"},{"content":"Selected Answer: A\nFor maintaining data consistency while handling high throughput streaming inserts and subsequent aggregations in Google BigQuery, the best approach is to re-write the application to load accumulated data every 2 minutes.","timestamp":"1729533660.0","upvote_count":"1","comment_id":"1301209","poster":"GHill1982"},{"upvote_count":"5","comment_id":"425837","content":"\"D\" seems to use the typical approximate terminology of a wrong answer. \"estimate the time\" (how do you do that? do you do that over different times of the day?) and \"wait twice as long\" (who tells you that there are not a lot of cases when lag is twice as long?). Instead, \"A\" seems good. You don't need to show the exact results, but an approximation thereof, but you still want consistency. So an aggregation of the data every 2 minutes is a viable thing.","timestamp":"1727155560.0","poster":"fire558787"},{"upvote_count":"2","content":"Selected Answer: D\nD is correct. The problem requirement is doing analytics on real-time data. You cannot do batch processing because the business requires it to be real-time even if it makes your job simpler, so B is incorrect. Other options are not streaming.","poster":"Parth_P","timestamp":"1727155560.0","comment_id":"687001"},{"poster":"jkhong","content":"Selected Answer: D\nThere are assumptions over the quality of data acceptable. If slight variations of the analytics against actual can be accepted, then D would be a good choice.\n\nMany people chose B, but this also requires some form of waiting for the late data to arrive.\n\nI think a combination of D and B can be applied, but for an intial fix, delaying the aggregation queries with D seems to make more sense. If the variance is small and the some late data leakage is acceptable, and we can remain as D.\n\nIf problems arise, we can always proceed to attempt B","timestamp":"1727155500.0","comment_id":"746252","upvote_count":"2"},{"content":"Selected Answer: D\nThe streaming mode may be in pending mode or buffered mode where the streaming data is not immediately available before committing or flushing. Thus, we need to wait before the data will be available. Or else we need to switch to commited mode (which is not present in the choices).","timestamp":"1727155500.0","upvote_count":"2","poster":"korntewin","comment_id":"768256"},{"content":"Answer: D\nWhat to learn or look for\n1. In-Flight data = (Real Time data, i.e still in streaming pipeline and not landed in BigQuery)\n2. Dataflow (assume in best case) streaming pipeline is running to send data to Bigquery. \nWhy not option B: change streaming to batch upload is not business requirement, we have to stuck to streaming and real time analysis. \n\nOption D: make bigquery run after waiting for sometime (twice here), How will you do it? \n- there is not setting in BQ to do it, right!. So, adjust it in your pipeline (dataflow)\n- For example, add Fixed window, and you want to execute aggregation query after 2 min. \nCode\n```pipeline.apply(...)\n .apply(Window.<TableRow>into(FixedWindows.of(Duration.standardMinutes(2))))\n .apply(BigQueryIO.writeTableRows()\n .to(\"my_dataset.my_table\")\n```","poster":"musumusu","upvote_count":"5","timestamp":"1727155440.0","comment_id":"819106"},{"poster":"philli1011","upvote_count":"1","comment_id":"1131643","timestamp":"1706185140.0","content":"Answer: D\nI agree with the first part of the D answer, but for the second part, I don't know how they came about the 2 mins, is it from a calculation?"},{"content":"A. Re-write the application to load accumulated data every 2 minutes.\n\nBy accumulating data and performing a batch load every 2 minutes, you can reduce the potential inconsistency caused by streaming inserts. While this introduces a slight delay, it provides a more consistent approach than streaming each individual message. This method can still meet the near real-time requirement, and the slight delay is often acceptable in scenarios where data consistency is paramount.","comment_id":"1027034","upvote_count":"3","timestamp":"1696647240.0","poster":"imran79"},{"upvote_count":"1","comment_id":"1018794","poster":"Nirca","timestamp":"1695818100.0","content":"Selected Answer: B\nBBBBB is the only option"},{"upvote_count":"4","comments":[{"upvote_count":"1","poster":"axantroff","timestamp":"1698599880.0","comment_id":"1057017","content":"Good point"}],"poster":"ckanaar","content":"I'd argue that this question became outdated with the introduction of the BigQuery Storage Write API: https://cloud.google.com/bigquery/docs/write-api","comment_id":"1010788","timestamp":"1695058800.0"},{"upvote_count":"3","timestamp":"1690908180.0","comment_id":"969215","content":"Streaming inserts in BigQuery are not immediately available to be queried, which is causing the weak consistency you're observing. A better approach is to batch the data and load it at regular intervals. Loading the data every two minutes is still relatively real-time, and it should help solve the consistency problem.\nAnswer A.","poster":"klughund"},{"poster":"NeoNitin","timestamp":"1690683060.0","comment_id":"966789","content":"All the options aim to address the challenge of strong consistency in the data and potential missing data that may occur with streaming inserts. Each approach has its pros and cons, so the best choice depends on the specific needs and requirements of the application. It's like having different strategies for keeping track of all the fun things the kids do and say on the playground, making sure nothing gets left behind!","upvote_count":"1"},{"comment_id":"919062","poster":"WillemHendr","timestamp":"1686297540.0","content":"Streaming Inserts is marked as Legacy now.\nhttps://cloud.google.com/bigquery/docs/streaming-data-into-bigquery#dataavailability\n\nThe documentation is hinting on it can take up to 90 minutes to process the buffered data. \nThis question is testing if you are aware of the possible long times the buffer can build up.","upvote_count":"3"},{"content":"Selected Answer: B\nIn my experience, estimation in D is not a technical solution. it is just a guess solution.\nYou might still get caught when loading get higher and easily take twice as long latency, then problem occur again.\nSo for a more permanent solution, you should definitely go with B","comment_id":"877709","upvote_count":"3","poster":"izekc","timestamp":"1682208480.0"},{"comment_id":"835663","timestamp":"1678510020.0","upvote_count":"3","content":"Selected Answer: D\n1s tline of question requires near real time queries so D is the best option as batch load is never near real time","poster":"bha11111"},{"poster":"techtitan","upvote_count":"3","timestamp":"1676818380.0","content":"Selected Answer: D\nnear realtime","comment_id":"814190"},{"content":"Selected Answer: D\nthe answer is AD","timestamp":"1675578420.0","comment_id":"798648","upvote_count":"2","poster":"donbigi"},{"comment_id":"757372","timestamp":"1672054440.0","content":"Selected Answer: D\nD make more sense","poster":"PrashantGupta1616","upvote_count":"2"},{"comment_id":"743387","content":"Selected Answer: D\nD make more sense","poster":"DGames","timestamp":"1670884920.0","upvote_count":"2"},{"comment_id":"741679","poster":"Nirca","upvote_count":"1","timestamp":"1670761740.0","content":"Selected Answer: B\nAnswer is B.\nif you use the Committed mode, records are available for reading immediately as you write them to the stream. It is recommended to use his mode for streaming workloads that need minimal read latency. And the requirement is near rea-time.\nhttps://cloud.google.com/bigquery/docs/write-api#overview_of_the"},{"upvote_count":"1","comment_id":"699635","poster":"kennyloo","content":"D cos consistency could be caused by latency","timestamp":"1666250640.0"},{"poster":"devaid","upvote_count":"1","timestamp":"1665151560.0","content":"D is the closest to follow in real time","comment_id":"688677"},{"content":"Selected Answer: D\nI vote for D, it make more sense to me","comment_id":"653166","timestamp":"1661722680.0","poster":"ducc","upvote_count":"2"},{"comment_id":"647422","upvote_count":"1","content":"D just made sense to me but I wish I could actually give a reason why","timestamp":"1660617660.0","poster":"rowan_"},{"upvote_count":"6","comment_id":"645522","timestamp":"1660228560.0","poster":"[Removed]","content":"Selected Answer: D\nif you convert the individual messages to batch load at 10k per min. you'll get in 10 min. to the limit of load jobs per day (100k).\nSource: https://cloud.google.com/bigquery/quotas#load_jobs"},{"poster":"AmirN","timestamp":"1654798020.0","content":"Wait how can you batch load individual messages? Isn't that the same as streaming inserts? If it said batch load blocks of messages that makes sense because then they can be aggregated and we wont have missing data.\n\nWith that said I think its D. Its not actually hard to trial and error (estimate) average latency. Like if you estimate latency is 5 seconds, try 10 second delay in aggregation, if you have missing data bump it up and see the results.","upvote_count":"3","comment_id":"614150"},{"poster":"alecuba16","upvote_count":"3","content":"Selected Answer: B\nEstimation is something that is only done by a designer that doesn't think technically, estimation requires a deep study of the distribution of the data latency and to be sure about the best time and most of the data you have to choose a narrow confidence interval with a big %.\n\nThe best way to have confidence that the data is complete, is to confirm from the producer side that the stream of data is finish, that is to confirm the current stream, creating batches.","timestamp":"1650451860.0","comment_id":"588568"},{"timestamp":"1649663940.0","comment_id":"584105","content":"Selected Answer: B\nB is more appropriate(insert in batch mode). A & D are not suitable one to me. Questions for: A - why data should be accumulated in every 2 mins(Why 2 mins, it can be 1.5 mins, 1.8 mins, 2.x mins, 3 mins) etc, what is the justification behind selecting \"2 mins\". D - why user should wait twice time of average latency of data availability. If average latency is 3 mins, then user should wait 6 mins to run query. BQ docs states \"some recently streamed rows might not be available for table copy typically for a few minutes. In rare cases, this can take up to 90 minutes\"","poster":"kmaiti","comments":[{"comments":[{"content":"here is the link to the BQ docs that @kmaiti mentioned\nhttps://cloud.google.com/bigquery/docs/streaming-data-into-bigquery#dataavailability","upvote_count":"2","comment_id":"630511","timestamp":"1657627320.0","poster":"absero1609"}],"poster":"BJPJowee","timestamp":"1657203720.0","comment_id":"628381","content":"Do you care to share a link to support your evidence?","upvote_count":"1"}],"upvote_count":"5"},{"comment_id":"539095","content":"Selected Answer: D\nD is correct. B only adds a code batch load, it does not mention what you are going to do with this code, in this exam you can not assume.","poster":"nana1995","timestamp":"1643827500.0","upvote_count":"4"},{"upvote_count":"1","content":"A. Assume DF processes 10k events/min, then flush and aggregate every 1 min, the inconsistency could be caused by late data.\nBy using sliding window of 2 minutes, it smooths out the aggregated value.","timestamp":"1640650020.0","poster":"kuik","comment_id":"510675"},{"upvote_count":"1","timestamp":"1639631820.0","comment_id":"502681","poster":"ramen_lover","content":"Selected Answer: A\nThe following blog is helpful to understand how the streaming job works.\n\"Life of a BigQuery streaming insert\"\nhttps://cloud.google.com/blog/products/bigquery/life-of-a-bigquery-streaming-insert\n\nAlthough the blog does not mention explicitly the right answer, it seems impossible for us to estimate the average latency for data availability.","comments":[{"timestamp":"1640594880.0","poster":"prasanna77","comment_id":"510152","upvote_count":"1","content":"In that case option A is not approriate than D.increasing it to 2 mins dont work I guess"}]},{"comment_id":"487239","comments":[{"comment_id":"502495","poster":"MarcoDipa","content":"Answer is B. \nif you use the Committed mode, records are available for reading immediately as you write them to the stream. It is recommended to use his mode for streaming workloads that need minimal read latency. And the requirement is near rea-time.\nhttps://cloud.google.com/bigquery/docs/write-api#overview_of_the\n\nBesides, \n- You can use the Storage Write API to stream records into BigQuery that become available for query as they are written. \n- The Storage Write API has a significantly lower cost than the older tabledata.insertAll streaming API. In addition, you can ingest up to 2 TB per month for free\nhttps://cloud.google.com/bigquery/docs/write-api#advantages","upvote_count":"3","timestamp":"1639605480.0","comments":[{"timestamp":"1650386280.0","upvote_count":"1","content":"B is \"Convert the streaming insert code to batch load for individual messages\"\ncommitted mode: Record data is available for reading as soon as they are written.\nPending Mode: the records are available only after the transaction has been committed, Pending mode is useful for bulk loading data in large batches\n\nif using BQ Storage API then pending mode is seems relevant rather committed mode in the context of given choice.","poster":"msaqib934","comment_id":"588289"}]}],"timestamp":"1637920560.0","poster":"StefanoG","upvote_count":"3","content":"Also this time I agree with @MaxNRG, the right answer is B\nhttps://cloud.google.com/bigquery/docs/write-api"},{"content":"D\nhttps://cloud.google.com/bigquery/streaming-data-into-bigquery\nChecking for data availability","poster":"AllenChen123","timestamp":"1637412900.0","upvote_count":"2","comment_id":"482545"},{"upvote_count":"1","timestamp":"1634878260.0","poster":"KokkiKumar","content":"Hi Radhika7893, Option D is correct one.","comment_id":"465999"},{"poster":"anji007","content":"Ans: D","timestamp":"1634309700.0","comment_id":"462664","upvote_count":"1"},{"timestamp":"1632851220.0","poster":"Chelseajcole","comment_id":"453570","upvote_count":"1","content":"Vote for D instead of A.\n\nHere it states that it is because of consistency issues and some data in-flight not included in the aggregate function. We are here guessing the BigQuery Streaming Du-Duplicate solution, but the key is latency. Option A doesn't address any latency concern and just said every 2 mins. What if the latency is longer than 2 mins? Option D at least explicitly mentioned we need to estimate latency."},{"timestamp":"1629207240.0","poster":"sandipk91","upvote_count":"1","comment_id":"426356","content":"Correct answer is A","comments":[{"timestamp":"1629207240.0","content":"sorry it's D","comment_id":"426357","upvote_count":"3","poster":"sandipk91"}]},{"timestamp":"1624899600.0","comment_id":"393101","poster":"kishanu","comments":[{"content":"Option B can also be considered. Instead of re-writing the app, BQ write-API can be used with the committed mode, to make the data available as it is written","timestamp":"1640165880.0","upvote_count":"1","poster":"kishanu","comment_id":"506892"}],"upvote_count":"4","content":"There is no way of estimating the average latency. So, no for option D.\n\nInstead, option A is suitable, where the data can be accumulated for 2minutes using the windowing technique in Dataflow and then use the data in BigQuery."},{"poster":"sumanshu","content":"Vote for 'D'","timestamp":"1624581960.0","upvote_count":"1","comment_id":"390033"},{"comment_id":"284963","content":"Correct A","poster":"naga","timestamp":"1612629720.0","upvote_count":"2"},{"comment_id":"264456","upvote_count":"2","poster":"StelSen","comments":[{"timestamp":"1630310460.0","comments":[{"content":"Saw this from this link: https://cloud.google.com/bigquery/streaming-data-into-bigquery\nSome recently streamed rows might not be available for table copy typically for a few minutes. In rare cases, this can take up to 90 minutes. To see whether data is available for table copy, check the tables.get response for a section named streamingBuffer. If the streammingBuffer section is absent, your data is available for copy. You can also use the streamingBuffer.oldestEntryTime field to identify the age of records in the streaming buffer.\nSo this might be a way to find out latency?","comment_id":"543911","timestamp":"1644423420.0","upvote_count":"1","poster":"pamepadero"}],"poster":"sergio6","upvote_count":"3","comment_id":"435257","content":"How can you calculate average latency for data availability AFTER streaming inserts into biqquery ?"}],"content":"Option-A is incorrect. How can I conclude that in-flight data will come within 2 minutes? Atleast for Option-D, I will try to find avg latency first and doubling the time. Makes more sense.","timestamp":"1610338200.0"},{"poster":"arghya13","timestamp":"1609245480.0","content":"should be D","comment_id":"254724","upvote_count":"2"},{"comment_id":"252654","content":"D is better as it at least talks about taking into account latency. WIndowing can be set in dataflow with it","timestamp":"1608990900.0","poster":"abunasar786","upvote_count":"4"},{"poster":"xfoursea","content":"I think should be A. \nReason: if inconsistency caused by duplication, \"when you supply insertId for an inserted row, BigQuery uses this ID to support best effort de-duplication for up to one minute. \"\nhttps://cloud.google.com/bigquery/streaming-data-into-bigquery#dataconsistency","timestamp":"1605418980.0","upvote_count":"1","comment_id":"219516"},{"timestamp":"1605099360.0","comment_id":"217250","upvote_count":"1","content":"According to me the correct answer is A. Based on what I read about streaming inserts from cloud data flow, there is always possibility of some lag and this is handled by watermark in cloud data flow and for streaming inserts. Watermark which is by default even time trigger can be used to accumulate the lag data by setting accumulation mode to accumulate.","poster":"Radhika7983"},{"comment_id":"193844","content":"Agreed with D","timestamp":"1601947260.0","poster":"Darlee","upvote_count":"2"},{"content":"Option D: In Streaming buffer also, we can query the data. Only We need to have some latency to fetch the results.","upvote_count":"5","poster":"PRABHUKKARTHI","comment_id":"159898","timestamp":"1597661520.0"}],"answer_images":[],"isMC":true,"topic":"1","answer_ET":"D","answer":"D","answer_description":"","exam_id":11,"answers_community":["D (68%)","B (23%)","9%"],"unix_timestamp":1584347820},{"id":"EeYrDuWTybtygFwNmorh","choices":{"B":"Allocate sufficient persistent disk space to the Hadoop cluster, and store the intermediate data of that particular Hadoop job on native HDFS","D":"Allocate additional network interface card (NIC), and configure link aggregation in the operating system to use the combined throughput when working with Cloud Storage","A":"Allocate sufficient memory to the Hadoop cluster, so that the intermediary data of that particular Hadoop job can be held in memory","C":"Allocate more CPU cores of the virtual machine instances of the Hadoop cluster so that the networking bandwidth for each instance can scale up"},"discussion":[{"comment_id":"68155","upvote_count":"39","poster":"[Removed]","content":"Correct: B\n\nLocal HDFS storage is a good option if:\n\nYour jobs require a lot of metadata operations—for example, you have thousands of partitions and directories, and each file size is relatively small.\nYou modify the HDFS data frequently or you rename directories. (Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards.)\nYou heavily use the append operation on HDFS files.\nYou have workloads that involve heavy I/O. For example, you have a lot of partitioned writes, such as the following:\n\nspark.read().write.partitionBy(...).parquet(\"gs://\")\n\nYou have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation.","timestamp":"1601043120.0"},{"poster":"Rajokkiyam","content":"Answer B\nIts google recommended approach to use LocalDisk/HDFS to store Intermediate result and use Cloud Storage for initial and final results.","comment_id":"70571","timestamp":"1601684880.0","comments":[{"poster":"Chelseajcole","timestamp":"1648777380.0","comment_id":"455233","upvote_count":"1","content":"Any link to support this recommended approach?"}],"upvote_count":"15"},{"upvote_count":"1","comment_id":"1099867","poster":"MaxNRG","timestamp":"1718721600.0","content":"Selected Answer: B\nLocal HDFS storage is a good option if:\n- You have workloads that involve heavy I/O. For example, you have a lot of partitioned writes such as the following:\nspark.read().write.partitionBy(...).parquet(\"gs://\")\n- You have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation.\n- Your jobs require a lot of metadata operations—for example, you have thousands of partitions and directories, and each file size is relatively small.\n- You modify the HDFS data frequently or you rename directories. (Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards.)\n- You heavily use the append operation on HDFS files.","comments":[{"content":"We recommend using Cloud Storage as the initial and final source of data in a big-data pipeline. For example, if a workflow contains five Spark jobs in series, the first job retrieves the initial data from Cloud Storage and then writes shuffle data and intermediate job output to HDFS. The final Spark job writes its results to Cloud Storage.\nhttps://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#choose_storage_options","comment_id":"1099868","poster":"MaxNRG","timestamp":"1718721600.0","upvote_count":"1"}]},{"comment_id":"1058355","content":"The correct answer is B.","poster":"squishy_fishy","timestamp":"1714426860.0","upvote_count":"1"},{"content":"Selected Answer: B\nDisk I/O Performance: In a Cloud Dataproc cluster, the default setup uses local persistent disks for HDFS storage. These disks offer good disk I/O performance and are well-suited for storing intermediate data generated during Hadoop jobs.\n\nData Locality: Storing intermediate data on native HDFS allows for better data locality. This means that the data is stored on the same nodes where computation occurs, reducing the need for data transfer over the network. This can significantly improve the performance of disk I/O-intensive jobs.\n\nScalability: Cloud Dataproc clusters can be easily scaled up or down to meet the specific requirements of your jobs. You can allocate additional disk space as needed to accommodate the intermediate data generated by this particular Hadoop job.","timestamp":"1711259760.0","poster":"barnac1es","comment_id":"1015476","upvote_count":"1"},{"upvote_count":"1","timestamp":"1710611160.0","comments":[{"upvote_count":"1","content":"Typo Correct Answer is B. . Allocate sufficient persistent disk space to the Hadoop cluster, and store the intermediate data of that particular Hadoop job on native HDFS","timestamp":"1710991740.0","poster":"DeepakVenkatachalam","comment_id":"1012723"}],"poster":"DeepakVenkatachalam","comment_id":"1009176","content":"Correct: A\nI'd choose A as the doc states adding more SSDs are good for disk-intensive jobs especially those with many individual read and write operations\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs"},{"poster":"arien_chen","upvote_count":"1","timestamp":"1708428780.0","content":"Selected Answer: A\nI would choose A.\n\nGoogle Storage is faster than HDFS in many cases.\n\nhttps://cloud.google.com/architecture/hadoop#:~:text=It%27s%20faster%20than%20HDFS%20in%20many%20cases. \n\nThe question mention '(8-core nodes with 100-GB RAM)' on-premises Hadoop.\nthe problem may caused by insufficient memory,\nand does not mention cost would be an issue,\nso A 'memory' approach would be a better option.","comment_id":"985685"},{"comment_id":"963263","upvote_count":"1","content":"Selected Answer: B\nBest option is B. However allocating sufficient persistent disk space to the Hadoop cluster, and storing the intermediate data of that particular Hadoop job on native HDFS, would not improve the performance of the Hadoop job. In fact, it might even slow down the Hadoop job, as the data would have to be read and written to disk twice.","poster":"vamgcp","timestamp":"1706243460.0"},{"content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#choosing_primary_disk_options\nIf your job is disk-intensive and is executing slowly on individual nodes, you can add more primary disk space. For particularly disk-intensive jobs, especially those with many individual read and write operations, you might be able to improve operation by adding local SSDs. Add enough SSDs to contain all of the space you need for local execution. Your local execution directories are spread across however many SSDs you add.","upvote_count":"4","timestamp":"1685615280.0","comment_id":"732598","poster":"zellck"},{"poster":"John_Pongthorn","timestamp":"1679589600.0","content":"Selected Answer: B\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#choosing_primary_disk_options","upvote_count":"1","comment_id":"677249"},{"comment_id":"653694","poster":"rrr000","comments":[{"upvote_count":"2","content":"Further the question states that original on prem machines has 100gb ram.\n8-core nodes with 100-GB RAM","comment_id":"653695","timestamp":"1677546960.0","poster":"rrr000"}],"upvote_count":"3","content":"B is not the right answer. The problem says that for intermediate data cloud storage is to be used, while B option says:\n\nB ... the intermediate data of that particular Hadoop job on native HDFS\n\nA is the right answer. If you have enough memory then the shuffle wont spill on the disk.","timestamp":"1677546900.0"},{"poster":"SoerenE","upvote_count":"1","content":"B should be the right answer: https://cloud.google.com/compute/docs/disks/performance#optimize_disk_performance","timestamp":"1657432620.0","comment_id":"520725"},{"poster":"medeis_jar","upvote_count":"3","comment_id":"519575","timestamp":"1657282500.0","content":"Selected Answer: B\nhttps://cloud.google.com/solutions/migration/hadoop/hadoop-gcp-migration-jobs"},{"poster":"JG123","comment_id":"487260","upvote_count":"3","timestamp":"1653554640.0","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: B"},{"content":"If your job is disk-intensive and is executing slowly on individual nodes, you can add more primary disk space. For particularly disk-intensive jobs, especially those with many individual read and write operations, you might be able to improve operation by adding local SSDs. Add enough SSDs to contain all of the space you need for local execution. Your local execution directories are spread across however many SSDs you add.\nIts B \nhttps://cloud.google.com/solutions/migration/hadoop/hadoop-gcp-migration-jobs","timestamp":"1631262720.0","poster":"RT30","comment_id":"307088","upvote_count":"3"},{"timestamp":"1624501740.0","comment_id":"251439","content":"yes B is correct","upvote_count":"2","poster":"ashuchip"},{"upvote_count":"5","comment_id":"218587","poster":"Alasmindas","content":"Correct Answer is Option B - Adding persistent disk space, reasons:-\n- The question mentions that the particular job is \"disk I/O intensive - so the word \"disk\" is explicitly mentioned. \n- Option B also mentions about local HDFS storage, which is ideally a good option of general I/O intensive work. \n-","timestamp":"1620913020.0"},{"upvote_count":"3","content":"B is correct . I/O","comment_id":"163573","timestamp":"1614001740.0","poster":"haroldbenites"},{"comment_id":"148775","poster":"Archy","timestamp":"1612225380.0","content":"B, this job is high on I/O, local HDFS on dish is best option","upvote_count":"4"},{"upvote_count":"2","content":"How can adding Disk help with job becoming faster? Adding memory will facilitate in memory operations so that makes it faster. Adding CPU power will increase processing speed so that can make it faster. But I didnt quite follow how adding disk can make the process faster. I will vote for A","comments":[{"content":"adding more disk space so that it doesn't runs out of space when intermediate data is kept on it","poster":"FARR","timestamp":"1613275380.0","comment_id":"157739","upvote_count":"2"},{"content":"Sorry a correction. I just re-read the question and answers. Option B seems to be correct for me","upvote_count":"2","timestamp":"1611943620.0","comment_id":"146639","poster":"sh2020"}],"timestamp":"1611934740.0","poster":"sh2020","comment_id":"146498"},{"comment_id":"121123","content":"B is correct answer. For I/O intensive jobs, increasing the disk size resolves the issue. In case the issue is with compute capacity then increasing CPUs may help.","poster":"PRC","timestamp":"1609069980.0","upvote_count":"4"},{"content":"Should be A","upvote_count":"1","comments":[{"comment_id":"66857","upvote_count":"6","comments":[{"upvote_count":"2","timestamp":"1608964380.0","content":"Since the job is Disk I/O intensive, adding PDs would help.. If we add memory, we may have to re-write the job to use memory instead of PD","poster":"dambilwa","comment_id":"120171"}],"timestamp":"1600753800.0","poster":"[Removed]","content":"Could be B - As the job is I/O intensive"}],"comment_id":"66855","poster":"[Removed]","timestamp":"1600753680.0"},{"comment_id":"66152","poster":"rickywck","timestamp":"1600560060.0","content":"After further study, I would pick A. According to Google, using persistent disk HDFS will not necessarily faster than using Cloud storage even though it gives more consistent performance.","upvote_count":"4"},{"poster":"rickywck","upvote_count":"3","content":"Why not B?","comment_id":"65205","timestamp":"1600343880.0"}],"answer":"B","answer_description":"","answer_images":[],"answers_community":["B (92%)","8%"],"url":"https://www.examtopics.com/discussions/google/view/16868-exam-professional-data-engineer-topic-1-question-150/","question_images":[],"isMC":true,"question_text":"You want to build a managed Hadoop system as your data lake. The data transformation process is composed of a series of Hadoop jobs executed in sequence.\nTo accomplish the design of separating storage from compute, you decided to use the Cloud Storage connector to store all input data, output data, and intermediary data. However, you noticed that one Hadoop job runs very slowly with Cloud Dataproc, when compared with the on-premises bare-metal Hadoop environment (8-core nodes with 100-GB RAM). Analysis shows that this particular Hadoop job is disk I/O intensive. You want to resolve the issue. What should you do?","topic":"1","timestamp":"2020-03-17 14:58:00","question_id":58,"unix_timestamp":1584453480,"exam_id":11,"answer_ET":"B"},{"id":"bFfIQPLHh2qqdmSVFy3d","answer_ET":"C","isMC":true,"question_images":[],"discussion":[{"content":"Selected Answer: C\nOption C : It is the most rapid way to migrate your existing training pipelines to Google Cloud.\nIt allows you to continue using your existing Spark ML models.\nIt allows you to take advantage of the scalability and performance of Dataproc.\nIt allows you to read data directly from BigQuery, which is a more efficient way to process large datasets","poster":"vamgcp","comment_id":"963254","timestamp":"1690338180.0","upvote_count":"6"},{"upvote_count":"6","content":"Selected Answer: A\nthe question is: is it faster to move a SparkML job to a Vertex AI or to Dataproc? I am personally not sure, I would go for Dataproc as notebooks are not mentioned, but reading the Google article:\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/announcing-serverless-spark-components-vertex-ai-pipelines/\n\n\"Dataproc Serverless components for Vertex AI Pipelines that further simplify MLOps for Spark, Spark SQL, PySpark and Spark jobs.\"","poster":"vaga1","timestamp":"1683712380.0","comment_id":"893800","comments":[{"timestamp":"1700511000.0","upvote_count":"3","poster":"emmylou","content":"But you would need to re-write your models which can be a block","comment_id":"1075807"}]},{"poster":"Anudeep58","timestamp":"1720320660.0","content":"Selected Answer: C\nVertex AI is better suited for TensorFlow or scikit-learn models. Direct Spark ML support isn't native to Vertex AI, making this a less straightforward migration path.","upvote_count":"2","comment_id":"1243655"},{"content":"C\nQuestion is about rapid lift and shift. So code changes should be minimul","comment_id":"1163901","timestamp":"1709351700.0","poster":"mothkuri","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nC looks more suitable as data is alerady on BigQuery. \nRef - https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml","timestamp":"1705498320.0","comment_id":"1124996","poster":"GCP001"},{"timestamp":"1705175940.0","upvote_count":"1","content":"Selected Answer: C\nOption C, agreed with other comments","poster":"Matt_108","comment_id":"1122008"},{"content":"Selected Answer: C\nUse Cloud Dataproc, BigQuery, and Apache Spark ML for Machine Learning\nhttps://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml\nUsing Apache Spark with TensorFlow on Google Cloud Platform\nhttps://cloud.google.com/blog/products/gcp/using-apache-spark-with-tensorflow-on-google-cloud-platform","poster":"MaxNRG","comment_id":"1100401","upvote_count":"4","timestamp":"1702969980.0"},{"upvote_count":"1","timestamp":"1702794780.0","content":"Why not option D? To spin up the spark cluster on compute engine, considering rapid migration it potentially could be best approach as team wont have to re-work on model (may be only few configurational changes) and again to get data from Bigquery which is required periodically not all the time, could be easy.\nWith Dataproc it would have more code changes eventually can take more time.\nWith Vertex AI it doesn't support spark ML natively and also training would be black box.\n\nFor me Answer should be D.","poster":"Nandababy","comment_id":"1098699"},{"content":"Selected Answer: C\nDataproc for Spark: Google Cloud Dataproc is a managed Spark and Hadoop service that allows you to run Spark jobs seamlessly on Google Cloud. It provides the flexibility to run Spark jobs using Spark MLlib and other Spark libraries.\n\nBigQuery Integration: You mentioned that your data is being migrated to BigQuery. Dataproc has native integration with BigQuery, allowing you to read data directly from BigQuery tables. This eliminates the need to export data from BigQuery to another storage system before processing it with Spark.\n\nRapid Migration: This approach allows you to quickly migrate your existing Spark ML models and training pipelines without the need for a complete rewrite or extensive changes to your existing workflows. You can continue using your Spark ML models while adapting them to read data from BigQuery.","timestamp":"1695528060.0","upvote_count":"2","comment_id":"1015478","poster":"barnac1es"},{"poster":"DeepakVenkatachalam","comment_id":"1013035","content":"they are talking about rapid lift and shift, in which case Dataproc cluster will be right one for Spark ML models for lift and shift. so I think the answer is C.","timestamp":"1695296400.0","upvote_count":"1"},{"comment_id":"1012245","timestamp":"1695209220.0","content":"Selected Answer: A\nThe updated answer seems A based on the following article: \n\nhttps://cloud.google.com/blog/topics/developers-practitioners/announcing-serverless-spark-components-vertex-ai-pipelines/","poster":"ckanaar","upvote_count":"4"},{"upvote_count":"1","comment_id":"991430","timestamp":"1693138500.0","content":"Selected Answer: C\nThe answer is C. Spin up a Cloud Dataproc Cluster, migrate spark jobs to there, and link the Cluster to Bgquery with the connector. It's a straightforward solution.","poster":"FP77"},{"comment_id":"964273","content":"Selected Answer: C\nIf you wanted to use Vertex AI for training Spark ML models, you would typically need to convert your Spark ML code to another supported machine learning framework like TensorFlow or scikit-learn. Then you could use Vertex AI's pre-built training and prediction services for those frameworks.","poster":"knith66","timestamp":"1690422660.0","upvote_count":"3"},{"comment_id":"953240","upvote_count":"1","timestamp":"1689504840.0","poster":"wan2three","content":"Selected Answer: A\nThrough Vertex AI Workbench, Vertex AI is natively integrated with BigQuery, Dataproc, and Spark. You can use BigQuery ML to create and execute machine learning models in BigQuery using standard SQL queries on existing business intelligence tools and spreadsheets, or you can export datasets from BigQuery directly into Vertex AI Workbench and run your models from there. \nhttps://cloud.google.com/vertex-ai#all-features:~:text=Data%20and%20AI%20integration"},{"poster":"blathul","comment_id":"931230","upvote_count":"1","timestamp":"1687496940.0","content":"Selected Answer: C\nDataproc is a managed Spark and Hadoop service on Google Cloud, which makes it an ideal choice for migrating your existing Spark ML training pipelines. By using Dataproc, you can continue to leverage Spark and its ML capabilities without the need for significant code changes or rewriting your models.\nBy combining Dataproc and BigQuery, you can create Spark jobs or workflows in Dataproc that read data from BigQuery and train your existing Spark ML models. This approach allows you to quickly migrate your training pipelines to Google Cloud and take advantage of the scalability and performance benefits of both Dataproc and BigQuery."},{"content":"Selected Answer: C\nIt is obviously C) Dataproc, since we don't want to rewrite the training from scratch, highly prefer Dataproc for anything Hadoop/Spark ecosystem, and Vertex AI doesn't support *training* with SparkML (but deploying existing models).","timestamp":"1687424460.0","poster":"KC_go_reply","comment_id":"930319","upvote_count":"4"},{"comment_id":"924918","upvote_count":"1","poster":"Takshashila","content":"Selected Answer: C\nUse Dataproc for training existing Spark ML models, but start reading data directly from BigQuery","timestamp":"1686897660.0"},{"timestamp":"1686587760.0","content":"Vertex AI does not currently support Spark ML models.","poster":"brandonriddle","comments":[{"poster":"ckanaar","upvote_count":"2","content":"Actually, it does support serving Spark ML models, but training is not mentioned anywhere: \nhttps://cloud.google.com/architecture/spark-ml-model-with-vertexai","timestamp":"1695209100.0","comment_id":"1012244"}],"upvote_count":"2","comment_id":"921581"},{"timestamp":"1684942740.0","content":"Selected Answer: A\nSelected A","comment_id":"906008","upvote_count":"1","poster":"spsengineer101"},{"poster":"mialll","comment_id":"886066","timestamp":"1682934900.0","upvote_count":"3","content":"Selected Answer: A\nhttps://cloud.google.com/blog/topics/developers-practitioners/announcing-serverless-spark-components-vertex-ai-pipelines/"},{"comment_id":"877184","timestamp":"1682157420.0","upvote_count":"2","poster":"Prudvi3266","content":"https://cloud.google.com/blog/topics/developers-practitioners/announcing-serverless-spark-components-vertex-ai-pipelines/"},{"poster":"Prudvi3266","comment_id":"877183","content":"Selected Answer: A\nThe new answer is Vertix AI which has the feature run spark ML workloads.","timestamp":"1682157360.0","upvote_count":"4"},{"comments":[{"poster":"zellck","content":"https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery\n\nYou can use a BigQuery connector to enable programmatic read/write access to BigQuery. This is an ideal way to process data that is stored in BigQuery. Command-line access is not exposed. The BigQuery connector is a library that enables Spark and Hadoop applications to process data from BigQuery and write data to BigQuery using its native terminology.","comment_id":"736024","timestamp":"1670251740.0","upvote_count":"2"}],"poster":"zellck","upvote_count":"4","comment_id":"732593","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/overview\nDataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.\n\nIntegrated — Dataproc has built-in integration with other Google Cloud Platform services, such as BigQuery, Cloud Storage, Cloud Bigtable, Cloud Logging, and Cloud Monitoring, so you have more than just a Spark or Hadoop cluster—you have a complete data platform. For example, you can use Dataproc to effortlessly ETL terabytes of raw log data directly into BigQuery for business reporting.","timestamp":"1669897440.0"},{"poster":"Atnafu","upvote_count":"1","timestamp":"1669179180.0","comment_id":"724890","content":"C\nDataproc PySpark job, to invoke Spark ML functions"},{"content":"Selected Answer: C\nC - https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery","comment_id":"721963","timestamp":"1668857520.0","upvote_count":"1","poster":"gudiking"},{"poster":"dn_mohammed_data","comment_id":"676113","content":"C : dataproc provide MLLIB for trainign ML models + dataproc can connect to bigquery","timestamp":"1663849740.0","upvote_count":"2"},{"poster":"[Removed]","timestamp":"1662404580.0","upvote_count":"2","comment_id":"660475","content":"Selected Answer: C\nData proc has bigquery connector"},{"poster":"ducc","timestamp":"1662180540.0","upvote_count":"2","content":"Selected Answer: C\nC is correct","comment_id":"658076"}],"question_text":"You work for an advertising company, and you've developed a Spark ML model to predict click-through rates at advertisement blocks. You've been developing everything at your on-premises data center, and now your company is migrating to Google Cloud. Your data center will be closing soon, so a rapid lift-and-shift migration is necessary. However, the data you've been using will be migrated to migrated to BigQuery. You periodically retrain your Spark ML models, so you need to migrate existing training pipelines to Google Cloud. What should you do?","answer_images":[],"answers_community":["C (65%)","A (35%)"],"answer":"C","url":"https://www.examtopics.com/discussions/google/view/79680-exam-professional-data-engineer-topic-1-question-151/","exam_id":11,"answer_description":"","unix_timestamp":1662180540,"topic":"1","question_id":59,"choices":{"D":"Spin up a Spark cluster on Compute Engine, and train Spark ML models on the data exported from BigQuery","A":"Use Vertex AI for training existing Spark ML models","C":"Use Dataproc for training existing Spark ML models, but start reading data directly from BigQuery","B":"Rewrite your models on TensorFlow, and start using Vertex AI"},"timestamp":"2022-09-03 06:49:00"},{"id":"iQqe3DPAJ7ujq3rBD3yO","answer":"A","discussion":[{"content":"Answer: A\nDescription: Geospatial and ML functionality is with bigquery","comment_id":"68910","timestamp":"1601302260.0","upvote_count":"22","poster":"[Removed]"},{"content":"Answer : A","upvote_count":"15","timestamp":"1600751100.0","comment_id":"66837","poster":"[Removed]"},{"comment_id":"1165484","poster":"moumou","content":"Selected Answer: A\n[Removed] Highly Voted 3 years, 11 months ago\nAnswer: A","timestamp":"1725436140.0","upvote_count":"1"},{"content":"Answer : A\nStatement \"You want to have a dashboard that shows how many and which ships are likely to cause delays within a region\" means we run analytical queries using ML. So BigQuery is Correct answer and it can able to store large volume of data","poster":"mothkuri","comment_id":"1163905","timestamp":"1725242340.0","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: A\nHere's why BigQuery is a good choice:\n\nScalable Data Storage: BigQuery is a fully managed, highly scalable data warehouse that can handle large volumes of data, including your 40 TB dataset. It allows you to store and manage your data efficiently.\n\nSQL for Predictive Analytics: BigQuery supports standard SQL and has built-in machine learning capabilities through BigQuery ML. You can easily build predictive models using SQL queries, which aligns with your goal of predicting ship delays.\n\nGeospatial Processing: BigQuery has robust support for geospatial data processing. It provides functions for working with GeoJSON and geospatial data types, making it suitable for your ship telemetry data and geospatial analysis.\n\nIntegration with Dashboards: BigQuery can be easily integrated with visualization tools like Google Data Studio or other BI tools. You can create interactive dashboards to monitor ship delays based on your model's predictions.","poster":"barnac1es","comment_id":"1015481","timestamp":"1711260240.0"},{"comments":[{"poster":"musumusu","content":"answer A: You are just looking for a storage solution not a workflow","comment_id":"820939","upvote_count":"4","timestamp":"1692903540.0"}],"comment_id":"812904","upvote_count":"1","content":"Answer B: BigTable, \nCatchup words: Telemetry (sensor- semi structured data) as data is bigger than 500GB, datastore is not a good option. \nGEOJSON , bigquery has geospatical capabilites but still not quick enough for semi structure geojson data. \nPrediction for delay of ships <<likely to>> For me its time crucial and almost real time requirement. BigQuery is not suitable for it. \nBest solution for this case is: Use BigTable for storage, create a datflow pipeline / google cloud AI platform for time senstive prediction.","poster":"musumusu","timestamp":"1692351780.0"},{"timestamp":"1685614740.0","comment_id":"732584","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/bigquery/docs/geospatial-intro\nIn a data warehouse like BigQuery, location information is very common. Many critical business decisions revolve around location data. For example, you may record the latitude and longitude of your delivery vehicles or packages over time. You may also record customer transactions and join the data to another table with store location data.\n\nYou can use this type of location data to determine when a package is likely to arrive or to determine which customers should receive a mailer for a particular store location. Geospatial analytics let you analyze and visualize geospatial data in BigQuery by using geography data types and Google Standard SQL geography functions.","poster":"zellck","upvote_count":"2"},{"poster":"Atnafu","upvote_count":"1","comment_id":"717773","content":"A\nGeospatial analytics let you analyze and visualize geospatial data in BigQuery by using geography data types and Google Standard SQL geography functions. + BigqueryML","timestamp":"1684043400.0"},{"upvote_count":"1","comment_id":"486421","poster":"JG123","timestamp":"1653444060.0","content":"Answer: C"},{"upvote_count":"3","poster":"Chihhanyu","content":"GeoJson + Native functionality for prediction -> BigQuery","timestamp":"1652875260.0","comment_id":"480760"},{"poster":"singh_payal_1404","timestamp":"1652334240.0","comment_id":"476765","content":"Answer : A","upvote_count":"1"},{"poster":"PM17","content":"This is more of a question that an answer but: How much data can Bigquery handle?\n\n40TB seems to be a lot and bigtable can handle that, but of course Bigquery is better when it comes to ML and GIS.","upvote_count":"3","comment_id":"458624","timestamp":"1649321880.0"},{"upvote_count":"3","poster":"haroldbenites","timestamp":"1614018480.0","content":"A is correct","comment_id":"163751"},{"content":"A\nhttps://cloud.google.com/bigquery/docs/gis-intro","upvote_count":"3","comment_id":"157742","timestamp":"1613275860.0","poster":"FARR"},{"poster":"Rajokkiyam","comment_id":"70573","content":"Answer A","timestamp":"1601685000.0","upvote_count":"2"}],"choices":{"B":"Cloud Bigtable","A":"BigQuery","C":"Cloud Datastore","D":"Cloud SQL for PostgreSQL"},"question_images":[],"question_id":60,"question_text":"You work for a global shipping company. You want to train a model on 40 TB of data to predict which ships in each geographic region are likely to cause delivery delays on any given day. The model will be based on multiple attributes collected from multiple sources. Telemetry data, including location in GeoJSON format, will be pulled from each ship and loaded every hour. You want to have a dashboard that shows how many and which ships are likely to cause delays within a region. You want to use a storage solution that has native functionality for prediction and geospatial processing. Which storage solution should you use?","unix_timestamp":1584860700,"url":"https://www.examtopics.com/discussions/google/view/17216-exam-professional-data-engineer-topic-1-question-152/","topic":"1","isMC":true,"answer_description":"","exam_id":11,"answer_ET":"A","answers_community":["A (100%)"],"answer_images":[],"timestamp":"2020-03-22 08:05:00"}],"exam":{"isMCOnly":true,"numberOfQuestions":319,"lastUpdated":"11 Apr 2025","name":"Professional Data Engineer","isBeta":false,"id":11,"provider":"Google","isImplemented":true},"currentPage":12},"__N_SSP":true}