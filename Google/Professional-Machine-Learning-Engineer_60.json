{"pageProps":{"questions":[{"id":"ymH7kNh8JUVVQm3xZodL","url":"https://www.examtopics.com/discussions/google/view/91570-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You work on the data science team for a multinational beverage company. You need to develop an ML model to predict the company’s profitability for a new line of naturally flavored bottled waters in different locations. You are provided with historical data that includes product types, product sales volumes, expenses, and profits for all regions. What should you use as the input and output for your model?","answer_description":"","answer":"C","timestamp":"2022-12-14 13:39:00","question_id":296,"question_images":[],"answers_community":["C (81%)","Other"],"choices":{"D":"Use product type and the feature cross of latitude with longitude, followed by binning, as features. Use revenue and expenses as model outputs.","C":"Use product type and the feature cross of latitude with longitude, followed by binning, as features. Use profit as model output.","B":"Use latitude, longitude, and product type as features. Use revenue and expenses as model outputs.","A":"Use latitude, longitude, and product type as features. Use profit as model output."},"answer_ET":"C","isMC":true,"unix_timestamp":1671021540,"exam_id":13,"answer_images":[],"topic":"1","discussion":[{"poster":"hiromi","upvote_count":"7","timestamp":"1687203540.0","comment_id":"750265","content":"Selected Answer: C\nC (not sure)\n- https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture\n- https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization"},{"upvote_count":"2","comment_id":"1134892","poster":"sonicclasps","timestamp":"1722246360.0","content":"Selected Answer: D\nthe question asks to predict profitability , not profit. \nprofitability is calculated from revenue and expenses. \nthe correct answer is D"},{"poster":"andresvelasco","upvote_count":"2","timestamp":"1711004940.0","comment_id":"1012794","comments":[{"upvote_count":"1","poster":"maukaba","comment_id":"1048465","timestamp":"1713594960.0","comments":[{"comment_id":"1048471","content":"In the following examples it is said that it is not possible to cross lat & lon without bucketized them before since continous values must be converted into discrete before crossing :\nhttps://www.kaggle.com/code/vikramtiwari/feature-crosses-tensorflow-mlcc","upvote_count":"1","poster":"maukaba","timestamp":"1713595500.0"}],"content":"I agree it is the way around. See example: \nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding\nOne feature cross: [binned latitude X binned longitude X binned roomsPerPerson]"}],"content":"Most people have chosen C but: \nDoes it make sense to do binning after feature cross? Isnt it the other way around?"},{"timestamp":"1699514460.0","upvote_count":"1","poster":"M25","content":"Selected Answer: C\nWent with C","comment_id":"892782"},{"poster":"tavva_prudhvi","comment_id":"848374","timestamp":"1695475620.0","upvote_count":"3","content":"Selected Answer: C\nOption C is the best option because it takes into account both the product type and location, which can affect profitability. Binning the feature cross of latitude and longitude can help capture the nonlinear relationship between location and profitability, and using profit as the model output is appropriate because it's the target variable we want to predict."},{"comment_id":"811920","upvote_count":"1","poster":"abneural","timestamp":"1692271920.0","content":"Selected Answer: C\nAgreeing with hiromi, taxberg\nFeature cross and bucket lat and lon on geographical problems"},{"content":"Selected Answer: C\nyour output is profit","upvote_count":"1","comment_id":"802807","timestamp":"1691545740.0","poster":"enghabeth"},{"timestamp":"1691060580.0","comment_id":"797052","poster":"taxberg","upvote_count":"2","content":"Selected Answer: C\nMust be C. Always feature cross lat and lon on geographical problems. Also, D can not be right as we do not have revenue in the dataset."},{"comments":[{"content":"binding and crossing*","poster":"mil_spyro","upvote_count":"1","timestamp":"1686998280.0","comment_id":"748052"},{"content":"Why no need to reduce?","comment_id":"755946","upvote_count":"1","poster":"hiromi","timestamp":"1687713780.0"}],"timestamp":"1686998160.0","content":"Selected Answer: A\nIn this case, there is no need to reduce the number of unique values in the latitude and longitude variables, and binning would reduce information from those features hence A","upvote_count":"2","comment_id":"748049","poster":"mil_spyro"},{"poster":"ares81","timestamp":"1686739140.0","content":"Selected Answer: C\nEasy C.","comment_id":"745069","upvote_count":"2"}]},{"id":"cmuNvY4tJ2fAPs73QHDo","topic":"1","question_images":[],"answer_images":[],"answers_community":["A (77%)","D (15%)","8%"],"answer":"A","answer_ET":"A","exam_id":13,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/91574-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You work as an ML engineer at a social media company, and you are developing a visual filter for users’ profile photos. This requires you to train an ML model to detect bounding boxes around human faces. You want to use this filter in your company’s iOS-based mobile phone application. You want to minimize code development and want the model to be optimized for inference on mobile phones. What should you do?","timestamp":"2022-12-14 13:55:00","choices":{"B":"Train a model using AutoML Vision and use the “export for Coral” option.","D":"Train a custom TensorFlow model and convert it to TensorFlow Lite (TFLite).","A":"Train a model using AutoML Vision and use the “export for Core ML” option.","C":"Train a model using AutoML Vision and use the “export for TensorFlow.js” option."},"question_id":297,"answer_description":"","unix_timestamp":1671022500,"discussion":[{"comments":[{"timestamp":"1729407900.0","comment_id":"1048488","upvote_count":"5","poster":"maukaba","content":"Updated Vertex AI link:https://cloud.google.com/vertex-ai/docs/export/export-edge-model\n\nTrained AutoML Edge image classification models can be exported in the following formats:\nTF Lite - to run your model on edge or mobile devices.\nEdge TPU TF Lite - to run your model on Edge TPU devices.\nContainer - to run on a Docker container.\nCore ML - to run your model on iOS and macOS devices.\nTensorflow.js - to run your model in the browser and in Node.js."}],"timestamp":"1703076840.0","poster":"pshemol","upvote_count":"16","content":"Selected Answer: A\nhttps://cloud.google.com/vision/automl/docs/export-edge\nCore ML -> iOS and macOS\nCoral -> Edge TPU-based device\nTensorFlow.js -> web","comment_id":"750888"},{"poster":"M25","upvote_count":"1","comment_id":"892783","content":"Selected Answer: A\nWent with A","timestamp":"1715232120.0"},{"comments":[{"poster":"TNT87","content":"https://cloud.google.com/vertex-ai/docs/export/export-edge-model#export","timestamp":"1713334680.0","upvote_count":"1","comment_id":"872391"}],"poster":"TNT87","comment_id":"872389","timestamp":"1713334560.0","content":"https://developer.apple.com/documentation/coreml\nAnswer A","upvote_count":"1"},{"comment_id":"819190","timestamp":"1708692180.0","comments":[{"poster":"tavva_prudhvi","comment_id":"848414","upvote_count":"1","content":"While Coral can be used to optimize machine learning models for inference on edge devices, it's not the best option for an iOS-based mobile phone application.","timestamp":"1711209900.0"}],"poster":"shankalman717","upvote_count":"1","content":"Selected Answer: B\nAutoML Vision is a service provided by Google Cloud that enables developers to train and deploy machine learning models for image recognition tasks, such as detecting bounding boxes around human faces. The “export for Coral” option generates a TFLite model that is optimized for running on Coral, a hardware platform specifically designed for edge computing, including mobile devices. The TFLite model is also compatible with iOS-based mobile phone applications, making it easy to integrate into the company's app."},{"comments":[{"poster":"tavva_prudhvi","comment_id":"848417","content":"Excellent reasoning for C,D but Core ML is Apple's machine learning framework that is optimized for iOS-based devices, and exporting the model to Core ML format can help minimize inference time on mobile devices.","upvote_count":"1","timestamp":"1711209960.0"}],"poster":"shankalman717","timestamp":"1708692120.0","content":"Selected Answer: B\nOption A, using AutoML Vision and exporting for Core ML, is also a viable option. Core ML is Apple's machine learning framework that is optimized for iOS-based devices. However, using this option would require more development effort to integrate the Core ML model into the app.\n\nOption C, using AutoML Vision and exporting for TensorFlow.js, is not the best option for this scenario since it is optimized for running on web browsers, not mobile devices.\n\nOption D, training a custom TensorFlow model and converting it to TFLite, would require significant development effort and time compared to using AutoML Vision. AutoML Vision provides a simple and efficient way to train and deploy machine learning models without requiring expertise in machine learning.","upvote_count":"1","comment_id":"819188"},{"poster":"enghabeth","comment_id":"802808","upvote_count":"1","comments":[{"content":"Its wrong, While TFLite is a mobile-optimized version of TensorFlow, it requires more code development than using AutoML Vision and exporting for Core ML. Therefore, it's not the best option for minimizing code development time.","comment_id":"848413","timestamp":"1711209840.0","upvote_count":"1","poster":"tavva_prudhvi"}],"content":"Selected Answer: D\nhttps://www.tensorflow.org/lite\nhttps://medium.com/the-ai-team/step-into-on-device-inference-with-tensorflow-lite-a47242ba9130","timestamp":"1707450720.0"},{"comment_id":"765482","content":"Selected Answer: A\nI correct myself: it's A!","upvote_count":"1","timestamp":"1704364320.0","poster":"ares81"},{"poster":"egdiaa","content":"A indeed as described here: https://cloud.google.com/vision/automl/docs/export-edge","comment_id":"754759","timestamp":"1703404320.0","upvote_count":"1"},{"timestamp":"1703024160.0","poster":"hiromi","comment_id":"750287","content":"Selected Answer: A\nA\n\"You want to minimize code development\" -> AutoML\n- https://cloud.google.com/vision/automl/docs/tflite-coreml-ios-tutorial\n- https://cloud.google.com/vertex-ai/docs/training-overview#image","upvote_count":"2"},{"poster":"mil_spyro","timestamp":"1702815960.0","upvote_count":"2","content":"Selected Answer: D\nTensorFlow Lite is a lightweight version of TensorFlow that is optimized for mobile and embedded devices, making it an ideal choice for use in an iOS-based mobile phone application.","comment_id":"748040"},{"content":"Selected Answer: D\nI find no answer is 100% right, but D seems closer to the truth.","upvote_count":"1","comments":[{"content":"It's A.","poster":"ares81","comment_id":"765483","upvote_count":"1","timestamp":"1704364380.0"}],"timestamp":"1702558500.0","poster":"ares81","comment_id":"745085"}]},{"id":"As8r20DWG8jY3usiKjWm","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/91575-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","question_id":298,"answer_description":"","answer_ET":"A","answer_images":[],"question_text":"You have been asked to build a model using a dataset that is stored in a medium-sized (~10 GB) BigQuery table. You need to quickly determine whether this data is suitable for model development. You want to create a one-time report that includes both informative visualizations of data distributions and more sophisticated statistical analyses to share with other ML engineers on your team. You require maximum flexibility to create your report. What should you do?","discussion":[{"comment_id":"1233720","upvote_count":"2","timestamp":"1718896680.0","poster":"dija123","content":"Selected Answer: A\nIt is a data science request that could be ended on Jupiter notebook"},{"comment_id":"1195527","timestamp":"1713100680.0","poster":"gscharly","upvote_count":"2","content":"Selected Answer: A\nMore Flexbility"},{"timestamp":"1707958620.0","upvote_count":"1","poster":"SubbuJV","comment_id":"1150667","content":"Selected Answer: A\nMore Flexbility"},{"content":"Selected Answer: A\nMax flexibility","poster":"Mickey321","upvote_count":"1","timestamp":"1700062980.0","comment_id":"1071591"},{"poster":"Krish6488","timestamp":"1699726140.0","content":"Selected Answer: A\nLooker studio is good too but it does not give the same depth in statistical analysis of the data as using matplotlib, seaborn etc gives on a notebook. So Jupyterlab notebook a.k.a Vertex AI workbench for me","comment_id":"1068003","upvote_count":"3"},{"content":"Selected Answer: A\nA as it is a one off report with maximum flexibility. Dont need a dashboard unless being reused","poster":"MCorsetti","timestamp":"1697970780.0","comment_id":"1050444","upvote_count":"1"},{"upvote_count":"2","timestamp":"1695571680.0","content":"Selected Answer: A\nA for more sophisticated statistical analyses and maximum flexibility","comment_id":"1015983","poster":"lalala_meow"},{"upvote_count":"1","comment_id":"1003842","timestamp":"1694336220.0","poster":"andresvelasco","content":"Selected Answer: A\nA (AI workbench): \"sophisticated\""},{"upvote_count":"1","comment_id":"960465","poster":"[Removed]","timestamp":"1690117980.0","content":"Selected Answer: A\nThe answer is A.\n\nB is wrong because you need more sophisticated statistical analyses and maximum flexibility to create your report."},{"upvote_count":"1","poster":"NickHapton","content":"1. one- time\n2. flexibility\ngo for A","comment_id":"946552","timestamp":"1688826000.0"},{"upvote_count":"1","poster":"SamuelTsch","timestamp":"1688787660.0","content":"Selected Answer: A\nwent with A, because of max. flexibility","comment_id":"946133"},{"comment_id":"936716","poster":"PST21","upvote_count":"2","content":"Correct Answer A . While Google Data Studio (Option B) is a powerful data visualization and reporting tool, it might not provide the same level of flexibility and sophistication for statistical analyses compared to a notebook environment.","timestamp":"1687959180.0"},{"content":"Selected Answer: C\nTensorFlow Data Validation(TFDV) can compute descriptive statistics that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions. Tools such as Facets Overview can provide a succinct visualization of these statistics for easy browsing.","upvote_count":"3","poster":"CloudKida","comment_id":"892923","timestamp":"1683620940.0"},{"timestamp":"1682826480.0","content":"Selected Answer: A\nA. Flexibility is the key.","upvote_count":"1","comment_id":"884858","poster":"lucaluca1982"},{"content":"Selected Answer: B\nI think has to be B. One of the keys is that it says quickly and BQ makes it very easy to export the query into Looker Studio. The other one is that there's maximum flexibility within the needs for this case (informative visualizations + statistical analysis), as we can develop and write custom formulas.\nA feels like overkill to use a Deep Learning VM Image to only describe data and perform some analysis.\nC also feels overkill to start developping a neural net for that.\nD although you may use Dataprep for this, it is less suited than A","poster":"frangm23","upvote_count":"2","timestamp":"1681906680.0","comment_id":"874615"},{"content":"Selected Answer: A\nA seçeneğini öneriyorum çünkü Vertex AI Workbench kullanıcı yönetimli not defterleri (user-managed notebooks), BigQuery tablosundaki verilerin analiz edilmesi ve görselleştirilmesi için daha fazla esneklik ve özelleştirme sağlar. Python kütüphaneleri (pandas, matplotlib, seaborn vb.) kullanarak, veri dağılımlarının görselleştirmelerini oluşturabilir ve daha karmaşık istatistiksel analizler gerçekleştirebilirsiniz.","comment_id":"856898","poster":"kucuk_kagan","upvote_count":"1","timestamp":"1680257520.0"},{"comment_id":"853148","comments":[{"upvote_count":"1","timestamp":"1680001920.0","comment_id":"853153","content":"Depending on your definition of \"You require maximum flexibility to create your report.\", it could very well be B too.","poster":"JamesDoe"}],"timestamp":"1680001740.0","content":"Selected Answer: A\nI think it's A.One time report containing real datasets STATISTICAL measurements to tell if the data is suitable for model development. Target audience is also other ML engineers.\nGetting a whole report of exactly this with TFDV/Facets is like two lines of code: https://www.tensorflow.org/tfx/data_validation/get_started\n\nA similar data studio report for this would take lots of time and work, and there would be no benefit from reuseability since task was a one-time job.","poster":"JamesDoe","upvote_count":"2"},{"comment_id":"852260","upvote_count":"1","poster":"hghdh5454","timestamp":"1679934060.0","content":"Selected Answer: A\nA. Use Vertex AI Workbench user-managed notebooks to generate the report.\n\nBy using Vertex AI Workbench user-managed notebooks, you can create a one-time report that includes both informative visualizations and sophisticated statistical analyses. The notebooks provide maximum flexibility for data analysis, as they allow you to use a wide range of libraries and tools to create visualizations, perform statistical tests, and share your findings with your team. You can easily connect to the BigQuery table from the notebook and perform the necessary data exploration and analysis."},{"comment_id":"850334","content":"Selected Answer: D\nUnderstand and explore data instantly with visual data distributions. Dataprep automatically detects schemas, data types, possible joins, and anomalies such as missing values, outliers, and duplicates so you get to skip the time-consuming work of assessing your data quality and go right to the exploration and analysis.","timestamp":"1679767320.0","upvote_count":"2","poster":"Yajnas_arpohc"},{"content":"Little confused in A, B as User-managed notebooks in Vertex AI Workbench provide maximum flexibility to explore and analyze data, as well as create informative visualizations and perform sophisticated statistical analyses. This is the best option for this scenario because it provides maximum flexibility, and it can handle the size of the dataset stored in the BigQuery table. \n\nGoogle Data Studio is a powerful reporting tool that can create informative visualizations, it may not be the best option for more sophisticated statistical analyses. Additionally, it may not provide the same level of flexibility as user-managed notebooks in Vertex AI Workbench.","timestamp":"1679587860.0","poster":"tavva_prudhvi","upvote_count":"2","comment_id":"848424"},{"content":"Selected Answer: C\nThe TFDV APIs are designed to enable connectors to different data formats, and to provide flexibility and scale. \nThe API also allows for the computation of custom statistics (in addition to the standard statistics computed by TFDV)\nThe statistics are stored in a statistics.proto and can be visualized from within the notebook.\n\n--> https://blog.tensorflow.org/2018/09/introducing-tensorflow-data-validation.html","upvote_count":"2","timestamp":"1679308560.0","poster":"Yajnas_arpohc","comments":[{"comment_id":"848419","upvote_count":"1","poster":"tavva_prudhvi","content":"While TensorFlow Data Validation can provide informative visualizations and statistical analyses, it may not be the best option for this scenario because it requires additional setup and configuration.","timestamp":"1679587740.0"}],"comment_id":"844762"},{"poster":"Yajnas_arpohc","comment_id":"842476","comments":[{"timestamp":"1679308740.0","upvote_count":"2","comment_id":"844767","poster":"Yajnas_arpohc","content":"TFDV is enabled to use from a notebook environment; so this is correct as well, but C is technically more precise, I think"}],"upvote_count":"1","timestamp":"1679116140.0","content":"Selected Answer: A\nA gives max flexibility to generate suitable reports for ML purposes/ML engineers"},{"timestamp":"1678430580.0","content":"Should me A, maximum flexibility","upvote_count":"1","comment_id":"834691","poster":"bda92b3"},{"poster":"TNT87","comment_id":"834364","upvote_count":"1","timestamp":"1678392240.0","content":"Selected Answer: B\nAnswer B."},{"content":"Selected Answer: B\nI think it's B. \n- BigQuery table\n- quickly (Data Studio Connects quickly with BQ)\n - one-time report, informative visualizations, sophisticated statistical analyses (Dashboard in Data Studio)","poster":"guilhermebutzke","timestamp":"1677266760.0","upvote_count":"1","comment_id":"820863"},{"upvote_count":"1","timestamp":"1675019280.0","comment_id":"791964","content":"Selected Answer: A\nThe requirement here isn't the ease of use but maximum flexibility (everything dev think of is doable). I bet for A\nI bet on A","poster":"pshemol"},{"comments":[{"timestamp":"1672329300.0","content":"It says maximum flexibility\nWhy not A?","poster":"Abhijat","comment_id":"761224","comments":[{"poster":"JJJJim","content":"I have the same question.","timestamp":"1673750220.0","upvote_count":"1","comment_id":"776120"}],"upvote_count":"2"}],"timestamp":"1671488340.0","poster":"hiromi","upvote_count":"3","content":"Selected Answer: B\nB\nBQ is great for analyzing and visualizing data (integrating with Data Studio)","comment_id":"750288"},{"comment_id":"748038","poster":"mil_spyro","timestamp":"1671279720.0","content":"Selected Answer: B\nGoogle Data Studio has native connector to BigQuery and allows to visualize data in easy way","upvote_count":"1"},{"upvote_count":"2","timestamp":"1671023520.0","poster":"ares81","content":"Selected Answer: B\nA. User-managed --> non quickly. C. No Tensorflow D. Analysis, not preparation for analysis --> no Dataprep. It seems that Data Studio is the way to go. The answer is B.","comment_id":"745097"}],"timestamp":"2022-12-14 14:12:00","unix_timestamp":1671023520,"choices":{"C":"Use the output from TensorFlow Data Validation on Dataflow to generate the report.","D":"Use Dataprep to create the report.","B":"Use the Google Data Studio to create the report.","A":"Use Vertex AI Workbench user-managed notebooks to generate the report."},"answers_community":["A (56%)","B (26%)","Other"],"exam_id":13,"answer":"A","isMC":true},{"id":"q0BxU0gnndk3TVBnGsS9","question_text":"You work on an operations team at an international company that manages a large fleet of on-premises servers located in few data centers around the world. Your team collects monitoring data from the servers, including CPU/memory consumption. When an incident occurs on a server, your team is responsible for fixing it. Incident data has not been properly labeled yet. Your management team wants you to build a predictive maintenance solution that uses monitoring data from the VMs to detect potential failures and then alerts the service desk team. What should you do first?","answers_community":["C (55%)","B (38%)","7%"],"question_id":299,"question_images":[],"answer_images":[],"unix_timestamp":1671024960,"answer_ET":"C","isMC":true,"exam_id":13,"choices":{"A":"Train a time-series model to predict the machines’ performance values. Configure an alert if a machine’s actual performance values significantly differ from the predicted performance values.","D":"Hire a team of qualified analysts to review and label the machines’ historical performance data. Train a model based on this manually labeled dataset.","B":"Implement a simple heuristic (e.g., based on z-score) to label the machines’ historical performance data. Train a model to predict anomalies based on this labeled dataset.","C":"Develop a simple heuristic (e.g., based on z-score) to label the machines’ historical performance data. Test this heuristic in a production environment."},"topic":"1","timestamp":"2022-12-14 14:36:00","url":"https://www.examtopics.com/discussions/google/view/91578-exam-professional-machine-learning-engineer-topic-1-question/","answer":"C","discussion":[{"upvote_count":"9","timestamp":"1671275340.0","comment_id":"748003","content":"Selected Answer: C\nI would go for C, it is important to have a clear understanding of what constitutes a potential failure and how to detect it. A heuristic based on z-scores, for example, can be used to flag instances where the performance values of a machine significantly differ from its historical baseline.","poster":"mil_spyro"},{"upvote_count":"1","timestamp":"1735570560.0","content":"Selected Answer: B\nSince when is developing and testing allowed in prod?\nAnswer is B","comment_id":"1334197","poster":"mouthwash"},{"comment_id":"1322193","poster":"rajshiv","upvote_count":"1","timestamp":"1733365320.0","content":"Selected Answer: B\nOption B - because it allows you to efficiently label the data using a heuristic approach (e.g., z-score), and then train an anomaly detection model on that labeled data."},{"content":"Selected Answer: B\nc -> no testing in prod. could lead to risks. Hence B.","poster":"Pau1234","timestamp":"1733166540.0","upvote_count":"1","comment_id":"1321069"},{"comment_id":"1318144","poster":"AB_C","upvote_count":"1","content":"Selected Answer: B\nWhy not C - Heuristic in production without a model: Directly deploying a heuristic in a production environment without testing it within a model can lead to many false positives and alert fatigue for the service desk team.","timestamp":"1732635300.0"},{"poster":"pico","timestamp":"1694595960.0","upvote_count":"4","content":"Selected Answer: B\nNOT C: when you have tested something directly in production??\n\nOption B involves labeling historical data using heuristics, which can be a practical and quick way to get started.","comment_id":"1006395"},{"comment_id":"932576","content":"Selected Answer: C\nVote for C\nReference: Rule #1: Don’t be afraid to launch a product without machine learning.\nhttps://developers.google.com/machine-learning/guides/rules-of-ml#before_machine_learning","timestamp":"1687613340.0","poster":"razmik","upvote_count":"1"},{"comment_id":"923644","poster":"julliet","content":"Selected Answer: C\nsimple solution goes first, more sophisticated one -- after","upvote_count":"2","timestamp":"1686793320.0"},{"poster":"M25","comment_id":"892785","timestamp":"1683609720.0","upvote_count":"2","content":"Selected Answer: C\nWent with C"},{"timestamp":"1681711920.0","poster":"TNT87","content":"Answer C \nSame as Question number 139","upvote_count":"2","comment_id":"872381"},{"poster":"studybrew","upvote_count":"3","comment_id":"851350","comments":[{"upvote_count":"2","comment_id":"903662","content":"in B you are labeling with heuristics and still develop a model\nin C you follow the ML-rules to adopt simple solution first and later decide if, how and where you need more sophisticated model","poster":"julliet","timestamp":"1684721700.0"}],"content":"What’s the difference between B and C?","timestamp":"1679852280.0"},{"content":"Selected Answer: C\nThis is the best option for this scenario because it's quick and inexpensive, and it can provide a baseline for labeling the historical performance data. Once we have labeled data, we can train a predictive maintenance model to detect potential failures and alert the service desk team.","upvote_count":"1","comment_id":"848435","poster":"tavva_prudhvi","timestamp":"1679588640.0"},{"timestamp":"1678618980.0","content":"why not D ?","upvote_count":"1","poster":"osaka_monkey","comment_id":"836924","comments":[{"content":"While this approach may result in accurate labeling of the historical performance data, it can be time-consuming and expensive.","upvote_count":"1","poster":"tavva_prudhvi","timestamp":"1679588880.0","comment_id":"848440"}]},{"timestamp":"1674881640.0","comment_id":"790298","content":"Selected Answer: C\nhttps://www.geeksforgeeks.org/z-score-for-outlier-detection-python/","upvote_count":"1","poster":"John_Pongthorn"},{"content":"Selected Answer: B\nI vote for B\n- https://developers.google.com/machine-learning/guides/rules-of-ml","comments":[{"poster":"hiromi","timestamp":"1671703020.0","comments":[{"comments":[{"poster":"guilhermebutzke","timestamp":"1677267300.0","comments":[{"comments":[{"poster":"evanfebrianto","timestamp":"1684414680.0","comment_id":"901219","upvote_count":"1","content":"But testing the heuristic in a production environment without training a model could be risky and lead to false alarms or misses."}],"upvote_count":"2","poster":"tavva_prudhvi","content":"While this approach may work in some cases, it's not guaranteed to work well in this scenario because we don't know the nature of the anomalies that we want to detect. Therefore, it may be difficult to come up with a heuristic that can accurately label the historical performance data.","comment_id":"848442","timestamp":"1679589000.0"}],"content":"Why not B? The team wants to create a model to predict failure. So, the z-score is used to label the failure scenario, for then to use this to build a prediction model.","comment_id":"820868","upvote_count":"2"}],"comment_id":"766179","content":"C.\nwe need detect potential failures","timestamp":"1672881420.0","poster":"jamesking1103","upvote_count":"1"}],"upvote_count":"4","content":"Sorry, I think C is the answer","comment_id":"753169"}],"poster":"hiromi","timestamp":"1671489120.0","comment_id":"750292","upvote_count":"3"},{"timestamp":"1671024960.0","comment_id":"745116","poster":"ares81","comments":[{"poster":"ares81","content":"Thinking about it, it should be C.","upvote_count":"1","comment_id":"768631","timestamp":"1673102100.0"}],"content":"Selected Answer: A\nThis is really tricky, but it could be A.","upvote_count":"2"}],"answer_description":""},{"id":"KPOrL9WNiSIa2CH52eAI","answer_description":"","timestamp":"2022-12-13 08:30:00","topic":"1","question_text":"You are developing an ML model that uses sliced frames from video feed and creates bounding boxes around specific objects. You want to automate the following steps in your training pipeline: ingestion and preprocessing of data in Cloud Storage, followed by training and hyperparameter tuning of the object model using Vertex AI jobs, and finally deploying the model to an endpoint. You want to orchestrate the entire pipeline with minimal cluster management. What approach should you use?","answer_ET":"C","question_images":[],"unix_timestamp":1670916600,"isMC":true,"answer_images":[],"discussion":[{"content":"From:\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk \n\"1. If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX.\nTo learn more about building a TFX pipeline, follow the TFX getting started tutorials.\nTo learn more about using Vertex AI Pipelines to run a TFX pipeline, follow the TFX on Google Cloud tutorials.\n2. For other use cases, we recommend that you build your pipeline using the Kubeflow Pipelines SDK. By building a pipeline with the Kubeflow Pipelines SDK, you can implement your workflow by building custom components or reusing prebuilt components, such as the Google Cloud Pipeline Components. Google Cloud Pipeline Components make it easier to use Vertex AI services like AutoML in your pipeline.\"\n\nSo I guess since it is image processing, it should be Kubeflow - answer C (TFX is for structured or text data).","comments":[{"content":"TFX absolutely does support things other than structured and text datasets.","timestamp":"1725817740.0","poster":"baimus","comment_id":"1280507","comments":[{"comment_id":"1366131","upvote_count":"1","content":"Agreed. The Answer is TFX. This question is possibly old or wrong. TFX supports image data, and Vertex pipelines automates cluster management which was a critical priority.","poster":"5091a99","timestamp":"1741319340.0"}],"upvote_count":"2"}],"poster":"qaz09","comment_id":"799589","timestamp":"1675677660.0","upvote_count":"13"},{"upvote_count":"1","content":"Selected Answer: C\nD is can be or not based on the data, C is 100% irrespective of data, therefore C","poster":"Ankit267","comment_id":"1333321","timestamp":"1735446840.0"},{"upvote_count":"2","comment_id":"1318175","poster":"AB_C","content":"Selected Answer: B\nVertex AI Pipelines: This is Google Cloud's managed service for orchestrating ML workflows. It handles the execution of your pipeline steps, manages dependencies, and provides monitoring and logging capabilities. Crucially, it minimizes cluster management, which is a key requirement for you.\nTensorFlow Extended (TFX) SDK: TFX is a powerful SDK specifically designed for building production-ready ML pipelines. It provides pre-built components for common tasks like data ingestion, preprocessing, training, evaluation, and deployment, making it well-suited for your video frame processing and object detection pipeline.\nIntegration with Vertex AI: TFX components are designed to work seamlessly with Vertex AI services, including training jobs and endpoints. This ensures smooth transitions between pipeline steps and simplifies model deployment.","timestamp":"1732638360.0"},{"poster":"baimus","content":"Selected Answer: B\nThis question is designed to be TFX. It would be a weird thing to do to say \"Vertex pipelines with kubeflow SDK\" because that's just one of the ways to implement stuff in vertex pipelines, which it doesn't normally specify. TFX adds the things in the question on top of the functionality of Vertex.","upvote_count":"1","comment_id":"1280508","timestamp":"1725817860.0"},{"content":"Selected Answer: B\nminimal cluster management should rule out option c. why has everyone chosen that!it should be b","poster":"San1111111111","comment_id":"1253101","upvote_count":"2","timestamp":"1721654640.0"},{"timestamp":"1715427000.0","comment_id":"1209716","content":"You have to understand the ML lifecycle and difference between TFX and KFP better here. For ML end to end life cycle TFX is a better option, you can ensure Drift Detection/ Train Serve SKew with TFDV, you can easily perform serving with Tf.Serve and easily integrate TFX with Vertex AI pipeline which runs serverless. All these features are not directly available/managed in KFP ( as its user centric and user managed library).\n\nSo I would go here with B.","upvote_count":"3","poster":"pawan94"},{"content":"Selected Answer: C\nIf you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, should use TFX. For other use cases, Kubeflow. Link: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipelin","comment_id":"1199039","upvote_count":"2","timestamp":"1713605160.0","poster":"pinimichele01"},{"poster":"Ulule","upvote_count":"2","timestamp":"1709459940.0","comment_id":"1164684","content":"Selected Answer: B\nOverall, using Vertex AI Pipelines with TensorFlow Extended (TFX) SDK provides a comprehensive and managed solution for handling video feed data in an ML pipeline, while minimizing the need for manual infrastructure management and maximizing scalability and efficiency."},{"comment_id":"1087650","content":"I vote for be. the question stated that the minumumn clustering management is required, and I found this on the google study guide\" Vertex AI Pipelines automatically provisions underlying infrastructure and managed it for you\"","poster":"vale_76_na_xxx","upvote_count":"1","timestamp":"1701695100.0"},{"content":"Selected Answer: B\nminimal managment","upvote_count":"1","timestamp":"1700064420.0","poster":"Mickey321","comment_id":"1071604"},{"timestamp":"1683609720.0","upvote_count":"1","poster":"M25","comment_id":"892786","content":"Selected Answer: C\nWent with C"},{"upvote_count":"2","comment_id":"850211","poster":"tavva_prudhvi","content":"Selected Answer: C\nVertex AI Pipelines with Kubeflow Pipelines SDK provides a high-level interface for building end-to-end machine learning pipelines. This approach allows for easy integration with Google Cloud services, including Cloud Storage for data ingestion and preprocessing, Vertex AI for training and hyperparameter tuning, and deployment to an endpoint. The Kubeflow Pipelines SDK also allows for easy orchestration of the entire pipeline, minimizing cluster management.","timestamp":"1679755320.0"},{"comment_id":"847371","content":"Answer is C. If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, should use TFX. For other use cases, Kubeflow. Link: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline","poster":"neochaotic","timestamp":"1679509020.0","upvote_count":"2"},{"content":"Selected Answer: C\nAnswer C...\nhttps://cloud.google.com/architecture/ml-on-gcp-best-practices#use-vertex-pipelines","poster":"TNT87","comments":[{"upvote_count":"2","poster":"TNT87","timestamp":"1681711560.0","comment_id":"872374","content":"https://cloud.google.com/architecture/ml-on-gcp-best-practices#use-kubeflow-pipelines-sdk-for-flexible-pipeline-construction"}],"timestamp":"1678391760.0","comment_id":"834353","upvote_count":"2"},{"poster":"John_Pongthorn","timestamp":"1674882540.0","content":"Google want you to use core native service Pipeline, Don't overthink but , need to think it over.\nThe anwser is in\n https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build\nhttps://cloud.google.com/vertex-ai/docs/pipelines","upvote_count":"2","comment_id":"790304"},{"poster":"zeic","timestamp":"1672921620.0","upvote_count":"3","content":"Selected Answer: B\n\" You want to orchestrate the entire pipeline with minimal cluster management\"\nbecause of that it cant be answer c \ni vote for b, becausse there is no cluster management with vertex ai","comments":[{"timestamp":"1683832800.0","content":"nope, not correct","comment_id":"895356","upvote_count":"1","poster":"TNT87"}],"comment_id":"766603"},{"poster":"hiromi","upvote_count":"4","comment_id":"750296","timestamp":"1671489360.0","content":"Selected Answer: C\nC\n\"If you are using other frameworks, we recommend using Kubeflow Pipeline, which is very flexible and allows you to use simple code to construct pipelines. Kubeflow Pipeline also provides Google Cloud pipeline components such as Vertex AI AutoML.\"\n(Journey to Become a Google Cloud Machine Learning Engineer: Build the mind and hand of a Google Certified ML professional)"},{"content":"Selected Answer: C\nvote C","comment_id":"748000","upvote_count":"2","poster":"mil_spyro","timestamp":"1671274620.0"},{"content":"Selected Answer: C\nI will go C, because for generic orchestration purpose kuberflow is recommended while TFX should go with large scale tasks.","timestamp":"1670916600.0","comment_id":"743706","upvote_count":"2","poster":"YangG"}],"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/91326-exam-professional-machine-learning-engineer-topic-1-question/","question_id":300,"answer":"C","choices":{"D":"Use Cloud Composer for the orchestration.","A":"Use Kubeflow Pipelines on Google Kubernetes Engine.","B":"Use Vertex AI Pipelines with TensorFlow Extended (TFX) SDK.","C":"Use Vertex AI Pipelines with Kubeflow Pipelines SDK."},"answers_community":["C (59%)","B (41%)"]}],"exam":{"id":13,"provider":"Google","isMCOnly":true,"isBeta":false,"name":"Professional Machine Learning Engineer","lastUpdated":"11 Apr 2025","numberOfQuestions":304,"isImplemented":true},"currentPage":60},"__N_SSP":true}