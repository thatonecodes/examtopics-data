{"pageProps":{"questions":[{"id":"LVsTqYj7eELoNeXffV8W","question_id":61,"unix_timestamp":1697922240,"answer_description":"","discussion":[{"comment_id":"1056760","upvote_count":"5","timestamp":"1730198700.0","content":"answer is C.\nFor B, they specified that a user manager service account is attached to the instance, so the default one will not gonna be used.","poster":"khoukha"},{"upvote_count":"1","poster":"xhilmi","comment_id":"1090843","content":"Selected Answer: C\nThe issue described suggests that the service account associated with the Compute Engine instance may not have the necessary permissions to write logs to Cloud Logging. To resolve this issue following Google-recommended practices, you should choose option C: Add the Logs Writer role to the service account.\n\nBy adding the Logs Writer role to the service account, you grant the necessary permissions to write logs to Cloud Logging. This role provides the required access for the agents running on the instance to send log entries to Cloud Logging. Make sure to follow the principle of least privilege and only grant the minimum permissions required for your application to function.\n\nTherefore, the recommended solution is to add the Logs Writer role to the user-managed service account attached to the Compute Engine instance.","timestamp":"1733643780.0"},{"upvote_count":"2","comment_id":"1086055","content":"Selected Answer: C\nVote C","timestamp":"1733135400.0","poster":"nqthien041292"},{"content":"Selected Answer: C\nSame reason as KHOUKHA.","timestamp":"1730759940.0","upvote_count":"4","poster":"mshafa","comment_id":"1062439"},{"poster":"Mar_Mar","upvote_count":"2","comment_id":"1053846","content":"B is correct : https://cloud.google.com/logging/docs/agent/logging/troubleshooting#verify_default_service_account_permission","timestamp":"1729871340.0"},{"upvote_count":"3","timestamp":"1729544640.0","comment_id":"1049826","content":"I think answer C is correct in granting the existing service account the least privilege to write logs.","poster":"activist"}],"url":"https://www.examtopics.com/discussions/google/view/124265-exam-professional-cloud-devops-engineer-topic-1-question-153/","answer_images":[],"timestamp":"2023-10-21 23:04:00","question_images":[],"answer":"C","answer_ET":"C","choices":{"D":"Enable Private Google Access on the subnet that the instance is in.","C":"Add the Logs Writer role to the service account.","A":"Export the service account key and configure the agents to use the key.","B":"Update the instance to use the default Compute Engine service account."},"exam_id":6,"question_text":"You are configuring Cloud Logging for a new application that runs on a Compute Engine instance with a public IP address. A user-managed service account is attached to the instance. You confirmed that the necessary agents are running on the instance but you cannot see any log entries from the instance in Cloud Logging. You want to resolve the issue by following Google-recommended practices. What should you do?","topic":"1","answers_community":["C (100%)"],"isMC":true},{"id":"3ZQLJXofKzJbXfUHsaFv","timestamp":"2023-10-28 20:19:00","exam_id":6,"question_images":[],"question_id":62,"answers_community":["C (71%)","D (29%)"],"isMC":true,"answer_description":"","answer_images":[],"unix_timestamp":1698517140,"url":"https://www.examtopics.com/discussions/google/view/124850-exam-professional-cloud-devops-engineer-topic-1-question-154/","question_text":"As a Site Reliability Engineer, you support an application written in Go that runs on Google Kubernetes Engine (GKE) in production. After releasing a new version of the application, you notice the application runs for about 15 minutes and then restarts. You decide to add Cloud Profiler to your application and now notice that the heap usage grows constantly until the application restarts. What should you do?","choices":{"B":"Add high memory compute nodes to the cluster.","A":"Increase the CPU limit in the application deployment.","C":"Increase the memory limit in the application deployment.","D":"Add Cloud Trace to the application, and redeploy."},"answer_ET":"C","answer":"C","discussion":[{"timestamp":"1698517140.0","poster":"koo_kai","content":"Selected Answer: C\ninsufficient memory","comment_id":"1056376","upvote_count":"5"},{"upvote_count":"2","content":"Selected Answer: D\nD. Cloud Trace can help identify inefficient memory usage and pinpoint the root cause of the memory leak. Since Cloud Profiler already showed that heap usage is constantly increasing, the next step is to analyze memory allocations in detail. Cloud Trace provides visibility into request execution and can highlight which functions are consuming excessive memory or not releasing it properly. This helps in debugging and fixing the leak instead of just mitigating its effects.\n\nC. Increasing the memory limit in the deployment would provide a temporary workaround by delaying the OOM kill. However, this does not fix the root causeâ€”excessive memory allocation and retention. Eventually, the application will still exhaust the available memory and restart.","comment_id":"1361423","poster":"cachopo","timestamp":"1740484380.0"},{"poster":"PhilipKoku","content":"Selected Answer: D\nD) I would add Cloud Trace to find the root cause of the heap usage growth. Adding more memory will only solve the issue temporarily, and you might now see a restart in 30 minutes.","timestamp":"1720790160.0","upvote_count":"2","comment_id":"1246764"},{"poster":"xhilmi","upvote_count":"1","content":"Selected Answer: C\nGiven the scenario where the heap usage of the application grows constantly until it restarts, the issue is likely related to memory consumption. Therefore, the appropriate action to take is to address the memory-related problem. In this context, option C is the recommended.\n\nBy increasing the memory limit in the application deployment configuration, you provide more memory resources to the application, potentially preventing it from exhausting its memory and triggering restarts.\n\nOptions A and B may not directly address the underlying memory problem. Option D, adding Cloud Trace, is more focused on request tracing and may not directly resolve the memory growth issue. Therefore, increasing the memory limit is the most relevant step to take based on the symptoms described.","comment_id":"1090849","timestamp":"1702021680.0"},{"upvote_count":"1","content":"Selected Answer: C\nVote C","poster":"nqthien041292","comment_id":"1086056","timestamp":"1701513060.0"},{"content":"Selected Answer: C\nThat's what I do at work :-)","timestamp":"1699818840.0","comment_id":"1068848","upvote_count":"3","poster":"pharao89"}],"topic":"1"},{"id":"0zk4tZYZDqKf4sSZjnrf","unix_timestamp":1697922900,"timestamp":"2023-10-21 23:15:00","answers_community":["D (100%)"],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/124266-exam-professional-cloud-devops-engineer-topic-1-question-155/","answer_description":"","answer":"D","question_images":[],"answer_ET":"D","answer_images":[],"question_id":63,"choices":{"B":"Create a storage bucket with the name specified in the Terraform configuration.","D":"Grant the roles/storage.objectAdmin Identity and Access Management (1AM) role to the Cloud Build service account on the state file bucket.","C":"Grant the roles/owner Identity and Access Management (IAM) role to the Cloud Build service account on the project.","A":"Change the Terraform code to use local state."},"topic":"1","discussion":[{"timestamp":"1730658420.0","content":"Selected Answer: D\nAgree with D","upvote_count":"1","poster":"dija123","comment_id":"1206203"},{"timestamp":"1719619620.0","content":"Selected Answer: D\nthis is regarding the permission, hence providing the correct role will resolve this","comment_id":"1108249","upvote_count":"1","poster":"kish18"},{"upvote_count":"2","poster":"xhilmi","timestamp":"1717825860.0","comment_id":"1090854","content":"Selected Answer: D\nTo resolve that issue, you should ensure that the Cloud Build service account has the necessary permissions to access the Cloud Storage bucket used for storing Terraform state.\n\nThe recommended practice is to grant the roles/storage.objectAdmin Identity and Access Management (IAM) role to the Cloud Build service account on the state file bucket. Therefore, the correct answer is (option D)\n\nThis permission grants the necessary access for Cloud Build to read and write objects (which include Terraform state files) in the specified Cloud Storage bucket, resolving the 403 error. It's important to follow the principle of least privilege and only grant the permissions needed for the specific task at hand."},{"content":"Selected Answer: D\nVote D","comment_id":"1086058","upvote_count":"1","timestamp":"1717317180.0","poster":"nqthien041292"},{"comment_id":"1060849","upvote_count":"1","timestamp":"1714672680.0","content":"https://cloud.google.com/storage/docs/access-control/iam-roles","poster":"activist"},{"timestamp":"1714619160.0","poster":"lelele2023","upvote_count":"1","comment_id":"1060269","content":"Selected Answer: D\nThink it's D since the 403 error occurred while the tf actions queries state file in bucket, you only need the object admin permission(state file ).\n\nStorage Object Admin (roles/storage.objectAdmin) Grants full control over objects, including listing, creating, viewing, and deleting objects, as well as setting object ACLs. Also grants access to create, delete, get, and list managed folders."},{"timestamp":"1713734100.0","comment_id":"1049841","upvote_count":"3","poster":"activist","content":"Answer D seems to be correct."}],"exam_id":6,"question_text":"You are deploying a Cloud Build job that deploys Terraform code when a Git branch is updated. While testing, you noticed that the job fails. You see the following error in the build logs:\n\nInitializing the backend...\n\nError: Failed to get existing workspaces: querying Cloud Storage failed: googleapi: Error 403\n\nYou need to resolve the issue by following Google-recommended practices. What should you do?"},{"id":"8FpKMtMmA3vulq9l7NPz","url":"https://www.examtopics.com/discussions/google/view/124851-exam-professional-cloud-devops-engineer-topic-1-question-156/","topic":"1","answer":"B","answer_images":[],"choices":{"C":"Locate all the Pods with emptyDir volumes. Use the df -h command to measure volume disk usage.","A":"Check the node/ephemeral_storage/used_bytes metric by using Metrics Explorer.","B":"Check the container/ephemeral_storage/used_bytes metric by using Metrics Explorer.","D":"Locate all the Pods with emptyDir volumes. Use the df -sh * command to measure volume disk usage."},"answer_description":"","timestamp":"2023-10-28 20:21:00","isMC":true,"discussion":[{"content":"Selected Answer: B\nThe container/ephemeral_storage/used_bytes metric seems correct.","poster":"dija123","comment_id":"1210272","upvote_count":"1","timestamp":"1731435240.0"},{"timestamp":"1723804500.0","content":"Selected Answer: B\nhttps://cloud.google.com/monitoring/api/metrics_kubernetes#:~:text=container/ephemeral_storage/used_bytes\nThe container/ephemeral_storage/used_bytes metric can help you identify which pods are consuming ephemeral storage and potentially causing the DiskPressure condition.\n\n\nwhy not A?\nThe node/ephemeral_storage/used_bytes metric would show the total ephemeral storage used on a node, but it wouldn't help you identify which pods are consuming the storage.","comment_id":"1151989","upvote_count":"2","poster":"alpha_canary"},{"upvote_count":"2","content":"Selected Answer: B\nTo identify which Pods are causing the DiskPressure node condition due to ephemeral volumes, you should check the container/ephemeral_storage/used_bytes metric using Metrics Explorer.\n\nTherefore, the correct answer is B.\n\nThis metric provides information about the ephemeral storage usage of containers within Pods. By examining this metric, you can identify the specific Pods that are contributing to the DiskPressure condition on the worker nodes. Metrics Explorer allows you to visualize and analyze metrics data, making it a suitable tool for investigating resource usage and identifying potential issues in a GKE environment.","timestamp":"1717826160.0","comment_id":"1090857","poster":"xhilmi"},{"upvote_count":"1","poster":"nqthien041292","content":"Selected Answer: B\nVote B","comment_id":"1086061","timestamp":"1717317300.0"},{"poster":"mshafa","timestamp":"1715497020.0","content":"Selected Answer: B\nThis approach provides the necessary detail to identify which Pods are causing the DiskPressure condition without the need for direct access to the nodes or the ability to execute commands on them. Metrics Explorer in Google Cloud's operations suite (formerly Stackdriver) allows you to query and visualize metrics from your GKE environment, making it a powerful tool for this kind of diagnostic task.","upvote_count":"2","comment_id":"1068379"},{"comment_id":"1064330","timestamp":"1715029980.0","poster":"YushiSato","upvote_count":"3","content":"Selected Answer: B\nI think the answer is B.\nYou need to use container/ephemeral_storage/used_bytes because you need to identify the pod that is causing the problem.\npod/ephemeral_storage/used_bytes also exists, but it is still in beta.\nhttps://cloud.google.com/monitoring/api/metrics_kubernetes"},{"content":"Why not B?","upvote_count":"1","poster":"macqly","timestamp":"1714895700.0","comment_id":"1062750"},{"comments":[{"upvote_count":"1","timestamp":"1714620000.0","comment_id":"1060277","content":"why not B though?","poster":"lelele2023"}],"timestamp":"1714381440.0","content":"A:\nnode/ephemeral_storage/used_bytes GA\nEphemeral storage usage\nGAUGE, INT64, By\nk8s_node Local ephemeral storage bytes used by the node. Sampled every 60 seconds.","upvote_count":"1","comment_id":"1056766","poster":"khoukha"},{"timestamp":"1714328460.0","poster":"koo_kai","upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/monitoring/api/metrics_kubernetes","comment_id":"1056377","comments":[]}],"exam_id":6,"answer_ET":"B","question_images":[],"unix_timestamp":1698517260,"question_text":"Your company runs applications in Google Kubernetes Engine (GKE). Several applications rely on ephemeral volumes. You noticed some applications were unstable due to the DiskPressure node condition on the worker nodes. You need to identify which Pods are causing the issue, but you do not have execute access to workloads and nodes. What should you do?","answers_community":["B (92%)","8%"],"question_id":64},{"id":"o87V75iYMom2NVZjpQSc","isMC":true,"answers_community":["A (100%)"],"answer_images":[],"exam_id":6,"url":"https://www.examtopics.com/discussions/google/view/124852-exam-professional-cloud-devops-engineer-topic-1-question-157/","answer_ET":"A","answer":"A","topic":"1","question_id":65,"timestamp":"2023-10-28 20:24:00","unix_timestamp":1698517440,"question_images":[],"question_text":"You are designing a new Google Cloud organization for a client. Your client is concerned with the risks associated with long-lived credentials created in Google Cloud. You need to design a solution to completely eliminate the risks associated with the use of JSON service account keys while minimizing operational overhead. What should you do?","answer_description":"","discussion":[{"upvote_count":"1","poster":"alpha_canary","timestamp":"1723862820.0","content":"Selected Answer: A\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts#disable_service_account_key_creation","comment_id":"1152365"},{"timestamp":"1717826640.0","poster":"xhilmi","comment_id":"1090871","upvote_count":"2","content":"Selected Answer: A\nCorrect answer is A.\n\nBy applying the constraints/iam.disableServiceAccountKeyCreation constraint to the organization, you can prevent the creation of JSON service account keys, thus minimizing the risk associated with long-lived credentials.\n\nThis constraint disables the ability to create new service account keys, reducing the potential for misuse or compromise of credentials."},{"poster":"mshafa","timestamp":"1714854480.0","comment_id":"1062429","upvote_count":"3","content":"Selected Answer: A\nYou can use the iam.disableServiceAccountCreation boolean constraint to disable the creation of new service accounts. This allows you to centralize management of service accounts while not restricting the other permissions your developers have on projects."},{"comment_id":"1060279","timestamp":"1714620300.0","poster":"lelele2023","upvote_count":"2","content":"Selected Answer: A\n\"You can use the iam.disableServiceAccountKeyCreation boolean constraint to disable the creation of new external service account keys. This allows you to control the use of unmanaged long-term credentials for service accounts. When this constraint is set, user-managed credentials cannot be created for service accounts in projects affected by the constraint.\"\n\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts#disable_service_account_key_creation"},{"content":"Selected Answer: A\nconstraints/iam.disableServiceAccountKeyCreation","timestamp":"1714328640.0","poster":"koo_kai","upvote_count":"3","comment_id":"1056378"}],"choices":{"C":"Apply the constraints/iam.disableServiceAccountKeyUpload constraint to the organization.","A":"Apply the constraints/iam.disableServiceAccountKevCreation constraint to the organization.","D":"Grant the roles/iam.serviceAccountKeyAdmin IAM role to organization administrators only.","B":"Use custom versions of predefined roles to exclude all iam.serviceAccountKeys.* service account role permissions."}}],"exam":{"numberOfQuestions":196,"provider":"Google","id":6,"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"Professional Cloud DevOps Engineer","isBeta":false},"currentPage":13},"__N_SSP":true}