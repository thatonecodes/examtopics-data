{"pageProps":{"questions":[{"id":"VPesyqytDr9eyPiOSEHi","answer_description":"","choices":{"D":"Invoke a subworkflow in Workflows to apply the logic on your JSON file.","B":"Create a Dataproc cluster, and use PySpark to apply the logic on your JSON file.","A":"Create a Cloud Composer environment and run the logic in Cloud Composer.","C":"Invoke a Cloud Function instance that uses Python to apply the logic on your JSON file."},"question_id":241,"unix_timestamp":1734316740,"question_images":[],"discussion":[{"timestamp":"1736155980.0","upvote_count":"2","poster":"Pime13","comment_id":"1337055","content":"Selected Answer: C\nC. Invoke a Cloud Function instance that uses Python to apply the logic on your JSON file.\n\nUsing a Cloud Function allows you to run your Python code in a serverless environment, which simplifies deployment and management. It also ensures quick execution and scalability, as Cloud Functions can handle the processing of your JSON response efficiently"},{"poster":"FireAtMe","upvote_count":"2","timestamp":"1734316740.0","content":"Selected Answer: C\nCloud Functions is a serverless compute service ideal for executing lightweight, event-driven tasks with low latency.","comment_id":"1327142"}],"question_text":"You are using Workflows to call an API that returns a 1KB JSON response, apply some complex business logic on this response, wait for the logic to complete, and then perform a load from a Cloud Storage file to BigQuery. The Workflows standard library does not have sufficient capabilities to perform your complex logic, and you want to use Python's standard library instead. You want to optimize your workflow for simplicity and speed of execution. What should you do?","answer":"C","timestamp":"2024-12-16 03:39:00","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/google/view/153020-exam-professional-data-engineer-topic-1-question-315/","topic":"1","answer_ET":"C","answer_images":[],"isMC":true,"exam_id":11},{"id":"aixpi4Hss8PWvC0xTst0","answer_images":[],"discussion":[{"comment_id":"1337056","content":"Selected Answer: C\nC. Build materialized views on top of the sales table to aggregate data at the day and month level.\n\nMaterialized views are precomputed views that cache the results of a query, which can significantly improve query performance and reduce costs by avoiding repeated computation. They automatically update with changes to the base table, ensuring data freshness without additional maintenance.\n\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro","poster":"Pime13","timestamp":"1736156100.0","upvote_count":"2"},{"poster":"hussain.sain","upvote_count":"2","content":"Selected Answer: C\nC is the answer.\nMaterialized Views:\n\nMaterialized views in BigQuery are precomputed views that store the results of a query, allowing for much faster query execution because BigQuery doesn’t need to recompute the results each time the query is run. The results are stored in a persistent table, which significantly improves performance for repeated queries that aggregate the same data.\nIn this case, you can create materialized views that aggregate the sales data at the day and month levels. This will reduce the amount of data that needs to be processed for each query and speed up the response time.\nMaterialized views also lower costs because BigQuery only scans the precomputed data in the materialized view, rather than the full 50 TB sales history table.","timestamp":"1735307820.0","comment_id":"1332432"}],"question_text":"You are administering a BigQuery on-demand environment. Your business intelligence tool is submitting hundreds of queries each day that aggregate a large (50 TB) sales history fact table at the day and month levels. These queries have a slow response time and are exceeding cost expectations. You need to decrease response time, lower query costs, and minimize maintenance. What should you do?","exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/153174-exam-professional-data-engineer-topic-1-question-316/","answers_community":["C (100%)"],"topic":"1","choices":{"D":"Create a scheduled query to build sales day and sales month aggregate tables on an hourly basis.","B":"Enable BI Engine and add your sales table as a preferred table.","C":"Build materialized views on top of the sales table to aggregate data at the day and month level.","A":"Build authorized views on top of the sales table to aggregate data at the day and month level."},"answer_description":"","answer_ET":"C","question_id":242,"unix_timestamp":1734563520,"timestamp":"2024-12-19 00:12:00","question_images":[],"answer":"C","isMC":true},{"id":"WsOG4euejpLzwxRqLZUL","question_images":[],"topic":"1","exam_id":11,"question_text":"You have several different unstructured data sources, within your on-premises data center as well as in the cloud. The data is in various formats, such as Apache Parquet and CSV. You want to centralize this data in Cloud Storage. You need to set up an object sink for your data that allows you to use your own encryption keys. You want to use a GUI-based solution. What should you do?","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/152975-exam-professional-data-engineer-topic-1-question-317/","choices":{"B":"Use Storage Transfer Service to move files into Cloud Storage","A":"Use BigQuery Data Transfer Service to move files into BigQuery.","C":"Use Dataflow to move files into Cloud Storage","D":"Use Cloud Data Fusion to move files into Cloud Storage."},"question_id":243,"answer_ET":"D","answer_images":[],"answer_description":"","timestamp":"2024-12-14 02:24:00","answer":"D","answers_community":["D (64%)","B (36%)"],"unix_timestamp":1734139440,"discussion":[{"upvote_count":"1","comment_id":"1337057","timestamp":"1736156340.0","poster":"Pime13","content":"Selected Answer: D\nCloud Data Fusion is a fully managed, cloud-native data integration service that provides a graphical interface for building and managing data pipelines. It supports various data formats and allows you to use your own encryption keys for secure data transfer"},{"upvote_count":"1","comment_id":"1332665","poster":"apoio.certificacoes.closer","timestamp":"1735339380.0","content":"Selected Answer: D\nI have read in previous questions that Transfer Services only uses CMEK in-transit. \nhttps://cloud.google.com/storage-transfer/docs/on-prem-security#in-flight"},{"poster":"m_a_p_s","comment_id":"1331217","timestamp":"1735068420.0","content":"Selected Answer: D\nD - only Cloud Data Fusion is a GUI-based solution.","upvote_count":"3"},{"timestamp":"1735053840.0","poster":"skycracker","content":"Selected Answer: D\ndata fusion allows encryption","upvote_count":"2","comment_id":"1331166"},{"comment_id":"1326308","poster":"noiz","upvote_count":"4","content":"Selected Answer: B\nIs B incorrect?\nTransfer service + CloudKMS","timestamp":"1734139440.0"}]},{"id":"tiQGd61SjbE9m33x9o4s","answer_images":[],"answer_ET":"A","topic":"1","answers_community":["A (55%)","D (27%)","Other"],"timestamp":"2024-12-04 17:57:00","answer_description":"","answer":"A","choices":{"A":"Schedule a daily export of the table to a Cloud Storage dual or multi-region bucket.","D":"Modify ETL job to load the data into both the current and another backup region.","C":"Schedule a daily BigQuery snapshot of the table.","B":"Schedule a daily copy of the dataset to a backup region."},"question_images":[],"exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/152516-exam-professional-data-engineer-topic-1-question-318/","isMC":true,"question_id":244,"discussion":[{"comment_id":"1399130","timestamp":"1742099460.0","upvote_count":"1","poster":"Parandhaman_Margan","content":"Selected Answer: C\nMeets the RPO requirement (< 24 hours)\nCost-effective solution\nQuick recovery from regional failures"},{"content":"Selected Answer: D\nhttps://cloud.google.com/blog/topics/developers-practitioners/backup-disaster-recovery-strategies-bigquery\n\nGoogle presents both A and D\nWhy A:\n- Cost: Lower. GCS storage is significantly cheaper than BigQuery storage. You pay for storage in GCS and minimal egress charges when exporting.\n- Complexity: Simpler. You schedule a daily export job. Restoring involves importing from GCS to BigQuery in another region.\n- Consistency: Easier to manage. The export process creates a consistent snapshot of the data at the time of export. You might have some latency (up to 24 hours in this scenario), but the data within the export is consistent.\n- RPO: Meets the requirement. A daily export ensures an RPO of less than 24 hours.\n- RTO: Depends on the restore process from GCS to BigQuery. You can pre-provision slots in the backup region to minimize restore time.","comment_id":"1356455","timestamp":"1739543880.0","upvote_count":"1","poster":"MarcoPellegrino"},{"content":"Selected Answer: A\nThis approach ensures that your data is stored in multiple regions, providing redundancy and protection against regional failures \n\nc is not possible it has some limitations compared to exporting the table to a Cloud Storage dual or multi-region bucket:\n\nRegional Limitation: BigQuery table snapshots are limited to the same region as the base table. This means that if the entire region experiences a failure, the snapshot may not be accessible.\n\nStorage Costs: While snapshots can be cost-effective, they still incur storage costs for the data that is different from the base table. Exporting to Cloud Storage can be more cost-effective, especially when using dual or multi-region buckets.\n\nRPO Considerations: Both options can meet an RPO of less than 24 hours, but exporting to Cloud Storage provides additional redundancy by storing data in multiple regions, enhancing data availability and durability","timestamp":"1736156520.0","upvote_count":"1","poster":"Pime13","comment_id":"1337060"},{"upvote_count":"1","timestamp":"1735332960.0","comment_id":"1332619","content":"Selected Answer: A\nBoth A and B works. But it is cheaper to save data in GCS.","poster":"FireAtMe"},{"comment_id":"1328767","content":"Selected Answer: D\nOpción D: Modify ETL job to load the data into both the current and another backup region\nEvaluación:\nAjustar el ETL para escribir en dos tablas (una en la región principal y otra en una región de respaldo) asegura que los datos estén disponibles en ambas ubicaciones casi en tiempo real.\nEsto garantiza un RPO de menos de 24 horas, ya que las actualizaciones intradía se reflejan en ambas regiones.\nAunque podría aumentar los costos de almacenamiento por duplicar los datos, es la solución más efectiva y directa para proteger contra fallos regionales.","poster":"joelcaro","timestamp":"1734564180.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1734478800.0","content":"Selected Answer: B\nIn most cases, it is cheaper to copy a BigQuery dataset to a new region directly rather than exporting it to a Cloud Storage bucket and then loading it into a new BigQuery dataset in the desired region, as you only pay for data transfer costs when copying within BigQuery, while exporting to a bucket incurs additional storage charges for the exported data in Cloud Storage, even if it's only temporary. \nKey points to consider:\n\n No extra storage cost for copying:\n When copying a BigQuery dataset to a new region, you only pay for the data transfer cost, not the storage of the data in a separate location. \n\nStorage cost for exporting:\nExporting data to a Cloud Storage bucket means you are charged for the storage of that data in the bucket until you delete it, even if you are just temporarily storing it for transfer.","comment_id":"1328208","poster":"mdell"},{"timestamp":"1733331420.0","comments":[{"timestamp":"1734475500.0","content":"Additionally it only mentions backing up the sales table and not the entire dataset","comment_id":"1328200","upvote_count":"1","poster":"mdell"}],"content":"Selected Answer: A\nOption A is the most cost efficient: https://cloud.google.com/blog/topics/developers-practitioners/backup-disaster-recovery-strategies-bigquery","upvote_count":"4","comment_id":"1322028","poster":"HectorLeon2099"}],"unix_timestamp":1733331420,"question_text":"You are using BigQuery with a regional dataset that includes a table with the daily sales volumes. This table is updated multiple times per day. You need to protect your sales table in case of regional failures with a recovery point objective (RPO) of less than 24 hours, while keeping costs to a minimum. What should you do?"},{"id":"t5vjpNX3CYHkAUntF8YF","answer_ET":"C","discussion":[{"timestamp":"1733330940.0","upvote_count":"6","content":"Selected Answer: C\nIt's C. \"A\" removes data and retaining all is a requirement.","poster":"HectorLeon2099","comment_id":"1322022"},{"poster":"Nagamanikanta","content":"Selected Answer: C\noption C\nwe can simply mask the data and process in biguery","timestamp":"1741350180.0","comment_id":"1366248","upvote_count":"1"},{"poster":"Pime13","timestamp":"1736156700.0","content":"Selected Answer: C\nC. Use Dataflow and the Cloud Data Loss Prevention API to mask sensitive data. Write the processed data in BigQuery.\n\nThis approach ensures that sensitive data elements are protected through masking, which meets data privacy requirements. At the same time, it retains the data in a usable form for future analyses","upvote_count":"3","comment_id":"1337061"}],"exam_id":11,"answers_community":["C (100%)"],"timestamp":"2024-12-04 17:49:00","answer_images":[],"isMC":true,"choices":{"B":"Use customer-managed encryption keys (CMEK) to directly encrypt the data in Cloud Storage. Use federated queries from BigQuery. Share the encryption key by following the principle of least privilege.","D":"Use Dataflow and Cloud KMS to encrypt sensitive fields and write the encrypted data in BigQuery. Share the encryption key by following the principle of least privilege.","C":"Use Dataflow and the Cloud Data Loss Prevention API to mask sensitive data. Write the processed data in BigQuery.","A":"Use the Cloud Data Loss Prevention API and Dataflow to detect and remove sensitive fields from the data in Cloud Storage. Write the filtered data in BigQuery."},"question_images":[],"unix_timestamp":1733330940,"question_id":245,"answer_description":"","topic":"1","question_text":"You are preparing an organization-wide dataset. You need to preprocess customer data stored in a restricted bucket in Cloud Storage. The data will be used to create consumer analyses. You need to follow data privacy requirements, including protecting certain sensitive data elements, while also retaining all of the data for potential future use cases. What should you do?","url":"https://www.examtopics.com/discussions/google/view/152515-exam-professional-data-engineer-topic-1-question-319/","answer":"C"}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Google","isBeta":false,"numberOfQuestions":319,"isMCOnly":true,"name":"Professional Data Engineer","id":11,"isImplemented":true},"currentPage":49},"__N_SSP":true}