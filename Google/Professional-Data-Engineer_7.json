{"pageProps":{"questions":[{"id":"heeVboCFqLzEhDDUjT5P","discussion":[{"upvote_count":"54","content":"B - You only need a PoC and it has be done quickly","poster":"Callumr","comment_id":"114675","timestamp":"1608465660.0"},{"content":"Correct - A","timestamp":"1600761120.0","comment_id":"66891","upvote_count":"20","poster":"[Removed]"},{"timestamp":"1737244020.0","comment_id":"1342801","content":"Selected Answer: A\nThe key difference between Google Cloud Vision AutoML and Cloud Vision API is that Cloud Vision API provides pre-trained models for basic image analysis tasks like object detection and labeling, while Cloud Vision AutoML allows you to train custom machine learning models to identify specific objects or concepts within images that are unique to your dataset, requiring you to provide labeled training data.","upvote_count":"1","poster":"grshankar9"},{"timestamp":"1732574580.0","comment_id":"1218563","upvote_count":"3","content":"Selected Answer: A\nAutoML Vision is deprecated since march 31, 2024. The question will refer to Vertex AI AutoML. And as bet practice, the minimum dataset size for each label is 1000. So, with an updated question, the answer would be A.","poster":"josech"},{"timestamp":"1728076560.0","content":"Selected Answer: A\nA. Use Cloud Vision AutoML with the existing dataset.\n\nHere's why this is the most suitable option:\n\nSpeed and Ease: AutoML simplifies model building. You simply upload your labeled images, and AutoML takes care of model selection, training, and evaluation.\nExisting Dataset Sufficiency: Your dataset (750 components x 1000 images each) is a decent starting point for AutoML, allowing you to quickly test its effectiveness.\nMinimal Custom Development: AutoML's out-of-the-box deployment options let you integrate the model into your app without extensive coding.","poster":"CGS22","upvote_count":"1","comment_id":"1189548"},{"upvote_count":"1","timestamp":"1709986680.0","comment_id":"1003088","poster":"saado9","content":"Selected Answer: B\nOption B is the fastest way to train a model that can be used to recognize the 750 different components."},{"timestamp":"1692269820.0","content":"Whats wrong with C, its fast, cheap and add your 750 labels which is not big work. \nAutoML is good to train on big dataset and costly as compared to APIs","comments":[{"poster":"forepick","comment_id":"911775","timestamp":"1701417360.0","content":"Adding custom labels to Vision API is done by training an AutoML model! That's the formal recommendation. And you don't need a big dataset for AutoML as it uses transfer learning.","upvote_count":"4"},{"content":"it is a labeled dataset and why do you need to label it once again? So no C","comment_id":"963313","poster":"knith66","upvote_count":"1","timestamp":"1706247120.0"}],"poster":"musumusu","upvote_count":"2","comment_id":"811877"},{"content":"A - https://cloud.google.com/vertex-ai/docs/beginner/beginners-guide Target at least 1000 examples per target","comments":[{"timestamp":"1691458860.0","comment_id":"801629","content":"The quick POC part can be achieved by using Auto ML instead of creating and training your own model","upvote_count":"1","poster":"techtitan"}],"upvote_count":"8","poster":"techtitan","comment_id":"801628","timestamp":"1691458740.0"},{"upvote_count":"8","comments":[{"upvote_count":"2","comment_id":"762463","poster":"AzureDP900","content":"A is correct","timestamp":"1688089740.0"}],"timestamp":"1686209340.0","comment_id":"738826","poster":"odacir","content":"Selected Answer: A\nFirst I think in Vision API, but that is a pre-trained AI, will not recognize my labels, so because you have 1000 samples per item, AUTO ML is perfect. B cannot be because have not sensed to reduce your dataset if you have the recommended number of info.\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label."},{"upvote_count":"4","comment_id":"734393","comments":[{"comment_id":"909633","upvote_count":"2","timestamp":"1701289860.0","comments":[{"timestamp":"1707061500.0","content":"I believe that the ideal would be to reduce the number of components for the POC and preserve the number of examples, so my answer is A.","poster":"NewDE2023","upvote_count":"1","comment_id":"972212"}],"content":"So how are you going to test that the model was able to adequately learn from the sample? The point of splitting a dataset is to train the model on one part of the data (say 80%), and then test it on the other part (20%). If your model is able to predict the outcome of (most of) the sample points in your test dataset, you can be confident that it will work well on future data. Without a test data set, however, you have no such feedback. Therefore, the answer is B.","poster":"ga8our"}],"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label.","timestamp":"1685784780.0","poster":"zellck"},{"poster":"gudiking","content":"A - https://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category","timestamp":"1684418340.0","comment_id":"721388","upvote_count":"1"},{"comment_id":"717339","poster":"MarielaYBird","timestamp":"1683978840.0","content":"Selected Answer: B\nBased on this:\n\"As a rule of thumb, we recommend to have at least 100 training samples per class if you have distinctive and few classes, and more than 200 training samples if the classes are more nuanced and you have more than 50 different classes\"\n\n750 different components = more than 50 different classes. That means we need more than 200 training samples. If we used 250 training samples out of the 1000 samples and multiply it to 750 different classes we get a total of 187,500 which is the equivalent of reducing the dataset twice. \n\nhttps://cloud.google.com/vision/automl/object-detection/docs/prepare#how_big_does_the_dataset_need_to_be","upvote_count":"5"},{"comment_id":"697420","content":"Selected Answer: A\nI choose A because on the vertex AI documentation (https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data), on the best practices of preparing data for image recognition recommend this: We recommend about 1000 training images per label. The minimum per label is 10. In general, it takes more examples per label to train models with multiple labels per image, and resulting scores are harder to interpret.\n\nI know that is PoC, but if you do it without enough accuracy, you maybe discard the solution because it isn't fit for your requirements. So is better to do it with enough data to be sure that the model is or not accuracy enough with this data, because you maybe haven't enough accuracy and the problem is the quality of the data and not the amount of it.","timestamp":"1681737300.0","poster":"josrojgra","upvote_count":"3"},{"upvote_count":"3","poster":"John_Pongthorn","timestamp":"1680313620.0","comments":[{"upvote_count":"1","content":"The more labels, the more accurate the result.","comment_id":"683921","timestamp":"1680313680.0","poster":"John_Pongthorn"}],"content":"Selected Answer: A\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\n\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label.","comment_id":"683920"},{"content":"Selected Answer: B\n750*1000 are a lot.","comment_id":"661901","poster":"changsu","upvote_count":"1","timestamp":"1678165620.0"},{"content":"Selected Answer: A\nIt is labeled, so A is correct","poster":"ducc","timestamp":"1677553440.0","upvote_count":"1","comment_id":"653721"},{"content":"It's A. \nhttps://cloud.google.com/vision/automl/docs/beginners-guide#data_preparation \n\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label.","poster":"civilizador","upvote_count":"5","comments":[{"poster":"civilizador","upvote_count":"1","timestamp":"1676908080.0","content":"So even for POC better to use 1000 . There would be no significant time differences anyway between 500 and 1000","comment_id":"649456"}],"timestamp":"1676908020.0","comment_id":"649455"},{"upvote_count":"3","content":"Option A & B are quite close. Refer: https://cloud.google.com/vision/automl/docs/beginners-guide#data_preparation – Says to target at least 1000 images per label for training.","poster":"TheRealBsh","timestamp":"1675355880.0","comment_id":"641304"},{"upvote_count":"1","content":"Selected Answer: B\nB\ncant choose A because model needs to pass through the dataset several times for a proof of concept, existing data set samples might not be all seen in several working days causing over generalization","timestamp":"1669771860.0","poster":"czokwe","comment_id":"609481"},{"content":"I don't get it, why B rather than A? I know it's a proof of concept, but it's not like a bigger dataset is any kind of a dealbreaker in training a model of that size, and more data provides more accuracy to the model.\n\nI would choose A or C.","comment_id":"607218","timestamp":"1669386480.0","upvote_count":"2","poster":"Kriegs"},{"timestamp":"1669331400.0","comment_id":"606903","content":"Selected Answer: A\nAutoML work well with more than 100 training images/label but ideally with 1000 of each.","upvote_count":"1","poster":"[Removed]"},{"poster":"michalsosn","timestamp":"1668534240.0","content":"To keep the training time low, you should set the node hour budget to the desired number of hours, right? Not reduce the training set. Unless there's some limit in AutoML that makes 375k images acceptable and 750k not, I think I'd pick A rather than B.","upvote_count":"2","comment_id":"602169"},{"timestamp":"1668417900.0","content":"Selected Answer: A\nhttps://cloud.google.com/vision/automl/object-detection/docs/prepare\n\nAnnotation requirements \nFor each label you must have at least 10 images, each with at least one annotation (bounding box and the label).\n\nHowever, for model training purposes it's recommended you use about 1000 annotations per label. In general, the more images per label you have the better your model will perform.","upvote_count":"3","poster":"VGalan","comment_id":"601468","comments":[{"poster":"homaj","upvote_count":"1","comment_id":"604112","timestamp":"1668890880.0","content":"IT'S A POC"}]},{"content":"Selected Answer: C\nC.\nTraining a model only for POC is not cost-effective. Further, vision model is very hard to train, not mention to train a workable model only in few days.","poster":"sw52099","upvote_count":"1","timestamp":"1668261060.0","comment_id":"600598"},{"upvote_count":"2","content":"If you're gonna reduce the size, will you get better results? because, in the cloud.google.com/vision/automl/object-detection/docs/prepare it's clearly said \"As a rule of thumb, we recommend to have at least 100 training samples per class if you have distinctive and few classes, and more than 200 training samples if the classes are more nuanced and you have more than 50 different classes.\" hence, as we have 750 classes on average with 1000 images, if you reduce it twice? Yes, the training will become faster but with better accuracy? Anyone please clarify if its' A/B?","poster":"tavva_prudhvi","comment_id":"576067","timestamp":"1664266800.0"},{"upvote_count":"2","content":"B is correct. C is wrong as Vision API's are pretrained models for generic use cases.","comment_id":"559412","poster":"rbeeraka","timestamp":"1662115200.0"},{"comment_id":"558734","upvote_count":"3","poster":"Aslkdup","timestamp":"1662028860.0","content":"My opinion is A. \n1000 labeled images for each item is the best fit with A. https://cloud.google.com/vision/automl/docs/prepare"},{"upvote_count":"1","timestamp":"1661094660.0","comment_id":"553039","content":"Selected Answer: A\nAnswer : A","poster":"Prasanna_kumar"},{"timestamp":"1658941140.0","upvote_count":"2","comment_id":"534043","poster":"sagar_dis","content":"Selected Answer: C\nhttps://cloud.google.com/vision\nAutoML Vision enables you to train machine learning models to classify your images according to your own defined labels.\n\nVision API\nVision API offers powerful pre-trained machine learning models through REST and RPC APIs. Assign labels to images and quickly classify them into millions of predefined categories"},{"poster":"davidqianwen","timestamp":"1658661600.0","upvote_count":"3","content":"Selected Answer: B\nB is the answer","comment_id":"531319"},{"timestamp":"1658644260.0","poster":"GCP2022","content":"Got this question in 17 Oct 2021 exam","comment_id":"531145","upvote_count":"4"},{"upvote_count":"2","content":"Got this question in 17 Oct 2022 exam","timestamp":"1658644200.0","comment_id":"531144","poster":"GCP2022","comments":[{"poster":"muky31dec","comment_id":"702188","timestamp":"1682255160.0","content":"What is correct Ans? A or B","upvote_count":"1"}]},{"comment_id":"525959","timestamp":"1658070360.0","upvote_count":"7","content":"Selected Answer: B\nAns B\nVision API is giving just a rough idea about the image. Not exactly what we want. Because of that, we can exclude C. Since D is time-consuming D also can exclude.\n\nFight between A and B. For Auto, ML minimum is required 100 images per label. Even we get half or full the dataset is completely OK. But remember this is POC. In industry after POC most of the time we add new features or there are some changes. We just need to verify that by using this dataset and technologies we can implement. So no needs to waste your resources. \n\nAns B\nSee the small Comparisum.\nhttps://www.youtube.com/watch?v=GbLQE2C181U","poster":"Bhawantha"},{"poster":"MaxNRG","comment_id":"520268","content":"Selected Answer: A\nA: https://cloud.google.com/automl/","timestamp":"1657371720.0","upvote_count":"4"},{"comment_id":"519502","upvote_count":"3","comments":[{"poster":"MaxNRG","comment_id":"531084","content":"good point, C make sense","upvote_count":"2","comments":[{"timestamp":"1658636580.0","upvote_count":"1","comment_id":"531085","poster":"MaxNRG","content":"https://cloud.google.com/vision/docs\nCloud Vision allows developers to easily integrate vision detection features within applications, including image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content."}],"timestamp":"1658636460.0"}],"poster":"medeis_jar","timestamp":"1657275480.0","content":"Selected Answer: C\nIn A and B with Cloud Vision AutoML you will make a model based on data and in question it clearly says build a PoC app ASAP, so Cloud Vision API (with prebuild image models ) + labeled data seems to be the fastest option."},{"content":"Answer is C","comment_id":"518840","timestamp":"1657175340.0","upvote_count":"1","poster":"Kalyan1206"},{"comment_id":"510027","content":"Selected Answer: C\nyou have a labeled the dataset. You are able to use the train the model.","timestamp":"1656298800.0","upvote_count":"1","poster":"Yonghai"},{"timestamp":"1648438680.0","comment_id":"453004","content":"Maximum data set size should be 150,000 only. So need to split data for one iteration of training --\"Images in each dataset : 150,000 maximum\"","poster":"navemula","upvote_count":"1"},{"comment_id":"422333","timestamp":"1644451920.0","poster":"Sushil_123","comments":[{"upvote_count":"3","timestamp":"1645019100.0","content":"how was your exam ? did you find good reference from here ?","poster":"KalyanSinha","comment_id":"425763","comments":[{"upvote_count":"1","poster":"BigQuery","content":"LMAO. NO ONE ANSWERING. I AM ALSO HAVING EXAM IN A MONTH. I WILL UPDATE YOU GUYS. AFTER MY EXAM.","comments":[{"upvote_count":"1","content":"Have you taken exam? how did it go? what percentage of questions came from examtopics? with all the questions there is so many discussions happening, for the topics which are new to me, getting confused which option to choose, even if the same question comes in actual exam.","poster":"GCPLearning2021","comment_id":"518693","timestamp":"1657149660.0"}],"timestamp":"1654402380.0","comment_id":"494108"}]},{"content":"@Sushil_123 How was your exam? How many questions did you get from examtopics?","timestamp":"1646512380.0","upvote_count":"1","comment_id":"439870","poster":"Ral17"},{"content":"I have the exam in a week or so. How was your exam? How helpful were examtopics questions?","timestamp":"1646959860.0","comment_id":"442721","poster":"tainangao","upvote_count":"2"}],"content":"Hi All,\nI have exam in few days. How many questions we can expect from examtopics","upvote_count":"2"},{"upvote_count":"3","comments":[{"timestamp":"1687929720.0","content":"but it has to be working version so no need to reduce the training data","comment_id":"759491","poster":"Wonka87","upvote_count":"1"}],"content":"Vote for B, POC is a small-scale experiments","timestamp":"1641216300.0","poster":"sumanshu","comment_id":"397527"},{"poster":"vbondoo7","timestamp":"1639984800.0","content":"Should be B - as its just a POC","comment_id":"385965","upvote_count":"2"},{"content":"I think it's A as well. In general, don't reduce the dataset to speed up the PoC, it's a very bad habit. PoC should just direct you to AutoML rather than training own model. B could work but since you have 750 Classes, it is not recommended to reduce the size of the dataset. Rule of thumb is 100 images / class only if you have very few distinctive classes. here you have 750 classes and no idea if they are different. \" \nFor each label you must have at least 10 images, each with at least one annotation (bounding box and the label).\n\nHowever, for model training purposes it's recommended you use about 1000 annotations per label. In general, the more images per label you have the better your model will perform.\" https://cloud.google.com/vision/automl/object-detection/docs/prepare","timestamp":"1639386720.0","upvote_count":"3","comment_id":"380920","poster":"mkport1"},{"upvote_count":"4","timestamp":"1623184500.0","poster":"beedle","content":"its says 750 diffrent components...so A","comments":[{"content":"x1000 average examples (set of 750.000 images). So B) it's enougth for a PoC.","comment_id":"508419","upvote_count":"1","poster":"Jlozano","timestamp":"1656057660.0"}],"comment_id":"238727"},{"comment_id":"221880","content":"C..similar vision api product search exists..pretrained labels are used while in automl we need to train so that skillset is required to customize..in vision api just in excel we can write the label as component and image n can use this to classify the new incoming images","upvote_count":"2","timestamp":"1621335660.0","comments":[{"comment_id":"221893","upvote_count":"1","timestamp":"1621336320.0","content":"But vision api supports rpc and api while automl has ui interface so no much coding required...confused between a and c","poster":"kavs"}],"poster":"kavs"},{"poster":"Alasmindas","content":"Option A , reason:-\n- Training data should be as much as possible for better prediction.","comments":[{"comments":[],"content":"But, the problem is they didnt ask for accuracy! they only want to the POC to be done within few working days!","upvote_count":"1","poster":"tavva_prudhvi","comment_id":"589858","timestamp":"1666429980.0"}],"timestamp":"1620654360.0","upvote_count":"4","comment_id":"216724"},{"comment_id":"216081","comments":[{"timestamp":"1623655500.0","content":"Somewhat right, but the question is about POC which should not be as accurate as possible. Instead of 1000 per class, 500 per class is still much better than the bare minimum (100) but allows for faster training.","upvote_count":"3","poster":"xrun","comment_id":"243412"}],"upvote_count":"8","timestamp":"1620571500.0","poster":"Cloud_Enthusiast","content":"A: \nhttps://cloud.google.com/vision/automl/docs/beginners-guide\n\n\"The bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label.\""},{"upvote_count":"4","comment_id":"196961","poster":"paragkhetam","timestamp":"1617995820.0","content":"B as it is POC"},{"upvote_count":"6","timestamp":"1616501460.0","content":"Answer should be A: Refer the below documentation. Even with complete data set total record set would be 750K. For record set between 100K to 1000K the suggested time by Google is 1-6 Hrs.","comments":[{"comment_id":"185176","upvote_count":"3","poster":"SteelWarrior","timestamp":"1616501520.0","content":"https://cloud.google.com/automl-tables/docs/train","comments":[{"content":"Does one picture compare to a single row though? A picture is more complicated than a normal table row","timestamp":"1616569920.0","poster":"Diqtator","upvote_count":"1","comment_id":"185882"}]}],"comment_id":"185175","poster":"SteelWarrior"},{"content":"Hmm, to reduce the dataset like that we'd have to delete pictures for each product by the same amount. We can't just reduce the whole dataset since that would probably skew the amount of data for each product.\n\nThe easiest way would be A and just let it chew through all of the pictures. I am not sure about the time needed to do that though.","comments":[{"content":"But for a PoC it might not matter, so B might be the best and fastest option anyway if we assume they reduce the dataset evenly over all products?","comment_id":"184237","upvote_count":"2","timestamp":"1616401980.0","poster":"Diqtator"}],"comment_id":"184236","poster":"Diqtator","timestamp":"1616401800.0","upvote_count":"3"},{"poster":"rajnishd","upvote_count":"1","timestamp":"1614796560.0","comment_id":"172792","content":"C is correct, Automl requires customized dataset"},{"poster":"haroldbenites","comment_id":"163129","timestamp":"1613940180.0","upvote_count":"2","content":"C is correct. This scenario don't require some customize trining.\nAuto ML is used better when the recognition is customized and it requires some more specialized."},{"timestamp":"1608955860.0","comment_id":"120098","poster":"dambilwa","content":"Option [A] & [B] are quite close. https://cloud.google.com/vision/automl/docs/beginners-guide#data_preparation - Says to target atleast 1000 images per label for training.. Hence, I'll got with Option [A]","upvote_count":"5","comments":[{"timestamp":"1614184500.0","upvote_count":"7","comment_id":"165283","poster":"dragon123","content":"The bare minimum required by AutoML Vision training is 100 image examples per category/label. So it should be B"}]},{"upvote_count":"4","timestamp":"1601599680.0","poster":"Rajokkiyam","content":"Answer A","comment_id":"70295"}],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/17235-exam-professional-data-engineer-topic-1-question-126/","question_id":31,"isMC":true,"answer_ET":"A","topic":"1","unix_timestamp":1584870720,"answer_images":[],"exam_id":11,"question_images":[],"answers_community":["A (57%)","B (31%)","12%"],"choices":{"A":"Use Cloud Vision AutoML with the existing dataset.","D":"Train your own image recognition model leveraging transfer learning techniques.","B":"Use Cloud Vision AutoML, but reduce your dataset twice.","C":"Use Cloud Vision API by providing custom labels as recognition hints."},"timestamp":"2020-03-22 10:52:00","answer":"A","question_text":"You work for a manufacturing company that sources up to 750 different components, each from a different supplier. You've collected a labeled dataset that has on average 1000 examples for each unique component. Your team wants to implement an app to help warehouse workers recognize incoming components based on a photo of the component. You want to implement the first working version of this app (as Proof-Of-Concept) within a few working days. What should you do?"},{"id":"s5cQAU8otxAcITELTq8y","discussion":[{"poster":"dhs227","upvote_count":"69","comments":[{"content":"the link doesn't say TPU does not support custom C++ tensorflow ops","comment_id":"1053168","upvote_count":"1","poster":"ffggrre","timestamp":"1698181560.0","comments":[{"upvote_count":"1","poster":"Helinia","comment_id":"1107933","content":"It does. TPU is good for \"Models with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\".","timestamp":"1703778780.0"}]}],"content":"The correct answer is C\nTPU does not support custom C++ tensorflow ops\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus","timestamp":"1585764000.0","comment_id":"70205"},{"comments":[{"upvote_count":"11","timestamp":"1615611300.0","poster":"gopinath_k","comment_id":"309431","content":"B:\n1. You need to provide support for the matrix multiplication - TPU\n2. You need to provide support for the Custom TF written in C++ - GPU"},{"content":"But, in the question it also says we have to decrease the time significantly?? If you gonna use the CPU, it will take more time to train, right?","timestamp":"1648370640.0","upvote_count":"1","comments":[{"poster":"cetanx","comment_id":"909957","timestamp":"1685426580.0","content":"Chat GPT says C\nOption D is not the most cost-effective or efficient solution. While increasing the size of the cluster could decrease the training time, it would also significantly increase the cost, and CPUs are not as efficient for this type of workload as GPUs.","comments":[{"timestamp":"1692193800.0","upvote_count":"3","comments":[{"comment_id":"1056229","upvote_count":"2","content":"Totally agree. ChatGPT is garbage. It is still learning.","poster":"squishy_fishy","timestamp":"1698499560.0"}],"comment_id":"982643","poster":"FP77","content":"chatgpt will give you different answers if you ask 10 times. The correct answer is B"}],"upvote_count":"1"}],"comment_id":"576078","poster":"tavva_prudhvi"}],"poster":"aiguy","content":"D:\nCloud TPUs are not suited to the following workloads: [...] Neural network workloads that contain custom TensorFlow operations written in C++. Specifically, custom operations in the body of the main training loop are not suitable for TPUs.","comment_id":"70171","upvote_count":"44","timestamp":"1585757100.0"},{"content":"Selected Answer: D\nAccording to the official documentation. Models that contain many custom TensorFlow operations written in C++ should keep using CPUs.","upvote_count":"3","timestamp":"1729789980.0","poster":"SamuelTsch","comment_id":"1302549"},{"timestamp":"1727351400.0","poster":"baimus","content":"I think this is D. I recently did the ML professional exam and they ask that there, and it's always \"c++ custom ops = CPU\", it's in fact the only scenario for non-small models on CPU. It's written in black and white here: https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus, check out the CPU/GPU/TPU \"when to use\" section.","comment_id":"1289411","upvote_count":"3"},{"timestamp":"1717833660.0","poster":"Anudeep58","comment_id":"1226620","content":"Selected Answer: C\nWhy Not Other Options?\nA. Use Cloud TPUs without any additional adjustment to your code:\n\nTPUs are optimized for standard TensorFlow operations and require custom TensorFlow ops to be adapted to TPU-compatible kernels, which is not trivial.\nWithout modifications, your custom C++ ops will not run efficiently on TPUs.\nB. Use Cloud TPUs after implementing GPU kernel support for your customs ops:\n\nImplementing GPU kernel support alone is not sufficient for running on TPUs. TPUs require specific optimizations and adaptations beyond GPU kernels.\nD. Stay on CPUs, and increase the size of the cluster you're training your model on:\n\nWhile increasing the CPU cluster size might reduce training time, it is not as efficient or cost-effective as using GPUs, especially for matrix multiplication tasks.","upvote_count":"1"},{"content":"Selected Answer: C\nC: TPUs are out of the picture due to the custom ops, so the next best option for accelerating matrix operations is using GPU. Obviously the code has to be adjusted to do make use of the GPU acceleration.","timestamp":"1717563960.0","upvote_count":"1","comment_id":"1224523","poster":"AlizCert"},{"content":"CPU : Simple models \nGPU: Custom TensorFlow/PyTorch/JAX operations","poster":"GCP_data_engineer","upvote_count":"1","timestamp":"1716647880.0","comment_id":"1218377"},{"upvote_count":"1","content":"Selected Answer: C\nThe best choice here is C. Use Cloud GPUs after implementing GPU kernel support for your customs ops. Here's why:\n\nCustom Ops & GPUs: Since your model relies heavily on custom C++ TensorFlow ops focused on matrix multiplications, GPUs are the ideal accelerators for this workload. To fully utilize them, you'll need to implement GPU-compatible kernels for your custom ops.\nSpeed and Cost-Efficiency GPUs offer a significant speed improvement for matrix-intensive operations compared to CPUs. They provide a good balance of performance and cost for this scenario.\nTPUs: Limitations Although Cloud TPUs are powerful, they aren't designed for arbitrary custom ops. Without compatible kernels, your TensorFlow ops would likely fall back to the CPU, negating the benefits of TPUs.","comment_id":"1189553","timestamp":"1712265840.0","poster":"CGS22"},{"upvote_count":"2","comment_id":"1159014","timestamp":"1708885380.0","poster":"Preetmehta1234","content":"Selected Answer: C\nTPU:\nModels with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\nLink: https://cloud.google.com/tpu/docs/intro-to-tpu#TPU\n\nSo, A&B eliminated\nCPU is very slow or built for simple operations. So C: GPU"},{"comment_id":"1122041","content":"Selected Answer: C\nto me, it's C","poster":"Matt_108","upvote_count":"1","timestamp":"1705178640.0"},{"comment_id":"1085704","poster":"Kimich","timestamp":"1701492840.0","upvote_count":"3","content":"Requirement 1: Significantly reduce the processing time while keeping costs low. \nRequirement 2: Bulky matrix multiplication takes up to several days.\n\nFirst, eliminate A & D:\nA: Cannot guarantee running on Cloud TPU without modifying the code.\nD: Cannot ensure performance improvement or cost reduction, and additionally, CPUs are not suitable for bulky matrix multiplication.\n\nIf it can be ensured that customization is easily deployable on both Cloud TPU and Cloud GPU,it seems more feasible to first try Cloud GPU.\n\nBecause:\nIt provides a better balance between performance and cost.\nModifying custom C++ on Cloud GPU should be easier than on Cloud TPU, which should also save on manpower costs."},{"content":"Answer D\nI did use Chat GPT and discovered that if you put at the beginning of the question -- \"Do not make assumption about changes to architecture. This is a practice exam question.\" All other answers require changes to the code and architecture.","upvote_count":"1","timestamp":"1700497680.0","poster":"emmylou","comment_id":"1075595"},{"comment_id":"1074047","content":"Selected Answer: B\nI think it should use tensor flow processing unit along with GPU kernel support.","upvote_count":"1","timestamp":"1700316660.0","poster":"DataFrame"},{"content":"Selected Answer: B\nTo use Cloud TPUs, you will need to:\n\nImplement GPU kernel support for your custom TensorFlow ops. This will allow your model to run on both Cloud TPUs and GPUs.","upvote_count":"1","comment_id":"1026527","timestamp":"1696591200.0","poster":"Nirca"},{"content":"Refer https://www.linkedin.com/pulse/cpu-vs-gpu-tpu-when-use-your-machine-learning-models-bhavesh-kapil","timestamp":"1696144620.0","upvote_count":"1","poster":"kumarts","comment_id":"1022071"},{"upvote_count":"1","content":"Answer C\nTPU not for custom C++ but GPU can","comment_id":"980379","timestamp":"1691975460.0","poster":"IrisXia"},{"upvote_count":"3","poster":"KC_go_reply","comment_id":"956270","content":"Selected Answer: C\nA + B: TPU doesn't support custom TensorFlow ops\nThen it says 'decrease training time significantly' and literally 'use accelerator'. Therefore, use GPU -> C, *not* D!","timestamp":"1689745920.0"},{"comment_id":"948478","upvote_count":"5","content":"Selected Answer: C\nD shouldn't be the answer b/c the question statement clearly said you should use accelerators.","poster":"ZZHZZH","timestamp":"1689033900.0"},{"poster":"Qix","content":"Selected Answer: C\nAnswer is C\nUse Cloud GPUs after implementing GPU kernel support for your customs ops.\n\nTPU support Models with no custom TensorFlow operations inside the main training loop so Option-A and B are eliminated as question says that 'These ops are used inside your main training loop'\nNow choices remain 'C' & 'D'. CPU is for Simple models that do not take long to train. Since question says that currently its taking up to several days to train a model and hence existing infra may be CPU and taking so many days. GPUs are for \"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\" as question says that model is dominated by TensorFlow ops leading to correct option as 'C'\n\nReference:\nhttps://cloud.google.com/tpu/docs/tpus\nhttps://www.tensorflow.org/guide/create_op#gpu_kernels","comment_id":"947881","timestamp":"1688975820.0","upvote_count":"4"},{"upvote_count":"5","comment_id":"850821","poster":"lucaluca1982","timestamp":"1679818500.0","content":"Selected Answer: C\nC. Use Cloud GPUs after implementing GPU kernel support for your customs ops.\n\nSince your model relies on custom C++ TensorFlow ops, using Cloud TPUs without any code adjustment (option A) would not be feasible, as TPUs might not support these custom operations. To significantly decrease training time while keeping costs low, you should use Cloud GPUs. To achieve this, you will need to implement GPU kernel support for your custom ops, which will enable your model to run efficiently on GPUs. Once the GPU kernel support is added, you can leverage the power of GPUs on Google Cloud to speed up"},{"upvote_count":"2","timestamp":"1679818020.0","comment_id":"850814","poster":"lucaluca1982","content":"Selected Answer: C\nC GPU support custom option"},{"content":"Selected Answer: B\nThe correct answer is: B. Use Cloud TPUs after implementing GPU kernel support for your customs ops.\n\nThe model is dominated by custom C++ TensorFlow ops, so it will not run on Cloud TPUs or GPUs without modification. Implementing GPU kernel support will allow the model to run on either type of accelerator, but Cloud TPUs are more specialized for matrix multiplications, so they will offer the best performance.\n\nCloud TPUs are also more cost-effective than Cloud GPUs, so they are the best option for reducing the cost of training the model.\n\nStaying on CPUs and increasing the size of the cluster would be the most expensive option, and it would not offer the same performance benefits as using Cloud TPUs.","comment_id":"847973","upvote_count":"3","poster":"midgoo","timestamp":"1679559960.0"},{"upvote_count":"2","comment_id":"844867","poster":"juliobs","timestamp":"1679315940.0","content":"Selected Answer: C\nAnswer C: \"image recognition domain\", \"bulky matrix multiplications\", \"accelerator\", \"several days to train a model\". All screams for GPUs."},{"upvote_count":"1","comment_id":"820850","poster":"musumusu","content":"Answer C:\nWhy not D: its already taking several days on CPUs, cmon, Time for parallel processing and this is GPUs concept. \nWhy not B: However its the fastest approach for tensorflow work specially, but we need to keep the cost lowest. \nOption C will reduce the time and overall cost.","timestamp":"1677265320.0"},{"comments":[{"content":"But GPU is not a step between CPU and TPU in cost and performance? Why not use GPU in this case?","timestamp":"1670491920.0","comment_id":"738835","poster":"odacir","upvote_count":"4"},{"timestamp":"1672458720.0","content":"D is right","comment_id":"762464","upvote_count":"1","poster":"AzureDP900"},{"timestamp":"1687348620.0","content":"And how on earth is a 'CPU' an accelerator? Do you even know what that means?","upvote_count":"1","poster":"KC_go_reply","comment_id":"929441"}],"timestamp":"1670066940.0","comment_id":"734390","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nCloud TPUs are optimized for specific workloads. In some situations, you might want to use GPUs or CPUs on Compute Engine instances to run your machine learning workloads. In general, you can decide what hardware is best for your workload based on the following guidelines:\nCPUs\n- Models that are dominated by custom TensorFlow operations written in C++","poster":"zellck","upvote_count":"8"},{"content":"CPUs\n-Quick prototyping that requires maximum flexibility\n-Simple models that do not take long to train\n-Small models with small effective batch sizes\n-Models that are dominated by custom TensorFlow operations written in C++\n-Models that are limited by available I/O or the networking bandwidth of the host system\nGPUs\n-Models for which source does not exist or is too onerous to change\n-Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\n-Models with TensorFlow ops that are not available on Cloud TPU (see the list of available TensorFlow ops)\n-Medium-to-large models with larger effective batch sizes\nTPUs\n-Models dominated by matrix computations\n-Models with no custom TensorFlow operations inside the main training loop\n-Models that train for weeks or months\n-Larger and very large models with very large effective batch sizes","timestamp":"1666701780.0","comments":[{"comment_id":"707882","upvote_count":"3","content":"Hi. I just did the exam. A similar question came up. It said that the model partially runs on custom TensorFlow ops. They need to speed up the time it takes for the model to complete its run. What type of accelerators to use etc. I chose GPUs as supported above. Hope this helps.","timestamp":"1667142660.0","poster":"Azlijaffar","comments":[{"upvote_count":"1","content":"That's a different question in this series. This is a diffferent question/scenario","poster":"Prakzz","timestamp":"1671765120.0","comment_id":"753820"}]},{"comment_id":"703863","timestamp":"1666701840.0","upvote_count":"2","content":"-Use TPUs if model takes weeks or months. This model only takes days. Since model is DOMINATED by custom TF ops in C++, can stick with CPUs. Ans should be D.","poster":"Azlijaffar"}],"upvote_count":"9","poster":"Azlijaffar","comment_id":"703861"},{"comment_id":"650592","timestamp":"1661231640.0","poster":"AmirNaik204","content":"Vote for C. GPU as the question says \"performing bulky matrix multiplications\" and \"low cost\"","upvote_count":"3"},{"timestamp":"1658130420.0","upvote_count":"3","poster":"Pime13","content":"Selected Answer: D\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus","comment_id":"632917"},{"poster":"VGalan","comment_id":"601477","upvote_count":"2","timestamp":"1652514420.0","content":"Selected Answer: D\nhttps://cloud.google.com/tpu/docs/tpus\n\nCPUs\n Models that are dominated by custom TensorFlow operations written in C++\nCloud TPUs are not suited to the following workloads:\n Neural network workloads that contain custom TensorFlow operations written in C++. \n Specifically, custom operations in the body of the main training loop are not suitable for \n TPUs."},{"upvote_count":"1","comments":[{"upvote_count":"2","timestamp":"1649694360.0","content":"Which documentation, please share the reference.","comment_id":"584310","poster":"tavva_prudhvi"}],"content":"Selected Answer: D\nD according to documentation","poster":"wences","comment_id":"578649","timestamp":"1648699200.0"},{"timestamp":"1648214460.0","upvote_count":"1","poster":"PJG_worm","comment_id":"575038","content":"B: TPU: performing bulky matrix multiplications\nGPU: support CPP"},{"upvote_count":"1","comment_id":"558737","poster":"Aslkdup","content":"Answer is B.\nTPUs are optimized to perform fast, bulky matrix multiplication and developed model is using bulky matrix multiplication.","timestamp":"1646139300.0"},{"poster":"AM1O593","comment_id":"550076","content":"Selected Answer: D\nD: CPU is recommended for models that use custom TensorFlow operations written in C++ [Dan Sullivan]","upvote_count":"5","timestamp":"1645168140.0"},{"content":"D. \n\"Cloud TPUs are not suited to Neural network workloads that contain custom TensorFlow operations written in C++. Specifically, custom operations in the body of the main training loop are not suitable for TPUs.\"\nbecause TPUs are not suited for this application.\n\nThe battle between GPU vs CPU.\nSpecially mentioned that the C++ part in the description. Since we can go with CPU.","poster":"Bhawantha","upvote_count":"3","timestamp":"1642441860.0","comment_id":"525978"},{"poster":"eagle_fang","comment_id":"524411","timestamp":"1642278000.0","content":"D \nhttps://cloud.google.com/tpu/docs/tpus","upvote_count":"1"},{"comment_id":"520269","upvote_count":"8","content":"Selected Answer: D\nD:\nhttps://cloud.google.com/tpu/docs/tpus\nCloud TPUs are optimized for specific workloads. In some situations, you might want to use GPUs or CPUs on Compute Engine instances to run your machine learning workloads. In general, you can decide what hardware is best for your workload based on the following guidelines:","poster":"MaxNRG","timestamp":"1641740760.0","comments":[{"upvote_count":"4","content":"CPUs:\n• Quick prototyping that requires maximum flexibility\n• Simple models that do not take long to train\n• Small models with small effective batch sizes\n• Models that are dominated by custom TensorFlow operations written in C++\n• Models that are limited by available I/O or the networking bandwidth of the host system","comment_id":"520270","timestamp":"1641740760.0","poster":"MaxNRG","comments":[{"content":"GPUs:\n• Models that are not written in TensorFlow or cannot be written in TensorFlow\n• Models for which source does not exist or is too onerous to change\n• Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\n• Models with TensorFlow ops that are not available on Cloud TPU (see the list of available TensorFlow ops)\n• Medium-to-large models with larger effective batch sizes","comment_id":"520271","timestamp":"1641740820.0","poster":"MaxNRG","upvote_count":"3","comments":[{"timestamp":"1641740820.0","comment_id":"520272","upvote_count":"3","poster":"MaxNRG","content":"TPUs:\n• Models dominated by matrix computations\n• Models with no custom TensorFlow operations inside the main training loop\n• Models that train for weeks or months\n• Larger and very large models with very large effective batch sizes\nTPUs are optimized to perform fast, bulky matrix multiplication.\nCloud TPUs are not suited to Neural network workloads that contain custom TensorFlow operations written in C++. Specifically, custom operations in the body of the main training loop are not suitable for TPUs.\nhttps://cloud.google.com/tpu/docs/tpus"}]}]}]},{"upvote_count":"2","timestamp":"1641644640.0","content":"Selected Answer: D\nOnly D -> https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus","poster":"medeis_jar","comment_id":"519509"},{"content":"Selected Answer: B\nConfused between B and D. Refer to the below link\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nCustom Tensorflow operations - CPUs\nBulky Matrix calc - TPUs","upvote_count":"3","comment_id":"506298","timestamp":"1640106780.0","poster":"kishanu"},{"upvote_count":"4","content":"Selected Answer: C\nI vote C, to know why see: https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\n- CPUs\n -- Quick prototyping that requires maximum flexibility\n -- Simple models that do not take long to train\n\n- GPUs\n -- Models for which source does not exist or is too onerous to change\n -- Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\n\n- TPUs\n -- Models with no custom TensorFlow operations inside the main training loop","poster":"StefanoG","comments":[{"timestamp":"1648370820.0","comment_id":"576079","upvote_count":"1","content":"Just copy pasting the options doesn't mean it's C, as in the same link there;'s also a statement saying about custom Tensorflow operations written in C++ in CPU section. Pleas explain about that!","poster":"tavva_prudhvi"}],"comment_id":"491531","timestamp":"1638353820.0"},{"comment_id":"455060","timestamp":"1633023720.0","poster":"Chelseajcole","comments":[{"upvote_count":"1","timestamp":"1634481600.0","poster":"squishy_fishy","content":"The answer is C. \nhttps://www.tensorflow.org/guide/create_op","comment_id":"463573"}],"content":"It could only be D. https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nModels that are dominated by custom TensorFlow operations written in C++","upvote_count":"4"},{"upvote_count":"3","poster":"Ysance_AGS","comment_id":"453912","timestamp":"1632900420.0","content":"C seem to be correct => TPU doesn't work on custom Tensorflow C++ so Not A or B,\nD will take more time"},{"poster":"bruno1942","timestamp":"1629567660.0","comment_id":"428860","upvote_count":"5","content":"Cpu are recommended for the following: Models that heavivly use custom TensorFlow operation witten in c++. Page 233 Dan Sullivan."},{"comment_id":"420389","poster":"EricM22","timestamp":"1628180700.0","content":"Should Be C:\nwe have a significant number of custom TensorFlow operations in main training loop","upvote_count":"2"},{"poster":"sumanshu","comment_id":"397534","timestamp":"1625312280.0","comments":[{"poster":"sumanshu","comments":[{"comments":[{"comment_id":"418873","timestamp":"1627927020.0","upvote_count":"3","content":"Did you pass this exam?","poster":"GCP_Guru"}],"content":"Selected - D","timestamp":"1625312520.0","poster":"sumanshu","comment_id":"397542","upvote_count":"4"}],"comment_id":"397538","upvote_count":"2","content":"Could be 'D' as well because we are increasing cluster and CPU suggested for Models that are dominated by custom TensorFlow operations written in C++","timestamp":"1625312460.0"}],"content":"vote for C\n\nA - wrong\nB - elimnated (cost will be high for TPU also TPU suggested for Models with no custom TensorFlow operations inside the main training loop), but here custome ops are inside main training loop.\nC - correct (time decreases on GPU and cost also not very high)\nD - eleminated ( it wil take more time for calculations and we need to decrease time)","upvote_count":"5"},{"upvote_count":"4","comment_id":"352993","content":"C \nkeep the cost low by using an accelerator on Google Cloud\nTPU - Eliminated \nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nModels with a significant number of custom TensorFlow operations that must run at least partially on CPUs","poster":"userd83","timestamp":"1620560820.0"},{"upvote_count":"2","poster":"Sumanth09","timestamp":"1617193380.0","comment_id":"325066","content":"Should be : A\n(From Dan Sullivan)\nGraphic processing units are accelerators that have multiple\narithmetic logic units (ALUs) that implement adders and multipliers. This architecture is\nwell suited to workloads that benefit from massive parallelization, such as training deep\nlearning models. GPUs and CPUs are both subject to the von Neumann bottleneck, which\nis the limited data rate between a processor and memory, and slow processing. TPUs are\nspecialized accelerators based on ASICs and created by Google to improve training of deep\nneural networks. These accelerators are designed for the TensorFlow framework. TPUs\nreduces the impact of the von Neumann bottleneck by implementing matrix multiplication\nin the processor."},{"timestamp":"1614789420.0","comment_id":"302748","upvote_count":"5","content":"D\n\nCPUs: Models that are dominated by custom TensorFlow operations written in C++\n\nsource: https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus","poster":"gcper"},{"content":"B:\nhttps://cloud.google.com/tpu/docs/tpus","upvote_count":"1","timestamp":"1613684640.0","poster":"daghayeghi","comment_id":"293748"},{"poster":"bobby8521","comment_id":"287602","content":"I will got with C.\nTPUs- anyway its not supported with custom Tensorflow Operations\nGPUs- Can support\nCPU's - Will support\nConsidering it is taking serval days to run, best way is to increase to GPU \n\nhttps://cloud.google.com/tpu/docs/tpus\nhttps://www.tensorflow.org/guide/create_op#gpu_kernels","timestamp":"1612968840.0","upvote_count":"3"},{"poster":"AnilKr","timestamp":"1610944620.0","upvote_count":"3","comment_id":"270023","content":"Correct-C\nTPU support Models with no custom TensorFlow operations inside the main training loop so Option-A and B are eliminated as question says that 'These ops are used inside your main training loop'\nNow choices remain 'C' & 'D'. CPU is for Simple models that do not take long to train. Since question says that currently its taking up to several days to train a model and hence existing infra may be CPU and taking so many days. GPUs are for \"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\" as question says that model is dominated by TensorFlow ops leading to correct option as 'C'"},{"comments":[{"timestamp":"1640342100.0","poster":"Jlozano","content":"Also: \"Simple models that do not take long to train\"","comment_id":"508427","upvote_count":"1"}],"poster":"VM_GCP","content":"https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nModels that are dominated by custom TensorFlow operations written in C++ -> CPU \nso answer is D","upvote_count":"1","comment_id":"239254","timestamp":"1607522580.0"},{"timestamp":"1607467500.0","content":"https://cloud.google.com/tpu/docs/tpus D it is","upvote_count":"1","poster":"beedle","comment_id":"238743"},{"upvote_count":"4","poster":"federicohi","content":"C i think. D doesnt becuase its cheaper but its asking to reduce much time and CPU is only for simple models","timestamp":"1605874740.0","comment_id":"223567"},{"content":"Should be C as there is no documentation on GPU kernels working with TPUs. I can see that B might work as you can set up a custom pip package for the C++ code, but this would be much easier if it was Cloud GPU with GPU kernel. Here is the doc on creating custom ops: https://www.tensorflow.org/guide/create_op#compiling_the_kernel_for_the_gpu_device\nThe question also adds bulky matrix computations which TPU is great for...so IDK. Tricky question.","comment_id":"199600","timestamp":"1602659640.0","upvote_count":"2","poster":"aleedrew"},{"comment_id":"190350","poster":"Twinkletoes","timestamp":"1601470560.0","upvote_count":"4","content":"It should be D...For operations written in C++ CPUs are recommended by Google in their Official Guide\n```\nCPUs are recommended for the following:\n■■ Prototyping\n■■ Simple models that train relatively quickly\n■■ Models that heavily use custom TensorFlow operations written in C++\n■■ Models that are limited by available I/O or network bandwidth of the host server\n```"},{"upvote_count":"1","timestamp":"1600860180.0","poster":"SteelWarrior","content":"Should be C as TensorFlow operations with C++ custom code are not supported with TPUs.","comment_id":"185215"},{"upvote_count":"3","poster":"shashankraj","comment_id":"181462","content":"Definitely C:\nKeep the cost Low and using an accelerator, considering these 2 main requirements","timestamp":"1600414020.0"},{"poster":"rajnishd","timestamp":"1599153120.0","comment_id":"172818","upvote_count":"2","content":"TPUs are optimized to perform fast, bulky matrix multiplication, Models that train for weeks or months , I will go for A"},{"timestamp":"1598035680.0","content":"I'm not sure , but I prefer the D.","poster":"haroldbenites","comment_id":"163131","upvote_count":"3"},{"upvote_count":"4","comments":[{"upvote_count":"2","comment_id":"157691","timestamp":"1597364040.0","content":"also, you need hardware accelerator as per question. GPUs and TPUs (and not CPUs) are accelerators AFAIK","poster":"FARR"}],"timestamp":"1597363740.0","poster":"FARR","comment_id":"157683","content":"C\nStaying on CPU may/may not reduce time\nGPU could be the answer as \"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\""},{"comment_id":"148316","timestamp":"1596248280.0","upvote_count":"1","content":"answer is D, \"Model that heavily use custome Tensor flow operation written in C++\"","poster":"Archy"},{"poster":"norwayping","upvote_count":"3","timestamp":"1593354000.0","comment_id":"121914","content":"C\nhttps://stackoverflow.com/questions/50799510/how-to-run-custom-gpu-tensorflowop-from-c-code"},{"poster":"AJKumar","upvote_count":"2","comment_id":"116076","timestamp":"1592806860.0","content":"C eliminated right away, A and B are both Cloud TPU's-which are not supported for C++. Answer D."},{"upvote_count":"6","comment_id":"80086","poster":"arnabbis4u","timestamp":"1587947520.0","content":"It should be D. Models dominated by Custom Tensorflow operations should be trained using CPUs.\nDescription : It cannot be B, because TPU does not support Custom Tensorflow operations."},{"content":"Answer D","timestamp":"1585789140.0","comments":[{"timestamp":"1593894480.0","comment_id":"126383","comments":[{"comments":[{"comment_id":"223106","content":"GPU kernel can be implemented as well: https://www.tensorflow.org/guide/create_op#gpu_kernels","poster":"snamburi3","timestamp":"1605822780.0","upvote_count":"2"}],"content":"I agree. also the emphasis is on accelerator, so maybe C","comment_id":"223105","timestamp":"1605822540.0","poster":"snamburi3","upvote_count":"2"}],"upvote_count":"3","poster":"droogie","content":"D is NOT the correct answer \"You want to decrease this time significantly and keep the cost low by using an accelerator on Google Cloud\""}],"comment_id":"70297","upvote_count":"4","poster":"Rajokkiyam"},{"content":"Should be B\nhttps://cloud.google.com/tpu/docs/tpus?hl=zh-tw#when_to_use_tpus","comment_id":"66893","timestamp":"1584871260.0","upvote_count":"3","comments":[{"upvote_count":"2","timestamp":"1584871320.0","comment_id":"66894","poster":"[Removed]","content":"https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus"}],"poster":"[Removed]"}],"unix_timestamp":1584871260,"topic":"1","question_id":32,"answer_ET":"D","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/17236-exam-professional-data-engineer-topic-1-question-127/","choices":{"B":"Use Cloud TPUs after implementing GPU kernel support for your customs ops.","D":"Stay on CPUs, and increase the size of the cluster you're training your model on.","A":"Use Cloud TPUs without any additional adjustment to your code.","C":"Use Cloud GPUs after implementing GPU kernel support for your customs ops."},"answer_images":[],"exam_id":11,"answer":"D","question_images":[],"timestamp":"2020-03-22 11:01:00","answers_community":["D (45%)","C (44%)","11%"],"question_text":"You are working on a niche product in the image recognition domain. Your team has developed a model that is dominated by custom C++ TensorFlow ops your team has implemented. These ops are used inside your main training loop and are performing bulky matrix multiplications. It currently takes up to several days to train a model. You want to decrease this time significantly and keep the cost low by using an accelerator on Google Cloud. What should you do?","answer_description":""},{"id":"8CLVoDBW5yCEgDyklk7K","url":"https://www.examtopics.com/discussions/google/view/17238-exam-professional-data-engineer-topic-1-question-128/","question_text":"You work on a regression problem in a natural language processing domain, and you have 100M labeled examples in your dataset. You have randomly shuffled your data and split your dataset into train and test samples (in a 90/10 ratio). After you trained the neural network and evaluated your model on a test set, you discover that the root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set. How should you improve the performance of your model?","unix_timestamp":1584871860,"choices":{"A":"Increase the share of the test sample in the train-test split.","C":"Try out regularization techniques (e.g., dropout of batch normalization) to avoid overfitting.","D":"Increase the complexity of your model by, e.g., introducing an additional layer or increase sizing the size of vocabularies or n-grams used.","B":"Try to collect more data and increase the size of your dataset."},"answer_ET":"D","question_id":33,"question_images":[],"timestamp":"2020-03-22 11:11:00","topic":"1","exam_id":11,"answers_community":["D (63%)","C (35%)","3%"],"discussion":[{"timestamp":"1592655120.0","poster":"Callumr","content":"This is a case of underfitting - not overfitting (for over fitting the model will have extremely low training error but a high testing error) - so we need to make the model more complex - answer is D","upvote_count":"72","comment_id":"114761","comments":[{"upvote_count":"4","content":"@callumr , \"root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set.\" clearly means testing error is twice of training error. So, it is clearly overfitting. Isn't it?","comment_id":"455853","timestamp":"1633149900.0","comments":[{"timestamp":"1633909920.0","upvote_count":"1","comment_id":"460260","comments":[{"upvote_count":"2","poster":"tavva_prudhvi","comments":[{"content":"It's overfitting.\n\nOverfitting->low rmse in train / high accuracy-f1 score in train for classification.\n\nUnderfitting -> high rmse / low f1score or accuracy in train, you don't have to look into test set if there is an underfitting problem.","comment_id":"648517","upvote_count":"1","comments":[{"comment_id":"929114","upvote_count":"1","timestamp":"1687328640.0","poster":"jfab","content":"But the question clearly states we have higher RMSE on the train than the test. So how would it be overfitting?"}],"poster":"alecuba16","timestamp":"1660839480.0"},{"content":"But in this scenario we'd have training RMSE = 0.4 & testing RMSE = 0.2 - you've not read the question properly","comment_id":"929118","upvote_count":"2","poster":"jfab","timestamp":"1687329300.0"}],"comment_id":"584308","content":"If you training RMSE=0.2. and testing RMSE = 0.4, and we want the RMSE to be low as its the error, now is it overfitting or underfitting? think wisely!","timestamp":"1649694240.0"}],"content":"So, answer should be C","poster":"hellofrnds"},{"comments":[{"comment_id":"477452","timestamp":"1636808040.0","poster":"velliger","content":"*underfitting","upvote_count":"2"}],"comment_id":"477451","upvote_count":"1","content":"High rmse: The model is underfitting the train data. To reduce overfitting, we increase the number of layers in the model or we change the type of layer.","poster":"velliger","timestamp":"1636807980.0"},{"content":"NO, its underfitting.","upvote_count":"3","poster":"odacir","timestamp":"1670492820.0","comment_id":"738853"},{"poster":"jfab","comment_id":"929120","content":"\"Twice as high on the train\". So clearly means TRAINING error is twice as high vs testing. So underfitting","upvote_count":"3","timestamp":"1687329480.0"}],"poster":"hellofrnds"},{"timestamp":"1691248260.0","comment_id":"973119","comments":[{"upvote_count":"2","timestamp":"1695198120.0","poster":"ckanaar","content":"Wrong! This scenario indicates a case of underfitting. The RSME is twice as high on the training dataset compared to the test dataset, so the model is underfitting.","comment_id":"1012032"}],"content":"Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data.","poster":"NeoNitin","upvote_count":"1"}]},{"comments":[],"poster":"[Removed]","timestamp":"1584871860.0","content":"should be D","upvote_count":"20","comment_id":"66896"},{"poster":"samtestking","upvote_count":"1","content":"Selected Answer: B\nCould be B (data requirement for task is vague), but let's assume 100 million data points is enough and rule that out.\n\nIndication of overfitting is significantly better performance on training data compared to unseen data. Here we are told that the unseen data is performing significantly better which is the opposite of what we should see if it were overfitting. Rule out C.\n\nSymptoms of model underfitting is poor performance in BOTH training AND unseen data. While underfitting might be the issue, the more pressing concern is that the test set is clearly not representative of the overall data and could be skewed. This is further supported by the 90/10 split (academic/industry standard is 80/20 or 75/25 based on the Pareto principle: https://en.wikipedia.org/wiki/Pareto_principle). A 90/10 split would be useful if we were doing k-fold cross validation (https://machinelearningmastery.com/k-fold-cross-validation/), however there is no indication of such in the prompt.\n\nNote: The question does not explicitly say that the model is performing poorly/errors are significantly bad, just that the error is twice as high in the training set (they could both have low error values).\nSo whilst it could be a case of underfitting (D), the first step taken should be addressing the obviously problematic data representation by adjusting the train-test split (option A).","timestamp":"1735832760.0","comment_id":"1335644"},{"content":"Selected Answer: D\nIt is underfitting problem, which means that the used models is too easy.","comment_id":"1302551","poster":"SamuelTsch","upvote_count":"2","timestamp":"1729790220.0"},{"upvote_count":"1","comment_id":"1289423","timestamp":"1727352540.0","content":"This is A. The key is that 90/10 is a weirdly small test set, that stood out to me straight away (I work professionally as a machine learning engineer and have the cert). Next tip, that everyone seems to be ignoring - this is not underfit OR overfit. The model outperforms on the TEST set, this is not a miswording. Test scores higher than train. The time you might expect to see this is if your test set is too small to be a representative sample, leading to unrepresentative results. Seeing as the question already set up this conclusion with the 90/10 thing, it's definitely A. None of the others (or indeed anything else) can address Test outperforming Train, and the conclusion of others below that this is due to a poorly worded question is a bizarre conclusion.","poster":"baimus"},{"timestamp":"1708008540.0","upvote_count":"2","comment_id":"1151063","content":"Selected Answer: D\nUnderfitting scenario","poster":"cuadradobertolinisebastiancami"},{"comment_id":"1120858","upvote_count":"2","content":"Selected Answer: D\nIt is an underfitting situation - D","timestamp":"1705070940.0","poster":"Sofiia98"},{"content":"Selected Answer: C\nShould be C\nC. Try out regularization techniques (e.g., dropout or batch normalization) to avoid overfitting:\n\nThis is a reasonable approach. Regularization techniques can help prevent overfitting, especially when the model shows a significantly higher error on the training set compared to the test set.\nD. Increase the complexity of your model (e.g., introducing an additional layer or increasing the size of vocabularies or n-grams):\n\nThis could potentially exacerbate the overfitting issue. Increasing model complexity without addressing overfitting concerns may lead to poor generalization on new data.","upvote_count":"2","comment_id":"1083273","comments":[{"comment_id":"1085691","upvote_count":"1","content":"https://dooinnkim.medium.com/what-are-overfitting-and-underfitting-855d5952c0b6","poster":"Kimich","timestamp":"1701490200.0"}],"poster":"Kimich","timestamp":"1701246600.0"},{"poster":"hallo","upvote_count":"3","comment_id":"1076541","timestamp":"1700591220.0","content":"Are the questions in this relevant for the new exam or are these all now outdated?"},{"content":"https://stats.stackexchange.com/questions/497050/how-big-a-difference-for-test-train-rmse-is-considered-as-overfit#:~:text=RMSE%20of%20test%20%3C%20RMSE%20of,is%20always%20overfit%20or%20underfit.\nRMSE of test > RMSE of train => OVER FITTING of the data.\nRMSE of test < RMSE of train => UNDER FITTING of the data.\nso for answer is D","timestamp":"1700515980.0","comment_id":"1075856","upvote_count":"1","poster":"pss111423"},{"timestamp":"1699533900.0","upvote_count":"1","poster":"steghe","comment_id":"1066414","content":"Underfitting models: In general High Train RMSE, High Test RMSE.\nOverfitting models: In general Low Train RMSE, High Test RMSE.\n\nhttps://daviddalpiaz.github.io/r4sl/regression-for-statistical-learning.html"},{"timestamp":"1696001580.0","content":"I passed the exam today. I am pretty sure it is overfitting. Answer must be c","upvote_count":"2","poster":"ha1p","comment_id":"1020904"},{"content":"Selected Answer: D\nRMSE is more on training. That means, model is not performing well on training dataset but performing well on testing dataset. This happens in the case of underfitting. So D.","upvote_count":"3","poster":"MULTITASKER","timestamp":"1695410340.0","comment_id":"1014429"},{"timestamp":"1694451360.0","poster":"[Removed]","content":"Selected Answer: D\nRMSE training = 2 x testing\nWhen training > testing, it is a case of underfitting\nHence D","comment_id":"1005007","upvote_count":"2"},{"content":"chatGPT says option C","upvote_count":"1","timestamp":"1693975380.0","poster":"pulse008","comment_id":"1000173"},{"content":"Selected Answer: D\n\"root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set.\" means the RMSE of training set is two time of RMSE of test set, which indicates the training is not as good as test, then underfiting, so D.","timestamp":"1693490520.0","upvote_count":"2","poster":"stonefl","comment_id":"995170"},{"upvote_count":"1","poster":"NeoNitin","content":"Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data.\n\nSo with dropout method we can overcome the overfitting so C is correct","comment_id":"973117","timestamp":"1691248200.0"},{"content":"Selected Answer: D\nunderfitting","comment_id":"946529","comments":[],"timestamp":"1688824320.0","poster":"MoeHaydar","upvote_count":"2"},{"upvote_count":"1","timestamp":"1688633400.0","comment_id":"944469","content":"Selected Answer: C\nC sounds like a valid answer.","poster":"neerajRathi"},{"upvote_count":"1","poster":"blathul","timestamp":"1687376040.0","content":"Selected Answer: C\nIf the root-mean-squared error (RMSE) of a model is twice as high on the training set compared to the test set, it suggests that the model is overfitting.\n\nOverfitting occurs when a machine learning model becomes too specialized to the training data and fails to generalize well to new, unseen data. In this case, it means that the model is performing better on the test set, which represents new data, compared to the training set it was initially trained on.\n\nThe fact that the RMSE is higher on the training set implies that the model is struggling to accurately predict the training data points. This could be due to the model learning intricate details and noise from the training data, to the extent that it cannot generalize well to new examples.\n\nTo address this issue, you may consider employing regularization techniques or adjusting hyperparameters to reduce overfitting.","comment_id":"929855"},{"comment_id":"929446","timestamp":"1687348860.0","upvote_count":"1","poster":"KC_go_reply","comments":[],"content":"Selected Answer: D\nOverfitting = high performance on train split, low performance on test split\nIn this case, we have the opposite of that. Therefore, the model is actually underfit, and the performance on the test split is probably just coincidence."},{"comment_id":"917312","content":"Selected Answer: D\nUnderfitting","timestamp":"1686149280.0","upvote_count":"1","poster":"vaga1","comments":[{"upvote_count":"1","poster":"NeoNitin","content":"Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data.","comment_id":"973123","timestamp":"1691248320.0"}]},{"comment_id":"908706","upvote_count":"1","content":"Selected Answer: C\nAnswer C","poster":"dataengineeruser34","timestamp":"1685286420.0"},{"timestamp":"1685004600.0","poster":"shabfat","upvote_count":"2","content":"Selected Answer: C\nTraining error is small and test error is big\" is an indication of overfitting.","comment_id":"906531"},{"timestamp":"1683095580.0","poster":"Oleksandr0501","upvote_count":"1","comments":[],"content":"it is D\nTraining dataset has many errors (RMSE twice as high). Test set - RMSE twice low (more correct). It tells about underfitting.\nSo, we need D. Increase the complexity of your model by, e.g., introducing an additional layer or increase sizing the size of vocabularies or n-grams used.\n\nOthers, who chose D, described it well. Read them also, and checked in internet how to differ overfitting and underf.","comment_id":"888226"},{"upvote_count":"2","content":"The fact that the RMSE of the model is twice as high on the train set as on the test set suggests that the model is overfitting on the training data. This means that the model is too complex and is fitting the noise in the training data, rather than the underlying patterns in the data. To improve the performance of the model, we need to reduce overfitting.\n\nTherefore, the correct option is C","poster":"muhusman","comment_id":"867189","timestamp":"1681208940.0"},{"poster":"midgoo","upvote_count":"6","comment_id":"847990","content":"Selected Answer: C\nI have checked and verified that the answer is C. There is a trick here about overfit and underfit. Depends on how high the RMSE of training set comparing to test set, we may have different meaning.\n\nIf the RMSE is higher on the training set than on the test set, but only by a small amount, this usually means that the model is UNDERFITTING the data. This means that the model is not complex enough to learn the underlying patterns in the data. To improve the performance of the model, you can try to increase the complexity of the model, such as by adding more layers or neurons.\n\nWhen the RMSE is much higher on the training set than on the test set, it suggests that the model has OVERFITTED the data. An overfitted model has learned the noise from the training data, and it will not perform well on new data. To improve the performance of the model, you can try to decrease the complexity of the model, such as by removing some of the layers or neurons.","timestamp":"1679561700.0","comments":[{"content":"No. When the RMSE is much higher on the training set, it suggests exactly the opposite of what you said, since it means that the model is not capable of fully understand the training data. You have overfit when the RMSE is significantly lower on the training data than the test data, which is not our case. We can therefore boost model performance by making it more complex - answer D has this purpose.","upvote_count":"1","poster":"tronunator","comment_id":"879313","timestamp":"1682338980.0"}]},{"timestamp":"1676639760.0","comments":[],"upvote_count":"3","poster":"musumusu","comment_id":"811895","content":"Answer D\nRMSE shows the variation from actual value and predictive value. If it is higher on RMSE, then you should definitely reject the model. In this case it is saying, RMSE is higher in training and lower in test. Which means model could be better trained and the guy was sleeping doing this work. \nSo, it means, it released the underfit model or poor model in simple term. So what you should do, to reduce RMSE, you increase complexity of model by adding more Neurons and Hidden layers. \n\nIf it was overfit, means very high RMSE in test while showing almost 0 during training, it means it was overfit. \nSo how you fix overfit, Feature Engineering, Regularisation, Hypertunning, dropout (reduce feature) .. bla bla bla"},{"content":"Obviously the question meant to say \"error twice as high on the test set as on the train set\", thus overfitting.","poster":"jkh_goh","timestamp":"1674548460.0","comment_id":"786312","upvote_count":"1"},{"content":"This will help in understanding overfitting vs. underfitting\nhttps://daviddalpiaz.github.io/r4sl/regression-for-statistical-learning.html","poster":"desertlotus1211","upvote_count":"1","comment_id":"786020","timestamp":"1674522120.0"},{"poster":"Astrophile","timestamp":"1672565940.0","comment_id":"763129","upvote_count":"1","content":"Selected Answer: C\nThere are 100M labeled examples in the dataset and the train and test ratio is 90:10, hence the model is capturing all the noises with a pattern in the data. It is sufficient enough to indicate the overfitting the model as \"Training error is small and test error is big\" ."},{"poster":"Azlijaffar","comment_id":"703881","comments":[{"comment_id":"703883","poster":"Azlijaffar","content":"Answer should be D.","timestamp":"1666702800.0","upvote_count":"1"},{"upvote_count":"1","content":"Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data.","timestamp":"1691248380.0","comment_id":"973128","poster":"NeoNitin"}],"content":"The wording is tricky. \"RMSE twice as high on train set as on test set\" means Train RMSE is higher than Test RMSE. Means train data has more errors than test data. More errors = underfitting. \nFYI: i have twice as many apples as you means i have more apples than you. hahaha.","upvote_count":"3","timestamp":"1666702740.0"},{"upvote_count":"2","comments":[{"poster":"jkhong","upvote_count":"1","timestamp":"1666011180.0","content":"Many cases, it can just be a coincidence that your test data yields a better performance than your train, where your underfitted training data is just the right fit for this specific test data","comment_id":"697410"}],"content":"I don't get how in an underfit context you can have twice the RMSE on the train set (meaning twice less on the test set ==> You have doubled performance on your testing set ?) The question seems very odd to me.","poster":"YorelNation","timestamp":"1662369780.0","comment_id":"659988"},{"upvote_count":"2","comment_id":"601499","timestamp":"1652518200.0","content":"Selected Answer: D\nThe lower the RMSE, the better a given model is able to “fit” a dataset \n\nOverfitting: Model perform well on train set and poorly on test set. In this case, model performs better in test set. Cant not be Overfitting.\nThe options B and C are for overfitting cases. \n\nWe need to improve the train perform so in my opinion, D is the correct answer","poster":"VGalan"},{"timestamp":"1641740940.0","poster":"MaxNRG","content":"Selected Answer: D\nD:\nA is incorrect since test sample is large enough.\nB is incorrect since dataset is pretty large already, and having more data typically helps with overfitting and not with underfitting.\nC is incorrect since regularization helps to avoid overfitting and we have a clear underfitting case.\nD is correct since increasing model complexity generally helps when you have an underfitting problem.","upvote_count":"6","comment_id":"520273","comments":[{"timestamp":"1641740940.0","poster":"MaxNRG","comment_id":"520274","content":"https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting\nhttps://towardsdatascience.com/deep-learning-3-more-on-cnns-handling-overfitting-2bd5d99abe5d\nhttps://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization\nhttps://developers.google.com/machine-learning/crash-course/training-neural-networks/best-practices","upvote_count":"1"}]},{"content":"Can you add the link to say is underfit or overfit. I understand that is overfit because in the training set there are not significant error but there are in the testing set. That means that there are noise in the training data set.","timestamp":"1638370920.0","comment_id":"491752","upvote_count":"1","poster":"Pupina"},{"upvote_count":"3","timestamp":"1637640960.0","poster":"lifebegins","content":"\"Training error is small and test error is big\" is an indication of overfitting. Answer is C:\n\nDont add more complexity.","comment_id":"484738"},{"content":"RMSE of test > RMSE of train => OVER FITTING of the data.\nRMSE of test < RMSE of train => UNDER FITTING of the data.","upvote_count":"6","timestamp":"1637640540.0","poster":"lifebegins","comment_id":"484736"},{"upvote_count":"6","timestamp":"1625314260.0","comment_id":"397557","poster":"sumanshu","content":"Vote for D"},{"content":"question says 100M labels and saying high error on testing data - if it's Underfitting model, model might get high error on train data.\n1) Testing data might not representing the training data. if it is the case, need to add more data \n2) Error might be due to more labels. if it is the case, need to apply regularization\nAnswer might be B or C","timestamp":"1617194220.0","comment_id":"325077","upvote_count":"3","poster":"Sumanth09"},{"timestamp":"1606023240.0","poster":"arghya13","content":"Problem of underfitting..D is the correct answer","comment_id":"224763","upvote_count":"2"},{"upvote_count":"8","poster":"snamburi3","timestamp":"1605823380.0","content":"D. this question is in the sample exam and the correct answer shows as D.","comment_id":"223111"},{"content":"Underfitting does not explain the higher test score. If the model is not complex enough it will also perform worse on the test data. A seems the only correct answer explaining the error differences between the sets.","timestamp":"1604760300.0","poster":"JulesT","upvote_count":"4","comments":[{"content":"Agreed","upvote_count":"1","timestamp":"1672461900.0","comment_id":"762465","poster":"AzureDP900"}],"comment_id":"214682"},{"content":"should be D","poster":"Athanasia","comment_id":"205966","upvote_count":"2","timestamp":"1603678680.0"},{"upvote_count":"2","timestamp":"1600924080.0","comment_id":"185881","content":"D is correct, as this is the case of underfitting.","poster":"SteelWarrior"},{"poster":"VIncent9261111","content":"C \nIf the RMSE for the test set is much higher than that of the training set, it is likely that you've badly over fit the data","timestamp":"1600314600.0","comment_id":"180663","upvote_count":"3"},{"comment_id":"180662","timestamp":"1600313880.0","poster":"Tanmoyk","content":"This is a question from Google practice exam and according to Google it's D","upvote_count":"5"},{"upvote_count":"3","content":"D is the right answer","timestamp":"1599775980.0","poster":"Anonymous999","comment_id":"177335"},{"poster":"haroldbenites","content":"C is correct.\nIt's overfiting.","upvote_count":"3","comment_id":"163139","timestamp":"1598036940.0"},{"poster":"dg63","upvote_count":"4","comment_id":"129160","timestamp":"1594142820.0","content":"D - high RMSE indicates underfitting"},{"content":"small RMS Error means--overfitting--fits well--so make it complex by dropping features.\nbig RMS Error means--underfitting--not good fit--so increase complexity by adding layers/features. Answer D.","comment_id":"116075","upvote_count":"11","timestamp":"1592806680.0","poster":"AJKumar"},{"timestamp":"1588026900.0","comment_id":"80511","content":"Correct answer should be C. Dropout regularization method helps in overfitting problems. \nhttps://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e\nD is not correct because, to reduce overfitting you should try to make your models less complex. Adding complexity will not help.","poster":"arnabbis4u","upvote_count":"4"}],"answer_images":[],"answer":"D","answer_description":"","isMC":true},{"id":"onVLxzDKBCkrrSpo9HBZ","question_images":[],"question_id":34,"answers_community":["D (51%)","B (49%)"],"answer_images":[],"timestamp":"2020-03-22 11:14:00","topic":"1","isMC":true,"discussion":[{"comment_id":"66898","upvote_count":"22","timestamp":"1584872040.0","content":"Should be B","poster":"[Removed]"},{"poster":"Ganshank","upvote_count":"12","comment_id":"74274","content":"B\nThe questions is specifically about organizing the data in BigQuery and storing backups.","timestamp":"1586818440.0"},{"upvote_count":"1","timestamp":"1742229240.0","poster":"desertlotus1211","content":"Selected Answer: B\nSnapshot decorators in BigQuery allow you to query a table at a past point in time, but they are limited by BigQuery’s time travel window (which is typically 7 days). Since errors are sometimes only detected after 2 weeks, snapshot decorators won’t be effective for recovering data beyond their retention period.","comment_id":"1399706"},{"content":"Selected Answer: D\nWith Storage Decorators, BigQuery only stores the differences between a snapshot and its base table, minimizing storage costs.","comment_id":"1342831","timestamp":"1737252240.0","comments":[{"timestamp":"1742229120.0","comment_id":"1399705","poster":"desertlotus1211","content":"Does this answer address how should you organize your data in BigQuery and store your backups?","upvote_count":"1"}],"poster":"grshankar9","upvote_count":"1"},{"timestamp":"1733028360.0","poster":"cloud_rider","content":"Selected Answer: D\nD is the most cost optimized solution to keep the backup. please read the link - https://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots","comment_id":"1320453","upvote_count":"2"},{"comment_id":"1302560","upvote_count":"2","poster":"SamuelTsch","content":"Selected Answer: D\nI think D is better.","timestamp":"1729790640.0"},{"poster":"Lenifia","upvote_count":"2","comment_id":"1241268","content":"Selected Answer: D\nThe best option is D. Organize your data in separate tables for each month, and use snapshot decorators to restore the table to a time prior to the corruption.","timestamp":"1719994200.0"},{"poster":"zevexWM","timestamp":"1713954120.0","content":"Selected Answer: D\nAnswer is D: \nSnapshots are different from time travel. They can hold data as long as we want.\nFurthermore \"BigQuery only stores bytes that are different between a snapshot and its base table\" so pretty cost effective as well.\n\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots","comment_id":"1201256","upvote_count":"2"},{"poster":"Farah_007","upvote_count":"1","comment_id":"1193160","content":"Selected Answer: B\nFrom : https://cloud.google.com/architecture/dr-scenarios-for-data#BigQuery\nIt can't be D\nIf the corruption is caught within 7 days, query the table to a point in time in the past to recover the table prior to the corruption using snapshot decorators.\nStore the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table. => D","timestamp":"1712766540.0"},{"upvote_count":"5","poster":"Nirca","timestamp":"1697963460.0","content":"Selected Answer: D\nD - this solution in integrated. No core is needed","comment_id":"1050364"},{"upvote_count":"7","comment_id":"1022902","poster":"Bahubali1988","timestamp":"1696230840.0","content":"90% of questions are having multiple answers and its very hard to get into every discussion where the conclusion is not there"},{"content":"Selected Answer: B\nThe answer is B: \n\nWhy not D? Because snapshot costs can become high if a lot of small changes are made to the base table: https://cloud.google.com/bigquery/docs/table-snapshots-intro#:~:text=Because%20BigQuery%20storage%20is%20column%2Dbased%2C%20small%20changes%20to%20the%20data%20in%20a%20base%20table%20can%20result%20in%20large%20increases%20in%20storage%20cost%20for%20its%20table%20snapshot.\n\nSince the question specifically states that the ETL pipeline is regularly modified, this means that lots of small changes are present. In combination with the requirement to optimize for storage costs, this means that option B is the way to go.","poster":"ckanaar","upvote_count":"6","comment_id":"1013074","timestamp":"1695300000.0"},{"poster":"arien_chen","upvote_count":"2","content":"Selected Answer: D\nkeyword: detected after 2 weeks.\nonly snapshot could resolve the problem.","timestamp":"1692508980.0","comment_id":"985562"},{"poster":"Lanro","upvote_count":"8","content":"Selected Answer: D\nFrom BigQuery documentation - Benefits of using table snapshots include the following:\n\n- Keep a record for longer than seven days. With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\n- Minimize storage cost. BigQuery only stores bytes that are different between a snapshot and its base table, so a table snapshot typically uses less storage than a full copy of the table.\n\nSo storing data in GCS will make copies of data for each table. Table snapshots are more optimal in this scenario.","timestamp":"1690780860.0","comment_id":"967780"},{"content":"Selected Answer: B\nOrganizing your data in separate tables for each month will make it easier to identify the affected data and restore it.\nExporting and compressing the data will reduce storage costs, as you will only need to store the compressed data in Cloud Storage.\nStoring your backups in Cloud Storage will make it easier to restore the data, as you can restore the data from Cloud Storage directly","timestamp":"1690431660.0","poster":"vamgcp","upvote_count":"1","comment_id":"964323"},{"poster":"phidelics","comment_id":"920522","timestamp":"1686473460.0","comments":[{"poster":"cetanx","comment_id":"921237","content":"Just an additional info!\nHere is an example for an export job;\n\n$ bq extract --destination_format CSV --compression GZIP 'your_project:your_dataset.your_new_table' 'gs://your_bucket/your_object.csv.gz'","timestamp":"1686557460.0","upvote_count":"1","comments":[{"content":"I will update my answer to D.\nThink of a scenario that you are in the last week of June and an error occurred 3 weeks ago (so still in June) however you do not have an export of the June table yet therefore you cannot recover the data simply because you don't have an export just yet.\n\nSo snapshots are way to go!","comment_id":"943585","upvote_count":"3","timestamp":"1688553660.0","poster":"cetanx"}]}],"content":"Selected Answer: B\nOrganize in separate tables and store in GCS","upvote_count":"2"},{"timestamp":"1686318480.0","content":"Selected Answer: D\nD\n\"With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\" [source: https://cloud.google.com/bigquery/docs/table-snapshots-intro]","upvote_count":"3","poster":"sdi_studiers","comment_id":"919374"},{"comment_id":"916987","content":"\"Store your data in different tables for specific time periods. This method ensures that you need to restore only a subset of data to a new table, rather than a whole dataset.\"\n\n\"Store the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table.\"\n\nB","timestamp":"1686124020.0","poster":"WillemHendr","upvote_count":"2"},{"timestamp":"1679029320.0","upvote_count":"3","comment_id":"841595","content":"Why not D?","poster":"lucaluca1982"},{"content":"Selected Answer: B\nB is the answer.","poster":"zellck","upvote_count":"1","comment_id":"734381","timestamp":"1670066400.0"},{"upvote_count":"2","poster":"John_Pongthorn","comment_id":"676871","timestamp":"1663917480.0","content":"Selected Answer: B\nB\nhttps://cloud.google.com/architecture/dr-scenarios-for-data#BigQuery"},{"content":"Selected Answer: B\nB seems the best solution (but C is also good candidate)\nD is incorrect - table decorators allow time travel back only up to 7 days (see https://cloud.google.com/bigquery/table-decorators) - if you want to keep older snapshots, you would have to save them into separate table yourself (and pay for storage).","timestamp":"1641741360.0","comment_id":"520280","upvote_count":"8","comments":[{"poster":"MaxNRG","comment_id":"520283","content":"BigQuery. If you want to archive data, you can take advantage of BigQuery's long term storage. If a table is not edited for 90 consecutive days, the price of storage for that table automatically drops by 50 percent. There is no degradation of performance, durability, availability, or any other functionality when a table is considered long term storage. If the table is edited, though, it reverts back to the regular storage pricing and the 90 day countdown starts again.","comments":[{"comment_id":"520284","poster":"MaxNRG","upvote_count":"3","content":"BigQuery is replicated, but this won't help with corruption in your tables. Therefore, you need to have a plan to be able to recover from that scenario. For example, you can do the following:\n• If the corruption is caught within 7 days, query the table to a point in time in the past to recover the table prior to the corruption using snapshot decorators.\n• Export the data from BigQuery, and create a new table that contains the exported data but excludes the corrupted data.\n• Store your data in different tables for specific time periods. This method ensures that you will need to restore only a subset of data to a new table, rather than a whole dataset.\n• Store the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table.\nhttps://cloud.google.com/solutions/dr-scenarios-for-data#BigQuery","timestamp":"1641741480.0"}],"upvote_count":"3","timestamp":"1641741480.0"}],"poster":"MaxNRG"},{"upvote_count":"4","timestamp":"1641645180.0","comment_id":"519512","content":"Selected Answer: B\n\"You need to provide a method to recover from these errors, and your backups should be optimized for storage costs\"\nCost -> GCS\nBackups -> Separate Tables + GCS","comments":[{"comment_id":"762466","content":"Agreed","timestamp":"1672459080.0","poster":"AzureDP900","upvote_count":"1"}],"poster":"medeis_jar"},{"poster":"lifebegins","comment_id":"484743","content":"https://cloud.google.com/bigquery/docs/time-travel\n\nAnswer should be B only, because snapshot decorator u can time travel only for 7 days","upvote_count":"3","timestamp":"1637642220.0"},{"poster":"vintop95","comment_id":"463486","timestamp":"1634461080.0","content":"B:\nStore your data in different tables for specific time periods. This method ensures that you will need to restore only a subset of data to a new table, rather than a whole dataset.\nhttps://cloud.google.com/architecture/dr-scenarios-for-data#managed-database-services-on-gcp","upvote_count":"4","comments":[{"poster":"Mcloudgirl","comment_id":"715446","upvote_count":"2","content":"Thanks for explaining this - I previously was unsure why separate tables was necessary.","timestamp":"1668103920.0"}]},{"content":"B since the question is about saving costs when storing backups : use Cloud Storage.","poster":"Ysance_AGS","timestamp":"1632901560.0","comment_id":"453922","upvote_count":"3"},{"content":"Point for discussion \n\nIs A an Option ? One of the information is \"BQ is Centralized analytics Platform\" - Will not be good to store the data in single table rather separate table for each month (for example, if One needs to analyze a trend for last two years, will not be complicated to have 24 tables in the query)","timestamp":"1627168980.0","upvote_count":"1","poster":"raf2121","comment_id":"413507"},{"comment_id":"399817","content":"why not A","timestamp":"1625564220.0","upvote_count":"3","poster":"p111111111111"},{"comment_id":"397566","content":"A & B - Not sure (what to choose)\nC - elimnated (because of cost)\nD - eliminated (because of error detected in 2 weeks)","upvote_count":"1","poster":"sumanshu","timestamp":"1625314920.0"},{"upvote_count":"1","content":"How about C?\nhttps://medium.com/google-cloud/how-to-backup-google-big-query-5f078138cedc\nSounds like alternative 2 in above article","timestamp":"1621912680.0","comments":[{"timestamp":"1622819520.0","poster":"z8zhong","upvote_count":"2","content":"Because \"your backups should be optimized for storage costs\"","comment_id":"374445"}],"comment_id":"366066","poster":"shanjin14"},{"timestamp":"1612758840.0","upvote_count":"4","comment_id":"285960","poster":"someshsehgal","content":"Why nobody supporting C. Data management will be easy with data reside only within BQ. Also long term storage cost in BQ is almost similar like GCS"},{"content":"Should be B\nUsing snapshot decorators , recovery is valid only for a period of 7 days. Here it says 2 weeks so \"D\" is ruled out.\n\nYou can undelete a table within seven days of deletion, including explicit deletions and implicit deletions due to table expiration. After seven days, it is not possible to undelete a table using any method, including opening a support ticket. \nhttps://cloud.google.com/bigquery/docs/managing-tables","upvote_count":"8","poster":"ABM9","comment_id":"245667","timestamp":"1608130440.0"},{"timestamp":"1602660120.0","poster":"aleedrew","comment_id":"199603","upvote_count":"2","content":"It says sometimes it is discovered in two weeks, not all of it is discovered in two weeks. I am inclined to choose D."},{"upvote_count":"7","comment_id":"189719","timestamp":"1601388720.0","poster":"kino2020","comments":[{"upvote_count":"2","timestamp":"1625575740.0","poster":"patitonav","comment_id":"399985","content":"The link you provided supports B!"}],"content":"B\nBigQuery. If you want to archive data, you can take advantage of BigQuery's long term storage. If a table is not edited for 90 consecutive days, the price of storage for that table automatically drops by 50 percent. There is no degradation of performance, durability, availability, or any other functionality when a table is considered long term storage. If the table is edited, though, it reverts back to the regular storage pricing and the 90 day countdown starts again.\nhttps://cloud.google.com/solutions/dr-scenarios-for-data#managed-database-services-on-gcp"},{"comment_id":"167259","timestamp":"1598499420.0","upvote_count":"2","poster":"atnafu2020","content":"D\nYou can easily revert changes without having to request a recovery from backups If not deleted. (When a table is explicitly deleted, its history is flushed after 7 days.)"},{"upvote_count":"5","comment_id":"163141","poster":"haroldbenites","content":"B is correct.","timestamp":"1598037120.0"},{"timestamp":"1594203540.0","poster":"SSV","content":"Answer is A: single table is enough. Why should we go for monthly tables.","comment_id":"129636","upvote_count":"6","comments":[{"upvote_count":"3","timestamp":"1600624380.0","poster":"vakati","comments":[{"timestamp":"1641523920.0","upvote_count":"1","content":"For a table that has Change data capture, is it even possible if data is maintained in separate monthly tables? I am having this question since question has this 'ETL pipeline modifies the original data and prepares it for the final users' this could refer to type 2 table operation. I am leaning towards A just because of this.","poster":"GCPLearning2021","comment_id":"518725"}],"comment_id":"183204","content":"Answer - B\n[A] would have been a good choice if the table was partitioned on timestamp. Since it's not explicitly mentioned, [B] is the correct option"},{"comments":[{"upvote_count":"1","poster":"sergio6","comment_id":"460130","timestamp":"1633879920.0","content":"A. you can restore only the desired ranges of data By partitions"}],"content":"Monthly table is a good choice if you find the error two weeks later and need to restore the table, you don't want to restore the whole history data","upvote_count":"7","poster":"dragon123","timestamp":"1598282580.0","comment_id":"165317"}]},{"comment_id":"119008","upvote_count":"5","content":"Though Option [B] & [D] are closest, I think none of them are correct. Point-inTime recovery is valid only for a period of 7 days. For option [B] to be correct, new tables should be created & backed up on a daily basis as data is ingested / modified daily","timestamp":"1593047820.0","poster":"dambilwa"},{"poster":"arnabbis4u","comment_id":"88559","content":"Correct B","timestamp":"1589413860.0","upvote_count":"5"},{"timestamp":"1585789680.0","content":"Answer D. Point in Time decorators needed to correct the data that are older than 2 weeks.","upvote_count":"2","comments":[{"upvote_count":"10","timestamp":"1591426860.0","content":"Decorators can only correct the data unto 7 days. Hence Option B","comment_id":"103623","poster":"Poojaji"}],"comment_id":"70298","poster":"Rajokkiyam"}],"answer":"D","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/17239-exam-professional-data-engineer-topic-1-question-129/","exam_id":11,"choices":{"B":"Organize your data in separate tables for each month, and export, compress, and store the data in Cloud Storage.","C":"Organize your data in separate tables for each month, and duplicate your data on a separate dataset in BigQuery.","A":"Organize your data in a single table, export, and compress and store the BigQuery data in Cloud Storage.","D":"Organize your data in separate tables for each month, and use snapshot decorators to restore the table to a time prior to the corruption."},"question_text":"You use BigQuery as your centralized analytics platform. New data is loaded every day, and an ETL pipeline modifies the original data and prepares it for the final users. This ETL pipeline is regularly modified and can generate errors, but sometimes the errors are detected only after 2 weeks. You need to provide a method to recover from these errors, and your backups should be optimized for storage costs. How should you organize your data in BigQuery and store your backups?","answer_ET":"D","unix_timestamp":1584872040},{"id":"bPK4yyeS7XJ5ahYXzK9e","exam_id":11,"question_images":[],"answer_images":[],"answer_ET":"D","topic":"1","url":"https://www.examtopics.com/discussions/google/view/16279-exam-professional-data-engineer-topic-1-question-13/","choices":{"A":"Cloud SQL","C":"Cloud Bigtable","D":"Cloud Datastore","B":"BigQuery"},"answers_community":["D (57%)","A (33%)","10%"],"isMC":true,"unix_timestamp":1583947320,"question_text":"You want to process payment transactions in a point-of-sale application that will run on Google Cloud Platform. Your user base could grow exponentially, but you do not want to manage infrastructure scaling.\nWhich Google database service should you use?","timestamp":"2020-03-11 18:22:00","discussion":[{"poster":"DeepakKhattar","comments":[{"comment_id":"473508","upvote_count":"10","poster":"canon123","comments":[{"poster":"BigQuery","timestamp":"1638766260.0","comment_id":"494899","content":"https://cloud.google.com/architecture/elastically-scaling-your-mysql-environment#objectives\n\nPlease read. It can be configured for autoscaling.","comments":[{"content":"That link explains how to set MySQL autoscaling with Google Compute Engine instances (you install and manage MySQL on the VM). This can not be applied to Cloud SQL (managed service). In Cloud SQL, only the storage can be automatically increased, and changing the Cloud SQL instance size requires a manual edit of the instance type.","comments":[{"timestamp":"1666974060.0","poster":"MisuLava","upvote_count":"3","content":"yes. and that is ok since this is a point of sale. an exponential increase in number of clients still means reduced parallel processing (how many customers can buy in the very same time) so an increase in memory and CPU is very unlikely to be necessary. yes, an exponential increase in the number of customers means more memory, and more storage, which in cloud SQL increases automatically.","comment_id":"706617"}],"timestamp":"1639708140.0","comment_id":"503303","poster":"hendrixlives","upvote_count":"4"},{"poster":"nkunwar","comment_id":"681374","content":"C SQL doesn't AUTO SCALE, you need to manually edit , Please show where does it says AUTO SCALING","timestamp":"1664336580.0","upvote_count":"2"}],"upvote_count":"3"}],"content":"CloudSql does not auto scale.","timestamp":"1636211700.0"},{"comment_id":"431181","comments":[{"timestamp":"1631682540.0","comment_id":"445000","poster":"Blobby","upvote_count":"5","content":"Based on a re-read of the above comments and other later questions agree with A. \npls ignore my first answer."}],"upvote_count":"3","content":"Can't online be considered PoS? CloudSQL does have constraints for scaling and Google seem to specifically be selling Datastore for transactional use cases so going with D: \nhttps://cloud.google.com/datastore/docs/concepts/transactions","poster":"Blobby","timestamp":"1629871260.0"}],"content":"Initially, thinking D is the best answer but when question is re-re-read, A seems to be correct answer for following reasons\n1. Is payment TRANSACTION -- DB should able to perform full blown transaction (updating inventory, sales info etc, though not specified) , not just ATOMIC which DataStore provides\n2. Its point-of-sale application, not ONLINE STORE where HIGH number of concurrent users ordering stuff. \n3. User Base could grow exponentially - again more users does mot mean concurrent users and more processing power. Its only about storage.\n4. Do not want to Manage infrastructure scaling. - Cloud SQL can scale in terms of storage.\n5. CloudStore is poor selection for OLTP application \n - Each property is index - so higher latency\n \nNot sure, during exam 2 min is enough to think on various point.. \nI may be wrong or wrong path ... lets brainstrom..","upvote_count":"78","comment_id":"101332","timestamp":"1591145820.0"},{"comment_id":"62577","upvote_count":"38","timestamp":"1583947320.0","comments":[{"timestamp":"1638766020.0","poster":"BigQuery","comments":[{"comment_id":"503306","comments":[{"poster":"imsaikat50","content":"I believe the key point is it's a POS, not an e-commerce. Keeping that in mind, exponential user increase in POS might not mean concurrent user increase, which could be a huge consideration in case of it is being e-commerce.\n\nI would rather go with 'Cloud SQL' as the best answer.","timestamp":"1670540220.0","comment_id":"739564","upvote_count":"2"}],"timestamp":"1639708380.0","upvote_count":"5","content":"Storage scale is automatic (e.g. you begin with a 50GB disk and it grows automatically as needed), but the instance size (CPU/memory) will be the same. The questions states that the user base may increase exponentially. Even if you have enough disk space to store all your user data, the increase in users will cause problems if your instance (CPU/memory) is too small, since the instance will not be able to process all the queries at the required speed.","poster":"hendrixlives"}],"upvote_count":"2","content":"Cloud SQL does scale automatically. THERE IS A SETTING WHERE YOU DEFINE INCREASE MEMORY SPACE WHEN IT REACHED 70%. \n\nhttps://cloud.google.com/sql/docs/features#features_3\n\nHere it say's \n-> Fully managed SQL Server databases in the cloud.\n-> Custom machine types with up to 624 GB of RAM and 96 CPUs.\n-> Up to 64 TB of storage available, with the ability to automatically increase storage size as needed.","comment_id":"494895"}],"poster":"jvg637","content":"D seems to be the right one. Cloud SQL doesn't automatically scale"},{"timestamp":"1744285320.0","comment_id":"1559549","poster":"fassil","content":"Selected Answer: D\nYou don't need to go far, guys. Cloud SQL does not support auto scale; cut the shit here based on the requirement—the question specifically says \"you do not want to manage infrastructure scaling.\"","upvote_count":"1"},{"content":"Selected Answer: D\nPOS app needing scalable DB without managing infrastructure. Cloud Datastore or Firestore? But options are Cloud SQL, BigQuery, Bigtable, Datastore. Since it's for transactions and scale, maybe Cloud Datastore (D) or Cloud Bigtable. But Cloud Bigtable is for high throughput. The question says not to manage scaling. Cloud Datastore is serverless. So D.","upvote_count":"1","poster":"Parandhaman_Margan","timestamp":"1742045940.0","comment_id":"1398846"},{"content":"Selected Answer: D\nB and C are discarded Since we are dealing with transactional data. \nA is discarded since we require to deal with Infrastructure scaling in case the base of users increase and also (not necessarily) the concurrent transactions. \n\nUsers base is gonna grow and we DO NOT WANT TO DEAL with infrastructure scaling. D is the most appropriated","comment_id":"1366712","upvote_count":"1","timestamp":"1741477920.0","poster":"monyu"},{"poster":"cqrm3n","upvote_count":"1","comment_id":"1342430","timestamp":"1737179640.0","content":"Selected Answer: D\nThe answer is D, Cloud Datastore (now Firestore in Datastore mode), because it supports auto scaling and low latency reads and writes. Cloud SQL is not the correct answer because it requires more active management for scaling."},{"timestamp":"1729533780.0","comment_id":"1301210","upvote_count":"3","content":"Selected Answer: C\nFor handling payment transactions in a point-of-sale application with potential exponential growth and without the need to manage infrastructure scaling, Cloud Bigtable would be the best choice.","poster":"GHill1982"},{"timestamp":"1727155200.0","content":"D seems to be the answer. This is what I think based on my analysis below.\nPOS is OLTP system but now a days NOSQL with ACID properties also are used for OLTP,\nCloud sql is good for relational database and it would have been an option here but it clearly says that \"you do not want to manage infrastructure scaling\". In cloud SQL, which is managed service and not server less, you need to manually do vertical scaling(scale up and scale down). \nHence I believe CLOUD SQL is not the option here. \nI also tried creating a datastore using google cloud console and it gives 2 options now that is cloud firestore in native mode and cloud firestore in data store mode. automatic scaling is available in both where there is no manual scaling up or down is required. Also, both firestore in native and datastore provides ACID properties. Also, firestore is now optimized for OLTP. Please see below\nhttps://cloud.google.com/solutions/building-scalable-apps-with-cloud-firestore\nThough the question only talks about datastore, I am just providing additional information. \nConsidering all what I read through, D is the answer.","upvote_count":"13","comment_id":"214051","poster":"Radhika7983","comments":[{"upvote_count":"1","content":"I agree. I think people are missing the part of the question that mentions they don't want to maintain the DB.","timestamp":"1625248320.0","poster":"awssp12345","comment_id":"397093"},{"timestamp":"1625248380.0","comment_id":"397097","poster":"awssp12345","upvote_count":"1","content":"This should be accepted and the highest voted answer."}]},{"poster":"kishanu","comment_id":"506855","upvote_count":"2","content":"Selected Answer: D\nD is the hero here.\nThough Cloud SQL has an upper hand when it comes to transactions(OLTP), it does not autoscale its computing capabilities as compared to datastore.\nDo visit: https://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for","timestamp":"1727155140.0"},{"content":"Selected Answer: D\nD is correct: Datastore (currently Firestore in native or datastore mode). It is a fully managed and serverless solution that allows for transactions and will autoscale (storage and compute) without the need to manage any infrastructure. \nA is wrong: Cloud SQL is fully a managed transactional DB, but only the storage grows automatically. As your user base increases, you will need to increase the CPU/memory of the instance, and to do that you must edit the instance manually (and the questions specifically says \"you do not want to manage infrastructure scaling\")\nB is wrong: Bigquery is OLAP (for analytics). NoOps, fully managed, autoscales and allows transactions, but it is not designed for this use case.\nC is wrong: Bigtable is a NoSQL database for massive writes, and to scale (storage and CPU) you must add nodes, so it is completely out of this use case.","comments":[{"content":"May be some history can help to decide which is best answer.\nDatastore built by Google uses BigTable as it's storage, while the company who built FireStore uses Cloud Spanner as it's storage. Google decided that they like the FireStore technology and acquired it.\nIf Cloud Spanner is an option I would choose it. So, D for me, although it's json storage format, but the Cloud Spanner it uses as storage fits all the requirements.","upvote_count":"2","comment_id":"510662","poster":"kuik","timestamp":"1640648940.0"}],"comment_id":"503320","upvote_count":"4","timestamp":"1727155140.0","poster":"hendrixlives"},{"timestamp":"1727154960.0","content":"Selected Answer: D\nB - not an option\nC - lack of ACID transactions\nA - lack of resource automatic scalability \nD - (correct, IMHO) support ACID, suitable for OLPT and scalable enough","comment_id":"1056987","poster":"axantroff","upvote_count":"1"},{"comment_id":"1062181","poster":"rocky48","upvote_count":"3","content":"Selected Answer: D\nB - not an option\nC - lack of ACID transactions\nA - lack of resource automatic scalability\nD - (correct, IMHO) support ACID, suitable for OLPT and scalable enough","timestamp":"1727154960.0"},{"poster":"TVH_Data_Engineer","comment_id":"1096236","upvote_count":"1","content":"Selected Answer: D\nCloud Datastore (now part of Google Cloud Firestore in Datastore mode) is designed for high scalability and ease of management for applications. It is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It's serverless, meaning it handles the scaling, performance, and management automatically, fitting your requirement of not wanting to manage infrastructure scaling.\n\nCloud SQL, while a fully-managed relational database service that makes it easy to set up, manage, and administer your SQL databases, is not as automatically scalable as Datastore. It's better suited for applications that require a traditional relational database.","timestamp":"1727154960.0"},{"poster":"baimus","comment_id":"1286842","content":"Selected Answer: A\nThis actually is A. I was initially in the D camp, and have spent considerable time reading about it now (circa 1 hour). D is explicitly not suitable for payment transactions, as Datastore supports ACID transactions, but only within entity groups, which are small, localized sets of data. This restriction means that transactions are not suitable for scenarios requiring multi-entity consistency across the entire database.\nThe only two products recommended for payments in the Google ecosystem are Cloud Spanner and Cloud SQL. Is Cloud SQL managed? I'd say it wasn't really, due to the need to configure instances, but that is trumped by the fact it is the only choice suitable for a payment transaction system.","upvote_count":"2","timestamp":"1726838940.0"},{"poster":"SatyamKishore","content":"Cloud Datastore is a fully managed, NoSQL document database that is highly scalable and designed to automatically handle large increases in traffic without requiring manual intervention. It's well-suited for applications with a rapidly growing user base.","timestamp":"1723369140.0","upvote_count":"1","comment_id":"1263997"},{"poster":"iooj","timestamp":"1722360060.0","comment_id":"1258318","upvote_count":"1","content":"Selected Answer: D\nFirestore extension of Datastore can handle acid transactions and allows autoscaling"},{"upvote_count":"1","comment_id":"1238168","content":"Selected Answer: A\nA. Requires Transactions.","timestamp":"1719494040.0","poster":"jamalkhan"}],"answer_description":"","answer":"D","question_id":35}],"exam":{"isMCOnly":true,"name":"Professional Data Engineer","isImplemented":true,"id":11,"provider":"Google","numberOfQuestions":319,"isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":7},"__N_SSP":true}