{"pageProps":{"questions":[{"id":"kghMnmzjiEUrWt91quqq","url":"https://www.examtopics.com/discussions/google/view/40836-exam-professional-cloud-developer-topic-1-question-79/","exam_id":7,"discussion":[{"comment_id":"253304","upvote_count":"17","poster":"fraloca","timestamp":"1609072680.0","content":"for me the solution is A and C:\nhttps://cloud.google.com/trace/docs/zipkin"},{"upvote_count":"6","content":"AC as Zipkin is used for to gather data for latency issues and SD trace purpose is to enable us to have a better view on the application code latency","comment_id":"275417","timestamp":"1611516480.0","poster":"dxxdd7"},{"timestamp":"1721276460.0","comment_id":"1250094","content":"Selected Answer: BC\nThe two steps you should take are:\n\nB. Use Fluentd agent to gather data. Fluentd is a great tool for collecting logs and metrics from various sources, including your API backend running on multiple cloud providers. It can be configured to collect network latency data and send it to a centralized location for analysis.\nC. Use Stackdriver Trace to generate reports. Stackdriver Trace is a distributed tracing system that helps you understand the performance of your applications. It can be used to collect and analyze network latency data, providing insights into bottlenecks and performance issues.","comments":[{"comment_id":"1250095","poster":"thewalker","upvote_count":"1","timestamp":"1721276520.0","content":"Here's why the other options are less suitable:\n\nA. Use Zipkin collector to gather data. While Zipkin is a popular distributed tracing system, it's not directly integrated with Stackdriver. Using Zipkin would require additional configuration and integration to send data to Stackdriver for reporting.\nD. Use Stackdriver Debugger to generate reports. Stackdriver Debugger is designed for debugging code, not for generating network latency reports. It focuses on inspecting the state of your application at specific points in time.\nE. Use Stackdriver Profiler to generate reports. Stackdriver Profiler is used for profiling your application's performance, focusing on CPU usage and memory allocation. It's not the ideal tool for analyzing network latency."}],"poster":"thewalker","upvote_count":"1"},{"poster":"santoshchauhan","timestamp":"1709871060.0","upvote_count":"1","content":"Selected Answer: AC\nFor generating reports on network latency for an API that is distributed across multiple cloud providers, you would typically need to gather trace data and then analyze it:\n\nA. Use Zipkin collector to gather data: Zipkin is a distributed tracing system that helps gather timing data needed to troubleshoot latency problems in service architectures. You can use Zipkin collectors to gather trace data from your API backend regardless of where it's running. This trace data can provide insights into the latency of different service calls.\n\nC. Use Stackdriver Trace to generate reports: Stackdriver Trace (part of Google Cloudâ€™s operations suite) allows you to analyze how requests propagate through your application and receive detailed latency reports for your API. If you are already using Stackdriver on Google Cloud, you can extend its usage to analyze trace data collected from other cloud providers as well.","comment_id":"1168541"},{"poster":"maxdanny","comment_id":"989971","timestamp":"1692961080.0","content":"Selected Answer: AC\nsolution is AC: \nhttps://cloud.google.com/trace/docs/zipkin","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: AC\nThe two steps you should take to generate reports for the network latency of your API running on multiple cloud providers are:\n\nA. Use Zipkin collector to gather data: Zipkin is a distributed tracing system that helps you gather data about the latency of requests made to your API. It allows you to trace requests as they flow through your system, and provides insight into the performance of your services. You can use Zipkin collectors to collect data from multiple cloud providers, and then generate reports to analyze the latency of your API.\n\nC. Use Stackdriver Trace to generate reports: Stackdriver Trace is a distributed tracing system that helps you trace requests across multiple services and provides detailed performance data about your applications. It allows you to visualize and analyze the performance of your API and its dependencies. You can use Stackdriver Trace to generate reports about the network latency of your API running on multiple cloud providers.\n\nTherefore, the correct options are A and C.","timestamp":"1680081720.0","comment_id":"854210","poster":"Teraflow"},{"comment_id":"769265","upvote_count":"1","poster":"omermahgoub","timestamp":"1673171640.0","content":"The correct answer would be: A. Use Zipkin collector to gather data and C. Use Stackdriver Trace to generate reports.\n\nUsing Zipkin collector will allow you to gather data from your instrumented application running on multiple cloud providers. Stackdriver Trace can then be used to generate reports based on this data.\n\nOption B, using Fluentd agent, is not related to generating reports on network latency for an API.\n\nOption D, using Stackdriver Debugger, is not related to generating reports on network latency for an API.\n\nOption E, using Stackdriver Profiler, is not related to generating reports on network latency for an API."},{"timestamp":"1669167540.0","poster":"miyakelp","upvote_count":"1","content":"Selected Answer: AC\nA/C\nhttps://cloud.google.com/trace/docs/zipkin","comment_id":"724809"},{"comment_id":"649245","content":"Selected Answer: BD\nBD are correct","upvote_count":"1","poster":"tomato123","timestamp":"1660974960.0"},{"comment_id":"639133","content":"Selected Answer: AC\nhttps://cloud.google.com/trace/docs/zipkin#frequently_asked_questions\n use a Zipkin server to receive traces from Zipkin clients and forward those traces to Cloud Trace for analysis.","timestamp":"1659090960.0","upvote_count":"2","poster":"nehaxlpb"},{"upvote_count":"1","timestamp":"1643017500.0","comment_id":"531197","content":"CE. E for latency Cloud Profiler. For tracing can be use also zipkin but better tracing","poster":"mariorossi"},{"timestamp":"1641663540.0","content":"Selected Answer: AC\nA and C are correct solution.","upvote_count":"2","comment_id":"519688","poster":"ParagSanyashiv"},{"upvote_count":"4","timestamp":"1626587580.0","poster":"celia20200410","content":"AC\nA: to support multiple cloud providers \nhttps://cloud.google.com/trace \nZipkin tracers to submit data to Cloud Trace. Projects running on App Engine are automatically captured.\n\nC: to generate reports for the network latency\nhttps://cloud.google.com/trace/docs/quickstart#analysis_reports_window","comment_id":"408733"},{"upvote_count":"3","content":"\"latency\" is the key word here so C is one of the answers; Stackdriver Trace\n\nhttps://cloud.google.com/trace/docs/zipkin:\n\"receive traces from Zipkin clients and forward those traces to Cloud Trace for analysis.\"\n\nA is the other answer","timestamp":"1625963520.0","poster":"syu31svc","comment_id":"403587"},{"upvote_count":"1","comment_id":"388479","poster":"yuchun","timestamp":"1624421280.0","content":"I think the answer is AC"},{"timestamp":"1617723180.0","content":"for me it is C and E, as profiler is used for performance analysis","poster":"shav789","comment_id":"329761","upvote_count":"1"},{"comments":[{"timestamp":"1609072740.0","upvote_count":"1","content":"C is correct. But B is used for logging and not for monitoring.","comment_id":"253305","poster":"fraloca"}],"poster":"MickeyRourke","comment_id":"253039","timestamp":"1609037580.0","content":"I would go with BC","upvote_count":"2"}],"answer_description":"","unix_timestamp":1609037580,"answer_ET":"AC","question_text":"Your API backend is running on multiple cloud providers. You want to generate reports for the network latency of your API.\nWhich two steps should you take? (Choose two.)","question_images":[],"topic":"1","answer":"AC","timestamp":"2020-12-27 03:53:00","isMC":true,"answer_images":[],"answers_community":["AC (82%)","Other"],"choices":{"E":"Use Stackdriver Profiler to generate report.","D":"Use Stackdriver Debugger to generate report.","B":"Use Fluentd agent to gather data.","A":"Use Zipkin collector to gather data.","C":"Use Stackdriver Trace to generate reports."},"question_id":316},{"id":"dDmJQXyHAhU4Cx5VRm0i","answer_ET":"C","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/36176-exam-professional-cloud-developer-topic-1-question-8/","answer":"C","question_id":317,"question_images":[],"unix_timestamp":1604597820,"discussion":[{"content":"C is correct answer here.\n\nThe only difference between Union and Union All is that Union All will not removes duplicate rows or records, instead, it just selects all the rows from all the tables which meets the conditions of your specifics query and combines them into the result table.","poster":"saurabh1805","upvote_count":"7","timestamp":"1636133820.0","comment_id":"213602"},{"upvote_count":"1","timestamp":"1726753200.0","content":"Selected Answer: C\nC is correct.","comment_id":"1011376","poster":"__rajan__"},{"content":"UNION removes duplicate rows.\nUNION ALL does not remove duplicate rows.","poster":"ash_meharun","upvote_count":"3","timestamp":"1702362480.0","comment_id":"742461"},{"content":"Selected Answer: C\nC is correct","comment_id":"649165","upvote_count":"1","poster":"tomato123","timestamp":"1692509160.0"},{"content":"C is correct","timestamp":"1658368140.0","upvote_count":"1","comment_id":"410595","poster":"wilwong"},{"upvote_count":"2","content":"If you know SQL well enough, C is the answer","comment_id":"385987","poster":"syu31svc","timestamp":"1655705160.0"}],"answer_description":"Reference:\nhttps://www.techonthenet.com/sql/union_all.php","topic":"1","timestamp":"2020-11-05 18:37:00","exam_id":7,"choices":{"C":"Use the UNION operator in SQL to combine the tables.","A":"Use the JOIN operator in SQL to combine the tables.","D":"Use the UNION ALL operator in SQL to combine the tables.","B":"Use nested WITH statements to combine the tables."},"question_text":"You have two tables in an ANSI-SQL compliant database with identical columns that you need to quickly combine into a single table, removing duplicate rows from the result set.\nWhat should you do?","isMC":true,"answers_community":["C (100%)"]},{"id":"4fZF2n6bdIQGnMRI3aEN","url":"https://www.examtopics.com/discussions/google/view/50397-exam-professional-cloud-developer-topic-1-question-80/","exam_id":7,"discussion":[{"poster":"fosky94","content":"In the case study is stated: \"Obtain user activity metrics to better understand how to monetize their product\", which means that they'll need to analyse the user activity, so... I'll go with answer A (BigQuery)","upvote_count":"13","timestamp":"1618751820.0","comment_id":"338205","comments":[{"content":"Agree with you on this one","poster":"syu31svc","timestamp":"1625964180.0","upvote_count":"3","comment_id":"403592"}]},{"timestamp":"1742705400.0","content":"Selected Answer: C\nIn the question, it mentions about using MySQL, and it should be accessed from multi locations -> Spanner","poster":"EMPERBACH","comment_id":"1402117","upvote_count":"1"},{"content":"Selected Answer: D\nCloud Datastore is a NoSQL database, which is ideal for storing unstructured or semi-structured data, such as user activity logs.\n\nWhile BigQuery is great for large-scale data analytics, it's not optimized for real-time data storage and retrieval. So, in my opinion, Option D is correct.","comment_id":"1322683","upvote_count":"1","timestamp":"1733479020.0","poster":"PinkeshExampTopics"},{"timestamp":"1721276760.0","content":"Selected Answer: A\nThe best answer here is A. BigQuery . Here's why:\n\nScalability and Analytics: HipLocal needs to store and analyze user activity data to understand how to monetize their product. BigQuery is designed for petabyte-scale data storage and offers powerful analytics capabilities. It's ideal for handling the large volumes of user activity data that HipLocal will generate as they expand globally.\nCost-Effectiveness: BigQuery is pay-per-use, making it cost-effective for storing large amounts of data that might not be accessed frequently. This aligns with HipLocal's goal of reducing infrastructure management costs.\nCompliance: BigQuery is a compliant platform, meeting requirements like GDPR. This is crucial for HipLocal as they expand into new regions with varying data privacy regulations.","comment_id":"1250097","poster":"thewalker","comments":[{"content":"Why other options are less suitable:\n\nB. Cloud SQL: While Cloud SQL is a good choice for relational databases, it's not designed for the massive scale and analytics needs of user activity data. It can be expensive for large datasets.\nC. Cloud Spanner: Cloud Spanner is a globally distributed, strongly consistent database. While it's excellent for transactional data, it's overkill for user activity data that doesn't require strong consistency. It's also more complex and expensive than BigQuery.\nD. Cloud Datastore: Cloud Datastore is a NoSQL database that's good for real-time updates. However, its query capabilities are less powerful than BigQuery, and it's not as well-suited for large-scale analytics.","comment_id":"1250098","poster":"thewalker","timestamp":"1721276760.0","upvote_count":"1"}],"upvote_count":"1"},{"poster":"santoshchauhan","timestamp":"1709871300.0","comment_id":"1168542","upvote_count":"1","content":"Selected Answer: C\nHere's why Cloud Spanner is the best fit for HipLocal's needs:\n\nGlobal Scalability: Cloud Spanner can scale horizontally to handle increased loads and number of concurrent users, which is aligned with the rapid growth that HipLocal is experiencing.\n\nStrong Consistency: It provides strong consistency guarantees, ensuring that users have a consistent experience regardless of the region they're accessing the application from.\n\nHigh Availability: Spanner's built-in replication across multiple regions makes it highly available, which helps to meet uptime requirements and ensure compliance with regulations like GDPR that may require data to be stored in certain regions.\n\nManaged Service: As a fully managed service, it reduces the time and cost associated with infrastructure management, which meets the business requirement to minimize management overhead."},{"poster":"__rajan__","timestamp":"1695185940.0","comment_id":"1011922","upvote_count":"1","content":"Selected Answer: D\nFor Storing user data Datastore is best.","comments":[{"upvote_count":"1","poster":"__rajan__","comment_id":"1012391","timestamp":"1695218940.0","content":"I think A would be better fit for this. Please ignore the above answer."}]},{"comment_id":"898731","poster":"sbonessi","content":"Selected Answer: A\nA (BigQuery) is more apropiated for user activities.\n\nIf it was manage user states, I would consider D (Datastore/Firestore) but this is not the case.\nSo, from my point of view, A is the correct answer.","timestamp":"1684185840.0","upvote_count":"1"},{"content":"Selected Answer: A\nA is correct","timestamp":"1660974960.0","upvote_count":"1","comment_id":"649247","poster":"tomato123"},{"comment_id":"639134","upvote_count":"2","poster":"nehaxlpb","content":"Selected Answer: A\nBigquery for user activity analysis . And also the user activity is kind of raw data which being used to segment user or according age , choice etc so Bigquery fits best fr this use cases","timestamp":"1659091080.0"},{"upvote_count":"1","poster":"brewpike","comment_id":"604450","content":"Toss b/w A and D . It depends what needs to be done on user activity, if analytics then A. (Big query) else if customizing customer experience then D (Datastore)","timestamp":"1653054000.0"},{"upvote_count":"1","comment_id":"555937","poster":"GCPCloudArchitectUser","timestamp":"1645790520.0","content":"Selected Answer: D\nI agree with having to use Datstore"},{"content":"A. database only for user activity","timestamp":"1643018160.0","poster":"mariorossi","comment_id":"531206","upvote_count":"1"},{"upvote_count":"2","timestamp":"1642149780.0","content":"I choose D, datastore.","comment_id":"523433","poster":"Nidie"},{"comment_id":"447794","upvote_count":"3","content":"#37 \"Your existing application keeps user state information in a single MySQL database. This state information is very user-specific and depends heavily on how long a user has been using an application. The MySQL database is causing challenges to maintain and enhance the schema for various users. Which storage option should you choose?\"\n\nhttps://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for\n\n\"Datastore/Firestore can store and query the following types of data: \nUser profiles that deliver a customized experience based on the userâ€™s past activities and preferences\"\n\nI feel like this is a toss up between these two since we're talking about user profiles/data, would vote for D here bc MySQL offers a really rigid schema and isn't well suited to massive scaling either.","poster":"boof","timestamp":"1632077640.0"}],"answer_description":"","answer_ET":"A","unix_timestamp":1618751820,"question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nCompany Overview -\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\nExecutive Statement -\nWe are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.\n\nSolution Concept -\nHipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.\n\nExisting Technical Environment -\nHipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:\n* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.\n* State is stored in a single instance MySQL database in GCP.\n* Data is exported to an on-premises Teradata/Vertica data warehouse.\n* Data analytics is performed in an on-premises Hadoop environment.\n* The application has no logging.\n* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.\n\nBusiness Requirements -\nHipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:\n* Expand availability of the application to new regions.\n* Increase the number of concurrent users that can be supported.\n* Ensure a consistent experience for users when they travel to different regions.\n* Obtain user activity metrics to better understand how to monetize their product.\n* Ensure compliance with regulations in the new regions (for example, GDPR).\n* Reduce infrastructure management time and cost.\n* Adopt the Google-recommended practices for cloud computing.\n\nTechnical Requirements -\n* The application and backend must provide usage metrics and monitoring.\n* APIs require strong authentication and authorization.\n* Logging must be increased, and data should be stored in a cloud analytics platform.\n* Move to serverless architecture to facilitate elastic scaling.\n* Provide authorized access to internal apps in a secure manner.\nWhich database should HipLocal use for storing user activity?","question_images":[],"topic":"1","answer":"A","timestamp":"2021-04-18 15:17:00","isMC":true,"answers_community":["A (50%)","D (30%)","C (20%)"],"answer_images":[],"choices":{"A":"BigQuery","D":"Cloud Datastore","C":"Cloud Spanner","B":"Cloud SQL"},"question_id":318},{"id":"CNJFjwHrSNn2G5rf74Ft","answers_community":["C (100%)"],"question_id":319,"url":"https://www.examtopics.com/discussions/google/view/58038-exam-professional-cloud-developer-topic-1-question-81/","question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nCompany Overview -\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\nExecutive Statement -\nWe are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.\n\nSolution Concept -\nHipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.\n\nExisting Technical Environment -\nHipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:\n* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.\n* State is stored in a single instance MySQL database in GCP.\n* Data is exported to an on-premises Teradata/Vertica data warehouse.\n* Data analytics is performed in an on-premises Hadoop environment.\n* The application has no logging.\n* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.\n\nBusiness Requirements -\nHipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:\n* Expand availability of the application to new regions.\n* Increase the number of concurrent users that can be supported.\n* Ensure a consistent experience for users when they travel to different regions.\n* Obtain user activity metrics to better understand how to monetize their product.\n* Ensure compliance with regulations in the new regions (for example, GDPR).\n* Reduce infrastructure management time and cost.\n* Adopt the Google-recommended practices for cloud computing.\n\nTechnical Requirements -\n* The application and backend must provide usage metrics and monitoring.\n* APIs require strong authentication and authorization.\n* Logging must be increased, and data should be stored in a cloud analytics platform.\n* Move to serverless architecture to facilitate elastic scaling.\n* Provide authorized access to internal apps in a secure manner.\nHipLocal is configuring their access controls.\nWhich firewall configuration should they implement?","timestamp":"2021-07-17 01:56:00","unix_timestamp":1626479760,"question_images":[],"exam_id":7,"choices":{"D":"Allow all traffic on port 443 into the network.","B":"Allow all traffic into the network.","A":"Block all traffic on port 443.","C":"Allow traffic on port 443 for a specific tag."},"discussion":[{"upvote_count":"7","content":"It depends on which authentication we are talking about. If it is an authentication to internal app, the answer is C (with specific tag). If it is an authentication to 'the' app that HipLocal offers to general users, the answer is D (with tag, all users outside that tag will be rejected). It is not clear to me, on which tag we are talking here.","comment_id":"608022","timestamp":"1653646380.0","poster":"[Removed]"},{"content":"Selected Answer: C\nThe best answer is C. Allow traffic on port 443 for a specific tag. Here's why:\n\nSecurity: Blocking all traffic on port 443 (option A) would prevent HTTPS communication, which is essential for secure web traffic. Allowing all traffic (option B) would be extremely insecure and leave the network vulnerable to attacks. Allowing all traffic on port 443 (option D) would also be insecure, as it would allow any device to access the network on that port.\nGranular Control: Using tags to control access on port 443 (option C) provides a granular and secure approach. HipLocal can create a specific tag for authorized devices or services and only allow traffic from those tagged resources on port 443. This ensures that only trusted entities can access the network over HTTPS.","comments":[{"comment_id":"1250108","timestamp":"1721277120.0","upvote_count":"1","poster":"thewalker","content":"Example:\n\nHipLocal could create a tag called \"trusted-services\" and apply it to their web servers and load balancers. They could then configure their firewall to allow traffic on port 443 only from resources with the \"trusted-services\" tag. This would prevent unauthorized access to their network while allowing legitimate HTTPS traffic.\n\nIn summary: Option C provides the most secure and flexible approach to configuring HipLocal's firewall, allowing them to control access to their network on port 443 while maintaining security."}],"comment_id":"1250106","upvote_count":"1","timestamp":"1721277120.0","poster":"thewalker"},{"comment_id":"1011923","timestamp":"1695186120.0","content":"Selected Answer: C\nC is correct","upvote_count":"1","poster":"__rajan__"},{"comment_id":"892073","content":"Selected Answer: C\napp is running on compute engine\ni assume nginx Is running on compute instance and you need to expose 443 and 80 for network tag","poster":"closer89","upvote_count":"1","timestamp":"1683545160.0"},{"timestamp":"1660974960.0","comment_id":"649249","upvote_count":"1","content":"Selected Answer: C\nC is correct","poster":"tomato123"},{"comment_id":"579202","poster":"dishum","upvote_count":"1","content":"C is correct","timestamp":"1648773960.0"},{"poster":"syu31svc","upvote_count":"3","comments":[{"poster":"syu31svc","comments":[{"content":"I would take C, to use tags as well, so that only traffic to selected VMs is allowed from outside, probably you don't want to expose every VM via port 443? \nhttps://cloud.google.com/vpc/docs/add-remove-network-tags","comment_id":"529989","timestamp":"1642872600.0","poster":"p4","upvote_count":"7"}],"upvote_count":"4","comment_id":"417798","content":"On second thought, correct answer is D as the application needs to be exposed externally the port 443 can be opened for all traffic.","timestamp":"1627723980.0"}],"content":"Port 443 -> HTTPS\n\nBlocking traffic on 443 does not make sense so A is wrong\nAllow all traffic is definitely not secure so B is out too\n\nBetween C and D I'll take C","comment_id":"408142","timestamp":"1626479760.0"}],"answer_ET":"C","answer_description":"","topic":"1","answer":"C","isMC":true,"answer_images":[]},{"id":"exE3kH6l7Or6N3IvqRmT","question_id":320,"answer_description":"","isMC":true,"topic":"1","timestamp":"2020-12-22 19:44:00","exam_id":7,"url":"https://www.examtopics.com/discussions/google/view/40579-exam-professional-cloud-developer-topic-1-question-82/","discussion":[{"comment_id":"254617","poster":"MickeyRourke","content":"Answer is B . Data loss prevention api is used for de-identification not natural language api","timestamp":"1609228200.0","upvote_count":"19"},{"poster":"thewalker","timestamp":"1721277360.0","comment_id":"1250114","upvote_count":"1","comments":[{"comment_id":"1250115","timestamp":"1721277360.0","upvote_count":"1","poster":"thewalker","content":"Why other options are less suitable:\n\nA. Cloud Data Loss Prevention API for redaction: Redaction involves completely removing sensitive information from a dataset. While this can be effective, it might not be the best approach for user reviews, as it could remove valuable context and insights.\nC. Cloud Natural Language Processing API for redaction: The Natural Language Processing API is designed for understanding and analyzing text. It's not specifically designed for data protection or de-identification.\nD. Cloud Natural Language Processing API for de-identification: The Natural Language Processing API is not equipped for de-identification. It's primarily focused on tasks like sentiment analysis, entity recognition, and text summarization."}],"content":"Selected Answer: B\nThe best answer here is B. Use the Cloud Data Loss Prevention API for de-identification of the review dataset. Here's why:\n\nData Loss Prevention API: This API is specifically designed for identifying and protecting sensitive data. It can be used to de-identify data, which means replacing sensitive information with non-sensitive substitutes. This is ideal for user reviews, as they might contain personal information like names, addresses, or other details that need to be protected.\nDe-identification: De-identification is the process of removing or replacing sensitive information in a dataset while preserving its usefulness for analysis. This is crucial for HipLocal's data science team, as they need to analyze user reviews without compromising user privacy."},{"comment_id":"1168545","upvote_count":"1","poster":"santoshchauhan","content":"Selected Answer: B\nB. Use the Cloud Data Loss Prevention API for de-identification of the review dataset.\n\nFor analyzing user reviews, especially if they contain sensitive user information, it's important to protect user privacy. The Cloud Data Loss Prevention (DLP) API provides ways to de-identify sensitive data, which includes redaction, masking, tokenization, and other transformation techniques to obscure or remove sensitive information.\n\nDe-identification refers to the process of removing or altering information that could be used to identify an individual, making the data safe for analysis without exposing personal information. This is crucial when handling user data to ensure compliance with privacy regulations and maintain user trust.","timestamp":"1709871960.0"},{"timestamp":"1707817140.0","content":"Selected Answer: B\nOf course it's DLP.\nNLP API makes no sense here.","poster":"theseawillclaim","upvote_count":"1","comment_id":"1149044"},{"comment_id":"1103669","content":"Selected Answer: B\nAnswer is B\nhttps://cloud.google.com/dlp/docs/deidentify-sensitive-data","poster":"Kadhem","timestamp":"1703275080.0","upvote_count":"1"},{"content":"Selected Answer: D\nhttps://www.exam-answer.com/hiplocal-data-preparation\n\n\"De-identification is the process of removing or obfuscating personally identifiable information (PII) from a dataset, so that individuals cannot be identified. In this case, the data science team needs to analyze user reviews, which could potentially contain PII such as names, email addresses, or other personal information. To protect the privacy of the users, the data should be de-identified before it is analyzed.\n\nThe Cloud Natural Language Processing API provides various features such as entity recognition, sentiment analysis, and syntax analysis. The API also includes a feature for de-identification, which can be used to remove PII from text data. This feature uses machine learning models to identify and mask or replace PII in the text.\n\nIn contrast, the Cloud Data Loss Prevention API is designed to identify and redact sensitive data, such as credit card numbers, social security numbers, or other types of PII. It is not intended for general de-identification of text data.\"","timestamp":"1700505360.0","comment_id":"1075725","poster":"wanrltw","upvote_count":"1"},{"comment_id":"1011925","poster":"__rajan__","content":"Selected Answer: D\nD is correct.","upvote_count":"1","timestamp":"1695186300.0"},{"poster":"jason0001","comment_id":"878498","content":"Selected Answer: D\nThe Cloud Natural Language Processing API can help to extract insights from the user reviews, such as sentiment analysis and entity recognition. Additionally, de-identification can help to protect user privacy by removing any personal information from the review data.","upvote_count":"1","timestamp":"1682261820.0"},{"comment_id":"649250","upvote_count":"1","content":"Selected Answer: B\nB is correct","poster":"tomato123","timestamp":"1660975020.0"},{"timestamp":"1653737040.0","comment_id":"608371","upvote_count":"1","poster":"[Removed]","comments":[{"comment_id":"608372","timestamp":"1653737400.0","poster":"[Removed]","upvote_count":"2","content":"B. \nI suspect this is more an English problem than a cloud problem. \"redaction of the review dataset\" means removing the review itself. \"de-identification of the review dataset\" means you keep the review text itself, but mask the reviewer's identity so that we do not know any more who wrote it."}],"content":"It looks like 'redaction' is a type of 'de-identification'.\nhttps://cloud.google.com/dlp/docs/transformations-reference#redaction"},{"content":"A or B?\n\nwhat speaks for de-identification over reduction?\n\nreduction: replace sensitive data with a mask\nde-identification: replace sensitive data, while keeping possibility of re-identification by trusted party\n\nreduction protects user's data even more, whereas de-identification might be better for analyzing the data and link them together, right?","poster":"p4","comment_id":"529986","upvote_count":"1","timestamp":"1642872240.0"},{"poster":"ParagSanyashiv","upvote_count":"1","comment_id":"519691","content":"Selected Answer: B\nB is the correct answer","timestamp":"1641663840.0"},{"comment_id":"492267","content":"I would take C as the purpose is to \"analyze user reviews\". Generally there is not sensitive data in reviews so I eliminate A and B. Natural Language Processing API is for analyzing things like reviews and comments, it has nothing to do with de-identification.","poster":"Gini","timestamp":"1638433920.0","comments":[{"poster":"Gini","upvote_count":"1","content":"Reviewing this question again, the question asks \"how to prepare the data\" so I change my mind to B, to de-identify the data by Cloud Data Loss Prevention first. After that Natural Language Processing can be used to analyze the data.","comment_id":"499199","timestamp":"1639210920.0"}],"upvote_count":"1"},{"timestamp":"1635776280.0","content":"Answer B: https://cloud.google.com/dlp/docs/deidentify-sensitive-data","comment_id":"471261","upvote_count":"1","comments":[{"poster":"GoatSack","upvote_count":"1","content":"Backs up: \"Ensure compliance with regulations in the new regions (for example, GDPR).\"","timestamp":"1635776340.0","comment_id":"471264"}],"poster":"GoatSack"},{"content":"B: https://cloud.google.com/architecture/de-identification-re-identification-pii-using-cloud-dlp\nDe-identification of PII in large-scale datasets using Cloud DLP\nCloud DLP enables transformations such as redaction, masking, tokenization, bucketing, and other methods of de-identification.","poster":"celia20200410","timestamp":"1626588840.0","comment_id":"408742","upvote_count":"4"},{"comment_id":"408149","poster":"syu31svc","content":"I would take C; Cloud Natural Language Processing API for redaction\n\nData Loss Prevention or DLP is not meant for analytics so A and B are wrong while de-identification is for DLP","upvote_count":"2","timestamp":"1626480240.0"},{"poster":"fraloca","comment_id":"253312","timestamp":"1609073880.0","upvote_count":"1","content":"For me the solution is C"},{"upvote_count":"1","timestamp":"1608662640.0","comment_id":"250358","poster":"donchick","content":"I'd choose Natural Language API de-identification."}],"question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nCompany Overview -\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\nExecutive Statement -\nWe are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.\n\nSolution Concept -\nHipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.\n\nExisting Technical Environment -\nHipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:\n* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.\n* State is stored in a single instance MySQL database in GCP.\n* Data is exported to an on-premises Teradata/Vertica data warehouse.\n* Data analytics is performed in an on-premises Hadoop environment.\n* The application has no logging.\n* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.\n\nBusiness Requirements -\nHipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:\n* Expand availability of the application to new regions.\n* Increase the number of concurrent users that can be supported.\n* Ensure a consistent experience for users when they travel to different regions.\n* Obtain user activity metrics to better understand how to monetize their product.\n* Ensure compliance with regulations in the new regions (for example, GDPR).\n* Reduce infrastructure management time and cost.\n* Adopt the Google-recommended practices for cloud computing.\n\nTechnical Requirements -\n* The application and backend must provide usage metrics and monitoring.\n* APIs require strong authentication and authorization.\n* Logging must be increased, and data should be stored in a cloud analytics platform.\n* Move to serverless architecture to facilitate elastic scaling.\n* Provide authorized access to internal apps in a secure manner.\nHipLocal's data science team wants to analyze user reviews.\nHow should they prepare the data?","choices":{"D":"Use the Cloud Natural Language Processing API for de-identification of the review dataset.","B":"Use the Cloud Data Loss Prevention API for de-identification of the review dataset.","A":"Use the Cloud Data Loss Prevention API for redaction of the review dataset.","C":"Use the Cloud Natural Language Processing API for redaction of the review dataset."},"question_images":[],"answer_ET":"B","unix_timestamp":1608662640,"answer_images":[],"answer":"B","answers_community":["B (67%)","D (33%)"]}],"exam":{"isMCOnly":false,"lastUpdated":"11 Apr 2025","name":"Professional Cloud Developer","isImplemented":true,"provider":"Google","id":7,"isBeta":false,"numberOfQuestions":338},"currentPage":64},"__N_SSP":true}