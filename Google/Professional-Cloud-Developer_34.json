{"pageProps":{"questions":[{"id":"71iNds4myVuelUm48hGR","answer":"D","question_images":["https://img.examtopics.com/professional-cloud-developer/image8.png"],"url":"https://www.examtopics.com/discussions/google/view/97502-exam-professional-cloud-developer-topic-1-question-248/","isMC":true,"discussion":[{"content":"Selected Answer: D\nD is correct.","timestamp":"1727188200.0","poster":"__rajan__","upvote_count":"1","comment_id":"1015899"},{"poster":"purushi","upvote_count":"1","comment_id":"975954","timestamp":"1723144440.0","content":"Selected Answer: D\nUse local cached image to save build time."},{"comment_id":"824483","poster":"Pime13","timestamp":"1709103660.0","upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/build/docs/optimize-builds/speeding-up-builds#using_a_cached_docker_image"},{"comment_id":"797937","content":"Selected Answer: D\nOption D, adding the --cache-from argument to the Docker build step in the build config file, would be the best option to optimize deployment times.\n\nThe --cache-from argument allows you to specify a list of images that Docker should use as a cache source when building the image. If a layer in the current build matches a layer in one of the cache source images, Docker uses the cached layer instead of building it again, reducing the build time.\n\nOptions A and C may not have a significant impact on deployment times, and option B would likely add complexity and increase deployment times, as it would require deploying and managing a new Docker registry and using a VPC-based Cloud Build worker pool.","timestamp":"1707050040.0","poster":"mrvergara","upvote_count":"1"},{"timestamp":"1706746320.0","content":"Selected Answer: D\nhttps://cloud.google.com/build/docs/optimize-builds/speeding-up-builds#using_a_cached_docker_image","comment_id":"794739","upvote_count":"1","poster":"TNT87"}],"topic":"1","answers_community":["D (100%)"],"answer_description":"","question_text":"You are deploying a Python application to Cloud Run using Cloud Source Repositories and Cloud Build. The Cloud Build pipeline is shown below:\n\n//IMG//\n\n\nYou want to optimize deployment times and avoid unnecessary steps. What should you do?","answer_ET":"D","unix_timestamp":1675210320,"timestamp":"2023-02-01 01:12:00","question_id":166,"answer_images":[],"choices":{"A":"Remove the step that pushes the container to Artifact Registry.","B":"Deploy a new Docker registry in a VPC, and use Cloud Build worker pools inside the VPC to run the build pipeline.","D":"Add the --cache-from argument to the Docker build step in your build config file.","C":"Store image artifacts in a Cloud Storage bucket in the same region as the Cloud Run instance."},"exam_id":7},{"id":"Jbto488kjnSB0itB0Zq7","choices":{"C":"Deploy the application on Google Kubernetes Engine. Use the Pub/Sub API to create a pull subscription to the Pub/Sub topic and read messages from it.","B":"Deploy your code on Cloud Functions. Use a Pub/Sub trigger to invoke the Cloud Function. Use the Pub/Sub API to create a pull subscription to the Pub/Sub topic and read messages from it.","A":"Deploy the application on Compute Engine. Use a Pub/Sub push subscription to process new messages in the topic.","D":"Deploy your code on Cloud Functions. Use a Pub/Sub trigger to handle new messages in the topic."},"exam_id":7,"topic":"1","discussion":[{"timestamp":"1720800960.0","poster":"d_ella2001","upvote_count":"1","comment_id":"1246897","content":"Selected Answer: D\nThis option is the most suitable. Cloud Functions are fully managed and serverless, meaning you only incur costs when your code is executed in response to incoming messages. Using a Pub/Sub trigger ensures that the Cloud Function is invoked in real-time when new messages arrive in the topic, perfectly aligning with the requirements."},{"content":"Selected Answer: D\nD: Deploying your code on Cloud Functions and using a Pub/Sub trigger to handle new messages in the topic allows for a real-time, event-driven architecture. Cloud Functions only incur costs when invoked, which aligns with the requirement to only incur costs when new messages arrive.\n\n\nB: With Cloud Functions, there's no need to manually create a pull subscription. The Pub/Sub trigger handles the message retrieval.","timestamp":"1712709960.0","upvote_count":"2","comment_id":"1192583","poster":"alpha_canary"},{"comment_id":"1140686","upvote_count":"1","timestamp":"1707110820.0","poster":"JonathanSJ","content":"Selected Answer: D\nI will go for D."},{"timestamp":"1701632040.0","poster":"examprof","content":"Alternative D is correct.\nA \"push subscription\" (not \"pull\"!) is more suitable when messages must be processed in real-time. Message ingested in the Pub/Sub topic, message pushed and retried recurrently until acknowledged.","upvote_count":"1","comment_id":"1087071"},{"upvote_count":"1","timestamp":"1696667640.0","poster":"RaghuNanda","content":"Selected Answer: D\nNot sure why we are complicating!! D is the right option","comment_id":"1027203"},{"timestamp":"1695566040.0","content":"Selected Answer: D\nI would go with D.","comment_id":"1015902","poster":"__rajan__","upvote_count":"1"},{"content":"Selected Answer: B\nB is a very detailed answer and it is a right choice.\nD is missing info like cloud function to subscribe pub sub topic to handle new messages.","timestamp":"1691522520.0","comment_id":"975961","poster":"purushi","upvote_count":"2"},{"content":"Selected Answer: D\nSelected Answer:D\nhttps://cloud.google.com/functions/docs/calling/pubsub\nWe selected D based on our experience with Cloud Functions and the material at the URL above.\nSince messages can be obtained from Cloud Functions arguments, we are not aware of the description of Subscription.\n\"only incur costs when new messages arrive.\" so it's OK to process on the trigger.\nI don't think real time means so strictly.\nFor the life of me, I can't find any reason why D is wrong, and it seems to me that B is an error because of the extra processing.","upvote_count":"4","poster":"NewComer200","comment_id":"885684","timestamp":"1682895600.0"},{"timestamp":"1682894940.0","comment_id":"885676","content":"Selected Answer:D\nhttps://cloud.google.com/functions/docs/calling/pubsub\nWe selected D based on our experience with Cloud Functions and the material at the URL above.\nSince messages can be obtained from Cloud Functions arguments, we are not aware of the description of Subscription.","poster":"NewComer200","upvote_count":"2"},{"upvote_count":"3","timestamp":"1677746340.0","content":"took my exam yesterday (01-03-2023) and this question was there","poster":"Pime13","comment_id":"826664","comments":[{"upvote_count":"1","content":"and what is the answer? option D?","comment_id":"1085465","timestamp":"1701460440.0","poster":"imiu"}]},{"timestamp":"1675514820.0","comment_id":"797945","upvote_count":"4","poster":"mrvergara","content":"Selected Answer: B\nOption D is not ideal because using a Pub/Sub trigger to handle new messages in a topic is not the most efficient way to process messages in real time. In a trigger-based architecture, Cloud Functions are invoked only when new messages are available, so there is a possibility of delays in processing.\n\nOn the other hand, Option B provides a more efficient architecture for real-time processing. A Cloud Function is invoked for each message received in the Pub/Sub topic, providing immediate processing as messages arrive. This way, the application is independent from any other system and incurs costs only when new messages arrive, fulfilling the requirements stated in the question."},{"comment_id":"794734","content":"Selected Answer: B\nhttps://cloud.google.com/solutions/event-driven-architecture-pubsub","upvote_count":"2","poster":"TNT87","timestamp":"1675209900.0"}],"answer_description":"","question_id":167,"answers_community":["D (56%)","B (44%)"],"url":"https://www.examtopics.com/discussions/google/view/97500-exam-professional-cloud-developer-topic-1-question-249/","unix_timestamp":1675209900,"timestamp":"2023-02-01 01:05:00","question_images":[],"answer_ET":"D","answer":"D","answer_images":[],"isMC":true,"question_text":"You are developing an event-driven application. You have created a topic to receive messages sent to Pub/Sub. You want those messages to be processed in real time. You need the application to be independent from any other system and only incur costs when new messages arrive. How should you configure the architecture?"},{"id":"rOwVo5bA0JBQVuUFy5Ej","answers_community":["D (100%)"],"question_images":[],"answer_description":"","choices":{"A":"Use Cloud Function to monitor resources and raise alerts.","B":"Use Cloud Pub/Sub to monitor resources and raise alerts.","D":"Use Stackdriver Monitoring to monitor resources and raise alerts.","C":"Use Stackdriver Error Reporting to capture errors and raise alerts."},"question_id":168,"answer":"D","unix_timestamp":1590750000,"topic":"1","isMC":true,"timestamp":"2020-05-29 13:00:00","answer_images":[],"question_text":"You want to notify on-call engineers about a service degradation in production while minimizing development time.\nWhat should you do?","discussion":[{"content":"I don't think the correct answer is A) Cloud Functions are not about monitoring at all, but I have found one mention of using cloud functions for monitoring: https://cloud.google.com/solutions/serverless-web-performance-monitoring-using-cloud-functions . But the mentioned article is about WEB page performance and it does require a lot of efforts. The question does not have info about the kind of service to monitor, so I think the answer should be D) - \"Use Stackdriver Monitoring to monitor resources and raise alerts\"","upvote_count":"11","poster":"emmet","comment_id":"98176","timestamp":"1606654800.0"},{"upvote_count":"9","timestamp":"1640497080.0","comment_id":"390892","poster":"syu31svc","content":"This is D for sure"},{"timestamp":"1725721920.0","poster":"santoshchauhan","content":"Selected Answer: D\nD. Use Stackdriver Monitoring to monitor resources and raise alerts.\n\nStackdriver Monitoring provides out-of-the-box and custom monitoring capabilities for Google Cloud resources and applications. It allows you to create alerting policies that notify you when certain system metrics violate user-defined thresholds. This is a quick and effective way to set up alerts for resource monitoring and service degradation without the need for extensive development time.","upvote_count":"1","comment_id":"1168206"},{"comment_id":"1011063","content":"Selected Answer: D\nStackdriver Monitoring is the best option here.","upvote_count":"1","poster":"__rajan__","timestamp":"1710831240.0"},{"upvote_count":"1","content":"Selected Answer: D\nD\nError Reporting is not about service degradation, more, Error Reporting uses Monitoring to send alerts.\nhttps://cloud.google.com/error-reporting/docs/notifications","timestamp":"1699191000.0","comment_id":"889993","poster":"closer89"},{"comment_id":"763664","content":"Selected Answer: D\nD is correct for monitoring.\nI'm baffled by the \"correct\" answers given by the site, 80% of the time they are wrong.","timestamp":"1688289660.0","upvote_count":"1","poster":"zevexWM"},{"upvote_count":"1","comment_id":"731870","content":"Selected Answer: D\nD is correct","timestamp":"1685465460.0","poster":"jcataluna"},{"content":"Selected Answer: D\nD is correct","comment_id":"649186","timestamp":"1676878560.0","upvote_count":"3","poster":"tomato123"},{"timestamp":"1658189100.0","poster":"herocc","content":"D is right one","upvote_count":"4","comments":[{"comments":[{"timestamp":"1675180260.0","content":"Because \"service DEGRADATION\" is in question, not errors.","poster":"rzabcio","upvote_count":"1","comment_id":"640127"}],"content":"why not C?","upvote_count":"1","comment_id":"585765","timestamp":"1665752040.0","poster":"[Removed]"}],"comment_id":"527155"},{"timestamp":"1657266240.0","content":"Selected Answer: D\nStackDriver Monitoring should be used to monitor and raising the disputes.","poster":"ParagSanyashiv","upvote_count":"3","comment_id":"519424"},{"content":"This is D for sure","comment_id":"515091","poster":"Flavio80","timestamp":"1656774300.0","upvote_count":"1"},{"poster":"ralf_cc","content":"D - https://cloud.google.com/blog/products/gcp/drilling-down-into-stackdriver-service-monitoring","comment_id":"387522","upvote_count":"2","timestamp":"1640142780.0"},{"upvote_count":"5","timestamp":"1620492300.0","comment_id":"215452","content":"D is correct answer here.","poster":"saurabh1805"},{"timestamp":"1614930240.0","poster":"google_learner123","content":"Answer is D","comment_id":"173744","upvote_count":"4"}],"exam_id":7,"url":"https://www.examtopics.com/discussions/google/view/21572-exam-professional-cloud-developer-topic-1-question-25/","answer_ET":"D"},{"id":"FsM4p4465bffDIpNgJG4","discussion":[{"content":"The answer is B since GKE is integrated with Cloud Logging by default.\n\n\"By default, GKE clusters are natively integrated with Cloud Logging (and Monitoring). When you create a GKE cluster, both Monitoring and Cloud Logging are enabled by default.\"\n\"GKE deploys a per-node logging agent that reads container logs, adds helpful metadata, and then sends the logs to the logs router, which sends the logs to Cloud Logging and any of the Logging sink destinations that you have configured. Cloud Logging stores logs for the duration that you specify or 30 days by default. Because Cloud Logging automatically collects standard output and error logs for containerized processes, you can start viewing your logs as soon as your application is deployed.\"\n\nSource: https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine","poster":"aldi22","timestamp":"1681760160.0","comment_id":"873039","upvote_count":"12"},{"content":"Correct answer\nA. Update your application to output logs in JSON format, and add the necessary metadata to the JSON.\n\nA is correct because it’s the easiest way to get a rich format into Cloud Logging. GKE automatically forwards logs sent to stdout to Cloud Logging. As long as it has the right JSON format, Cloud Logging will ingest the rich message\nB is not correct because it would require a lot of extra work to replace the library, and replicate the extra information (such as pod name) that the GKE logs exporter automatically provides.\nC is not correct because this is only needed for pods (normally special/privileged pods) that write directly to the GKE file system.\nD is not correct because Cloud Logging doesn’t support ingesting CSV.\n \nhttps://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine\n \nhttps://cloud.google.com/logging/docs/structured-logging#special-payload-fields","poster":"sac60","comment_id":"1333406","timestamp":"1735461060.0","upvote_count":"1"},{"comment_id":"1299973","timestamp":"1729330860.0","upvote_count":"1","poster":"anshad666","content":"Selected Answer: B\neasy way"},{"content":"Selected Answer: B\nThis question is from the Google Official Example Questions, B is the correct answer because it is the easiest way.","poster":"cloud_unicorn_99","comment_id":"1274037","timestamp":"1724847600.0","upvote_count":"1"},{"comments":[{"comments":[{"content":"Feedback\nB is correct because it’s the easiest way to get a rich format into Cloud Logging. GKE automatically forwards logs sent to stdout to Cloud Logging. As long as it has the right JSON format, Cloud Logging will ingest the rich message\nA is not correct because it would require a lot of extra work to replace the library, and replicate the extra information (such as pod name) that the GKE logs exporter automatically provides.\nD is not correct because this is only needed for pods (normally special/privileged pods) that write directly to the GKE file system.\nC is not correct because Cloud Logging doesn’t support ingesting CSV.","poster":"d_ella2001","timestamp":"1720892940.0","comment_id":"1247410","upvote_count":"1"}],"content":"answer from google practice exam","comment_id":"1247408","poster":"d_ella2001","upvote_count":"1","timestamp":"1720892820.0"}],"upvote_count":"3","timestamp":"1720892760.0","poster":"d_ella2001","content":"Selected Answer: B\nB is correct because it’s the easiest way to get a rich format into Cloud Logging. GKE automatically forwards logs sent to stdout to Cloud Logging. As long as it has the right JSON format, Cloud Logging will ingest the rich message","comment_id":"1247406"},{"upvote_count":"1","comment_id":"1228315","content":"Selected Answer: A\nEnhanced Metadata: The Cloud Logging library can automatically include valuable metadata about each request, such as request ID, user agent, and resource information.\n\nwhy not B. Update your application to output logs in JSON format, and add the necessary metadata to the JSON: While this is possible, it requires more manual effort to format logs and ensure all the necessary metadata is included.","timestamp":"1718092440.0","poster":"pico"},{"content":"Selected Answer: B\nI will go for B.\n\n In Google Kubernetes Engine (GKE), the standard output (stdout) of containers is automatically sent to Cloud Logging. This means that if your application in GKE prints logs to standard output, these logs will be captured and can be viewed in Cloud Logging without additional configuration.\n\nAnd if the logs are in JSON is better for processing.\n\nOption A is irrelevant because the logs have been sent already to cloud logging via standar output.","timestamp":"1707111540.0","poster":"JonathanSJ","comment_id":"1140691","upvote_count":"3"},{"timestamp":"1703539440.0","poster":"Kadhem","content":"Selected Answer: A\nsince we need to update the application, the best solution between A, B and C is A.","upvote_count":"2","comment_id":"1105547"},{"timestamp":"1701648780.0","content":"Selected Answer: B\nB for me: we're looking for the simplest method and I feel it's easier to configure the existing library to output JSON and include some context metadata rather than changing every log statement to use Cloud Logging library.","comment_id":"1087222","upvote_count":"2","poster":"82e0b6209c"},{"comments":[{"upvote_count":"1","comment_id":"1087219","timestamp":"1701648720.0","poster":"82e0b6209c","content":"How is it simpler to change every logging statement to use a different library rather than configuring the existing one to ouput in JSON and automatically append context metadata? I'd go for B"}],"upvote_count":"1","poster":"purushi","content":"Selected Answer: A\nThe key here is \"use the simplest method to accomplish this...\", using Cloud logging library is a very simple and straight forward solution. If the app is running on the single and multiple VMs in a instance group, then installing the cloud logging agent must be the correct answer. This environment is GKE cluster and separate some normal VM workflow.","timestamp":"1691523060.0","comment_id":"975968"},{"content":"Selected Answer: A\nto log request metadata\nhttps://cloud.google.com/logging/docs/reference/libraries#write_request_logs","poster":"closer89","timestamp":"1681671180.0","comment_id":"872064","upvote_count":"1"},{"comment_id":"826665","poster":"Pime13","content":"took my exam yesterday (01-03-2023) and this question was there","upvote_count":"2","timestamp":"1677746340.0"},{"comments":[{"content":"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#what_logs","timestamp":"1677567840.0","upvote_count":"1","comment_id":"824488","poster":"Pime13"}],"timestamp":"1676828160.0","content":"Selected Answer: A\nWhen you write logs from your service or job, they will be picked up automatically by Cloud Logging so long as the logs are written to any of these locations:\n\nStandard output (stdout) or standard error (stderr) streams\nAny files under the /var/log directory\nsyslog (/dev/log)\nLogs written using Cloud Logging client libraries, which are available for many popular languages\n\nhttps://cloud.google.com/run/docs/logging#container-logs","poster":"Pime13","upvote_count":"1","comment_id":"814321"},{"poster":"mrvergara","timestamp":"1675515180.0","content":"Selected Answer: A\nOption D, installing the Fluent Bit agent on each of your GKE nodes, is not the most straightforward method for exporting logs to Cloud Logging, as it requires manual configuration and management of the Fluent Bit agent. While Fluent Bit can be used to collect and forward logs to Cloud Logging, it is typically used for more complex logging scenarios where custom log processing is required.\n\nUsing the Cloud Logging library, as described in Option A, is a simpler and more direct method for exporting logs to Cloud Logging, as it eliminates the need to manage an additional log agent and provides a more integrated solution for logging in a GKE environment.","upvote_count":"4","comment_id":"797950"},{"timestamp":"1675252020.0","content":"https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine","upvote_count":"1","comment_id":"795062","poster":"TNT87"},{"comments":[{"content":"Answer A not D","upvote_count":"2","comment_id":"799138","timestamp":"1675628160.0","poster":"TNT87"}],"timestamp":"1675209540.0","poster":"TNT87","comment_id":"794732","upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/run/docs/logging#container-logs"}],"choices":{"C":"Update your application to output logs in CSV format, and add the necessary metadata to the CSV.","A":"Change your application’s logging library to the Cloud Logging library, and configure your application to export logs to Cloud Logging.","B":"Update your application to output logs in JSON format, and add the necessary metadata to the JSON.","D":"Install the Fluent Bit agent on each of your GKE nodes, and have the agent export all logs from /var/log."},"answer_images":[],"answer":"A","isMC":true,"question_text":"You have an application running on Google Kubernetes Engine (GKE). The application is currently using a logging library and is outputting to standard output. You need to export the logs to Cloud Logging, and you need the logs to include metadata about each request. You want to use the simplest method to accomplish this. What should you do?","unix_timestamp":1675209540,"answers_community":["A (48%)","B (48%)","5%"],"timestamp":"2023-02-01 00:59:00","question_id":169,"exam_id":7,"url":"https://www.examtopics.com/discussions/google/view/97499-exam-professional-cloud-developer-topic-1-question-250/","answer_ET":"A","question_images":[],"topic":"1","answer_description":""},{"id":"0FgiwPdhDQFHU90v3XXz","answer_description":"","timestamp":"2023-02-01 00:47:00","answer_images":[],"exam_id":7,"answer":"B","question_text":"You are working on a new application that is deployed on Cloud Run and uses Cloud Functions. Each time new features are added, new Cloud Functions and Cloud Run services are deployed. You use ENV variables to keep track of the services and enable interservice communication, but the maintenance of the ENV variables has become difficult. You want to implement dynamic discovery in a scalable way. What should you do?","answer_ET":"B","topic":"1","question_id":170,"url":"https://www.examtopics.com/discussions/google/view/97498-exam-professional-cloud-developer-topic-1-question-251/","isMC":true,"discussion":[{"comment_id":"1016338","upvote_count":"1","timestamp":"1727234160.0","content":"Selected Answer: B\nB is correct.","poster":"__rajan__"},{"poster":"purushi","upvote_count":"1","timestamp":"1723145700.0","comments":[{"content":"One example of creating service directory to use in subsequent MS calls is seen in Microservice Orchestration Design principles and patterns.","poster":"purushi","comment_id":"978069","timestamp":"1723323000.0","upvote_count":"1"}],"content":"Selected Answer: B\nCreating a service registry is a right choice. B is correct.\nOne more way is to write the new service urls to the config server registry so that other application/services can fetch those urls dynamically as and when required.","comment_id":"975974"},{"upvote_count":"2","timestamp":"1709368680.0","content":"took my exam yesterday (01-03-2023) and this question was there","comment_id":"826661","poster":"Pime13"},{"upvote_count":"2","poster":"Pime13","content":"Selected Answer: B\nservice directory for egistration and discovery of services","comment_id":"822414","timestamp":"1708954320.0"},{"comment_id":"797977","content":"Selected Answer: B\nService Directory provides a scalable way to manage the registration and discovery of services. By creating a namespace, you can use API calls to register your Cloud Run and Cloud Functions services, and query them during runtime. This allows for dynamic discovery and eliminates the need for manually updating environment variables. Service Directory also provides features such as service health checks and metadata, which can be used to further improve the reliability and scalability of your application.","timestamp":"1707052740.0","upvote_count":"2","poster":"mrvergara"},{"content":"Selected Answer: B\nhttps://medium.com/google-cloud/fine-grained-cloud-dns-iam-via-service-directory-446058b4362e\nhttps://cloud.google.com/service-directory/docs/overview","poster":"TNT87","comment_id":"794724","timestamp":"1706744820.0","upvote_count":"2"}],"question_images":[],"choices":{"D":"Deploy Hashicorp Consul on a single Compute Engine instance. Register the services with Consul during deployment, and query during runtime.","B":"Create a Service Directory namespace. Use API calls to register the services during deployment, and query during runtime.","C":"Rename the Cloud Functions and Cloud Run services endpoint is using a well-documented naming convention.","A":"Configure your microservices to use the Cloud Run Admin and Cloud Functions APIs to query for deployed Cloud Run services and Cloud Functions in the Google Cloud project."},"unix_timestamp":1675208820,"answers_community":["B (100%)"]}],"exam":{"id":7,"numberOfQuestions":338,"name":"Professional Cloud Developer","isMCOnly":false,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Google"},"currentPage":34},"__N_SSP":true}