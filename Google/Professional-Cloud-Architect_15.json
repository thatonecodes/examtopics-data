{"pageProps":{"questions":[{"id":"I9HQfkkLTL4FvdjuGJFJ","question_images":[],"discussion":[{"upvote_count":"30","comments":[{"comment_id":"1226243","content":"you are incorrect. It is D","poster":"ccpmad","upvote_count":"1","timestamp":"1717774380.0"},{"content":"D is ok","comment_id":"150972","poster":"tartar","timestamp":"1596613440.0","upvote_count":"10"}],"timestamp":"1571763300.0","comment_id":"16767","content":"D is correct. A and B are talking about appending the file system to a new VM, not setting it at the root in a new VM set. Option C is not offered within the GCP because the image must be on the GCP platform to run the gcloud of Google Console instructions to create a VM with the image.","poster":"Eroc"},{"upvote_count":"16","comments":[{"poster":"Jack_in_Large","content":"You can't use the snapshot created by another project","comments":[{"comment_id":"184728","poster":"noussy","timestamp":"1600804800.0","comments":[{"content":"I think the question has 2 different answers now as Google improve the snapshot function.\nQuoted from the link:\n'You can create snapshots from disks even while they are attached to running instances. Snapshots are global resources, so you can use them to restore data to a new disk or instance within the same project. You can also share snapshots across projects.'","comments":[{"timestamp":"1675459260.0","upvote_count":"4","content":"B would have been the answer in the current context. But as I read carefully, it doesnt mention the step of sharing snapshot across projects. It directly expects to use the snapshot. Hence D may be the right answer!","poster":"VSMu","comment_id":"797395"}],"poster":"JasminL","upvote_count":"11","comment_id":"237919","timestamp":"1607405700.0"},{"timestamp":"1621751640.0","upvote_count":"16","content":"Only if its in the same zone: https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots\n\"Note: The disk must be in the same zone as the instance.\"\n\nBut this is not the case here, we have:\nDifferent zones and different project hence, you must use a bucket.","comment_id":"364193","poster":"ArthurL20"}],"upvote_count":"8","content":"According to the documentation we can now https://cloud.google.com/compute/docs/disks/create-snapshots"}],"timestamp":"1598263440.0","comment_id":"165066","upvote_count":"13"}],"timestamp":"1578997920.0","comment_id":"38869","content":"Why Not B.\nhttps://cloud.google.com/compute/docs/instances/create-start-instance#createsnapshot\nThis clearly tells we can use snapshot to create a VM instance, and only need a custom image if we need to create many instances. Here we are creating only one.","poster":"Sudipta"},{"poster":"Toothpick","comment_id":"1255774","timestamp":"1722007800.0","upvote_count":"2","comments":[{"timestamp":"1722008040.0","content":"Edit, \nB might even be easier , you can directly create a snapshot in a target project and when creating a new vm, simply choose it as a root disk, just tried and verified it myself now","comment_id":"1255775","upvote_count":"1","poster":"Toothpick"}],"content":"In the current state of GCP ,\nBoth B and D can be done in 3 gcloud commands each, (and both require equal amount of snapshot handling). \nD is better if you have more than one instance to create but in this case , they are both equally valid."},{"timestamp":"1722185700.0","comment_id":"1256926","upvote_count":"1","content":"U can directly create the snapshot in any target project","poster":"Toothpick"},{"upvote_count":"2","content":"B isn't correct because you have to create a disk first. You cannot create a VM from a snapshot directly.\ngcloud compute disks create DISK_NAME \\\n --source-snapshot SNAPSHOT_NAME \\\n --project SOURCE_PROJECT_ID \\ --zone ZONE","comment_id":"1107876","poster":"adoyt","timestamp":"1703775240.0"},{"timestamp":"1700384100.0","comment_id":"1074541","content":"Selected Answer: D\nhttps://cloud.google.com/compute/docs/instances/copy-vm-between-projects","upvote_count":"3","poster":"odacir"},{"content":"Selected Answer: D\nhttps://cloud.google.com/compute/docs/instances/copy-vm-between-projects\nyou have to create Image from Snapshot and share it to the destination project.","upvote_count":"3","timestamp":"1696847760.0","poster":"Prakzz","comment_id":"1028753"},{"upvote_count":"1","poster":"didek1986","timestamp":"1695300960.0","comment_id":"1013082","content":"Selected Answer: B\nB is correct"},{"upvote_count":"1","comments":[{"content":"No, B is not correct, try your answer in GCP. Yo will not see your snapshot, because in different regions you don't have visibility of snapshots, that's why we need to move it to Cloud Storage first.","timestamp":"1717774500.0","poster":"ccpmad","upvote_count":"1","comment_id":"1226244"}],"comment_id":"966918","poster":"ananta93","timestamp":"1690698900.0","content":"B is the correct answer and It is straight forward."},{"content":"B is incorrect as it doesnt create an image uses the snapshot and hence D is the only corret option","comment_id":"834906","poster":"PST21","timestamp":"1678448520.0","upvote_count":"2"},{"timestamp":"1677612960.0","upvote_count":"1","content":"Selected Answer: D\nD seems better, but B actually works too.","comment_id":"825211","poster":"AugustoKras011111"},{"poster":"romandrigo","timestamp":"1677610200.0","upvote_count":"4","content":"Selected Answer: D\nhttps://cloud.google.com/compute/docs/instances/copy-vm-between-projects#zonal-boot-disk","comment_id":"825169"},{"content":"Selected Answer: B\nB is the right one as of 01/2023","upvote_count":"2","comment_id":"788481","timestamp":"1674718380.0","poster":"Clauther"},{"poster":"n_nana","content":"Selected Answer: B\nCurrently , It is possible to create VM from snapshot within same project, different project or even different organisation. so answer B is more straight forward.","comment_id":"773632","comments":[{"poster":"n_nana","comment_id":"773634","content":"https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots\nhttps://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots_across_orgs","upvote_count":"2","timestamp":"1673536740.0"}],"upvote_count":"2","timestamp":"1673536680.0"},{"poster":"examch","content":"Selected Answer: B\nB is the correct answer,\n\nWe can create VM from snapshot across zones and regions, please read through the link,\n\nhttps://cloud.google.com/compute/docs/instances/moving-instance-across-zones#moving-an-instance-manually","timestamp":"1673173320.0","comment_id":"769279","upvote_count":"1"},{"poster":"surajkrishnamurthy","content":"Selected Answer: D\nD is the correct answer","comment_id":"745982","upvote_count":"1","timestamp":"1671102780.0"},{"comment_id":"717732","content":"Selected Answer: D\nD is correct","poster":"arpitshah20","timestamp":"1668405900.0","upvote_count":"1"},{"timestamp":"1666546800.0","comment_id":"702362","poster":"Mahmoud_E","content":"Selected Answer: D\nD is the right answer, B would be a good answer if mentioned to share the snapshot with other project","upvote_count":"1"},{"upvote_count":"1","comment_id":"695585","timestamp":"1665857100.0","poster":"AzureDP900","content":"I will go with D"},{"content":"The generic flow goes like this: \nsnapshot --> Image --> Instance Template --> MIG \n\nThe instance is created after the image without an instance template in this question. That will also work.","timestamp":"1663599180.0","upvote_count":"2","comment_id":"673390","poster":"riyer1"},{"poster":"Nirca","timestamp":"1663487400.0","content":"Selected Answer: B\nB must be the correct one","upvote_count":"1","comment_id":"672097"},{"timestamp":"1639540020.0","upvote_count":"1","poster":"vincy2202","content":"Selected Answer: D\nD is the correct answer","comment_id":"501840"},{"upvote_count":"2","poster":"joe2211","comment_id":"487187","content":"Selected Answer: D\nvote D","timestamp":"1637916000.0"},{"poster":"Omni_Omnom","timestamp":"1636934220.0","comment_id":"478388","content":"D is the correct answer. Key word - different project in the same region. https://cloud.google.com/compute/docs/instances/copy-vm-between-projects","upvote_count":"2"},{"timestamp":"1628464140.0","content":"B & D are both Correct . With Option B you can make a copy of the instance in fewer steps than Option D. \n\n(a) Create a snapshot from the instance lets say \"Snapshot-1\"\n(b) Go to \"Snapshot-1\" &\n(b) \"Create instance\" from the snapshot\n\nSnapshot does not have regional/Project limitation (as long as Projects are under the same ORG\" ) . Why B is not the right option ?","comments":[{"upvote_count":"1","content":"I agree snapshots are org level resources. They can be restored to any region","timestamp":"1631092080.0","comment_id":"441345","comments":[{"comment_id":"453951","content":"Here is the article explaining how to share snapshots across projects in the same org. \nhttps://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots","upvote_count":"1","poster":"MikeB19","timestamp":"1632903840.0"}],"poster":"MikeB19"}],"upvote_count":"3","poster":"muneebarshad","comment_id":"421868"},{"content":"Copy of production Linux virtual machine in the US-Central region:\n- Copy = Boot disk + Local VM data ( SSD / HDD ) \n- Snapshot first. We cannot boot from snapshot. Need to create an image out of it.\n- Image - Right choice. Booting is possible + data is available in the new VM in new region","timestamp":"1627208460.0","upvote_count":"3","comment_id":"413797","poster":"purushi"},{"upvote_count":"2","comment_id":"386315","content":"D is correct. see https://cloud.google.com/compute/docs/instances/copy-vm-between-projects#zonal-boot-disk","timestamp":"1624199580.0","poster":"tifo16"},{"comment_id":"375499","upvote_count":"2","poster":"Svar","timestamp":"1622935260.0","content":"C is correct. You can create a snapshot of the root disk directly, and use it to create a new VM, as long as the new VM will be created in the same US geographical place, and this is the case becase we're going from us-central to u-east region."},{"timestamp":"1621315020.0","upvote_count":"1","comment_id":"360063","content":"B - Create a snapshot of the root disk and select the snapshot as the root disk when you create a new virtual machine instance in the US-East region.","poster":"victory108","comments":[{"poster":"viv9","timestamp":"1622021700.0","upvote_count":"2","comment_id":"367007","content":"D- Disk creation from the shared snapshot is required while using snapshot across project. Since \"B\" only emphasise over snapshot sharing & not over the disk creation and then attaching it to the new instance in different project so \"D\" seems correct.\nhttps://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots"}]},{"content":"I go with D","timestamp":"1620143640.0","poster":"un","comment_id":"349574","upvote_count":"1"},{"content":"D is correct answer. B is wrong because from snapshot can not use as a the root disk. You can created snapshot in one region. share snap shot with other region, create disk from snap shot and use in VM... B is saying use snapshot as a root disk","upvote_count":"1","poster":"getAlok","comment_id":"347813","timestamp":"1619965860.0"},{"timestamp":"1619539740.0","content":"Manage and replace being the key factors.\nImage are easier to maintain due to a lifecycle, snapshots may have dependencies as well. \nB and D are possible. D is recommend due to lifecycle.","upvote_count":"2","poster":"register829","comment_id":"344089"},{"poster":"tzKhalil","comment_id":"332744","upvote_count":"1","content":"B and D is the ans","timestamp":"1618077960.0"},{"comment_id":"323108","content":"Answer is D","timestamp":"1616993160.0","poster":"Ausias18","upvote_count":"1"},{"poster":"lkjhgfdsa","upvote_count":"2","content":"It is D: https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots","comment_id":"322092","timestamp":"1616874780.0"},{"comment_id":"322088","upvote_count":"1","content":"D: https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots","timestamp":"1616874540.0","poster":"lkjhgfdsa"},{"content":"D is ok because here we are talking about setting a new VM in a different project.","comment_id":"284627","timestamp":"1612595640.0","upvote_count":"3","poster":"gauravagrawal"},{"timestamp":"1610754060.0","poster":"VenV","content":"Answer is D","comment_id":"268366","upvote_count":"2"},{"upvote_count":"2","poster":"willan","content":"Agree with D","comment_id":"267207","timestamp":"1610644680.0"},{"timestamp":"1607702340.0","comment_id":"241041","content":"ITs not D anymore \nIf your project belongs to an organization, the organization can have several other projects with varying levels of access to other projects. When you create custom images, you can share these images with other users from other projects within the organization.\nhttps://cloud.google.com/compute/docs/images/managing-access-custom-images","poster":"Bijesh","upvote_count":"1"},{"comment_id":"231321","content":"It seems D is correct, according to:\nhttps://cloud.google.com/compute/docs/machine-images#when-to-use\nPersistent disk snapshot is *not* for instance cloning and replication, but custom image is expected for this action.","poster":"aGr_pX","upvote_count":"2","timestamp":"1606763700.0"},{"comment_id":"202805","upvote_count":"2","content":"Selected D as final for me, passed exam","poster":"emybreth","timestamp":"1603138200.0"},{"timestamp":"1602512520.0","poster":"Aru23","content":"D is correct","upvote_count":"1","comment_id":"198473"},{"timestamp":"1601618220.0","upvote_count":"2","comment_id":"191429","content":"After thought.. Though it is possible to create a vm, I do not agree with 'select the snapshot as the root disk' statement, hence agree with D","poster":"shajusn"},{"content":"B should be ok.. You can create a new VM in a different project using snapshot. It is not possible to create through console, but you can use gcloud command. \ncloud compute instances create inst2 --project strategic-atom-283503 --source-snapshot https://www.googleapis.com/compute/v1/projects/new-itid/global/snapshots/snapshot-2 --subnet new-vpc","comments":[{"content":"You need to create a disk from snapshot before using it across projects. https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots. Hence D is the best answer.","comment_id":"305127","poster":"register829","upvote_count":"2","timestamp":"1615123860.0"}],"timestamp":"1601617740.0","comment_id":"191419","upvote_count":"1","poster":"shajusn"},{"comment_id":"189037","poster":"whitley030390","timestamp":"1601298300.0","upvote_count":"1","content":"I agree with D being the correct answer here."},{"poster":"AshokC","content":"D is right","upvote_count":"1","comment_id":"188007","timestamp":"1601163480.0"},{"content":"D is correct. Requirement to create a vm in diff project.","poster":"gupta2020","timestamp":"1600101360.0","upvote_count":"1","comment_id":"179416"},{"upvote_count":"1","poster":"kumarp6","content":"According to me B is the correct answer.\n\nTwo things to Consider to answer question: In \n1) Create a snapshot and create VM in US-EAST, while creating snapshot you can select to US-EAST as a region so that Snap is created. Create VM there after. Note: In this you will incur some cost, \"There may be a network transfer fee if you choose to store this snapshot in a location other than the source disk.\"\n\n2) Option D: This might be right if you sharing across projects and/or publicly. I dont see any as such requirements mentioned in Question. \n\nWhich Justify my answer, hope this helps.","timestamp":"1598959080.0","comment_id":"171318"},{"comments":[{"timestamp":"1610754000.0","poster":"VenV","content":"It clearly says different project. The answer is D","upvote_count":"1","comment_id":"268365"}],"comment_id":"171316","timestamp":"1598959020.0","content":"According to me B is the correct answer.\n\nTwo things to Consider to answer question: In \n1) Create a snapshot and create VM in US-EAST, while creating snapshot you can select to US-EAST as a region so that Snap is created. Create VM there after. Note: In this you will incur some cost, \"There may be a network transfer fee if you choose to store this snapshot in a location other than the source disk.\"\n\n2) Option D: This might be right if you sharing across projects and/or publicly. I dont see any as such requirements mentioned in Question. \n\nWhich Justify my answer, hope this helps.","poster":"kumarp6","upvote_count":"1"},{"comment_id":"151063","upvote_count":"1","poster":"teeess","content":"Answer cannot be B (because even though snapshots can be exposed across different projects, the wording says, select snapshot as the root disk (rather than the boot disk). Answer is D.","timestamp":"1596620700.0"},{"upvote_count":"5","comment_id":"143119","timestamp":"1595653800.0","poster":"anant88","content":"People are getting confused with option B and D \nB is not possible - Snapshots are not directly shared across projects (requirement is to put in a new project)"},{"comment_id":"129181","upvote_count":"1","poster":"Barry123456","content":"The answer is B. The snapshot is copied into a disk when you make the VM. The snapshot is not destroyed either, you can use it again if you like. Try it, it works fine!","timestamp":"1594145640.0"},{"comment_id":"126958","upvote_count":"1","content":"Both B and D are correct but best answer is D","poster":"Khannas","timestamp":"1593964440.0"},{"poster":"mlantonis","comment_id":"116596","timestamp":"1592843460.0","content":"At first I was confused with B, but then I figured out that Snapshots are not reachable in a different project. So you need to create an Image. D is the correct answer.","upvote_count":"1"},{"upvote_count":"2","content":"I think the key here is that the question says \"Deploy in a new project\" - so it can't be B because snapshots aren't cross-project.","poster":"devnull10","comment_id":"107837","timestamp":"1591887060.0"},{"poster":"gfhbox0083","timestamp":"1591764360.0","content":"D, for sure","comment_id":"106469","upvote_count":"1"},{"content":"D is correct. \nB is wrong as snapshots are not reachable in a different project.","comment_id":"106338","poster":"HectorLeon2099","upvote_count":"2","timestamp":"1591747500.0"},{"content":"B and D both are correct","comment_id":"104002","timestamp":"1591465500.0","upvote_count":"1","poster":"asure"},{"content":"D is the correct answer","comment_id":"100796","poster":"Nirms","upvote_count":"1","timestamp":"1591100460.0"},{"timestamp":"1590741240.0","content":"D is the correct answer","comment_id":"98117","upvote_count":"1","poster":"Ziegler"},{"poster":"Javed","upvote_count":"1","timestamp":"1590268620.0","content":"Agree D","comment_id":"94570"},{"upvote_count":"1","timestamp":"1589317020.0","content":"No doubt B is the best solution","comment_id":"87955","poster":"Jack_in_Large"},{"poster":"clouddude","content":"I am going to go with D. A and C cannot be correct because using DD in the cloud to create a boot image is not a valid pattern. B doesn't make sense because you can't boot in a new region from a snapshot in another region. D is the only answer that references the use of images.","comment_id":"86139","upvote_count":"3","timestamp":"1589045760.0"},{"comments":[{"poster":"rbrto","content":"correct images are cross proyect. the anwer is D","upvote_count":"1","timestamp":"1588634280.0","comment_id":"83855"},{"comment_id":"264157","upvote_count":"1","timestamp":"1610300100.0","poster":"fraloca","content":"Snapshots are a global resources: https://cloud.google.com/compute/docs/disks/snapshots#accessing_snapshots"}],"poster":"gcp_aws","content":"Because the question talks about creating an instance in a different project, the answer is D. You must create an image. Snapshots can not be shared across projects. Correct me if I am wrong.","comment_id":"82923","timestamp":"1588491180.0","upvote_count":"2"},{"comment_id":"67127","upvote_count":"2","timestamp":"1584919920.0","poster":"desertlotus1211","comments":[{"poster":"definepi314","content":"This should be the correct explanation.","upvote_count":"1","comment_id":"121549","timestamp":"1593310740.0"}],"content":"Answer is D:\n\nhttps://stackoverflow.com/questions/36441423/migrate-google-compute-engine-instance-to-a-different-region"},{"poster":"Rathish","content":"Ans: D\n\nMigrating VM to new region:\n\nStop the instance\nTake a snapshot of instance\nCreate an custom image from snapshot\nCreate new VM\nChange OS\nSelect Custom Image\nChoose the image created previously","comment_id":"54920","upvote_count":"2","timestamp":"1582624440.0"},{"poster":"ADVIT","content":"It's D https://cloud.google.com/compute/docs/images/export-image","comment_id":"49821","upvote_count":"2","timestamp":"1581566580.0"},{"comments":[{"timestamp":"1590255300.0","poster":"desertlotus1211","upvote_count":"4","content":"However the 'real' question is asks: 'You want to manage and replace the copy easily if there are changes on the production virtual machine'. \n\nThis ask supersede deploy the copy as a new instance in the US-East region...","comment_id":"94476"}],"upvote_count":"4","comment_id":"47993","content":"I agree with Sudipta, Google recommends you create a image of a snapshot disk if you plan on creating multiple VMs. But we're not, here it says only creating one VM. So B would be the best answer","timestamp":"1581177300.0","poster":"kmanb"},{"content":"answer: D","comment_id":"44668","timestamp":"1580387880.0","poster":"2g","upvote_count":"2"},{"upvote_count":"4","comment_id":"37782","poster":"AWS56","timestamp":"1578764400.0","content":"Agreed with D"},{"timestamp":"1576528140.0","content":"There is no argument to this, D is the correct answer","comment_id":"30179","upvote_count":"3","poster":"passnow"},{"timestamp":"1575252960.0","poster":"JJu","upvote_count":"5","content":"i think answer is D","comment_id":"25859"},{"upvote_count":"5","timestamp":"1573914060.0","comment_id":"21970","poster":"AWS56","content":"D is fine"},{"comment_id":"21534","poster":"JoeShmoe","timestamp":"1573741980.0","content":"B and D both work but better answer is D","upvote_count":"5"}],"unix_timestamp":1571763300,"answers_community":["D (72%)","B (28%)"],"choices":{"B":"Create a snapshot of the root disk and select the snapshot as the root disk when you create a new virtual machine instance in the US-East region.","C":"Create an image file from the root disk with Linux dd command, create a new virtual machine instance in the US-East region","D":"Create a snapshot of the root disk, create an image file in Google Cloud Storage from the snapshot, and create a new virtual machine instance in the US-East region using the image file the root disk.","A":"Use the Linux dd and netcat commands to copy and stream the root disk contents to a new virtual machine instance in the US-East region."},"topic":"1","question_text":"You want to make a copy of a production Linux virtual machine in the US-Central region. You want to manage and replace the copy easily if there are changes on the production virtual machine. You will deploy the copy as a new instance in a different project in the US-East region.\nWhat steps must you take?","answer_ET":"D","exam_id":4,"answer":"D","question_id":71,"isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/7018-exam-professional-cloud-architect-topic-1-question-162/","answer_description":"","timestamp":"2019-10-22 18:55:00"},{"id":"zP19rI0jeiQA4CEZNkuY","timestamp":"2019-10-22 19:28:00","question_text":"Your company runs several databases on a single MySQL instance. They need to take backups of a specific database at regular intervals. The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance.\nHow should you configure the storage?","exam_id":4,"unix_timestamp":1571765280,"answer":"B","answer_images":[],"answers_community":["B (69%)","C (20%)","6%"],"question_images":[],"answer_description":"","topic":"1","question_id":72,"url":"https://www.examtopics.com/discussions/google/view/7020-exam-professional-cloud-architect-topic-1-question-163/","answer_ET":"B","isMC":true,"choices":{"B":"Mount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the backup to Google Cloud Storage.","D":"Mount additional persistent disk volumes onto each virtual machine (VM) instance in a RAID10 array and use LVM to create snapshots to send to Cloud Storage","C":"Use gcsfise to mount a Google Cloud Storage bucket as a volume directly on the instance and write backups to the mounted location using mysqldump.","A":"Configure a cron job to use the gcloud tool to take regular backups using persistent disk snapshots."},"discussion":[{"comment_id":"25211","content":"I think it's B. If you use a tool like GCFUSE it will write immediatly to GCS which is a cost benefit because you don't need intermediate storage. In this case however \"Quickly as possible\" key for understanding. GCFUSE will write to GCS which is much slower than writing directly to an added SSD. During the write to GCS it would also execute reads for a longer period on the production database. Therefor writing to the extra SSD would be my recommended solution. Offloading from the SSD to GCS would not impact the running database because the data is already separated.","comments":[{"upvote_count":"1","poster":"heelhook_ambassador","timestamp":"1637867280.0","content":"Thanks!","comment_id":"486885"},{"upvote_count":"3","content":"We cannot attach and mount a local SSD to a running instance. I think it's C (GCFUSE)","timestamp":"1638268680.0","poster":"kvenkatasudhakar","comment_id":"490591"},{"timestamp":"1630453860.0","content":"Point for Discussion\nCan local SSD be mounted in a running instance.","upvote_count":"4","comment_id":"436746","poster":"raf2121","comments":[{"upvote_count":"2","content":"Good point, Because Local SSDs are located on the physical machine where your virtual machine instance is running, they can be created only during the instance creation process","timestamp":"1633957260.0","comment_id":"460606","poster":"JasonL_GCP"},{"upvote_count":"2","content":"Yes they can. That's precisely why it makes Local SSD a good scratch / temp storage with very high IOPS. \n\nhttps://cloud.google.com/compute/docs/disks/local-ssd#formatandmount","comments":[{"upvote_count":"3","content":"No, you cannot attach a local SSD after the instance is created. \n\"Because Local SSDs are located on the physical machine where your virtual machine instance is running, they can be created only during the instance creation process.\" \nThe above is from https://cloud.google.com/compute/docs/disks/local-ssd#formatandmount","comment_id":"521646","timestamp":"1641918900.0","poster":"nymets"}],"comment_id":"441720","poster":"pr2web","timestamp":"1631153520.0"},{"comment_id":"682953","content":"The local SSD can be created only during the VM creation process.\nAfter than you can mount disk for in the destination path for export mysqldump. gsutil is the supported tool that you may used to migrate the dump to bucket.","poster":"SerGCP","timestamp":"1664475000.0","upvote_count":"2"},{"content":"In addition the mysqldump command can be run with local ssd as destination, and NOT impacting existing DB performance by using the --databases dbname --single-transaction flag. \n\nThat said, this boils down to which is quicker to complete, a local SSD dump and gsutil to gs://bucket \nOR \ngcsfuse the bucket directly on the VM, and mysqldump with the --single-transaction command","timestamp":"1631154300.0","comments":[{"upvote_count":"5","comments":[{"comment_id":"800454","poster":"RVivek","upvote_count":"1","timestamp":"1675736700.0","content":"Locall SSD is used for backup to improve performnace. Later the backup will be moved to Cloud storage using gsutil"}],"poster":"blitzzzz","comment_id":"641409","timestamp":"1659464400.0","content":"who what to backup to a local SSD. If your instance is down, you lost all data."}],"comment_id":"441723","poster":"pr2web","upvote_count":"2"}]}],"timestamp":"1575034200.0","upvote_count":"53","poster":"hannibal1969"},{"comment_id":"54925","content":"Ans: B\nPersistent Disk snapshot not required: \"They need to take backups of a specific database at regular intervals.\"\n\n\"The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance.\"\n\nThis can be achieved by using both Local SSD & GCS Fuse (mounting GCS as directory), but as the question stats needs to complete as quickly as possible.\n\nGeneral Rule: Any addition of components introduce a latency. I could not get write throughput of GCS & Local SSD, even if we consider both provides same throughput, streaming data through network to GCS Bucket introduce latency. Attached Local SSD has advantage in this case, since there is no network involved.\n\nFrom Local SSD to GCS bucket - copy job does not impact the mysql data disk.","upvote_count":"16","timestamp":"1582625520.0","poster":"Rathish"},{"comment_id":"1364692","content":"Selected Answer: B\nB is Correct Answer\nGCS FUSE performance - https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/benchmarks.md#sequential-write\nLocal SSD Mount Performance - https://cloud.google.com/compute/docs/disks/local-ssd","upvote_count":"1","timestamp":"1741055400.0","poster":"cloud_rider"},{"timestamp":"1740465780.0","poster":"david_tay","upvote_count":"1","comment_id":"1361351","content":"Selected Answer: A\nAnswer is A since B is not possible (local SSD can only be mounted during VM creation for MySQL)."},{"poster":"Ekramy_Elnaggar","upvote_count":"2","timestamp":"1732290900.0","comments":[{"timestamp":"1737066000.0","comment_id":"1341894","upvote_count":"1","content":"D seems to be the right answer because 1) Local SSD cannot be added to a VM already running 2) GCSfuse is slow 3) Adding multiple Persistant Disks in RAID increases IOPs (Speed) to quickly get the job done 4) LVM is a linux tool used to make copy of file systems 5) The answer ends with moving data to GCS.","poster":"Clouddude123"}],"content":"Selected Answer: D\nLocal SSDs can only be attached when the VM is being created.","comment_id":"1316391"},{"content":"Selected Answer: A\n\"A\" is the only one which gives an automated way to do it. All the rest involves a person action","poster":"46f094c","upvote_count":"2","comment_id":"1232766","timestamp":"1718783700.0"},{"timestamp":"1705757400.0","upvote_count":"2","comment_id":"1127294","content":"I'll go with B","poster":"Saikatms"},{"upvote_count":"2","poster":"Santhoshsunkari","timestamp":"1705409940.0","comment_id":"1124216","content":"B,https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up"},{"upvote_count":"1","poster":"bandegg","comment_id":"1120054","content":"Selected Answer: C\nIt says a specific database, not all of it. Otherwise, why not just use the snapshots? They are no cost.","timestamp":"1704997200.0"},{"upvote_count":"1","comment_id":"1104112","timestamp":"1703347020.0","poster":"Tamim321","comments":[{"upvote_count":"1","timestamp":"1703347080.0","comment_id":"1104115","content":"Refer to link - https://cloud.google.com/compute/docs/disks/local-ssd#:~:text=You%20can%20only%20add%20Local,the%20types%20that%20do%20not.","poster":"Tamim321"}],"content":"Selected Answer: C\nYou can only add local ssd to a VM during creation.Hence going with option C"},{"comment_id":"848828","content":"Selected Answer: B\nOption B would be the best choice for this scenario. Mounting a Local SSD volume as the backup location would ensure high performance and minimal impact on disk performance, while also allowing for quick backups. After the backup is complete, using gsutil to move the backup to Google Cloud Storage would provide a reliable and secure storage location for the backups. This approach is also cost-effective, as Local SSD volumes are less expensive than persistent disks.","upvote_count":"3","poster":"JC0926","timestamp":"1679619000.0","comments":[{"poster":"Murtuza","comment_id":"1023994","content":"Local SSD are considered ephermeral and they are the most cost-effective and they are fast","timestamp":"1696341060.0","upvote_count":"1"}]},{"content":"B is incorrect. The Local SSD volumes are only available on certain instance types, and they are not suitable for long-term storage as they are ephemeral and are deleted when the instance is deleted or stopped. For long-term storage, it is recommended to use persistent disks or Google Cloud Storage.","comments":[{"content":"I guess umissed the second paryt of the answer B whic says \"After the backup is complete, use gsutil to move the backup to Google Cloud Storage\"","comment_id":"800458","timestamp":"1675736820.0","upvote_count":"1","poster":"RVivek"}],"poster":"NodummyIQ","comment_id":"763373","timestamp":"1672613520.0","upvote_count":"1"},{"comment_id":"757201","content":"Option B is the most appropriate solution in this case. Mounting a Local SSD volume as the backup location will allow the backups to be taken quickly and efficiently, as Local SSDs have very high I/O performance and low latencies. Additionally, using gsutil to move the backups to Google Cloud Storage after they have been taken will provide a secure and durable storage location for the backups.\n\nA, configuring a cron job to use the gcloud tool to take regular backups using persistent disk snapshots, may not be the most efficient option because persistent disks have relatively lower I/O performance compared to Local SSDs.\n\nC, using gcsfuse to mount a Google Cloud Storage bucket as a volume directly on the instance and writing the backups to the mounted location using mysqldump, may not be the most efficient option because the backups would need to be transferred over the network, which could impact the performance of the backups.","comments":[{"timestamp":"1672041840.0","content":"D, mounting additional persistent disk volumes onto each VM instance in a RAID10 array and using LVM to create snapshots to send to Cloud Storage, may not be the most efficient option because it would require additional disk space and setup, and LVM snapshots may not be as fast as Local SSDs for taking backups.","comment_id":"757202","poster":"omermahgoub","upvote_count":"1"}],"poster":"omermahgoub","timestamp":"1672041840.0","upvote_count":"4"},{"comment_id":"708480","content":"Selected Answer: C\nGcsfuse needs local storage for caching, usually local/non-persistent disks are used for this purpose. With gcsfuse you can have the backend storage mounted as a filesystem on the server. Mysqldump allows for hot database backups.\nOption C provides the automated solution needed to backup and store the database.\nOption B is the manual version where you need to mount the local SSD, run the backup and then transfer it to a bucket manually.","poster":"minmin2020","timestamp":"1667226000.0","upvote_count":"1"},{"poster":"AzureDP900","timestamp":"1665857220.0","upvote_count":"3","comment_id":"695587","content":"I will go with B"},{"comment_id":"671991","comments":[{"poster":"ashrafh","content":"best answer thank you","comment_id":"722447","timestamp":"1668929280.0","upvote_count":"1"}],"content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.","poster":"zellck","timestamp":"1663476960.0","upvote_count":"11"},{"comment_id":"646362","poster":"Pradeepkumar","timestamp":"1660401360.0","content":"https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up\n\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.\n\nThough it is mentioned for SQL Server, the best practices are common for most of the databases. Also it is assumed that the Local SSD are already mounted while creating the VM","upvote_count":"4"},{"content":"Selected Answer: C\n>backups of a specific database","upvote_count":"2","poster":"xman3","comment_id":"634208","timestamp":"1658343840.0"},{"comments":[{"content":"This clear confusion, Thank you.","upvote_count":"1","comment_id":"773419","poster":"n_nana","timestamp":"1673525520.0"},{"poster":"Ric350","comment_id":"632672","upvote_count":"3","timestamp":"1658083260.0","content":"Also, When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket. See here under \"formatting secondary disks, backing up.\"\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices#formatting_secondary_disks"}],"upvote_count":"5","content":"B - I think this will clear things up. Local SSD is ATTACHED when CREATING the VM. The local SSDs are just LOCATED (on the physical host) where the VM is running. See here. \nhttps://cloud.google.com/compute/docs/disks/add-local-ssd#create_local_ssd\n\nYou can have a VM with locally attached SSD in an unformatted and unmounted state or just not mounted! Maybe it was umounted and now needs to be re-mounted? Answer B says to MOUNT the local SSD. MOUNTING the SSD is done when the VM is RUNNING! We need to assume the VM was built with locally attached SSD but not formatted and mounted yet. See here.\nhttps://cloud.google.com/compute/docs/disks/add-local-ssd#format_and_mount_a_local_ssd_device!","poster":"Ric350","timestamp":"1658082840.0","comment_id":"632669"},{"comment_id":"629162","upvote_count":"2","poster":"Nirca","content":"Selected Answer: B\nIt is B. Writing to Local SSD and the fasted method. (Expansive too) Coping from SSD to GCS is slow, yet not affecting the database.","timestamp":"1657370040.0"},{"upvote_count":"4","timestamp":"1655030940.0","poster":"H_S","content":"Selected Answer: C\nThe only way to have a specific data base backup is mysqldump \nmates trust me I am a data person it can't be anything else than C","comment_id":"615270"},{"content":"Selected Answer: B\nBacking up\nBest practice: \nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.","upvote_count":"5","poster":"cmamiusa","comment_id":"586790","timestamp":"1650114720.0"},{"content":"https://cloud.google.com/database-migration/docs/mysql/mysql-dump","timestamp":"1648306500.0","comment_id":"575610","poster":"SAMBIT","upvote_count":"2"},{"upvote_count":"2","timestamp":"1648306440.0","comment_id":"575608","content":"Mysqldump does export. All other means do a backup. Backup doesnâ€™t let select specific DB which is the ask. So, answer is C.","poster":"SAMBIT"},{"content":"So every day one will attach a SSD & run gsutil to complete the work. Not practical. Hence option B is rejected.","upvote_count":"1","timestamp":"1648295100.0","poster":"SAMBIT","comment_id":"575510"},{"poster":"gcp_guy","upvote_count":"1","timestamp":"1646579700.0","comment_id":"562088","content":"B: https://cloud.google.com/compute/docs/instances/sql-server/best-practices"},{"content":"B Is the right answer.\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.","poster":"kapara","comment_id":"553529","timestamp":"1645518420.0","upvote_count":"1"},{"poster":"yeahlon","content":"Selected Answer: B\nBacking up\nBest practice: Have a plan for backups and perform backups regularly.\n\nOla Hallengren's site provides a good starting point for understanding how to implement a solid backup and maintenance plan.\n\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.\n\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices","timestamp":"1644326220.0","upvote_count":"4","comment_id":"543052"},{"upvote_count":"1","content":"Selected Answer: C\nLocal SSDs only cannot be mounted on a Running Instance-- Also a Single local SSD has a capacity of 375 GB -- Writing to a local SSD risks running out of Space if the Backup > 375 GB -- GCS is unlimited","comment_id":"531264","poster":"KevPinto","timestamp":"1643023980.0"},{"content":"Answer B","poster":"esnecho","comment_id":"517966","timestamp":"1641446220.0","upvote_count":"1"},{"upvote_count":"3","poster":"vincy2202","content":"Selected Answer: B\nB is the correct answer\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices#moving_data_files_and_log_files_to_a_new_disk","timestamp":"1639546860.0","comment_id":"501891"},{"content":"B\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices\nFrom the Docs: When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket","timestamp":"1639247820.0","poster":"shindeswap","upvote_count":"3","comment_id":"499605"},{"content":"Selected Answer: B\nB is correct","timestamp":"1639042500.0","comment_id":"497543","poster":"Bobch","upvote_count":"2"},{"content":"Selected Answer: B\nvote B","timestamp":"1637916120.0","upvote_count":"3","comment_id":"487189","poster":"joe2211"},{"comment_id":"463036","poster":"[Removed]","content":"B is correct option. Local SSD is fastest option for making the backup of running instance.","upvote_count":"1","timestamp":"1634385720.0"},{"upvote_count":"1","timestamp":"1633896480.0","comment_id":"460205","content":"local SSD - Bad choice for backup\nLocal SSDs are suitable only for temporary storage such as caches, processing space, or low value data. To store data that is not temporary or ephemeral in nature, use one of our durable storage options.\nhttps://cloud.google.com/compute/docs/disks/local-ssd\ngcfuse wouldn't cut it either - latency\nwhy not A, if regular backup is required here? on a timely schedule","poster":"rottzy"},{"comment_id":"437501","comments":[{"comment_id":"441721","comments":[{"content":"No you can not, attached them once the VM is running, you can only add them while creating the VM\n\nhttps://cloud.google.com/compute/docs/disks/add-local-ssd#create_local_ssd","upvote_count":"1","timestamp":"1650403320.0","poster":"S_marquez","comment_id":"588361"}],"content":"You're incorrect. Local SSDs can be mounted on a running VM just fine. Just be aware of the limitations if you shutdown/stop the VM then the ephemeral Local SSD will lose all data. \n\nYes they can. That's precisely why it makes Local SSD a good scratch / temp storage with very high IOPS. \n\nhttps://cloud.google.com/compute/docs/disks/local-ssd#formatandmount\n\nAnswer B is indeed correct.","upvote_count":"2","poster":"pr2web","timestamp":"1631153580.0"}],"upvote_count":"2","timestamp":"1630534680.0","poster":"sudarchary","content":"\"C\" is correct. . Local SSD cannot be mounted on running VMs"},{"timestamp":"1627361820.0","poster":"hdlife","comment_id":"415147","upvote_count":"1","content":"Got most of the Qs from here, good help. Got Qs on MountKirk Game and HRL case studies around 12-13 Qs"},{"comments":[{"content":"I considered this but why snapshot the whole disk when you just need one DB from one instance.","timestamp":"1636483440.0","poster":"AWSPro24","upvote_count":"1","comment_id":"475007"}],"comment_id":"413811","upvote_count":"3","content":"B is ok, but what about backup at regular intervals. Are we missing cron job here?","timestamp":"1627209180.0","poster":"purushi"},{"content":"B is ok, but what about backup at regular intervals. Are we missing cron job here?","timestamp":"1627208760.0","comment_id":"413805","upvote_count":"1","poster":"purushi"},{"comment_id":"400355","poster":"bala786","timestamp":"1625615880.0","upvote_count":"2","content":"Option B is correct"},{"content":"Correct Ans B - https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.","timestamp":"1623748500.0","upvote_count":"5","comment_id":"382484","poster":"VishalB"},{"timestamp":"1621316580.0","content":"B. Mount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the backup to Google Cloud Storage.","upvote_count":"2","comment_id":"360096","poster":"victory108"},{"content":"B is correct","comment_id":"350424","poster":"un","timestamp":"1620229860.0","upvote_count":"2"},{"comment_id":"347046","poster":"jchacana","timestamp":"1619878200.0","upvote_count":"4","content":"My reason to say C is that in order to perform a \"specific database backup\" you need to go with a mysqldump command. Other alternatives are all or nothing"},{"comments":[{"comment_id":"441722","poster":"pr2web","timestamp":"1631153700.0","content":"The important things is to understand the limitations of a Local SSD. If its a one time thing then yes it'll be the best option. A repeatable option requires something like Filestore instead of Local SSD, to persist through Stop/Starts. However, a local SSD will persist through a reboot. \n\nhttps://cloud.google.com/compute/docs/disks/local-ssd#formatandmount","upvote_count":"1"}],"upvote_count":"1","comment_id":"343071","poster":"kakarooky","content":"B is not.\nWhat if VM is restarted after backup done before uploading to Cloud Storage?","timestamp":"1619424300.0"},{"poster":"ansh0692","comment_id":"334881","upvote_count":"3","timestamp":"1618340040.0","content":"for B, I understand the \"quick\" part but what about backup of a specific database and in regular interval of times, (mounting local SSD seems manual and it's whole MySQL instance instead of just 1 database, overkill of storage)\nBecause of these reasons I think it's C"},{"content":"It's B.\nMysql has to do the backup to capture those changes too that might not have been flushed to disk yet, so A and D is ruled out. GCSFUSE is sloooow, so that leaves us with option B.","poster":"mrhege","comment_id":"330683","timestamp":"1617828000.0","upvote_count":"1"},{"upvote_count":"1","poster":"lynx256","timestamp":"1617027240.0","comment_id":"323535","content":"IMO - B is ok"},{"content":"I'll go with C","timestamp":"1616993460.0","upvote_count":"2","poster":"Ausias18","comment_id":"323112"},{"upvote_count":"6","poster":"AD3","comment_id":"316709","content":"C is the correct answer because it's asked that \"backup specific database\". The mysqldmp provides option to backup specific database.","timestamp":"1616366940.0"},{"poster":"kartikjena31","timestamp":"1615313520.0","comment_id":"306599","content":"B is correct Answer","upvote_count":"2","comments":[{"content":"Hi,\nHave you cleared the exam? Did your paper had questions from this site?","timestamp":"1616364240.0","poster":"padma29","upvote_count":"1","comment_id":"316691"}]},{"upvote_count":"3","poster":"nitinz","comment_id":"303136","timestamp":"1614838320.0","content":"Please read the question. The backup activity needs to complete *as quickly as possible* and __cannot be allowed to impact disk performance__.\nHow should you configure the storage?\nB. Mount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the backup to Google Cloud Storage.\n\nfuse is slow.... Take backup on SSD which is the fastest way and then use gsutil to copy it over at slow speed of network.\n\nBest answer is B"},{"timestamp":"1614811920.0","comment_id":"302914","upvote_count":"5","poster":"sekhrivijay","content":"B is correct\n\nDocumentation from GCP \nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices\nBest practice: Have a plan for backups and perform backups regularly.\n\nOla Hallengren's site provides a good starting point for understanding how to implement a solid backup and maintenance plan.\n\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket."},{"comment_id":"294976","timestamp":"1613821560.0","content":"B is the correct answer.\nC: with C the throughput may be reduced.\n(google docs): Cloud Storage FUSE has much higher latency than a local file system. As such, throughput may be reduced when reading or writing one small file at a time. Using larger files and/or transferring multiple files at a time will help to increase throughput.","upvote_count":"1","poster":"Darzan"},{"timestamp":"1612064820.0","comment_id":"280325","poster":"raz","content":"\"When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.\"","upvote_count":"2"},{"upvote_count":"1","poster":"nherrerab","timestamp":"1611331620.0","comment_id":"273837","content":"B is correct. https://cloud.google.com/sql/docs/mysql/backup-recovery/backups"},{"poster":"bnlcnd","comment_id":"273443","timestamp":"1611284760.0","content":"B could be OK. But the wording is vague. \"Backup\" is so vague. Not fit the required backup specific database.\nC is clearer. Maybe gsfuse is not fastest. but it is OK and no need to move data again. what's more, it explicitly saying to use db dump command. This is the way to backup specific database not the whole DB engine.","upvote_count":"2"},{"content":"This is a bit ambiguous question. Option C actually does impact the disk performance because of how gcsfuse is implemented. New and modified files are locally written in a temporary directory in their entirety then transferred to GCS. Therefore, while mysqldump is creating dump files, those dump files will be written to a temp directory before sent to GCS thus potentially violating the \"cannot be allowed to impact disk performance\" part of the question depending on the temp dir location. Option B will avoid impacting the disk performance of the file system but will require the restart of the VM. Since the question does not state uninterrupted operation, B seems to be more reasonable.","poster":"HKim","comment_id":"267916","timestamp":"1610714880.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"251733","comments":[{"content":"For B, it isn't only a question if you can mount the SSD, which you can, but what do you do with it once you mount it!? Do a dump of the DB hence read the DB? What does it matter if you are writing to SSD if you need to read the whole DB from \"regular\" disk? \n\nI agree it is A simply because as you say it utilizes snapshots which are faster than any copy backup operation and because it is the ONLY method which mentions how to schedule periodic backups","upvote_count":"1","poster":"mgm7","comment_id":"495370","timestamp":"1638818940.0"}],"timestamp":"1608838860.0","content":"I think its A. Because regaular snapshots using cron will take less time as incremental backups will be taken and with instance running. \nB: How do you mount Local SSD in the existing instance? \nC: Cloud Storage FUSE could work however as per the documentation: \"Performance: Cloud Storage FUSE has much higher latency than a local file system. As such, throughput may be reduced when reading or writing one small file at a time. Using larger files and/or transferring multiple files at a time will help to increase throughput.\" So should take time while backing up. \nD: Mounting additional PDisk in VM and use gsutil might be too much to ask for.","poster":"Arimaverick"},{"upvote_count":"1","comment_id":"251641","poster":"Prakzz","timestamp":"1608825540.0","content":"It should be D as it is talking about a regular backup not a one time backup"},{"timestamp":"1607694900.0","upvote_count":"1","content":"Since backup has to be taken at regular intervals for a specific database either of B and C can work. If the database is very big then B will be better.","poster":"Surjit24","comment_id":"240976"},{"comment_id":"237563","timestamp":"1607365440.0","upvote_count":"1","poster":"svjl","content":"B:\nCloud Storage FUSE has much higher latency than a local file system.\nThe gsutil rsync command can be particularly affected by latency because it reads and writes one file at a time. Using the top-level -m flag with the command is often faster than fuse\nSmall random reads are slow due to latency to first byte (don't run a database over Cloud Storage FUSE!)\nhttps://cloud.google.com/storage/docs/gcs-fuse"},{"poster":"pepYash","content":"C is correct.\nhttps://cloud.google.com/compute/docs/disks/local-ssd#create_local_ssd","comment_id":"214740","timestamp":"1604767620.0","upvote_count":"2"},{"comments":[{"content":"it doesn't say \"add\" it says \"mount\" which can be the command... so it probably has already had it attached, doesn't need to stop the VM","comment_id":"210334","timestamp":"1604220120.0","poster":"aloha","upvote_count":"1"}],"upvote_count":"2","content":"As Local SSD can only be added during vm creation, I will choose C. But in real life I would be adding a SSD Persistent disk for backups and then move to Cloud Storage.","comment_id":"206599","timestamp":"1603746600.0","poster":"mexblood1"},{"content":"I think the correct answer is C since FUSE is the open-source used to mount any GCS bucket on Linux VM or Mac OS servers as a file system server.\n\nhttps://cloud.google.com/storage/docs/gcs-fuse","comment_id":"202306","poster":"gcparchitect007","upvote_count":"1","timestamp":"1603060680.0"},{"timestamp":"1602571560.0","comment_id":"199011","upvote_count":"1","content":"C is the correct ans.Use gcsfuse to mount a Google Cloud Storage","poster":"Aru23"},{"content":"B is correct","poster":"Aru23","upvote_count":"1","timestamp":"1602513120.0","comment_id":"198478"},{"upvote_count":"1","timestamp":"1601299140.0","poster":"whitley030390","comment_id":"189055","content":"Hard to argue that B isn't correct since gcp's best practices indicate that it is."},{"comment_id":"188009","upvote_count":"1","content":"B is correct","timestamp":"1601163900.0","poster":"AshokC"},{"comment_id":"179233","poster":"itsmebharat","upvote_count":"1","timestamp":"1600082340.0","content":"B is correct \n\"When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.\"\n\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices"},{"timestamp":"1598637240.0","content":"For me the correct answer is B.\nA: discarded because the backup of a specific database is requested.\nC: Discarded because \"he backup activity needs to complete as quickly as possible\" and GCS bucket is not a high performance option\nD: discarded because it does not make sense for me XDDDDD","upvote_count":"1","comment_id":"168660","poster":"Ale1973"},{"poster":"Pupina","content":"It is not A because gcloud sql backups create is for the hole instance. D is out because is a linux software-based RAID-like system. B does not mention the type of backup so I can assume that this type of backup could be for one database and faster than option c. So i go with B.","upvote_count":"1","timestamp":"1598398920.0","comment_id":"166326"},{"timestamp":"1597136820.0","comment_id":"155249","content":"I think B) is right because:\n* B) ok because Local SSD might be aggregated up to 9 TB (24 disks of 375 GB), enough for MySQL\n* D) talks about RAID 10, but you cannot configure any RAID config on persistent disks. You can only choose Zonal or Regional (Raid 1)\n* C) uses GCS Fuse that by definition is NOT efficient regarding VM resources. B) takes 2 steps, but the first one (Disk writing) is hardware, no penalty on main disk. The second one (GCS transfer) is Network transfer on Google's private Network\n* B) is ok because we are talking of backups. Even if you miss a backup is not a big deal, so, we are in a situation where preemption is acceptable\n* B) Local SSD is the fastest way of writing from an app to disk. So, MySQL will be liberated sooner.","poster":"jespinosar","upvote_count":"2"},{"comment_id":"146927","upvote_count":"1","content":"It should be D:\nMysqldump provide â€œMulti-threaded parallel processing of schemas and object to speed the dumping processâ€\nhttps://mydbops.wordpress.com/2015/10/14/getting-started-with-mysqlpump-2/\n\nPerformance: Cloud Storage FUSE has much higher latency than a local file system. As such, throughput may be reduced when reading or writing one small file at a time. Using larger files and/or transferring multiple files at a time will help to increase throughput.\nIndividual I/O streams run approximately as fast as gsutil.\nhttps://cloud.google.com/storage/docs/gcs-fuse\n\nOption B => max(speed limit of the write in the disk VS reading from the current databases â€“reading from disk is faster that write, but assuming that the database is in use could not be the case of the free resources) + max (speed limit for the gsutil in parallel of 10 files VS reading from SSD)\nOption D => speed limit for the gsutil in parallel of as many files as Mysqldump provide\n\nAssuming that the level of parallelism of Mysqldump is equal or more than 10, which is the parallelism that gsutil provide, I will chose D.","poster":"Vodor","timestamp":"1596055200.0"},{"timestamp":"1595540280.0","comment_id":"142329","content":"B for sure","poster":"RajHari","upvote_count":"2"},{"timestamp":"1594877220.0","poster":"zzaric","upvote_count":"1","content":"B for sure","comment_id":"136198"},{"poster":"RM07","upvote_count":"3","comments":[{"content":"Cloud Storage FUSE has much higher latency than a local file system. Using larger files and/or transferring multiple files at a time will help to increase throughput.","poster":"Karna","comment_id":"134036","upvote_count":"1","timestamp":"1594652760.0"},{"upvote_count":"1","comment_id":"237176","content":"\"Individual I/O streams run approximately as fast as gsutil.\"\nAnswer B has also to dump to the SSD + moving dump file with gsutil, then double job","timestamp":"1607334840.0","poster":"gianberg"}],"comment_id":"130461","timestamp":"1594287600.0","content":"Correct Ans is C. Please refer https://cloud.google.com/storage/docs/gcs-fuse"},{"poster":"saurabh1805","upvote_count":"1","comment_id":"129680","content":"C is correct answer for this. You can refer below link for more details, it does not say directly but gives hint on how gsfuse to be used with cloud storage for exporting backup.","timestamp":"1594208400.0"},{"content":"I am not quite sure which one between B or C is correct.","comment_id":"119336","timestamp":"1593080100.0","upvote_count":"1","poster":"mlantonis"},{"upvote_count":"1","comment_id":"109660","timestamp":"1592078820.0","poster":"iamgcp","content":"B cannot be the answer. https://cloud.google.com/compute/docs/disks/local-ssd#create_local_ssd Because Local SSDs are located on the physical machine where your virtual machine instance is running, they can be created only during the instance creation process. So, option C seems to be correct."},{"poster":"HectorLeon2099","timestamp":"1591999980.0","comment_id":"109043","content":"I'l go with B. Cloud Fuse is never the answer. Is not ready for a productive environment.","upvote_count":"1"},{"timestamp":"1591783920.0","content":"Answer is B hands down --> https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up\n\nUse the local SSD to stage your backups and then push them to a Cloud Storage bucket.","comment_id":"106683","upvote_count":"7","poster":"syu31svc"},{"upvote_count":"1","timestamp":"1591764480.0","content":"B, for sure","poster":"gfhbox0083","comment_id":"106471"},{"comment_id":"104005","timestamp":"1591465680.0","poster":"asure","upvote_count":"1","content":"Mount a Local SSD volume as the backup location. After the backup is complete,\nuse gsutil to move the backup to Google Cloud Storage.\nThis is similar scenario described as Multitiered backup recommended by Google. Disaster Recovery\nCookbook https://cloud.google.com/solutions/disaster-recovery-cookbook"},{"content":"B is the correct answer","poster":"Nirms","upvote_count":"1","timestamp":"1591100280.0","comment_id":"100783"},{"upvote_count":"1","content":"Final Decision to go with C as per the discussion","timestamp":"1590641100.0","comment_id":"97252","poster":"AD2AD4"},{"timestamp":"1590612060.0","poster":"amralieg","comment_id":"97024","upvote_count":"1","content":"Answer is B"},{"timestamp":"1590158700.0","poster":"jamjam2020","upvote_count":"1","comment_id":"93977","content":"B is the answer"},{"poster":"deaglee","timestamp":"1590074280.0","upvote_count":"4","comment_id":"93494","content":"I'd go for B, C seems wrong because:\n1. mportant: Cloud Storage FUSE is a Google-developed and community-supported open-source tool, written in Go and hosted on GitHub. Cloud Storage FUSE is governed solely by the Apache License and not by any other terms or conditions. It is distributed as-is, without warranties of any kind.\n2. Performance: Cloud Storage FUSE has much higher latency than a local file system."},{"timestamp":"1590045000.0","upvote_count":"3","content":"I think the answer is \"C\" because:\nin \"B\" you have to write to the SSD one time and then transfer to GCS\nin \"C\" you write directly to GCS\n\nThere is no way that \"B\" is faster than \"C\".\nAlso if you stop the VM instance the local before the dump is transferred to GCS, the dump will be lost.","comment_id":"93229","poster":"misho","comments":[{"comment_id":"313205","timestamp":"1615983000.0","content":"In B you backup only an increment, but in C you dump the whole db. B is faster and requires less space on GCS.","upvote_count":"1","poster":"pawel_ski"}]},{"comment_id":"91945","timestamp":"1589879820.0","content":"I think it's B, because 1) it mentioned one DB inside a MySQL instance, which means they DONT want you to back up the entire disk; 2) \"as quickly as possible\".","upvote_count":"2","poster":"huangmeiguai"},{"timestamp":"1589821440.0","comment_id":"91513","content":"A is the answer and the preferred GCP approach. B and C are good candidates but unfortunately for the following reasons - Local SSDs can only be created during instance creation process and the volume is good for data that is ephemeral in nature (B is wrong for that reason); gcsfuse is not a GCP supported service but a community supported service that you use at your own peril (C is a wrong choice as well). Considering that we are talking of database backup which is expected to be reliable, using a cron job to schedule backup to cloud (using gsutil). This is an effective GCP file transfer supported service.\nhttps://cloud.google.com/compute/docs/disks/local-ssd\nhttps://cloud.google.com/compute/docs/disks/local-ssd#create_local_ssd\nhttps://cloud.google.com/storage/docs/gcs-fuse","comments":[{"upvote_count":"2","poster":"Ziegler","comment_id":"98156","comments":[{"poster":"Ziegler","timestamp":"1590747840.0","upvote_count":"1","comment_id":"98159","content":"mysqldump doesn't usually consume much CPU resources on modern hardware as by default it uses a single thread. This method is good for a heavily loaded server.\n\nDisk input/outputs per second (IOPS), can however increase for multiple reasons. When you back-up on the same device as the database, this produces unnecessary random IOPS. The dump is done sequentially, on a per table basis, causing a full-table scan and many buffer page misses on tables that are not fully cached in memory.\n\nIt's recommended that you back-up from a network location to remove disk IOPS on the database server, but it is vital to use a separate network card to keep network bandwidth available for regular traffic.\n\nAlthough mysqldump will by default preserve your resources for regular spindle disks and low-core hardware, this doesn't mean that concurrent dumps cannot benefit from hardware architecture like SAN, flash storage, low write workload. The back-up time would benefit from a tool such as MyDumper."}],"timestamp":"1590747540.0","content":"Forget about my previous answer, C is my updated choice."}],"upvote_count":"3","poster":"Ziegler"},{"content":"I choose C because it's the only answer that let's you dump one specific database.","poster":"clouddude","comment_id":"86165","upvote_count":"5","timestamp":"1589050260.0"},{"content":"The answers are very confusing but I would choose C as the question states \"need to take backups of a specific database\", mysqldump allows you to specify the database you want to do a backup. B is a potential answer but it only mentions \"backup\" which I am afraid it means the whole backup of the instance.","poster":"Gini","timestamp":"1588822080.0","comment_id":"84820","upvote_count":"3"},{"comments":[{"upvote_count":"2","comments":[{"content":"The question says \"mount\" not \"attach\", so the local SSD may have already attached, we just need to \"mount /dev/sdb1 /mnt/temp\" mount the volume?\n\nThe question itself may has already misleaded us... lol","poster":"aloha","timestamp":"1604219760.0","comment_id":"210330","upvote_count":"2"}],"content":"You cannot attach local SSD if an instance is already running.","poster":"Rafaa","timestamp":"1591339080.0","comment_id":"102955"}],"content":"Local SSD has some disadvantages for mounting \n1. VM can not be stopped if it has SSD mounted \n2. Local SSD can not be mounted if VM is already created . \nFor this question though B seems to be right as it is quick approach to take backups","upvote_count":"1","poster":"tomvik","timestamp":"1588687200.0","comment_id":"84144"},{"upvote_count":"1","comment_id":"82958","timestamp":"1588498020.0","content":"B is the answer.","poster":"gcp_aws"},{"upvote_count":"4","timestamp":"1580387940.0","poster":"2g","content":"answer: C\nPlease replace gcsfise by gcsfuse","comment_id":"44669"},{"poster":"natpilot","upvote_count":"3","timestamp":"1580020440.0","comment_id":"42779","content":"B is fine."},{"poster":"rockstar9622","content":"B) is correct","timestamp":"1578773580.0","upvote_count":"1","comment_id":"37835"},{"comment_id":"37783","poster":"AWS56","timestamp":"1578764520.0","upvote_count":"3","content":"Agree with B"},{"poster":"cloudryder","comment_id":"26999","content":"The requirement said \"as quickly as possible and cannot be allowed to impact disk performance\", and anything other than A will impact disk performance and cannot be as fast as A.","comments":[{"content":"Tell me you didn't just recommend disk snapshots for DB backup.","comment_id":"51214","upvote_count":"3","timestamp":"1581856560.0","comments":[{"content":"There's not necessarily anything wrong with disk snapshots for a DB backup (depends on your recovery requirements) but this question specifically talked about backing up one particular database so a snapshot isn't going to meet the criteria.","comment_id":"260274","poster":"mwilbert","upvote_count":"2","timestamp":"1609854840.0"}],"poster":"mpguard"},{"content":"But, the question also want you to backup specific database on this instance. Can you take snapshot on a specific database? I guess snapshot is on disk. Whole disk of the DB instance will be saved in the snapshot","comment_id":"273438","poster":"bnlcnd","timestamp":"1611284400.0","upvote_count":"1"}],"timestamp":"1575592080.0","upvote_count":"4"},{"poster":"JJu","upvote_count":"10","timestamp":"1575253020.0","content":"i think answer is B.","comment_id":"25861"},{"content":"C is the Answer","upvote_count":"2","comment_id":"21974","timestamp":"1573914660.0","poster":"AWS56"},{"comment_id":"21535","upvote_count":"2","poster":"JoeShmoe","timestamp":"1573742100.0","content":"A is preferred GCP approach but gcsfuse is preferred for mysql"},{"upvote_count":"7","comment_id":"19711","timestamp":"1573131840.0","poster":"KouShikyou","comments":[{"content":"Best practice: Have a plan for backups and perform backups regularly.\n\nOla Hallengren's site provides a good starting point for understanding how to implement a solid backup and maintenance plan.\n\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.","timestamp":"1596336900.0","comment_id":"148863","poster":"kaush","upvote_count":"7","comments":[{"timestamp":"1610300940.0","content":"the solution si B: https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up","comment_id":"264169","poster":"fraloca","upvote_count":"2"}]}],"content":"In case of SQL server, B is correct.\nDoes it also work for MySQL?\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices"},{"upvote_count":"1","comment_id":"16769","timestamp":"1571765280.0","poster":"Eroc","comments":[{"comment_id":"93424","content":"Anything particular you are pointing in the article? At least I am unable to find anything useful for this question.","timestamp":"1590067380.0","poster":"deaglee","upvote_count":"1"},{"comment_id":"150989","upvote_count":"6","timestamp":"1596614460.0","poster":"tartar","comments":[{"poster":"Bolek","comments":[{"comment_id":"219036","upvote_count":"1","content":"it does not say the running instance but the question is not clear. the answer can be C or B depending on how you interpret.","timestamp":"1605347220.0","poster":"certificatores"}],"timestamp":"1601380380.0","upvote_count":"2","content":"You cannot attach local SSD if an instance is already running. C is better.","comment_id":"189629"}],"content":"B is ok"},{"content":"This clearly states that \"For large-scale backup and restore, a physical backup is more appropriate, to copy the data files in their original format that can be restored quickly\". so, mysqldump is not the best option. disk backups are. Local SSD would be fastest","poster":"Ananthtm","comment_id":"223142","upvote_count":"1","timestamp":"1605829260.0"}],"content":"https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html"}]},{"id":"wFeEugjAV06hih1UM382","answer":"ABF","exam_id":4,"timestamp":"2021-06-03 09:23:00","answers_community":["ABF (54%)","BDF (30%)","Other"],"topic":"1","question_text":"You are helping the QA team to roll out a new load-testing tool to test the scalability of your primary cloud services that run on Google Compute Engine with Cloud\nBigtable.\nWhich three requirements should they include? (Choose three.)","answer_images":[],"choices":{"E":"Instrument the production services to record every transaction for replay by the load-testing tool","A":"Ensure that the load tests validate the performance of Cloud Bigtable","F":"Instrument the load-testing tool and the target services with detailed logging and metrics collection","B":"Create a separate Google Cloud project to use for the load-testing environment","D":"Ensure all third-party systems your services use is capable of handling high load","C":"Schedule the load-testing tool to regularly run against the production environment"},"url":"https://www.examtopics.com/discussions/google/view/54371-exam-professional-cloud-architect-topic-1-question-164/","question_images":[],"unix_timestamp":1622704980,"discussion":[{"timestamp":"1638639360.0","content":"after reading link: https://cloud.google.com/bigtable/docs/performance\nA:Run your typical workloads against Bigtable :Always run your own typical workloads against a Bigtable cluster when doing capacity planning, so you can figure out the best resource allocation for your applications.\nB. Create a separate Google Cloud project to use for the load-testing environment\nF : The most important/standard factor of testing, you gather logs and metrics in TEST environment for further scaling.","poster":"rishab86","comments":[{"poster":"mikesp","timestamp":"1650871500.0","content":"I agree. It is important to verity that current BitTable cluster can deal with incoming traffic: \nA cluster must have enough nodes to support its current workload and the amount of data it stores. Otherwise, the cluster might not be able to handle incoming requests, and latency could go up.\nSo although it is a managed service, it does not auto-scale.","upvote_count":"2","comment_id":"467298"},{"upvote_count":"4","comment_id":"384263","timestamp":"1639759140.0","content":"There is no relevance to D here. So ABF","poster":"AK2020"}],"upvote_count":"34","comment_id":"374461"},{"upvote_count":"10","poster":"e5019c6","comment_id":"1108018","content":"Selected Answer: BDF\nA: No. Not needed since it's a managed GCP product. It'll scale to satisfy demand.\nB: Yes. You could leave it in the same project as the app, but it'll eventually be deployed to production and be a risk if anyone accidentally runs it against prod.\nC: No. You musn't run load testing against prod.\nD: Yes. The capability of the third party systems should be tested. They are another link in the chain and if they are not up to the task, they may be replaced.\nE: No. There is no need to use real data in the requests, this is a load test, not a behavior one.\nF: Yes. Having detailed logs and metrics helps diagnosing problems during the tests.","timestamp":"1719589740.0"},{"timestamp":"1741057860.0","poster":"cloud_rider","comment_id":"1364710","content":"Selected Answer: ABF\nA - Validation of Big Table is correct\nB - Separate Project helps in ring fencing the test.\nF - To capture all the metrics","upvote_count":"1"},{"upvote_count":"2","comment_id":"1361352","timestamp":"1740466140.0","poster":"david_tay","content":"Selected Answer: ABF\nABF for sure, even Gemini explains it correctly."},{"timestamp":"1737511920.0","comment_id":"1344558","upvote_count":"4","poster":"Kayceetalks","content":"Selected Answer: ABF\nA. Ensure that the load tests validate the performance of Cloud Bigtable: This is essential. Your primary services rely on Cloud Bigtable, so the load tests must assess how Bigtable performs under stress (latency, throughput, error rates).\nB. Create a separate Google Cloud project to use for the load-testing environment: Isolation is key! A separate project prevents the load tests from interfering with your production environment and gives you better control over costs and resource usage.\nF. Instrument the load-testing tool and the target services with detailed logging and metrics collection: You need visibility! Without proper logging and metrics (e.g., request rates, error rates, resource utilization), it's impossible to analyze the results of your load tests and identify bottlenecks."},{"timestamp":"1735731960.0","comment_id":"1335181","poster":"gcloud007","comments":[{"content":"It's ABF. You have to choose the size of your BT cluster, and you can easily structure your database in an inefficient way that would cause huge performance issues. Lower on the same page you link it says this: \"Always run your own typical workloads against a Bigtable cluster when doing capacity planning, so you can figure out the best resource allocation for your applications.\"","timestamp":"1736733720.0","poster":"ryaryarya","upvote_count":"1","comment_id":"1339771"}],"content":"Selected Answer: BDF\nThe first line in the below doc reads ... \"Bigtable delivers highly predictable performance that is linearly scalable. \" then why do you need to test Bigtable performance... https://cloud.google.com/bigtable/docs/performance","upvote_count":"1"},{"comment_id":"1156030","content":"Selected Answer: ABF\nIt's ABF","poster":"Nad1122","upvote_count":"2","timestamp":"1724286480.0"},{"timestamp":"1717590720.0","content":"Selected Answer: ABF\nThe quota impact of not using an isolated project in a region with higher quota make me think about ABF","comment_id":"1088566","upvote_count":"2","poster":"guzmanelmalo"},{"timestamp":"1703776080.0","comment_id":"936689","content":"Don't think it would be this option: Ensure all third-party systems your services use is capable of handling high load. This is some extra information and we are not sure if the application in this question is even using any third party tools.","poster":"sampon279","upvote_count":"1"},{"timestamp":"1696446360.0","comment_id":"861431","poster":"jlambdan","upvote_count":"5","content":"Selected Answer: BEF\nHere is my take, I respectfully disagree with ya all :)\n\nA. Ensure that the load tests validate the performance of Cloud Bigtable Most Voted\n=> not the requirement\nB. Create a separate Google Cloud project to use for the load-testing environment Most Voted\n=> yes, you don't want to use production quota.\nC. Schedule the load-testing tool to regularly run against the production environment\n=> yes please kill the prod ! \nD. Ensure all third-party systems your services use is capable of handling high load\n=> well, that is what we shall test, so, it was more the task of the development team, not the QA team.\nE. Instrument the production services to record every transaction for replay by the load-testing tool\n=> yes, this way you can build your test dataset with realistic behavior.\nF. Instrument the load-testing tool and the target services with detailed logging and metrics collection Most Vo\n=> yes, otherwise you test for nothing, you have no data at the end to evaluate the system's performance.","comments":[{"content":"E. Instrument the production services to record every transaction for replay by the load-testing tool\n=> yes, this way you can build your test dataset with realistic behavior.\nlol you dont test on prd but still ned prd's records .... how can bro","poster":"ductrinh","upvote_count":"1","comment_id":"1019749","timestamp":"1711630620.0"}]},{"comments":[{"comment_id":"757218","upvote_count":"2","content":"Why not B, C and E:\nB: creating a separate Google Cloud project to use for the load-testing environment, could also be a good idea by not necessary in order to ensure that the load tests do not impact the performance of the production environment.\n\nC: scheduling the load-testing tool to regularly run against the production environment, is not recommended, as this could potentially impact the performance of the production environment and could lead to unexpected behavior or issues.\n\nE: instrumenting the production services to record every transaction for replay by the load-testing tool, could also be a useful requirement, as it would allow the QA team to accurately replay real-world workloads during the load tests in order to more accurately simulate the expected production environment.","poster":"omermahgoub","timestamp":"1687759860.0"}],"poster":"omermahgoub","upvote_count":"2","comment_id":"757215","timestamp":"1687759800.0","content":"Answer ADF\nA: It is important to ensure that the load-testing tool is able to accurately test the performance of Cloud Bigtable in order to ensure that it can handle the expected load.\n\nD: It is important to ensure that all third-party systems that your primary cloud services rely on are able to handle the expected load in order to avoid any potential bottlenecks or failures.\n\nF: Instrumenting the load-testing tool and the target services with detailed logging and metrics collection can provide valuable insights into the performance and behavior of the system under test, allowing the QA team to identify any potential issues or bottlenecks."},{"poster":"surajkrishnamurthy","content":"Selected Answer: ABF\nABF is the correct answer","upvote_count":"2","timestamp":"1686820980.0","comment_id":"745989"},{"comment_id":"699431","timestamp":"1681949760.0","poster":"Mahmoud_E","content":"Selected Answer: ABF\nA, B, F are the correct answer","upvote_count":"2"},{"comment_id":"699012","content":"why testing Bigtable... it's per definition of Google would absorb practically any load... don't you trust Google? :-)","upvote_count":"5","timestamp":"1681906740.0","poster":"andras"},{"upvote_count":"2","content":"ABF is right","comment_id":"695589","timestamp":"1681582140.0","poster":"AzureDP900"},{"poster":"alexandercamachop","comments":[{"poster":"AMEJack","timestamp":"1680970620.0","upvote_count":"3","comment_id":"689500","content":"It is Google best practice to create a separate project for testing"},{"upvote_count":"2","content":"hi, in the sentence it's underline that you what to test the scalability of your primary CLOUD SERVICS, so I think that D is not required. For me it's ABF","comment_id":"672017","timestamp":"1679126220.0","poster":"kiappy81"}],"timestamp":"1678581360.0","comment_id":"666491","content":"Selected Answer: ADF\nThere is no necessary reason for running it in a separate project.\nA we have to test Bigtable.\nF Important to record all the outputs and be able to review it.\nD Important to stress test third party solutions or change it.","upvote_count":"1"},{"upvote_count":"2","timestamp":"1655264820.0","content":"Selected Answer: ABF\nABF is the correct answer","poster":"vincy2202","comment_id":"501894"},{"timestamp":"1653547440.0","content":"Selected Answer: ABF\nVote ABF","upvote_count":"3","comment_id":"487193","poster":"joe2211"},{"comment_id":"470557","timestamp":"1651298220.0","poster":"MaxNRG","upvote_count":"2","content":"BCF\n1) B - you need to have a separate project for Load-Testing tool. That would at least separate role based access - dev and test. Also, test will have their own code/project/config for testing, so no any chance of collision.\n2) C - testing on production? because per Google recommendation, BigTable should be tested on production instances (not on development) and for at least 10 min / 300 GB of data. Check \"Testing Performance with Cloud Bigtable\" here. \nI understand this as a requirement for integration test for projects using BigTable. Testing of BigTable on Dev instances won't give proper results.\n3) F - collecting of metrics would be useful anyway....\nWhy not A, D, E?\n1) A - isolation testing of BigTable likely doesn't make sense, if anyway integration test will need to run. That's covered in C.\n2) D - testing of 3rd party tools in isolated mode likely is a one time effort (only useful when upgrading these tools). No point to run them regularly.\n3) E - collecting metrics on production env just to replay them on load-testing tool? What's a point? We need test max load anyway..."},{"upvote_count":"9","comment_id":"427914","poster":"PeppaPig","comments":[{"poster":"PeppaPig","content":"You won't want load testing to consume the service quotas in your product project\nhttps://cloud.google.com/docs/quota","timestamp":"1645345080.0","upvote_count":"1","comment_id":"427915"}],"content":"AB&F \nCreating a separate project is highly recommended. It gives you total isolation from your product environment, and make sure it will not share the resources with your product env such as service quota","timestamp":"1645344960.0"},{"poster":"kopper2019","timestamp":"1642516800.0","comment_id":"408940","comments":[{"poster":"firecloud","comment_id":"409507","content":"Hi Kopper please send to thecloudit@outlook.com","timestamp":"1642598640.0","upvote_count":"1"},{"upvote_count":"1","poster":"gionny","timestamp":"1643192700.0","comment_id":"414515","content":"Hi Kopper please send to giodionisi@yahoo.it"}],"content":"hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152","upvote_count":"1"},{"timestamp":"1641928740.0","poster":"k_grdn","upvote_count":"1","comment_id":"404145","content":"@kopper2019 - email k_grdn@tiscali.co.uk, many thanks."},{"comment_id":"397605","timestamp":"1641224880.0","content":"A. Ensure that the load tests validate the performance of Cloud Bigtable\nB. Create a separate Google Cloud project to use for the load-testing environment\nF. Instrument the load-testing tool and the target services with detailed logging and metrics collection","upvote_count":"5","poster":"victory108"},{"upvote_count":"3","comment_id":"394387","poster":"kopper2019","comments":[{"poster":"Braindump","upvote_count":"1","content":"Did any one got the new questions from Kopper? Plz share at nad_far@rocketmail.com","comment_id":"414493","timestamp":"1643190360.0"},{"upvote_count":"1","poster":"gcpexam_ca","timestamp":"1641132060.0","comment_id":"396844","content":"are the new questions appeared in the real exam occurred after May 1st?"}],"content":"all New Questions released in June 2021 are in Question number 3 or share you email","timestamp":"1640837880.0"},{"upvote_count":"4","content":"Answer should be BEF","timestamp":"1640775060.0","poster":"MrXBasit","comment_id":"393629"},{"timestamp":"1639643760.0","content":"CEF. \nNot A: because the request il related to the requirements the test infrastructure shall have; the purpose is not to test Cloud BigTable but the specific application !\nNot B: no need of another project (different VM is enough)\nNot D: that woud be a requirement of the developed application, not of the testing environment","comment_id":"383152","poster":"__INSIDEOUT__","comments":[{"upvote_count":"1","timestamp":"1646671320.0","content":"Yes B because you don't want to disrupt production\nNot C because of the same reason.","comment_id":"440972","poster":"JustJack21"}],"upvote_count":"3"},{"content":"ADF, A: Because if im using bigtable, is important test it too, D becuase if I don't do it the test could fail because a third cause, and F is obvious, I don't understand why you said B, I can use the same project for load test with different infrastructure and is completly right.","comments":[{"comment_id":"383905","upvote_count":"1","poster":"manhmaluc","content":"i also thing ADF is right answer","timestamp":"1639722720.0"}],"upvote_count":"4","comment_id":"375510","timestamp":"1638754800.0","poster":"Svar"},{"comment_id":"373822","content":"I agree, should be BDF","upvote_count":"4","poster":"Ashii","timestamp":"1638564900.0"},{"poster":"rishab86","comments":[{"timestamp":"1638686460.0","content":"Why are 3rd party systems relevant here?","upvote_count":"3","comment_id":"374745","poster":"[Removed]"}],"comment_id":"373336","content":"Answer should be BDF","upvote_count":"4","timestamp":"1638523380.0"}],"answer_ET":"ABF","isMC":true,"answer_description":"","question_id":73},{"id":"EOWHOR3Dy0cLSNMsMsng","timestamp":"2019-10-23 13:51:00","exam_id":4,"answers_community":["B (100%)"],"discussion":[{"timestamp":"1606411440.0","comment_id":"24647","upvote_count":"32","content":"A is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.\n\nB is correct because:-Org viewer grants the security team permissions to view the organization's display name.\n-Project viewer grants the security team permissions to see the resources within projects.\n\nC is not correct because Org admin is too broad. The security team does not need to be able to make changes to the organization.\n\nD is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.","comments":[{"comment_id":"751080","poster":"Pr44","content":"I agree.","timestamp":"1703085000.0","upvote_count":"1"}],"poster":"shandy"},{"upvote_count":"12","poster":"Eroc","comment_id":"16943","comments":[{"content":"B is ok","upvote_count":"6","poster":"tartar","timestamp":"1628152080.0","comment_id":"151005"}],"timestamp":"1603453860.0","content":"B is the best answer because according to Google documentation i is best to use predefined roles and give the every team the least amount of access. (https://cloud.google.com/iam/docs/using-iam-securely) The question states the security must be able to view things, and the viewer role allows just that."},{"poster":"Rajarshi_Bhattacharyya","upvote_count":"1","timestamp":"1738415340.0","comment_id":"1349872","content":"Selected Answer: B\nB is correct"},{"comment_id":"1047971","timestamp":"1729345860.0","content":"B, security team want to have visibility to all the project, so viewer to Org and Project is sufficiency.","poster":"tamj123","upvote_count":"1"},{"timestamp":"1702639440.0","poster":"surajkrishnamurthy","upvote_count":"1","content":"Selected Answer: B\nB is the correct answer","comment_id":"745991"},{"poster":"allen_y_q_huang","comment_id":"727392","timestamp":"1700987760.0","content":"Agree B as security team does not need Project owner permission, but why need to grant project viewer after granting organization viewer?","upvote_count":"1"},{"comment_id":"671686","timestamp":"1694969340.0","upvote_count":"1","content":"Selected Answer: B\nB. Org viewer, project viewer!","poster":"Nirca"},{"content":"Very similar question was presented on 15 July 2022 exam","poster":"mahima123k","comment_id":"632877","upvote_count":"3","comments":[{"comment_id":"633777","timestamp":"1689809220.0","upvote_count":"3","content":"Are the 260 exam topic questions enough to pass the exam?","poster":"Bill76"}],"timestamp":"1689661980.0"},{"timestamp":"1681644660.0","poster":"methamode","content":"Selected Answer: B\nB is the answer!","comment_id":"586759","upvote_count":"1"},{"upvote_count":"1","poster":"Surls","comment_id":"507700","content":"Selected Answer: B\nB is correct","timestamp":"1671788220.0"},{"timestamp":"1671087420.0","content":"Selected Answer: B\nB is the correct answer","upvote_count":"1","poster":"vincy2202","comment_id":"501923"},{"upvote_count":"1","poster":"nqthien041292","comment_id":"492645","content":"Selected Answer: B\nVote B","timestamp":"1669996440.0"},{"upvote_count":"2","timestamp":"1669547340.0","poster":"mudot","comment_id":"488069","content":"Selected Answer: B\nA is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.\n\nB is correct because:\n-Organization viewer grants the security team permissions to view the organization's display name.\n-Project viewer grants the security team permissions to see the resources within projects.\n\nC is not correct because Organization Administrator is too broad. The security team does not need to be able to make changes to the organization.\n\nD is not correct because Project Owner is too broad. The security team does not need to be able to make changes to projects."},{"upvote_count":"2","timestamp":"1657152420.0","poster":"bala786","comment_id":"400361","content":"Option B is correct as per Least Privilege"},{"poster":"victory108","comment_id":"360088","content":"B. Org viewer, project viewer","upvote_count":"2","timestamp":"1652852040.0"},{"upvote_count":"1","timestamp":"1651767840.0","comment_id":"350458","content":"B is correct","poster":"un"},{"poster":"lynx256","comment_id":"324025","upvote_count":"1","timestamp":"1648620960.0","content":"B is ok"},{"content":"Answer is B","poster":"Ausias18","comment_id":"323115","upvote_count":"1","timestamp":"1648529760.0"},{"comment_id":"289964","upvote_count":"1","timestamp":"1644809100.0","content":"\"B\" is definitely the correct answer.\nQuestion talks about visibility for security team.","poster":"Joyjit_Deb"},{"upvote_count":"1","content":"B is the best","timestamp":"1634049720.0","poster":"Aru23","comment_id":"198485"},{"upvote_count":"1","poster":"Vijen012","timestamp":"1633189920.0","comment_id":"191697","content":"B is correct"},{"timestamp":"1632700440.0","upvote_count":"1","poster":"AshokC","content":"B is correct","comment_id":"188014"},{"upvote_count":"1","timestamp":"1630838280.0","poster":"gkdinesh","comment_id":"173873","content":"agree with option B"},{"timestamp":"1628258400.0","content":"b is correct","poster":"mohitchy","comment_id":"151996","upvote_count":"1"},{"comment_id":"150645","timestamp":"1628100540.0","poster":"Ani26","content":"B \n-Org viewer grants the security team permissions to view the organization's display name.\n-Project viewer grants the security team permissions to see the resources within projects.","upvote_count":"1"},{"content":"I agree with B","comment_id":"130462","upvote_count":"1","timestamp":"1625823840.0","poster":"RM07"},{"poster":"saurabh1805","content":"This is no brainer, B will most suited answer following least privilege IAM principle.","upvote_count":"1","comment_id":"129691","timestamp":"1625744820.0"},{"poster":"mlantonis","content":"Agree B is correct","upvote_count":"1","timestamp":"1624379820.0","comment_id":"116604"},{"poster":"gfhbox0083","comment_id":"106476","content":"B, for sure","timestamp":"1623300720.0","upvote_count":"1"},{"timestamp":"1622636280.0","poster":"Nirms","content":"B is the correct answer","comment_id":"100785","upvote_count":"1"},{"poster":"Ziegler","content":"B is correct","upvote_count":"1","comment_id":"98162","timestamp":"1622284140.0"},{"upvote_count":"1","timestamp":"1621804860.0","comment_id":"94574","content":"Answer B","poster":"Javed"},{"timestamp":"1620587040.0","content":"I will go with B. A and D do not seem right because the security team has no need to own anything. I haven't heard of \"Project Browser\" so that rules out C.","comment_id":"86173","poster":"clouddude","upvote_count":"1"},{"timestamp":"1620034980.0","comment_id":"82964","poster":"gcp_aws","content":"B is the correct anwer","upvote_count":"1"},{"upvote_count":"1","comment_id":"74122","poster":"PRC","content":"Agree with B","timestamp":"1618324860.0"},{"content":"answer: B","comment_id":"44671","poster":"2g","timestamp":"1612010520.0","upvote_count":"1"},{"timestamp":"1610387160.0","upvote_count":"1","poster":"AWS56","content":"Agree with B","comment_id":"37785"},{"comment_id":"25874","poster":"JJu","upvote_count":"2","timestamp":"1606884060.0","content":"i agree B."},{"poster":"AWS56","content":"I agree, B","upvote_count":"3","timestamp":"1605537840.0","comment_id":"21977"}],"unix_timestamp":1571831460,"question_images":[],"question_text":"Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of all projects in the organization. You provision the Google Cloud Resource Manager and set up yourself as the org admin.\nWhat Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team?","answer_images":[],"choices":{"C":"Org admin, project browser","A":"Org viewer, project owner","B":"Org viewer, project viewer","D":"Project owner, network admin"},"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/7068-exam-professional-cloud-architect-topic-1-question-165/","question_id":74,"isMC":true,"answer_ET":"B","topic":"1","answer":"B"},{"id":"MCBlA2lbAvAxkQiHs0Eo","url":"https://www.examtopics.com/discussions/google/view/54372-exam-professional-cloud-architect-topic-1-question-166/","answer":"BE","exam_id":4,"answer_ET":"BE","answer_description":"","answer_images":[],"question_images":[],"answers_community":["BE (54%)","DE (33%)","10%"],"discussion":[{"comments":[{"timestamp":"1636893900.0","poster":"robotgeek","upvote_count":"2","comment_id":"478118","comments":[{"content":"But when we select E , it might auto include B . SOme VA scanning tools also do SAST.\nSo why choose B and E in that case. \nD makes more sense with E .\nAuthorised repo will add an additional layer of security with verified images and artifacts in it.","upvote_count":"2","comments":[{"comment_id":"1095027","upvote_count":"1","timestamp":"1702424160.0","content":"At work, we do B and E.","poster":"squishy_fishy"}],"comment_id":"664156","timestamp":"1662690960.0","poster":"Ishu_awsguy"}],"content":"I understand that would be a requirement for security"}],"timestamp":"1629440580.0","upvote_count":"51","comment_id":"427921","content":"B&E\nCode signing only verifies the author. In other words it only check who you are, but not what have you done","poster":"PeppaPig"},{"upvote_count":"36","comment_id":"373341","comments":[{"timestamp":"1623652920.0","upvote_count":"3","comments":[{"upvote_count":"10","timestamp":"1638280680.0","comment_id":"490734","comments":[{"upvote_count":"2","timestamp":"1662691080.0","content":"Speed will nit get hampered if the images are verified and attested. Checks need to be there. If you argument would be true than why to introduce VA scanner , as that will also induce delay in deployment.\nwhen we select E , it might auto include B . Some VA scanning tools also do SAST.\nSo why choose B and E in that case.\nD makes more sense with E .\nAuthorised repo will add an additional layer of security with verified images and artifacts in it.\nAnswer - D & E","poster":"Ishu_awsguy","comment_id":"664159"}],"poster":"ravisar","content":"Here the question is to provide solution for \"Speed and Agility\". The Binary authorization prevent unauthorized deployments in production for GKE, Anthos Servicemesh and Cloud run, however will add delay in deployment process. So D may not be suitable in this scenario. Answer is B&E."}],"comment_id":"381635","content":"Agree with this. https://cloud.google.com/container-registry/docs/container-analysis","poster":"AK2020"}],"content":"I think answer is D & E.","timestamp":"1622705280.0","poster":"rishab86"},{"timestamp":"1734691860.0","comment_id":"1329409","content":"Selected Answer: BE\nB. Source Code Security Analyzers:\nIntegrating source code security analyzers into the CI/CD pipeline helps identify vulnerabilities in the codebase early in the development cycle. This ensures that security errors are caught and addressed before they make it into production.\nE. Vulnerability Security Scanner:\nRunning a vulnerability scanner as part of the CI/CD pipeline identifies weaknesses in dependencies, configurations, and deployed artifacts. This provides an additional layer of security by detecting risks that might not be evident in the source code alone.","upvote_count":"4","poster":"balajisreenivas"},{"comment_id":"1325060","upvote_count":"1","content":"The question clearly states that \"primary business objective are release speed and agility\". To achieve this, you should have good unit tests in place (C).\nFor this reason I think BC is a more balanced choice.","poster":"Qix","timestamp":"1733927340.0"},{"content":"Selected Answer: BD\nBD, D because it enables only validated trusted images to be deployed.","upvote_count":"1","comment_id":"1319451","timestamp":"1732827060.0","poster":"vjk1991"},{"comment_id":"1316398","timestamp":"1732292400.0","content":"Selected Answer: BE\nSAST & DAST ( @aAbdelhamid: our EA Work hahahahaha )","poster":"Ekramy_Elnaggar","upvote_count":"2"},{"timestamp":"1727546460.0","poster":"wooyourdaddy","comment_id":"1290739","upvote_count":"3","content":"Selected Answer: BE\nOption D does not directly address the primary concern of reducing the chance of security errors being accidentally introduced. Hereâ€™s why:\n\nFocus on Integrity: Code signing and using a trusted binary repository primarily ensure that the code and binaries have not been tampered with and are from a trusted source. While this is important for security, it doesnâ€™t specifically target the detection and prevention of security vulnerabilities within the code itself.\n\nIndirect Impact on Security Errors: While code signing can help prevent the introduction of malicious code, it doesnâ€™t directly scan for or identify security vulnerabilities that might be accidentally introduced by developers."},{"comment_id":"1206791","timestamp":"1714891740.0","content":"Selected Answer: BE\nwhy the other options aren't as ideal:\n\nA. Ensure every code check-in is peer reviewed by a security SME: Manual reviews can become a bottleneck in agile environments and are less scalable than automated tools.\nC. Ensure you have stubs to unit test all interfaces between components: Good practice, but primarily focuses on functional rather than security testing.\nD. Enable code signing and a trusted binary repository...: Integrity checks are essential but don't directly prevent the introduction of the security errors themselves.","poster":"pico","upvote_count":"2"},{"poster":"phantomsg","content":"Selected Answer: BE\nCyber Sec professional here. Question asks to reduce chance of security errors accidentally introduced. This means to integrate Static Application Security Tests (SAST) and Dynamic Application Security Tests (DAST) as part of CI/CD pipeline. Hence B and E are the right match. D is to ensure only trusted code is deployed to production, not reduce 'security error accidentally introduced'.","upvote_count":"5","comment_id":"1173087","timestamp":"1710388980.0"},{"timestamp":"1705729800.0","upvote_count":"1","comment_id":"1127057","content":"I guess A and C are both time consuming and labor intensive. Also, aren't C stubs supposed to be used for unit tests?\n\nWhat remains is BDE.\nB is source code inspection.\nDoing D ensures that the repository is not contaminated.\nE's vulnerability scan detects whether there are any CVEs.\nI think all of them are correct. If you had to choose two, what would it be?\nIsn't it really slow if you do B and E?","poster":"OrangeTiger"},{"comment_id":"1084768","timestamp":"1701377580.0","poster":"02fc23a","upvote_count":"5","content":"Selected Answer: DE\nhttps://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services"},{"timestamp":"1698500280.0","comment_id":"1056235","upvote_count":"2","content":"Selected Answer: DE\nThe thing that makes me think D makes sense is that it ensures that only images that have passed though the configured CI/CD pipeline (with vulnerability checks) will be able to be deployed. This is better explained here: https://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-vulnerability-scanning","poster":"cchiaramelli"},{"content":"Selected Answer: DE\nhttps://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-vulnerability-scanning","comment_id":"1056233","upvote_count":"1","timestamp":"1698499980.0","poster":"cchiaramelli"},{"comment_id":"1043458","upvote_count":"2","poster":"steghe","timestamp":"1697289000.0","content":"Selected Answer: BE\nCode signing only verifies the author not content"},{"content":"DE\nhttps://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-vulnerability-scanning","comment_id":"1020568","upvote_count":"2","poster":"someone2011","timestamp":"1695971940.0"},{"poster":"sampon279","upvote_count":"2","timestamp":"1687958160.0","content":"Selected Answer: BE\ntrusted binary repository option seems a static thing. For a release if we haven not used any new packages, trusted binary repository would not add any extra value. So B&E which will are needed for every checking/release.","comment_id":"936697"},{"content":"Selected Answer: BE\nB and E is the answer for me also.","upvote_count":"2","timestamp":"1687200000.0","comment_id":"927816","poster":"red_panda"},{"poster":"mateuszma","upvote_count":"4","comment_id":"897026","timestamp":"1684005960.0","content":"Selected Answer: DE\nhere you can find why: https://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services"},{"poster":"JC0926","timestamp":"1679621520.0","content":"Selected Answer: B\nB) Using source code security analyzers as part of the CI/CD pipeline can help identify security vulnerabilities and issues early in the development process. This can help reduce the risk of security errors being accidentally introduced and ensure that security is integrated into the development process from the beginning.\n\nE) Running a vulnerability security scanner as part of the CI/CD pipeline can help identify vulnerabilities and issues in the code and infrastructure before they are deployed to production. This can help reduce the risk of security errors being accidentally introduced and ensure that security is integrated into the development process from the beginning.","comment_id":"848844","upvote_count":"1"},{"upvote_count":"1","comment_id":"839695","timestamp":"1678869720.0","poster":"WinSxS","content":"Selected Answer: BE\nB. Use source code security analyzers as part of the CI/CD pipeline\nE. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline\n\nThese actions ensure that security is integrated into the development and deployment processes and helps catch security issues early in the software development lifecycle."},{"poster":"PST21","upvote_count":"3","comment_id":"834929","content":"ChatGPT says B & E :-)","timestamp":"1678449540.0"},{"comment_id":"817135","upvote_count":"2","timestamp":"1677011160.0","poster":"nick_name_1","comments":[{"upvote_count":"1","content":"Polaris, BlackDuck, etc","comment_id":"834899","poster":"essadequeiroz","timestamp":"1678447740.0"}],"content":"B? There is no category of product called \"source code security analyzer\""},{"content":"Selected Answer: BE\nFor me, Option D, enabling code signing and a trusted binary repository integrated with your CI/CD pipeline. It's to ensure that the correct code is put in production but not link to security errors.","comment_id":"809780","timestamp":"1676481060.0","poster":"telp","upvote_count":"1"},{"comment_id":"809772","content":"a, b \ne is the same as b but source code scanning if more efficient","poster":"xval","upvote_count":"1","timestamp":"1676480640.0"},{"content":"B,D should be correct","poster":"izekc","comment_id":"796811","timestamp":"1675407780.0","upvote_count":"1"},{"comment_id":"766342","content":"Selected Answer: AB\nRemember : \"You want to reduce the chance of security errors being accidentally introduced\".\nA & B\nA: By doing a peer-review you are reducing the risk of introducing security error.\nB: Source code analyzer may detect security errors\n\nNot E : because the question is not about vulnerabilities but about introducing security errors by the developers","poster":"Medofree","upvote_count":"1","timestamp":"1672905300.0"},{"timestamp":"1672215420.0","poster":"Wael216","content":"Selected Answer: BE\nB & E \ncode signing only checks the author not what he commits","upvote_count":"1","comment_id":"759559"},{"content":"B. Use source code security analyzers as part of the CI/CD pipeline: By using source code security analyzers as part of the CI/CD pipeline, you can automatically detect and alert on security vulnerabilities in the code as it is being developed, which can help prevent security errors from being introduced.\n\nE. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline: By running a vulnerability security scanner as part of the CI/CD pipeline, you can automatically detect and alert on security vulnerabilities in the application as it is being deployed, which can help prevent security errors from being introduced.","comment_id":"757232","upvote_count":"2","poster":"omermahgoub","comments":[{"upvote_count":"3","timestamp":"1672042920.0","content":"Option A, ensuring every code check-in is peer reviewed by a security SME, could be a good way to prevent security errors from being introduced, as peer review can help catch mistakes before they are committed. However, it may not be practical to always have a security SME available to review every code check-in, especially if the company places a high value on release speed and agility.\n\nOption C, ensuring you have stubs to unit test all interfaces between components, is not directly related to preventing security errors from being introduced. While unit testing can help ensure that the code is working correctly, it is not specifically focused on security.\n\nOption D, enabling code signing and a trusted binary repository integrated with your CI/CD pipeline, could be a good way to ensure that only trusted code is being deployed. However, it may not be sufficient on its own to prevent security errors from being introduced, as it does not directly address vulnerabilities in the code itself.","poster":"omermahgoub","comment_id":"757233"}],"timestamp":"1672042920.0"},{"comment_id":"745999","timestamp":"1671103680.0","poster":"surajkrishnamurthy","content":"Selected Answer: BE\nBE is the correct answer","upvote_count":"1"},{"comment_id":"742574","upvote_count":"1","poster":"ale_brd_111","timestamp":"1670835480.0","content":"Selected Answer: BE\nB & E are the correct ones."},{"comment_id":"699439","poster":"Mahmoud_E","timestamp":"1666225740.0","content":"Selected Answer: BE\nB & E seems right, code signing is not needed here","upvote_count":"1"},{"timestamp":"1665857640.0","comment_id":"695592","poster":"AzureDP900","upvote_count":"1","content":"B and E is right choice, code signing doesn't help finding security issue"},{"timestamp":"1662397620.0","content":"Selected Answer: DE\nOnly check the source code is not enough, the analysis need to be done in the whole package level with all third part libs and , that's why E is required and A is not sufficient. After this stage, the package itself need to be sure is THE package, that's why D is required. Please review the workflow chart in https://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-vulnerability-scanning","poster":"[Removed]","upvote_count":"4","comment_id":"660386"},{"upvote_count":"1","comment_id":"656401","timestamp":"1662046320.0","content":"Selected Answer: BE\nD is not helping you to prevent security issues. It just makes it easier (or possible) to find out who introduced them.","poster":"jabrrJ68w02ond1"},{"content":"Selected Answer: BE\nI will go for B & E. D in my opinion is incorrect because Trusted Binary Repository is not same as Binary Authorization","comment_id":"653203","timestamp":"1661731380.0","poster":"pp0709","upvote_count":"3"},{"content":"Selected Answer: BD\nLet's understand the ask first. There are many automated security analyzers available for different use case such as Static Application Security Testing(SAST), Dynamic Application Security Testing(DAST), Software Composition Analysis(SCA), Interactive Application Security Testing(IAST), Runtime Application Self Protection(RASP), etc. \nBasis the available options it is all about SAST and SCA, where\nSAST: Detects open source components with known vulnerabilities in public domain. \nSCA: Detects potential vulnerabilities in the proprietary code, written in house. Can be integrated with IDEs for real time feedback.\n\nWe can rule out Option A (manual) and Option C(stubs for integration testing which is not a kind of security testing). We are not left with:\nOption B: SCA \nOption D: SAST\nOption E: SAST\n\nOption E is a subset of Option D, i.e. Binary Authorization in GCP covers SAST as one of the available features: https://cloud.google.com/binary-authorization/docs/overview#attestations","comments":[{"upvote_count":"1","timestamp":"1662967560.0","content":"amazing Ritwik! I must say I was confident about BE until I read your comments.","poster":"gee1979","comment_id":"666698"},{"poster":"GrandAM","comment_id":"855800","upvote_count":"1","content":"in support of these options chatGPT:\nBased on the given scenario, the two actions that can be taken to reduce the chance of security errors being accidentally introduced are:\n\nB. Use source code security analyzers as part of the CI/CD pipeline: By using source code security analyzers as part of the CI/CD pipeline, potential security vulnerabilities in the source code can be identified early on, allowing the development team to fix them before they become part of the production environment.\n\nD. Enable code signing and a trusted binary repository integrated with your CI/CD pipeline: By enabling code signing and using a trusted binary repository integrated with the CI/CD pipeline, the development team can ensure that the code being deployed is authentic and has not been tampered with.\n\nTherefore, the correct options are B and D.","timestamp":"1680182760.0"}],"timestamp":"1660986360.0","upvote_count":"7","comment_id":"649351","poster":"RitwickKumar"},{"upvote_count":"1","timestamp":"1660818300.0","poster":"[Removed]","content":"Selected Answer: BE\nD is not adapted in this context","comment_id":"648383"},{"comment_id":"637829","poster":"Matalf","timestamp":"1658901000.0","content":"Selected Answer: BE\nB & E IMHO","upvote_count":"1"},{"content":"It's D&E for sure. https://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-vulnerability-scanning","upvote_count":"1","comment_id":"632674","poster":"Ric350","comments":[{"content":"Also right from Google's website - https://cloud.google.com/container-registry/docs/container-analysis","upvote_count":"1","poster":"Ric350","timestamp":"1658083920.0","comment_id":"632675"}],"timestamp":"1658083800.0"},{"upvote_count":"1","comment_id":"621787","poster":"Kishanravindra","timestamp":"1656091080.0","content":"D is incorrect because even though code signing is a good practice, it slows\ndown the release speed, which is a requirement."},{"timestamp":"1653589080.0","comment_id":"607761","poster":"rahulvsharma","upvote_count":"1","content":"Selected Answer: BE\nB & E\nBinary authorization prevents only unauthorized deployments."},{"comment_id":"605350","poster":"amxexam","content":"Selected Answer: BE\nWill go wilth B & E","upvote_count":"1","timestamp":"1653215760.0"},{"timestamp":"1652248740.0","poster":"ryzior","comment_id":"599928","upvote_count":"1","content":"Selected Answer: BE\nI am not sure D is correct, you can have a vulnerable image in your trusted binary repo approved by whoever is responsible and signed properly - but still vulnerable.\nB and E are standard CI/CD procedures - 1st one static code analysis (which replaces manual SME review) and 2nd - any external binary dependencies can introduce a vulnerability in a specific case which is visible only in a built image.\nSo I'd rather opt for BE here."},{"upvote_count":"1","poster":"DarioFama23","timestamp":"1651493340.0","comment_id":"596026","content":"Selected Answer: BE\nDefinetely B and E"},{"timestamp":"1651234800.0","content":"I think the answer in real life will be B & E , but because we are in GCP exam we have to be biased and use Google tools and the source code security Analyser is third party tools so if i got this question in the exam i will go with D & E","comment_id":"594442","upvote_count":"2","poster":"kimharsh"},{"upvote_count":"2","comment_id":"583452","timestamp":"1649531820.0","poster":"cloudmon","content":"Selected Answer: DE\nD & E based on this link, which contains multiple references to using vulnerability scanning with binary authorization, such as \"Container Analysis provides vulnerability information that you can use with Binary Authorization to control deployment. Separately, Container Analysis stores trusted metadata that is used in the authorization process\":\nhttps://cloud.google.com/binary-authorization/docs/overview"},{"comment_id":"575666","upvote_count":"1","timestamp":"1648314360.0","poster":"SAMBIT","content":"Final ans CE. First check is always Unit test. Last check is CI CD."},{"comment_id":"534762","poster":"Pime13","content":"Selected Answer: BE\ni vote BE since binary authorization only works with kubernetes (GKE/Antthos) and Cloud run (https://cloud.google.com/binary-authorization) : Binary Authorization is a deploy-time security control that ensures only trusted container images are deployed on Google Kubernetes Engine (GKE) or Cloud Run.","upvote_count":"2","timestamp":"1643379480.0"},{"comment_id":"530460","timestamp":"1642935300.0","poster":"Narinder","upvote_count":"1","content":"I think BE is the correct answer\nB) For Identifying security bugs at code level, this solve the purpose of SAST (Static application security testing)\nE) Container Analysis scans new images when they're uploaded to Artifact Registry or Container Registry. This scan extracts information about the \"system packages\" in the container. \nhttps://cloud.google.com/container-analysis/docs/container-scanning-overview"},{"comment_id":"520045","upvote_count":"1","comments":[{"content":"D is must.\nWitch prefer Source Code Analize or Vulnerability Scan?Which one matches CICD?\nB Source Code Analize can passive code scan.\nE Vulnerability Scanner can find any vlunerability includ continar.Its covered wider than B.\nhttps://cloud.google.com/container-registry/docs/container-analysis\nIt seems include source code analysis.\nSo I choose D&E.","upvote_count":"2","timestamp":"1641719100.0","comment_id":"520057","poster":"OrangeTiger"}],"poster":"OrangeTiger","content":"At first, I didn't understand the difference between B and E.","timestamp":"1641718020.0"},{"comment_id":"513846","upvote_count":"2","content":"Establish security early in pipelines\n\nHave security checks and balances as early as possible in the development life cycle. By finding security risks before you build artifacts or deploy, you can reduce the time and cost spent to address these risks.\n\nTo help achieve early detection, you can implement the following security measures in your pipelines:\n\nRequire that subject matter experts review any code integrated into your production repository.\nImplement linting and static code analysis early in your pipeline. This testing helps you find weaknesses such as not escaping inputs, accepting raw input data for SQL queries, or vulnerabilities in your code.\nScan your built container image for vulnerabilities with vulnerability scanning.\nPrevent images that contain vulnerabilities from being deployed to your clusters, by using Binary Authorization. Binary Authorization requires an Anthos subscription. To provide you with higher confidence in the produced images, Binary Authorization also lets you require attestations by different entities or systems. For example, these attestations could include the following:\n\nPassed vulnerability scan\nPassed QA testing\nSign off from product owner\n\nHence B and E","timestamp":"1640920560.0","poster":"Gimyjony"},{"content":"Selected Answer: BE\nCode signing is the process of digitally signing your executables and scripts to confirm you are the software author and guarantee that the code has not been altered or corrupted since it was signed. If you are consuming binaries from external parties, it is a good idea to use signed binaries from trusted repositories but enabling the code signing on our code isn't likely to reduce the chance of security errors in our applications.","comment_id":"506330","poster":"ABO_Doma","comments":[{"poster":"Wonka","upvote_count":"1","timestamp":"1642526040.0","content":"how different B and D really is, there is an overlap really so why not choose one that addresses vulnerabilities","comment_id":"526854"}],"upvote_count":"2","timestamp":"1640109840.0"},{"content":"Selected Answer: DE\nDE should be the right answer","upvote_count":"2","poster":"vincy2202","timestamp":"1639557180.0","comment_id":"501989"},{"content":"Selected Answer: DE\nD&E is the answer here","poster":"pakilodi","upvote_count":"2","timestamp":"1638477360.0","comment_id":"492784"},{"upvote_count":"2","content":"Selected Answer: DE\nvote DE","poster":"joe2211","timestamp":"1637972940.0","comment_id":"487723"},{"poster":"nileshlg","content":"Selected Answer: DE\nI think its D & E","comment_id":"484034","upvote_count":"2","timestamp":"1637573040.0"},{"timestamp":"1635915180.0","poster":"Christoph2","comment_id":"471919","upvote_count":"4","content":"The question asks to prevent security erros accidentally being introduced. Binary signing does not prevent mistakes from happen. Therefore I am with B & E"},{"poster":"[Removed]","comment_id":"467307","timestamp":"1635147720.0","upvote_count":"2","content":"ID & E is right answer. B is merely covering the security check. D has more cover. \nD. Enable code signing and a trusted binary repository integrated with your CI/CD pipeline\nE. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline"},{"timestamp":"1633856700.0","poster":"XAvenger","comment_id":"459986","upvote_count":"8","content":"After watching \"Google Cloud CI/CD: Speed with Security and Compliance (Cloud Next â€˜19 UK)\"\nhttps://www.youtube.com/watch?v=0PY0QDNjgJ4\nI think the correct answer is D and E. \nYou can start watching from 17:05 \nIn the video it is mentioned to run vulnerability security scanner and use binary authorization as part of the CI/CD pipline"},{"comment_id":"447630","content":"D& E are correct","upvote_count":"2","poster":"CA1966","timestamp":"1632058740.0"},{"poster":"sudarchary","upvote_count":"3","timestamp":"1631396220.0","comment_id":"443181","content":"D&E is the final."},{"content":"I will go with B & E\nB â€“ Static analysis inspects your source code to identify defects, vulnerabilities, and compliance\nissues as you code without having to run the program. This makes static analysis an essential\ncomponent of your pipeline.\n https://www.perforce.com/blog/kw/static-analysis-ci-cd-pipelines\n E - Container Analysis provides vulnerability scanning and metadata storage for containers\nthrough Container Analysis. The scanning service performs vulnerability scans on images in\nArtifact Registry and Container Registry, then stores the resulting metadata and makes it\navailable for consumption through an API. Metadata storage allows storing information from\ndifferent sources, including vulnerability scanning, other Cloud services, and third-party\nproviders.\n https://cloud.google.com/container-registry/docs/container-analysis","timestamp":"1627620000.0","upvote_count":"6","comment_id":"417174","poster":"VishalB"},{"upvote_count":"3","timestamp":"1626611940.0","poster":"kopper2019","comment_id":"408934","content":"hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152"},{"poster":"victory108","comment_id":"408879","content":"D. Enable code signing and a trusted binary repository integrated with your CI/CD pipeline\nE. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline","timestamp":"1626606420.0","upvote_count":"2"},{"poster":"sha1pain","upvote_count":"3","comment_id":"408634","timestamp":"1626562500.0","content":"B & E look good. \nD is more like an authentication solution so I'll leave it."},{"comment_id":"402417","timestamp":"1625805360.0","upvote_count":"7","content":"B & E as per whizlabs","poster":"sandeep777"},{"poster":"Badugu","comment_id":"400149","content":"i also think D & E is correct","timestamp":"1625589300.0","upvote_count":"3"}],"unix_timestamp":1622705280,"isMC":true,"timestamp":"2021-06-03 09:28:00","question_id":75,"topic":"1","question_text":"Your company places a high value on being responsive and meeting customer needs quickly. Their primary business objectives are release speed and agility. You want to reduce the chance of security errors being accidentally introduced.\nWhich two actions can you take? (Choose two.)","choices":{"B":"Use source code security analyzers as part of the CI/CD pipeline","D":"Enable code signing and a trusted binary repository integrated with your CI/CD pipeline","E":"Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline","C":"Ensure you have stubs to unit test all interfaces between components","A":"Ensure every code check-in is peer reviewed by a security SME"}}],"exam":{"isBeta":false,"provider":"Google","id":4,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":false,"numberOfQuestions":279,"name":"Professional Cloud Architect"},"currentPage":15},"__N_SSP":true}