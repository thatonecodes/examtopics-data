{"pageProps":{"questions":[{"id":"so2V7z4rsapyDxuf8pal","answer_description":"","exam_id":11,"choices":{"C":"Create an application that publishes events to Cloud Pub/Sub, and create Spark jobs on Cloud Dataproc to convert the JSON data to Avro format, stored on HDFS on Persistent Disk.","D":"Create an application that publishes events to Cloud Pub/Sub, and create a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery.","B":"Create an application that writes to a Cloud SQL database to store the data. Set up periodic exports of the database to write to Cloud Storage and load into BigQuery.","A":"Create an application that provides an API. Write a tool to poll the API and write data to Cloud Storage as gzipped JSON files."},"url":"https://www.examtopics.com/discussions/google/view/17234-exam-professional-data-engineer-topic-1-question-135/","answers_community":["D (100%)"],"isMC":true,"question_images":[],"answer_ET":"D","question_text":"You are building a new application that you need to collect data from in a scalable way. Data arrives continuously from the application throughout the day, and you expect to generate approximately 150 GB of JSON data per day by the end of the year. Your requirements are:\n✑ Decoupling producer from consumer\n✑ Space and cost-efficient storage of the raw ingested data, which is to be stored indefinitely\n✑ Near real-time SQL query\n✑ Maintain at least 2 years of historical data, which will be queried with SQL\nWhich pipeline should you use to meet these requirements?","discussion":[{"timestamp":"1584870540.0","upvote_count":"44","poster":"[Removed]","comment_id":"66890","content":"Correct - D"},{"upvote_count":"16","content":"Answer: D\nDescription: All the requirements meet with D","poster":"[Removed]","comment_id":"68853","timestamp":"1585397520.0"},{"poster":"edre","comment_id":"1252845","content":"Selected Answer: D\nGoogle recommended approach","timestamp":"1721627220.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1695591120.0","comment_id":"1016195","poster":"juliorevk","content":"Selected Answer: D\nD because pub/sub decouples while dataflow processes; Cloud Storage can be used to store the raw ingested data indefinitely and BQ can be used to query."},{"upvote_count":"3","comment_id":"1015428","content":"Selected Answer: D\nHere's how this option aligns with your requirements:\nDecoupling Producer from Consumer: Cloud Pub/Sub provides a decoupled messaging system where the producer publishes events, and consumers (like Dataflow) can subscribe to these events. This decoupling ensures flexibility and scalability.\nSpace and Cost-Efficient Storage: Storing data in Avro format is more space-efficient than JSON, and Cloud Storage is a cost-effective storage solution. Additionally, Cloud Pub/Sub and Dataflow allow you to process and transform data efficiently, reducing storage costs.\nNear Real-time SQL Query: By using Dataflow to transform and load data into BigQuery, you can achieve near real-time data availability for SQL queries. BigQuery is well-suited for ad-hoc SQL queries and provides excellent query performance.","timestamp":"1695521880.0","poster":"barnac1es"},{"poster":"FP77","timestamp":"1692200880.0","comment_id":"982737","content":"Selected Answer: D\nShould be D","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nFor sure D","poster":"vaga1","comment_id":"918054","timestamp":"1686215220.0"},{"timestamp":"1685601300.0","content":"Selected Answer: D\nD is the most suitable, however the stored format should be JSON, and AVRO isn't JSON...","upvote_count":"1","poster":"forepick","comment_id":"911808"},{"poster":"OberstK","timestamp":"1675418040.0","content":"Selected Answer: D\nCorrect - D","comment_id":"796941","upvote_count":"1"},{"comment_id":"786041","timestamp":"1674523740.0","upvote_count":"1","poster":"desertlotus1211","content":"I believe this was also on the GCP PCA exam as well! ;)"},{"poster":"AzureDP900","content":"D. Create an application that publishes events to Cloud Pub/Sub, and create a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery.","comment_id":"762718","upvote_count":"1","timestamp":"1672505520.0"},{"content":"Selected Answer: D\nD is the answer.","upvote_count":"1","timestamp":"1669991220.0","poster":"zellck","comment_id":"733819"},{"content":"Selected Answer: D\nFor sure D","upvote_count":"1","poster":"mbacelar","timestamp":"1668333600.0","comment_id":"717226"},{"comment_id":"676510","upvote_count":"1","timestamp":"1663874580.0","content":"D it is!","poster":"clouditis"},{"poster":"Prasanna_kumar","content":"Answer is D","upvote_count":"2","timestamp":"1645466880.0","comment_id":"553074"},{"poster":"MaxNRG","comment_id":"520324","content":"Selected Answer: D\nD:\nCloud Pub/Sub, Cloud Dataflow, Cloud Storage, BigQuery https://cloud.google.com/solutions/stream-analytics/","timestamp":"1641745500.0","upvote_count":"4"},{"comment_id":"519525","poster":"medeis_jar","content":"Selected Answer: D\nOMG only D","upvote_count":"1","timestamp":"1641646980.0"},{"upvote_count":"11","comment_id":"487119","timestamp":"1637908980.0","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: D","poster":"JG123"},{"comment_id":"422055","poster":"sandipk91","upvote_count":"4","timestamp":"1628505780.0","content":"Answer is D for sure"},{"timestamp":"1625343600.0","content":"Vote for D","comment_id":"397832","upvote_count":"3","poster":"sumanshu"},{"upvote_count":"3","content":"D:\nbecause we have to be able to query over historical 2 years data only BigQuery address this issue.","poster":"daghayeghi","comment_id":"293833","comments":[{"upvote_count":"3","timestamp":"1613693280.0","content":"and because we have lots of input data we have to use Dataflow for processing","comment_id":"293837","poster":"daghayeghi"}],"timestamp":"1613693100.0"},{"timestamp":"1598539680.0","content":"D is correct","upvote_count":"2","comment_id":"167623","poster":"atnafu2020"},{"comment_id":"163170","content":"D is correct","timestamp":"1598042460.0","poster":"haroldbenites","upvote_count":"3"},{"timestamp":"1592810880.0","poster":"AJKumar","content":"A and B can be eliminated righaway. between C and D; C has no bigquery , Answer D.","upvote_count":"5","comment_id":"116129"},{"timestamp":"1588289520.0","upvote_count":"4","content":"Correct : D","comment_id":"81894","poster":"arnabbis4u"},{"poster":"Rajokkiyam","timestamp":"1585790460.0","upvote_count":"4","comment_id":"70306","content":"Answer D"}],"topic":"1","question_id":41,"answer_images":[],"unix_timestamp":1584870540,"answer":"D","timestamp":"2020-03-22 10:49:00"},{"id":"UtPafEwyjwLHWOCOv9eg","answer_description":"","choices":{"A":"Increase the number of max workers","D":"Create a temporary table in Bigtable that will act as a buffer for new data. Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Bigtable to BigQuery","B":"Use a larger instance type for your Dataflow workers","E":"Create a temporary table in Cloud Spanner that will act as a buffer for new data. Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Cloud Spanner to BigQuery","C":"Change the zone of your Dataflow pipeline to run in us-central1"},"topic":"1","timestamp":"2020-03-18 16:39:00","url":"https://www.examtopics.com/discussions/google/view/16932-exam-professional-data-engineer-topic-1-question-136/","answer_images":[],"unix_timestamp":1584545940,"question_text":"You are running a pipeline in Dataflow that receives messages from a Pub/Sub topic and writes the results to a BigQuery dataset in the EU. Currently, your pipeline is located in europe-west4 and has a maximum of 3 workers, instance type n1-standard-1. You notice that during peak periods, your pipeline is struggling to process records in a timely fashion, when all 3 workers are at maximum CPU utilization. Which two actions can you take to increase performance of your pipeline? (Choose two.)","answer":"AB","discussion":[{"timestamp":"1600436340.0","content":"A & B\ninstance n1-standard-1 is low configuration and hence need to be larger configuration, definitely B should be one of the option.\nIncrease max workers will increase parallelism and hence will be able to process faster given larger CPU size and multi core processor instance type is chosen. Option A can be a better step.","comments":[{"content":"Agreed","poster":"AzureDP900","upvote_count":"2","timestamp":"1688137080.0","comment_id":"762720"}],"upvote_count":"50","poster":"jvg637","comment_id":"65686"},{"poster":"sumanshu","upvote_count":"14","comment_id":"398143","timestamp":"1641292740.0","content":"A & B.\n\nWith autoscaling enabled, the Dataflow service does not allow user control of the exact number of worker instances allocated to your job. You might still cap the number of workers by specifying the --max_num_workers option when you run your pipeline. Here as per question CAP is 3, So we can change that CAP.\n\nFor batch jobs, the default machine type is n1-standard-1. For streaming jobs, the default machine type for Streaming Engine-enabled jobs is n1-standard-2 and the default machine type for non-Streaming Engine jobs is n1-standard-4. When using the default machine types, the Dataflow service can therefore allocate up to 4000 cores per job. If you need more cores for your job, you can select a larger machine type."},{"content":"Selected Answer: AB\nA & B is correct","timestamp":"1724212080.0","comment_id":"1155275","poster":"et2137","upvote_count":"1"},{"poster":"kcl10","timestamp":"1712023380.0","upvote_count":"1","comment_id":"1022734","content":"Selected Answer: AB\nA & B is correct"},{"upvote_count":"1","comment_id":"1016200","content":"Selected Answer: AB\nA because more workers improves performance through parallel work\nB because the current instance size is too small","poster":"juliorevk","timestamp":"1711324500.0"},{"timestamp":"1711254420.0","content":"Selected Answer: AB\nA. Increase the number of max workers:\nBy increasing the number of maximum workers, you allow Dataflow to allocate more computing resources to handle the peak load of incoming data. This can help improve processing speed and reduce CPU utilization per worker.\n\nB. Use a larger instance type for your Dataflow workers:\nUsing a larger instance type with more CPU and memory resources can help your Dataflow workers handle a higher volume of data and processing tasks more efficiently. It can address CPU bottlenecks during peak periods.","poster":"barnac1es","comment_id":"1015429","upvote_count":"3"},{"upvote_count":"1","comment_id":"733818","poster":"zellck","timestamp":"1685708760.0","content":"Selected Answer: AB\nAB is the answer."},{"upvote_count":"1","poster":"mbacelar","comment_id":"717228","content":"Selected Answer: AB\nScale in and Scale Out","timestamp":"1683965100.0"},{"timestamp":"1670422680.0","poster":"FrankT2L","comment_id":"612748","content":"Selected Answer: AB\nmaximum of 3 workers: Increase the number of max workers (A)\ninstance type n1-standard-1: Use a larger instance type for your Cloud Dataflow workers (B)","upvote_count":"2"},{"content":"Selected Answer: AB\nA & B, other options don't make sense","upvote_count":"4","comment_id":"520364","timestamp":"1657379940.0","poster":"MaxNRG"},{"content":"Selected Answer: AB\nOnly A & B make sense for improving pipeline performance.","timestamp":"1657278300.0","upvote_count":"2","comment_id":"519527","poster":"medeis_jar"},{"timestamp":"1656343380.0","comment_id":"510507","poster":"Mjvsj","upvote_count":"2","content":"Selected Answer: AB\nShould be A & B"},{"upvote_count":"2","comment_id":"293951","timestamp":"1629342300.0","comments":[{"upvote_count":"3","timestamp":"1638101220.0","poster":"Vasu_1","comment_id":"368642","content":"A & B is the right answer: You can set disable auto-scaling by setting the option --numWorkers (default is 3) and select the machine type by setting --workerMachineType at the time of creation of the pipeline (this applies to both auto and manual scaling)"}],"poster":"daghayeghi","content":"B, E:\nB: Dataflow manage number of worker automatically, then we only can define machine type worker.\nhttps://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline\nE: and adding a horizontally scale-able database like cloud spanner will reduce pressure on dataflow as it don't have to move data to specific zone and can be remain in same zone of EU, then E is correct."},{"timestamp":"1621344300.0","content":"Dataset is in EU so data can't be moved outside EU due to privacy law so zone option is ruled out. AB is Ok but intermediate table will boost perf apanee ruled out not sure of bigtable","poster":"kavs","comment_id":"222013","upvote_count":"3"},{"poster":"Alasmindas","comment_id":"217204","timestamp":"1620720720.0","upvote_count":"3","content":"Option A and B for sure, \nOption C : Changing Zone has nothing to do in improving performance \nOption D and E : Adding BQ and BT is waste of many and does not solve the purpose of the question."},{"comments":[{"content":"automatically taking care of workers up to 3 (as the maximum worker is 3 set as per questions)","comment_id":"398142","poster":"sumanshu","timestamp":"1641292560.0","upvote_count":"1"},{"content":"On second thought, A B is looking right","upvote_count":"2","poster":"SureshKotla","timestamp":"1616564940.0","comment_id":"185854"}],"poster":"SureshKotla","content":"B & D\nDF will automatically take care of increasing workers. Developers won't need to access the settings . https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#autoscaling","comment_id":"185845","upvote_count":"2","timestamp":"1616564640.0"},{"timestamp":"1614444660.0","poster":"atnafu2020","comment_id":"167627","content":"AB \nis correct","upvote_count":"2"},{"poster":"haroldbenites","content":"A , E is correct","comment_id":"163190","timestamp":"1613951520.0","upvote_count":"5"},{"timestamp":"1613838300.0","comment_id":"162294","content":"Worker nodes increase and large instance will give good performance","upvote_count":"2","poster":"Ravivarma4786"},{"poster":"FARR","content":"AB\nhttps://cloud.google.com/dataflow/docs/guides/common-errors#tsg-resource-exhausted-no-disk-space","timestamp":"1613273700.0","comment_id":"157728","upvote_count":"2"},{"poster":"kino2020","upvote_count":"2","comment_id":"142695","timestamp":"1611496560.0","content":"postscript\nmaxNumWorkers int The maximum number of Compute Engine instances to be made available to your pipeline during execution. Note that this can be higher than the initial number of workers (specified by numWorkers to allow your job to scale up, automatically or otherwise.\nhttps://cloud.google.com/dataflow/docs/guides/specifying-exec-params\n\nIn these examples, the single-cluster instance can handle twice the write throughput that the replicated instance can handle, even though each instance's clusters have a total of 6 nodes.\nhttps://cloud.google.com/bigtable/docs/performance"},{"content":"Adding nodes to the original cluster: You can add 3 nodes to the cluster, for a total of 6 nodes. The write throughput for the instance doubles, but the instance's data is available in only one zone:\nhttps://cloud.google.com/bigtable/docs/performance\n\nYou are writing large amounts of data and you need the writes to complete quickly. A request that reads and then modifies a row is slower than a simple write request. As a result, this type of write is often not the best approach at scale. For example, if you want to count something that will number in the millions, such as page views, you should consider recording each view as a simple write rather than incrementing a value. Then you can use a Dataflow job to aggregate the data.\nhttps://cloud.google.com/bigtable/docs/writes\n\nTherefore, BC.","poster":"kino2020","timestamp":"1611495960.0","comment_id":"142689","upvote_count":"1"},{"comment_id":"116219","timestamp":"1608634560.0","poster":"AJKumar","content":"D and E can be eliminated right away as the question does not talk abt spanner or bigtable, C is also eliminated as changing time zones is irrelevant. Answer A&B.","upvote_count":"4"},{"timestamp":"1608483720.0","upvote_count":"1","comment_id":"114849","poster":"Callumr","content":"B assumes the Jobs can run faster if given more cores - but we don't know that (are they using ParDo?). So D could also be a legit answer - either A&B or A&D"},{"poster":"[Removed]","content":"Answer: A, B\nDescription: Instance type is basic and max workers needs to be high","timestamp":"1601291040.0","comment_id":"68859","upvote_count":"3"},{"timestamp":"1600757220.0","content":"Correct - AB","comment_id":"66877","upvote_count":"3","poster":"[Removed]"}],"answer_ET":"AB","question_images":[],"exam_id":11,"answers_community":["AB (100%)"],"question_id":42,"isMC":true},{"id":"T8hGtYXebY2nbrTeaeI1","discussion":[{"upvote_count":"9","poster":"arpitagrawal","timestamp":"1662459000.0","content":"Selected Answer: BC\nIt should be B and C","comment_id":"661069"},{"content":"Selected Answer: BC\nBC is correct\n\nWhy the comments is deleted?","comment_id":"658069","upvote_count":"7","timestamp":"1662179760.0","poster":"ducc"},{"upvote_count":"1","poster":"loki82","content":"Selected Answer: CE\nIf there's a write speed bottleneck on bigtable, more dataflow workers won't make a difference. If I add more bigtable nodes, or group my writes together, I can increase update throughput.\n\nhttps://cloud.google.com/dataflow/docs/guides/write-to-bigtable#best-practices","comment_id":"1346436","timestamp":"1737808080.0"},{"upvote_count":"1","comment_id":"1288320","timestamp":"1727127840.0","content":"Selected Answer: BC\nthe goal is to reduce the write latency not to improve data flow code","poster":"Preetmehta1234"},{"comment_id":"1069707","upvote_count":"2","poster":"emmylou","content":"The \"Correct Answers\" are just put in with a random generator :-) B and C","timestamp":"1699905780.0"},{"comment_id":"1054501","poster":"BlehMaks","upvote_count":"4","timestamp":"1698319920.0","content":"Selected Answer: BC\nB - opportunity to parallelise the process\nC - increase throughput"},{"content":"Exactly opposite answers in the discussions","comment_id":"1022910","timestamp":"1696231260.0","poster":"Bahubali1988","upvote_count":"1"},{"poster":"barnac1es","comment_id":"1015432","content":"Selected Answer: BC\nB. Increase the maximum number of Dataflow workers by setting maxNumWorkers in PipelineOptions:\nIncreasing the number of Dataflow workers can help parallelize the processing of your data, which can result in faster data updates to Bigtable and improved concurrency. You can set maxNumWorkers to a higher value to achieve this.\n\nC. Increase the number of nodes in the Bigtable cluster:\nIncreasing the number of nodes in your Bigtable cluster can improve the overall throughput and reduce latency when writing data. It allows Bigtable to handle a higher rate of data ingestion and queries, which is essential for supporting additional concurrent users.","upvote_count":"4","timestamp":"1695522840.0"},{"content":"Selected Answer: CD\nC definetely is correct, as it improves the read and write performance of Bigtable. \n\nHowever, I do think that the second option is actually D instead of B, because the question specifically states that the pipeline aggregates data. Flatten merges multiple PCollection objects into a single logical PCollection, allowing for faster aggregation of time series data.","comment_id":"1012192","timestamp":"1695205020.0","poster":"ckanaar","upvote_count":"2"},{"upvote_count":"1","comment_id":"972280","content":"Selected Answer: BE\nB - I believe it is consensus.\nD - The question mentions \"a Dataflow job that \"aggregates\" and writes time series metrics to Bigtable\". So CoGroupByKey performs a shuffle (grouping) operation to distribute data across workers.\n\nhttps://cloud.google.com/dataflow/docs/guides/develop-and-test-pipelines","timestamp":"1691162520.0","poster":"NewDE2023"},{"content":"Selected Answer: DE\nI read this question as: BigTable Write operations are all over the place (key-wise), and BigTable doesn't like that. When creating groups (batch writes), of similar keys (close to each other), BigTable is happy again, which I loosely translate into DE.","upvote_count":"1","poster":"WillemHendr","comment_id":"917483","timestamp":"1686160860.0"},{"upvote_count":"1","timestamp":"1683209040.0","content":"B is correct. But I don't see how you increase the write throughput of Bigtable increasing its cluster size. It should be dataflow instance resources that have to be increased","poster":"vaga1","comment_id":"889408"},{"upvote_count":"1","timestamp":"1679415000.0","content":"Selected Answer: BC\nBC make sense","poster":"juliobs","comment_id":"846144"},{"timestamp":"1674989460.0","upvote_count":"1","content":"BC only makes sense here , no mention of data, no mention of keeping cost low","comment_id":"791559","poster":"NamitSehgal"},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1672505940.0","content":"B. Increase the maximum number of Dataflow workers by setting maxNumWorkers in PipelineOptions Most Voted\nC. Increase the number of nodes in the Bigtable cluster","comment_id":"762722"},{"upvote_count":"2","poster":"ovokpus","comment_id":"724907","timestamp":"1669181340.0","content":"Selected Answer: BC\nIncrease max num of workers increases pipeline performance in Dataflow\nIncrease number of nodes in Bigtable increases write throughput"}],"answer_images":[],"unix_timestamp":1662179760,"isMC":true,"exam_id":11,"question_id":43,"answer_ET":"BC","answers_community":["BC (85%)","Other"],"url":"https://www.examtopics.com/discussions/google/view/79674-exam-professional-data-engineer-topic-1-question-137/","answer_description":"","question_text":"You have a data pipeline with a Dataflow job that aggregates and writes time series metrics to Bigtable. You notice that data is slow to update in Bigtable. This data feeds a dashboard used by thousands of users across the organization. You need to support additional concurrent users and reduce the amount of time required to write the data. Which two actions should you take? (Choose two.)","question_images":[],"topic":"1","timestamp":"2022-09-03 06:36:00","answer":"BC","choices":{"D":"Modify your Dataflow pipeline to use the Flatten transform before writing to Bigtable","B":"Increase the maximum number of Dataflow workers by setting maxNumWorkers in PipelineOptions","E":"Modify your Dataflow pipeline to use the CoGroupByKey transform before writing to Bigtable","C":"Increase the number of nodes in the Bigtable cluster","A":"Configure your Dataflow pipeline to use local execution"}},{"id":"NLX7knKdVRGGh9SED0KC","topic":"1","answers_community":["C (80%)","A (20%)"],"discussion":[{"timestamp":"1680467100.0","upvote_count":"5","content":"Correct answer is A. https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows","comment_id":"685048","poster":"LP_PDE"},{"poster":"skhaire","timestamp":"1738527900.0","content":"A. Create a Cloud Dataproc Workflow Template\nDataproc Workflow Template can be used to run jobs concurrently and sequentially. DAG is an overkill. \nhttps://cloud.google.com/dataproc/docs/concepts/workflows/use-workflows","upvote_count":"1","comment_id":"1350643"},{"poster":"MaxNRG","comment_id":"1099526","comments":[{"comment_id":"1099527","poster":"MaxNRG","comments":[{"poster":"MaxNRG","upvote_count":"3","content":"While the other options have some merit, they fall short in certain aspects:\nA. Cloud Dataproc Workflow Templates: While workflow templates can automate job submission on a cluster, they lack the ability to define dependencies and coordinate concurrent execution effectively.\nB. Initialization action: An initialization action can only run a single script before a Dataproc cluster starts, not suitable for orchestrating multiple scheduled jobs with dependencies.\nD. Bash script: A Bash script might work for simple cases, but it can be cumbersome to manage and lacks the advanced scheduling and error handling capabilities of Cloud Composer.\nTherefore, utilizing a Cloud Composer DAG offers the most comprehensive and flexible solution for automating your scheduled Spark jobs with sequential and concurrent execution on Cloud Dataproc.","comment_id":"1099528","timestamp":"1718691720.0"}],"content":"Here's why:\nDAG workflows: Cloud Composer excels at orchestrating complex workflows with dependencies, making it ideal for managing sequential and concurrent execution of your Spark jobs. You can define dependencies between tasks to ensure certain jobs only run after others finish.\nAutomation: Cloud Composer lets you schedule workflows to run automatically based on triggers like time intervals or data availability, eliminating the need for manual intervention.\nIntegration: Cloud Composer integrates seamlessly with Cloud Dataproc, allowing you to easily launch and manage your Spark clusters within the workflow.\nScalability: Cloud Composer scales well to handle a large number of jobs and workflows, making it suitable for managing complex data pipelines.","upvote_count":"2","timestamp":"1718691720.0"}],"content":"Selected Answer: C\nThe best option for automating your scheduled Spark jobs on Cloud Dataproc, considering sequential and concurrent execution, is:\nC. Create a Directed Acyclic Graph (DAG) in Cloud Composer.","upvote_count":"3","timestamp":"1718691660.0"},{"poster":"emmylou","timestamp":"1716226440.0","comment_id":"1075792","upvote_count":"1","content":"Selected Answer: C\nI thought it might be A but the templates can only run sequentially, not concurrently."},{"poster":"barnac1es","comment_id":"1015434","timestamp":"1711255020.0","content":"Selected Answer: C\nDirected Acyclic Graph (DAG): Cloud Composer (formerly known as Cloud Composer) is a managed Apache Airflow service that allows you to create and manage workflows as DAGs. You can define a DAG that includes tasks for running Spark jobs in sequence or concurrently.\n\nScheduling: Cloud Composer provides built-in scheduling capabilities, allowing you to specify when and how often your DAGs should run. You can schedule the execution of your Spark jobs at specific times or intervals.\n\nDependency Management: In a DAG, you can define dependencies between tasks. This means you can set up tasks to run sequentially or concurrently based on your requirements. For example, you can specify that Job B runs after Job A has completed, or you can schedule jobs to run concurrently when there are no dependencies.","upvote_count":"1"},{"timestamp":"1694583420.0","poster":"midgoo","upvote_count":"2","comment_id":"837728","content":"Selected Answer: C\nI would choose A if there was one more step to schedule the Template. It is like creating DAG without running it in Airflow.\nSo only option C is correct here."},{"comment_id":"762726","upvote_count":"3","content":"C. Create a Directed Acyclic Graph in Cloud Composer","poster":"AzureDP900","timestamp":"1688137200.0"},{"upvote_count":"2","poster":"saurabhsingh4k","timestamp":"1687168200.0","content":"Selected Answer: A\nWhy go for an expensive Composer when you only have to schedule and create a DAG for Dataproc, A is sufficient.","comments":[{"content":"I've would've gone for Workflow Templates as well. But those are lacking the scheduling capability. Hence you would need to use Cloud Composer (or Cloud Functions or Cloud Scheduler) anyway. Hence C seems to be the better solution.\n\nPls see here:\nhttps://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions","upvote_count":"5","timestamp":"1688464440.0","comment_id":"765581","poster":"captainbu"}],"comment_id":"749781"},{"comment_id":"733812","poster":"zellck","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions#cloud_composer\nCloud Composer is a managed Apache Airflow service you can use to create, schedule, monitor, and manage workflows. Advantages:\n- Supports time- and event-based scheduling\n- Simplified calls to Dataproc using Operators\n- Dynamically generate workflows and workflow parameters\n- Build data flows that span multiple Google Cloud products","upvote_count":"2","timestamp":"1685708400.0"},{"content":"Selected Answer: C\nC.\nComposer fits better to schedule Dataproc Workflows, check the documentation:\nhttps://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions\n\nAlso A is not enough. Dataproc Workflow Template itself don't has a native schedule option.","poster":"devaid","timestamp":"1681677240.0","upvote_count":"4","comment_id":"696541"},{"comment_id":"696029","poster":"louisgcpde","timestamp":"1681625580.0","content":"Selected Answer: C\nSo that I thing the answer should be C (Composer).","upvote_count":"1"},{"comment_id":"696028","upvote_count":"2","content":"To me, the point is \"automate\" the process, so that Composer DAG is needed and can be used with Dataproc Workflow Template.","poster":"louisgcpde","timestamp":"1681625520.0"},{"content":"Selected Answer: A\nAns A makes more sense, since a question is regarding Dataproc jobs only","poster":"dmzr","timestamp":"1681059540.0","comment_id":"690366","upvote_count":"2"},{"content":"Selected Answer: C\nOption c","comment_id":"663080","timestamp":"1678255260.0","poster":"HarshKothari21","upvote_count":"1"},{"poster":"ducc","comment_id":"658070","timestamp":"1677825540.0","upvote_count":"1","content":"Selected Answer: C\nYou have streaming and batch job, so Composer is the choice for me"}],"question_id":44,"isMC":true,"answer_images":[],"answer_ET":"C","choices":{"A":"Create a Cloud Dataproc Workflow Template","B":"Create an initialization action to execute the jobs","C":"Create a Directed Acyclic Graph in Cloud Composer","D":"Create a Bash script that uses the Cloud SDK to create a cluster, execute jobs, and then tear down the cluster"},"answer_description":"","question_text":"You have several Spark jobs that run on a Cloud Dataproc cluster on a schedule. Some of the jobs run in sequence, and some of the jobs run concurrently. You need to automate this process. What should you do?","timestamp":"2022-09-03 06:39:00","exam_id":11,"answer":"C","unix_timestamp":1662179940,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/79675-exam-professional-data-engineer-topic-1-question-138/"},{"id":"U1FaHbfHbOFY2plF1MaQ","answer_ET":"B","question_text":"You are building a new data pipeline to share data between two different types of applications: jobs generators and job runners. Your solution must scale to accommodate increases in usage and must accommodate the addition of new applications without negatively affecting the performance of existing ones. What should you do?","answers_community":["B (100%)"],"choices":{"B":"Use a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute them","A":"Create an API using App Engine to receive and send messages to the applications","C":"Create a table on Cloud SQL, and insert and delete rows with the job information","D":"Create a table on Cloud Spanner, and insert and delete rows with the job information"},"unix_timestamp":1662180000,"answer_description":"","answer":"B","timestamp":"2022-09-03 06:40:00","url":"https://www.examtopics.com/discussions/google/view/79676-exam-professional-data-engineer-topic-1-question-139/","topic":"1","exam_id":11,"question_id":45,"answer_images":[],"question_images":[],"isMC":true,"discussion":[{"content":"Selected Answer: B\nJob generators (they would be the publishers).\nJob runners = subscribers\n\nQuestion mentions that it must scale (of which push subscription has automatic scaling) and can accommodate additional new applications (this can be solved by having multiple subscriptions, with each relating to a unique application) to a central topic","timestamp":"1686145920.0","poster":"jkhong","upvote_count":"9","comments":[{"poster":"AzureDP900","upvote_count":"4","comment_id":"762727","content":"Yes it is \nB. Use a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute them","timestamp":"1688137320.0"}],"comment_id":"738070"},{"poster":"srivastavas08","upvote_count":"1","comment_id":"1146312","timestamp":"1723286820.0","content":"A. App Engine API: While scalable, it introduces a central point of failure and might not be as performant as Pub/Sub for high-volume data.\nC. Cloud SQL: Not designed for real-time data sharing and continuous updates, leading to potential bottlenecks and performance issues.\nD. Cloud Spanner: Offers strong consistency and global distribution, but its pricing model might be less suitable for high-volume, cost-sensitive workloads compared to Pub/Sub."},{"timestamp":"1711325640.0","upvote_count":"1","poster":"juliorevk","comment_id":"1016203","content":"Selected Answer: B\nB to decouple jobs being generated and run. Pub/Sub also scales seamlessly"},{"timestamp":"1711255140.0","poster":"barnac1es","upvote_count":"2","comment_id":"1015436","content":"Selected Answer: B\nB. Use a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute them.\n\nScalability: Cloud Pub/Sub is a highly scalable messaging service that can handle a significant volume of messages and subscribers. It can easily accommodate increases in usage as your data pipeline scales.\n\nDecoupling: Using Pub/Sub decouples the job generators from the job runners, which is a good architectural choice for flexibility and scalability. Job generators publish messages to a topic, and job runners subscribe to that topic to execute jobs when they are available.\n\nAdding New Applications: With Cloud Pub/Sub, adding new applications (new publishers or subscribers) is straightforward. You can simply create new publishers to send jobs or new subscribers to consume jobs without impacting existing components."},{"upvote_count":"2","poster":"musumusu","content":"key words here: job generators (pushlish message on pub/sub) and job runners(subscribe message for further analysis). You may add as much as pushlishing job and subscribing job to same topic. So Answer B.\nUsing API , app engine is also good approach but its more complex than pub/sub.","comment_id":"812349","timestamp":"1692297780.0"},{"upvote_count":"2","poster":"zellck","timestamp":"1685618340.0","comment_id":"732646","content":"Selected Answer: B\nB is the answer."},{"timestamp":"1683905520.0","upvote_count":"1","poster":"Atnafu","comment_id":"716836","content":"A\nSince it's application i will go with"},{"comment_id":"661074","upvote_count":"3","poster":"arpitagrawal","content":"Selected Answer: B\nuse pubsub","timestamp":"1678104840.0"},{"upvote_count":"2","comment_id":"660015","content":"Selected Answer: B\nI would tend to think B , one of the use of pub/sub is decoupling app","timestamp":"1678017360.0","poster":"YorelNation"},{"poster":"ducc","upvote_count":"2","timestamp":"1677825600.0","content":"Selected Answer: B\nI choose B","comment_id":"658071"}]}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"isImplemented":true,"provider":"Google","id":11,"name":"Professional Data Engineer","numberOfQuestions":319,"isMCOnly":true},"currentPage":9},"__N_SSP":true}