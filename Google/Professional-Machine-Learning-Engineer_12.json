{"pageProps":{"questions":[{"id":"ReGak3zw0yX7lhsbDfgm","question_text":"You recently deployed an ML model. Three months after deployment, you notice that your model is underperforming on certain subgroups, thus potentially leading to biased results. You suspect that the inequitable performance is due to class imbalances in the training data, but you cannot collect more data. What should you do? (Choose two.)","answer_description":"","answer":"BD","unix_timestamp":1676443680,"choices":{"C":"Remove the features that have the highest correlations with the majority class.","A":"Remove training examples of high-performing subgroups, and retrain the model.","D":"Upsample or reweight your existing training data, and retrain the model","B":"Add an additional objective to penalize the model more for errors made on the minority class, and retrain the model","E":"Redeploy the model, and provide a label explaining the model's behavior to users."},"isMC":true,"answer_images":[],"question_id":56,"discussion":[{"comments":[{"content":"A. Removing High-Performing Subgroup Examples: This removes valuable data and can worsen overall model performance.\nC. Removing High-Correlation Features: This might eliminate informative features and could negatively impact model accuracy.\nE. Redeploying with Explanation: While transparency is essential, it doesn't address the underlying performance disparity","comment_id":"1200409","upvote_count":"2","poster":"fitri001","timestamp":"1729640460.0"}],"upvote_count":"4","content":"Selected Answer: BD\nPenalizing Errors on Minority Class (B): This technique, also known as cost-sensitive learning, modifies the loss function during training. Assigning a higher penalty to misclassifications of the minority class steers the model to prioritize learning from those examples.\n\nUpsampling/Reweighting Training Data (D):\n\nUpsampling increases the representation of the minority class in the training data by duplicating existing data points.\nReweighting assigns higher weights to data points from the minority class during training, making their influence more significant.","timestamp":"1729640400.0","comment_id":"1200408","poster":"fitri001"},{"poster":"Carlose2108","timestamp":"1724877180.0","upvote_count":"1","content":"Selected Answer: BD\nI went B & D.","comment_id":"1162029"},{"comment_id":"957594","poster":"PST21","upvote_count":"1","timestamp":"1705763520.0","content":"D. Upsample or reweight your existing training data, and retrain the model.\n\nE. Redeploy the model, and provide a label explaining the model's behavior to users.\n\nOption D: Upsampling or reweighting your existing training data and retraining the model can help address the class imbalance issue and improve the performance on certain subgroups. By duplicating or adjusting the weights of samples from the minority class, the model will receive more exposure to these samples during training, leading to better learning and performance on the underrepresented subgroups.\n\nOption E: Redeploying the model and providing a label explaining the model's behavior to users is essential for transparency and accountability. If the model exhibits biased behavior or inequitable performance on certain subgroups, informing users about this issue can help them interpret the model's predictions more effectively and make informed decisions based on the model's output."},{"poster":"M25","upvote_count":"2","timestamp":"1699639140.0","comment_id":"894153","content":"Selected Answer: BD\nWent with B, D"},{"poster":"hakook","upvote_count":"2","comment_id":"832954","content":"Selected Answer: BD\nshould be B,D","timestamp":"1694170020.0"},{"poster":"TNT87","timestamp":"1694081340.0","content":"Selected Answer: BD\nOption B and D could be good approaches to address the issue.\n\nB. Adding an additional objective to penalize the model more for errors made on the minority class can help the model to focus more on correctly classifying the underrepresented class.\n\nD. Upsampling or reweighting the existing training data can help balance the class distribution and increase the model's sensitivity to the underrepresented class.","comment_id":"831847","upvote_count":"4"},{"upvote_count":"2","comments":[{"timestamp":"1692675960.0","upvote_count":"2","content":"https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/","comment_id":"817500","poster":"TNT87"}],"poster":"TNT87","comment_id":"810430","timestamp":"1692166860.0","content":"https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"},{"timestamp":"1692074880.0","comment_id":"809211","content":"Selected Answer: BD\nhttps://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/","upvote_count":"1","poster":"John_Pongthorn"}],"url":"https://www.examtopics.com/discussions/google/view/99253-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["BD (100%)"],"topic":"1","exam_id":13,"timestamp":"2023-02-15 07:48:00","question_images":[],"answer_ET":"BD"},{"id":"94pmy3s1sJYzI8wmutSd","url":"https://www.examtopics.com/discussions/google/view/54659-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"D","exam_id":13,"timestamp":"2021-06-05 21:01:00","discussion":[{"content":"Should be D","comment_id":"382988","upvote_count":"21","timestamp":"1639620960.0","poster":"chohan"},{"upvote_count":"15","comment_id":"495915","content":"My option is D.\n\nCite from Google Pag: to construct a Dataset from data in memory, use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). When input data is stored in a file (not in memory), the recommended TFRecord format, you can use tf.data.TFRecordDataset().\n\ntf.data.Dataset is for data in memory.\ntf.data.TFRecordDataset is for data in non-memory storage.","poster":"alphard","timestamp":"1654593780.0"},{"content":"Selected Answer: A\nthis is one of the review questions in Chapter 3 in the book \"Official Google Cloud Certified Professional Machine Learning Engineer Study Guide\".\ntf.data.Dataset.prefetch transformation decouples the time when data is produced to the time when data is consumed so it can reduce the latency. Also the transformation can reduce the memory usage. By the way, tf.data.Dataset.interleave transformation can also used to reduce the latency and memory usage.","comment_id":"1366750","upvote_count":"1","poster":"RyanTan","timestamp":"1741491060.0"},{"comment_id":"1225167","upvote_count":"2","timestamp":"1733471520.0","poster":"PhilipKoku","content":"Selected Answer: D\nD) Storing images in TFRecords optimises storage for images."},{"comment_id":"1194366","upvote_count":"2","timestamp":"1728740460.0","content":"Selected Answer: D\ntf.data.Dataset is for data in memory.\ntf.data.TFRecordDataset is for data in non-memory storage.","poster":"pinimichele01"},{"content":"Selected Answer: D\nwhy this website shows wrong option as answer, this is my observation from so many questions?","poster":"samratashok","upvote_count":"3","timestamp":"1725610380.0","comment_id":"1167071"},{"poster":"fragkris","timestamp":"1717240260.0","comment_id":"1085226","content":"Selected Answer: D\nD is correct","upvote_count":"1"},{"comment_id":"1070463","content":"Selected Answer: D\nD because:\ntf.data.Dataset is for data in memory.\ntf.data.TFRecordDataset is for data in non-memory storage.","timestamp":"1715689020.0","upvote_count":"2","poster":"Sum_Sum"},{"poster":"boobyg1","content":"Selected Answer: D\nall \"correct\" answers are wrong","upvote_count":"2","timestamp":"1714570800.0","comment_id":"1059864"},{"timestamp":"1699513020.0","content":"Selected Answer: D\nWent with D","comment_id":"892691","poster":"M25","upvote_count":"1"},{"comments":[{"timestamp":"1697800560.0","poster":"Alfredo_OSS","upvote_count":"2","content":"You should consider the voted ones.","comment_id":"875513"}],"upvote_count":"2","comment_id":"865309","content":"For all questions the given answers and voted answers are different. Which one should be considered for exam?","poster":"India_willsmith","timestamp":"1696830780.0"},{"upvote_count":"1","content":"Selected Answer: D\nConverting your data into TFRecord has many advantages, such as: More efficient storage: the TFRecord data can take up less space than the original data; it can also be partitioned into multiple files. Fast I/O: the TFRecord format can be read with parallel I/O operations, which is useful for TPUs or multiple hosts","comment_id":"799232","timestamp":"1691267580.0","poster":"enghabeth"},{"timestamp":"1691267460.0","poster":"enghabeth","upvote_count":"1","comment_id":"799230","content":"Selected Answer: D\nmy option is D"},{"timestamp":"1687772640.0","comment_id":"757374","poster":"Omi_04040","upvote_count":"1","content":"Ans: D"},{"poster":"wish0035","upvote_count":"1","content":"Selected Answer: D\nans: D","timestamp":"1686856500.0","comment_id":"746509"},{"timestamp":"1684906140.0","comment_id":"725631","content":"Selected Answer: D\nFor data in memory use tf.data.Dataset, for data in non-memory storage use tf.data.TFRecordDataset.\nSince data don't fit in memory, go with option D.","upvote_count":"1","poster":"EFIGO"},{"comment_id":"647184","content":"Selected Answer: D\nCorrect answer is \"D\"","poster":"GCP72","upvote_count":"1","timestamp":"1676471580.0"},{"timestamp":"1670860860.0","content":"Selected Answer: D\n- by options eliminations A is the first option to be dropped , prefetch will use additional memory overhead to buffer images\n- answer in B,C,D but D is the best answer as we save the huge images dataset on gcs then load batches of data for training\n- B,C not good as they did not provide a solution to images are not fit in memory","poster":"Mohamed_Mossad","upvote_count":"2","comment_id":"615356"},{"content":"converting to TFrecords and writing to storage account is low latency ? shouldn't it A as tf.data.Dataset also read the data in batch in memory ?","poster":"rsamant","timestamp":"1666493820.0","comment_id":"590364","upvote_count":"1"},{"poster":"morgan62","timestamp":"1665049680.0","comment_id":"581737","upvote_count":"1","content":"Selected Answer: D\nD for sure"},{"content":"Can't it be B after all?\nThe tf.data.Dataset API supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern:\n\nCreate a source dataset from your input data.\nApply dataset transformations to preprocess the data.\nIterate over the dataset and process the elements.\nIteration happens in a streaming fashion, so the full dataset does not need to fit into memory.","timestamp":"1664341260.0","comment_id":"576626","poster":"lukacs16","upvote_count":"1"},{"upvote_count":"2","timestamp":"1661655960.0","comment_id":"557865","content":"Selected Answer: D\nCommunity vote","poster":"caohieu04"},{"comment_id":"531832","timestamp":"1658717340.0","content":"which one is correct B or D?","upvote_count":"2","poster":"Twis"},{"upvote_count":"4","timestamp":"1658217840.0","content":"TFRecord for non-memeory","comment_id":"527389","poster":"stefant"},{"upvote_count":"2","poster":"ashii007","timestamp":"1655039700.0","comment_id":"500089","content":"B and C are wrong because both are in-memory operations. Question clearly states that data is too large for memory. Tf records is the way to go as mentioned in Option D."},{"comment_id":"482585","timestamp":"1653048180.0","content":"Selected Answer: D\nexamtopic says B. But shouldn't be D https://www.tensorflow.org/guide/data#consuming_tfrecord_data","poster":"santy79","upvote_count":"2"},{"poster":"93alejandrosanchez","comment_id":"465207","content":"The answer is D as TFRecords are very efficient memory-wise.","upvote_count":"2","timestamp":"1650463140.0"},{"content":"Another vote for answer D. B and C are in memory if I'm not wrong. \n\nFurthermore, this is google-recommended best practices: \n\nhttps://cloud.google.com/architecture/ml-on-gcp-best-practices#store-image-video-audio-and-unstructured-data-on-cloud-storage\n\n\" Store image, video, audio and unstructured data on Cloud Storage Store these data in large container formats on Cloud Storage. This applies to sharded TFRecord files if you're using TensorFlow, or Avro files if you're using any other framework. Combine many individual images, videos, or audio clips into large files, as this will improve your read and write throughput to Cloud Storage. Aim for files of at least 100mb, and between 100 and 10,000 shards. To enable data management, use Cloud Storage buckets and directories to group the shards. \"","poster":"george_ognyanov","upvote_count":"3","comment_id":"459631","timestamp":"1649508780.0"},{"timestamp":"1647230400.0","comment_id":"444279","poster":"Y2Data","upvote_count":"1","content":"C is the way how distributed learning is done. Since distributed learning is essentially cutting up dataset and distribute both data and computation to different machines, I think it's viable for dataset that's too large to fit in memory as well."},{"content":"Can be D as well as we can use from slices to get data for processing","upvote_count":"3","comment_id":"375398","timestamp":"1638738060.0","poster":"inder0007"}],"answer":"D","choices":{"A":"Create a tf.data.Dataset.prefetch transformation.","B":"Convert the images to tf.Tensor objects, and then run Dataset.from_tensor_slices().","C":"Convert the images to tf.Tensor objects, and then run tf.data.Dataset.from_tensors().","D":"Convert the images into TFRecords, store the images in Cloud Storage, and then use the tf.data API to read the images for training."},"answers_community":["D (96%)","4%"],"answer_images":[],"question_id":57,"topic":"1","question_text":"You have been asked to develop an input pipeline for an ML training model that processes images from disparate sources at a low latency. You discover that your input data does not fit in memory. How should you create a dataset following Google-recommended best practices?","answer_description":"","isMC":true,"unix_timestamp":1622919660,"question_images":[]},{"id":"1w4YDyOCCowLCH2nsNk3","question_id":58,"timestamp":"2023-02-16 09:10:00","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/99380-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"content":"B. Recall\n\nIn a highly imbalanced dataset like the one described (96% of examples are in the negative class), the metric that would give the most confidence in the model's performance is recall.\n\nRecall (also known as sensitivity or true positive rate) is the proportion of actual positive cases that were correctly identified by the model. In this context, it means the percentage of images containing the company's logo that the model correctly classified as positive out of all the actual positive cases. Since the dataset is heavily skewed, a high recall value would indicate that the model is effectively capturing the positive cases (images with the logo) despite the class imbalance.\nF1 score (D) is a balance between precision and recall and is a useful metric for imbalanced datasets. However, in this specific case, recall is more important because we want to be confident in detecting the logo images, even if it comes at the cost of having some false positives (lower precision).","comment_id":"957611","comments":[{"content":"I go for B as well","poster":"vale_76_na_xxx","upvote_count":"1","timestamp":"1718374140.0","comment_id":"1096625"}],"poster":"PST21","timestamp":"1705764060.0","upvote_count":"8"},{"poster":"fitri001","upvote_count":"7","timestamp":"1729640580.0","comment_id":"1200410","content":"Selected Answer: D\nPrecision vs. Recall:\nPrecision focuses on the percentage of predicted positive cases (logo present) that are actually correct.\nRecall emphasizes the model's ability to identify all actual positive cases (correctly identifying all logos).\nIn a highly imbalanced dataset, a naive model could simply predict \"no logo\" for every image and achieve very high accuracy (almost 96%!). However, this wouldn't be a useful model since it would miss all the actual logos (low recall).\n\nF1 Score: The F1 score strikes a balance between precision and recall. It takes the harmonic mean of these two metrics, giving a more comprehensive picture of the model's performance in both identifying logos (recall) and avoiding false positives (precision).","comments":[{"timestamp":"1732166160.0","comment_id":"1214685","content":"very well explained!","poster":"AzureDP900","upvote_count":"1"}]},{"poster":"8619d79","comment_id":"1350598","upvote_count":"1","content":"Selected Answer: B\nThe focus here is on detecting images with logo (that is the minority class). Recall is the right metric. If the question would have highlighted that also the detection of images without logo is important I would have voted for D. But here is not the case. And of course F1 avoids that the model always mark as \"with logo\", but this is more on how the metric is used and interpreted. Otherwise when should be Recall useful?","timestamp":"1738520100.0"},{"poster":"gscharly","content":"Selected Answer: D\nWent with D","upvote_count":"1","comment_id":"1195912","timestamp":"1728981540.0"},{"content":"Selected Answer: B\nB\nSee #90, should be F score with Recall weights more than Precision.","upvote_count":"3","timestamp":"1727763960.0","comment_id":"1187238","poster":"Yan_X"},{"content":"Selected Answer: B\nI went with B.","timestamp":"1726233780.0","upvote_count":"1","poster":"CHARLIE2108","comment_id":"1172675"},{"poster":"vaibavi","comment_id":"1146867","timestamp":"1723322460.0","content":"Selected Answer: D\nF1 score provides a comprehensive evaluation by penalizing models that excel in just one aspect at the expense of the other. By considering both precision and recall, it helps identify models that effectively balance true positive identification with minimal false positives, making it a more suitable metric for imbalanced data like your logo detection problem.","upvote_count":"2"},{"comment_id":"894154","upvote_count":"2","poster":"M25","content":"Selected Answer: D\nSee #90!","timestamp":"1699639140.0"},{"upvote_count":"1","poster":"FherRO","timestamp":"1692819720.0","comment_id":"819783","content":"Selected Answer: D\nF1 score works well for imbalanced data sets"},{"content":"Selected Answer: D\nhttps://stephenallwright.com/imbalanced-data-metric/","comment_id":"810417","timestamp":"1692166200.0","poster":"TNT87","upvote_count":"2"}],"answers_community":["D (75%)","B (25%)"],"answer":"D","question_text":"You are working on a binary classification ML algorithm that detects whether an image of a classified scanned document contains a company’s logo. In the dataset, 96% of examples don’t have the logo, so the dataset is very skewed. Which metric would give you the most confidence in your model?","choices":{"D":"F1 score","C":"RMSE","B":"Recall","A":"Precision"},"unix_timestamp":1676535000,"exam_id":13,"answer_description":"","isMC":true,"answer_ET":"D","answer_images":[],"topic":"1"},{"id":"iRYnZy7MKjcsPeGKSU58","answer_ET":"A","topic":"1","answers_community":["A (76%)","D (24%)"],"answer":"A","exam_id":13,"choices":{"C":"Migrate your pipeline to Kubeflow hosted on Google Kubernetes Engine, and specify the appropriate node parameters for the evaluation step.","A":"Include the flag -runner=DataflowRunner in beam_pipeline_args to run the evaluation step on Dataflow.","B":"Move the evaluation step out of your pipeline and run it on custom Compute Engine VMs with sufficient memory.","D":"Add tfma.MetricsSpec () to limit the number of metrics in the evaluation step."},"isMC":true,"question_text":"While running a model training pipeline on Vertex Al, you discover that the evaluation step is failing because of an out-of-memory error. You are currently using TensorFlow Model Analysis (TFMA) with a standard Evaluator TensorFlow Extended (TFX) pipeline component for the evaluation step. You want to stabilize the pipeline without downgrading the evaluation quality while minimizing infrastructure overhead. What should you do?","answer_images":[],"unix_timestamp":1675610100,"timestamp":"2023-02-05 16:15:00","discussion":[{"content":"Selected Answer: A\nLinks already provided below:\n“That works fine for one hundred records, but what if the goal was to process all 187,002,0025 rows in the dataset? For this, the pipeline is switched from the DirectRunner to the production Dataflow runner.” [Option A] https://blog.tensorflow.org/2020/03/tensorflow-extended-tfx-using-apache-beam-large-scale-data-processing.html.\n\"Metrics to configure (only required if additional metrics are being added outside of those saved with the model).” https://www.tensorflow.org/tfx/guide/evaluator#using_the_evaluator_component\nwill thus add, not “limit the number of metrics in the evaluation step”. [Option D]","timestamp":"1683734460.0","upvote_count":"6","poster":"M25","comment_id":"894156"},{"comment_id":"1057162","poster":"MultipleWorkerMirroredStrategy","upvote_count":"6","comments":[{"comment_id":"1073523","timestamp":"1700243940.0","content":"If we have to add dataflow then this condition is not met: minimizing infrastructure overhead","poster":"pico","comments":[{"timestamp":"1731550920.0","comment_id":"1311682","content":"Dataflow requires no infrastructure management.","poster":"f084277","upvote_count":"1"},{"upvote_count":"3","poster":"Zepopo","timestamp":"1711046160.0","content":"No, it is. If we choose another option, there would be:\nB - you need to configure VMs and migrate all workloads\nC - also overhead with migrating \nD - downgrading the evaluation quality\nSo just switch runner seems a very easy option","comment_id":"1179501"}],"upvote_count":"1"}],"timestamp":"1698617460.0","content":"Selected Answer: A\n\"Evaluator leverages the TensorFlow Model Analysis library to perform the analysis, which in turn use Apache Beam for scalable processing.\" Since Dataflow is Google Cloud's serverless Apache Beam offering, this option can easily be implemented to address the issue while leaving the evaluation logic as such identical\n\nhttps://www.tensorflow.org/tfx/guide/evaluator#evaluator_and_tensorflow_model_analysis"},{"timestamp":"1739841240.0","comment_id":"1358067","upvote_count":"1","poster":"NamitSehgal","content":"Selected Answer: A\nTFMA Integration with Dataflow"},{"poster":"gscharly","content":"Selected Answer: A\nwith D we're downgrading evaluation. Dataflow is serverless so no infrastructure overhead is included","timestamp":"1713691620.0","upvote_count":"2","comment_id":"1199572"},{"comment_id":"1073522","upvote_count":"2","poster":"pico","timestamp":"1700243820.0","content":"Selected Answer: D\nLimiting Metrics: TensorFlow Model Analysis (TFMA) allows you to define a subset of metrics that you are interested in during the evaluation step. By using tfma.MetricsSpec(), you can specify a subset of metrics to be computed during the evaluation, which can help reduce the memory requirements.\n\nOut-of-Memory Error: Out-of-memory errors during model evaluation often occur when the system is trying to compute and store a large number of metrics, especially if the model or dataset is large. By limiting the number of metrics using tfma.MetricsSpec(), you can potentially reduce the memory footprint and resolve the out-of-memory error."},{"poster":"PST21","content":"Based on the question's context, the correct option to stabilize the pipeline without downgrading the evaluation quality while minimizing infrastructure overhead is:\n\nD. Add tfma.MetricsSpec() to limit the number of metrics in the evaluation step.\n\nThe question specifies that the evaluation step is failing due to an out-of-memory error. In such a scenario, limiting the number of metrics to be computed during evaluation using tfma.MetricsSpec() can help reduce memory requirements and potentially resolve the out-of-memory issue.","comment_id":"957639","upvote_count":"1","timestamp":"1689860400.0"},{"comment_id":"944834","poster":"tavva_prudhvi","content":"Selected Answer: D\nBy adding tfma.MetricsSpec(), you can limit the number of metrics that are computed during the evaluation step, thus reducing the memory requirement. This will help stabilize the pipeline without downgrading the evaluation quality, while minimizing infrastructure overhead. This option is a quick and easy solution that can be implemented without significant changes to the pipeline or infrastructure.\n\nOption A: Including the flag -runner=DataflowRunner in beam_pipeline_args to run the evaluation step on Dataflow may help to increase memory availability, but it may also increase infrastructure overhead.","upvote_count":"1","timestamp":"1688660880.0","comments":[{"upvote_count":"1","content":"it seems, Option D might reduce memory usage, it could potentially compromise the evaluation quality by not considering all the necessary metrics. Confused in A/D!","poster":"tavva_prudhvi","comment_id":"964022","timestamp":"1690391760.0"}]},{"timestamp":"1682617800.0","content":"Selected Answer: D\nD does not harm the evaluation quality.","poster":"Gudwin","comment_id":"882898","upvote_count":"1"},{"comment_id":"879486","upvote_count":"2","content":"Selected Answer: A\nSurely removing evaluation metrics downgrades the quality of the evaluation","poster":"[Removed]","timestamp":"1682349360.0"},{"timestamp":"1682270760.0","comment_id":"878678","content":"I'm not very sure, but wouldn't be A?.D is degrading evaluation quality (if you're getting less metrics, then the evaluation is worse, at least less complete)","upvote_count":"2","poster":"frangm23"},{"poster":"Yajnas_arpohc","content":"Selected Answer: A\nTFX 0.30 and above adds an interface, with_beam_pipeline_args, for extending the pipeline level beam args per component\n\ntfma.MetricSpec() OOB has recommended metrics; reducing any further might not serve the purpose.","upvote_count":"2","timestamp":"1679293440.0","comment_id":"844585"},{"timestamp":"1678190760.0","poster":"TNT87","comment_id":"831843","content":"Selected Answer: D\nAdd tfma.MetricsSpec () to limit the number of metrics in the evaluation step.\n\nLimiting the number of metrics in the evaluation step using tfma.MetricsSpec() can reduce the memory usage during evaluation and address the out-of-memory error. This can help stabilize the pipeline without downgrading the evaluation quality or incurring additional infrastructure overhead. Running the evaluation step on Dataflow or custom Compute Engine VMs can be resource-intensive and expensive, while migrating the pipeline to Kubeflow would require additional setup and configuration.\n\n\n\nANSWER D","upvote_count":"4"},{"content":"A is wrong , it does not even make sense , the default runner for evaluator component of TFX is data flow so setting runner to dataflow does not change anything , the answer is D because it does not include the any infrastructure minpulation and reduce the memory useable of the TfX component","poster":"Ml06","upvote_count":"2","comment_id":"829237","timestamp":"1677953580.0","comments":[{"poster":"f084277","upvote_count":"1","timestamp":"1731551100.0","content":"It uses the beam local runner by default, not Dataflow. You are wrong.","comment_id":"1311683"},{"upvote_count":"1","poster":"TNT87","comment_id":"831544","content":"https://www.tensorflow.org/tfx/guide/evaluator","timestamp":"1678161960.0"}]},{"comments":[{"upvote_count":"1","poster":"TNT87","content":"Answer D","timestamp":"1678190820.0","comment_id":"831846"}],"upvote_count":"2","timestamp":"1676536380.0","comment_id":"810447","poster":"TNT87","content":"Selected Answer: A\nAnswer A"},{"upvote_count":"4","comment_id":"798925","content":"Selected Answer: A\nhttps://blog.tensorflow.org/2020/03/tensorflow-extended-tfx-using-apache-beam-large-scale-data-processing.html","poster":"RaghavAI","timestamp":"1675610100.0"}],"question_id":59,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/98090-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":""},{"id":"8Eo8Yl0dUmoH8clgzbmj","topic":"1","question_images":[],"question_id":60,"choices":{"C":"Apply one-hot encoding on the categorical variables in the test data","B":"Randomly redistribute the data, with 70% for the training set and 30% for the test set","D":"Collect more data representing all categories","A":"Use sparse representation in the test set."},"timestamp":"2023-02-16 09:00:00","answers_community":["C (65%)","13%","13%","9%"],"answer":"C","answer_ET":"C","answer_description":"","exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/99379-exam-professional-machine-learning-engineer-topic-1-question/","answer_images":[],"unix_timestamp":1676534400,"isMC":true,"question_text":"You are developing an ML model using a dataset with categorical input variables. You have randomly split half of the data into training and test sets. After applying one-hot encoding on the categorical variables in the training set, you discover that one categorical variable is missing from the test set. What should you do?","discussion":[{"content":"Selected Answer: C\nThe crucial point is that the same encoding scheme must be applied to both the training and test sets.","timestamp":"1739841420.0","comment_id":"1358069","poster":"NamitSehgal","upvote_count":"1"},{"upvote_count":"1","comment_id":"1281591","poster":"baimus","comments":[{"content":"Yes I agree! Wish they worded it better.","timestamp":"1741266720.0","poster":"Futtsie","upvote_count":"1","comment_id":"1365879"}],"timestamp":"1725977460.0","content":"Selected Answer: C\nI've very grudgingly ticket C, as the question is missing \"handle the missing category by one hot encoding all zeros for the missing feature column\". It otherwise doesn't make sense as will have the wrong amount of entries."},{"timestamp":"1713751260.0","comments":[{"poster":"baimus","content":"But your description includes a missing critical step that the question is missing to make it make sense.","comment_id":"1281585","upvote_count":"1","timestamp":"1725977220.0"}],"comment_id":"1199918","poster":"fitri001","upvote_count":"2","content":"Selected Answer: C\nThe correct approach is to handle the missing category during one-hot encoding of the test data. Here's how to address this issue:\n\nIdentify the Missing Category: After applying one-hot encoding to the training set, compare the categories (unique values) present in the training data with the categories in the test data. This will reveal the missing category.\n\nAdd a Column for the Missing Category in the Test Data: Include a new column in the test data specifically for the missing category. Initialize the values in this column with 0.\n\nApply One-Hot Encoding to the Test Data: Now that the test data includes a column for the missing category, proceed with one-hot encoding the categorical variables in the test data. This will ensure the test data has the same structure as the encoded training data."},{"poster":"CHARLIE2108","comment_id":"1144740","timestamp":"1707412020.0","content":"Selected Answer: C\nAnswer C","upvote_count":"1"},{"upvote_count":"3","timestamp":"1692767100.0","content":"Selected Answer: C\nAnswer options analysis:\n\nC. Since one categorical variable is missing from the test set, (As I understand: “a categorical variable is in the test but not in train”) apply one hot encoding (trained with the train set?) to the test set, for the variables not present in train we just would obtain an array of all 0’s, so that would be OK.\nD. That data collection could be not feasible depending on the real-world-problem.\nB. Randomness would not always fix the problem.\nA. Not recommended to use different representations for train/test. Sparse representation doesn't magically recover missing categories; it's a way to efficiently store data with a large number of zeros.\n\nI would go with C.","poster":"Nxtgen","comment_id":"987937"},{"comment_id":"946262","content":"Selected Answer: C\nC but not really sure","timestamp":"1688799840.0","upvote_count":"1","poster":"SamuelTsch"},{"upvote_count":"3","comment_id":"906608","timestamp":"1685014680.0","poster":"Scipione_","content":"Selected Answer: C\nYou must apply one hot enconding alsto for the test dataset. However, i find this answer incomplete.","comments":[{"upvote_count":"1","timestamp":"1725977280.0","content":"Yeah 100% - it's missing the \"but make sure it deals with the missing category by adding a \"missing\" or something to it so the one hot representation has the right number of items.","comment_id":"1281587","poster":"baimus"}]},{"poster":"nescafe7","timestamp":"1684920420.0","content":"Selected Answer: D\nAdd data to the test set to get the same OHE","comment_id":"905711","comments":[{"timestamp":"1688660160.0","poster":"tavva_prudhvi","content":"Option D (collecting more data) may not be feasible or necessary if the missing category is not significant or if one-hot encoding is sufficient to handle it.","upvote_count":"2","comment_id":"944824"}],"upvote_count":"3"},{"comments":[{"timestamp":"1698327780.0","content":"https://cloud.google.com/vertex-ai/docs/tabular-data/data-splits#classification-random\nI think it's applicable to VertexAI auto ML only","comment_id":"1054636","upvote_count":"1","poster":"maukaba"},{"timestamp":"1683734520.0","comment_id":"894158","upvote_count":"1","poster":"M25","content":"Sparse representation is one “in which only nonzero values are stored”, excluding [Option A]: https://developers.google.com/machine-learning/crash-course/representation/feature-engineering#sparse-representation.\nApplying “one-hot encoding” to the columns will not help finding the missing column, thus excluding [Option C]. No indication provided for a need to “collect more data”, excluding [Option D]."},{"poster":"julliet","comment_id":"906455","content":"it is possible that category is very rare and that is the reason we don't have it in the test. So I guess we should just apply the train data transformations and use one-hot","upvote_count":"2","timestamp":"1684997760.0"}],"poster":"M25","comment_id":"894157","timestamp":"1683734460.0","upvote_count":"2","content":"Selected Answer: B\n“Rows are selected for a data split randomly, but deterministically. (…) Training a new model with the same training data results in the same data split.” https://cloud.google.com/vertex-ai/docs/tabular-data/data-splits#classification-random. “Randomly redistribute data” [Option B] with different fractions, will result in a different data split. Having a higher fraction split of 70% for the training set will additionally help the model to better generalize (compared to only 50%), thus perform better when testing, the ultimate goal."},{"timestamp":"1682618940.0","upvote_count":"2","content":"Selected Answer: C\nBy using a sparse representation, you will be losing the information contained in the missing categorical variable. This could lead to the model making incorrect predictions on the test set.","poster":"Gudwin","comment_id":"882912"},{"poster":"wrosengren","content":"I agree with formazioneQl that if a different one hot encoding is used for the test set compared to the train set then the results would be poor. However, there is no problem with not having all combinations in the test set if all possibilities are present in the training set. So assuming that we are using the same mapping of data in the train and test set, I would vote C. If we don't encode the test set, the variable is meaningless anyways. So I would lean C.","timestamp":"1682485740.0","comment_id":"881131","upvote_count":"1"},{"comment_id":"873697","timestamp":"1681826700.0","poster":"formazioneQI","upvote_count":"3","comments":[{"timestamp":"1688660220.0","poster":"tavva_prudhvi","upvote_count":"1","content":"Option A (sparse representation) may not work well in this case, as it can lead to sparsity issues and affect the model's performance.","comment_id":"944826"}],"content":"Selected Answer: A\nSince one categorical variable is missing from the test set, C would result in a different number of columns in the training and test sets."},{"timestamp":"1678190700.0","upvote_count":"2","poster":"TNT87","content":"C. Apply one-hot encoding on the categorical variables in the test data.\n\nWhen using one-hot encoding on categorical variables, each unique value of the variable is represented as a separate binary variable. Therefore, it is important to ensure that the same set of binary variables is present in both the training and test datasets. Since one categorical variable is missing in the test set, the recommended approach is to apply one-hot encoding on the categorical variables in the test set to ensure that the same set of binary variables is present in both datasets.","comment_id":"831841"},{"comment_id":"810408","timestamp":"1676534400.0","content":"Selected Answer: C\nAnswer C","upvote_count":"1","poster":"TNT87"}]}],"exam":{"isImplemented":true,"numberOfQuestions":304,"name":"Professional Machine Learning Engineer","isBeta":false,"id":13,"lastUpdated":"11 Apr 2025","isMCOnly":true,"provider":"Google"},"currentPage":12},"__N_SSP":true}