{"pageProps":{"questions":[{"id":"bLooLWhrJd4WG36qsXYs","unix_timestamp":1615632240,"topic":"1","answers_community":["A (100%)"],"discussion":[{"upvote_count":"53","comment_id":"310878","comments":[{"upvote_count":"6","poster":"djgodzilla","comment_id":"381491","content":"from the same link : \nCan I export a backup?\nNo, you can't export a backup. You can only export instance data. See Exporting data from Cloud SQL to a dump in Cloud storage.","timestamp":"1639458000.0"}],"poster":"TAvenger","content":"https://cloud.google.com/sql/docs/mysql/backup-recovery/backups\nnot B: Automatic backups are made EVERY SINGLE DAY. You can set only the number of backups up to 365. Also you cannot choose your Archival storage as destination\nnot C: You cannot setup \"on-demand\" backup. User would have to make backups manually every month. Also you cannot choose your Archival storage as destination\nnot D: You cannot conver backup to export file. Also Coldline class is less cost-effective than Archival class.\n\nThe only option left is \"A\" \nYou can set up your job with any date/time schedule. You can export file to any storage with any storage class.","timestamp":"1631645760.0"},{"comment_id":"391726","upvote_count":"16","timestamp":"1640584980.0","poster":"JieHeng","content":"First need to understand backup vs export, two different concepts. - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups\n\nA – yes, you can export data from Cloud SQL to Cloud Storage- https://cloud.google.com/sql/docs/mysql/import-export/exporting#cloud-sql\n\nNot B, C, D – be it automatic or on-demand backup, according to the doc “No, you can't export a backup. You can only export instance data.”"},{"timestamp":"1735910040.0","poster":"kamee15","upvote_count":"1","content":"Selected Answer: A\nYes, the solution works and aligns with best practices:\n\n\"Set up an export job for the first of the month. Write the export file to an Archive class Cloud Storage bucket.\"\n\nThe Archive class is cost-effective for long-term storage, and exporting the database ensures a consistent snapshot for audit purposes. Ensure the export job is automated and retains the required files for three years by configuring appropriate lifecycle policies in Cloud Storage.","comment_id":"1336019"},{"upvote_count":"1","poster":"omunoz","comment_id":"1202950","timestamp":"1730009160.0","content":"Should be A:\nBackups are managed by Cloud SQL according to retention policies, and are stored separately from the Cloud SQL instance. Cloud SQL backups differ from an export uploaded to Cloud Storage, where you manage the lifecycle. Backups encompass the entire database. Exports can select specific contents.\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#:~:text=for%20more%20information.-,Backups%20versus%20exports,-Backups%20are%20managed"},{"comments":[{"timestamp":"1714029300.0","content":"cause you're the one wrong:\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#can_i_export_a_backup","upvote_count":"2","poster":"Abbru00","comment_id":"1053540"}],"poster":"santhush","timestamp":"1710568260.0","comment_id":"1008830","upvote_count":"1","content":"https://www.exam-answer.com/retain-month-end-copy-cloud-sql-mysql-database-three-years B is the correct answer.. I am not sure why people are posting wrong answers here."},{"content":"Selected Answer: A\nA is the correct answer, as in a you can export as per your requirement and then moved it to archive class , but in b,c,d you can't do that","timestamp":"1709695560.0","comment_id":"1000062","upvote_count":"1","poster":"Captain1212"},{"timestamp":"1699865700.0","content":"Option B is the best choice because it allows you to leverage the automatic first-of-the-month backup feature that is provided by Cloud SQL. Cloud SQL provides automated backups that can be configured to run at specific times, including the first of the month. By retaining the first-of-the-month backup for three years, you can be sure that you have a complete copy of the database for that month.","poster":"akkinepallyn","comment_id":"896444","upvote_count":"1"},{"comment_id":"828701","upvote_count":"1","poster":"Vismaya","timestamp":"1693799460.0","content":"Answer A"},{"comment_id":"783601","timestamp":"1689953340.0","content":"Answer A is the correct one according to \"https://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler\" and \"https://cloud.google.com/sql/docs/mysql/backup-recovery/backups#backups_versus_exports\".","poster":"researched_answer_boi","upvote_count":"1"},{"comment_id":"717522","poster":"Kopy","content":"Selected Answer: A\nSo Answer is A.\nYou can't export back up. Very clear.\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#can_i_export_a_backup","timestamp":"1684004100.0","upvote_count":"1"},{"poster":"Charumathi","content":"Selected Answer: A\nA is correct answer,\nExport the SQL month end data as a CSV file to cloud storage bucket, and move the data to Archival Storages for 3 years for audit purpose.\n\nhttps://cloud.google.com/sql/docs/mysql/import-export?authuser=1","timestamp":"1681034160.0","upvote_count":"2","comment_id":"690047"},{"upvote_count":"1","content":"Selected Answer: A\nA. Although Cloud SQL doesn't provide a built-in way to automate database exports, you can build your own automation tool using several Google Cloud components.\nhttps://cloud.google.com/sql/docs/mysql/import-export#automating_export_operations","timestamp":"1680093120.0","comment_id":"682676","poster":"learn_GCP"},{"poster":"Cornholio_LMC","comment_id":"678005","upvote_count":"4","content":"had this question today","timestamp":"1679682780.0"},{"content":"B is correct","comment_id":"676072","poster":"butki","timestamp":"1679493360.0","upvote_count":"1"},{"timestamp":"1677853920.0","upvote_count":"2","content":"A seems right to me key word: \"month-end copy\"","poster":"snkhatri","comment_id":"658452"},{"timestamp":"1673569020.0","comment_id":"630705","poster":"patashish","upvote_count":"4","content":"Correct Ans - C \nOn demand backup\n\nYou can create a backup at any time (Here we need backup a month-end copy of the database for three years). You can create on-demand backups for any instance, whether the instance has automatic backups enabled or not.\n\nReason : \n1) you can't export a backup. You can only export instance data so export option A is out from answer.\nBackups encompass the entire database. Exports can select specific contents.\nAs per question You need to retain a month-end copy of the database not specific contents.\n\n2) Automated backups are taken daily, within a 4-hour backup window. Up to seven most recent backups are retained, by default.\n*Cost** to store all backups .. \n\n3) Option D not applicable **Coldline class** *Cost*"},{"content":"why not D? it is the only one that doesnt store it as Archive class, and since it is for Audit purposes this cant be used\n as Archive allows LESS than one access per year.","comment_id":"542666","poster":"mk1471","timestamp":"1659898440.0","upvote_count":"2"},{"content":"Selected Answer: A\nI would go with export not backup. Question stated that this is copy. In addition restore of three years old backup might be not possible in newer version. In addition you can retain max 365 backups.","timestamp":"1654614360.0","comment_id":"496200","poster":"look1","upvote_count":"4"},{"content":"As question itself states that we need month end backup, automatic backup runs daily which not required as per the question. https://cloud.google.com/sql/docs/mysql/backup-recovery/backups","timestamp":"1650000240.0","poster":"jackwillis","upvote_count":"3","comment_id":"462427"},{"comment_id":"451954","upvote_count":"5","content":"A is incorrect because there is no need to create an export job as the export functionality is built-in with Cloud SQL- i would go with B","poster":"vamgcp","timestamp":"1648320360.0"},{"timestamp":"1637432580.0","content":"B is correct","upvote_count":"1","comment_id":"362339","poster":"arsh1916"},{"poster":"jahnu","comment_id":"341982","content":"Ans: B Backups are managed by Cloud SQL according to retention policies, and are stored separately from the Cloud SQL instance. so take the back up and store in Bucket.\ncheck: https://cloud.google.com/sql/docs/mysql/backup-recovery/backups","timestamp":"1635072240.0","upvote_count":"2"},{"content":"Answer is A, there is no way to take the backups made by SQL-MySQL and send them to Cloud Storage so you have to make an export... Default backup only allows 365 retention points so you need to make export I mean program exports and send them to Cloud Datastore so A.\n\nBackups are managed by Cloud SQL according to retention policies, and are stored separately from the Cloud SQL instance. Cloud SQL backups differ from an export uploaded to Cloud Storage, where you manage the lifecycle. Backups encompass the entire database. Exports can select specific contents.\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups","upvote_count":"1","timestamp":"1634503200.0","comment_id":"337793","poster":"kopper2019"},{"upvote_count":"2","content":"A. Set up an export job for the first of the month. Write the export file to an Archive class Cloud Storage bucket.","comment_id":"310995","timestamp":"1631653680.0","poster":"GCP_Student1"},{"upvote_count":"3","comment_id":"309630","poster":"victory108","content":"B- Save the automatic first-of-the-month backup for three years. Store the backup file in an Archive class Cloud Storage bucket.","comments":[{"timestamp":"1632626580.0","content":"A is correct one. Set up an export job for the first of the month. Write the export file to an Archive class Cloud Storage bucket.","comments":[{"poster":"gcpengineer","content":"you can not export backup","comments":[{"comment_id":"549961","poster":"[Removed]","upvote_count":"2","timestamp":"1660786680.0","content":"You can\nhttps://cloud.google.com/sql/docs/mysql/import-export/import-export-sql"}],"timestamp":"1643433060.0","upvote_count":"1","comment_id":"416584"}],"upvote_count":"1","poster":"victory108","comment_id":"320807"}],"timestamp":"1631522640.0"}],"answer_description":"","choices":{"C":"Set up an on-demand backup for the first of the month. Write the backup to an Archive class Cloud Storage bucket.","A":"Set up an export job for the first of the month. Write the export file to an Archive class Cloud Storage bucket.","B":"Save the automatic first-of-the-month backup for three years. Store the backup file in an Archive class Cloud Storage bucket.","D":"Convert the automatic first-of-the-month backup to an export file. Write the export file to a Coldline class Cloud Storage bucket."},"question_text":"You are working with a Cloud SQL MySQL database at your company. You need to retain a month-end copy of the database for three years for audit purposes.\nWhat should you do?","exam_id":1,"answer":"A","url":"https://www.examtopics.com/discussions/google/view/46865-exam-associate-cloud-engineer-topic-1-question-171/","timestamp":"2021-03-13 11:44:00","answer_images":[],"answer_ET":"A","isMC":true,"question_images":[],"question_id":81},{"id":"6Q4hwNaVxRtmkQ7DkLpG","question_text":"You are monitoring an application and receive user feedback that a specific error is spiking. You notice that the error is caused by a Service Account having insufficient permissions. You are able to solve the problem but want to be notified if the problem recurs. What should you do?","choices":{"C":"Create a custom log-based metric for the specific error to be used in an Alerting Policy.","A":"In the Log Viewer, filter the logs on severity 'Error' and the name of the Service Account.","B":"Create a sink to BigQuery to export all the logs. Create a Data Studio dashboard on the exported logs.","D":"Grant Project Owner access to the Service Account."},"exam_id":1,"answer_ET":"C","unix_timestamp":1615513080,"answer":"C","answer_description":"","timestamp":"2021-03-12 02:38:00","url":"https://www.examtopics.com/discussions/google/view/46607-exam-associate-cloud-engineer-topic-1-question-172/","question_images":[],"question_id":82,"answers_community":["C (100%)"],"topic":"1","discussion":[{"upvote_count":"25","poster":"GCP_Student1","content":"C. Create a custom log-based metrics for the specific error to be used in an Alerting Policy.","comment_id":"310999","timestamp":"1631653920.0"},{"poster":"greatsam321","content":"C seems to be the right answer.","upvote_count":"11","timestamp":"1631406720.0","comment_id":"308509"},{"content":"Selected Answer: C\nhttps://cloud.google.com/logging/docs/logs-based-metrics","poster":"pumajd","upvote_count":"2","comment_id":"1166557","timestamp":"1725541560.0"},{"poster":"Captain1212","timestamp":"1709695680.0","comment_id":"1000064","upvote_count":"4","content":"Selected Answer: C\nUser wants to check the if problem recurs, that can be only possible by Alert, C is the correct option"},{"content":"C. The keyword here is \"want to be notified\" that means an alert.","upvote_count":"2","poster":"_F4LLEN_","comment_id":"873845","timestamp":"1697645820.0"},{"poster":"Charumathi","timestamp":"1681034460.0","upvote_count":"2","content":"Selected Answer: C\nC is the correct answer,\nSince the problem is resolved, We need to monitor if the error recurs, hence we create a custom log based metrics to monitor only the particular service account.","comment_id":"690049"},{"poster":"snkhatri","timestamp":"1677854040.0","comment_id":"658453","content":"C as Keyword \"want to be notified if the problem recurs\"","upvote_count":"1"},{"timestamp":"1671886260.0","poster":"AzureDP900","upvote_count":"1","content":"C right","comment_id":"621597"},{"upvote_count":"1","content":"C is correct.","timestamp":"1670972520.0","poster":"AzureDP900","comment_id":"615958"},{"upvote_count":"3","poster":"PAUGURU","content":"Selected Answer: C\nC - the only answer that outputs a notification","timestamp":"1666793700.0","comment_id":"592501"},{"comment_id":"531587","poster":"Raz0r","upvote_count":"3","content":"Selected Answer: C\n\"C\" is right, the only answer that includes setting up an alert!","timestamp":"1658689860.0"},{"poster":"Wolf13ts","comment_id":"528315","upvote_count":"1","timestamp":"1658304360.0","content":"Selected Answer: C\nAnswer should be C"},{"poster":"alaahakim","content":"Ans: C","comment_id":"482135","timestamp":"1652991720.0","upvote_count":"2"},{"poster":"AD_0525","timestamp":"1639837020.0","upvote_count":"4","comment_id":"384799","content":"You want to be aerted next time, so only option C meets that criteria."},{"content":"C is correct : You are able to solve the problem but want to be notified if the problem recurs.","comment_id":"370865","poster":"Enzo","upvote_count":"4","timestamp":"1638271320.0"},{"timestamp":"1637432580.0","upvote_count":"1","content":"A is correct","poster":"arsh1916","comment_id":"362340"},{"timestamp":"1634939220.0","comments":[{"upvote_count":"1","content":"The question is saying \"want to be notified if the problem recurs\", I don't see how A meets that requirement.","poster":"lxgywil","timestamp":"1636509720.0","comment_id":"353344"},{"poster":"Petza","timestamp":"1635449820.0","comment_id":"344869","upvote_count":"2","content":"A. \"Every time the dashboard is refreshed, it pulls new data from the view, which in turn dynamically reflects the latest data in BigQuery\". Data Science on the Google Cloud Platform: Implementing End-to-End Real-Time (C)"},{"content":"Question 178","poster":"kopper2019","timestamp":"1634939280.0","upvote_count":"1","comment_id":"341286"},{"upvote_count":"2","content":"I think it's D. Can anyone confirm?","timestamp":"1635184860.0","comment_id":"342765","poster":"mj98"}],"poster":"kopper2019","content":"You are managing a project for the Business Intelligence (BI) department in your company. A data pipeline ingests data into BigQuery via streaming. You want the users in the BI department to be able to run the custom SQL queries against the latest data in BigQuery. What should you do?\n\nA. Create a Data Studio dashboard that uses the related BigQuery tables as a source and give the BI team view access to the Data Studio dashboard.\nB. Create a Service Account for the BI team and distribute a new private key to each member of the BI team.\nC. Use Cloud Scheduler to schedule a batch Dataflow job to copy the data from BigQuery to the BI team’s internal data warehouse.\nD. Assign the IAM role of BigQuery User to a Google Group that contains the members of the BI team.\n\nit's A","upvote_count":"2","comment_id":"341285"},{"content":"You have developed an application that consists of multiple microservices, with each microservice packaged in its own Docker container image. You want to deploy the entire application on Google Kubernetes Engine so that each microservice can be scaled individually. What should you do?\n A. Create and deploy a Custom Resource Definition per microservice.\n B. Create and deploy a Docker Compose File.\n C. Create and deploy a Job per microservice.\n D. Create and deploy a Deployment per microservice.\n\nI think is D","upvote_count":"3","timestamp":"1634504160.0","comment_id":"337799","poster":"kopper2019","comments":[{"comment_id":"339774","comments":[{"upvote_count":"1","poster":"AmineHM","timestamp":"1634845260.0","content":"It's D, yes !","comments":[{"content":"its c; Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.","timestamp":"1635138660.0","poster":"noreen","comments":[{"comment_id":"342350","content":"b * B. Create and deploy a Docker Compose File.","timestamp":"1635139860.0","upvote_count":"1","poster":"noreen"}],"comment_id":"342342","upvote_count":"2"}],"comment_id":"340599"}],"upvote_count":"1","content":"I think B","poster":"AmineHM","timestamp":"1634751000.0"}]},{"comment_id":"337798","upvote_count":"3","comments":[{"timestamp":"1634751060.0","upvote_count":"1","content":"it's D","poster":"AmineHM","comment_id":"339776"}],"timestamp":"1634504100.0","content":"You are about to deploy a new Enterprise Resource Planning (ERP) system on Google Cloud. The application holds the full database in-memory for fast data access, and you need to configure the most appropriate resources on Google Cloud for this application. What should you do?\n A. Provision preemptible Compute Engine instances.\n B. Provision Compute Engine instances with GPUs attached.\n C. Provision Compute Engine instances with local SSDs attached.\n D. Provision Compute Engine instances with M1 machine type.\n\nAnswer is D type M,","poster":"kopper2019"},{"comments":[],"comment_id":"337794","content":"hey guys go Question number 1 for New Question from 173 to 179","upvote_count":"3","poster":"kopper2019","timestamp":"1634503560.0"},{"comment_id":"327562","poster":"NARWAL","timestamp":"1633288140.0","content":"based on the \"You are able to solve the problem but want to be notified if the problem recurs.\" , we definitely need to create an alerting policy. So Answer should be C.\n\nhttps://cloud.google.com/logging/docs/logs-based-metrics/charts-and-alerts","upvote_count":"3"},{"timestamp":"1631646420.0","content":"C.\nJust checked in GC Console. Created custom log-based metric and set up an allert.","upvote_count":"3","comment_id":"310896","poster":"TAvenger"},{"content":"Answer is C.\nhttps://cloud.google.com/logging/docs/logs-based-metrics/charts-and-alerts","poster":"jackycc","comment_id":"310208","upvote_count":"4","timestamp":"1631583480.0"},{"comment_id":"308481","poster":"ravixkumar","upvote_count":"5","content":"Ans : C","timestamp":"1631403480.0"}],"isMC":true,"answer_images":[]},{"id":"EyE3VZEqN1YzRIfIbqpE","question_id":83,"url":"https://www.examtopics.com/discussions/google/view/51203-exam-associate-cloud-engineer-topic-1-question-173/","answer_images":[],"isMC":true,"answer":"C","timestamp":"2021-04-30 10:53:00","answers_community":["C (96%)","4%"],"choices":{"B":"Use Cloud SQL for data storage.","D":"Use Firestore for data storage.","C":"Use Cloud Spanner for data storage.","A":"Use Cloud Bigtable for data storage."},"question_images":[],"unix_timestamp":1619772780,"question_text":"You are developing a financial trading application that will be used globally. Data is stored and queried using a relational structure, and clients from all over the world should get the exact identical state of the data. The application will be deployed in multiple regions to provide the lowest latency to end users. You need to select a storage option for the application data while minimizing latency. What should you do?","answer_ET":"C","discussion":[{"timestamp":"1656303000.0","content":"C, Cloud Spanner, keywords are globally, relational structure and lastly \"clients from all over the world should get the exact identical state of the data\" which implies strong consistency is needed.","comment_id":"391730","poster":"JieHeng","upvote_count":"17"},{"comment_id":"1336020","content":"Selected Answer: C\nThe best option is:\n\nC. Use Cloud Spanner for data storage.\n\nReason:\nCloud Spanner is the only Google Cloud database that provides:\n\nGlobal consistency: Ensures all users worldwide see the exact same state of the data.\nRelational structure: Fully supports SQL queries and relational database schema.\nLow latency: Replicates data across multiple regions to minimize read/write latency for global users.\nScalability: Designed to handle high-scale applications like financial trading.\nOther options don't meet the requirements:\n\nA. Cloud Bigtable: No relational structure; optimized for wide-column use cases, not relational data.\nB. Cloud SQL: Limited to regional deployments, not suitable for globally distributed applications.\nD. Firestore: Designed for document-based structures, not ideal for relational database use.","upvote_count":"1","timestamp":"1735910400.0","poster":"kamee15"},{"poster":"Captain1212","comment_id":"1000065","content":"Selected Answer: C\nquestion demands, exact state of data and minimum latency to users , for this cloud spanner is the only option","upvote_count":"4","timestamp":"1725586200.0"},{"content":"Selected Answer: C\nfinancial trading application\nrelational structure \nmultiple regions","timestamp":"1713433680.0","poster":"sabrinakloud","comment_id":"873467","upvote_count":"2"},{"upvote_count":"4","poster":"ashtonez","content":"Selected Answer: C\nC, always you need to select BBDD check for data analysis bigquery, something very big or fast bigtable, something with HA cloud sql, and something globally available cloud spanner, the key here is globaly available","timestamp":"1710070020.0","comment_id":"834893"},{"comment_id":"690051","content":"Selected Answer: C\nC is the correct answer,\nKeywords, Financial data (large data) used globally, data stored and queried using relational structure (SQL), clients should get exact identical copies(Strong Consistency), Multiple region, low latency to end user, select storage option to minimize latency.","timestamp":"1696846140.0","upvote_count":"3","comments":[{"upvote_count":"1","timestamp":"1696846320.0","comment_id":"690054","poster":"Charumathi","content":"Spanner powers business-critical applications in retail, financial services, gaming, media and entertainment, technology, healthcare and more.\n\nUse cases for Cloud Spanner\nhttps://www.youtube.com/watch?v=1b4flZwAQfM&t=1s"}],"poster":"Charumathi"},{"comment_id":"682437","content":"Selected Answer: C\nit's C 100%\nGuys come on, it's a pretty straight forward scenario.\nif you have the keywords \"relational DB\" and the word \"Globally\" in a sentence always go for Cloud Spanner.","poster":"ale_brd_111","timestamp":"1695972300.0","upvote_count":"4"},{"upvote_count":"1","content":"Selected Answer: C\nC. is the answer","comment_id":"681909","poster":"learn_GCP","timestamp":"1695917460.0"},{"upvote_count":"1","timestamp":"1695732420.0","comment_id":"679752","content":"Why not A. Big table as per keywords relational, global and low latency","comments":[{"content":"BigTable is not a relational database. Everything else is true for it but it a noSQL non Relational Database.","comment_id":"774952","poster":"tonyg_2023","timestamp":"1705190280.0","upvote_count":"3"}],"poster":"sri333"},{"timestamp":"1694699820.0","comment_id":"669077","content":"Selected Answer: C\nC is the answer.\n\nCloud Spanner is a global relational database.","poster":"zellck","upvote_count":"1"},{"timestamp":"1693829460.0","upvote_count":"1","content":"Selected Answer: C\nShould be the correct answer.","comment_id":"659246","poster":"Mr_MIXER007"},{"upvote_count":"1","content":"C, Cloud Spanner. Globally and trading (tend to receive 1000s Records per second) are key here.","timestamp":"1693460760.0","poster":"GayuSundar","comment_id":"654799"},{"poster":"VietmanOfficiel","upvote_count":"1","content":"Selected Answer: B\nWhy not CLoud Sql with replicas in multiple region to serve \"global\" ?","comment_id":"651321","timestamp":"1692889380.0"},{"content":"This is straight forward question, Answer is C.","comment_id":"615956","timestamp":"1686689940.0","poster":"AzureDP900","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nC is correct - Relational and Global","timestamp":"1682518380.0","comment_id":"592499","poster":"PAUGURU"},{"timestamp":"1668896100.0","comment_id":"482128","content":"Choose - C","poster":"alaahakim","upvote_count":"2"},{"poster":"associatecloudexamuser","content":"Yes. C is right answer","timestamp":"1657726320.0","comment_id":"405532","upvote_count":"3"},{"comment_id":"347559","upvote_count":"4","timestamp":"1651482240.0","content":"C) Spanner - Global, low latency, relational","poster":"Fidget_"},{"timestamp":"1651413720.0","upvote_count":"2","comment_id":"347041","poster":"Rightsaidfred","content":"Yes C, Cloud Spanner"},{"poster":"norrec9","upvote_count":"2","content":"C is the answer","timestamp":"1651343220.0","comment_id":"346381"},{"timestamp":"1651324560.0","content":"C Cloud Spanner to store relational data","poster":"Biju1","upvote_count":"2","comment_id":"346213"}],"exam_id":1,"topic":"1","answer_description":""},{"id":"giwnJ9Nzgwn5BKJBRX1k","discussion":[{"poster":"Rightsaidfred","comment_id":"347045","upvote_count":"31","timestamp":"1619878020.0","content":"Yes D, M1 Machine types for ERP i.e. SAP-HANA:\nhttps://cloud.google.com/compute/docs/machine-types"},{"comment_id":"1336023","timestamp":"1735910760.0","upvote_count":"1","poster":"kamee15","content":"Selected Answer: D\nThe best option is:\n\nD. Provision Compute Engine instances with M1 machine type.\n\nReason:\nThe M1 machine type is designed for applications that require large amounts of memory, making it ideal for in-memory databases like an ERP system. It provides high memory-to-CPU ratios, ensuring the database can be fully loaded into memory for fast data access.\n\nOther options don't align with the requirements:\n\nA. Preemptible Compute Engine instances: Not suitable for critical applications like ERP systems as they can be terminated at any time.\nB. Compute Engine instances with GPUs attached: GPUs are optimized for parallel processing tasks like machine learning, not for memory-intensive in-memory databases.\nC. Compute Engine instances with local SSDs attached: While local SSDs provide fast storage, the application's requirement is for in-memory data, not fast storage."},{"poster":"denno22","comment_id":"1295498","upvote_count":"1","timestamp":"1728550440.0","content":"Selected Answer: D\nD"},{"poster":"Captain1212","comment_id":"1000068","content":"Selected Answer: D\nD is the right answer as M1 one are best for the databases i.e SAP hana","upvote_count":"3","timestamp":"1693964340.0"},{"upvote_count":"4","content":"C. Provision Compute Engine instances with local SSDs attached.\n\nThe best option for an ERP system that holds the full database in-memory for fast data access is to provision Compute Engine instances with local SSDs attached. Local SSDs offer high input/output operations per second (IOPS) and low latency, which can significantly improve the performance of in-memory databases. Preemptible Compute Engine instances are designed for short-lived and fault-tolerant workloads and are not recommended for a critical system like an ERP. GPUs are typically used for specialized compute-intensive workloads like machine learning and deep learning. M1 machine type is a general-purpose machine type and may not provide enough performance for an in-memory database.","comment_id":"808854","poster":"Bobbybash","timestamp":"1676408820.0"},{"content":"Selected Answer: D\nD is the correct answer,\nM1 machine series \nMedium in-memory databases such as SAP HANA\nTasks that require intensive use of memory with higher memory-to-vCPU ratios than the general-purpose high-memory machine types.\nIn-memory databases and in-memory analytics, business warehousing (BW) workloads, genomics analysis, SQL analysis services.\nMicrosoft SQL Server and similar databases.","upvote_count":"3","poster":"Charumathi","comment_id":"690058","timestamp":"1665310800.0"},{"upvote_count":"1","content":"D, keyword \"Full database in-memory \"","poster":"snkhatri","timestamp":"1662208560.0","comment_id":"658456"},{"content":"Selected Answer: D\nVote for D as the right answer. M1 machine type is the one of two Memory-Optimized machine types in GCP.\n\nhttps://cloud.google.com/compute/docs/machine-types","timestamp":"1659765540.0","upvote_count":"2","poster":"ryumada","comment_id":"643213","comments":[{"timestamp":"1659765660.0","upvote_count":"1","comment_id":"643214","poster":"ryumada","content":"Read this also to see the difference of the two.\n\nhttps://cloud.google.com/compute/docs/memory-optimized-machines"}]},{"upvote_count":"2","comment_id":"639377","timestamp":"1659122940.0","content":"Selected Answer: D\nD: M1 Machine types for ERP i.e. SAP-HANA\n\nMedium-large in-memory databases such as SAP HANA\nIn-memory databases and in-memory analytics\nMicrosoft SQL Server and similar databases","poster":"abirroy"},{"comment_id":"615954","upvote_count":"1","timestamp":"1655153580.0","poster":"AzureDP900","content":"D is right choice, when answer selected by author is C doesn't make any sense. User also need to understand the services well before attempting exam."},{"upvote_count":"1","timestamp":"1653397860.0","comment_id":"606728","content":"I chose option D, because the first three didn't make any sense :D","poster":"akshaychavan7"},{"comment_id":"531584","poster":"Raz0r","content":"D!!! \nThe \"M1\" VM type is right, it offers between 1.4TB and 3.75TB of RAM.","timestamp":"1643058480.0","upvote_count":"3"},{"content":"https://cloud.google.com/compute/docs/machine-types#:~:text=databases%20such%20as-,SAP%20HANA,-In%2Dmemory%20databases\n\nhttps://www.sap.com/india/products/hana.html#:~:text=is%20SAP%20HANA-,in%2Dmemory,-database%3F","comment_id":"508063","poster":"[Removed]","upvote_count":"1","timestamp":"1640280240.0"},{"timestamp":"1639407660.0","poster":"jaffarali","comment_id":"500670","content":"Selected Answer: D\nCorrect Answer is D","upvote_count":"1"},{"timestamp":"1637360100.0","content":"D is the Answer","comment_id":"482130","poster":"alaahakim","upvote_count":"1"},{"content":"\"m1-megamem-96\" can attach local SSD. Correct is D.","poster":"maggieli","timestamp":"1636433640.0","comment_id":"474617","upvote_count":"1"},{"upvote_count":"2","poster":"jackdbd","comment_id":"414020","timestamp":"1627225680.0","content":"Note that VM instances m1-megamem-96 are both from the M1 family AND can have local SSDs attached to them.\nhttps://cloud.google.com/compute/docs/memory-optimized-machines#m1_vms"},{"timestamp":"1626190680.0","comment_id":"405533","poster":"associatecloudexamuser","content":"Answer is D.\nApplications of Memory optimized VMs are,\n1. Medium-large in-memory databases such as SAP HANA\n2. In-memory databases and in-memory analytics\n3. Microsoft SQL Server and similar databases","upvote_count":"4"},{"upvote_count":"4","content":"D, \"The application holds the full database in-memory for fast data access\", so it'll be more appropriate to use memory-optimized machine types - https://cloud.google.com/compute/docs/machine-types#m1_machine_types","poster":"JieHeng","comment_id":"391731","timestamp":"1624767180.0"},{"content":"Here the question is about data access not work load type, hence local SSDs are best option. So it is C","poster":"AD_0525","comments":[{"poster":"csrazdan","timestamp":"1657410900.0","upvote_count":"3","comment_id":"629380","content":"Local SSD is used to improve IO. A memory store or GCE with a larger memory footprint will help with fast data access. D is the correct answer."}],"timestamp":"1624018080.0","upvote_count":"2","comment_id":"384792"},{"upvote_count":"2","content":"C: All the options use compute engine. A is incorrect because it uses preemptible instances, which only runs for max 24 hours. Hence A is not an option. Then look at the key \"fast data access\". Local SSD is the fastest","poster":"sanhoo","comments":[],"comment_id":"374196","timestamp":"1622795400.0"},{"comment_id":"362341","content":"D 100%","upvote_count":"1","poster":"arsh1916","timestamp":"1621527780.0"},{"content":"ERP solutions - M1/M2. Its D","comment_id":"358738","timestamp":"1621170420.0","poster":"Ashii","upvote_count":"1"},{"upvote_count":"1","comment_id":"346374","content":"D is the answer","timestamp":"1619806920.0","poster":"norrec9"}],"url":"https://www.examtopics.com/discussions/google/view/51202-exam-associate-cloud-engineer-topic-1-question-174/","unix_timestamp":1619772720,"answer":"D","answer_ET":"D","isMC":true,"answers_community":["D (100%)"],"topic":"1","answer_description":"","timestamp":"2021-04-30 10:52:00","question_id":84,"question_text":"You are about to deploy a new Enterprise Resource Planning (ERP) system on Google Cloud. The application holds the full database in-memory for fast data access, and you need to configure the most appropriate resources on Google Cloud for this application. What should you do?","question_images":[],"exam_id":1,"answer_images":[],"choices":{"A":"Provision preemptible Compute Engine instances.","C":"Provision Compute Engine instances with local SSDs attached.","D":"Provision Compute Engine instances with M1 machine type.","B":"Provision Compute Engine instances with GPUs attached."}},{"id":"T6DKRzDWcHmcqzBLhOus","answers_community":["D (100%)"],"timestamp":"2021-04-30 10:52:00","answer":"D","answer_images":[],"isMC":true,"question_images":[],"answer_description":"","discussion":[{"poster":"obeythefist","upvote_count":"25","comments":[{"upvote_count":"2","timestamp":"1684934100.0","content":"Thanks for your insights! Makes sense.","poster":"akshaychavan7","comment_id":"606732"}],"timestamp":"1677652500.0","content":"I was a little unsure about this question, here's how I understand why D is the best answer\n\nA. Custom Resource Definition... we have docker containers already, which is an established kind of resource for Kubernetes. We don't need to create a whole new type of resource, so this is wrong.\nB. Docker Compose is a wholly different tool from Kubernetes. \nC. A Kubernetes job describes a specific \"task\" which involves a bunch of pods and things. It makes no sense to have one job per microservice, a \"Job\" would be a bunch of different microservices executing together. \nD. is the leftover, correct answer. You can add scaling to each Deployment, an important aspect of the question.","comment_id":"558619"},{"timestamp":"1651407600.0","comment_id":"346937","upvote_count":"20","content":"D is the correct answer","poster":"Kollipara"},{"poster":"taylz876","content":"Selected Answer: D\nTo deploy a microservices-based application on Google Kubernetes Engine (GKE), it's common to create and deploy a Deployment per microservice.\n\nD. Create and deploy a Deployment per microservice.\n\nHere's why:\n\nDeployment: In Kubernetes, a Deployment is a resource that allows you to define, create, and manage the desired number of replicas of your application. Each microservice can be independently managed and scaled using its own Deployment.\nThis approach provides the flexibility to scale individual microservices as needed and manage their lifecycle effectively. Each microservice will have its own set of pods that can be scaled up or down independently, making it suitable for a microservices architecture.","timestamp":"1728793320.0","comment_id":"1042280","upvote_count":"5"},{"timestamp":"1725934740.0","comment_id":"1003622","content":"Selected Answer: D\nD is the corrrect answer","upvote_count":"1","poster":"Captain1212"},{"comment_id":"1001127","timestamp":"1725678840.0","upvote_count":"1","content":"Selected Answer: D\nD is the correct answer.","poster":"scanner2"},{"comment_id":"1000071","content":"Selected Answer: D\nD seems more correct , as in A , we already have the own docker container image , so no need to create , \nb is completely diffenret tool,\nc is also of no use","timestamp":"1725586920.0","poster":"Captain1212","upvote_count":"2"},{"timestamp":"1693744620.0","comment_id":"658457","upvote_count":"1","poster":"snkhatri","content":"D, keyword \"each microservice can be scaled individually\"!"},{"poster":"abirroy","content":"Selected Answer: D\nD is the correct answer","comment_id":"639376","timestamp":"1690658880.0","upvote_count":"1"},{"comment_id":"615951","upvote_count":"1","poster":"AzureDP900","timestamp":"1686689340.0","content":"D is the best answer among other choices."},{"comment_id":"531580","poster":"Raz0r","upvote_count":"2","timestamp":"1674594240.0","content":"D is right!\nIt's one of Googles main ideas to distribute a complex system into microservices. They do it as well and encourage customers to do the same."},{"timestamp":"1668896160.0","content":"Selected Answer: D\nD is the Answer","upvote_count":"2","poster":"alaahakim","comment_id":"482131"},{"timestamp":"1668601260.0","content":"Ans: D","upvote_count":"2","comment_id":"479358","poster":"alaahakim"},{"poster":"associatecloudexamuser","upvote_count":"5","timestamp":"1657727100.0","content":"Yes. D is correct. Can deploy each service through\nkubectl apply -f <deployment_config.yaml>","comment_id":"405539"},{"content":"D is the correct answer","comment_id":"404776","upvote_count":"1","timestamp":"1657641240.0","poster":"pacman_user"},{"comment_id":"383496","timestamp":"1655391900.0","poster":"AD_0525","content":"D is correct","upvote_count":"2"},{"content":"D is the answer","comment_id":"346379","timestamp":"1651343160.0","upvote_count":"6","poster":"norrec9"}],"choices":{"A":"Create and deploy a Custom Resource Definition per microservice.","D":"Create and deploy a Deployment per microservice.","C":"Create and deploy a Job per microservice.","B":"Create and deploy a Docker Compose File."},"topic":"1","question_id":85,"question_text":"You have developed an application that consists of multiple microservices, with each microservice packaged in its own Docker container image. You want to deploy the entire application on Google Kubernetes Engine so that each microservice can be scaled individually. What should you do?","answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/51200-exam-associate-cloud-engineer-topic-1-question-175/","exam_id":1,"unix_timestamp":1619772720}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":true,"id":1,"isImplemented":true,"numberOfQuestions":285,"provider":"Google","isBeta":false,"name":"Associate Cloud Engineer"},"currentPage":17},"__N_SSP":true}