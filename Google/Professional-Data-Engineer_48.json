{"pageProps":{"questions":[{"id":"iaX8HzptNkVnBC00fzNv","question_text":"You need to look at BigQuery data from a specific table multiple times a day. The underlying table you are querying is several petabytes in size, but you want to filter your data and provide simple aggregations to downstream users. You want to run queries faster and get up-to-date insights quicker. What should you do?","answer_description":"","exam_id":11,"topic":"1","answers_community":["D (100%)"],"answer":"D","answer_images":[],"question_id":236,"discussion":[{"upvote_count":"7","content":"Selected Answer: D\nCreate a materialized view as query source.\nMaterialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency.","poster":"AllenChen123","comment_id":"1132156","timestamp":"1721944380.0"},{"upvote_count":"1","timestamp":"1724866380.0","poster":"Shenbasekhar","content":"Selected Answer: D\nOption D. Materialized view","comment_id":"1161948"},{"content":"Selected Answer: D\nmaterialized view","comment_id":"1138526","timestamp":"1722595380.0","poster":"Sofiia98","upvote_count":"1"}],"choices":{"A":"Run a scheduled query to pull the necessary data at specific intervals dally.","C":"Limit the query columns being pulled in the final result.","B":"Use a cached query to accelerate time to results.","D":"Create a materialized view based off of the query being run."},"timestamp":"2024-01-26 00:53:00","isMC":true,"unix_timestamp":1706226780,"answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/132186-exam-professional-data-engineer-topic-1-question-310/","question_images":[]},{"id":"FRhi0oOwaScM2By92ase","answer":"C","answers_community":["C (93%)","7%"],"isMC":true,"choices":{"B":"Create a transactional database that monitors the pending messages.","D":"Create a new Pub/Sub push subscription to monitor the orders processed in the agent's system.","A":"Use a Deduplicate PTransform in Dataflow before sending the messages to the sales agents.","C":"Use Pub/Sub exactly-once delivery in your pull subscription."},"exam_id":11,"timestamp":"2024-01-26 05:50:00","answer_images":[],"answer_ET":"C","topic":"1","discussion":[{"comment_id":"1264467","upvote_count":"12","poster":"meh_33","timestamp":"1723437600.0","content":"Believe me all questions were from Exam topic all were there yesterday in exam. But yes dont go with starting questions mainly focus questions after 200 and latest questions are at last page."},{"upvote_count":"7","content":"Selected Answer: C\nI remember seeing this in the exam.","timestamp":"1706531100.0","comment_id":"1134931","poster":"JimmyBK","comments":[{"timestamp":"1706656500.0","comment_id":"1136241","upvote_count":"3","poster":"Jordan18","content":"how many questions were from here?","comments":[{"upvote_count":"3","content":"also got this one. about 70%","poster":"iooj","comment_id":"1260549","timestamp":"1722755700.0"}]}]},{"poster":"Pime13","timestamp":"1736154720.0","comment_id":"1337045","content":"Selected Answer: C\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery\n\nOption A, using a Deduplicate PTransform in Dataflow, could help remove duplicate messages, but it adds complexity to your workflow. Dataflow is a separate service that would need to be integrated and managed, which might not be necessary if Pub/Sub's exactly-once delivery can handle the deduplication for you.\n\nUsing Pub/Sub's exactly-once delivery (Option C) is a more straightforward solution that directly addresses the issue of duplicate processing without adding extra components to your system.","upvote_count":"1"},{"timestamp":"1721793420.0","content":"Selected Answer: A\nWhy not C - Exactly-once delivery in Pub/Sub guarantees that a message is delivered to a subscriber exactly once. However, it doesn't prevent multiple subscribers from processing the same message.","comment_id":"1254088","upvote_count":"1","poster":"cien91"},{"poster":"JyoGCP","comment_id":"1156327","timestamp":"1708601520.0","upvote_count":"1","content":"Selected Answer: C\nOption C"},{"poster":"Sofiia98","upvote_count":"1","timestamp":"1706877840.0","comment_id":"1138529","content":"Selected Answer: C\nC, of course"},{"timestamp":"1706244600.0","comment_id":"1132258","upvote_count":"4","content":"Selected Answer: C\nStraightforward.\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery","poster":"AllenChen123"}],"question_id":237,"question_text":"Your chemical company needs to manually check documentation for customer order. You use a pull subscription in Pub/Sub so that sales agents get details from the order. You must ensure that you do not process orders twice with different sales agents and that you do not add more complexity to this workflow. What should you do?","url":"https://www.examtopics.com/discussions/google/view/132198-exam-professional-data-engineer-topic-1-question-311/","question_images":[],"answer_description":"","unix_timestamp":1706244600},{"id":"OtWsVxKmOFs0qgNQE2bO","url":"https://www.examtopics.com/discussions/google/view/153019-exam-professional-data-engineer-topic-1-question-312/","timestamp":"2024-12-16 03:30:00","discussion":[{"timestamp":"1736155020.0","upvote_count":"1","comment_id":"1337048","poster":"Pime13","content":"Selected Answer: A\nAnalytics Hub is designed to enable secure and scalable data sharing across organizational boundaries. It allows teams to publish, discover, and subscribe to datasets without the need to replicate data, ensuring data freshness and minimizing cost\n\nhttps://cloud.google.com/blog/products/data-analytics/bigquery-analytics-hub-for-data-sharing\nhttps://cloud.google.com/bigquery/docs/analytics-hub-introduction"},{"comment_id":"1327140","poster":"FireAtMe","timestamp":"1734316200.0","content":"Selected Answer: A\nAnalytics Hub is a fully managed data sharing platform provided by Google Cloud. It allows organizations to publish, discover, and subscribe to datasets securely and efficiently. It facilitates collaboration across teams or even across organizations by enabling self-service access to shared data without duplicating or moving it.","upvote_count":"4"}],"topic":"1","isMC":true,"unix_timestamp":1734316200,"choices":{"B":"Create authorized datasets to publish shared data in the subscribing team's project.","C":"Create a new dataset for sharing in each individual team’s project. Grant the subscribing team the bigquery.dataViewer role on the dataset.","A":"Use Analytics Hub to facilitate data sharing.","D":"Use BigQuery Data Transfer Service to copy datasets to a centralized BigQuery project for sharing."},"answer_ET":"A","question_id":238,"answer_description":"","answer":"A","answer_images":[],"question_images":[],"exam_id":11,"answers_community":["A (100%)"],"question_text":"You are migrating your on-premises data warehouse to BigQuery. As part of the migration, you want to facilitate cross-team collaboration to get the most value out of the organization’s data. You need to design an architecture that would allow teams within the organization to securely publish, discover, and subscribe to read-only data in a self-service manner. You need to minimize costs while also maximizing data freshness. What should you do?"},{"id":"AwXggl8FXahvq3bl7Qtl","answers_community":["D (69%)","C (31%)"],"isMC":true,"answer":"D","answer_images":[],"unix_timestamp":1733579100,"discussion":[{"comment_id":"1324614","content":"Selected Answer: D\nPriority is \"minimize installation and management effort\" which is done via Dataproc Serverless. Furthermore, with Dataproc serverless you can still specify resource settings for your job, such as the number of vCPUs and memory per executor (https://cloud.google.com/dataproc-serverless/docs/concepts/properties)","upvote_count":"8","timestamp":"1733843160.0","poster":"chicity_de"},{"content":"I agree that minimizing installation and management means using Dataproc Serverles. \nAlso, Serverles can be configured with up to 16 VPU and up to 29696m of memory in for the premium tier. https://cloud.google.com/dataproc-serverless/docs/concepts/properties#:~:text=Total%20driver%20memory%20per%20driver%20core%2C%20including%20driver%20memory%20overhead%2C%20which%20must%20be%20between%201024m%20and%207424m%20for%20the%20Standard%20compute%20tier%20(24576m%20for%20the%20Premium%20compute%20tier).%20For%20example%2C%20if%20spark.driver.cores%20%3D%204%2C%20then%204096m%20%3C%3D%20spark.driver.memory%20%2B%20spark.driver.memoryOverhead%20%3C%3D%2029696m.","comment_id":"1361118","timestamp":"1740415920.0","poster":"gabazzzo","upvote_count":"1"},{"timestamp":"1739453940.0","comment_id":"1356130","poster":"a494e30","upvote_count":"1","content":"Selected Answer: C\nNeeds to be able to configure \"similar settings\""},{"comment_id":"1351289","timestamp":"1738664580.0","upvote_count":"1","poster":"plum21","content":"Selected Answer: C\nIt's not possible to specify a machine type using Dataproc Serverless"},{"poster":"marlon.andrei","content":"Selected Answer: C\nI choice \"C\", just: \"where each executor has 8 vCPU and 16 GB memory, and you want to be able to choose similar settings\"","timestamp":"1736977800.0","upvote_count":"1","comment_id":"1341263"},{"upvote_count":"1","content":"Selected Answer: D\nDataproc Serverless allows you to run Spark jobs without needing to manage the underlying infrastructure. It automatically handles resource provisioning and scaling, which simplifies the process and reduces management overhead","timestamp":"1736155320.0","comment_id":"1337051","poster":"Pime13"},{"comment_id":"1323120","content":"Selected Answer: C\nDataproc supports Spark 3, ensuring compatibility with your existing job.\n\n It also allows you to customize the cluster configuration, including the number of executors, vCPUs, and memory per executor, to match your on-premises setup (8 vCPU and 16 GB memory)","poster":"mcdaley","timestamp":"1733579100.0","upvote_count":"1"}],"choices":{"C":"Execute the job in a new Dataproc cluster.","B":"Execute the job from a new Compute Engine VM.","D":"Execute as a Dataproc Serverless job.","A":"Execute the job as part of a deployment in a new Google Kubernetes Engine cluster."},"question_images":[],"topic":"1","question_id":239,"url":"https://www.examtopics.com/discussions/google/view/152659-exam-professional-data-engineer-topic-1-question-313/","exam_id":11,"answer_description":"","timestamp":"2024-12-07 14:45:00","answer_ET":"D","question_text":"You want to migrate an Apache Spark 3 batch job from on-premises to Google Cloud. You need to minimally change the job so that the job reads from Cloud Storage and writes the result to BigQuery. Your job is optimized for Spark, where each executor has 8 vCPU and 16 GB memory, and you want to be able to choose similar settings. You want to minimize installation and management effort to run your job. What should you do?"},{"id":"yu3XEBHajzr1HxwCO5ZV","question_id":240,"unix_timestamp":1735068540,"url":"https://www.examtopics.com/discussions/google/view/153398-exam-professional-data-engineer-topic-1-question-314/","isMC":true,"question_text":"You are configuring networking for a Dataflow job. The data pipeline uses custom container images with the libraries that are required for the transformation logic preinstalled. The data pipeline reads the data from Cloud Storage and writes the data to BigQuery. You need to ensure cost-effective and secure communication between the pipeline and Google APIs and services. What should you do?","exam_id":11,"answer":"A","discussion":[{"timestamp":"1736155740.0","upvote_count":"3","poster":"Pime13","content":"Selected Answer: A\nA. Disable external IP addresses from worker VMs and enable Private Google Access.\n\nThis approach ensures that your worker VMs can access Google APIs and services securely without using external IP addresses, which reduces costs and enhances security by keeping the traffic within Google's network","comment_id":"1337054"},{"timestamp":"1735068540.0","upvote_count":"1","content":"Selected Answer: A\nWhile option C is technically implementable, option A is a straightforward and a simpler solution.","poster":"m_a_p_s","comment_id":"1331220"}],"question_images":[],"answers_community":["A (100%)"],"topic":"1","answer_images":[],"answer_description":"","answer_ET":"A","timestamp":"2024-12-24 20:29:00","choices":{"A":"Disable external IP addresses from worker VMs and enable Private Google Access.","C":"Disable external IP addresses and establish a Private Service Connect endpoint IP address.","B":"Leave external IP addresses assigned to worker VMs while enforcing firewall rules.","D":"Enable Cloud NAT to provide outbound internet connectivity while enforcing firewall rules."}}],"exam":{"provider":"Google","numberOfQuestions":319,"lastUpdated":"11 Apr 2025","isImplemented":true,"isBeta":false,"name":"Professional Data Engineer","id":11,"isMCOnly":true},"currentPage":48},"__N_SSP":true}