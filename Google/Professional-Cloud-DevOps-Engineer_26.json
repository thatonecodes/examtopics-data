{"pageProps":{"questions":[{"id":"kMOPaRQeC6MAJyExucHX","url":"https://www.examtopics.com/discussions/google/view/54281-exam-professional-cloud-devops-engineer-topic-1-question-35/","answer":"A","answer_ET":"A","isMC":true,"topic":"1","choices":{"C":"Create a new service account with the Container Developer role and use it to run Cloud Build.","D":"Create a separate step in Cloud Build to retrieve service account credentials and pass these to kubectl.","B":"Specify the Container Developer role for Cloud Build in the cloudbuild.yaml file.","A":"Assign the Container Developer role to the Cloud Build service account."},"question_text":"Your team uses Cloud Build for all CI/CD pipelines. You want to use the kubectl builder for Cloud Build to deploy new images to Google Kubernetes Engine\n(GKE). You need to authenticate to GKE while minimizing development effort. What should you do?","question_id":126,"answers_community":["A (81%)","C (19%)"],"timestamp":"2021-06-02 20:11:00","answer_description":"","question_images":[],"discussion":[{"timestamp":"1638534660.0","comment_id":"373479","content":"I think A","poster":"[Removed]","upvote_count":"25"},{"upvote_count":"11","poster":"TNT87","comment_id":"438354","content":"A\nhttps://cloud.google.com/build/docs/securing-builds/configure-user-specified-service-accounts","timestamp":"1646296860.0"},{"upvote_count":"3","timestamp":"1717312560.0","poster":"jomonkp","content":"Selected Answer: C\nOption C","comment_id":"1086007"},{"timestamp":"1710615960.0","upvote_count":"1","content":"A \nhttps://cloud.google.com/build/docs/deploying-builds/deploy-gke#required_iam_permissions","comment_id":"1009211","poster":"Orzechowski"},{"poster":"sidharthwader","comments":[{"comment_id":"953434","content":"are these questions still relevant?","timestamp":"1705427100.0","upvote_count":"2","poster":"aswani","comments":[{"poster":"Magist3r","upvote_count":"1","comment_id":"954042","comments":[{"comment_id":"1126466","timestamp":"1721361960.0","poster":"joshtechgroup","content":"can someone confirm now?","upvote_count":"1"}],"timestamp":"1705491660.0","content":"I hope that are still relevant. I'll take the exam this week, can someone confirm about aswani question?"}]}],"content":"Doesn't C makes more sense ? Why is it A?","comment_id":"940384","upvote_count":"1","timestamp":"1704165000.0"},{"comment_id":"935347","upvote_count":"1","content":"I think that the A is incorrect... The good practices says that the CB like the other resources should avoid to use the default SA, so the correct one is the C which creates a SA and then give the required roles.","timestamp":"1703687520.0","poster":"samuelmorher","comments":[{"content":"are these questions still relevant?","comment_id":"952772","timestamp":"1705369080.0","upvote_count":"1","poster":"aswani"}]},{"content":"Selected Answer: A\n100% A and this is the doc the proves this: https://cloud.google.com/build/docs/deploying-builds/deploy-gke#required_iam_permissions","comment_id":"869232","upvote_count":"3","poster":"felipeschossler","timestamp":"1697186220.0"},{"timestamp":"1689212700.0","upvote_count":"3","comment_id":"774074","content":"Selected Answer: A\nThe best option for authenticating to GKE while minimizing development effort would be A. Assign the Container Developer role to the Cloud Build service account.\n\nGoogle Cloud Build uses a default service account to run the build, this service account is automatically created by Cloud Build and it has the necessary permissions to access the resources used by the build. By assigning the Container Developer role to this service account, it will have the necessary permissions to deploy new images to GKE. This way you don't need to create a new service account or specify the role in the cloudbuild.yaml file. This is an easy and secure way to authenticate to GKE without adding extra steps to the CI/CD pipeline.","poster":"JonathanSJ"},{"upvote_count":"1","timestamp":"1688202780.0","poster":"juliefighting","content":"Answer is A\nhttps://cloud.google.com/build/docs/deploying-builds/deploy-gke#required_iam_permissions","comment_id":"763165"},{"timestamp":"1687670940.0","poster":"floppino","content":"Selected Answer: A\nAns: A\n\nExam passed and taken on 19/12/2022, 50/50 from this dump without buying the full access and looking for 'devops' word here: https://www.examtopics.com/discussions/google/1/","upvote_count":"1","comment_id":"755491"},{"comment_id":"726344","upvote_count":"2","content":"i think A, new service account needs \" Cloud Build Service Account \" role and \" kubernete engine developer\" role to execute the build steps for cloud build.","poster":"hanweiCN","timestamp":"1684973820.0"},{"upvote_count":"1","poster":"AzureDP900","content":"A is more suitable for this scenario\n\nhttps://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account","timestamp":"1682299920.0","comment_id":"702586"},{"comment_id":"696110","content":"I think A is correct, but please note that question specify that kubectl builder (https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/kubectl) and NOT gke-deploy (https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gke-deploy) is being used!\nhttps://cloud.google.com/build/docs/deploying-builds/deploy-gke\nIn any case, as specified in kubectl builder documentation: When executed in the Cloud Build environment, commands are executed with credentials of the builder service account for the build project.","poster":"okercho","upvote_count":"1","timestamp":"1681635120.0"},{"upvote_count":"1","poster":"GCP72","content":"Selected Answer: A\nAnswer is A","timestamp":"1674646020.0","comment_id":"636596"},{"content":"Selected Answer: A\nA is the correct answer","poster":"Pankul","timestamp":"1666608000.0","upvote_count":"1","comment_id":"590985"},{"timestamp":"1656741360.0","upvote_count":"4","content":"Selected Answer: A\nA should be the correct one. because assigning permission to cloud build service account will give permission to deploy while minimizing additional overhead.","comment_id":"514881","poster":"cloudbee"},{"timestamp":"1654970640.0","comment_id":"499649","upvote_count":"3","poster":"Biden","content":"Agree with A. \nReference to container.developer role: https://cloud.google.com/kubernetes-engine/docs/how-to/iam"},{"content":"A is correct","comment_id":"393457","upvote_count":"6","timestamp":"1640757120.0","poster":"Charun"},{"upvote_count":"2","poster":"francisco_guerra","comment_id":"388428","content":"Ans: A\nminimizing development effort\nSo create another account with all the needed roles its not an option","timestamp":"1640231640.0"},{"poster":"ralf_cc","timestamp":"1639819260.0","comment_id":"384644","upvote_count":"1","content":"A - https://cloud.google.com/build/docs/deploying-builds/deploy-gke"},{"comment_id":"381035","content":"User managed service account in the option C can be used only for manual builds. \n\nAs CI/CD is being used so assigning the permission to default build service account could be good idea. so it seems best answer is A\n\nhttps://cloud.google.com/build/docs/securing-builds/configure-user-specified-service-accounts","upvote_count":"4","timestamp":"1639399560.0","poster":"akg001"},{"comment_id":"372947","timestamp":"1638475860.0","poster":"devopsbatch","upvote_count":"1","content":"i think D"}],"answer_images":[],"unix_timestamp":1622657460,"exam_id":6},{"id":"NmSfFHHpTPKoe3nIMVmp","question_id":127,"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/google/view/54282-exam-professional-cloud-devops-engineer-topic-1-question-36/","answer_description":"","isMC":true,"timestamp":"2021-06-02 20:15:00","choices":{"D":"Configure BigQuery as a sink for Stackdriver Logging. Create a scheduled query to filter the cache miss logs and write them to a separate table.","C":"Create a logs-based metric in Stackdriver Logging and a dashboard for that metric in Stackdriver Monitoring.","A":"Link Stackdriver Logging as a source in Google Data Studio. Filter the logs on the cache misses.","B":"Configure Stackdriver Profiler to identify and visualize when the cache misses occur based on the logs."},"exam_id":6,"answer":"C","answer_ET":"C","topic":"1","question_text":"You support an application that stores product information in cached memory. For every cache miss, an entry is logged in Stackdriver Logging. You want to visualize how often a cache miss happens over time. What should you do?","answer_images":[],"unix_timestamp":1622657700,"discussion":[{"timestamp":"1640757120.0","upvote_count":"18","poster":"Charun","content":"C for sure","comment_id":"393458"},{"comment_id":"377498","content":"I think C: https://cloud.google.com/logging/docs/logs-based-metrics#counter-metric","poster":"syslog","upvote_count":"11","timestamp":"1638972300.0"},{"timestamp":"1731399300.0","poster":"habla2019pasta","comment_id":"1210150","content":"Selected Answer: C\nOption C","upvote_count":"1"},{"content":"Selected Answer: C\nOption C","upvote_count":"1","poster":"jomonkp","comment_id":"1086008","timestamp":"1717312680.0"},{"timestamp":"1710369180.0","upvote_count":"1","poster":"ngz123","content":"are these questions still valid?","comment_id":"1006965"},{"content":"Selected Answer: C\nThe best option for visualizing how often a cache miss happens over time would be C. Create a logs-based metric in Stackdriver Logging and a dashboard for that metric in Stackdriver Monitoring.\n\nStackdriver Logging provides the ability to extract metrics from logs, these metrics are called logs-based metrics. You can create a logs-based metric that counts the number of cache miss logs and configure it to be collected at a regular interval, this way you can see how often a cache miss happens over time. Additionally, Stackdriver Monitoring provides the ability to create dashboards that display the metrics collected by logs-based metrics, you can use this dashboard to visualize the cache misses over time and easily identify trends or spikes in the data.","timestamp":"1689212880.0","poster":"JonathanSJ","upvote_count":"1","comment_id":"774076"},{"upvote_count":"2","poster":"floppino","comment_id":"755492","timestamp":"1687671000.0","content":"Selected Answer: C\nAns: C\n\nExam passed and taken on 19/12/2022, 50/50 from this dump without buying the full access and looking for 'devops' word here: https://www.examtopics.com/discussions/google/1/"},{"timestamp":"1674646200.0","comment_id":"636597","poster":"GCP72","content":"Selected Answer: C\nAnswer is C","upvote_count":"1"},{"comment_id":"599132","content":"Selected Answer: C\nSubmitted C","timestamp":"1668016560.0","upvote_count":"1","poster":"Ananda"},{"comment_id":"535470","timestamp":"1659093000.0","content":"Selected Answer: C\nAnswer C","poster":"vijaigcp","upvote_count":"2"},{"timestamp":"1646296980.0","comment_id":"438355","content":"C\nhttps://cloud.google.com/logging/docs/logs-based-metrics#counter-metric","poster":"TNT87","upvote_count":"4"},{"comments":[{"timestamp":"1650305520.0","content":"and how would you visualize the data in bq?","poster":"tycho","upvote_count":"2","comment_id":"464250"}],"comment_id":"372950","poster":"devopsbatch","upvote_count":"1","timestamp":"1638476100.0","content":"D export to biguery as sink and analyse logs"}],"question_images":[]},{"id":"ZSqS4ksB3KNDE3zh2AWP","answer":"C","topic":"1","timestamp":"2021-06-02 20:21:00","question_images":[],"answers_community":["C (100%)"],"answer_images":[],"question_text":"You need to deploy a new service to production. The service needs to automatically scale using a Managed Instance Group (MIG) and should be deployed over multiple regions. The service needs a large number of resources for each instance and you need to plan for capacity. What should you do?","question_id":128,"discussion":[{"upvote_count":"15","poster":"Charun","comment_id":"393459","content":"C 100%","timestamp":"1640757120.0"},{"upvote_count":"12","poster":"TNT87","timestamp":"1646297160.0","comment_id":"438359","content":"C\nhttps://cloud.google.com/compute/quotas"},{"content":"Selected Answer: C\nExam on 2024-04-26","comment_id":"1202916","poster":"trashbox","timestamp":"1730001360.0","upvote_count":"1"},{"timestamp":"1717313280.0","poster":"jomonkp","content":"Selected Answer: C\noption c","upvote_count":"2","comment_id":"1086013"},{"content":"Selected Answer: C\nC 100%","poster":"calex1","upvote_count":"1","timestamp":"1709232180.0","comment_id":"994318"},{"poster":"JonathanSJ","upvote_count":"2","content":"Selected Answer: C\nC. Validate that the resource requirements are within the available quota limits of each region. It is important to ensure that the resource requirements are within the available quota limits in each region before deploying the service, to avoid exceeding the limits and causing problems. This is essential to ensure that the service is deployed correctly and has the necessary capacity to handle the load.","comment_id":"774093","timestamp":"1689215040.0"},{"content":"Selected Answer: C\nAns: C\n\nExam passed and taken on 19/12/2022, 50/50 from this dump without buying the full access and looking for 'devops' word here: https://www.examtopics.com/discussions/google/1/","comment_id":"755493","poster":"floppino","timestamp":"1687671000.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1674646320.0","content":"Selected Answer: C\nAnswer is C","poster":"GCP72","comment_id":"636598"},{"poster":"ghadxx","content":"Selected Answer: C\nKnowing available quota limits allows you to plan for capacity","comment_id":"593904","timestamp":"1666969440.0","upvote_count":"2"},{"poster":"asdqweasd","timestamp":"1661270520.0","content":"Selected Answer: C\nYou need to make sure of quota limit other things will be handled by MIG","comment_id":"554745","upvote_count":"1"},{"timestamp":"1659055320.0","poster":"cyrus86","comment_id":"535104","content":"Selected Answer: C\nC shud be the answer","upvote_count":"1"},{"content":"Selected Answer: C\nCorrect answer is C. this service is deployed into MIG and we only have to take care of quota","comment_id":"501785","timestamp":"1655250480.0","poster":"meetplanet","upvote_count":"2"},{"comment_id":"500477","poster":"Shasha1","comments":[{"timestamp":"1668081660.0","comment_id":"599505","content":"D says \"one\" region, the solution is meant to be multi regional","upvote_count":"1","poster":"Aiffone"}],"content":"Answer is D \nNetworking and load balancing quotas are not regional quotes, those are global quotas. Any region can use a global quota. For example, in-use and static external IP addresses assigned to load balancers and HTTP(S) proxies consume global quotas.\nreference https://cloud.google.com/compute/quotas","timestamp":"1655103600.0","upvote_count":"1"},{"timestamp":"1643017620.0","content":"absolutely C 'cause this service is deployed into MIG and you only have to take care of quota.","comment_id":"413039","poster":"kubosuke","upvote_count":"3"},{"upvote_count":"4","comment_id":"385168","timestamp":"1639890540.0","content":"C - it is asking for quota","poster":"ralf_cc"},{"upvote_count":"4","content":"I think C is the answer. \nhttps://cloud.google.com/compute/quotas#understanding_quotas\nhttps://cloud.google.com/compute/quotas","comments":[{"content":"Agree for the answer : C","poster":"akg001","comment_id":"376957","timestamp":"1638906660.0","upvote_count":"2"}],"poster":"DucLee3110","comment_id":"376520","timestamp":"1638862620.0"},{"comment_id":"372953","poster":"devopsbatch","timestamp":"1638476460.0","comments":[{"poster":"akg001","upvote_count":"1","comments":[{"content":"And nobody is saying you need 96 CPU.","comment_id":"377500","timestamp":"1638972480.0","poster":"syslog","upvote_count":"4"}],"comment_id":"376958","content":"you have to check the required resources available on those particular regions or not. As you know not all type of VM or a resource available in all regions. There is not surety that n1-highcpu-96 machine type available in all regions.","timestamp":"1638906780.0"}],"content":"A can be the answer","upvote_count":"1"}],"isMC":true,"exam_id":6,"answer_description":"","choices":{"D":"Deploy the service in one region and use a global load balancer to route traffic to this region.","A":"Use the n1-highcpu-96 machine type in the configuration of the MIG.","C":"Validate that the resource requirements are within the available quota limits of each region.","B":"Monitor results of Stackdriver Trace to determine the required amount of resources."},"url":"https://www.examtopics.com/discussions/google/view/54283-exam-professional-cloud-devops-engineer-topic-1-question-37/","answer_ET":"C","unix_timestamp":1622658060},{"id":"izWx6QPTG33tp2O37sAu","url":"https://www.examtopics.com/discussions/google/view/54285-exam-professional-cloud-devops-engineer-topic-1-question-38/","isMC":true,"exam_id":6,"answers_community":["B (86%)","14%"],"unix_timestamp":1622658420,"discussion":[{"poster":"DucLee3110","upvote_count":"19","comments":[{"poster":"holahola","upvote_count":"5","content":"B to me as well. Because fluentd can filter the logs quite nicely before passing information to Stackdriver. It can cober sensitive information such as credit card details, social security numbers, etc. Once the filtering is done, then the log can be passed to Cloud Storage, but the unfiltered information should not even reach stackdriver, so most of the answers are wrong.","timestamp":"1624253940.0","comment_id":"386804"},{"upvote_count":"2","content":"to me , looks B is the correct answer .","comment_id":"376962","poster":"akg001","comments":[{"comment_id":"377509","poster":"syslog","upvote_count":"2","timestamp":"1623154440.0","comments":[{"upvote_count":"6","poster":"francisco_guerra","timestamp":"1624761900.0","content":"prevent them from leaking to Stackdriver Logging.\nIf you need to create a log export & log filter so the information is leaking to logging.","comment_id":"391697"}],"content":"Why not D?"}],"timestamp":"1623088800.0"},{"upvote_count":"1","content":"Agree with B","timestamp":"1636596180.0","comment_id":"475921","poster":"Manh"},{"comment_id":"1089174","upvote_count":"1","content":"Not available any more.","timestamp":"1701856980.0","poster":"irmingard_examtopics"}],"timestamp":"1623045000.0","comment_id":"376534","content":"looks like it is B. https://medium.com/google-cloud/fluentd-filter-plugin-for-google-cloud-data-loss-prevention-api-42bbb1308e76"},{"upvote_count":"9","content":"Im not pretty sure but \nAns B\nPrevent them form leaking to Stackdriver logging\nA: Incorrect, Leaking to Stackdriver\nB: Correct, not leaking to Stackdriver & fluentD\nC: Incorrect, Leaking\nD: If we removed why we need to create a filter matching there will not be logs with userinfo?","timestamp":"1624502460.0","poster":"francisco_guerra","comment_id":"389175"},{"comment_id":"1263456","content":"Why not D?\nUse fluentd filter to remove logs with userinfo and export existing logs for review?","poster":"account123","upvote_count":"1","timestamp":"1723291380.0"},{"timestamp":"1701509700.0","comment_id":"1086016","upvote_count":"1","content":"Selected Answer: B\noption b","poster":"jomonkp"},{"comments":[{"upvote_count":"1","content":"are these questions still relevant? @jeffersonkozak","timestamp":"1689463140.0","poster":"aswani","comment_id":"952758"}],"comment_id":"907602","upvote_count":"1","timestamp":"1685138100.0","poster":"jeffersonkozak","content":"Selected Answer: C\nA suggests creating a basic log filter and configuring a log export to Cloud Storage, but it does not address preventing the PII entries from leaking to Stackdriver Logging. By creating only a basic log filter, the PII data would still be accessible within Stackdriver Logging."},{"content":"Selected Answer: B\nB. Use a Fluentd filter plugin with the Stackdriver Agent to remove log entries containing userinfo, and then copy the entries to a Cloud Storage bucket.\n\nBy using Fluentd filter plugin, you can remove log entries that contain PII information and configure it to send to a designated cloud storage bucket. This way you prevent the logs that contain PII from leaking to Stackdriver Logging, and have them stored in a secure location for later review.","comment_id":"774103","timestamp":"1673584200.0","upvote_count":"2","poster":"JonathanSJ"},{"comment_id":"762540","timestamp":"1672473240.0","poster":"Greg123123","upvote_count":"1","content":"All A,C and D are leaking to stackdriver. So the ans has to be B"},{"content":"Selected Answer: B\nAns: B\n\nExam passed and taken on 19/12/2022, 50/50 from this dump without buying the full access and looking for 'devops' word here: https://www.examtopics.com/discussions/google/1/","poster":"floppino","timestamp":"1671953400.0","comment_id":"755494","upvote_count":"1"},{"comment_id":"748222","content":"Selected Answer: B\nwill go with B","upvote_count":"1","timestamp":"1671296280.0","poster":"raghupothula"},{"upvote_count":"3","comment_id":"668603","poster":"saiprasathdv","content":"Selected Answer: B\nOption B: https://cloud.google.com/logging/docs/agent/logging/configuration. Custom defined log entries has this structure \"[TAG_NAME]+Payload+timestamp+Severity+labels\". Here \"Userinfo\" is the TAG_NAME. Fluentd filter plugins used to filter out logs based on TAG_NAME. finally this could be stored in Cloud storage.","timestamp":"1663130160.0"},{"poster":"GCP72","content":"Selected Answer: B\nAnswer is B","comment_id":"636600","upvote_count":"1","timestamp":"1658741580.0"},{"comments":[{"comment_id":"628743","content":"Nevermind... I though B is ugly, if the PII data is removed with the fluentd filter, it will never arrive at the advanced filter.","poster":"mgm7","upvote_count":"1","timestamp":"1657277820.0"}],"poster":"mgm7","comment_id":"628301","upvote_count":"1","content":"Selected Answer: C\nB is a poor answer IMHO because it seems to include a manual task of copying the PII data to the bucket at some later date (cron job or personal intervention, ugly either way). https://cloud.google.com/logging/docs/routing/overview makes it quite clear that this should be taken care by routing and the exclusion filter which would imply that C is the correct answer. \"Cloud logging\" is only one of the possible sink choices.","timestamp":"1657186800.0"},{"upvote_count":"2","content":"Selected Answer: B\nAns B\nhttps://medium.com/google-cloud/fluentd-filter-plugin-for-google-cloud-data-loss-prevention-api-42bbb1308e76","timestamp":"1640592600.0","comment_id":"510117","poster":"TNT87"},{"upvote_count":"1","content":"Selected Answer: B\nAgree with B","comment_id":"502443","poster":"garmstrong","timestamp":"1639596060.0"},{"upvote_count":"4","comment_id":"500483","timestamp":"1639386840.0","poster":"Shasha1","content":"A\nquestion is about capture logs entries in a secure location for later review, not removing the log sensitive data before store then in a secure location. so answer is A"},{"content":"B\nhttps://medium.com/google-cloud/fluentd-filter-plugin-for-google-cloud-data-loss-prevention-api-42bbb1308e76","comment_id":"438363","timestamp":"1630652100.0","upvote_count":"3","poster":"TNT87","comments":[{"comment_id":"1089177","content":"The author deleted this Medium story.","poster":"irmingard_examtopics","upvote_count":"1","timestamp":"1701857040.0"}]},{"comment_id":"405673","timestamp":"1626204060.0","content":"B. filter_record_transformer to be exact.","poster":"j3e","upvote_count":"3"},{"content":"B answer","comment_id":"393461","timestamp":"1624938720.0","poster":"Charun","upvote_count":"5"},{"comments":[{"comment_id":"413044","timestamp":"1627113480.0","content":"I think C is wrong 'cause we need to leak logs to Cloud Logging as below:\n> prevent them from leaking to Stackdriver Logging. \n\nand C is just filter Logs on the Cloud Logging, so the logs has leaked into Cloud Logging.\nso we need to filter Logs not on the Cloud Logging, but fluentd.\nfor these backgrounds, the answer gonna be B.","poster":"kubosuke","upvote_count":"2"}],"poster":"ralf_cc","comment_id":"385175","upvote_count":"2","content":"C - https://cloud.google.com/logging/docs/exclusions\nfluentd filter plugin is on the client-side, the question is asking the server (gcp) side","timestamp":"1624073460.0"},{"comment_id":"372958","content":"C could be","upvote_count":"2","poster":"devopsbatch","timestamp":"1622658420.0"}],"topic":"1","question_text":"You are running an application on Compute Engine and collecting logs through Stackdriver. You discover that some personally identifiable information (PII) is leaking into certain log entry fields. All PII entries begin with the text userinfo. You want to capture these log entries in a secure location for later review and prevent them from leaking to Stackdriver Logging. What should you do?","choices":{"C":"Create an advanced log filter matching userinfo, configure a log export in the Stackdriver console with Cloud Storage as a sink, and then configure a log exclusion with userinfo as a filter.","D":"Use a Fluentd filter plugin with the Stackdriver Agent to remove log entries containing userinfo, create an advanced log filter matching userinfo, and then configure a log export in the Stackdriver console with Cloud Storage as a sink.","B":"Use a Fluentd filter plugin with the Stackdriver Agent to remove log entries containing userinfo, and then copy the entries to a Cloud Storage bucket.","A":"Create a basic log filter matching userinfo, and then configure a log export in the Stackdriver console with Cloud Storage as a sink."},"timestamp":"2021-06-02 20:27:00","question_id":129,"answer_ET":"B","answer":"B","answer_images":[],"question_images":[],"answer_description":""},{"id":"vMIkOVZWvuNaV14uoSd1","url":"https://www.examtopics.com/discussions/google/view/54286-exam-professional-cloud-devops-engineer-topic-1-question-39/","isMC":true,"exam_id":6,"answers_community":["D (100%)"],"discussion":[{"content":"D is correct","comment_id":"393462","poster":"Charun","upvote_count":"21","timestamp":"1640757120.0"},{"content":"D - find out what's changed in the build spec","poster":"ralf_cc","timestamp":"1639891920.0","upvote_count":"12","comment_id":"385177"},{"poster":"habla2019pasta","timestamp":"1731399480.0","upvote_count":"1","content":"Selected Answer: D\nOption D - Figure out what was the recent change","comment_id":"1210152"},{"comment_id":"1086017","poster":"jomonkp","upvote_count":"2","content":"Selected Answer: D\nOption D","timestamp":"1717313820.0"},{"comment_id":"952757","content":"are these questions relevant yet? with the new exam version launched in may","upvote_count":"3","timestamp":"1705367760.0","poster":"aswani"},{"content":"Selected Answer: D\nD. Run a Git compare between the previous and current Cloud Build Configuration files to find and fix the bug.\nThis option allow you to compare the previous and current Cloud Build Configuration files and find what has been changed and understand what could be causing the issue. After identifying the problem, you can fix it and ensure that the pipeline is working correctly again. This approach is based on the SRE practices of identifying and resolving issues quickly and effectively.","poster":"JonathanSJ","timestamp":"1689215520.0","comment_id":"774109","upvote_count":"2"},{"timestamp":"1688105160.0","poster":"Greg123123","upvote_count":"2","content":"D is correct.\nHonestly I couldn't relate this to any SRE practices. Anyone can give me a hints? I could deduce the answer though.\nA: It didn't mention anything urgent, so no need to mitigate in this ugly way.\nB: miss the point. It worked perfectly without CR, why change it?\nC: upload the yaml to CS won't identify the issue for you.\nD: this has to be correct. The question emphasize \"code versioning\" and \"change in the Cloud Build YAML\". We can easily identify the problem through comparison.","comment_id":"762544"},{"content":"Selected Answer: D\nAns: D\n\nExam passed and taken on 19/12/2022, 50/50 from this dump without buying the full access and looking for 'devops' word here: https://www.examtopics.com/discussions/google/1/","timestamp":"1687671000.0","poster":"floppino","upvote_count":"2","comment_id":"755495"},{"content":"D is correct","poster":"EvelynSU","comment_id":"724034","upvote_count":"1","timestamp":"1684714200.0"},{"upvote_count":"1","timestamp":"1674646500.0","comment_id":"636602","content":"Selected Answer: D\nAnswer is D","poster":"GCP72"},{"comment_id":"612785","content":"Guys, I'm wondering if we wouldn't solve the incident (A) before get the root cause(D).","poster":"[Removed]","upvote_count":"2","timestamp":"1670427840.0"},{"content":"Selected Answer: D\nSubmitted D in the exam","comment_id":"599133","timestamp":"1668016620.0","poster":"Ananda","comments":[{"comment_id":"604798","upvote_count":"2","content":"How many questions are from this site for exam?","timestamp":"1669032300.0","poster":"akshay_jadhav"}],"upvote_count":"2"},{"content":"Selected Answer: D\nWould go with D","upvote_count":"3","poster":"vijaigcp","timestamp":"1659094500.0","comment_id":"535482"},{"upvote_count":"5","comment_id":"500489","content":"B is correct \nartifact can not push to Docker hub only docker images possible. therefore, need to push the artifacts to google cloud container registry not to the public docker hub. \nhttps://cloud.google.com/build/docs/interacting-with-dockerhub-images","timestamp":"1655105520.0","poster":"Shasha1"},{"poster":"[Removed]","timestamp":"1638564780.0","upvote_count":"5","content":"I think D","comment_id":"373821"},{"comments":[{"upvote_count":"9","content":"\"After making a change in the Cloud Build YAML configuration, you notice that no new artifacts are being built by the pipeline\"- means something wrong on the recent change not with the image registry.\n\ncorrect answer should be - D","timestamp":"1639460100.0","comment_id":"381499","poster":"akg001"}],"content":"B push images into container registry instead docker hub","upvote_count":"1","timestamp":"1638476940.0","comment_id":"372961","poster":"devopsbatch"}],"unix_timestamp":1622658540,"topic":"1","question_text":"You have a CI/CD pipeline that uses Cloud Build to build new Docker images and push them to Docker Hub. You use Git for code versioning. After making a change in the Cloud Build YAML configuration, you notice that no new artifacts are being built by the pipeline. You need to resolve the issue following Site\nReliability Engineering practices. What should you do?","choices":{"C":"Upload the configuration YAML file to Cloud Storage and use Error Reporting to identify and fix the issue.","D":"Run a Git compare between the previous and current Cloud Build Configuration files to find and fix the bug.","B":"Change the CI pipeline to push the artifacts is Container Registry instead of Docker Hub.","A":"Disable the CI pipeline and revert to manually building and pushing the artifacts."},"timestamp":"2021-06-02 20:29:00","question_id":130,"answer":"D","answer_ET":"D","answer_images":[],"answer_description":"","question_images":[]}],"exam":{"id":6,"isBeta":false,"isMCOnly":true,"lastUpdated":"11 Apr 2025","isImplemented":true,"name":"Professional Cloud DevOps Engineer","provider":"Google","numberOfQuestions":196},"currentPage":26},"__N_SSP":true}