{"pageProps":{"questions":[{"id":"9ukTSzwmshu5F6BSMXVm","unix_timestamp":1707516240,"exam_id":71,"url":"https://www.examtopics.com/discussions/microsoft/view/133471-exam-dp-600-topic-1-question-50-discussion/","answer_images":["https://img.examtopics.com/dp-600/image231.png"],"answers_community":[],"answer_description":"","question_id":126,"isMC":false,"question_text":"HOTSPOT -\nYou have a Fabric workspace named Workspace1 and an Azure Data Lake Storage Gen2 account named storage1. Workspace1 contains a lakehouse named Lakehouse1.\nYou need to create a shortcut to storage1 in Lakehouse1.\nWhich connection and endpoint should you specify? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n//IMG//","answer":"","timestamp":"2024-02-09 23:04:00","question_images":["https://img.examtopics.com/dp-600/image70.png"],"answer_ET":"","discussion":[{"poster":"wellingtonluis","comment_id":"1165024","timestamp":"1709493120.0","upvote_count":"41","comments":[{"poster":"testtaker45","timestamp":"1737243180.0","upvote_count":"3","comment_id":"1342799","content":"Someone give WellingtonLuis a medal for this answer. Spot on. \nSo if we are working in apache spark, we would use the abfss protocol. \nAs we are creating the shortcut in Fabric the protocol is https, and the endpoint is dfs.core.windows.net. I had to go deep to figure this out, and your documentation was very helpful."}],"content":"HTTPS, DFS\nhttps://learn.microsoft.com/en-us/fabric/onelake/create-adls-shortcut"},{"poster":"Felgas","comment_id":"1161691","upvote_count":"12","timestamp":"1709130060.0","content":"It's asking to create a shortcut in the lakehouse. To do that, the URL should be https://adls.dfs.core.windows.net/file.\nTo access the shortcut in Fabric, you use the abfss path"},{"comment_id":"1312468","content":"Abfss and dfs","poster":"Rakesh16","upvote_count":"4","timestamp":"1731651120.0"},{"content":"I have just passed the exam, instead of 'connection' the question said: 'protocol'","comment_id":"1232839","poster":"ca63a55","upvote_count":"5","timestamp":"1718794740.0"},{"comment_id":"1229735","poster":"DarioReymago","timestamp":"1718271660.0","content":"HTTPS, DFS\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#access","upvote_count":"4"},{"comment_id":"1229622","upvote_count":"2","poster":"ManuelG00","content":"Connection: 2. abfss\nEndpoint: 2. dfs","timestamp":"1718254860.0"},{"comment_id":"1220243","timestamp":"1716900240.0","upvote_count":"7","poster":"282b85d","content":"• Connection (abfss): The abfss (Azure Blob File System Secure) protocol is used for secure connections to Azure Data Lake Storage Gen2. This protocol ensures that the connection is encrypted, providing a secure method to access the storage.\n• Endpoint (dfs): The dfs (Data Lake Storage) endpoint is used to connect to the hierarchical namespace of Azure Data Lake Storage Gen2, which is optimized for big data analytics workloads. It allows for file and directory-based operations, making it suitable for data lake scenarios."},{"content":"https, dfs\nIf you look at the tooltip in the connection settings it has the following about the endpoint, where the example shows https:\n\"The URL of the ADLSG2 endpoint to connect to. To avoid invalid credential errors, be aware to use the '.dfs' rather than '.blob' endpoint, ensure you are assigned a blob-specific role, and have the networking access set appropriately.\"","upvote_count":"3","timestamp":"1711035660.0","poster":"a_51","comment_id":"1179370"},{"content":"Just tried it and when copying https://strorage.dfs.core.windows.net/ it worked so definitely http, dfs","comment_id":"1166465","timestamp":"1709646300.0","poster":"Valcon_doo_NoviSad","upvote_count":"4"},{"poster":"Momoanwar","timestamp":"1708195860.0","content":"Correct path exemple :\nabfss://Dev@onelake.dfs.fabric.microsoft.com/lakehouse1...","comment_id":"1152771","upvote_count":"4"},{"poster":"IshtarSQL","timestamp":"1707516240.0","content":"abfs, dfs","comment_id":"1145824","upvote_count":"3","comments":[{"content":"Access Azure storage\nOnce you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the abfss driver for greater security.\n\nPython\n\nCopy\nspark.read.load(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\nhttps://learn.microsoft.com/en-us/azure/databricks/connect/storage/azure-storage","timestamp":"1708196520.0","poster":"Momoanwar","upvote_count":"1","comment_id":"1152775"}]}],"topic":"1"},{"id":"BScDdrHvChZcV1v5mlWM","answer":"A","answers_community":["A (100%)"],"unix_timestamp":1708197900,"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/134072-exam-dp-600-topic-1-question-51-discussion/","answer_description":"","topic":"1","discussion":[{"poster":"Momoanwar","timestamp":"1723915500.0","upvote_count":"31","content":"Selected Answer: A\nIn Apache Spark, broadcasting refers to an optimization technique for join operations. When you join two DataFrames or RDDs and one of them is significantly smaller than the other, Spark can \"broadcast\" the smaller table to all nodes in the cluster. This approach avoids the need for network shuffles for each row of the larger table, significantly reducing the execution time of the join operation.","comment_id":"1152784"},{"content":"Selected Answer: A\nA - Broadcasting generates a copy of the data across all the nodes in the Spark cluster. Therefore, during a join operation, it won't require any I/Os from other nodes, thereby, reducing the shuffling requirement.","poster":"sraakesh95","upvote_count":"7","timestamp":"1725194220.0","comment_id":"1163575"},{"content":"Selected Answer: A\nBroadcasting: The F.broadcast(customers) function is used to broadcast the smaller DataFrame (customers). This ensures that the smaller DataFrame is replicated across all nodes, and each node can perform the join locally with its partition of the larger DataFrame (transactions). This significantly reduces the data movement (shuffling) required during the join operation.","upvote_count":"1","timestamp":"1732806240.0","comment_id":"1220269","poster":"282b85d"},{"poster":"stilferx","comment_id":"1209124","content":"Selected Answer: A\nIMHO, \"A\" is correct!\n\nBroadcast joining copies the smaller table to each worker in Spark, which may significantly improve performance by reducing shuffling","timestamp":"1731206580.0","upvote_count":"3"},{"comment_id":"1153119","content":"Selected Answer: A\nA. transactions.join(F.broadcast(customers), transactions.customer_id == customers.customer_id)\n\nOptimized method to perform a join between a very large table and a smaller one.\n\nSource: https://sparkbyexamples.com/spark/broadcast-join-in-spark/\"","timestamp":"1723962660.0","poster":"SamuComqi","upvote_count":"2"}],"answer_images":[],"choices":{"A":"transactions.join(F.broadcast(customers), transactions.customer_id == customers.customer_id)","C":"transactions.join(customers, transactions.customer_id == customers.customer_id)","B":"transactions.join(customers, transactions.customer_id == customers.customer_id).distinct()","D":"transactions.crossJoin(customers).where(transactions.customer_id == customers.customer_id)"},"question_id":127,"question_text":"You are analyzing customer purchases in a Fabric notebook by using PySpark.\nYou have the following DataFrames:\ntransactions: Contains five columns named transaction_id, customer_id, product_id, amount, and date and has 10 million rows, with each row representing a transaction. customers: Contains customer details in 1,000 rows and three columns named customer_id, name, and country.\nYou need to join the DataFrames on the customer_id column. The solution must minimize data shuffling.\nYou write the following code.\nfrom pyspark.sql import functions as F\nresults =\nWhich code should you run to populate the results DataFrame?","timestamp":"2024-02-17 20:25:00","exam_id":71,"answer_ET":"A","question_images":[]},{"id":"Sg7Q8hYzbKvCWoOM9Tbx","question_images":["https://img.examtopics.com/dp-600/image71.png","https://img.examtopics.com/dp-600/image72.png"],"answer_images":["https://img.examtopics.com/dp-600/image232.png"],"answer":"","question_id":128,"unix_timestamp":1708198800,"answer_description":"","timestamp":"2024-02-17 20:40:00","exam_id":71,"discussion":[{"timestamp":"1709285820.0","upvote_count":"32","poster":"lordcarlosv","comment_id":"1163364","comments":[{"poster":"SKM1964","timestamp":"1734557400.0","content":"In the article mentioned, it is written \n\"If the semantic model falls back to DirectQuery mode to process the visual’s DAX query, you see a Direct query performance metric, as shown in the following image:\".\nSo in my opinion, it should be Automatic and DirectQuery","comment_id":"1328720","upvote_count":"4"},{"poster":"Qordata","upvote_count":"1","content":"In the link there is only a single card visual placed not two card visuals\nSee image below point\nPlace a card visual on the report canvas, select a data column to create a basic report, and then on the View menu, select Performance analyzer.","timestamp":"1722951840.0","comment_id":"1261688"}],"content":"The answer is Automatic and Direct Lake, actually the picture comes from\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/directlake-analyze-qp \nIn this article you can see there are table1 and view1, performance analyser shows: \n• First card is linked to Table1 so direct lake is used\n• Second card is linked to View1 so it does direct query\nAs the model can use direct lake and direct query you can conclude that the fallback behavior is automatic. \nFor direct lake behavior you can read this: https://powerbi.microsoft.com/en-us/blog/leveraging-pure-direct-lake-mode-for-maximum-query-performance"},{"content":"There is no table visual in this image.","comment_id":"1165030","timestamp":"1709494200.0","upvote_count":"8","comments":[{"poster":"Felix_G","timestamp":"1715215920.0","upvote_count":"1","content":"The table visual is on the bottom of the link, it says \"processed in Direct Lake Mode.\" on top of the table.","comment_id":"1208607"}],"poster":"wellingtonluis"},{"poster":"gtc108","comment_id":"1234517","content":"https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-analyze-query-processing\nDirect Lake Mode, Direct Query","timestamp":"1718981940.0","upvote_count":"2"},{"timestamp":"1716902160.0","content":"1. The Direct Lake fallback behavior is set to: DirectQueryOnly\nThe Performance analyzer shows that the query type is \"Direct query\". This indicates that the fallback behavior is set to only use DirectQuery and not Direct Lake. If the behavior were set to \"DirectLakeOnly\", the query would fail if Direct Lake could not be used. \"Automatic\" would use Direct Lake when possible and fall back to DirectQuery, but since it's specifically showing \"Direct query\", it suggests \"DirectQueryOnly\".\n\n2.The query for the table visual is executed by using: Direct Query\nThe Performance analyzer directly mentions \"Direct query\" as the query type for the visual. This confirms that the data is being retrieved using DirectQuery mode, not Direct Lake or any composite model.","upvote_count":"1","poster":"282b85d","comments":[{"content":"No, the 1st card shows that Direct Lake was used. DirectQueryOnly means that only Direct Query can be used.\nAutomatic mode - it uses Direct Lake where it can and fallbacks to Direct Query\nhttps://learn.microsoft.com/en-us/fabric/get-started/direct-lake-manage#set-the-direct-lake-behavior-property","poster":"AdventureChick","comment_id":"1326258","timestamp":"1734126120.0","upvote_count":"1"}],"comment_id":"1220279"},{"poster":"AndreaRosho1","timestamp":"1716640800.0","comment_id":"1218324","upvote_count":"6","comments":[{"content":"In the link provided says: If the semantic model falls back to DirectQuery mode to process the visual’s DAX query, you see a \"Direct query\" performance metric, as shown in the following image (the image of question). Is that correct?","comment_id":"1228162","upvote_count":"1","poster":"282b85d","timestamp":"1718058900.0"}],"content":"Asnwer: Automatic - DirectLake.\n the image is cut bad. if you follow this link: https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-analyze-query-processing you can see that in Data section of Power BI they are using two different tables"},{"comment_id":"1209128","content":"IMHO, Automatic & DirectQuery","timestamp":"1715302380.0","poster":"stilferx","upvote_count":"2","comments":[{"poster":"stilferx","comment_id":"1209129","timestamp":"1715302440.0","content":"sorry, Automatic & Direct Lake","upvote_count":"1"}]},{"timestamp":"1709646840.0","comments":[{"upvote_count":"5","content":"Additionally, this link (https://learn.microsoft.com/en-us/power-bi/enterprise/directlake-analyze-qp) explicitly says the following: If the dataset falls back to DirectQuery mode to process the visual’s DAX query, you see a Direct query performance metric, as shown in the following image --> and the image is the exact same as in the question.","poster":"Valcon_doo_NoviSad","comment_id":"1166506","timestamp":"1709647200.0"}],"upvote_count":"3","comment_id":"1166491","poster":"Valcon_doo_NoviSad","content":"The question itself is uniquely confusing and ambiguous and at times simply wrong, but I would go with Automatic and DirectQuery bc DAX falls back to DirectQuery always (it is a known shortcoming --> all explained here: https://learn.microsoft.com/en-us/power-bi/enterprise/directlake-overview)"},{"comment_id":"1163602","timestamp":"1709306700.0","poster":"sraakesh95","content":"Automatic, DirectLake\nIMO, as the fallback method is not explicitly mentioned, by default any DirectLake query falls. back to DirectQuery, hence, it is automatic.\nAfter the changes have been made, the query method has fallen back to DirectQuery, hence, I would choose DirectQuery.\nWould be great to know further opinion into this.","upvote_count":"4"},{"content":"I think automatic and direct lake : we have fallback direct query for card and no informations about table visual so its direct lake.","poster":"Momoanwar","comments":[{"timestamp":"1711711260.0","comment_id":"1185381","poster":"a_51","upvote_count":"3","content":"The final block of card shows Direct Query. I think the wording was wrong as there are no tables shown. So I suggest automatic and Direct query as noted by others."}],"comment_id":"1152788","timestamp":"1708198800.0","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/microsoft/view/134073-exam-dp-600-topic-1-question-52-discussion/","question_text":"HOTSPOT -\nYou have a Microsoft Power BI report and a semantic model that uses Direct Lake mode.\nFrom Power BI Desktop, you open Performance analyzer as shown in the following exhibit.\n//IMG//\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\n//IMG//","answers_community":[],"topic":"1","isMC":false,"answer_ET":""},{"id":"ds37QsrnTD2Vr5LsMt4m","topic":"1","answers_community":[],"discussion":[{"comment_id":"1160860","upvote_count":"43","timestamp":"1709054160.0","poster":"XiltroX","content":"The correct answers:\n1. withColumn\n2. cast('date')\n3. .filter('fareAmount......."},{"comment_id":"1152805","content":"We need to add column not rename existing column. Here is the correct answer :\ndf.withColumn('pickupDate', df['pickupDateTime'].cast(DateType())) \\\n .filter(\"fareAmount > 0 AND fareAmount < 100\")","poster":"Momoanwar","timestamp":"1708202280.0","upvote_count":"11"},{"timestamp":"1723557300.0","poster":"c119533","comment_id":"1265190","content":"Tested answer\ndf = spark.read.format(\"delta\").load(\"Tables/factinternetsales\")\ndf2 = df.withColumn(\"pickupdate\", df['pickupDateTime'].cast('date')).filter(\"fareAmount > 0 AND fareAmount < 100\")\ndf2.show()","upvote_count":"2"},{"timestamp":"1717950900.0","upvote_count":"8","comment_id":"1227420","poster":"woliveiras","content":"The correct answers:\n1. withColumn\n2. cast('date')\n3. .filter('fareAmount\nTested!!!"},{"content":"why the expression with where is not correct? Is it because it includes 100? not less than 100?","comment_id":"1214322","timestamp":"1716207480.0","upvote_count":"1","poster":"72bd3bc"},{"poster":"PiyushT","content":".filter(fareamount >0 and <100) does not work, I tried the code myself. You have to use same condition but with \"col\" like this .filter(col(fareamount) > 0 and col(fareamount) < 100)","upvote_count":"2","comment_id":"1210676","timestamp":"1715567040.0"},{"poster":"stilferx","timestamp":"1715303880.0","upvote_count":"6","content":"IMHO, \nwithColumn -> cast(dateType()) -> filter(\"fareAmount > 0 AND fareAmount < 100\")\nis correct.\n\nAs colleagues said, cast(dateType()), not cast('date'). Let's consider it a typo from Microsoft.","comment_id":"1209136"},{"poster":"earlqq","timestamp":"1708806480.0","comment_id":"1158142","content":"A bit incorrect.\n.withcolumn\n.cast(date)\n.filter(fareamount >0 and <100)","upvote_count":"8"}],"answer_images":["https://img.examtopics.com/dp-600/image233.png"],"answer_ET":"","question_images":["https://img.examtopics.com/dp-600/image75.png","https://img.examtopics.com/dp-600/image76.png"],"timestamp":"2024-02-13 12:52:00","exam_id":71,"answer":"","answer_description":"","unix_timestamp":1707825120,"question_id":129,"question_text":"HOTSPOT -\nYou have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a table named Nyctaxi_raw. Nyctaxi_row contains the following table:\n//IMG//\n\nYou create a Fabric notebook and attach it to Lakehouse1.\nYou need to use PySpark code to transform the data. The solution must meet the following requirements:\nAdd a column named pickupDate that will contain only the date portion of pickupDateTime.\nFilter the DataFrame to include only rows where fareAmount is a positive number that is less than 100.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n//IMG//","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/133739-exam-dp-600-topic-1-question-53-discussion/"},{"id":"36PnVvKi8DCd1Q08aMDN","topic":"1","question_id":130,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have a Fabric tenant that contains a new semantic model in OneLake.\nYou use a Fabric notebook to read the data into a Spark DataFrame.\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.\nSolution: You use the following PySpark expression:\ndf.explain()\nDoes this meet the goal?","choices":{"B":"No","A":"Yes"},"answer_description":"","answer_ET":"B","unix_timestamp":1708202580,"question_images":[],"timestamp":"2024-02-17 21:43:00","answer_images":[],"isMC":true,"discussion":[{"comment_id":"1220292","poster":"282b85d","content":"Selected Answer: B\nThe df.explain() method in PySpark is used to print the logical and physical plans of a DataFrame, which helps in understanding how Spark plans to execute the query. It does not compute any statistical values like min, max, mean, or standard deviation.\n**To achieve the goal, you should use: df.describe().show()","upvote_count":"9","timestamp":"1732807860.0"},{"poster":"SamuComqi","upvote_count":"5","timestamp":"1723963320.0","comment_id":"1153121","comments":[{"poster":"SamuComqi","comment_id":"1153125","content":"Also df.summary() is a valid solution.\n\nSource ---> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.summary.html","timestamp":"1723963560.0","upvote_count":"1"}],"content":"Selected Answer: B\nThe correct syntax is df.describe().\n\nSources:\n* describe --> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html\n* explain --> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.explain.html"},{"timestamp":"1731208860.0","poster":"stilferx","upvote_count":"1","comment_id":"1209139","content":"Selected Answer: B\nIMHO, NOOO\n\nexplain() shows the execution plan..."},{"poster":"a_51","upvote_count":"2","comment_id":"1179390","timestamp":"1726928100.0","content":"Selected Answer: B\ndescribe is how you get the information."},{"upvote_count":"4","timestamp":"1723920180.0","comment_id":"1152808","poster":"Momoanwar","content":"Selected Answer: B\nNo explain is for the execut plan"}],"answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/134075-exam-dp-600-topic-1-question-54-discussion/","exam_id":71,"answers_community":["B (100%)"]}],"exam":{"isMCOnly":false,"lastUpdated":"12 Apr 2025","numberOfQuestions":179,"provider":"Microsoft","id":71,"isBeta":false,"isImplemented":true,"name":"DP-600"},"currentPage":26},"__N_SSP":true}