{"pageProps":{"questions":[{"id":"2V8jhpdWjk073rdxyfHJ","timestamp":"2021-03-12 14:14:00","question_text":"DRAG DROP -\nYou are designing a real-time processing solution for maintenance work requests that are received via email. The solution will perform the following actions:\n✑ Store all email messages in an archive.\n✑ Access weather forecast data by using the Python SDK for Azure Open Datasets.\n✑ Identify high priority requests that will be affected by poor weather conditions and store the requests in an Azure SQL database.\nThe solution must minimize costs.\nHow should you complete the solution? To answer, drag the appropriate services to the correct locations. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","answer":"","question_id":96,"discussion":[{"content":"I believe it is Databricks in the second box due to the Python - Azure Open Datasets.","comments":[{"poster":"vrmei","upvote_count":"1","content":"Databrick is the correct answer","timestamp":"1624215540.0","comment_id":"386536"},{"poster":"H_S","upvote_count":"5","timestamp":"1615839120.0","comment_id":"311735","content":"it's for sure azure databricks"},{"content":"How? The processing unit has to pick up incoming events and then process. ASA is best fit for it","poster":"Debjit","timestamp":"1616552700.0","comment_id":"318693","upvote_count":"3","comments":[{"poster":"cadio30","comment_id":"366218","content":"ASA doesn't have Python SDK on it","timestamp":"1621928280.0","upvote_count":"1"}]},{"comments":[],"content":"Jony is right. \nhttps://docs.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets","timestamp":"1617016200.0","comment_id":"323366","poster":"devpool","upvote_count":"6"},{"poster":"cadio30","timestamp":"1621928340.0","upvote_count":"2","comment_id":"366219","content":"Agree with this solution"}],"timestamp":"1615750920.0","poster":"Jony","upvote_count":"36","comment_id":"310801"},{"comment_id":"308862","content":"Can we use Stream Analytics instead of the Logic app?","upvote_count":"8","poster":"AlexD332","timestamp":"1615554840.0"},{"upvote_count":"1","content":"The NOAA data options only show Databricks or Synapse, which was not an option in the question, and Azure Notebooks which was in preview and no longer available (and also not an option in the question):\n\nhttps://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/#AzureDatabricks","comment_id":"376173","poster":"MMM777","timestamp":"1622994360.0"},{"content":"The key here is 'Identify high priority requests that will be affected by poor weather conditions..' - This can be done efficiently by Logic app.","comment_id":"341901","comments":[{"upvote_count":"1","comment_id":"367515","poster":"BobFar","timestamp":"1622075340.0","content":"check this link\nhttps://docs.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets"}],"timestamp":"1619254020.0","upvote_count":"4","poster":"Alka3"},{"content":"Azure open dataset can be accessed from databricks or any Python environment with or without Spark. Hence, the second box should be Databricks.\nhttps://docs.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets","upvote_count":"3","comment_id":"321081","timestamp":"1616758800.0","poster":"sdas1"},{"timestamp":"1616240220.0","upvote_count":"1","poster":"ekko1224","comments":[{"content":"I think they are not in preview right now, this thread is from 2018, so it is not up to date","upvote_count":"1","comment_id":"316538","comments":[{"comment_id":"317165","upvote_count":"2","timestamp":"1616418000.0","comments":[{"poster":"maynard13x8","comment_id":"329790","content":"I think Logic App (in which you can use Python by mean of function App) o stream analytics are better.","timestamp":"1617726180.0","upvote_count":"1"}],"poster":"H_S","content":"yet databricks is good condidate"}],"poster":"jms309","timestamp":"1616347920.0"}],"comment_id":"315588","content":"Function Apps would be the way to go for any custom code needed in logic apps.\nPython in Azure Functions is still in Preview, so it's not recommended for production use.\nYou can refer to this for more information on creating a new python function app.\nhttps://social.msdn.microsoft.com/Forums/en-US/1b46dcb2-2832-4861-a214-e49e85247d53/how-can-i-run-a-python-script-in-logic-apps"}],"isMC":false,"exam_id":66,"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/46706-exam-dp-201-topic-2-question-34-discussion/","unix_timestamp":1615554840,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0018500001.png"],"answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0018500002.png"],"answer_ET":"","answer_description":"Box 1: Azure Storage -\nAzure Event Hubs enables you to automatically capture the streaming data in Event Hubs in an Azure Blob storage or Azure Data Lake Storage Gen 1 or Gen 2 account of your choice, with the added flexibility of specifying a time or size interval. Setting up Capture is fast, there are no administrative costs to run it, and it scales automatically with Event Hubs throughput units. Event Hubs Capture is the easiest way to load streaming data into Azure, and enables you to focus on data processing rather than on data capture.\n\nBox 2: Azure Logic Apps -\nYou can monitor and manage events sent to Azure Event Hubs from inside a logic app with the Azure Event Hubs connector. That way, you can create logic apps that automate tasks and workflows for checking, sending, and receiving events from your Event Hub.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview https://docs.microsoft.com/en-us/azure/connectors/connectors-create-api-azure-event-hubs"},{"id":"cGFAcKi1nI8PGYsz44GW","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/47546-exam-dp-201-topic-2-question-35-discussion/","topic":"2","answer_ET":"C","choices":{"A":"Azure SQL Database","B":"Azure Cosmos DB","D":"Azure Databricks","C":"Azure Stream Analytics"},"discussion":[{"timestamp":"1616761440.0","upvote_count":"5","content":"As per below link the answer is correct.\nhttps://azure.microsoft.com/en-in/blog/new-capabilities-in-stream-analytics-reduce-development-time-for-big-data-apps/","poster":"sdas1","comment_id":"321103"},{"poster":"cadio30","comments":[{"content":"Opt to choose Azure Databricks instead of Azure Streaming Analytics due to the keywork 'large dataset' \n\nReference: https://techcommunity.microsoft.com/t5/analytics-on-azure/azure-stream-analytics-real-time-analytics-for-big-data-made/ba-p/549621","upvote_count":"2","timestamp":"1623287760.0","comment_id":"378638","poster":"cadio30"},{"upvote_count":"2","content":"ASA doesn't support Parquet format.!","timestamp":"1622075520.0","comment_id":"367518","comments":[{"timestamp":"1622075640.0","comment_id":"367519","poster":"BobFar","upvote_count":"1","content":"I was wrong, it supports now \n\nhttps://azure.microsoft.com/en-us/updates/stream-analytics-offers-native-support-for-parquet-format/#:~:text=Azure%20Stream%20Analytics%20now%20offers,in%20the%20Big%20Data%20ecosystems."}],"poster":"BobFar"},{"upvote_count":"1","content":"One of the requirements is to be able to interactively query the whole (possibly very large) dataset according to the scenario. This requirement alone is a perfect fit for Spark. I highly doubt there is a sensible way to achieve this with ASA. Therefore I vote for Databricks.","poster":"mbravo","timestamp":"1622278560.0","comment_id":"369294"}],"timestamp":"1621930080.0","comment_id":"366240","content":"Both Azure Databricks and Azure Stream analytics can output data to parquet format and have interactive queries as well. For simplicity, I'll choose Azure Stream Analytics","upvote_count":"5"},{"timestamp":"1626769500.0","comment_id":"410100","content":"By outputting data in parquet format into a blob store or a data lake, you can take advantage of Azure Stream Analytics to power large scale streaming extract, transfer, and load (ETL), to run batch processing, to train machine learning algorithms, or to run interactive queries on your historical data.\n\nSoure: https://azure.microsoft.com/en-in/blog/new-capabilities-in-stream-analytics-reduce-development-time-for-big-data-apps/","comments":[{"content":"What this quote says is that ASA can output parquet format to blob storage, so that another tool can then run interactive queries on the data. ASA itself can't do interactive queries on parquet in blob storage, which is what is required here. I'd go with databricks.","upvote_count":"1","comment_id":"422843","timestamp":"1628610420.0","poster":"hello_there_"}],"poster":"daradev","upvote_count":"1"},{"content":"Native support for egress in Apache parquet format into Azure Blob Storage is now generally available. Parquet is a columnar format enabling efficient big data processing. By outputting data in parquet format into a blob store or a data lake, you can take advantage of Azure Stream Analytics to power large scale streaming extract, transfer, and load (ETL), to run batch processing, to train machine learning algorithms, or to run interactive queries on your historical data. We are now announcing general availability of this feature for egress to Azure Blob Storage.","upvote_count":"3","timestamp":"1619998560.0","poster":"VG2007","comment_id":"348117"},{"timestamp":"1616829180.0","upvote_count":"2","poster":"jms309","comments":[{"upvote_count":"3","comment_id":"338241","timestamp":"1618755420.0","poster":"anamaster","content":"interactive querying eliminates ASA"},{"comments":[{"comment_id":"350176","poster":"saifone","timestamp":"1620208920.0","upvote_count":"2","content":"It does as of July 2019 https://azure.microsoft.com/en-us/updates/stream-analytics-offers-native-support-for-parquet-format/"}],"content":"Azure Stream Analytics does not support Parquet data format.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns","comment_id":"343178","poster":"niwe","upvote_count":"2","timestamp":"1619434080.0"}],"comment_id":"321659","content":"I think that Databrick is a good answer. I'm not sure if Azure Stream Analytics is another right answer but maybe there are two possibilities"},{"comment_id":"321102","timestamp":"1616761380.0","content":"As per below link the answer is correct.\nnew-capabilities-in-stream-analytics-reduce-development-time-for-big-data-app","poster":"sdas1","upvote_count":"1"},{"comment_id":"313360","upvote_count":"2","timestamp":"1615992060.0","comments":[{"upvote_count":"7","timestamp":"1616418240.0","comment_id":"317176","content":"i think it's D because the interactive querying of the entire dataset.\nentire dataset/interative isn't possible with A.stream","poster":"H_S"}],"content":"is C really the correct answer pls?","poster":"YOMYOM"}],"answer_description":"Azure Stream Analytics is a fully managed PaaS offering that enables real-time analytics and complex event processing on fast moving data streams.\nBy outputting data in parquet format into a blob store or a data lake, you can take advantage of Azure Stream Analytics to power large scale streaming extract, transfer, and load (ETL), to run batch processing, to train machine learning algorithms, or to run interactive queries on your historical data.\nReference:\nhttps://azure.microsoft.com/en-us/blog/new-capabilities-in-stream-analytics-reduce-development-time-for-big-data-apps/","isMC":true,"answer":"C","question_images":[],"question_id":97,"answers_community":[],"unix_timestamp":1615992060,"exam_id":66,"timestamp":"2021-03-17 15:41:00","question_text":"You have a large amount of sensor data stored in an Azure Data Lake Storage Gen2 account. The files are in the Parquet file format.\nNew sensor data will be published to Azure Event Hubs.\nYou need to recommend a solution to add the new sensor data to the existing sensor data in real-time. The solution must support the interactive querying of the entire dataset.\nWhich type of server should you include in the recommendation?"},{"id":"tv5KYsIgobir34KLerb8","exam_id":66,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0018800001.jpg"],"answer_description":"Box 1: Incremental load -\nWhen you start to build the end to end data integration flow the first challenge is to extract data from different data stores, where incrementally (or delta) loading data after an initial full load is widely used at this stage. Now, ADF provides a new capability for you to incrementally copy new or changed files only by\nLastModifiedDate from a file-based store. By using this new feature, you do not need to partition the data by time-based folder or file name. The new or changed file will be automatically selected by its metadata LastModifiedDate and copied to the destination store.\n\nBox 2: Tumbling window -\nTumbling window triggers are a type of trigger that fires at a periodic time interval from a specified start time, while retaining state. Tumbling windows are a series of fixed-sized, non-overlapping, and contiguous time intervals. A tumbling window trigger has a one-to-one relationship with a pipeline and can only reference a singular pipeline.\nReference:\nhttps://azure.microsoft.com/en-us/blog/incrementally-copy-new-files-by-lastmodifieddate-with-azure-data-factory/ https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger","timestamp":"2021-03-26 13:12:00","answers_community":[],"topic":"2","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0018900001.jpg"],"question_id":98,"unix_timestamp":1616760720,"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/48245-exam-dp-201-topic-2-question-36-discussion/","discussion":[{"upvote_count":"1","poster":"satyamkishoresingh","timestamp":"1629102360.0","comment_id":"425678","content":"I believe it can be tumbling as well as fixed schedule as they both can do fixed hour"},{"comment_id":"378046","content":"According the MS documentation, incremental loads are used together with tumbling window. Tumbling window is used in both of these examples where we are performing an incremental load from Blob Storage. \n\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool\n\nand \n\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-partitioned-file-name-copy-data-tool","upvote_count":"3","timestamp":"1623220500.0","poster":"mbravo"},{"upvote_count":"1","comment_id":"376206","poster":"MMM777","timestamp":"1622997360.0","content":"Tumbling Window trigger is a \"smarter\" run - what if the pipeline takes longer than an hour to run? \n\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison"},{"poster":"BobFar","comment_id":"374633","upvote_count":"2","content":"The appropriate solutions are 'incremental load' and schedule is mentioned as once hourly which is 'Tumbling window' the answer is correct!","timestamp":"1622854440.0"},{"comment_id":"366247","timestamp":"1621930740.0","upvote_count":"3","content":"The appropriate solutions are 'incremental load' and 'fixed schedule' as the basis is 1 hour trigger and the use of tumbling window requires further configuration than the mentioned schedule earlier. It would be better if there is an option to use 'storage events' as the ADF will trigger if a blob is created or deleted.\n\nReference: https://www.mssqltips.com/sqlservertip/6061/create-tumbling-window-trigger-in-azure-data-factory-adf/","poster":"cadio30","comments":[{"comment_id":"383032","timestamp":"1623807180.0","upvote_count":"1","content":"In the event the requirement requires to take in consideration the load processing time then tumbling window is the appropriate configuration as the trigger won't overlap.","poster":"cadio30"}]},{"upvote_count":"1","timestamp":"1620995160.0","content":"tumbling window will be used for stream analytics...","comment_id":"357180","poster":"tamil1006"},{"content":"But schedule is mentioned as once hourly , why would it be Tumbling window ?","timestamp":"1619714760.0","comment_id":"345546","upvote_count":"4","poster":"Amy007"},{"content":"If its tumbling window then why not new individual file as they arrive? Tumbling window works only when a new event occurs","comments":[{"timestamp":"1616831940.0","poster":"Debjit","comment_id":"321674","upvote_count":"4","content":"ignore. The answer is correct"}],"comment_id":"321100","poster":"Debjit","timestamp":"1616760720.0","upvote_count":"1"}],"answer":"","question_text":"HOTSPOT -\nYou have an Azure Storage account that generates 200,000 new files daily. The file names have a format of {YYYY}/{MM}/{DD}/{HH}/{CustomerID}.csv.\nYou need to design an Azure Data Factory solution that will load new data from the storage account to an Azure Data Lake once hourly. The solution must minimize load times and costs.\nHow should you configure the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point\nHot Area:\n//IMG//","answer_ET":""},{"id":"3biQaOm6SDdjA5D2fKKh","url":"https://www.examtopics.com/discussions/microsoft/view/46709-exam-dp-201-topic-2-question-37-discussion/","answer_images":[],"choices":{"B":"Filter by the last modified date of the source files.","C":"Delete the source files after they are copied.","D":"Specify a file naming pattern for the destination.","A":"Delete the files in the destination before loading new data."},"isMC":true,"question_text":"You are designing a solution that will copy Parquet files stored in an Azure Blob storage account to an Azure Data Lake Storage Gen2 account.\nThe data will be loaded daily to the data lake and will use a folder structure of {Year}/{Month}/{Day}/.\nYou need to design a daily Azure Data Factory data load to minimize the data transfer between the two accounts.\nWhich two configurations should you include in the design? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","unix_timestamp":1615555620,"question_images":[],"exam_id":66,"topic":"2","answer_description":"B: To copy a subset of files under a folder, specify folderPath with a folder part and fileName with a wildcard filter.\nC: After completion: Choose to do nothing with the source file after the data flow runs, delete the source file, or move the source file. The paths for the move are relative.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage","answer":"BC","question_id":99,"answers_community":[],"discussion":[{"poster":"phi618t","comment_id":"377106","comments":[{"upvote_count":"2","content":"This is a basic question. Copy data from one place to another. The requirements are : 1- need to minimize transfert and 2- need to adapte data to the destination folder structure. Filter on LastModifiedDate will copy everything that have changed since the latest load while minimizing the data transfert. Specifying the file naming pattern allows to copy data at the right place to the destination Data Lake. The answer is BD","timestamp":"1633014360.0","comment_id":"454987","poster":"Marcus1612"},{"comment_id":"427206","content":"How naming pattern gonna minimize the Data Transfer? BC should be correct answer.","poster":"Bhagya123456","timestamp":"1629346860.0","upvote_count":"1"}],"content":"If you choose C. Delete the source files after they are copied, why do you choose B. Filter by the last modified date of the source files? I prefer BD.","timestamp":"1623111420.0","upvote_count":"12"},{"comment_id":"356603","timestamp":"1620930540.0","content":"Correct answer is BC.\nIn the source option of copy activities. There are three choices: 1. No Action 2. Delete Source files 3. Move","upvote_count":"10","poster":"Wendy_DK"},{"upvote_count":"4","poster":"BigMF","comments":[{"upvote_count":"1","content":"B ensures minimized data transfer. If it copies everything every time, then data transfer is not minimized.","comment_id":"444576","timestamp":"1631625000.0","poster":"YLiu"}],"comment_id":"379941","content":"A is obviously out and you're are not going to do both B and C so D is in by default. Your only choice at that point is B or C to go along with D. In my experience, you cannot rely 100% on any job to run every single day (assuming this process is daily). Therefore, if the job does not run for one or more days, if you were to choose B you would only copy over the most recent files and there would be files left in the storage account. Therefore, my choice would be to not filter and load everything that is in the storage account and then delete the files once they have been copied. So, C and D are my choices.","timestamp":"1623430440.0"},{"content":"I would like to choose CD.","poster":"mter2007","upvote_count":"3","comment_id":"339986","timestamp":"1618966260.0"},{"timestamp":"1617887460.0","content":"The was no requirement what to do with original files, so why i the world anwer C - delete them???","upvote_count":"3","poster":"maciejt","comment_id":"331214","comments":[{"timestamp":"1622854560.0","poster":"BobFar","upvote_count":"1","content":"I guess to make sure you dont read the file again!","comment_id":"374634"}]},{"upvote_count":"2","comment_id":"319055","poster":"Nik71","timestamp":"1616586060.0","content":"C seems not correcct as to deletion you can do life cycle mgmt in storage, so D should be second answer."},{"upvote_count":"1","comments":[{"content":"I think it\"s BD","comments":[{"timestamp":"1616208240.0","comment_id":"315320","comments":[{"content":"yes BD.. i think you are right","poster":"etl","upvote_count":"4","comment_id":"315321","timestamp":"1616208360.0"},{"content":"but this applies to finding a source files and D was about destintion file naming pattern... which there were no requirement to change the file name","timestamp":"1617887400.0","upvote_count":"2","poster":"maciejt","comment_id":"331213"}],"upvote_count":"1","poster":"etl","content":"Wildcard path: Using a wildcard pattern will instruct ADF to loop through each matching folder and file in a single Source transformation. This is an effective way to process multiple files within a single flow. Add multiple wildcard matching patterns with the + sign that appears when hovering over your existing wildcard pattern.\n\nFrom your source container, choose a series of files that match a pattern. Only container can be specified in the dataset. Your wildcard path must therefore also include your folder path from the root folder."},{"poster":"cadio30","upvote_count":"3","comment_id":"366776","content":"Agree with the answer B and D as this kind of setup doesn't perform any deletion from both storages which lessen the processing.","timestamp":"1621992660.0"}],"upvote_count":"22","comment_id":"311740","poster":"H_S","timestamp":"1615839420.0"}],"comment_id":"308873","timestamp":"1615555620.0","content":"thought it's the only logical choice but they said copy activity not moving files","poster":"AlexD332"}],"timestamp":"2021-03-12 14:27:00","answer_ET":"BC"},{"id":"Un48nxEqGUWpTIPC0MTq","topic":"2","answer_description":"Azure Stream Analytics on IoT Edge empowers developers to deploy near-real-time analytical intelligence closer to IoT devices so that they can unlock the full value of device-generated data. UDF are available in C# for IoT Edge jobs\nAzure Stream Analytics on IoT Edge runs within the Azure IoT Edge framework. Once the job is created in Stream Analytics, you can deploy and manage it using\nIoT Hub.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge","unix_timestamp":1617888240,"answers_community":["C (100%)"],"question_text":"You have a C# application that process data from an Azure IoT hub and performs complex transformations.\nYou need to replace the application with a real-time solution. The solution must reuse as much code as possible from the existing application.","question_id":100,"answer_images":[],"exam_id":66,"answer_ET":"C","discussion":[{"comment_id":"356589","content":"Correct answer is C","upvote_count":"12","timestamp":"1620929400.0","comments":[{"comment_id":"427208","content":"Nope It has to be Databricks.","upvote_count":"2","poster":"Bhagya123456","timestamp":"1629347100.0"}],"poster":"Wendy_DK"},{"upvote_count":"10","comments":[{"comments":[{"poster":"BigMF","upvote_count":"1","timestamp":"1623790860.0","comment_id":"382904","content":"ASA supports C#: https://azure.microsoft.com/en-us/blog/supercharge-your-azure-stream-analytics-query-with-c-code/","comments":[{"upvote_count":"2","comment_id":"393500","poster":"dinesh_tng","timestamp":"1624942200.0","content":"That is limited only to IoT Edge devices....Az Databricks looks more relevant"}]}],"content":"This seems to be correct answer as Databricks, as from the available options only Databricks have the support for C#. so the question says maximum reusability of the code, hence Databricks must be the correct answer. Thanks.","comment_id":"370665","poster":"Saravjeet","timestamp":"1622439120.0","upvote_count":"2"}],"timestamp":"1620127260.0","content":"Ans : A\nApache Spark in Azure Databricks supports C#.\nRef: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing","comment_id":"349352","poster":"rmk4ever"},{"upvote_count":"1","content":"Selected Answer: C\nAzure synapse analytics \n\nAzure Databricks is a big data analytics platform. While it's powerful, it may not be the most straightforward choice if the goal is to reuse existing C# code for real-time processing.\n\nGiven the requirements, Azure Stream Analytics is likely the most appropriate option for a real-time solution with the potential for code reuse. \n\nchat gpt","poster":"dakku987","comment_id":"1106233","timestamp":"1703609760.0"},{"upvote_count":"1","content":"question says complex transformation, I think ASA is not the right fit for that kind calculation. I would go with databricks","timestamp":"1629104220.0","poster":"satyamkishoresingh","comment_id":"425697"},{"comments":[{"poster":"coldog86","timestamp":"1619573820.0","upvote_count":"5","comment_id":"344329","content":"SPARK does real time processing, not Databricks\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/real-time-processing","comments":[{"poster":"dinesh_tng","upvote_count":"1","timestamp":"1624942260.0","content":"yeah, but spark is core of Databricks solution","comment_id":"393501"}]}],"timestamp":"1617888240.0","content":"why not databricks? can also utilize c# code and process data in real time","upvote_count":"5","poster":"maciejt","comment_id":"331219"}],"answer":"C","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/49608-exam-dp-201-topic-2-question-38-discussion/","choices":{"C":"Azure Stream Analytics","D":"Azure Data Factory","A":"Azure Databricks","B":"Azure Event Grid"},"timestamp":"2021-04-08 15:24:00"}],"exam":{"id":66,"isMCOnly":false,"provider":"Microsoft","numberOfQuestions":206,"isBeta":false,"lastUpdated":"12 Apr 2025","isImplemented":true,"name":"DP-201"},"currentPage":20},"__N_SSP":true}