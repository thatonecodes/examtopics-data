{"pageProps":{"questions":[{"id":"6xjCQFngfSapuwJDpR9i","url":"https://www.examtopics.com/discussions/microsoft/view/60780-exam-dp-203-topic-1-question-16-discussion/","answers_community":[],"isMC":false,"discussion":[{"upvote_count":"85","poster":"EddyRoboto","comment_id":"432494","comments":[{"poster":"ayn1","timestamp":"1742579100.0","content":"yes, and a d Binary is fast as it is direct copy, Parquet reads and writs at destination which can be slower.","comment_id":"1401650","upvote_count":"1"},{"content":"But the doc says \"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset.\" So I guess it's parquet then?","poster":"GameLift","comments":[{"timestamp":"1699010880.0","poster":"conscience","content":"I have used Binary to copy entire folders with its subfolder and files which were csv & parquet both. So, IMO binary would be correct answer.","upvote_count":"1","comment_id":"1061389"},{"content":"This note is referring to the fact that, in the template, you have to specify “BinarySink” as the type for the target Sink; and that exactly what the Copy data tool does. (you can check this by editing the created copy pipeline and see the code). Choosing BInary and PreserveHierarchy copy all file as they are perfectly.","timestamp":"1634488740.0","poster":"captainpike","comment_id":"463627","upvote_count":"5"}],"timestamp":"1634281560.0","comment_id":"462450","upvote_count":"11"},{"timestamp":"1644864540.0","poster":"iooj","upvote_count":"5","comment_id":"547305","content":"Agree. I've checked it. With binary source and sink datasets it works."},{"upvote_count":"2","comment_id":"460194","content":"no it must be parquet because The type property of the dataset must be set to Binary. and it's parquet hear so answer are correct","poster":"jed_elhak","timestamp":"1633893900.0"},{"upvote_count":"6","poster":"michalS","timestamp":"1630583760.0","content":"I agree. If it's just copying then binary is fine and would probably be faster","comment_id":"437840"},{"upvote_count":"9","comments":[{"poster":"Arend78","content":"\"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset.\"","timestamp":"1718700900.0","comment_id":"1232343","upvote_count":"2"}],"timestamp":"1632561420.0","comment_id":"451331","poster":"rav009","content":"agree. When using Binary dataset, the service does not parse file content but treat it as-is.\nNot parsing the file will save the time. (https://docs.microsoft.com/en-us/azure/data-factory/format-binary)\nSo Binary!"}],"timestamp":"1726814940.0","content":"This could be binary as source and sink, since there are no transformations on files. I tend to believe that would be binary the correct anwer."},{"poster":"AbhiGola","upvote_count":"64","comment_id":"439681","comments":[{"content":"As question has mentioned, Minimize time required to perform the copy activity.\nAnd binary is faster than Parquet. Hence, Binary is answer","poster":"NintyFour","comments":[{"timestamp":"1670858820.0","content":"No: req1 \"no transformation\", req2 \"Minimize time required to perform the copy activity\". Both must be met hence it's Parquet cause it's the second fastest choice and it requires no transformations.","comments":[{"timestamp":"1683523260.0","upvote_count":"4","content":"when doing a binary copy, you're not doing any transformation!","comment_id":"891827","poster":"mhi"}],"comment_id":"742980","upvote_count":"6","poster":"anto69"}],"timestamp":"1652704740.0","comment_id":"602580","upvote_count":"6"}],"timestamp":"1630844280.0","content":"Answer seems correct as data is store is parquet already and requirement is to do no transformation so answer is right"},{"timestamp":"1727077740.0","poster":"JustAnotherDBA","content":"The answer is correct. 3 reasons. \n \nThe file format is Parquet.\nParquet has the 2nd fastest load time.\nNo data transformations should happen,\n\nIf we are going to quote articles, please read the WHOLE article before posting. Check out the formats that the binary can handle. \n\n\"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset.\"","upvote_count":"12","comments":[{"content":"https://learn.microsoft.com/en-us/azure/data-factory/format-binary","upvote_count":"3","poster":"JustAnotherDBA","timestamp":"1672238700.0","comment_id":"759963"},{"poster":"mtc9","content":"Binary to binary copies the files as they are, retaining the same content, hence retaining the format and it;s faster than parquet, because it doesn;t require load at all just copy.","comment_id":"922226","timestamp":"1686660000.0","upvote_count":"2"}],"comment_id":"759962"},{"poster":"Lestrang","comment_id":"776414","timestamp":"1727077740.0","content":"According to ChatGPT\n\nWhile \"binary\" dataset type would be the fastest in terms of copying the data from one Azure storage account to another, it would not be the correct option in this scenario because it does not retain the original format of the files.\n\nIf the files contain data stored in the Apache Parquet format, specifying the source dataset type as \"binary\" would cause Data Factory to treat the files as generic binary files, and it would copy the data as is, without recognizing the original format of the files. This would result in losing the original format of the files, and possibly losing the structure of the data, it could also make it more difficult to read the data.\n\nAlso, When you copy files using binary dataset type, Data Factory will not be able to detect the changes in files and it copies the entire data each time, this can be inefficient in terms of time and storage.\n\nit really gives shitty azure answers in general, but ill go for parquet for this one.","upvote_count":"12","comments":[{"upvote_count":"1","poster":"mtc9","content":"ChatGPT is plainly wrong, binary type retais the original parquet format, ebcause it means to copy the files as they are and it;s faster than parquet dataset, because it's doesn't require parsing the files. Binary is correct.","timestamp":"1686659940.0","comment_id":"922225"}]},{"comment_id":"940949","timestamp":"1727077740.0","poster":"klayytech","upvote_count":"1","content":"The answer is still Source dataset type: Parquet Copy activity copy behavior: Preserve Hierarchy.\n\nEven though Binary can be used as the source dataset type, it is not the best option in this scenario. The original folder structure is important, and using Parquet as the source dataset type will ensure that it is preserved.\nSource dataset type: Parquet\nCopy activity copy behavior: Preserve Hierarchy\nThis will ensure that the files are copied in their original format, and that the original folder structure is preserved in the destination container. This is the best option for this scenario, as it meets all of the requirements."},{"comment_id":"925063","content":"Massimo Manganiello <massimo.manganiello@gmail.com>\n13:36 (49 minuti fa)\na me\n\nWhen it comes to efficiency, copying data from a Parquet file to another Parquet file is generally more efficient than copying to a binary format. This is because Parquet is a columnar storage format specifically designed for efficient data compression and query performance. It leverages advanced compression techniques and data encoding to minimize storage size and optimize query execution.\n\nCopying data from a Parquet file to a binary format may require additional steps and conversions. Binary formats, such as plain text or custom binary formats, may not have the same level of built-in compression and optimization as Parquet. Therefore, the copy process may involve additional serialization and deserialization steps, resulting in increased processing overhead and potentially larger storage requirements.\n\nIn summary, when the source and destination formats are both Parquet, copying between Parquet files is generally more efficient in terms of storage utilization and query performance.\n\nIn my opinion, the provided answer are corrects!","timestamp":"1727077740.0","poster":"auwia","upvote_count":"3"},{"poster":"Fusejonny1","comment_id":"1135892","upvote_count":"2","content":"Source dataset type should be set to binary.\nThe reason for this is that you’re not performing any transformations on the data, you’re simply copying it from one location to another while retaining the original folder structure. The binary dataset in Azure Data Factory is used for copying files as-is without parsing the file data.","timestamp":"1706626200.0"},{"timestamp":"1693368120.0","content":"Binary & PerserveHierarchy","upvote_count":"3","comment_id":"993687","poster":"kkk5566"},{"poster":"tonyfig","upvote_count":"4","content":"Binary & PerserveHierarchy\n\nThe Parquet option is used when you want to copy data stored in the Apache Parquet format and perform transformations on the data during the copy activity. However, in this scenario, the requirement is to perform no transformations and minimize the time required to perform the copy activity. The Binary option is better suited for this scenario as it copies the data as-is, without performing any transformations, and minimizes the time required to perform the copy activity.","timestamp":"1692502560.0","comment_id":"985539"},{"comment_id":"946792","content":"Answer seems correct as data is store is parquet already and requirement is to do no transformation so answer is right.\n\nSource dataset type: Parquet\nCopy activity copy behavior: Preserve Hierarchy","timestamp":"1688860140.0","poster":"rocky48","upvote_count":"4"},{"poster":"trantrongw","timestamp":"1680161520.0","content":"Agree. I've checked it.","upvote_count":"1","comment_id":"855374"},{"comment_id":"727695","timestamp":"1669481580.0","upvote_count":"1","poster":"Rrk07","content":"Answer is correct ."},{"poster":"temacc","upvote_count":"2","comment_id":"719157","timestamp":"1668549180.0","content":"Binary - copy files as is in fastest way.\nPreserveHierarchy - for saving folder structure."},{"poster":"OldSchool","upvote_count":"1","timestamp":"1668266640.0","content":"Answer is correct. No transformation and preserve hierarchy","comment_id":"716779"},{"timestamp":"1668246300.0","comment_id":"716616","content":"I believe the answer should be Binary, since it is stated that no transformations must be done.\n\n\"You can use Binary dataset in Copy activity, GetMetadata activity, or Delete activity. When using Binary dataset, the service does not parse file content but treat it as-is.\"\nhttps://learn.microsoft.com/en-us/azure/data-factory/format-binary\n\nI couldn't found any information saying that parquet won't be parsed if the source and sink are parquets files. So I -think- it will parse, and we can understand that it is a transformation.","upvote_count":"2","comments":[{"comment_id":"1140008","poster":"alphilla","upvote_count":"1","timestamp":"1707042900.0","content":"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset. Why do you pretend to preserver the hierarchy if the same hierarchy \"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset.\n\""}],"poster":"RBKasemodel"},{"comment_id":"682457","content":"Answer seems correct, \nadvice don't overthink, the source is parquet and it's one of the options so it is parquet.","upvote_count":"7","timestamp":"1664437800.0","poster":"allagowf"},{"content":"given ans is correct","upvote_count":"2","poster":"Deeksha1234","comment_id":"646419","timestamp":"1660410720.0"},{"comment_id":"631915","comments":[{"content":"The questtions states the requirement to choose the fastest option. Both would work, but binary is faster than parquet","comment_id":"922228","timestamp":"1686660120.0","poster":"mtc9","upvote_count":"1"}],"timestamp":"1657911660.0","upvote_count":"1","poster":"AKC11","content":"Binary would work. But since the question refers to the source file type as Parquet and that is one of the option provided, I would go with Parquet."},{"content":"Agree with answer","comment_id":"607902","upvote_count":"1","poster":"Rrk07","timestamp":"1653625920.0"},{"content":"Is it binary or parquet?","comment_id":"600041","timestamp":"1652264640.0","upvote_count":"5","poster":"AzureRan"},{"content":"I've just tested it in Azure, created two Gen2 storage accounts, used Binary as source and destination, placed two parquet files in account one. Created pipeline in ADF, added copy data activity and then defined first binary as source with wildcard path (*.parquet) and the sink as binary, with linked service for account 2, selected PreserveHierarchy. It worked.","comment_id":"567077","timestamp":"1647190020.0","upvote_count":"11","poster":"kamil_k"},{"poster":"AnshulSuryawanshi","comment_id":"557908","content":"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset","timestamp":"1646031300.0","upvote_count":"2"},{"timestamp":"1641714720.0","upvote_count":"2","poster":"Sandip4u","content":"this should be binary","comment_id":"520009"},{"comment_id":"512470","content":"The type property of the dataset must be set to Parquet\nhttps://docs.microsoft.com/en-us/azure/data-factory/format-parquet#parquet-as-source","upvote_count":"3","poster":"VeroDon","timestamp":"1640797440.0"},{"poster":"Mahesh_mm","upvote_count":"3","comment_id":"510036","timestamp":"1640582400.0","content":"I think it is Parquet as When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset."},{"content":"If you only copy over files from one storage to another, don't need to read data inside the file, binary should be selected for better performance.","comment_id":"502264","poster":"Canary_2021","timestamp":"1639580400.0","upvote_count":"7"},{"timestamp":"1639499160.0","comment_id":"501503","upvote_count":"7","poster":"m2shines","content":"Binary and Preserve Hierarchy should be the answer"},{"poster":"Lucky_me","upvote_count":"5","timestamp":"1639405860.0","comment_id":"500656","content":"The answers are correct! Binary doesn't work; I just tried.","comments":[{"poster":"kamil_k","upvote_count":"2","content":"hmm what did you try? I literally created it the same way as described i.e. two gen2 storage accounts. I chose gen2 as source linked service with binary as file type and the same for destination. In the copy data activity in ADF pipeline I specified preserve hierarchy and it worked as expected.","comment_id":"567496","timestamp":"1647244080.0"}]},{"upvote_count":"3","poster":"Ozzypoppe","content":"https://docs.microsoft.com/en-us/azure/data-factory/format-parquet#parquet-as-source","timestamp":"1638892200.0","comment_id":"496158"},{"timestamp":"1634059680.0","poster":"medsimus","comment_id":"461209","upvote_count":"10","content":"The correct answer is Binary , I test it"}],"answer_ET":"","unix_timestamp":1629998880,"answer_description":"Box 1: Parquet -\nFor Parquet datasets, the type property of the copy activity source must be set to ParquetSource.\n\nBox 2: PreserveHierarchy -\nPreserveHierarchy (default): Preserves the file hierarchy in the target folder. The relative path of the source file to the source folder is identical to the relative path of the target file to the target folder.\nIncorrect Answers:\n✑ FlattenHierarchy: All files from the source folder are in the first level of the target folder. The target files have autogenerated names.\n✑ MergeFiles: Merges all files from the source folder to one file. If the file name is specified, the merged file name is the specified name. Otherwise, it's an autogenerated file name.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/format-parquet https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage","answer":"","exam_id":67,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0004700001.jpg"],"question_text":"HOTSPOT -\nYou have two Azure Storage accounts named Storage1 and Storage2. Each account holds one container and has the hierarchical namespace enabled. The system has files that contain data stored in the Apache Parquet format.\nYou need to copy folders and files from Storage1 to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements:\n✑ No transformations must be performed.\n✑ The original folder structure must be retained.\n✑ Minimize time required to perform the copy activity.\nHow should you configure the copy activity? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2021-08-26 19:28:00","topic":"1","question_id":26,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0004600001.jpg"]},{"id":"KH4s49yTgz8PIKodmgp5","exam_id":67,"answer_description":"","answer_images":[],"answer_ET":"A","isMC":true,"answer":"A","unix_timestamp":1629756600,"question_text":"You have an Azure Data Lake Storage Gen2 container that contains 100 TB of data.\nYou need to ensure that the data in the container is available for read workloads in a secondary region if an outage occurs in the primary region. The solution must minimize costs.\nWhich type of data redundancy should you use?","choices":{"B":"read-access geo-redundant storage (RA-GRS)","C":"zone-redundant storage (ZRS)","D":"locally-redundant storage (LRS)","A":"geo-redundant storage (GRS)"},"topic":"1","timestamp":"2021-08-24 00:10:00","discussion":[{"comment_id":"430941","upvote_count":"127","comments":[{"timestamp":"1641621420.0","content":"A looks correct answer. RA-GRS is always avialable because its auto failover. Since this is not asked in the question but more importantly the question is about reducing cost which GRS.","upvote_count":"36","comment_id":"519335","poster":"dev2dev","comments":[{"poster":"kenmexam","comments":[{"poster":"dylan_t","timestamp":"1685436000.0","content":"You misunderstanding the question : GRS also give the possibilities to read. it's not specified that we need to read from the second region when the first is available\n+ You have to reduce the cost : GRS is cheaper than RA-GRS because GRS will be available only if the first region failover (in the subject we can read IF AN OUTAGE OCCURES) : https://azure.microsoft.com/en-us/pricing/details/storage/blobs/","upvote_count":"3","comment_id":"910077"}],"timestamp":"1667240400.0","upvote_count":"16","content":"The question clearly says \"is available for read workloads in a secondary region\". This is only available when choosing RA-GRS.* With GRS, when a disaster happens in the primary region, the user has to initiate a failover so that the secondary region becomes the primary region**. At no point you are reading from your secondary region with GRS. Hence i believe the answers should be B.\n*https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#geo-redundant-storage\n**https://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance","comment_id":"708650"},{"comments":[{"comment_id":"729474","upvote_count":"7","poster":"Billybob0604","timestamp":"1669657140.0","content":"Exactly. This is the point. It clearly states ' in case of an outage' RA-GRS --> secondary region can be read also not in a case of outage"},{"content":"its not A, dude, if you dont understand the difference between GRS and RA-GRS then u need az 101. With GRS, the 2nd region is NEVER available for access until Microsoft fails over the first failed region. Otherwise, you can NEVER access the 2nd regions data. Hence RA-GRS.","comment_id":"889902","timestamp":"1683276180.0","poster":"AnonymousJhb","upvote_count":"5","comments":[{"comment_id":"962522","poster":"semauni","timestamp":"1690274280.0","content":"No need to be rude. The question specifies that the data in the second region needs to be available IF an outage occurs. So GRS is more than enough. It's not because you think otherwise that you're right.","upvote_count":"10"}]}],"content":"It should be A because of two reasons:\n1. Minimize cost\n2. When primary is unavailable. \nHence No need for RA_GRS","upvote_count":"32","comment_id":"546571","poster":"BK10","timestamp":"1644769620.0"}]}],"content":"B is right\nGeo-redundant storage (with GRS or GZRS) replicates your data to another physical location in the secondary region to protect against regional outages. However, that data is available to be read only if the customer or Microsoft initiates a failover from the primary to secondary region. When you enable read access to the secondary region, your data is available to be read at all times, including in a situation where the primary region becomes unavailable.","timestamp":"1726815000.0","poster":"meetj"},{"poster":"Sasha_in_San_Francisco","comment_id":"472676","upvote_count":"79","comments":[{"upvote_count":"4","comment_id":"511771","timestamp":"1640751420.0","poster":"SabaJamal2010AtGmail","content":"It's not about common sense rather about technology. With GRS, data remains available even if an entire data center becomes unavailable or if there is a widespread regional failure. There would be a down time when a region becomes unavailable. Alternately, you could implement read-access geo-redundant storage (RA-GRS), which provides read-access to the data in alternate locations."}],"content":"In my opinion, I believe the and answer is A, and this is why. \n\nIn the question they state \"...available for read workloads in a secondary region IF AN OUTAGE OCCURES in the primary...\". Well, answer B (RA-GRS) states in Microsoft documentation that RA-GRS is for when \"...your data is available to be read AT ALL TIMES, including in a situation where the primary region becomes unavailable.\" \n\nTo me, the nature of the question is what is the cheapest solution which allows for failover to read workload, when there is an outage. Answer (A).\n\nCommon sense would be 'A' too because that is probably the most often real-life use case.","timestamp":"1726815000.0"},{"poster":"technoguy","comment_id":"1413315","upvote_count":"1","timestamp":"1743349860.0","content":"Selected Answer: A\nA is correct since cost is concern"},{"poster":"Jolyboy","content":"Selected Answer: B\nFor the keyword \"is available for read workloads in a secondary region\" B is the right choice","upvote_count":"1","timestamp":"1743083760.0","comment_id":"1410913"},{"poster":"ayn1","comment_id":"1401651","upvote_count":"1","content":"Selected Answer: B\n'Data must be available for read workloads in a secondary region', even is GRS is cheaper, no data read in secondary region unless manually initiated i.e. not useful for Read workload from secondary region.","timestamp":"1742579760.0"},{"upvote_count":"1","poster":"IMadnan","content":"Selected Answer: B\nread-access geo-zone-redundant storage (RA-GZRS)","timestamp":"1739383320.0","comment_id":"1355711"},{"content":"Selected Answer: B\nRead-Access Geo-Redundant Storage (RA-GRS): Data is replicated to a secondary region, and read access is enabled in the secondary region. This means that if the primary region fails, the data can still be read from the secondary region","comment_id":"1352693","poster":"jxs221","timestamp":"1738882140.0","upvote_count":"1"},{"poster":"JustImperius","content":"Selected Answer: A\nI was Team B up until the last sentence \"the solution must minimize costs\". To minimize costs and to ensure the data is available for read IF an outage occurs points to A. They never said anything about it being available at the same time. B goes against the minimizing of cost so A. Not the best questions in my opinion...leaves too much room for interpretation.","timestamp":"1737136560.0","comment_id":"1342275","upvote_count":"1"},{"comment_id":"1332263","timestamp":"1735282200.0","upvote_count":"1","poster":"Manash_chottu","content":"Selected Answer: B\nUnderstating the question very clearly that asking for an outage not failover, so I would say my answer to \"B\". \nHere is the reference to follow:\nLink: https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#read-"},{"timestamp":"1735280520.0","comment_id":"1332256","content":"Selected Answer: A\nQuestion is clearly mentioned that \"read workloads in a secondary region if an outage occurs in the primary region + optimize cost\" - where GRS solution is the best option compared to others.","poster":"Manash_chottu","upvote_count":"1"},{"comment_id":"1329607","timestamp":"1734719160.0","upvote_count":"1","poster":"s1852","content":"Selected Answer: B\nB is right\nGeo-redundant storage (with GRS or GZRS) replicates your data to another physical location in the secondary region to protect against regional outages. However, that data is available to be read only if the customer or Microsoft initiates a failover from the primary to secondary region. When you enable read access to the secondary region, your data is available to be read at all times, including in a situation where the primary region becomes unavailable."},{"upvote_count":"1","comment_id":"1323241","poster":"DP2032024","timestamp":"1733599260.0","content":"Selected Answer: B\nQuestion has asked about \"read workloads in a secondary region\" - Hence answer would be B"},{"timestamp":"1733498400.0","comment_id":"1322809","content":"Selected Answer: B\nPour assurer la disponibilité des données dans une région secondaire en cas de panne dans la région principale tout en minimisant les coûts, vous devriez utiliser le stockage géo-redondant à accès en lecture (RA-GRS) (Option B). Voici pourquoi :\n\nRA-GRS réplique vos données de manière asynchrone vers une région secondaire géographiquement éloignée, offrant ainsi une protection contre les pannes régionales1.\nEn cas de panne dans la région principale, RA-GRS permet un accès en lecture aux données répliquées dans la région secondaire, ce qui assure la continuité des charges de travail de lecture1.\nCette option équilibre bien la disponibilité et les coûts, car elle offre une haute disponibilité pour les lectures sans les coûts plus élevés associés à d'autres options de redondance.","upvote_count":"1","poster":"moize"},{"content":"Selected Answer: B\nRA-GRS offers the benefits of geo-redundant storage with additional read access to the secondary region. This means your data will be available for reading in a secondary region if the primary region goes down, ensuring high availability at a relatively low cost.","comment_id":"1321634","upvote_count":"1","poster":"shinypriti23","timestamp":"1733280360.0"},{"timestamp":"1732686240.0","comment_id":"1318445","upvote_count":"1","content":"Selected Answer: A\nCorrect Answer: A","poster":"EmnCours"},{"timestamp":"1727077920.0","upvote_count":"3","comment_id":"787758","content":"Selected Answer: A\nWhile it is true that the customer/Microsoft has to initiate the failover, this is not elaborated in any sense in the question. What is the point of GRS if you cannot read from it after a failover? It provides the service needed, at the lowest cost.\n\nThis would be different if there were keywords like \"available immediately without downtime\" or \"automatically\" but there are none, so well, if a region fails, you fail over, and read from secondary region. \n\nBottom line: A. GRS","poster":"Lestrang"},{"comment_id":"962526","timestamp":"1727077920.0","poster":"semauni","upvote_count":"3","content":"Selected Answer: A\nIn this scenario, the data in the secondary region only needs to be available IF the data isn't available in the primary region. Both GRS and RA-GRS accomplish that. The difference between GRS and RA-GRS is that the data in RA-GRS is always readable, even if the primary region is up, which also makes it more expensive. That is not necessary in this case, so GRS is the answer.\nSource: https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy"},{"comment_id":"967751","content":"The correct answer is:\nB. read-access geo-redundant storage (RA-GRS)\n\nWith RA-GRS, your data is not only replicated to a secondary region (geo-redundant storage, GRS) but also allows read access to the data in the secondary region. This means that if there is an outage in the primary region, you can access and read the data from the secondary region, providing business continuity and reducing downtime.\n\nWhile geo-redundant storage (GRS) on its own provides data redundancy across regions, it only allows read and write access in the primary region. To meet the requirement of having read access in the secondary region during an outage, RA-GRS is the appropriate option.\n\nZone-redundant storage (ZRS) and locally-redundant storage (LRS) do not provide the capability of data redundancy across regions, so they are not suitable for ensuring read access in a secondary region during a primary region outage.","timestamp":"1727077920.0","upvote_count":"1","poster":"Amitj2625"},{"poster":"Joanna0","upvote_count":"1","comment_id":"1117000","timestamp":"1727077860.0","content":"Selected Answer: B\nB. read-access geo-redundant storage (RA-GRS) Most \n\nWhen configured to use globally redundant storage (GRS, GZRS, and RA-GZRS), Azure copies your data asynchronously to a secondary geographic region located hundreds of miles away. This level of redundancy allows you to recover your data if there's an outage throughout the entire primary region.\n\nRead-access geo-redundant storage (RA-GRS) and read-access geo-zone-redundant storage (RA-GZRS) also provide geo-redundant storage, but offer the added benefit of read access to the secondary endpoint. These options are ideal for applications designed for high availability business-critical applications. If the primary endpoint experiences an outage, applications configured for read access to the secondary region can continue to operate. Microsoft recommends RA-GZRS for maximum availability and durability of your storage accounts.\n\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance"},{"timestamp":"1720528980.0","comment_id":"1244922","upvote_count":"2","poster":"Nanda_123456789","content":"A is the correct answer - question clearly says 'if an outage' - then A - GRS, if it had said Secondary region should be always available for read then RA-GRS"},{"timestamp":"1720356720.0","upvote_count":"1","comment_id":"1243853","content":"Selected Answer: B\nChatGPT 4o\nTo ensure that the data in your Azure Data Lake Storage Gen2 container is available for read workloads in a secondary region in case of an outage in the primary region, while also minimizing costs, you should use Read-Access Geo-Redundant Storage (RA-GRS).","poster":"e56bb91"},{"timestamp":"1713152940.0","upvote_count":"2","content":"Selected Answer: B\nwhile GRS focuses solely on disaster recovery, RA-GRS extends this by allowing read access to secondary data\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy","comment_id":"1195785","poster":"dgerok"},{"timestamp":"1712213160.0","content":"Selected Answer: B\nB. read-access geo-redundant storage (RA-GRS) \n\nIf you want to have read-access to the storage in another area, you must choose RA-GZRS since otherwise you'll have not the second storage URL. Impossible to choose the cheapest option in this case.","upvote_count":"3","poster":"MBRSDG","comment_id":"1189137"},{"upvote_count":"1","timestamp":"1711356780.0","comment_id":"1182335","poster":"jppdks","content":"Selected Answer: B\nIt is clearly B. That's Azure Infrastructure 1.01. With GRS you cannot Read only failover"},{"upvote_count":"1","content":"Selected Answer: A\nMinimize costs is a","comment_id":"1181282","timestamp":"1711246920.0","poster":"gplusplus"},{"poster":"gplusplus","content":"Selected Answer: B\nDoc clearly states that With an account configured for GRS or GZRS, data in the secondary region is not directly accessible to users or applications, unless a failover occurs. The failover process updates the DNS entry provided by Azure Storage so that the secondary endpoint becomes the new primary endpoint for your storage account. During the failover process, your data is inaccessible","upvote_count":"1","timestamp":"1711246800.0","comment_id":"1181281"},{"timestamp":"1710660240.0","content":"Selected Answer: B\nI'll go for B. Keep in mind with GRS the failover process might take about an hour and within that time you WON´T BE ABLE TO READ your data from the secondary region.","upvote_count":"1","poster":"lcss27","comment_id":"1175645"},{"comment_id":"1171226","upvote_count":"1","poster":"BlessedChild","content":"RA-GRS is an option on top of GRS. So, the regions defined for GRS are the same for RA-GRS","timestamp":"1710181740.0"},{"upvote_count":"1","content":"Selected Answer: B\nGeo-redundant storage (with GRS or GZRS) replicates your data to another physical location in the secondary region to protect against regional outages. With an account configured for GRS or GZRS, data in the secondary region is not directly accessible to users or applications, unless a failover occurs. The failover process updates the DNS entry provided by Azure Storage so that the secondary endpoint becomes the new primary endpoint for your storage account. During the failover process, your data is inaccessible","timestamp":"1707835200.0","comment_id":"1149230","poster":"[Removed]"},{"poster":"Khadija10","upvote_count":"1","content":"Selected Answer: B\nthe answer is RA-GRS.\nRead-access geo-redundant storage (RA-GRS) is more expensive than geo-redundant storage (GRS) because it is adding additional feature - the ability to read data from the secondary location.","timestamp":"1706648040.0","comment_id":"1136178"},{"content":"Selected Answer: B\nFor ensuring data availability in a secondary region for read workloads in case of an outage, you should use **read-access geo-redundant storage (RA-GRS)**. This provides geo-replication with the additional capability of read access to the data in the secondary region. It minimizes costs by allowing you to read from the secondary region but write only to the primary, thus optimizing for both redundancy and cost.","poster":"prshntdxt7","comment_id":"1127806","upvote_count":"1","timestamp":"1705841220.0"},{"comment_id":"1074063","comments":[{"timestamp":"1727077860.0","upvote_count":"2","comment_id":"1140947","poster":"saqib839","content":"absolutely right! If your application requires read access to the replicated data in the secondary region if the primary region becomes unavailable for some reason (geographic replication with read access)."}],"content":"Selected Answer: A\nA is cheaper than B.\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy","poster":"lucassn_","upvote_count":"2","timestamp":"1700317920.0"},{"timestamp":"1699011480.0","poster":"conscience","content":"Selected Answer: A\n2 points need to note:\n-> secondary regions need IF AN OUTAGE OCCURES in the primary region\n-> Cost minimizes\n\nRA-GRS cannot meet the second point hence correct answer is A","upvote_count":"3","comment_id":"1061393"},{"poster":"jiriz","content":"Selected Answer: B\nRA-GRS - It's clearly said there https://learn.microsoft.com/cs-cz/azure/storage/common/storage-redundancy","comment_id":"1028569","timestamp":"1696834920.0","upvote_count":"1"},{"comment_id":"1007876","content":"A) is correct because your priority is to minimize costs not to have instant read access to the second region, therefore GRS is the answer because if region1 fails, Microsoft will start a failover process to set the second region as the primary and you will get your data much cheaper (but slower) than RA-GRS","upvote_count":"1","timestamp":"1694718840.0","poster":"mav2000"},{"content":"Since you need to minimize cost, there is not need to get confused.\n\nGRS is cheaper than RA-GRS.\n\nYou can check the pricing here - https://azure.microsoft.com/en-in/pricing/details/storage/blobs/","timestamp":"1693969800.0","poster":"AvSUN","upvote_count":"1","comment_id":"1000126"},{"upvote_count":"1","timestamp":"1693368240.0","content":"Selected Answer: A\nis correct","comment_id":"993688","poster":"kkk5566"},{"comment_id":"962569","upvote_count":"2","content":"I would think it's A\nThe difference between GRS and RA GRS is fairly simple, GRS only allows to be read in the secondary zone in the even of a failover from the primary to secondary while RA GRS allows the option to read in the secondary whenever.","poster":"TechieBloke","timestamp":"1690278960.0"},{"timestamp":"1688714580.0","poster":"matiandal","content":"vote , for sure , A as the correct answer.\n\nnotes:\n--> Azure Storage offers two options for copying your data to a secondary region:\n GRS | GZRS\n\n--> With GRS or GZRS, the data in the secondary region isn't available for read or write access unless there's a failover to the primary region. \n\n>> Q >>\"ensure that the data in the container is available for read workloads in a secondary region IF an OUTAGE OCCURS in the primary region.\" -- aka --> the Q does not imply something about read available without a system failure \n-- so --> no need of a RA-GRS\n\nMS DOC: \nFor read access to the secondary region, configure your storage account to use read-access geo-redundant storage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS). \nR: https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy","upvote_count":"1","comment_id":"945431"},{"upvote_count":"1","timestamp":"1688307360.0","poster":"klayytech","content":"The answer is B. RA-GRS.\n\nThe question explicitly specifies that you need to ensure that the data in the container is available for read workloads in a secondary region. This means that you need to be able to read data from the secondary region even if the primary region is unavailable.\n\nRA-GRS (Read-Access Geo-Redundant Storage) is the only option that allows you to read data from the secondary region. With RA-GRS, your data is replicated to a secondary region in a different geographic location, and you can also read data from the secondary region.","comment_id":"940959"},{"upvote_count":"1","comment_id":"940957","timestamp":"1688307240.0","content":"Selected Answer: B\nHowever, the question specifically states that you need to ensure that the data in the container is available for read workloads in a secondary region. This means that you need to be able to read data from the secondary region even if the primary region is unavailable.","poster":"klayytech"},{"upvote_count":"1","content":"In this case, the need for read access to the secondary region outweighs the cost of RA-GRS. This is because the application needs to be able to continue reading data even if the primary region is unavailable. RA-GRS provides this functionality, while GRS does not.\n\nTherefore, the best option for this scenario is RA-GRS. This will ensure that the data is always available, while minimizing costs.","comment_id":"940954","timestamp":"1688306880.0","poster":"klayytech"},{"poster":"Ayman79","comment_id":"934860","upvote_count":"2","content":"Selected Answer: A\nRead-access geo-redundant storage (RA-GRS) is a more expensive option that allows you to read data from the secondary region without having to initiate a failover. However, RA-GRS does not provide any additional protection against regional outages.","timestamp":"1687821660.0"},{"timestamp":"1683060060.0","poster":"henryphchan","upvote_count":"2","comment_id":"887898","content":"Selected Answer: B\nBased on the scenario, you need to ensure that the data in the container is available for read workloads in a secondary region if an outage occurs in the primary region. The solution must minimize costs. Therefore, the best option is B, read-access geo-redundant storage (RA-GRS). This option provides the highest level of availability for your data, while also minimizing costs compared to premium performance accounts."},{"poster":"steveo123","content":"Selected Answer: B\nFrom the docs (on GRS): \" During the failover process, your data is inaccessible.\"","comment_id":"883212","upvote_count":"1","timestamp":"1682653680.0"},{"comment_id":"882889","timestamp":"1682616660.0","poster":"Mohamedali.Cintellic","content":"Selected Answer: B\nB is correct","upvote_count":"1"},{"timestamp":"1682473260.0","poster":"rocky48","comment_id":"881017","upvote_count":"1","content":"Selected Answer: A\nGeo-redundant storage"},{"upvote_count":"1","poster":"Honour","content":"Selected Answer: B\nRA-GRS is the right answer because all we need is just for the data to be read, and not replicated.","comment_id":"854412","timestamp":"1680093720.0"},{"content":"A is correct answer\nbecause it is cost effective and data is available if any failure (or) disaster takes place in the primary region","comment_id":"844740","upvote_count":"2","timestamp":"1679307780.0","poster":"Cheaary"},{"comment_id":"815027","timestamp":"1676883900.0","content":"Selected Answer: A\nIt says about minimizing cost.","comments":[{"poster":"dylan_t","comment_id":"910075","timestamp":"1685435760.0","content":"https://azure.microsoft.com/en-us/pricing/details/storage/blobs/\nGRS is cheaper than RA-GRS","upvote_count":"1"}],"poster":"Abhishek_C86","upvote_count":"1"},{"content":"A is right. The difference between GRS and RA GRS is fairly simple, GRS only allows to be read in the secondary zone in the even of a failover from the primary to secondary while RA GRS allows the option to read in the secondary whenever.","timestamp":"1675578060.0","upvote_count":"1","comment_id":"798641","poster":"SajidF"},{"poster":"akk_1289","upvote_count":"2","comment_id":"792365","timestamp":"1675051020.0","content":"Read-Access Geo-Redundant Storage (RA-GRS)"},{"comment_id":"787533","poster":"astone42","upvote_count":"1","content":"Correct answer is B. The question explicitly specifies \"You need to ensure that the data in the container is available for read workloads in a secondary region\"\n\nThe data is read from the secondary region only when using \"RA-GZRS\"","timestamp":"1674644760.0"},{"poster":"Vedjha","content":"As per price comparison, GRS is cost effective for the worklods which require read access on failover only.\n\nStorage Capacity LRS GRS RA-GRS ZRS GZRS RA-GZRS\nStorage in GB / month $0.045 per GB $0.06 per GB $0.075 per GB $0.0562 per GB $0.1012 per GB $0.1265 per GB","timestamp":"1674553680.0","comment_id":"786376","upvote_count":"1"},{"upvote_count":"1","comment_id":"779060","poster":"shakes103","timestamp":"1673969280.0","content":"Selected Answer: B\nFor read access to the secondary region, configure your storage account to use read-access geo-redundant storage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS).\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#geo-redundant-storage:~:text=For%20read%20access%20to%20the%20secondary%20region%2C%20configure%20your%20storage%20account%20to%20use%20read%2Daccess%20geo%2Dredundant%20storage%20(RA%2DGRS)%20or%20read%2Daccess%20geo%2Dzone%2Dredundant%20storage%20(RA%2DGZRS)."},{"timestamp":"1673656140.0","comment_id":"774973","poster":"nicky87654","content":"The final answer is that to ensure that the data in the Azure Data Lake Storage Gen2 container is available for read workloads in a secondary region if an outage occurs in the primary region, while minimizing costs, you should use Read-access geo-redundant storage (RA-GRS) .\nRA-GRS stores three copies of your data across two regions, and provides read-only access to the data in the secondary region, this way you can minimize costs by only paying for the read operations in the secondary region.","upvote_count":"1"},{"timestamp":"1672860420.0","comment_id":"766025","poster":"yogiazaad","upvote_count":"1","content":"As pet the question the data should be available for read workload in case if primary region outage. Both GRS and RA-GRS provide this capability. In terms of cost RA-GRS cost more compared to GRS. Since we also have to minimize cost GRS would be correct answer because it is cheaper .\nFor 100TB/month the cost for RA-GRS is $3,863 for US West under \"Azure Storage Reserved Capacity\" but for GSA it is $3,090. Below is the link for pricing.\n\nhttps://azure.microsoft.com/en-us/pricing/details/storage/blobs/"},{"comment_id":"759434","content":"Selected Answer: A\nAnswer is A GRS","timestamp":"1672208220.0","poster":"Vishalbarvaliya","upvote_count":"1"},{"upvote_count":"1","comment_id":"758836","poster":"akk_1289","timestamp":"1672161660.0","content":"B is the correct answer, \nTo ensure that the data in the Azure Data Lake Storage Gen2 container is available for read workloads in a secondary region in the event of an outage in the primary region, while minimizing costs, you should use read-access geo-redundant storage (RA-GRS).\n\nRA-GRS provides asynchronous replication of data to a secondary region. This means that the data in the primary region is constantly replicated to the secondary region, but there may be a delay in the data being available in the secondary region. RA-GRS is a cost-effective option for data that does not need to be immediately available in the secondary region, as it only incurs a small additional cost over locally-redundant storage (LRS).\nIt is important to note that RA-GRS does not provide write access to the data in the secondary region. If you need to ensure that data is available for both read and write workloads in the event of an outage, you may need to consider using a different type of data redundancy, such as geo-redundant storage (GRS) or zone-redundant storage (ZRS). These options provide synchronous replication of data to a secondary region, but they come at a higher cost than RA-GRS."},{"poster":"Rrk07","upvote_count":"1","comment_id":"727696","timestamp":"1669481700.0","content":"Read-access geo-redundant storage (RA-GRS) is correct answer."},{"timestamp":"1668267060.0","comment_id":"716784","upvote_count":"2","poster":"OldSchool","content":"Selected Answer: A\nRA-GRS is always ON if selected no matter regional outage. GRS activates only in case of regional outage and that was the question, also it's cheaper."},{"upvote_count":"2","poster":"lluvia1029","content":"Selected Answer: B\nWith GRS or GZRS, the data in the secondary region isn't available for read or write access unless there is a failover to the secondary region.\nFor read access to the secondary region, configure your storage account to use\n⮚Read access geo redundant storage (RA GRS)\n⮚Read access geo zone redundant storage (RA GZRS).","comment_id":"704553","timestamp":"1666777980.0"},{"timestamp":"1662106080.0","comment_id":"657162","content":"B is for sure wrong Answer .\n\nCorrect Answer is A.\n\nBecause question ask minimum cost.... which is A.\nFurthermore, RA-GRS is always readable in 2nd region even if you don't have any availability issue in primary region. So Correct answer is A.","upvote_count":"2","poster":"proserv"},{"comment_id":"651764","timestamp":"1661426220.0","upvote_count":"1","poster":"yyyhhh","content":"Selected Answer: B\nGRS does NOT initiate automatic failover. So B is correct."},{"poster":"kevinl1210","comment_id":"646976","content":"Selected Answer: A\nShould be A","upvote_count":"2","timestamp":"1660527840.0"},{"content":"Selected Answer: A\na is cheaper","comment_id":"644833","poster":"bagusttc","timestamp":"1660114500.0","upvote_count":"3"},{"timestamp":"1660025640.0","content":"also vote for A after reading the comments.","poster":"aurorafang","upvote_count":"1","comment_id":"644367"},{"comment_id":"643748","upvote_count":"1","poster":"kutty09","content":"read-access geo-redundant storage (RA-GRS) is the answer as in the question it's mentioned. Only read access to be available..","timestamp":"1659881700.0"},{"poster":"Jainh225","upvote_count":"5","comment_id":"636591","timestamp":"1658740500.0","content":"Selected Answer: A\nGRS vs Ra GRS\nThe difference between GRS and RA GRS is fairly simple, GRS only allows to be read in the secondary zone in the even of a failover from the primary to secondary while RA GRS allows the option to read in the secondary whenever."},{"comment_id":"636590","upvote_count":"1","content":"GRS vs Ra GRS\nThe difference between GRS and RA GRS is fairly simple, GRS only allows to be read in the secondary zone in the even of a failover from the primary to secondary while RA GRS allows the option to read in the secondary whenever.","poster":"Jainh225","timestamp":"1658740440.0"},{"content":"A is Correct for Minimize cost $ When primary is unavailable.","comment_id":"604005","timestamp":"1652973360.0","upvote_count":"1","poster":"prathamesh1996"},{"comment_id":"592454","content":"Selected Answer: A\nA because of costs aspect","timestamp":"1650978960.0","upvote_count":"4","poster":"Andushi"},{"upvote_count":"3","comment_id":"584782","content":"A is correct because of cost, RA-GRS will cost $5,910.73, GRS will cost 4,596.12","comments":[{"comments":[{"upvote_count":"2","poster":"Dicer","content":"Sorry. You are right. GRS is cheaper than RA-GRS. https://azure.microsoft.com/en-us/pricing/details/storage/blobs/","timestamp":"1655998920.0","comment_id":"621074"}],"poster":"Dicer","content":"If you go to open a Azure storage account and choose GRS/ZRS/LRS/RA-GRS, there will be a notice reminding you which options are cheaper. From cheapest to most expensive: LRS <ZRS<RA_GRS<GRS","upvote_count":"1","timestamp":"1655998740.0","comment_id":"621073"}],"timestamp":"1649772900.0","poster":"muove"},{"upvote_count":"1","poster":"Egocentric","timestamp":"1649671440.0","comment_id":"584138","content":"GRS is the correct answer,the key in the question is reducing costs"},{"poster":"Somesh512","comment_id":"582974","timestamp":"1649438700.0","content":"Selected Answer: A\nTo reduce cost GRS should be right option","upvote_count":"3"},{"content":"Selected Answer: A\nGRZ is cheaper","comment_id":"576961","poster":"KosteK","upvote_count":"3","timestamp":"1648483620.0"},{"timestamp":"1648229580.0","comment_id":"575211","content":"Selected Answer: B\nThe explanation is right. The given answer is wrong","upvote_count":"1","poster":"praticewizards"},{"upvote_count":"2","comment_id":"572232","content":"B is incorrect. The answer is A. GRS is cheaper than RA-GRS. GRS read access is available ONLY once primary region failover occurs (therefore lower cost). The requirement is for read-access availability in secondary region at lower cost WHEN a failover occurs in primary. Therefore, A is the answer","timestamp":"1647867840.0","poster":"DingDongSingSong"},{"timestamp":"1647613260.0","poster":"phdphd","upvote_count":"14","content":"Selected Answer: A\nGot this question on the exam. RA_GRS was not an option, so it should to be A.","comment_id":"570581"},{"timestamp":"1646968620.0","content":"A is right. GRS means secondary is available ONLY when primary is down. And it is cheaper than RA-GRS (where secondary read access is always available). The question sneaks in the word 'read workloads' just to confuse.","upvote_count":"3","poster":"vineet1234","comment_id":"565165"},{"content":"Selected Answer: B\nB is correct.\nGeo-redundant storage (with GRS or GZRS) replicates your data to another physical location in the secondary region to protect against regional outages. However, that data is available to be read only if the customer or Microsoft initiates a failover from the primary to secondary region. When you enable read access to the secondary region, your data is available to be read at all times, including in a situation where the primary region becomes unavailable. For read access to the secondary region, enable read-access geo-redundant storage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS).","timestamp":"1646474820.0","poster":"Sgarima","comment_id":"561351","upvote_count":"1"},{"comment_id":"555805","content":"A should be the answer as we need data in read only secondary only when something happens at region A, not always.","poster":"NamitSehgal","upvote_count":"2","timestamp":"1645768740.0"},{"poster":"MANESH_PAI","content":"Selected Answer: A\nIt is GRS because GRS is cheaper than RA-GRS\nhttps://azure.microsoft.com/en-gb/pricing/details/storage/blobs/","comment_id":"548750","timestamp":"1645027980.0","upvote_count":"5"},{"upvote_count":"2","comment_id":"533549","timestamp":"1643264280.0","content":"Selected Answer: B\ncorrect","poster":"PallaviPatel"},{"comment_id":"530323","upvote_count":"2","content":"Selected Answer: B\nWhile Geo-redundant storage (GRS) is cheaper than Read-Access Geo-Redundant Storage (RA-GRS), GRS does NOT initiate automatic failover.","poster":"Tinaaaaaaa","timestamp":"1642922400.0"},{"comment_id":"528333","content":"I think A is the right answer, if you see the question it’s has 2 requirements.\n1 . On failure of primary \n2. Cheap \n\nThe difference between GRS and RA GRS is fairly simple, GRS only allows to be read in the secondary zone in the even of a failover from the primary to secondary while RA GRS allows the option to read in the secondary whenever. It should be noted that your application and data will come with two endpoints with RA GRS requiring you to manage both endpoints.\nOverall LRS is the cheapest, followed by ZRS, then GRS and finally RA GRS","upvote_count":"6","poster":"Shatheesh","timestamp":"1642675260.0"},{"poster":"RuiCarvalhoDEV","comment_id":"526869","upvote_count":"1","timestamp":"1642527600.0","content":"Selected Answer: B\nB is right"},{"upvote_count":"2","poster":"ishika01","comment_id":"524960","content":"hope below note from microsoft will clear the confusion : \n\nWith GRS or GZRS, the data in the secondary region isn't available for read or write access unless there is a failover to the secondary region. For read access to the secondary region, configure your storage account to use read-access geo-redundant storage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS).","timestamp":"1642339200.0"},{"upvote_count":"2","poster":"tesen_tolga","content":"Selected Answer: A\nI think...","timestamp":"1642250160.0","comment_id":"524164"},{"upvote_count":"5","timestamp":"1641308220.0","content":"Selected Answer: A\nA in my opinion. Clearly states it should be cheaper and that you should be able to read from the secondary if there is a geographical issue (doesn't say this secondary region should be readable automatically).","poster":"Jaws1990","comment_id":"516746"},{"comment_id":"510040","content":"It is B","poster":"Mahesh_mm","timestamp":"1640582760.0","upvote_count":"1"},{"upvote_count":"3","comment_id":"500627","poster":"ItHYMeRIsh","content":"Selected Answer: B\nThe answer is B.\n\nI thought it was A as well. However, it appears by the link below that the primary obligation of a failover is on the customer. Microsoft only does the failover in an extreme case - likely due to anticipated data loss by the client and Microsoft wanting to reduce impact.\n\nSee this documentation:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance","timestamp":"1639402680.0"},{"upvote_count":"2","content":"Review the first sentence. That storage type uses files. Now review https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy#read-access-to-data-in-the-secondary-region.\nIt clearly states \"Note: Azure Files does not support read-access geo-redundant storage (RA-GRS) and read-access geo-zone-redundant storage (RA-GZRS).\"\nThat leaves A as the correct answer.","poster":"MyCat","comments":[{"comment_id":"502688","poster":"Yuri1101","upvote_count":"4","content":"Azure Files is a different service from Azure Data Lake Gen2. They are not the same.","timestamp":"1639632480.0"}],"timestamp":"1639395360.0","comment_id":"500562"},{"timestamp":"1638786120.0","poster":"rashjan","content":"Selected Answer: B\ni agree with the 'if'","comment_id":"495038","upvote_count":"2"},{"content":"The answer B is correct. The question says, \"If\" and not \"When\"....","timestamp":"1637614380.0","comment_id":"484551","poster":"paoloscott","upvote_count":"4"},{"poster":"Aslam208","upvote_count":"9","timestamp":"1635670920.0","comment_id":"470587","comments":[{"poster":"albertozgz","content":"yes, will be available to read IF the primary fail\n(read careful the IF, is the key)","upvote_count":"4","timestamp":"1636064160.0","comment_id":"472782"}],"content":"Question is tricky, it did not talk about auto faileover but cheaper solution.\nAnd GRS is cheaper than RA-GRS, therefore answer should be A"},{"timestamp":"1634821260.0","upvote_count":"4","comments":[{"comment_id":"467811","poster":"Avi_Bdj","content":"A is incorrect. GRS will not allow you to read if primary becomes unavailable. That needs to be achieved by RA-GRS. Microsoft docs also mention the same.\n\nCorrect answer is B.","comments":[{"upvote_count":"4","comment_id":"467812","poster":"Avi_Bdj","timestamp":"1635226020.0","content":"https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy"}],"timestamp":"1635226020.0","upvote_count":"2"}],"poster":"Vardhan_Brahmanapally","content":"A is Right \nGRS allows to read only when primary region is not available(asked in question)\nWhere as RA-GRS allows you to read even your primary region is active (acts as two endpoints)","comment_id":"465725"},{"content":"I will go for A , as Question doesn't say automatic , and also the cost factor is in consideration","upvote_count":"5","poster":"satyamkishoresingh","timestamp":"1634532120.0","comment_id":"463865"},{"content":"LRS GRS RA-GRS\n$0.045 per GB $0.06 per GB $0.075 per GB","poster":"rikku33","upvote_count":"3","comment_id":"453846","timestamp":"1632893760.0"},{"comment_id":"451712","poster":"rikku33","upvote_count":"5","content":"The difference between GRS and RA GRS is fairly simple, GRS only allows to be read in the secondary zone in the even of a failover from the primary to secondary while RA GRS allows the option to read in the secondary whenever. It should be noted that your application and data will come with two endpoints with RA GRS requiring you to manage both endpoints.\nAnswer should be A -> GRS","timestamp":"1632642120.0"},{"upvote_count":"4","poster":"rav009","content":"The question didn't ask auto failover switch, but mention cheaper.\nSo I choose A.","comment_id":"448235","timestamp":"1632143040.0"},{"content":"While Geo-redundant storage (GRS) is cheaper than Read-Access Geo-Redundant Storage (RA-GRS), GRS does NOT initiate automatic failover.","upvote_count":"2","poster":"kimalto452","timestamp":"1631708400.0","comment_id":"445195"},{"poster":"yrrrrrrrrrr","timestamp":"1631422200.0","upvote_count":"3","content":"GRS does NOT initiate automatic failover.","comment_id":"443278"}],"answers_community":["A (61%)","B (39%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/60427-exam-dp-203-topic-1-question-17-discussion/","question_images":[],"question_id":27},{"id":"x77YRDqf3nCDcj40yvSx","discussion":[{"comments":[{"content":"Yes, well said, that's the correct answer.","timestamp":"1647469020.0","comment_id":"569355","upvote_count":"2","poster":"Ozren"},{"comment_id":"546408","timestamp":"1644750360.0","upvote_count":"2","content":"Well explained!","poster":"Narasimhap"}],"timestamp":"1640790180.0","upvote_count":"112","comment_id":"512297","poster":"MadEgg","content":"Selected Answer: D\nFirst, about the Question:\nWhat fails? -> The (complete) DataCenter, not the region and not components inside a DataCenter.\n\nSo, what helps us in this situation?\nLRS: \"..copies your data synchronously three times within a single physical location in the primary region.\" Important is here the SINGLE PHYSICAL LOCATION (meaning inside the same Data Center. So in our scenario all copies wouldn't work anymore.)\n-> C is wrong.\nZRS: \"...copies your data synchronously across three Azure availability zones in the primary region\" (meaning, in different Data Centers. In our scenario this would meet the requirements)\n-> D is right\nGRS/GZRS: are like LRS/ZRS but with the Data Centers in different azure regions. This works too but is more expensive than ZRS. So ZRS is the right answer.\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy"},{"upvote_count":"80","comment_id":"429904","content":"This can't be correct. Should be D.","poster":"JohnMasipa","comments":[{"upvote_count":"1","comment_id":"430897","comments":[{"comment_id":"441393","upvote_count":"13","poster":"Vitality","comments":[{"content":"Also, note that the question talks about failure in \"a data center\". As long as other data centers are running fine(as in ZRS which will have many), ZRS would be the least expensive option.","poster":"azurearmy","timestamp":"1635101640.0","upvote_count":"6","comment_id":"467092"}],"timestamp":"1631098560.0","content":"It is cheaper but LRS helps to replicate data in the same data center while ZRS replicates data synchronously across three storage clusters in one region. So if one data center fails you should go for ZRS."}],"content":"Why, LRS is cheaper?","poster":"JayBird","timestamp":"1629822120.0"}],"timestamp":"1629717000.0"},{"upvote_count":"1","content":"Selected Answer: B\nGZRS combines the benefits of both geo-redundant storage and zone-redundant storage. It provides redundancy across both regions and availability zones, ensuring data availability even if a data center fails.","timestamp":"1738426140.0","poster":"Maor23","comment_id":"1349972"},{"upvote_count":"2","content":"Selected Answer: D\nCorrect Answer: D","timestamp":"1732686300.0","comment_id":"1318447","poster":"EmnCours"},{"comment_id":"462452","poster":"GameLift","upvote_count":"2","content":"D \nRedundancy in the primary region\nData in an Azure Storage account is always replicated three times in the primary region. Azure Storage offers two options for how your data is replicated in the primary region:\n\nLocally redundant storage (LRS) copies your data synchronously three times within a single physical location in the primary region. LRS is the least expensive replication option, but is not recommended for applications requiring high availability or durability.\nZone-redundant storage (ZRS) copies your data synchronously across three Azure availability zones in the primary region. For applications requiring high availability, Microsoft recommends using ZRS in the primary region, and also replicating to a secondary region.","timestamp":"1727078040.0"},{"comment_id":"505050","timestamp":"1727077980.0","content":"Selected Answer: D\nLRS is the lowest-cost redundancy option and offers the least durability compared to other options. LRS protects your data against server rack and drive failures. However, if a disaster such as fire or flooding occurs within the data center, all replicas of a storage account using LRS may be lost or unrecoverable. To mitigate this risk, Microsoft recommends using zone-redundant storage (ZRS), geo-redundant storage (GRS), or geo-zone-redundant storage (GZRS).\n\nSource: https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy#locally-redundant-storage","poster":"Daarora","upvote_count":"5"},{"upvote_count":"1","timestamp":"1726696500.0","comment_id":"1286012","content":"The Correct Answer is B - based on microsoft documentation\nZRS provides excellent performance, low latency, and resiliency for your data if it becomes temporarily unavailable. However, ZRS by itself might not protect your data against a regional disaster where multiple zones are permanently affected. For protection against regional disasters, we recommend using geo-zone-redundant storage (GZRS), which uses ZRS in the primary region and also geo-replicates your data to a secondary region.\n\nhttps://learn.microsoft.com/en-us/azure/storage/files/files-redundancy#zone-redundant-storage","poster":"hardenlv"},{"comment_id":"1171225","poster":"BlessedChild","upvote_count":"1","content":"RA-GRS is an option on top of GRS. So, the regions defined for GRS are the same for RA-GRS","timestamp":"1710181620.0"},{"comment_id":"1102675","timestamp":"1703172660.0","poster":"lisa710","content":"zone-redundant storage (ZRS)","upvote_count":"1"},{"comment_id":"1071000","upvote_count":"2","poster":"SillyChili","content":"I don't understand why the answer is ZRS. ZRS redundancy is across availability zones, not other region. The question mentioned that \"if data center fails in the primary Azure region\". Isn't it ZRS will not be available when primary Azure region fails? Correct answer should then be GRS then, which redundancy is across region, and is lower cost compare to GZRS.","timestamp":"1700012100.0"},{"upvote_count":"1","timestamp":"1694496420.0","poster":"74gjd_37","comment_id":"1005422","content":"Selected Answer: D\nAccording to Microsoft \"Locally redundant storage (LRS) replicates your storage account three times within a single data center in the primary region.\" Therefore, if a the center fails, all three copies will be unavailable. However, according to the condition, the data lake should remain available if a data center fails. Azure Data Lake Gen 2 storage is based on blob storage. There is no separate data redundancy options for the Azure Data Lake Gen 2 comparing to that of blob storage within a storage account. Thereofer, option C is incorrect."},{"content":"Selected Answer: D\nis correct","comment_id":"993691","timestamp":"1693368600.0","upvote_count":"1","poster":"kkk5566"},{"upvote_count":"2","content":"LRS/ZRS doesnt come to picture if anything needs to available in other regions. so GRS is rite one.","timestamp":"1687839720.0","comment_id":"935012","poster":"Mani_V"},{"upvote_count":"2","poster":"deutscher","comment_id":"865370","content":"I understood it this way, \n1. LRS : single 3-storey building in Frankfurt > Each floor has a data center > if the data center fails then everything is lost\n\n2. LRS: Single 3-storey building in Frankfurt and Berlin, if data in Frankfurt center is lost, then we still have in Berlin\n\nHence it's even cheaper because they are in the same Geolocation","timestamp":"1681030080.0"},{"comment_id":"793375","comments":[{"upvote_count":"1","content":"sorry ignore above... i reread the question \"one of the data centres in the primary region\".","comment_id":"798491","poster":"anoj_cha","timestamp":"1675554660.0"}],"poster":"anoj_cha","timestamp":"1675114320.0","upvote_count":"1","content":"Has the question recently changed? Most of the conversation below is talking about zone failures (Availability Zone) whereas the question is talking about a \"primary Azure region\" (region). In case of a region failure, would a GRS not be required as the ZRS will protect one if one of the availability zones go down (and not the entire region)?"},{"upvote_count":"1","poster":"nicky87654","content":"that the data lake remains available if a data center fails in the primary Azure region, while minimizing costs, you should use geo-redundant storage (GRS) for the storage account. GRS stores 3 copies of the data across 2 regions, so that if a data center fails in the primary region, the data can still be accessed from the secondary region and you only pay for the primary region's storage cost.","comment_id":"774979","timestamp":"1673656320.0"},{"timestamp":"1665313380.0","upvote_count":"1","content":"Selected Answer: D\nMicrosoft recommends using ZRS in the primary region for Azure Data Lake Storage Gen2 workloads.","poster":"greenlever","comment_id":"690085"},{"timestamp":"1660411080.0","poster":"Deeksha1234","content":"Selected Answer: D\nD is correct","upvote_count":"1","comment_id":"646421"},{"upvote_count":"1","comment_id":"607905","poster":"Rrk07","timestamp":"1653626040.0","content":"D is correct as it talks about \"a data center\" means we can not use the LRS (LOCAL )"},{"content":"Selected Answer: D\nD -> Data is replicated synchronously","timestamp":"1652093160.0","comment_id":"598996","poster":"olavrab8","upvote_count":"1"},{"upvote_count":"2","poster":"Egocentric","timestamp":"1650148320.0","comment_id":"586982","content":"D is correct"},{"timestamp":"1649458440.0","poster":"ravi2931","comments":[{"upvote_count":"2","poster":"ravi2931","comment_id":"583075","content":"see this explained clearly - \nLRS is the lowest-cost redundancy option and offers the least durability compared to other options. LRS protects your data against server rack and drive failures. However, if a disaster such as fire or flooding occurs within the data center, all replicas of a storage account using LRS may be lost or unrecoverable. To mitigate this risk, Microsoft recommends using zone-redundant storage (ZRS), geo-redundant storage (GRS), or geo-zone-redundant storage (GZRS)","timestamp":"1649458620.0"}],"content":"it should be D","comment_id":"583074","upvote_count":"1"},{"upvote_count":"1","comment_id":"581184","poster":"ASG1205","content":"Selected Answer: D\nAnswer should be D, as LRS won't be helpfull in case of whole datacenter failure.","timestamp":"1649156220.0"},{"comment_id":"574117","poster":"Andy91","upvote_count":"2","timestamp":"1648106940.0","content":"Selected Answer: D\nThis is the correct answer indeed"},{"comment_id":"556615","upvote_count":"1","content":"Selected Answer: C\nAnswer is LRS.\n\nFrom microsoft docs:\n\nLRS replicates data in a single AZ. An AZ can contain one or more data centers. So, even if one data center fails, data can be accessed through other data centers in the same AZ.\n\nhttps://docs.microsoft.com/en-us/azure/availability-zones/az-overview#availability-zones\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy#redundancy-in-the-primary-region","poster":"bhanuprasad9331","timestamp":"1645869120.0"},{"timestamp":"1643264340.0","content":"Selected Answer: D\nD is correct.","upvote_count":"3","poster":"PallaviPatel","comment_id":"533550"},{"timestamp":"1643032860.0","upvote_count":"2","poster":"vimalnits","content":"Correct answer is D.","comment_id":"531345"},{"content":"LRS helps to replicate data in the same data center while ZRS replicates data synchronously across three storage clusters in one region","poster":"Tinaaaaaaa","upvote_count":"1","timestamp":"1642922580.0","comment_id":"530327"},{"upvote_count":"1","timestamp":"1642676040.0","comment_id":"528339","content":"D is the correct answer, In question it’s clearly mentioned if data center fails it should be available, LRS stores everything in sane data center so it’s not the correct answer, next cheapest option is ZRS.","poster":"Shatheesh"},{"upvote_count":"3","poster":"Jaws1990","comment_id":"516749","content":"Selected Answer: D\nMentions data centre (Availability Zone) failure, not rack failure, so should be Zone Redundant Storage.","timestamp":"1641308460.0"},{"poster":"DrTaz","upvote_count":"2","content":"Selected Answer: D\nnote that the \"data centre fails\"","timestamp":"1641136620.0","comment_id":"515046"},{"content":"After reading all the comments ill go with LRS. it doesn't mention a disaster. \"LRS protects your data against server rack and drive failures\" https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy.","upvote_count":"1","comment_id":"512503","timestamp":"1640799420.0","poster":"VeroDon"},{"poster":"ArunMonika","upvote_count":"1","content":"I will go with D","comment_id":"510202","timestamp":"1640598660.0"},{"content":"Answer is D","comment_id":"510062","timestamp":"1640586300.0","poster":"Mahesh_mm","upvote_count":"1"},{"timestamp":"1639431660.0","content":"I'd go with D. LRS does NOT meet the requirement to maintain availability in the event of a data center failure.","poster":"KenTsung","comment_id":"500920","upvote_count":"2"},{"upvote_count":"1","comment_id":"499400","timestamp":"1639226340.0","poster":"jv2120","content":"There are 2 catch in questions, Cheap and failure of a data center. Though Geo options will be best but not cheap. Then we come down to LRS that will only copy data in same data center. So we have D(ZRS) as correct Answer."},{"upvote_count":"4","content":"It says the solution MUST minimize cost. \n\nLocally redundant storage (LRS)—synchronously replicates data to three disks within a data center in the primary region. Offers a moderate level of availability at a LOWER cost.\n\nZone-redundant storage (ZRS)—synchronously replicates data amongthree Azure availability zones in the primary region. Provides a higher level of resilience at HIGHER cost.\n\nWith that in mind, I will go with C.","comment_id":"497415","timestamp":"1639035780.0","poster":"A_BOGALHO"},{"comment_id":"487170","poster":"vmisirlis","timestamp":"1637914500.0","content":"Selected Answer: D\nLocally redundant storage (LRS) replicates your data three times within a single data center in the primary region. We need to have our files in an other datacenter and the cheapest is ZRS","upvote_count":"5"},{"upvote_count":"1","poster":"paoloscott","content":"The selected answer is correct.","timestamp":"1637614260.0","comment_id":"484550"},{"poster":"FredNo","content":"Selected Answer: D\nAnswer is D. The primary zone has an outtage so ZRS is plenty to fulfill the need","timestamp":"1637173740.0","upvote_count":"4","comment_id":"480202"},{"timestamp":"1636731480.0","content":"I will go for option D","comment_id":"477027","poster":"paoloscott","upvote_count":"1"},{"timestamp":"1635498360.0","upvote_count":"2","content":"In the Microsoft certified practice tests by Measure Up, the answer was given as LRS with same explanation. However I am confused.","poster":"berserksap","comment_id":"469674"},{"comment_id":"469193","poster":"kilowd","timestamp":"1635418560.0","upvote_count":"1","content":"C is wrong ...Locally redundant storage (LRS) replicates your data three times within a single data center in the primary region. LRS provides at least 99.999999999% (11 nines) durability of objects over a given year."},{"content":"Should be D. Confusing question","timestamp":"1634539380.0","comment_id":"463949","poster":"itacshish","upvote_count":"1"},{"content":"It should be D : Zone because, zone will have 3 copy in different data center in same region,in case of data center failure , other data center will be available","timestamp":"1634367420.0","upvote_count":"2","poster":"HaliBrickclay","comment_id":"462954"},{"poster":"estrelle2008","content":"Although question doesn't state disaster recovery, it does state Primary region, so I may assume there is a region pair and thus a Secondary region. I would go for geo-redundant then? Why mention the region in the question otherwise? Confusing question.","upvote_count":"3","timestamp":"1633611600.0","comment_id":"458750"},{"comment_id":"458698","poster":"mark9999","content":"Question states that the Datacentre fails, that's going to take out all racks with LRS data. You would need at least ZRS to ensure data in another datacentre in the same region.","upvote_count":"1","timestamp":"1633605540.0"},{"upvote_count":"2","timestamp":"1633436880.0","comment_id":"457695","poster":"Fahd92","content":"LRS protects your data against server rack and drive failures. However, if a disaster such as fire or flooding occurs within the data center, all replicas of a storage account using LRS may be lost or unrecoverable.\n\nIn the question the mentionned if a datacenter fail and not a disaster so LRS would be ok, i say C."},{"timestamp":"1632912180.0","poster":"Blueko","content":"Answer C (Locally-redundant storage) is correct if you consider that the question asks \" if the datacenter fails\".\n \nA single zone has more independent datacenters, Microsoft:\"Each zone is made up of one or more datacenters equipped with independent power, cooling, and networking.\"\nhttps://docs.microsoft.com/en-us/azure/availability-zones/az-overview","upvote_count":"1","comment_id":"454040"},{"comment_id":"445292","poster":"tanssive","upvote_count":"1","timestamp":"1631719200.0","content":"LRS is the lowest-cost redundancy option and offers the least durability compared to other options. LRS protects your data against server rack and drive failures. However, if a disaster such as fire or flooding occurs within the data center, all replicas of a storage account using LRS may be lost or unrecoverable. To mitigate this risk, Microsoft recommends using zone-redundant storage (ZRS), geo-redundant storage (GRS), or geo-zone-redundant storage (GZRS)."},{"poster":"GameLift","upvote_count":"2","content":"Shouldn't the correct answer be: A ? Geo-Zone Redundancy?","comment_id":"442428","timestamp":"1631270520.0"},{"upvote_count":"4","poster":"Amyqwertyu","content":"Zone-redundant storage (ZRS) copies your data synchronously across three Azure availability zones in the primary region. So A is the answer","comment_id":"440855","timestamp":"1631010240.0"},{"timestamp":"1630304460.0","comment_id":"435150","upvote_count":"2","poster":"gemealex","content":"it should be D"},{"timestamp":"1630239900.0","comment_id":"434575","upvote_count":"3","poster":"laszek","content":"Correct anwer should be D. LRS copies data 3 times within one Datacenter. ZRS copies data in separate availability zones in one region"},{"poster":"hsetin","upvote_count":"14","comment_id":"432587","timestamp":"1630015560.0","comments":[{"comment_id":"464480","content":"NO..Zone-redundant storage (ZRS) replicates your Azure Storage data synchronously across three Azure availability zones in the primary region. Each availability zone is a separate physical location with independent power, cooling, and networking","timestamp":"1634624820.0","upvote_count":"4","poster":"kilowd"}],"content":"Answer should be A Geo-redundant as the requirement is for primary region failure. (Zone- Redundant means multiple copies in the same zone of a region)."},{"timestamp":"1629985620.0","comment_id":"432376","content":"It's D!","upvote_count":"3","poster":"GJ123"},{"comment_id":"431317","timestamp":"1629882960.0","upvote_count":"6","content":"ZRS: if we need to ensure that the data lake will remain available if a data center fails in the primary region, we need to select ZRS not LRS because LRS provides 1 datancenter","poster":"Amalbenrebai"}],"answer_ET":"D","answer_description":"","question_text":"You plan to implement an Azure Data Lake Gen 2 storage account.\nYou need to ensure that the data lake will remain available if a data center fails in the primary Azure region. The solution must minimize costs.\nWhich type of replication should you use for the storage account?","topic":"1","choices":{"D":"zone-redundant storage (ZRS)","A":"geo-redundant storage (GRS)","B":"geo-zone-redundant storage (GZRS)","C":"locally-redundant storage (LRS)"},"question_images":[],"answer":"D","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/60376-exam-dp-203-topic-1-question-18-discussion/","unix_timestamp":1629717000,"exam_id":67,"timestamp":"2021-08-23 13:10:00","answer_images":[],"answers_community":["D (99%)","1%"],"question_id":28},{"id":"11Jnw17Bj6a6iKtu475I","exam_id":67,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0005100001.jpg"],"topic":"1","question_id":29,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0005000001.jpg"],"answer_ET":"","answer":"","answer_description":"Box 1: Hash -\nHash-distributed tables improve query performance on large fact tables. They can have very large numbers of rows and still achieve high performance.\nIncorrect Answers:\nRound-robin tables are useful for improving loading speed.\n\nBox 2: Clustered columnstore -\nWhen creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed.\n\nBox 3: Date -\nTable partitions enable you to divide your data into smaller groups of data. In most cases, table partitions are created on a date column.\nPartition switching can be used to quickly remove or replace a section of a table.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","timestamp":"2021-08-23 12:26:00","discussion":[{"timestamp":"1629749220.0","comment_id":"430315","comments":[{"poster":"gssd4scoder","content":"Agree 100%.\nAll in paragraphs under this: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview.","comments":[{"content":"Also agree 100%","timestamp":"1641136740.0","comment_id":"515047","upvote_count":"4","poster":"DrTaz"}],"upvote_count":"10","timestamp":"1635133500.0","comment_id":"467228"},{"timestamp":"1724071020.0","comment_id":"1268658","content":"The ET engineer who answered probably thought we have to select the options which are outrageously foolish","upvote_count":"7","poster":"roopansh.gupta2"},{"timestamp":"1644750660.0","content":"Round- Robin\nHeap\nNone.\nNo brainer for this question.","poster":"Narasimhap","upvote_count":"28","comment_id":"546411"},{"upvote_count":"2","comment_id":"1173422","timestamp":"1710421620.0","content":"Agree - Round Robin, Heap and None are the correct options. The solution doesn't make any sense for a staging table to truncate every day without any JOINs.Don't know why it says Hash/Columnstored/Date.","poster":"Homer23"},{"poster":"azure900test","content":"Not sure if the table is already to larg for heap?\n\"Do not use a heap when there are no nonclustered indexes and the table is large, unless you intend to return the entire table content without any specified order. In a heap, all rows of the heap must be read to find any row.\"\nhttps://docs.microsoft.com/en-us/sql/relational-databases/indexes/heaps-tables-without-clustered-indexes?view=sql-server-ver16","comment_id":"625059","timestamp":"1656569400.0","upvote_count":"1"},{"content":"I agree too","timestamp":"1641772740.0","comment_id":"520538","poster":"anto69","upvote_count":"7"},{"poster":"Deepshikha1228","comment_id":"635964","timestamp":"1658657400.0","content":"I agree, Round Robin, Heap and None is the correct option","upvote_count":"8"}],"content":"Round-Robin\nHeap\nNone","upvote_count":"426","poster":"A1000"},{"upvote_count":"68","comments":[{"content":"Had doubts regarding why there is no need for a partition. While what you suggested is true won't it be better if there is a date partition to truncate the table ?","comments":[{"poster":"andy_g","content":"There is no filter on a truncate statement so no benefit in having a partition","comment_id":"545378","timestamp":"1644590460.0","upvote_count":"4"}],"comment_id":"469675","upvote_count":"2","timestamp":"1635498900.0","poster":"berserksap"},{"comments":[{"comments":[{"timestamp":"1645583820.0","comment_id":"554143","poster":"SQLDev0000","content":"DrTaz is right, in addition, when you populate an indexed table, you are also writing to the index, so this adds an additional overhead in the write process","upvote_count":"3"}],"timestamp":"1641136860.0","content":"The term heap basically refers to a table without a clustered index. Adding a clustered index to a temp table makes absolutely no sense and is a waste of compute resources for a table that would be entirely truncated daily. \n\nno clustered index = heap.","comment_id":"515049","upvote_count":"13","poster":"DrTaz"}],"timestamp":"1635858900.0","content":"Can you explain me why should we use heap?","comment_id":"471649","upvote_count":"1","poster":"Vardhan_Brahmanapally"}],"content":"Round-robin - this is the simplest distribution model, not great for querying but fast to process\nHeap - no brainer when creating staging tables\nNo partitions - this is a staging table, why add effort to partition, when truncated daily?","timestamp":"1630240260.0","comment_id":"434578","poster":"laszek"},{"poster":"HaliBrickclay","content":"as per Microsoft document \n\nLoad to a staging table\nTo achieve the fastest loading speed for moving data into a data warehouse table, load data into a staging table. Define the staging table as a heap and use round-robin for the distribution option.\n\nConsider that loading is usually a two-step process in which you first load to a staging table and then insert the data into a production data warehouse table. If the production table uses a hash distribution, the total time to load and insert might be faster if you define the staging table with the hash distribution. Loading to the staging table takes longer, but the second step of inserting the rows to the production table does not incur data movement across the distributions.","comment_id":"462963","upvote_count":"5","comments":[{"comment_id":"512514","timestamp":"1640800080.0","poster":"VeroDon","content":"It doesn't mention the prd table. Only the staging. So, round Robin/Heap is the answer, correct? tricky questions. \n:)","upvote_count":"2"}],"timestamp":"1727078100.0"},{"content":"Answer: Round-Robin (1), Heap (2), None (3).\nWithin this doc:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview\n#1. Search for “Use round-robin for the staging table.”\n#2. Search for: “A heap table can be especially useful for loading data, such as a staging table,…”\nWithin this doc:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition?context=/azure/synapse-analytics/context/context\n#3. Partitioning by date is useful when stage destination has data because you can hide the inserting data’s new partition (to keep users from hitting it), complete the load and then unhide the new partition.\nHowever, in this question it states, “the table will be truncated before each daily load”, so, it appears it’s a true Staging table and there are no users with access, no existing data, and I see no reason to have a Date partition. To me, such a partition would do nothing but slow the load.","timestamp":"1727078100.0","upvote_count":"17","comment_id":"472688","poster":"Sasha_in_San_Francisco"},{"poster":"jhargett1","timestamp":"1727078100.0","content":"Since it's the staging table, the main focus should be minimizing the load time, as noted in the question. Heap table does not have any index and it's the fastest option for loading large amounts of data. Using a round-robin distribution will help to evenly distribute the data across all the distributions, further reducing the load time. As the data is truncated before each load, partitioning is not necessary, so it is best to choose None.\nRound-robin\nHeap\nNone","comment_id":"792145","upvote_count":"1"},{"poster":"akk_1289","content":"Distribution: Round Robin\nIndexing: Clustered Columnstore\nPartitioning: Date\n\nThe recommended configuration for a staging table that will be loaded daily with approximately 1 million rows of data and truncated before each load is to use a round robin distribution, a clustered columnstore index, and date-based partitioning. Round robin distribution will evenly distribute the data across nodes, reducing the load time. Clustered columnstore index provides efficient compression and supports fast bulk load operations. Date-based partitioning will allow for easy archiving and maintenance of the table.","timestamp":"1727078100.0","upvote_count":"2","comment_id":"792359"},{"upvote_count":"6","comment_id":"1005427","content":"Round-Robing, Heap, None.\n\nThe question is to configure the staging table.\n\nAccording to the conditions, \"The solution must minimize how long it takes to load the data to the staging table.\"\n\nTherefore, loading time is the most essential condition here.\n\nAccording to Microsoft documentation at\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview\n\n\nDistribution: A round-robin table distributes table rows evenly across all distributions. The rows are distributed randomly. Loading data into a round-robin table is fast. But, queries can require more data movement than the other distribution methods.\n\nIndexing: A heap table can be especially useful for loading transient data, such as a staging table, which is transformed into a final table.\n\nPartitioning: None. Since the table is truncated before each daily load, we can not benefit of partitioning to drop date ranges.","poster":"74gjd_37","timestamp":"1727078100.0"},{"upvote_count":"3","poster":"MBRSDG","comment_id":"1189133","content":"1 --> REPLICATED \nSince table has always 1mln rows (it is a small staging table, loaded in truncate-insert everyday!)\nAlso Round-Robin is wrong, since 1mln/60nodes = almost 17k rows per compute node, which is a unrecommended situation. You haven't data enough to choose round-robin. \n\n2 --> HEAP\nWhy do you have to choose clustered columnstore index? This is not a big table. \n\n3 --> None\nSince you haven't data enough to create a efficient partition. It's more convenient to have only one partition in this case.","timestamp":"1727078100.0"},{"timestamp":"1721926740.0","content":"Copilot\nSent by Copilot:\nTo optimize the loading process for your staging table in Azure Synapse, here are the recommended configurations:\n\nDistribution: Use Round-robin distribution. This method evenly distributes the data across all distributions, which helps in achieving balanced data loading and minimizes data skew.\nIndexing: Use Heap. Since the table will be truncated before each load, using a heap (a table without a clustered index) will speed up the data loading process as it avoids the overhead of maintaining indexes during the load.\nPartitioning: Do not partition the staging table. Partitioning can add overhead to the data loading process. Since the table is truncated daily, partitioning is not necessary and can be avoided to keep the load process efficient.\nThese configurations will help minimize the time it takes to load data into the staging table.","upvote_count":"2","poster":"iceberge","comment_id":"1255103"},{"content":"RR\nHeap\nDate","upvote_count":"1","timestamp":"1717430340.0","poster":"RakshithaReddy","comment_id":"1223672"},{"upvote_count":"2","poster":"Dusica","timestamp":"1714461240.0","comment_id":"1204383","content":"Round robin Heap None"},{"poster":"Alongi","content":"RR\nHeap\nNone","comment_id":"1193904","timestamp":"1712849940.0","upvote_count":"1"},{"poster":"__Tom","upvote_count":"5","content":"Round-Robin\nHeap\nNone","comment_id":"1072452","timestamp":"1700142540.0"},{"content":"Round-Robin Heap None","upvote_count":"1","comment_id":"993694","poster":"kkk5566","timestamp":"1693368780.0"},{"timestamp":"1692858360.0","upvote_count":"1","content":"Round-Robin\nHeap\nNone","comment_id":"988886","poster":"kkk5566"},{"poster":"janaki","comment_id":"911496","content":"Never ever use date partioning with hash distribution.\nThe correct answer is: Round robin, Heap and None","upvote_count":"2","timestamp":"1685558160.0"},{"timestamp":"1682473620.0","poster":"rocky48","upvote_count":"1","content":"Round-robin - this is the simplest distribution model, not great for querying but fast to process\nHeap - no brainer when creating staging tables\nNo partitions - this is a staging table","comment_id":"881023"},{"upvote_count":"3","comment_id":"795414","timestamp":"1675276920.0","content":"Round-Robin\nHeap\nNone","poster":"SHENOOOO"},{"upvote_count":"2","timestamp":"1674336840.0","comment_id":"783742","content":"Round-robin - the default once. As for any staging table the Round Robin should be selected. if you don't select during table design this will be considered by default. \nHeap - this is a staging table\nNo partitions - this is a staging table","poster":"DindaS"},{"upvote_count":"4","timestamp":"1674224400.0","content":"Answer is \n1.) Round-Robin\n2.) Heap\n3.) None","poster":"JosephVishal","comment_id":"782335"},{"timestamp":"1673415840.0","upvote_count":"1","content":"requirement is to optimize LOAD not QUERY performanc\nRound-Robin;Heap;None","poster":"Dusica","comment_id":"772075"},{"comments":[{"upvote_count":"1","timestamp":"1673191680.0","poster":"ToddW","content":"The 1 million row will be loaded daily, but nothing is said that these records are for one day. In addition, the table is truncated before each load so it is pointless.","comment_id":"769582"}],"timestamp":"1670230080.0","poster":"rj02","content":"Round robin and heap sounds good but why not date partition as question states daily load to staging","comment_id":"735777","upvote_count":"2"},{"upvote_count":"2","timestamp":"1669481880.0","comment_id":"727698","content":"As it only about stage table so it should be\n Round-Robin\nHeap\nNone","poster":"Rrk07"},{"poster":"rohitbinnani","content":"#1 Round-Robin - \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute.\n#2 Heap - A heap table can be especially useful for loading transient data, such as a staging table, which is transformed into a final table.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview\n#3 None - When you do full truncate and load daily, there is no point in partitioning.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition","comment_id":"722759","upvote_count":"4","timestamp":"1668961020.0"},{"comment_id":"704558","poster":"lluvia1029","timestamp":"1666778520.0","upvote_count":"2","content":"Round-Robin \nHeap\nNone \nStaging table don’t need partition and better to set distribution as round-robin as it improve load speed. Also Heap is fast and no need to create index"},{"content":"Not sure why the wrong answer is given for this simple question. Any clue how the answer will be given?","upvote_count":"1","timestamp":"1666006800.0","poster":"Ram9298","comment_id":"697368"},{"timestamp":"1665569460.0","poster":"PreetDP900","content":"Correct answer is Round-Robin, Heap, None. Valid reason to choose these answers as the requirement is creating a staging table.\nWhy they are showing the incorrect answer.","comment_id":"692938","upvote_count":"2"},{"content":"But, why does not anybody changes the correct answer???? (Round-Robin, Heap, None)","poster":"bakstorage00001","comment_id":"673262","timestamp":"1663589580.0","upvote_count":"4"},{"comment_id":"646422","timestamp":"1660411320.0","content":"Round-Robin\nHeap\nNone","upvote_count":"4","poster":"Deeksha1234"},{"comment_id":"645128","poster":"PugazhManohar","timestamp":"1660159260.0","upvote_count":"1","content":"Question clearly says solution for staging \"You need to create the staging table. The solution must minimize how long it takes to load the data to the staging table.\" Hence the ans is Round - Robin, Heap and None"},{"comment_id":"634768","timestamp":"1658430060.0","upvote_count":"1","poster":"AKC11","content":"As the question is related to the load performance - Round Robin distribution would be ideal. \nHeap index is recommended for tables with truncate and load. \nPartition is None. Because it will have an indirect impact on load. MOreover there is no mention of the date/distinct date values etc."},{"timestamp":"1656478680.0","upvote_count":"2","content":"Staging Table best Practices:-\nRound Robin, Heap , None","poster":"NamitSehgal","comment_id":"624427"},{"comment_id":"621805","poster":"Dicer","timestamp":"1656093420.0","upvote_count":"1","content":"Round-Robin\n\n1. Hash-distribution improves query performance on large fact tables, and is the focus of this article. Round-robin distribution is useful for improving loading speed."},{"poster":"Rrk07","comment_id":"607906","timestamp":"1653626160.0","upvote_count":"1","content":"It should be Round-Robin\nHeap\nNone"},{"comment_id":"598844","upvote_count":"1","content":"Round-Robin\nHeap\nNone","timestamp":"1652073300.0","poster":"SandipSingha"},{"poster":"Sandip4u","upvote_count":"2","timestamp":"1641715080.0","content":"Round-robin,heap,none","comment_id":"520011"},{"poster":"Mahesh_mm","content":"Round-Robin\nHeap\nNone","timestamp":"1640610780.0","comment_id":"510313","upvote_count":"2"},{"comment_id":"510205","upvote_count":"1","content":"Answer: Round-Robin (1), Heap (2), None (3).","timestamp":"1640598900.0","poster":"ArunMonika"},{"comment_id":"501611","content":"Round-robin, Heap and None","timestamp":"1639507440.0","poster":"m2shines","upvote_count":"1"},{"poster":"Sasha_in_San_Francisco","content":"Answer: Round-Robin (1), Heap (2), None (3).","timestamp":"1636044300.0","upvote_count":"1","comment_id":"472687"},{"timestamp":"1635672540.0","poster":"Aslam208","content":"Round-Robin, Heap, Noe.\nA polite request to the moderator, please verify these answers and correct. For some people, wrong answers will be detrimental.","comment_id":"470594","upvote_count":"8"},{"comment_id":"465741","poster":"Vardhan_Brahmanapally","upvote_count":"7","comments":[{"timestamp":"1651237920.0","comment_id":"594480","poster":"dJeePe","content":"Did MS hack this site to make it give wrong answers ? ;-)","upvote_count":"3"}],"content":"Many of the answers provided in this website are incorrect","timestamp":"1634827800.0"},{"upvote_count":"2","poster":"itacshish","comment_id":"463953","timestamp":"1634539560.0","content":"Round-Robin\nHeap\nNone"},{"poster":"estrelle2008","timestamp":"1633624200.0","upvote_count":"2","content":"Please correct the answers ExamTopics, as Microsoft itself recently published best practices on data loading in Synapse, and describes staging as 100% FAB answers is correct instead of ADF. https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices","comment_id":"458845"},{"comment_id":"456427","content":"Round-Robin\nHeap\nNone","poster":"RinkiiiiiV","timestamp":"1633240260.0","upvote_count":"1"},{"upvote_count":"2","poster":"hugoborda","content":"Round-Robin\nHeap\nNone","comment_id":"452764","timestamp":"1632777600.0"},{"upvote_count":"1","poster":"hsetin","comment_id":"441147","content":"Why heap and not CCI?","timestamp":"1631060340.0"},{"poster":"viper16752","content":"Answers should be:\nDistribution - Round Robin (See https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute)\nIndexing - Heap (See https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index)\nPartitioning - (It's a staging table, no sense in partitioning here)","upvote_count":"6","timestamp":"1629738180.0","comment_id":"430193"},{"upvote_count":"1","poster":"Gopinath601","timestamp":"1629732420.0","comment_id":"430128","content":"I feel that answer is \nDistribution = Hash\nIndexing = Heap\nPartitioning = Date\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index"},{"upvote_count":"2","content":"I think answer should be\n1. Round Robin\n2. Clustered Columnstore\n3. None\n\nIs partitioning allowed in round robin distribution? Please someone confirm and accordingly modify the answer if needed.","timestamp":"1629721680.0","comment_id":"429949","poster":"Nilay95","comments":[{"upvote_count":"5","comment_id":"434763","content":"When you are temporarily landing data in a dedicated SQL pool, you may find that using a heap table makes the overall process faster. This is because loads to heaps are faster than to index tables and in some cases, the subsequent read can be done from the cache. If you are loading data only to stage it before running more transformations, loading the table to heap table is much faster than loading the data to a clustered columnstore table.","poster":"echerish","timestamp":"1630257360.0"}]},{"poster":"Blueko","content":"Request: \"The solution must minimize how long it takes to load the data to the staging table\" \n\nThe distribution should be Round-Robin, not Hash, as in the answer's motivations: \"Round-robin tables are useful for improving loading speed\"","upvote_count":"9","comment_id":"429877","timestamp":"1629714360.0"}],"url":"https://www.examtopics.com/discussions/microsoft/view/60369-exam-dp-203-topic-1-question-19-discussion/","unix_timestamp":1629714360,"answers_community":[],"isMC":false,"question_text":"HOTSPOT -\nYou have a SQL pool in Azure Synapse.\nYou plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load.\nYou need to create the staging table. The solution must minimize how long it takes to load the data to the staging table.\nHow should you configure the table? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//"},{"id":"WmJBhKVnINs9BxHDYycR","choices":{"B":"an error","C":"a null value","A":"24"},"discussion":[{"comments":[{"content":"i test too and confirm that the right answer is A","upvote_count":"4","timestamp":"1709311440.0","comment_id":"1163626","poster":"cecbc1f"},{"timestamp":"1668176460.0","content":"Did you tried the same query that is presented here? with \"mytestdb.dbo.myParquetTable\"??","poster":"maximilianogarcia6","upvote_count":"4","comment_id":"716118","comments":[{"comment_id":"793301","content":"The table and Column names are case insensitive.","upvote_count":"3","timestamp":"1675110780.0","poster":"yogiazaad"},{"content":"I tried with all upper case, and it still return record for name Alice.\nAnswer is A","timestamp":"1675998660.0","comment_id":"803925","upvote_count":"6","poster":"Virul"}]}],"content":"I did a test, waited for one minute and tried the query in a serverless sql pool and received 24 as the result, so I don't understand that B has been voted so much because the answer is A) 24 without a doubt","upvote_count":"63","poster":"gerrie1979","comment_id":"707212","timestamp":"1667050020.0"},{"comments":[{"poster":"psicktrick","content":"But if you look at the docs, that's exactly what has been done\n https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#expose-a-spark-table-in-sql:~:text=mytestdb.myparquettable%22)%3B-,Now%20you%20can%20read%20the%20data%20from%20your%20serverless%20SQL%20pool%20as%20follows%3A,-SQL","timestamp":"1672976640.0","comment_id":"767246","comments":[{"poster":"Bedmed","comment_id":"1274280","content":"NO The Create table is in Azure Synapse Analytics Spark pool and select is in the serverless","upvote_count":"4","timestamp":"1724887080.0"},{"upvote_count":"2","poster":"ck8.kakade","content":"Yes, so the right answer if A. It will return an output without any errors","comment_id":"1257050","timestamp":"1722197340.0"},{"content":"Thanks @psicktrick for the link","poster":"goldy29","comment_id":"943900","timestamp":"1688571720.0","upvote_count":"4"},{"upvote_count":"4","content":"Unless you use Either USING CVS or USING PARQUET While you create the table in the Spark. You will get error. If you use the below code and then query in SQL pool it works with out error. \nDROP TABLE IF EXISTS mytestdb.myParquetTable;\nCREATE TABLE IF NOT EXISTS mytestdb.myParquetTable(\nEmployeeID int,\nEmployeeName string,\nEmployeeStartDate date) USING PARQUET;\nINSERT INTO mytestdb.myParquetTable VALUES (24, 'Alice',DATE'2023-01-01');\nSELECT * FROM mytestdb.myParquetTable","comment_id":"793298","poster":"yogiazaad","timestamp":"1675110660.0"}],"upvote_count":"20"},{"content":"i think B option is the correct too","comment_id":"1075991","timestamp":"1700536200.0","upvote_count":"1","poster":"devnginx"},{"comment_id":"1045588","upvote_count":"6","poster":"Shaik_Shahul","content":"i think you don't about sql server bro, Dbo means database object so it is not a issue for this the correct answer is A","comments":[{"comment_id":"1190020","content":"Dbo means database owner actually bro","poster":"__Tom","upvote_count":"3","timestamp":"1712340060.0"}],"timestamp":"1697522460.0"},{"timestamp":"1702808280.0","content":"kindly clarify, which can be the right option? the conversations are confusing. :( any explanations are appreciated. thank you!!","poster":"SenMia","upvote_count":"2","comment_id":"1098792"},{"poster":"tlb_20","comment_id":"930577","content":"I just tried to run the commands, and that error you had is due to the fact that you queried through Spark pool (!!), I did that as a test and got the exact same error. To query the data using Spark Pool, you don't use the \".dbo\" reference, this only works if you're using a Synapse Serverless Pool.\nSo the correct answer is A!","timestamp":"1687440300.0","upvote_count":"6","comments":[{"upvote_count":"2","timestamp":"1688648280.0","comment_id":"944669","comments":[{"timestamp":"1694970120.0","upvote_count":"5","poster":"darkraijin","comment_id":"1009969","content":"24 . Because you are using SQL serverless pool to query the data. As the last line."}],"poster":"bblord","content":"is the answer A or B?"}]},{"upvote_count":"17","content":"Actually the error it is because of the lower case.\n\"Table names will be converted to lower case and need to be queried using the lower case\".\n\nThere is nothing wrong with \"dbo\", since the docs shows this exact same example and contains the \"dbo\" in the query.\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table","poster":"vinicius_cb","comment_id":"809493","timestamp":"1676464260.0"}],"timestamp":"1666197000.0","poster":"dmitriypo","comment_id":"699190","upvote_count":"56","content":"Answer is B, but not because of the lowercase. The case has nothing to do with the error.\nIf you look attentively, you will notice that we create table mytestdb.myParquetTable, but the select statement contains the reference to table mytestdb.dbo.myParquetTable (!!! - dbo).\nHere is the error message I got:\nError: spark_catalog requires a single-part namespace, but got [mytestdb, dbo]."},{"timestamp":"1743011820.0","comment_id":"1410500","content":"Selected Answer: A\nA is correct \ndbo is default schema for a table.","upvote_count":"1","poster":"technoguy"},{"timestamp":"1741841460.0","comment_id":"1388182","upvote_count":"1","content":"Selected Answer: A\nThe clue is in the name apache cluster database. The database is mytestdb . Basically you can create a table with <db>.<tablename> . it will default to the dbo schema","poster":"AMJB"},{"upvote_count":"1","timestamp":"1741772280.0","content":"Selected Answer: B\nSince there's no configuration in the given scenario to link the Spark-managed Parquet table to the serverless SQL pool, an error will occur when attempting to execute the SQL query.","poster":"Lethahavm","comment_id":"1387782"},{"timestamp":"1738845360.0","content":"Selected Answer: A\nI think A is the right answer. Firstly, dbo is added automatically (see examples in documentation). Furthermore, even though the created table in the spark pool will be saved with the name in lower case (as is stated in the documentation), SQL serverless pool in Synapse is case insensitive per default wehn it comes to table names and column names.","comment_id":"1352373","poster":"Ciske92","upvote_count":"1"},{"comment_id":"1343070","upvote_count":"1","content":"Selected Answer: A\nI believe A is correct because the selection query occurs after the insertion operation.","timestamp":"1737301320.0","poster":"Rayenwalid"},{"comment_id":"1340798","upvote_count":"1","timestamp":"1736934120.0","content":"Selected Answer: B\ndbo should not be there.","poster":"Romanx"},{"poster":"Asheesh1909","content":"Selected Answer: A\noption A is correct \ntested it , there is no issue with the case , since the table and db names are saved in the lowercase , what ever case we use in the query azure / sql engine convert that into lower case before performing the query . \n\n2. issue with .dbo \nthe table is created in the spark pool , the table is store in the hive metadata . so without using the .dbo (schema layer ) the serverless sql pool , cannot access the table and gives error . since we are using mytestdb.dbo.myparquetTable --> the serverless poll serches for the table in both its schema layer and hive schema layer giving the output 24 .\nAlso , once a table is created in spark pool , it can be accessed in serverless sql Pool without any issues; there is no need for an external table to be created .","upvote_count":"1","comment_id":"1332663","timestamp":"1735339080.0"},{"upvote_count":"1","poster":"abhi_11kr1","comments":[{"comment_id":"1335841","upvote_count":"2","timestamp":"1735866900.0","poster":"hypersam","content":"you should at least test it out before posting wrong answers"}],"timestamp":"1733137080.0","comment_id":"1320901","content":"Selected Answer: B\nThe query will fail because the serverless SQL pool cannot directly access Spark-managed tables without additional configuration. Use an external table or expose the data via a view to make it accessible."},{"poster":"19a3424","comment_id":"1318053","content":"Selected Answer: A\nThe example here shows \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#create-an-external-table-in-spark-and-query-from-serverless-sql-pool","upvote_count":"1","timestamp":"1732623720.0"},{"comment_id":"1317899","poster":"EmnCours","timestamp":"1732594920.0","upvote_count":"1","content":"Selected Answer: A\nSelected Answer: A"},{"content":"Option A:\ndbo is the default schema where the object gets created in synapse if not specified explicitly.","timestamp":"1732165140.0","poster":"Anithec0der","comment_id":"1315651","upvote_count":"1"},{"poster":"BrilliantBeast","content":"Does the order of insertion doesn't matter here? As the column order is different in the table than the one used in insertion.","upvote_count":"1","comment_id":"1312169","timestamp":"1731602460.0"},{"timestamp":"1727501160.0","upvote_count":"1","comment_id":"1290515","content":"24 is correct answer\nLink : https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#create-a-managed-table-in-spark-and-query-from-serverless-sql-pool","poster":"examdemo"},{"content":"As per ChatGPT, The query will fail with an error message as the table myParquetTable is created using Spark and the USING Parquet option, which means it is stored in a Parquet file format. Serverless SQL pool does not support querying Parquet files directly, so it cannot query myParquetTable in its current form.\n\nTo make the table accessible from the serverless SQL pool, you need to create an external table that references the Parquet file. Then, you can query the external table instead.\n\nAssuming you have created an external table named myExternalParquetTable that references the Parquet file containing the data in myParquetTable, the query to select EmployeeID where EmployeeName is 'Alice' would be:\n\nSELECT EmployeeID\nFROM myExternalParquetTable\nWHERE EmployeeName = 'Alice';","timestamp":"1727072820.0","comment_id":"830555","upvote_count":"6","poster":"esaade"},{"upvote_count":"3","content":"Selected Answer: B\nThe query will fail with an error because mytestdb.myParquetTable is a Spark table, not a SQL table.\n\nWhen you created the table using Spark, you used the Spark SQL syntax, and the table is stored in the Spark engine's metadata. Serverless SQL pool in Azure Synapse Analytics cannot directly query Spark tables; it can only query SQL tables.\n\nIf you want to query the data stored in the mytestdb.myParquetTable table using a serverless SQL pool, you need to create an external table that maps to the same Parquet file. You can do this by using the CREATE EXTERNAL TABLE statement in a SQL pool.","comments":[{"upvote_count":"2","content":"or as \"Managed table\". The answer seems to be very similar to the section \"Create a managed table in Spark and query from serverless SQL pool\" in the link below:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#expose-a-spark-table-in-sql","poster":"AlejandroU","timestamp":"1696933920.0","comment_id":"1039373"}],"comment_id":"891113","timestamp":"1727072820.0","poster":"milad2021"},{"content":"Right answer is B.\nThe query must run into Serverless SQL poll, not into Apache spark.\n\n\"One minute later, you execute the following query from a serverless SQL pool in MyWorkspace.\"\n\nIf we run that query into apache spark pool, using notebook for example, we must use \"SELECT database.table\".\n\nSo, according the question, we must use serverless sql pool and, cause of that, we have to use \"SELECT database.dbo.table\" OR \"use database; select table\"","poster":"Katiane","comment_id":"1043746","timestamp":"1727072820.0","upvote_count":"1"},{"poster":"jhargett1","timestamp":"1727072820.0","content":"The given query is trying to select the EmployeeID from the Parquet table myParquetTable in the mytestdb database, where the EmployeeName is 'Alice'. However, it seems like there's a minor typo in the query. The query should be like this:\n\nSELECT EmployeeID\nFROM mytestdb.myParquetTable\nWHERE EmployeeName = 'Alice';\n\n\nThe given query is trying to select the EmployeeID from the Parquet table myParquetTable in the mytestdb database, where the EmployeeName is 'Alice'. However, it seems like there's a minor typo in the query. The query should be like this:\n\nsql\nCopy code\nSELECT EmployeeID\nFROM mytestdb.myParquetTable\nWHERE EmployeeName = 'Alice';\nThe corrected query would return:\n\nA. 24\n\nSo, the correct answer is A. The query will return the EmployeeID, which is 24.","upvote_count":"3","comment_id":"1055841"},{"content":"The question is bad, the Spark database table created VIA Spark pool is not automatically generated in the SQL pool, the user should create one external table via serverless SQL pool first, the schema is not required in the external table. So whether schema dbo is needed in the 2nd query is depending on if dbo is included when creating external table.","poster":"8ac3742","upvote_count":"1","comment_id":"1282655","timestamp":"1726147860.0"},{"comment_id":"1279105","content":"The question is to query from Serverless to Spark.\nTo query directly in Spark, must not use \"dbo\".\nTo query from Serverless, \"dbo\" is required.\nI tested this myself just now.\nCorrect: A.","poster":"renan_ineu","upvote_count":"1","timestamp":"1725560160.0"},{"upvote_count":"2","comment_id":"1276294","poster":"a85becd","timestamp":"1725242700.0","content":"Selected Answer: A\nAnswer A is correct based on logic once you create a table in Spark DB it will automatically Create it under db.table for serverless so query returns the result"},{"poster":"NAWRESS96","upvote_count":"3","timestamp":"1724050440.0","content":"Selected Answer: A\nthe right answer is A","comment_id":"1268447"},{"timestamp":"1723152120.0","content":"Answer is A, because after Create a table in Spark pool the same one will be automatically created under dbo for Serverless SQL Pool. so the query returns 24","upvote_count":"2","poster":"a85becd","comment_id":"1262668"},{"poster":"207680a","timestamp":"1722193500.0","comment_id":"1257013","content":"Answer A ,\n\nYou have inserted one row and the ID is 24 , The select query is correct so B is wrong and there is no Null value in the inserted records","upvote_count":"1"},{"timestamp":"1722188700.0","content":"Selected Answer: A\nLearning, unsure about answer first but after checking document, can confirm it's A.\nQuestion is nearly same example as of doc.","poster":"UdayKaudgaonkar","upvote_count":"3","comment_id":"1256950"},{"timestamp":"1721280120.0","comment_id":"1250146","upvote_count":"1","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#examples","poster":"rajasuman"},{"upvote_count":"1","comment_id":"1249982","timestamp":"1721249160.0","poster":"juanluisacebal","content":"Selected Answer: B\ni think"},{"content":"Selected Answer: A\nCorrect answer is A. Clearly explained in the link https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#create-a-managed-table-in-spark-and-query-from-serverless-sql-pool","comment_id":"1239952","upvote_count":"3","timestamp":"1719804660.0","poster":"learnwell"},{"timestamp":"1719418620.0","poster":"Nadine_nm","comment_id":"1237570","upvote_count":"3","content":"There is exactly the same example in MS docs, and the think you need to focus on, is that u are executing the query from a serverless SQL pool, so the query SELECT * FROM mytestdb.dbo.myparquettable WHERE name = 'Alice'; is correct"},{"timestamp":"1718514060.0","comment_id":"1231207","poster":"shivanishans","upvote_count":"2","content":"I believe the correct answer is option A as we can use a different schema which in this case is .dbo to access a database. Whether it be .db or .dbo it wouldn't make a difference(unless there is a specific configurational change in the environment and connection settings which hinders this process)"},{"content":"Selected Answer: B\nI believe it is B.","upvote_count":"1","timestamp":"1718482320.0","comment_id":"1231116","poster":"ageorgieva"},{"comment_id":"1223440","upvote_count":"3","timestamp":"1717393740.0","content":"Selected Answer: A\nthe same example here https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#create-a-managed-table-in-spark-and-query-from-serverless-sql-pool","poster":"fahfouhi94"},{"comment_id":"1219615","upvote_count":"2","timestamp":"1716820560.0","poster":"slamcity","content":"Selected Answer: B\nerror becouse of reference to dbo in select"},{"timestamp":"1715304420.0","comment_id":"1209145","content":"Answer is A， check this out https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#expose-a-spark-table-in-sql","poster":"datanerdChen","upvote_count":"2"},{"poster":"Dusica","comment_id":"1204371","content":"dbo is the default schema so table will be created in it. Answer is A","upvote_count":"3","timestamp":"1714458060.0"},{"timestamp":"1714207080.0","comment_id":"1203020","poster":"Alongi","content":"Selected Answer: A\nA is correct, is explained there:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#expose-a-spark-table-in-sql","upvote_count":"4"},{"content":"Selected Answer: B\nEmployeeName string ????\nShould be EmployeeName VARCHAR or NVARCHAR\nI think, the answer should be B","timestamp":"1713149640.0","comment_id":"1195775","upvote_count":"1","poster":"dgerok"},{"upvote_count":"1","content":"there is no STRING format... VARCHAR or NVARCHAR should be used instead, while you create a table... So, the answer is B","comment_id":"1195774","poster":"dgerok","timestamp":"1713149580.0"},{"poster":"Charley92","upvote_count":"1","timestamp":"1713019440.0","comment_id":"1195013","content":"Selected Answer: B\nSELECT * FROM mytestdb.dbo.myparquettable WHERE name = 'Alice';"},{"upvote_count":"1","content":"Selected Answer: A\nCorrect","comment_id":"1193885","poster":"Alongi","timestamp":"1712848020.0"},{"comment_id":"1190393","upvote_count":"1","poster":"bibabrian","content":"bad question. But it should be A people. See the example here:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#create-a-managed-table-in-spark-and-query-from-serverless-sql-pool","timestamp":"1712407440.0"},{"upvote_count":"2","content":"I just implemented the same and answer is 24 so Answer is A. You can access Spark Table using SparkDB.DBO.SparkTable format.","poster":"ankeshpatel2112","comment_id":"1185903","timestamp":"1711779480.0"},{"poster":"gplusplus","timestamp":"1711243140.0","comment_id":"1181254","upvote_count":"2","content":"Selected Answer: A\nIts ok to use dbo as the default catalog"},{"comment_id":"1178162","timestamp":"1710930600.0","poster":"mo2325","upvote_count":"2","content":"the dash(-) after employeeid is quite misleading"},{"poster":"rocky48","upvote_count":"2","content":"Assuming the table is indeed created in the mytestdb database without specifying a schema, the correct reference in the serverless SQL query would be:\n\nsql\nCopy code\nSELECT EmployeeID\nFROM mytestdb.myParquetTable\nWHERE EmployeeName = 'Alice';\n\nNow, if the table is in the dbo schema, then the correct query would be:\n\nsql\nCopy code\nSELECT EmployeeID\nFROM mytestdb.dbo.myParquetTable\nWHERE EmployeeName = 'Alice';\n\nB is the answer if the question is correct and not a type with dbo","comment_id":"1139022","timestamp":"1706942280.0"},{"timestamp":"1706074380.0","upvote_count":"2","poster":"ChrisGe1234","comment_id":"1130253","content":"Selected Answer: A\nThe first query is executed in Spark Pool and the 2nd in Serverless. Since spark doesn't have any schema, in serverless everything is in the dbo schema.\nSo A) 24 is correct"},{"upvote_count":"3","poster":"sdg2844","content":"Selected Answer: B\nThere are multiple issues here. In the select line, there is a dash after EmployeeID. I don't see any case issues with this, but still have to think it would error out with that dash... although I'll bet that's a typo.\n\nB is the safest guess here.","timestamp":"1704235080.0","comment_id":"1112329"},{"comment_id":"1101452","timestamp":"1703069700.0","poster":"lisa710","upvote_count":"1","content":"the answer is B.The query filters on name = 'Alice', but the actual column name in the table is EmployeeName (case-sensitive)."},{"poster":"dakku987","comment_id":"1099510","content":"Selected Answer: B\nyes getting error bcz of dbo so ans is b","upvote_count":"1","timestamp":"1702886280.0"},{"poster":"ChrisGe1234","timestamp":"1701873360.0","upvote_count":"1","content":"Selected Answer: A\ndbo is the default schema for replicated dbs","comment_id":"1089441"},{"poster":"Abdulwahab1983","content":"Once a database has been created by a Spark job, you can create tables in it with Spark that use Parquet, Delta, or CSV as the storage format. Table names will be converted to lower case and need to be queried using the lower case name. These tables will immediately become available for querying by any of the Azure Synapse workspace Spark pools. They can also be used from any of the Spark jobs subject to permissions.","comment_id":"1073473","upvote_count":"1","timestamp":"1700238420.0"},{"poster":"EduardPaul","comments":[{"upvote_count":"1","poster":"ELJORDAN23","timestamp":"1705316400.0","content":"Thanks for the source!","comment_id":"1123262"},{"content":"I used also this exaple in MS and got the same anser(A. 24)","comment_id":"1114449","timestamp":"1704453720.0","poster":"fadaei","upvote_count":"2"}],"timestamp":"1700223420.0","content":"A is correct, see here (same sample from MS):\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#create-a-managed-table-in-spark-and-query-from-serverless-sql-pool","upvote_count":"7","comment_id":"1073297"},{"content":"Answer is A guys if we ignore the probably type \"-\". https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table There is an exact example from microsoft","timestamp":"1698148020.0","upvote_count":"5","comment_id":"1052786","poster":"alphilla"},{"poster":"Shaik_Shahul","upvote_count":"3","timestamp":"1697522340.0","content":"No it is not a error, the correct answer is A=24","comment_id":"1045586"},{"upvote_count":"1","content":"It seems A, if it is created as a \"managed table\" and ignoring the probable typing error [-] in the select statement. A similar example of creating a \"managed table\" is in the section \"Create a managed table in Spark and query from serverless SQL pool\" in the link below:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#expose-a-spark-table-in-sql","poster":"AlejandroU","timestamp":"1696934580.0","comment_id":"1039383"},{"upvote_count":"1","timestamp":"1696506900.0","poster":"tpositive","content":"A is the answer","comment_id":"1025604"},{"comment_id":"1005339","timestamp":"1694491020.0","content":"Selected Answer: B\nFirst, the name \"MyWorkspace\" is invalid; it cannot contain uppercase character; it could only have been \"myworkspace\"","upvote_count":"1","poster":"74gjd_37"},{"poster":"MBRSDG","content":"did anyone notice the dash before the col name? It returns a syntax error, hence B is the correct answer.","timestamp":"1694420880.0","upvote_count":"3","comment_id":"1004562"},{"comment_id":"982460","content":"I believe it needs to wait for a minute\nA is correct 👍","upvote_count":"2","timestamp":"1692183960.0","poster":"kkk5566"},{"poster":"Lavi29","content":"The query:\nSELECT EmployeeID\nFROM mytestdb.dbo.myParquetTable\nWHERE EmployeeName = 'Alice';\nwill return an error. This is because you're trying to query an Apache Spark table (myParquetTable) directly from a serverless SQL pool in Azure Synapse Analytics, and this is not supported. The two engines have different storage formats and metadata structures, so querying Spark tables directly from a serverless SQL pool isn't possible and will result in an error.","comment_id":"977728","timestamp":"1691674380.0","upvote_count":"1"},{"timestamp":"1690434720.0","comment_id":"964361","content":"Answer A is correct. \nRefer link : https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#expose-a-spark-table-in-sql","comments":[{"timestamp":"1695376140.0","content":"Shabash Munna","comment_id":"1013929","upvote_count":"1","poster":"dawoodiee"}],"upvote_count":"5","poster":"Amitj2625"},{"upvote_count":"2","poster":"bblord","comment_id":"944666","timestamp":"1688648220.0","content":"Selected Answer: A\nwhat is the correct answer. as per the docs i am getting A. \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-spark-tables"},{"poster":"abrakadabra200","timestamp":"1688575980.0","comment_id":"943964","upvote_count":"3","content":"Answer is A: \nThe Spark created databases and their Parquet-backed or CSV-backed tables become visible in the workspace serverless SQL pool. Databases are created automatically in the serverless SQL pool metadata, and both the external and managed tables created by a Spark job are made accessible as external tables in the serverless SQL pool metadata in the 'dbo' schema of the corresponding database.\nHere is the link (see #3): https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/overview#support-the-modern-data-warehouse"},{"content":"Selected Answer: B\nFrom the docos:\n\"...need to be queried using the lower case name.\"","poster":"steveo123","comment_id":"926945","timestamp":"1687128780.0","upvote_count":"2"},{"upvote_count":"2","poster":"Deeksha1234","content":"Selected Answer: B\nB is correct","timestamp":"1686521400.0","comment_id":"921010"},{"poster":"flames136","timestamp":"1685083500.0","content":"answer is B. spark pool runs using notebooks wherein you select the name of spark pool and the language (pyspark, scala, sql). you cannot select the spark pool when running a serverless sql pool","comment_id":"907127","upvote_count":"2"},{"comment_id":"879854","poster":"rocky48","upvote_count":"1","content":"Answer is B - dbo is the issue.","timestamp":"1682393640.0"},{"upvote_count":"1","timestamp":"1682182560.0","poster":"NickWerbung","content":"How were the values inserted?\n1. INSERT INTO myParquetTable (EmployeeName, EmployeeID, EmployeeStartDate) VALUES (\"Alice\",24, DATE'2020-01-25');\nor\n2. INSERT INTO myParquetTable VALUES (\"Alice\",24, DATE'2020-01-25');\n\n1. A and 2. B.\nCan someone help?","comment_id":"877470"},{"comment_id":"824570","content":"so it's either B or C, depending on which doc. ref. you look at.\nAnswer is B based on: [synapse-analytics ref](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table)\nAnswer is C based on: [t-sql ref](https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&tabs=dedicated#location--folder_or_filepath-1)\n\nfrom testing out practically in azure synapse, i find that the behaviour follows answer B as per synapse-analytics ref. \n\nthe explicit requirement to have <path>/** for recursiveness is similar to the linux terminal interface where you go \"ls -r\" or \"rm -r\", having /** for recursive behaviour is better(less ambiguous) and more aligned with standards i suppose.","poster":"cashew","upvote_count":"1","timestamp":"1677574620.0"},{"comment_id":"811137","poster":"chouraamine","content":"Selected Answer: B\nExplained by dmitriypo 4 months ago from now !","timestamp":"1676584560.0","upvote_count":"1"},{"poster":"CapMorgan63","timestamp":"1676004180.0","content":"i tried to prove the study case in a spark pool and query it from a serverless sql pool. The error a got was the data type converting one. i didnt change anything abput the lines. So, for me the correct answer is B","upvote_count":"1","comment_id":"803967"},{"upvote_count":"3","comment_id":"803924","timestamp":"1675998600.0","content":"No, Query in SQL pool is not case sensitive.\nI believe it needs to wait for a minute, Answer should be A.\nJust tested it.","poster":"Virul"},{"poster":"bubby248","comment_id":"802222","content":"B.because of dbo","timestamp":"1675872720.0","upvote_count":"1"},{"timestamp":"1675281780.0","content":"Almost the same example shown in this doc : https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table \nI think the answer is B , the doc above mention this point 'Table names will be converted to lower case and need to be queried using the lower case name.' so in the query , the table name must be in written in lowercase, and for the dbo database added to the query is correct since the doc used the same example with this query : SELECT * FROM mytestdb.dbo.myexternalparquettable WHERE name = 'Alice';","comment_id":"795472","upvote_count":"1","comments":[{"poster":"sheissham","timestamp":"1675282020.0","upvote_count":"1","content":"So the reason for the error : the table name 'SELECT EmployeeID -\nFROM mytestdb.dbo.myParquetTable' table name myParquetTable not in the lowercase format.","comment_id":"795481"}],"poster":"sheissham"},{"comment_id":"787190","poster":"AhmedDaffaie","content":"I noticed the author has corrected the query. Previously it was\nSELECT EmployeeID -\nFROM mytestdb.dbo.myParquetTable\nWHERE name = 'Alice';","timestamp":"1674612120.0","upvote_count":"1"},{"comment_id":"786558","timestamp":"1674566820.0","content":"Some people claim they have attempted this, and got 24, but the documentation says\n \"Table names will be converted to lower case and need to be queried using the lower case name\"\n\nand in the documentation example\n\"SELECT * FROM mytestdb.dbo.myexternalparquettable WHERE name = 'Alice';\"\ntable name is clearly in lower case.\n\nI don't get how this would not error out? can someone like really really confirm that given an error on query excution?","upvote_count":"1","poster":"Lestrang"},{"upvote_count":"1","comment_id":"783684","timestamp":"1674327480.0","content":"The dash/minus sign after the integer column creates an invalid computation statement. \nAnd the documentation does not cover this as the Microsoft examples all use Select * \nThis is a bad study question.","poster":"Faeyth"},{"comment_id":"780734","content":"Answer is A.","poster":"Suganyasadhun","upvote_count":"1","timestamp":"1674104580.0"},{"timestamp":"1673624100.0","upvote_count":"2","content":"Unfortunately, the answer is A...\n1. Create Spark pool in Synapse (not built in pool, not SQL pool)\n2. Open a notebook and connect to the spark pool\n3. Create mytestdb, create mytestdb.myParquetTable using Parquet and insert some sample rows into it\n4. Now open a new SQL script and connect it to the built-in pool. This is what the questions says. You are no longer in the notebook. You are no longer connected to the Spark pool\n5. Run SELECT * FROM mytestdb.dbo.myParquetTable, it properly retrieves the data\n\nIf in point 4 you would stay in the notebook and run the query from there, you will receive an error that dmitriypo has mentioned. Unfortunately, that's not what the question was about.","poster":"wasilak","comment_id":"774628"},{"timestamp":"1671829020.0","upvote_count":"3","comment_id":"754539","content":"Answer is A , I tried the queries again ,and got the output right","poster":"Achu24"},{"comment_id":"752947","poster":"Achu24","upvote_count":"1","timestamp":"1671680280.0","content":"Answer is B.you can refer to this link https://docs.microsoft.com/en-us/azure/synapse-analytics/metadata/table"},{"comment_id":"751828","poster":"VivekMadas","upvote_count":"1","content":"https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table\nSELECT * FROM mytestdb.dbo.myparquettable WHERE name = 'Alice';\nAnswer is correct.","timestamp":"1671592560.0"},{"poster":"vigilante89","comment_id":"746758","timestamp":"1671161700.0","upvote_count":"3","content":"Selected Answer: B\nThe create table statement creates a table 'myParquetTable' within the database 'mytestdb' not within the default sql server database 'dbo'. \n\nSo providing the table name as \"mytestdb.dbo.myParquetTable\" in the select query would definitely throw an error."},{"timestamp":"1670678340.0","content":"Selected Answer: A\nif you consider the \"-\" after EmployeeID a type, the answer should be \"A\".\nThe scenario is described here: https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table","comment_id":"740995","upvote_count":"2","poster":"Oliviervandersluis"},{"comment_id":"728901","poster":"Billybob0604","upvote_count":"1","content":"there's another consideration, as to know the way the record is inserted. when this is done specifying the columns it will succeed but with a select * it won't, hence the C would be the answer. But let's assume the record is inserted correctly.","timestamp":"1669625640.0"},{"content":"I tried this in Azure Synapse and it returns an error. Removing the \"dbo\" part works. So answer is B.","poster":"gabrielkuka","comment_id":"725220","upvote_count":"3","timestamp":"1669215840.0","comments":[{"timestamp":"1669587660.0","comment_id":"728651","upvote_count":"1","content":"Did you mentioned table name in Lowercase in the query?","poster":"abrar880"}]},{"poster":"zafnad","content":"Selected Answer: A\ndash is just a typo, simply ignore it.","comment_id":"722536","upvote_count":"3","timestamp":"1668940500.0"},{"comment_id":"713658","poster":"withrocky","content":"\"dbo\" in the select statement will result an error. If this SQL statement is given to check presence of mind, then answer is B or if the dbo in the SQL Statement is a typo then answer is A.","timestamp":"1667901960.0","upvote_count":"2"},{"upvote_count":"2","content":"Answer is shown as B here. I tried to recreate the scenario and when we execute the select query ,it's throwing an error because of hyphen symbol only but not because of upper & lower case . \nSo I am not sure if hyphen is typo error . If yes then answer should be A else B.\nSuppose in select query if dbo object is missing then error will occur","timestamp":"1667457540.0","comment_id":"710303","poster":"ashwin_joshi85"},{"poster":"ted0809","timestamp":"1666826940.0","upvote_count":"1","content":"We are not asking lower or upper case here.\nLook at the bigger picture.\nReturning 24","comment_id":"705062"},{"poster":"igormmpinto","comment_id":"697578","upvote_count":"2","content":"Answer is B\nBut I think that is because in CREATE TABLE statement we have \"mytestdb.myParquetTable\" and in SELECT statement we have mytestdb.dbo.myParquetTable.","timestamp":"1666026300.0"},{"comment_id":"683515","content":"Selected Answer: B\nAnswer is B.\nIt will return an error. table name will be converted to lowercase\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table","timestamp":"1664538660.0","upvote_count":"4","poster":"Winwan33"},{"timestamp":"1664405940.0","poster":"Jawidkaderi","content":"The query is correct, The spelling is correct. I am not sure why the upper and lower case is of any concern here; it has to do with the collation. So, if I ignore the dash[-] after the end of SELECT line, the answer would be 24.","comment_id":"682181","upvote_count":"3"},{"upvote_count":"1","poster":"Mano1997","comment_id":"680746","content":"Selected Answer: B\nit is the answer","timestamp":"1664281740.0"},{"poster":"anks84","comments":[{"comment_id":"660345","timestamp":"1662394440.0","poster":"anks84","content":"Once the table has been created****. Ignore the Typo above.","upvote_count":"1"}],"content":"Selected Answer: B\nOnce you create has been created, the table names will be converted to lower case and need to be queried using the lower case name.\nHence, the answer is B (Error).","upvote_count":"1","timestamp":"1662394380.0","comment_id":"660344"},{"comment_id":"660136","poster":"debarun","timestamp":"1662378120.0","upvote_count":"1","content":"Selected Answer: B\nError ... Table names will be converted to lower case and need to be queried using the lower case name"},{"upvote_count":"1","comment_id":"660135","content":"Error ... Table names will be converted to lower case and need to be queried using the lower case name","poster":"debarun","timestamp":"1662378060.0"},{"upvote_count":"1","comment_id":"657462","timestamp":"1662124560.0","content":"Selected Answer: B\nsince you wrote the table name in mixed lower and uppercase it should give you error because it expects lowercase table name\nFROM mytestdb.dbo.myParquetTable","poster":"Remedios79"},{"content":"answer is B. An error will occur","poster":"rafeek00007","upvote_count":"1","comment_id":"657219","timestamp":"1662109920.0"}],"isMC":true,"topic":"1","exam_id":67,"question_text":"You have an Azure Synapse workspace named MyWorkspace that contains an Apache Spark database named mytestdb.\nYou run the following command in an Azure Synapse Analytics Spark pool in MyWorkspace.\nCREATE TABLE mytestdb.myParquetTable(\nEmployeeID int,\nEmployeeName string,\nEmployeeStartDate date)\n\nUSING Parquet -\nYou then use Spark to insert a row into mytestdb.myParquetTable. The row contains the following data.\n//IMG//\n\nOne minute later, you execute the following query from a serverless SQL pool in MyWorkspace.\n\nSELECT EmployeeID -\nFROM mytestdb.dbo.myParquetTable\nWHERE EmployeeName = 'Alice';\nWhat will be returned by the query?","url":"https://www.examtopics.com/discussions/microsoft/view/79349-exam-dp-203-topic-1-question-2-discussion/","unix_timestamp":1662109920,"answers_community":["A (54%)","B (46%)"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0001700001.png"],"answer_description":"","answer_ET":"A","answer_images":[],"timestamp":"2022-09-02 11:12:00","answer":"A","question_id":30}],"exam":{"numberOfQuestions":384,"provider":"Microsoft","lastUpdated":"12 Apr 2025","id":67,"isMCOnly":false,"name":"DP-203","isBeta":false,"isImplemented":true},"currentPage":6},"__N_SSP":true}