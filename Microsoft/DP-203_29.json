{"pageProps":{"questions":[{"id":"jI1fDayR3Ozg4ERf1Ofz","isMC":false,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/130645-exam-dp-203-topic-2-question-119-discussion/","answer_images":["https://img.examtopics.com/dp-203/image364.png"],"question_id":141,"question_images":["https://img.examtopics.com/dp-203/image363.png"],"answer":"","exam_id":67,"unix_timestamp":1704744300,"timestamp":"2024-01-08 21:05:00","question_text":"DRAG DROP\n-\n\nYou have an Azure subscription that contains an Azure data factory.\n\nYou are editing an Azure Data Factory activity JSON.\n\nThe script needs to copy a file from Azure Blob Storage to multiple destinations. The solution must ensure that the source and destination files have consistent folder paths.\n\nHow should you complete the script? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_ET":"","topic":"2","discussion":[{"upvote_count":"8","poster":"Lewiasskick","timestamp":"1720461900.0","comment_id":"1116971","content":"Correct, refer to https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"},{"timestamp":"1731416520.0","comment_id":"1210211","content":"ForEach - As multiple destination\nPreserve Hierachy - Consistent folder structure between Source and Target","poster":"Manoj1127","upvote_count":"2"},{"timestamp":"1727888760.0","poster":"Alongi","upvote_count":"1","content":"Correct","comment_id":"1188182"}],"answer_description":""},{"id":"1J0vGi9gORxQDqKcb5fa","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0017200002.png"],"isMC":false,"question_text":"HOTSPOT -\nThe following code segment is used to create an Azure Databricks cluster.\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","unix_timestamp":1629723480,"answer_ET":"","discussion":[{"comment_id":"462518","poster":"GameLift","timestamp":"1650016320.0","comments":[{"comments":[{"timestamp":"1706440860.0","content":"I hadn't read the other comments yet, apparently it's in 'serverless' :)","poster":"semauni","comment_id":"965440","upvote_count":"1"}],"comment_id":"965438","content":"For 1, where do you see high concurrency?","poster":"semauni","upvote_count":"3","timestamp":"1706440860.0"},{"timestamp":"1666029240.0","comment_id":"587332","upvote_count":"3","content":"agree on this one","poster":"Egocentric"}],"content":"FROM DP-201, thanks to rmk4ever ::\n\n1. Yes\nA cluster mode of ‘High Concurrency’ is selected, unlike all the others which are ‘Standard’. This results in a worker type of Standard_DS13_v2.\nref: https://adatis.co.uk/databricks-cluster-sizing/\n\n2. NO\nrecommended: New Job Cluster.\nWhen you run a job on a new cluster, the job is treated as a data engineering (job) workload subject to the job workload pricing. When you run a job on an existing cluster, the job is treated as a data analytics (all-purpose) workload subject to all-purpose workload pricing.\nref: https://docs.microsoft.com/en-us/azure/databricks/jobs\nScheduled batch workload- Launch new cluster via job\nref: https://docs.databricks.com/administration-guide/capacity-planning/cmbp.html#plan-capacity-and-control-cost\n\n3.YES\nDelta Lake on Databricks allows you to configure Delta Lake based on your workload patterns.\nref: https://docs.databricks.com/delta/index.html","upvote_count":"56"},{"upvote_count":"39","poster":"Canary_2021","content":"Answer is Correct.\nBox 1: Yes\n\"spark.databricks.cluster.profile\": \"serverless\" means that the cluster is a High Concurrency Cluster, which support multi-users. \nBox 2: No\nScheduled jobs should run in standard cluster. High Concurrency clusters are intended for multi-users and won’t benefit a cluster running a single job.\nBox 3:Yes","comment_id":"515190","timestamp":"1656785640.0"},{"comment_id":"1177549","upvote_count":"1","content":"N, N, Y. 1 : N because that's for single user, shared not support R","timestamp":"1726757340.0","poster":"Software_One"},{"poster":"kkk5566","comment_id":"999325","timestamp":"1709639280.0","content":"the answer is Yes, No, Yes.","upvote_count":"1"},{"timestamp":"1707633420.0","poster":"hiyoww","content":"the naming of the clusters are changed in recent UI:\nhttps://docs.databricks.com/en/archive/compute/cluster-ui-preview.html","comment_id":"978268","upvote_count":"1"},{"upvote_count":"3","comment_id":"895028","content":"yes / no / yes","timestamp":"1699711260.0","poster":"mamahani"},{"comment_id":"714126","timestamp":"1683574080.0","upvote_count":"3","comments":[{"comment_id":"876218","timestamp":"1697867040.0","upvote_count":"3","poster":"WieIK","content":"They still use this question, I had this one on my exam this week"}],"poster":"Igor85","content":"i guess this question won't be relevant anymore, since cluster creation UI has changed"},{"upvote_count":"4","content":"1. should be 'No'. Its a standard cluster and it also has scala which is not supported on High Concurrence cluster.","comment_id":"639831","timestamp":"1675125960.0","poster":"US007"},{"timestamp":"1675105080.0","upvote_count":"2","comment_id":"639754","content":"Yes, No, Yes","poster":"Deeksha1234"},{"comment_id":"535471","upvote_count":"4","timestamp":"1659093000.0","poster":"PallaviPatel","content":"Correct Answer. I agree with Canary_2021"},{"upvote_count":"3","comment_id":"497262","content":"I would say the answer is Yes, No, Yes. Delta lake was supported starting from Azure Databricks Runtime 6.0 with Scala 2.11.12. https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/6.0#system-environment","poster":"edba","timestamp":"1654729620.0"},{"poster":"thuggie300","comment_id":"461086","upvote_count":"3","content":"what is the answer lol","timestamp":"1649770020.0"},{"content":"the same question is in DP-201 with the same answer. https://www.examtopics.com/discussions/microsoft/view/16875-exam-dp-201-topic-2-question-11-discussion/","upvote_count":"1","poster":"aarthy2","timestamp":"1649259780.0","comment_id":"458338"},{"comment_id":"451378","timestamp":"1648211940.0","upvote_count":"1","poster":"rav009","content":"IMO NO, YES, YES","comments":[{"content":"Sorry, it should be NO,NO,YES. \nFor Box 2, the cheapest way is creating the cluster when it's time to execute the job and terminate immediately after the task completes. This is called New Job Clusters .\nhttps://docs.microsoft.com/en-us/azure/databricks/jobs","comment_id":"451383","poster":"rav009","upvote_count":"3","timestamp":"1648212240.0"}]},{"upvote_count":"2","comments":[{"upvote_count":"9","comment_id":"443187","content":"Yes No No","timestamp":"1647045000.0","poster":"amma"}],"poster":"parwa","comment_id":"442388","content":"what is correct answer here please?","timestamp":"1646909760.0"},{"content":"High Concurrency clusters are intended for use by multiple users. hence correct answer","timestamp":"1646888160.0","poster":"Amyqwertyu","upvote_count":"2","comment_id":"442259"},{"poster":"Amalbenrebai","comment_id":"437002","upvote_count":"3","content":"NO, NO, YES","timestamp":"1646133300.0"},{"comment_id":"435125","poster":"petulda","timestamp":"1646034420.0","upvote_count":"2","content":"The cluster enables autoscaling. Does this mean it minimize costs ?"},{"timestamp":"1645827540.0","comment_id":"431743","poster":"[Removed]","content":"While it is true that the config jumps to Standard_DS13_v2 when switching from Standard to High Concurrency you could perfectly select Standard_DS13_v2 on your own in a Standard Cluster Mode.\n\nThe hint is from the \"ResourceClass\": \"Serverless\". or same in cluster.profile\nSince the Cluster Mode info also says: \"High Concurrency: [...] Previously known as Serverless.\"","upvote_count":"8"},{"content":"Delta lake is only available from Scala version 2.12 but the json data has a version of scala of 2.11.","upvote_count":"11","comments":[{"content":"You are partially wrong. Starting from Delta version 0.7.0, Delta Lake is only available with Scala version 2.12","poster":"nyende","timestamp":"1647959160.0","upvote_count":"4","comment_id":"449498","comments":[{"content":"Meaning it supports the creation of a Delta Lake table, even if it's only for Scala version 2.12. Thus, the answer is Yes, No, Yes.","timestamp":"1649493180.0","upvote_count":"2","comment_id":"459555","poster":"Vitality"}]}],"comment_id":"429973","poster":"fbraza","timestamp":"1645628280.0"}],"answer":"","answer_description":"Box 1: Yes -\nA cluster mode of 'High Concurrency' is selected, unlike all the others which are 'Standard'. This results in a worker type of Standard_DS13_v2.\n\nBox 2: No -\nWhen you run a job on a new cluster, the job is treated as a data engineering (job) workload subject to the job workload pricing. When you run a job on an existing cluster, the job is treated as a data analytics (all-purpose) workload subject to all-purpose workload pricing.\n\nBox 3: Yes -\nDelta Lake on Databricks allows you to configure Delta Lake based on your workload patterns.\nReference:\nhttps://adatis.co.uk/databricks-cluster-sizing/\nhttps://docs.microsoft.com/en-us/azure/databricks/jobs\nhttps://docs.databricks.com/administration-guide/capacity-planning/cmbp.html https://docs.databricks.com/delta/index.html","question_id":142,"exam_id":67,"topic":"2","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0017100001.png","https://www.examtopics.com/assets/media/exam-media/04259/0017200001.png"],"timestamp":"2021-08-23 14:58:00","url":"https://www.examtopics.com/discussions/microsoft/view/60391-exam-dp-203-topic-2-question-12-discussion/"},{"id":"yr7s8juesxvPxqcY99fz","answer_ET":"B","isMC":true,"answers_community":["B (100%)"],"unix_timestamp":1704740280,"answer_description":"","question_images":[],"answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/130621-exam-dp-203-topic-2-question-120-discussion/","exam_id":67,"topic":"2","timestamp":"2024-01-08 19:58:00","answer_images":[],"question_id":143,"discussion":[{"comment_id":"1188184","content":"correct","upvote_count":"1","timestamp":"1727888880.0","poster":"Alongi"},{"upvote_count":"1","comment_id":"1169495","poster":"tung_dao","timestamp":"1725879480.0","content":"Selected Answer: B\nUse the Alter Row transformation to set insert, delete, update, and upsert policies on rows. \nRef: https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row"},{"comment_id":"1116904","upvote_count":"3","timestamp":"1720457880.0","content":"correct, chatGPT:\nFor upsert operations (insert or update), you typically need to determine whether a record already exists in the destination table based on some condition. In Azure Data Factory's data flow, you would use the \"Alter Row\" transformation for this purpose.","poster":"warre"}],"choices":{"A":"join","D":"select","C":"surrogate key","B":"alter row"},"question_text":"You are building a data flow in Azure Data Factory that upserts data into a table in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to add a transformation to the data flow. The transformation must specify logic indicating when a row from the input data must be upserted into the sink.\n\nWhich type of transformation should you add to the data flow?"},{"id":"yhH8Ppjv3gxbJ7Tqf8T9","isMC":true,"answers_community":["C (100%)"],"answer_ET":"C","question_text":"You have an on-premises database named db1 and a set-hosted integration runtime.\n\nYou have an Azure subscription that contains an Azure Data Lake Storage account named dl1.\n\nYou need to develop four data pipeline projects that will use Microsoft Power Query to copy data from db1 to dl1. The solution must meet the following requirements:\n\n• All pipelines must use the self-hosted integration runtime.\n• Each project must be stored in a separate Git repository.\n• Development effort must be minimized.\n\nWhat should you use?","url":"https://www.examtopics.com/discussions/microsoft/view/130620-exam-dp-203-topic-2-question-121-discussion/","question_images":[],"topic":"2","timestamp":"2024-01-08 19:58:00","exam_id":67,"question_id":144,"answer_images":[],"answer":"C","answer_description":"","unix_timestamp":1704740280,"discussion":[{"comment_id":"1123024","content":"Selected Answer: C\ncorrect","upvote_count":"1","poster":"jsav1","timestamp":"1721004660.0"},{"comment_id":"1120703","content":"Selected Answer: C\nTo develop data pipeline projects that use Microsoft Power Query and meet the specified requirements, you should use Azure Data Factory (option C). Azure Data Factory is a cloud-based data integration service that allows you to create, schedule, and manage data pipelines that can move data between supported on-premises and cloud-based data stores.\n\nWith Azure Data Factory, you can use Power Query to transform and prepare your data before moving it to the destination. It supports the use of self-hosted integration runtimes for on-premises data movement.","poster":"Happynewyear1001","timestamp":"1720773180.0","upvote_count":"3"},{"comment_id":"1116902","timestamp":"1720457880.0","upvote_count":"1","poster":"warre","content":"correct, chat gpt confirmed"}],"choices":{"C":"Azure Data Factory","A":"Azure Synapse Analytics","D":"Microsoft Power BI","B":"Azure Logic Apps."}},{"id":"fPLximpn6vLlFaKwLvnG","answer_ET":"B","isMC":true,"answers_community":["B (100%)"],"unix_timestamp":1704549660,"answer_description":"","question_images":["https://img.examtopics.com/dp-203/image365.png"],"answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/130463-exam-dp-203-topic-2-question-122-discussion/","exam_id":67,"topic":"2","timestamp":"2024-01-06 15:01:00","answer_images":[],"discussion":[{"poster":"Tapaskaro","upvote_count":"5","content":"Answer B, tested in ADF","timestamp":"1720727820.0","comment_id":"1120222"},{"comment_id":"1121729","poster":"[Removed]","upvote_count":"5","content":"Selected Answer: B\nCorrect.\nhttps://learn.microsoft.com/es-es/azure/data-factory/tutorial-pipeline-failure-error-handling","timestamp":"1720873680.0"},{"poster":"AlbertoSoriano","content":"Selected Answer: B\nTo me the answer is not complete. If only a Skipped dependency on \"Upon Success\" is used, the pipeline may still fail if \"Upon Failure\" is an unhandled terminal activity. Since ADF determines the pipeline’s final status based on all unhandled leaf activities, adding a Completion dependency on \"Upon Failure\" ensures that a final successfully executed activity is present, guaranteeing that the pipeline always ends in Success.\n\nI guess B is the most accurate but not completely correct.","timestamp":"1738669620.0","comment_id":"1351324","upvote_count":"1"},{"comments":[{"timestamp":"1727617200.0","comment_id":"1185554","content":"for sure the correct answer.\n\ndirectly from docs --> https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-skip-else-block","poster":"MBRSDG","upvote_count":"1"}],"content":"Selected Answer: B\nIn this approach, customer defines the business logic, and defines both the Upon Failure path, and Upon Success path, with a dummy Upon Skipped activity attached. This approach renders pipeline succeeds, if Upon Failure path succeeds.","poster":"Azure_2023","upvote_count":"4","timestamp":"1723485240.0","comment_id":"1148572"},{"upvote_count":"3","poster":"jongert","content":"Selected Answer: B\nShould be B, creating an 'Do If Skip Else' statement by adding a skipped dependency to the success path means that the pipeline is always succeeded.","timestamp":"1720267260.0","comment_id":"1115207"}],"question_id":145,"question_text":"You have the Azure Synapse Analytics pipeline shown in the following exhibit.\n\n//IMG//\n\n\nYou need to add a set variable activity to the pipeline to ensure that after the pipeline’s completion, the status of the pipeline is always successful.\n\nWhat should you configure for the set variable activity?","choices":{"A":"a skipped dependency on the Upon Failure activity","D":"a failure dependency on the Upon Failure activity","C":"a success dependency on the Business Activity That Fails activity","B":"a skipped dependency on the Upon Success activity"}}],"exam":{"name":"DP-203","isBeta":false,"provider":"Microsoft","isImplemented":true,"id":67,"numberOfQuestions":384,"lastUpdated":"12 Apr 2025","isMCOnly":false},"currentPage":29},"__N_SSP":true}