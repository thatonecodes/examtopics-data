{"pageProps":{"questions":[{"id":"37IaXE4iwCXNTWNJALDY","unix_timestamp":1674568860,"isMC":true,"choices":{"A":"Yes","B":"No"},"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have the Power BI data model shown in the exhibit. (Click the Exhibit tab.)\n//IMG//\n\nUsers indicate that when they build reports from the data model, the reports take a long time to load.\nYou need to recommend a solution to reduce the load times of the reports.\nSolution: You recommend denormalizing the data model.\nDoes this meet the goal?","answer":"A","question_id":176,"timestamp":"2023-01-24 15:01:00","answer_images":[],"question_images":["https://img.examtopics.com/dp-500/image122.png"],"answer_ET":"A","topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/96740-exam-dp-500-topic-1-question-92-discussion/","answer_description":"","exam_id":70,"answers_community":["A (90%)","10%"],"discussion":[{"upvote_count":"5","comment_id":"808186","poster":"ThariCD","content":"Selected Answer: A\nThis link clearly explains the answer https://data-mozart.com/mastering-dp-500-exam-optimize-data-model-by-using-denormalization/","timestamp":"1676361240.0"},{"poster":"Qordata","content":"Selected Answer: B\nDefinitely the answer should no as per the best practices which suggest using star schema then the deformalized single table\nhttps://learn.microsoft.com/en-us/power-bi/guidance/star-schema\n\nAdvantage of using star schema slim model over denormalized table fat model is very well captured in this article\nhttps://www.sqlbi.com/articles/power-bi-star-schema-or-single-table/\n\nBut these questions really depend upon the different options that come up here if there is no question with star schema then this might be correct but i would go with Star Schema option if i do not know what the other questions options would be given.","comment_id":"1041478","timestamp":"1697096280.0","upvote_count":"1"},{"content":"see q108 > normalizing is not the way","comment_id":"1008259","poster":"Deloro","upvote_count":"1","timestamp":"1694765100.0"},{"content":"Selected Answer: A\nA is correct answer.","timestamp":"1687944360.0","comment_id":"936421","poster":"Eltooth","upvote_count":"1"},{"content":"Selected Answer: A\nDenormalization is an answer here","poster":"07kamil","upvote_count":"3","timestamp":"1674568860.0","comment_id":"786587"}]},{"id":"RxWJDHaxkqv8fPnHdgbG","unix_timestamp":1673871600,"answer_images":["https://img.examtopics.com/dp-500/image124.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/95526-exam-dp-500-topic-1-question-93-discussion/","question_text":"DRAG DROP -\nYou have a shared dataset in Power BI named Dataset1.\nYou have an on-premises Microsoft SQL Server database named DB1.\nYou need to ensure that Dataset1 refreshes data from DB1.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n//IMG//","discussion":[{"comment_id":"984986","poster":"SamuComqi","upvote_count":"7","content":"I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Install the on-premises data gateway (standard mode)\n- From powerbi.com, add a data source to the gateway clusters\n- From powerbi.com, configure Dataset1 to use a data gateway","timestamp":"1692422580.0"},{"poster":"CeliaML","comment_id":"1057406","timestamp":"1698648180.0","content":"Agree with you, but, why not a gateway in a personal mode?","upvote_count":"1","comments":[{"content":"standard mode is recommended \nhttps://powerbidocs.com/2021/02/22/personal-vs-on-premises-data-gateway-standard/","poster":"MaryemSB","upvote_count":"1","comment_id":"1106910","timestamp":"1703687880.0"}]},{"poster":"DarioReymago","upvote_count":"2","comment_id":"869238","timestamp":"1681375740.0","comments":[{"upvote_count":"2","comment_id":"869240","poster":"DarioReymago","timestamp":"1681376100.0","content":"I correct myself, it is ok\nhttps://powerbi.tips/2019/10/building-a-gateway-cluster/"}],"content":"It can be ok but\ncluster is for high availability and use more than 1 gateway at the same time for high volumen , the needed dont talk about this and does not talk about more than 1 gateway.\nthis step is not necesary in fact"},{"poster":"stfglv","upvote_count":"2","comments":[{"poster":"stfglv","comment_id":"785226","timestamp":"1674470100.0","upvote_count":"2","content":"Also the 2nd step (with clusters) makes perfect sense considering how clusters are defined here https://learn.microsoft.com/en-us/data-integration/gateway/service-gateway-high-availability-clusters"}],"comment_id":"777625","timestamp":"1673871600.0","content":"I would agree with this answer"}],"topic":"1","question_images":["https://img.examtopics.com/dp-500/image123.png"],"answer_ET":"","timestamp":"2023-01-16 13:20:00","exam_id":70,"answer_description":"","answer":"","isMC":false,"answers_community":[],"question_id":177},{"id":"ZIZtq8Itxq1CfKehyC6z","answers_community":["B (83%)","C (17%)"],"answer":"B","question_text":"You have a deployment pipeline for a Power BI workspace. The workspace contains two datasets that use import storage mode.\nA database administrator reports a drastic increase in the number of queries sent from the Power BI service to an Azure SQL database since the creation of the deployment pipeline.\nAn investigation into the issue identifies the following:\nOne of the datasets is larger than 1 GB and has a fact table that contains more than 500 million rows.\nWhen publishing dataset changes to development, test, or production pipelines, a refresh is triggered against the entire dataset.\nYou need to recommend a solution to reduce the size of the queries sent to the database when the dataset changes are published to development, test, or production.\nWhat should you recommend?","question_id":178,"discussion":[{"content":"C says ChatGPT.","poster":"TheSwedishGuy","comment_id":"1054561","upvote_count":"1","timestamp":"1698324180.0"},{"upvote_count":"1","comment_id":"1041523","content":"Selected Answer: C\nsorry did not vote for the answer C","poster":"Qordata","timestamp":"1697097780.0"},{"timestamp":"1697097660.0","poster":"Qordata","upvote_count":"1","content":"To reduce the size of queries so it does not refresh the complete dataset i think most logical solution is incremental refresh the recommended approach is to enable the large dataset storage format for workspace\n\nFor datasets published to workspaces assigned to Premium capacities, if you think the dataset will grow beyond 1 GB, you can improve refresh operation performance and ensure the dataset doesn't max out size limits by enabling Large dataset storage format before performing the first refresh operation in the service.\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-overview#publish","comment_id":"1041520"},{"content":"Selected Answer: B\nB is the correct answer.","upvote_count":"3","comment_id":"935249","poster":"Eltooth","timestamp":"1687862280.0"},{"content":"Not only what per_ing said, but also A and D apply only to DirectQuery mode (whereas the Q clearly states Import) and the Large format setting makes no sense for a dataset of the size 1GB considering that the Premium capacity has the 10GB limit and we know we are talking about Premium because that is the prerequisites for deployment pipelines altogether. In short, the system of elimination (if not common sense) leaves us with B.","poster":"stfglv","comment_id":"785233","timestamp":"1674470400.0","upvote_count":"4"},{"comments":[{"poster":"Qordata","upvote_count":"1","content":"The recommendation is for reduction of size of the dataset and here we need to reduce the size of the queries during refresh so this solution does not makes sense","timestamp":"1697097480.0","comment_id":"1041514"}],"timestamp":"1674371580.0","comment_id":"784009","upvote_count":"2","poster":"per_ing","content":"Selected Answer: B\nI think this is correct as it is one of the suggestions from Microsoft https://learn.microsoft.com/en-us/power-bi/guidance/import-modeling-data-reduction#switch-to-mixed-mode"}],"unix_timestamp":1674371580,"url":"https://www.examtopics.com/discussions/microsoft/view/96440-exam-dp-500-topic-1-question-94-discussion/","choices":{"D":"From Capacity settings in the Power BI Admin portal, increase the Max Intermediate Row Set Count setting.","B":"Configure the dataset to use a composite model that has a DirectQuery connection to the fact table.","C":"Enable the large dataset storage format for workspace.","A":"From Capacity settings in the Power BI Admin portal, reduce the Max Intermediate Row Set Count setting."},"answer_description":"","topic":"1","timestamp":"2023-01-22 08:13:00","question_images":[],"isMC":true,"exam_id":70,"answer_ET":"B","answer_images":[]},{"id":"Kg2Hg5w8X0oepTKSSikN","unix_timestamp":1672327620,"isMC":true,"question_text":"You are using GitHub as a source control solution for an Azure Synapse Studio workspace.\nYou need to modify the source control solution to use an Azure DevOps Git repository.\nWhat should you do first?","choices":{"B":"Create a new pull request.","D":"Change the active branch.","A":"Disconnect from the GitHub repository.","C":"Change the workspace to live mode."},"answer":"A","question_id":179,"timestamp":"2022-12-29 16:27:00","answer_images":[],"question_images":[],"answer_ET":"A","topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/93175-exam-dp-500-topic-1-question-95-discussion/","answer_description":"","answers_community":["A (100%)"],"exam_id":70,"discussion":[{"comment_id":"984987","timestamp":"1692422580.0","poster":"SamuComqi","upvote_count":"1","content":"Selected Answer: A\nI took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Disconnect from the GitHub repository"},{"comment_id":"761190","poster":"Maazi","upvote_count":"4","content":"Selected Answer: A\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/source-control","timestamp":"1672327620.0"}]},{"id":"pOZKdar1hVNER2L8svsV","question_id":180,"topic":"1","answer_images":[],"answer":"CD","choices":{"A":"Create and save the dataflows to the internal storage of Power BI.","D":"Create and save the dataflows to an Azure Data Lake Storage account.","C":"Associate the Power BI tenant to an Azure Data Lake Storage account.","B":"Add the managed identity for Data Factory as a member of the workspaces."},"question_text":"You have a Power BI tenant that contains 10 workspaces.\nYou need to create dataflows in three of the workspaces. The solution must ensure that data engineers can access the resulting data by using Azure Data Factory.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","isMC":true,"question_images":[],"discussion":[{"content":"Selected Answer: CD\nI think this is correct https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration","comment_id":"784025","poster":"per_ing","timestamp":"1674372240.0","upvote_count":"6"},{"upvote_count":"1","content":"Selected Answer: BD\nwould gues bd","comment_id":"1020529","timestamp":"1695968400.0","poster":"Deloro"},{"poster":"SamuComqi","comment_id":"984988","content":"Selected Answer: BD\nI took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Add the managed identity for Data Factory as a member of the workspaces.\n- Create and save the dataflows to an Azure Data Lake Storage account.","upvote_count":"1","timestamp":"1692422640.0"},{"upvote_count":"2","comment_id":"869265","poster":"DarioReymago","timestamp":"1681379340.0","content":"Selected Answer: BD\nByD as ajaysharma2061 said"},{"comments":[{"comment_id":"880623","poster":"sgodd_0298","timestamp":"1682439480.0","upvote_count":"6","content":"You just gave the answer in your Option C explanation. Answers are C and D because you associate the PBI tenant to an Azure Data Lake Storage account, THEN create and save the dataflows to an Azure Data Lake Storage account. Each answer is PART of a solution."}],"comment_id":"854717","content":"Correct Answer: BD\n\nTo ensure that data engineers can access the resulting data by using Azure Data Factory, you should perform the following two actions:\n\nB. Add the managed identity for Data Factory as a member of the workspaces: This step ensures that Data Factory has access to the Power BI workspaces where the dataflows are located.\n\nD. Create and save the dataflows to an Azure Data Lake Storage account: This step ensures that the dataflows are saved to a location that can be accessed by Azure Data Factory.\n\nTherefore, the correct answers are B and D.\n\nOption A is incorrect because saving dataflows to the internal storage of Power BI does not provide a location that can be accessed by Azure Data Factory.\n\nOption C is also incorrect because associating the Power BI tenant to an Azure Data Lake Storage account does not automatically save the dataflows to the storage account. The dataflows must still be created and saved to the storage account separately.","upvote_count":"2","poster":"ajaysharma2061","timestamp":"1680109920.0"}],"exam_id":70,"unix_timestamp":1674372240,"answers_community":["CD (60%)","BD (40%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/96444-exam-dp-500-topic-1-question-96-discussion/","answer_ET":"CD","answer_description":"","timestamp":"2023-01-22 08:24:00"}],"exam":{"provider":"Microsoft","id":70,"numberOfQuestions":183,"name":"DP-500","lastUpdated":"12 Apr 2025","isBeta":false,"isImplemented":true,"isMCOnly":false},"currentPage":36},"__N_SSP":true}