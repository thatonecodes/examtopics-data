{"pageProps":{"questions":[{"id":"N1yRniG2aYpblVnkZv2E","answers_community":[],"unix_timestamp":1590575400,"exam_id":65,"question_text":"You develop data engineering solutions for a company.\nA project requires the deployment of data to Azure Data Lake Storage.\nYou need to implement role-based access control (RBAC) so that project members can manage the Azure Data Lake Storage resources.\nWhich three actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer":"ADE","timestamp":"2020-05-27 12:30:00","url":"https://www.examtopics.com/discussions/microsoft/view/21447-exam-dp-200-topic-3-question-9-discussion/","question_images":[],"discussion":[{"poster":"[Removed]","timestamp":"1597988880.0","content":"Given answer is correct per first sentence: https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data","comment_id":"162717","upvote_count":"14"},{"upvote_count":"5","comments":[{"timestamp":"1593241260.0","comment_id":"121059","upvote_count":"3","content":"My thoughts exactly! The question itself is asking RBAC then why ACLs!","poster":"svv1612"}],"content":"I don't understand why we should configure ACLs","comment_id":"96713","timestamp":"1590575400.0","poster":"frakcha"},{"content":"ADE is supported by link provided","timestamp":"1606309020.0","poster":"syu31svc","comment_id":"227576","upvote_count":"3"},{"content":"Shouldn't it be DAE\n1st we create a group in AAD and assign that group to ADLS","upvote_count":"3","comment_id":"110592","timestamp":"1592202120.0","comments":[{"upvote_count":"8","content":"It really does not matter... because you just check boxes... So the answer is correct.","poster":"Ikrom","timestamp":"1593565980.0","comment_id":"123833"}],"poster":"Abhilvs"}],"answer_images":[],"answer_ET":"ADE","question_id":166,"answer_description":"AD: Create security groups in Azure Active Directory. Assign users or security groups to Data Lake Storage Gen1 accounts.\nE: Assign users or security groups as ACLs to the Data Lake Storage Gen1 file system\nReferences:\nhttps://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data","choices":{"C":"Configure service-to-service authentication for the Azure Data Lake Storage account.","A":"Assign Azure AD security groups to Azure Data Lake Storage.","E":"Configure access control lists (ACL) for the Azure Data Lake Storage account.","D":"Create security groups in Azure Active Directory (Azure AD) and add project members.","B":"Configure end-user authentication for the Azure Data Lake Storage account."},"isMC":true,"topic":"3"},{"id":"T3rVn7FBoOLj20JcQsU3","discussion":[{"timestamp":"1605239640.0","poster":"ThieryLeLuronNadieline","comment_id":"218264","upvote_count":"7","content":"it's a correct answer","comments":[{"upvote_count":"3","timestamp":"1605277560.0","comments":[{"poster":"Dazzza","content":"Have you already written and didn't pass?","upvote_count":"2","timestamp":"1622451540.0","comment_id":"370853"}],"content":"Have you cleared DP-200? Can you please guide on how to clear it","comment_id":"218530","poster":"neha2804"}]},{"comments":[{"poster":"hoangton","comment_id":"380765","timestamp":"1623547680.0","upvote_count":"1","content":"C are correct answer after I check this \nPossible Reason:\nThe reason this error happens is because each file has different schema. The PolyBase external table DDL when pointed to a directory recursively reads all the files in that directory. When a column or data type mismatch happens, this error could be seen in SSMS.\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-errors-and-possible-solutions?view=sql-server-ver15"}],"comment_id":"359913","timestamp":"1621298100.0","poster":"hoangton","upvote_count":"2","content":"Why not B?"}],"answer_images":[],"isMC":true,"choices":{"B":"EXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [No FileSystem for scheme: wasbs] occurred while accessing external file.'","C":"Cannot execute the query \"Remote Query\" against OLE DB provider \"SQLNCLI11\" for linked server \"(null)\", Query aborted- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.","D":"EXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [Unable to instantiate LoginClass] occurred while accessing external file.'","A":"EXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [com.microsoft.polybase.client.KerberosSecureLogin] occurred while accessing external file.'"},"unix_timestamp":1605239640,"answer_description":"Customer Scenario:\nSQL Server 2016 or SQL DW connected to Azure blob storage. The CREATE EXTERNAL TABLE DDL points to a directory (and not a specific file) and the directory contains files with different schemas.\nSSMS Error:\nSelect query on the external table gives the following error:\nMsg 7320, Level 16, State 110, Line 14\nCannot execute the query \"Remote Query\" against OLE DB provider \"SQLNCLI11\" for linked server \"(null)\". Query aborted-- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.\nPossible Reason:\nThe reason this error happens is because each file has different schema. The PolyBase external table DDL when pointed to a directory recursively reads all the files in that directory. When a column or data type mismatch happens, this error could be seen in SSMS.\nPossible Solution:\nIf the data for each table consists of one file, then use the filename in the LOCATION section prepended by the directory of the external files. If there are multiple files per table, put each set of files into different directories in Azure Blob Storage and then you can point LOCATION to the directory instead of a particular file.\nThe latter suggestion is the best practices recommended by SQLCAT even if you have one file per table.\nIncorrect Answers:\nA: Possible Reason: Kerberos is not enabled in Hadoop Cluster.\nReferences:\nhttps://techcommunity.microsoft.com/t5/DataCAT/PolyBase-Setup-Errors-and-Possible-Solutions/ba-p/305297","timestamp":"2020-11-13 04:54:00","answers_community":[],"question_text":"You configure monitoring for an Azure Synapse Analytics implementation. The implementation uses PolyBase to load data from comma-separated value (CSV) files stored in Azure Data Lake Storage Gen 2 using an external table.\nFiles with an invalid schema cause errors to occur.\nYou need to monitor for an invalid schema error.\nFor which error should you monitor?","answer":"C","url":"https://www.examtopics.com/discussions/microsoft/view/36903-exam-dp-200-topic-4-question-1-discussion/","question_id":167,"topic":"4","question_images":[],"exam_id":65,"answer_ET":"C"},{"id":"6KTqNXLuH2RSBpyLMyVD","exam_id":65,"answer_description":"From the Azure Storage account that contains log data, open the Azure Storage account blade associated with Data Lake Storage Gen1 for logging, and then click Blobs. The Blob service blade lists two containers.\n\nNote:\nYou can enable diagnostic logging for your Azure Data Lake Storage Gen1 accounts, blobs, files, queues and tables.\nDiagnostic logs aren't available for Data Lake Storage Gen2 accounts [as of August 2019].\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-diagnostic-logs https://github.com/MicrosoftDocs/azure-docs/issues/34286","discussion":[{"content":"yes, CORRECT","comment_id":"361256","poster":"memo43","timestamp":"1621418700.0","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/microsoft/view/53110-exam-dp-200-topic-4-question-10-discussion/","answer_ET":"A","topic":"4","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0039300001.png"],"answer":"A","question_id":168,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nA company uses Azure Data Lake Gen 1 Storage to store big data related to consumer behavior.\nYou need to implement logging.\nSolution: Configure Azure Data Lake Storage diagnostics to store logs and metrics in a storage account.\nDoes the solution meet the goal?","timestamp":"2021-05-19 12:05:00","isMC":true,"unix_timestamp":1621418700,"answers_community":[],"choices":{"B":"No","A":"Yes"},"question_images":[]},{"id":"KPoKBbAIaTAnsVsyqf2g","answers_community":[],"unix_timestamp":1589457660,"exam_id":65,"answer":"B","question_text":"Your company uses several Azure HDInsight clusters.\nThe data engineering team reports several errors with some applications using these clusters.\nYou need to recommend a solution to review the health of the clusters.\nWhat should you include in your recommendation?","url":"https://www.examtopics.com/discussions/microsoft/view/20555-exam-dp-200-topic-4-question-12-discussion/","timestamp":"2020-05-14 14:01:00","question_images":[],"discussion":[{"poster":"runningman","comments":[{"comment_id":"134028","content":"I think Ambari cannot recommend solution","upvote_count":"1","timestamp":"1594652280.0","poster":"indradatabricks"}],"upvote_count":"24","content":"I'm glad Ambari was not one of the answers!","timestamp":"1589457660.0","comment_id":"88897"},{"comment_id":"177614","timestamp":"1599820740.0","poster":"vinodawe","upvote_count":"4","content":"correct answer is B"},{"comment_id":"113926","timestamp":"1592568420.0","poster":"JJBarns","upvote_count":"1","content":"In a real world scenario, using App Insights would be highly recommended in addition to Azure Monitor for correct error diagnosis. Since the question points to \"review health of clusters\" Azure Monitor should be the correct one."},{"comments":[{"content":"I don't think so. \nApplication Insights is used to monitor applications. \n\n\"Application Insights, a feature of Azure Monitor, is an extensible Application Performance Management (APM) service for developers and DevOps professionals. Use it to monitor your live applications. It will automatically detect performance anomalies, and includes powerful analytics tools to help you diagnose issues and to understand what users actually do with your app. It's designed to help you continuously improve performance and usability.\"\n\nThe question is about the health of the clusters. \nFor this you can use Azure Monitor, for which you need a Log Analytics Workspace, like in the explanation.\n\nSources:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-oms-log-analytics-tutorial","comment_id":"95814","poster":"damew26089","upvote_count":"10","timestamp":"1590474480.0"}],"poster":"kavya26","content":"Application Insights is right one","timestamp":"1590430320.0","comment_id":"95590","upvote_count":"1"}],"answer_images":[],"answer_ET":"B","question_id":169,"answer_description":"Azure Monitor logs integration. Azure Monitor logs enables data generated by multiple resources such as HDInsight clusters, to be collected and aggregated in one place to achieve a unified monitoring experience.\nAs a prerequisite, you will need a Log Analytics Workspace to store the collected data. If you have not already created one, you can follow the instructions for creating a Log Analytics Workspace.\nYou can then easily configure an HDInsight cluster to send many workload-specific metrics to Log Analytics.\nReferences:\nhttps://azure.microsoft.com/sv-se/blog/monitoring-on-azure-hdinsight-part-2-cluster-health-and-availability/","choices":{"A":"Azure Automation","B":"Log Analytics","C":"Application Insights"},"topic":"4","isMC":true},{"id":"nse1bdPwgSuEBYLxULcA","answer":"","exam_id":65,"answer_description":"Execution results and diagnostics: Azure Storage\nJob launcher and tracker: Job Service\nJob metadata and state: Control database\nThe Job database is used for defining jobs and tracking the status and history of job executions. The Job database is also used to store agent metadata, logs, results, job definitions, and also contains many useful stored procedures, and other database objects, for creating, running, and managing jobs using T-SQL.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-job-automation-overview","answers_community":[],"topic":"4","discussion":[{"comment_id":"69862","comments":[{"comment_id":"357747","timestamp":"1621074480.0","content":"Agree that both first and third box should be Control Database.\n\n- First box: \n\"The Job database is also used to store agent metadata, logs, results, job definitions, and also contains many useful stored procedures and other database objects for creating, running, and managing jobs using T-SQL.\"\n\n- Third box:\n\"Job output\nThe outcome of a job's steps on each target database are recorded in detail, and script output can be captured to a specified table. You can specify a database to save any data returned from a job.\"\n\nReference: https://docs.microsoft.com/en-us/azure/azure-sql/database/job-automation-overview#job-database","poster":"111222333","upvote_count":"1"}],"timestamp":"1585664580.0","poster":"kempstonjoystick","upvote_count":"49","content":"I think the first and third boxes on this one should both be Control Database as per the documentation https://docs.microsoft.com/en-us/azure/sql-database/sql-database-job-automation-overview#job-database\n\nThe Job database is used for defining jobs and tracking the status and history of job executions. The Job database is also used to store agent metadata, logs, results, job definitions, and also contains many useful stored procedures and other database objects for creating, running, and managing jobs using T-SQL."},{"poster":"suhas16c","content":"How can the result after execution be stored in the control database? \nIt should be Azure storage. Hence the answers given are correct","comment_id":"140844","upvote_count":"7","timestamp":"1595390580.0"},{"upvote_count":"2","poster":"syu31svc","comments":[{"upvote_count":"2","comment_id":"281103","content":"you are wrong on execution. how you execute on azure storage?","poster":"dev2dev","timestamp":"1612163880.0"}],"comment_id":"229608","timestamp":"1606567020.0","content":"I would say:\nAzure storage for execution\nControl database for the jobs"},{"comment_id":"190236","upvote_count":"1","content":"A should be Azure storage as Control database can have logs but to do diagnostics the log need to be stored somewhere so that they can be diagnosed.","poster":"hart232","timestamp":"1601457780.0"},{"upvote_count":"4","poster":"SiddharthB","timestamp":"1600169760.0","comment_id":"179833","content":"Should the answer be like below if referred the below documentation?\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-job-automation-overview#job-database \n1)Job database\n2)Job agent\n3)Job database"},{"timestamp":"1594620660.0","comment_id":"133711","content":"true.. no storage required.. 1st one is control database","poster":"krisspark","upvote_count":"3"},{"timestamp":"1590394380.0","poster":"Divs123","upvote_count":"5","content":"Both first and third boxes should have Control Date\nbase","comment_id":"95267"}],"timestamp":"2020-03-31 16:23:00","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/17735-exam-dp-200-topic-4-question-13-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0039600001.jpg"],"unix_timestamp":1585664580,"question_text":"DRAG DROP -\nYour company uses Microsoft Azure SQL Database configured with Elastic pools. You use Elastic Database jobs to run queries across all databases in the pool.\nYou need to analyze, troubleshoot, and report on components responsible for running Elastic Database jobs.\nYou need to determine the component responsible for running job service tasks.\nWhich components should you use for each Elastic pool job services task? To answer, drag the appropriate component to the correct task. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","question_id":170,"isMC":false,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0039700001.jpg"]}],"exam":{"name":"DP-200","numberOfQuestions":228,"isMCOnly":false,"isBeta":false,"isImplemented":true,"provider":"Microsoft","lastUpdated":"12 Apr 2025","id":65},"currentPage":34},"__N_SSP":true}