{"pageProps":{"questions":[{"id":"vXLjUKJGIijyXtGeYKv3","url":"https://www.examtopics.com/discussions/microsoft/view/11218-exam-ai-100-topic-1-question-14-discussion/","exam_id":39,"isMC":false,"answer_description":"Box 1: Azure Blob Storage -\nContainers and blobs support custom metadata, represented as HTTP headers.\n\nBox 2: NV -\nThe NV-series enables powerful remote visualisation workloads and other graphics-intensive applications backed by the NVIDIA Tesla M60 GPU.\nNote: The N-series is a family of Azure Virtual Machines with GPU capabilities. GPUs are ideal for compute and graphics-intensive workloads, helping customers to fuel innovation through scenarios like high-end remote visualisation, deep learning and predictive analytics.\n\nBox 3: F -\nF-series VMs feature a higher CPU-to-memory ratio. Example use cases include batch processing, web servers, analytics and gaming.\nIncorrect:\nA-series VMs have CPU performance and memory configurations best suited for entry level workloads like development and test.\nReferences:\nhttps://azure.microsoft.com/en-in/pricing/details/virtual-machines/series/","answer_images":["https://www.examtopics.com/assets/media/exam-media/03857/0001600001.png"],"question_text":"HOTSPOT -\nYou are designing an AI solution that will be used to find buildings in aerial pictures.\nUsers will upload the pictures to an Azure Storage account. A separate JSON document will contain for the pictures.\nThe solution must meet the following requirements:\n✑ Store metadata for the pictures in a data store.\n✑ Run a custom vision Azure Machine Learning module to identify the buildings in a picture and the position of the buildings' edges.\n✑ Run a custom mathematical module to calculate the dimensions of the buildings in a picture based on the metadata and data from the vision module.\nYou need to identify which Azure infrastructure services are used for each component of the AI workflow. The solution must execute as quickly as possible.\nWhat should you identify? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"comments":[{"upvote_count":"1","comments":[{"comments":[{"poster":"Miles19","content":"It depends on which consistency level you choose for cosmos DB. Doesn't necessarily need to be faster than blob storage.","upvote_count":"1","comment_id":"72166","timestamp":"1586273400.0"}],"content":"it costs, but the ask on the scenario is to execute as fast as possible, which makes cosmos a better candidate","timestamp":"1581965820.0","comment_id":"51795","upvote_count":"15","poster":"junkz"}],"poster":"bandono","comment_id":"41472","timestamp":"1579655400.0","content":"It costs higher than Blob."},{"upvote_count":"3","timestamp":"1620623940.0","poster":"DonGeo","content":"I think it should be Cosmos DB as the requirement is to have the fastest possible solution without worrying about cost","comment_id":"353451"}],"poster":"Exposer","upvote_count":"7","content":"Why not Cosmos DB for JSON documents?","timestamp":"1577917200.0","comment_id":"34334"},{"timestamp":"1621543320.0","content":"Yes. I am also wondering why the answer is not File storage, they are only talking about JSOn meta data, not the actual pictures.","poster":"Hinzzz","comment_id":"362480","upvote_count":"1"},{"timestamp":"1609997820.0","upvote_count":"2","content":"The answer seem correct in my opinion. Azure Blob is the recommended storage for HD images and similar large size file storage.","comment_id":"261562","poster":"aitruthseeker"},{"upvote_count":"3","comments":[{"content":"Store metadata only. Not files.","comment_id":"291994","timestamp":"1613499660.0","poster":"Cornholioz","upvote_count":"2"}],"content":"This might be wrong, but I'm actually going to say file storage. There is metadata for every picture, and it would probably help to keep it organized. File storage would be quickly available, and would probably make analysis easier. The question doesn't ask about cost, so I didn't consider that","poster":"AcetheTest","timestamp":"1606700160.0","comment_id":"230726"},{"poster":"ExamPwnr","content":"\"Run a custom mathematical module to calculate the dimensions of the buildings in a picture based on the metadata and data from the vision module.\" - Since you are doing custom math module - why not create one that will take advantage of GPU acceleration - hence use NV series?","comment_id":"107451","upvote_count":"3","timestamp":"1591853700.0"}],"answers_community":[],"unix_timestamp":1577917200,"answer_ET":"","topic":"1","question_id":6,"answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03857/0001500004.png"],"timestamp":"2020-01-01 23:20:00"},{"id":"8JHCPF4RQdjqtw0mkO9T","isMC":true,"exam_id":39,"question_text":"Your company has recently deployed 5,000 Internet-connected sensors for a planned AI solution.\nYou need to recommend a computing solution to perform a real-time analysis of the data generated by the sensors.\nWhich computing solution should you recommend?","timestamp":"2019-08-07 21:43:00","answer":"C","answer_ET":"C","question_images":[],"question_id":7,"answers_community":[],"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/3346-exam-ai-100-topic-1-question-15-discussion/","unix_timestamp":1565206980,"choices":{"C":"an Azure HDInsight Hadoop cluster","A":"an Azure HDInsight Storm cluster","D":"an Azure HDInsight R cluster","B":"Azure Notification Hubs"},"discussion":[{"poster":"exam_taker5","content":"Should be a Storm Cluster. This allows for real-time event processing.","comment_id":"6274","timestamp":"1565206980.0","upvote_count":"31","comments":[{"upvote_count":"1","comment_id":"147603","timestamp":"1596133080.0","content":"Hadoop comes with Spark","poster":"Arinze"}]},{"comment_id":"34100","poster":"Stants","timestamp":"1577846280.0","content":"I agree the correct answer should be Storm Cluster for real time analytic. https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview","upvote_count":"10"},{"comment_id":"927577","timestamp":"1687183320.0","upvote_count":"1","poster":"rveney","content":"To perform real-time analysis of data generated by 5,000 Internet-connected sensors, the recommended computing solution would be A. an Azure HDInsight Storm cluster"},{"poster":"hachascloud","upvote_count":"2","content":"Storm cluster since when you instantiate a HDINsights service you select which type of cluster you need. (Hadoop, Spark, Storm..) Storm is the best suitable tech to the task and Haddop does not have realtime capabillities by itself","comment_id":"366894","timestamp":"1622012040.0"},{"timestamp":"1618934400.0","poster":"heirro39","upvote_count":"1","comment_id":"339696","content":"Answer is A.\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview"},{"timestamp":"1611295260.0","content":"Strom is correct","upvote_count":"3","comment_id":"273492","poster":"Hotjo"},{"comment_id":"271771","timestamp":"1611123000.0","content":"Apache Hadoop: A framework that uses HDFS, YARN resource management, and a simple MapReduce programming model to process and analyze batch data in parallel.\nApache Storm: A distributed, real-time computation system for processing large streams of data fast. Storm is offered as a managed cluster in HDInsight.\n\nAs per above definition Storm Cluster fits better, as question says real-time.","upvote_count":"2","poster":"San_S"},{"upvote_count":"2","content":"\"Apache Storm is a distributed, fault-tolerant, open-source computation system. You can use Storm to process streams of data in real time with Apache Hadoop. Storm solutions can also provide guaranteed processing of data, with the ability to replay data that wasn't successfully processed the first time.\"\nsource: https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview\n\nMy verdict is HDInsight Storm Cluster, hence A","comment_id":"261565","poster":"aitruthseeker","timestamp":"1609998000.0"},{"upvote_count":"1","comment_id":"252541","content":"Answer is an Azure HDInsight Storm cluster","timestamp":"1608972840.0","poster":"valar_morghulis"},{"comment_id":"176907","poster":"sayak17","upvote_count":"4","content":"This link says it all: https://storm.apache.org/ Apache Storm is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing.","timestamp":"1599714720.0"},{"timestamp":"1599128100.0","comment_id":"172590","upvote_count":"3","content":"It should be Storm \nhttps://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview#apache-storm-use-cases\n\nThe following are some common scenarios for which you might use Storm on HDInsight:\n\nInternet of Things (IoT)\nFraud detection\nSocial analytics\nExtraction, transformation, and loading (ETL)\nNetwork monitoring\nSearch\nMobile engagement","poster":"sk20"},{"poster":"vik007","comment_id":"170028","upvote_count":"2","content":"Real time - Apache Storm\nApache Storm is a distributed, fault-tolerant, open-source computation system. You can use Storm to process streams of data in real time with Apache Hadoop. Storm solutions can also provide guaranteed processing of data, with the ability to replay data that wasn't successfully processed the first time.","timestamp":"1598791800.0"},{"content":"A tricky question. Apache Storm is a Hadoop component available with HDInsight. So, generally speaking, it could be HDInsight Hadoop as well...I'm a bit confused.\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-component-versioning","comments":[{"upvote_count":"6","poster":"giusecozza","timestamp":"1590753660.0","comment_id":"98203","content":"Sorry, I think my statement is wrong. Storm is an HDInsight component, not an Apache Hadoop one. Both are HDInsight components. Hadoop modules consist in Hadoop Common, HDFS, YARN, MapReduce, Ozone, as stated here: https://hadoop.apache.org/\nSo, yes, Azure HDInsight Storm should be the correct answer."}],"upvote_count":"1","timestamp":"1587726180.0","comment_id":"79083","poster":"giusecozza"},{"upvote_count":"4","content":"Azure HDInsight Storm cluster fits better for this scenario as we need a real-time event processing. Hadoop is for batch processing and would introduce higher latency than Storm.","comment_id":"72170","poster":"Miles19","timestamp":"1586274120.0"},{"content":"It should be Storm","comment_id":"39345","poster":"shell","upvote_count":"2","timestamp":"1579096500.0"},{"comment_id":"37953","timestamp":"1578816480.0","upvote_count":"2","poster":"Jazzday","content":"definitely storm cluster"},{"comment_id":"8409","timestamp":"1566841440.0","poster":"Bharat","content":"I agree. Here is the link: https://storm.apache.org/","upvote_count":"5"}],"answer_description":"Azure HDInsight makes it easy, fast, and cost-effective to process massive amounts of data.\nYou can use HDInsight to process streaming data that's received in real time from a variety of devices.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-introduction"},{"id":"X1BWN30OXObKx2kTendg","isMC":false,"unix_timestamp":1578029460,"answer_ET":"","answers_community":[],"topic":"1","question_text":"HOTSPOT -\nNote: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to deploy an application that will perform image recognition. The application will store image data in two Azure Blob storage stores named Blob1 and\nBlob2.\nYou need to recommend a security solution that meets the following requirements:\n✑ Access to Blob1 must be controlled by using a role.\n✑ Access to Blob2 must be time-limited and constrained to specific operations.\nWhat should you recommend using to control access to each blob store? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_id":8,"question_images":["https://www.examtopics.com/assets/media/exam-media/03857/0001900001.jpg"],"discussion":[{"comments":[{"content":"Thanks for letting us know!! :D","timestamp":"1580982060.0","comment_id":"47251","comments":[{"timestamp":"1582004640.0","content":":D But Vatan's comment was helpful to me as many answers are wrong, an additional confirmation adds up more reliably.","poster":"Daniel_vahid","comment_id":"51973","upvote_count":"21"}],"poster":"SunnyS","upvote_count":"3"}],"poster":"vatan","upvote_count":"33","timestamp":"1578029460.0","content":"Answer is right","comment_id":"34784"},{"poster":"Ajitk27","timestamp":"1623599640.0","comment_id":"381194","upvote_count":"2","content":"Answer seems right. Thanks everyone for all the discussions."},{"content":"But I don’t think Azure blobs support Azure Active Directory…","timestamp":"1623336600.0","poster":"tkifle","upvote_count":"1","comment_id":"379104"},{"timestamp":"1620612480.0","content":"Answer's right, and basically, \"Role\" == \"Azure AD\".","comment_id":"353411","poster":"Y2Data","upvote_count":"1"},{"poster":"aitruthseeker","upvote_count":"1","timestamp":"1609998120.0","content":"Correct","comment_id":"261566"},{"poster":"avestabrzn","comment_id":"71458","timestamp":"1586097240.0","upvote_count":"1","content":"thanks guys :)"},{"content":"@SunnyS lol :D","upvote_count":"2","poster":"bhanuj","timestamp":"1581243360.0","comment_id":"48257"}],"timestamp":"2020-01-03 06:31:00","answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/11304-exam-ai-100-topic-1-question-16-discussion/","answer_description":"References:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-auth","answer_images":["https://www.examtopics.com/assets/media/exam-media/03857/0002000001.jpg"],"exam_id":39},{"id":"YBxbZqafszasix5XPAyP","question_text":"You deploy an application that performs sentiment analysis on the data stored in Azure Cosmos DB.\nRecently, you loaded a large amount of data to the database. The data was for a customer named Contoso, Ltd.\nYou discover that queries for the Contoso data are slow to complete, and the queries slow the entire application.\nYou need to reduce the amount of time it takes for the queries to complete. The solution must minimize costs.\nWhat is the best way to achieve the goal? More than one answer choice may achieve the goal. Select the BEST answer.","exam_id":39,"timestamp":"2020-07-13 01:33:00","answer":"B","isMC":true,"answer_ET":"B","url":"https://www.examtopics.com/discussions/microsoft/view/25535-exam-ai-100-topic-1-question-17-discussion/","topic":"1","answer_description":"Throughput provisioned for a container is divided evenly among physical partitions.\nIncorrect:\nNot A: Increasing request units would also improve throughput, but at a cost.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning","answers_community":[],"answer_images":[],"question_images":[],"discussion":[{"poster":"cloudpandit","comment_id":"215696","timestamp":"1604901420.0","upvote_count":"6","content":"More than one answer choice may achieve the goal. \n\nA. Change the request units.\n Request unit is a performance currency abstracting the system resources such as CPU, IOPS, and memory that are required to perform the database operations supported by Azure Cosmos DB.\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/request-units\n\nB. Change the partitioning strategy.\nImprove performance. Data access operations on each partition take place over a smaller volume of data. Correctly done, partitioning can make your system more efficient. Operations that affect more than one partition can run in parallel.\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning"},{"upvote_count":"5","content":"Seems to be correct","poster":"ammarkareem","timestamp":"1594596780.0","comment_id":"133325"},{"comment_id":"927579","poster":"rveney","upvote_count":"1","timestamp":"1687183380.0","content":"To reduce the amount of time it takes for queries to complete on the Contoso data in Azure Cosmos DB, while minimizing costs, the BEST way to achieve the goal is B. Change the partitioning strategy."},{"timestamp":"1613967660.0","content":"Question Say More than one answer choice may achieve the goal but Choose the Best Answer, it would be B and if it says to choose two then we can go with A & B.","comments":[{"upvote_count":"1","content":"It's a trick question - The question does have provision to give multiple answers but it also says to choose the BEST answer. So B is indeed the right answer for this question.","comment_id":"366334","poster":"allanm","timestamp":"1621939140.0"}],"upvote_count":"2","poster":"ayush_singh","comment_id":"296312"},{"timestamp":"1613439600.0","comment_id":"291402","upvote_count":"1","poster":"Cornholioz","content":"This is about provisioning throughput in Azure Cosmos DB.\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/set-throughput\nBoth A and B serve the purpose. RU's impact the cost directly, so it makes Partitioning Strategy (B) a lower cost option. \nSince the question asks for more than one answer, both A and B are correct... if this is a multichoice question."},{"timestamp":"1606109040.0","poster":"awron_durat","upvote_count":"2","content":"A and B both work but as you want to minimize cost, B becomes the better choice.","comment_id":"225480"}],"unix_timestamp":1594596780,"question_id":9,"choices":{"B":"Change the partitioning strategy.","D":"Migrate the data to the Cosmos DB database.","A":"Change the request units.","C":"Change the transaction isolation level."}},{"id":"RZ6tDoRjHDXLRIkaqJ0l","exam_id":39,"answers_community":[],"choices":{"B":"Soft delete","C":"Purge protection","E":"The activation date on the keys","A":"The expiration date on the keys","D":"Auditors"},"answer_images":[],"question_id":10,"question_text":"You have an AI application that uses keys in Azure Key Vault.\nRecently, a key used by the application was deleted accidentally and was unrecoverable.\nYou need to ensure that if a key is deleted, it is retained in the key vault for 90 days.\nWhich two features should you configure? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","topic":"1","timestamp":"2019-08-31 05:11:00","answer_description":"References:\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning","unix_timestamp":1567221060,"answer":"BC","discussion":[{"poster":"jyth_82","upvote_count":"19","comment_id":"18520","timestamp":"1572524220.0","content":"Answer is BC - but the link reference should be this : https://docs.microsoft.com/en-us/azure/key-vault/key-vault-ovw-soft-delete"},{"upvote_count":"10","comment_id":"11058","poster":"CodeAnant","content":"Soft-delete behavior\nWith this feature, the DELETE operation on a key vault or key vault object is a soft-delete, effectively holding the resources for a given retention period (90 days), while giving the appearance that the object is deleted. The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion.\n\nSoft-delete is an optional Key Vault behavior and is not enabled by default in this release. It can be turned on via CLI or Powershell.\n\nPurge protection\nWhen purge protection is on, a vault or an object in deleted state cannot be purged until the retention period of 90 days has passed. These vaults and objects can still be recovered, assuring customers that the retention policy will be followed.\n\nPurge protection is an optional Key Vault behavior and is not enabled by default. It can be turned on via CLI or Powershell.","timestamp":"1568456880.0"},{"upvote_count":"1","timestamp":"1687183500.0","poster":"rveney","content":"two features that should be configured to ensure that deleted keys are retained in Azure Key Vault for 90 days are B. Soft delete and C. Purge protection.","comment_id":"927581"},{"timestamp":"1568456040.0","upvote_count":"3","content":"Soft Delete is correct answer\n“Soft delete” is a Key Vault feature that may be enabled on a vault. When this is true, if a Key Vault is deleted, it is recoverable for 90 days. It disappears from the Azure portal and it looks like the Key Vault has been completely deleted, like any other resource or service in Azure. This isn’t the case, however. It is held by Azure for 90 days and can be restored for any reason. Because of this precaution, a new Key Vault with the same name cannot be added to the Azure subscription until the “soft deleted” vault is truly deleted.","poster":"CodeAnant","comment_id":"11057"},{"poster":"CodeAnant","upvote_count":"1","content":"Can it be The expiration date on the keys and The activation date on the keys.","comment_id":"9060","comments":[{"upvote_count":"1","content":"no, but you corrected yourself later. has to be B and C","timestamp":"1606330980.0","comment_id":"227824","poster":"danflr"}],"timestamp":"1567221060.0"}],"question_images":[],"isMC":true,"answer_ET":"BC","url":"https://www.examtopics.com/discussions/microsoft/view/4404-exam-ai-100-topic-1-question-18-discussion/"}],"exam":{"provider":"Microsoft","lastUpdated":"12 Apr 2025","numberOfQuestions":206,"isImplemented":true,"name":"AI-100","isBeta":false,"isMCOnly":false,"id":39},"currentPage":2},"__N_SSP":true}