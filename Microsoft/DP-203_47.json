{"pageProps":{"questions":[{"id":"saPVBDDumdzyhAWfci0E","answer_description":"","answer":"A","question_text":"You have an activity in an Azure Data Factory pipeline. The activity calls a stored procedure in a data warehouse in Azure Synapse Analytics and runs daily.\nYou need to verify the duration of the activity when it ran last.\nWhat should you use?","topic":"2","discussion":[{"upvote_count":"9","comment_id":"643649","content":"Selected Answer: A\nMonitor, in ADF we have monitor to check all activity runs","poster":"Deeksha1234","timestamp":"1691399220.0"},{"poster":"RanjitManuel","content":"Azure monitor is different from the Monitor option shown in the screenshot.","timestamp":"1687703280.0","upvote_count":"6","comment_id":"622157"},{"content":"Selected Answer: A\nMonitor, in ADF we have monitor to check all activity runs","poster":"kkk5566","timestamp":"1725713220.0","upvote_count":"1","comment_id":"1001574"},{"poster":"TechMgr","upvote_count":"1","timestamp":"1714556100.0","comment_id":"886043","content":"Selected Answer: A\nA is correct"},{"comment_id":"758570","upvote_count":"4","content":"Selected Answer: A\nis correct answer","poster":"MScapris","timestamp":"1703684580.0"},{"content":"Selected Answer: A\nA IS CORRECT ANSWER","comment_id":"751726","timestamp":"1703119740.0","poster":"nicky87654","upvote_count":"2"},{"content":"I'd go with A using this:\n https://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor","poster":"Franz58","comment_id":"637331","timestamp":"1690367100.0","upvote_count":"3"},{"content":"answer is correct. screenshot shows azure data factory pipeline run","poster":"demirsamuel","comments":[{"poster":"demirsamuel","timestamp":"1685531820.0","upvote_count":"3","content":"* under monitor section in ADF","comment_id":"609686"}],"comment_id":"609685","timestamp":"1685531760.0","upvote_count":"2"},{"content":"based upon the screen shot, isn't that part of the azure synapse analytics (one of the icons from the left)?","poster":"g2000","comments":[{"content":"answer is correct, monitoring is under Monitor and Dashboard","upvote_count":"3","timestamp":"1684846140.0","comment_id":"606082","poster":"upliftinghut"},{"poster":"sdokmak","comment_id":"607471","content":"Looks like Data Factory to me. If Data Factory was there I would have picked it.","upvote_count":"1","timestamp":"1685077500.0"}],"timestamp":"1682018580.0","upvote_count":"2","comment_id":"588919"}],"url":"https://www.examtopics.com/discussions/microsoft/view/73955-exam-dp-203-topic-2-question-74-discussion/","isMC":true,"answers_community":["A (100%)"],"exam_id":67,"unix_timestamp":1650482580,"answer_ET":"A","choices":{"B":"Activity log in Azure Synapse Analytics","D":"an Azure Resource Manager template","C":"the sys.dm_pdw_wait_stats data management view in Azure Synapse Analytics","A":"activity runs in Azure Monitor"},"question_id":231,"question_images":[],"timestamp":"2022-04-20 21:23:00","answer_images":[]},{"id":"WHvLcI9Jy7BCgjPb8Zrw","question_images":[],"answer_images":[],"answers_community":["B (95%)","5%"],"question_text":"You have an Azure Data Factory pipeline that is triggered hourly.\nThe pipeline has had 100% success for the past seven days.\nThe pipeline execution fails, and two retries that occur 15 minutes apart also fail. The third failure returns the following error.\nErrorCode=UserErrorFileNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ADLS Gen2 operation failed for: Operation returned an invalid status code 'NotFound'. Account: 'contosoproduksouth'. Filesystem: wwi. Path: 'BIKES/CARBON/year=2021/month=01/day=10/hour=06'. ErrorCode: 'PathNotFound'. Message: 'The specified path does not exist.'. RequestId: '6d269b78-901f-001b-4924-e7a7bc000000'. TimeStamp: 'Sun, 10 Jan 2021 07:45:05\nWhat is a possible cause of the error?","choices":{"A":"The parameter used to generate year=2021/month=01/day=10/hour=06 was incorrect.","C":"From 06:00 to 07:00 on January 10, 2021, the file format of data in wwi/BIKES/CARBON was incorrect.","D":"The pipeline was triggered too early.","B":"From 06:00 to 07:00 on January 10, 2021, there was no data in wwi/BIKES/CARBON."},"timestamp":"2022-04-20 21:30:00","unix_timestamp":1650483000,"exam_id":67,"isMC":true,"answer_description":"","answer":"B","answer_ET":"B","discussion":[{"comment_id":"598555","upvote_count":"32","poster":"KashRaynardMorse","timestamp":"1652014920.0","content":"Selected Answer: B\nThe error message says a missing file, which matches with answer B: missing data from 06:00. The process had re-tried three times, 15 mins apart, which explains that the error was generated 07:45.","comments":[{"poster":"Billybob0604","content":"i don't agree. the path is not created correctly and therefore the file is ' missing' . It is in the error message too.","timestamp":"1670171340.0","comments":[{"upvote_count":"3","timestamp":"1671445740.0","comment_id":"749721","content":"Answer A. The path in error message shows hour=06 whereas the hour of retry run is 07.","poster":"MS2710"}],"upvote_count":"3","comment_id":"735240"},{"content":"Agree, noting that previous runs were succeeded which indicates it is not parameter issue","poster":"hunno_basha","comment_id":"1194361","upvote_count":"3","timestamp":"1712928840.0"},{"content":"Thank you for the detail","poster":"Remedios79","comment_id":"620559","upvote_count":"1","timestamp":"1655922180.0"},{"content":"To elaborate, the pipeline is triggered hourly, any processing that is done has to be applied to the data that was received in the last hour. In other words, at 07:00 the data received between 06:00 and 07:00 is processed. Accounting for 3*15m results in 07:45.\n\nMore elegantly however, the timestamps worked fine for a week which indicates that path creation is not a problem. Answer is B as you say.","comment_id":"1109562","upvote_count":"4","timestamp":"1703927820.0","poster":"jongert"}]},{"comment_id":"834117","upvote_count":"9","content":"For 7 days, this job was succeeding.\nSo path rule seems to be right.","timestamp":"1678375980.0","poster":"sugiats"},{"content":"Selected Answer: B\nThe pipeline ran fine for 7 days?\n\nNo miscalculation, unless summer time was applied, which usually doesn't happen at 7 AM anywhere. Also, no bad params or it would never have run.\n\nIt's about data.","poster":"renan_ineu","comment_id":"1286917","upvote_count":"1","timestamp":"1726842300.0"},{"comment_id":"1205309","timestamp":"1714626840.0","content":"when something runs smoothly and then all of the sudden it does not then it must be data\neither missing or incorrect\npresuming that the data is also loaded by consistent process it would not be wrong formatting but only missing data","poster":"Dusica","upvote_count":"2"},{"content":"I think A is the closest matching answer.\nIt's been unable to find the file, it can be a valid reason that someone has changed the folder, therefore the path is not found.\nIf it's a payload or data-related matter, I would expect a different error rather than not found.","timestamp":"1708141800.0","poster":"j888","comment_id":"1152358","upvote_count":"1"},{"comment_id":"1147168","timestamp":"1707649320.0","upvote_count":"1","content":"Selected Answer: B\nThe process is scheduled to process the file on a daily basis every day at 7am. The last try at 7:45 return the error - the file is missing. Therefore, the data is missing data from 06:00. And this is the normal schedule. For 7 days, this job was succeeding.","poster":"Azure_2023"},{"comment_id":"1143494","timestamp":"1707322020.0","upvote_count":"1","content":"Selected Answer: B\nB is the answer","poster":"Alongi"},{"content":"Selected Answer: B\nB is correct","poster":"kkk5566","comment_id":"993735","upvote_count":"1","timestamp":"1693374780.0"},{"content":"A. The parameter used to generate year=2021/month=01/day=10/hour=06 was incorrect.\n\nThe error message indicates that the specified path 'BIKES/CARBON/year=2021/month=01/day=10/hour=06' does not exist. This suggests that the parameters used to generate the path might be incorrect, resulting in an invalid or non-existent path. Double-checking the parameter values used to construct the path would be the most likely reason for this error.","comment_id":"989060","upvote_count":"1","timestamp":"1692871380.0","poster":"Altgeeky"},{"upvote_count":"2","content":"Selected Answer: B\nThis option matches with the parameter 6th hour of Jan 10th . Option A might or might not be true but we have no data about what hour of data to be retrieved. Whereas, Option B clearly points to the time mentioned in the question and Path not found could be due to no data prsent","comment_id":"985335","timestamp":"1692464940.0","poster":"Tightbot"},{"poster":"esaade","comment_id":"831914","upvote_count":"3","content":"Selected Answer: B\nThe error message states that the specified path does not exist. Therefore, a possible cause of the error could be that the data for the specified path, which is wwi/BIKES/CARBON/year=2021/month=01/day=10/hour=06, does not exist in the storage account. This could be due to missing data or incorrect path or container name. Option B is the most likely cause of the error as it suggests that there was no data in the specified path during the given time frame.","timestamp":"1678194180.0"},{"upvote_count":"2","content":"Selected Answer: A\nProvided answer is correct : PATH NOT FOUND . The right path must be BIKES/CARBON/2021/01/06/Filename.*\nFilesystem: wwi. Path: 'BIKES/CARBON/year=2021/month=01/day=10/hour=06'. ErrorCode: 'PathNotFound'. Message: 'The specified path does not exist.'.","poster":"raphasc","comments":[{"timestamp":"1673732100.0","upvote_count":"4","content":"question says that it ran fine earlier. Parameter must have been set correctly. Answer is B","poster":"Venub28","comment_id":"775950"}],"comment_id":"763409","timestamp":"1672619520.0"},{"comment_id":"709979","poster":"dmitriypo","timestamp":"1667412120.0","content":"Selected Answer: B\nNo file","upvote_count":"2"},{"content":"Selected Answer: B\nB is correct","poster":"Rohanh","upvote_count":"3","timestamp":"1665464460.0","comment_id":"691754"},{"timestamp":"1661611320.0","comment_id":"652603","content":"Selected Answer: B\nB of course","poster":"dom271219","upvote_count":"3"},{"timestamp":"1661610960.0","comment_id":"652601","comments":[{"upvote_count":"9","poster":"pangas2567","comment_id":"661931","timestamp":"1662521940.0","content":"With run starting at 7.00 pointing to the hour=07 folder, you wouldn't have anything to work with. One hour delay needed here."}],"upvote_count":"5","content":"The question states the pipeline runs hourly and in the timestamp of the error we can see that the time is 7:45 for the third run. So the initial run was at 7:00, but the folder it was looking at is hour=06, which is wrong, it should be hour = 07. So I agree with the option A","poster":"CloudixExamTopics"},{"content":"Selected Answer: B\nB is correct","poster":"Deeksha1234","timestamp":"1659863520.0","comment_id":"643650","upvote_count":"3"},{"poster":"ROLLINGROCKS","timestamp":"1659105060.0","upvote_count":"5","comment_id":"639272","content":"The issue with B is that it says that there is no data in BIKES/CARBON which is false, because it has been loading for a week. There might not be data in a subdirectory of BIKES/CARBON but there is data in BIKES/CARBON for sure, making B false in my opinion."},{"comment_id":"636925","poster":"Sriramiyer92","content":"B\nReason : Operation returned an invalid status code 'NotFound'. & 'Message: 'The specified path does not exist.'.'","upvote_count":"1","timestamp":"1658777280.0"},{"poster":"virendrapsingh","timestamp":"1654104060.0","content":"Selected Answer: B\nAnswer should be B.","comment_id":"610277","upvote_count":"2"},{"comment_id":"609690","timestamp":"1653996180.0","content":"Selected Answer: B\n100% B. The error shows at 7:45. So 45 min after 7. o'clock. and thats equal to 3 times 15 min intervall. Additionally the stacktrace shows that no filepath exists.","upvote_count":"4","poster":"demirsamuel"},{"timestamp":"1652952420.0","comment_id":"603749","poster":"RamboRinky","content":"Selected Answer: A\nSince the parameter that generates the path reference did not generate properly, we cannot look into the proper folder to check if the file is really missing. No telling if the file is missing if you do not look at the proper place where the file is supposed to be. year = 2021 should be 2021/....","comments":[{"content":"it was correct for 7 days so there's no way that's true.","poster":"sdokmak","upvote_count":"6","timestamp":"1653543000.0","comment_id":"607478"}],"upvote_count":"1"},{"content":"I agree B for me","upvote_count":"4","poster":"chuckas","comment_id":"591348","timestamp":"1650854760.0"},{"upvote_count":"4","comment_id":"588921","timestamp":"1650483000.0","content":"B seems a better fit. no data","poster":"g2000"}],"topic":"2","question_id":232,"url":"https://www.examtopics.com/discussions/microsoft/view/73956-exam-dp-203-topic-2-question-75-discussion/"},{"id":"Lv634GjcIoLIOE5Zckzi","answer":"C","choices":{"D":"From Azure Monitor, run a Kusto query against the SparkLoggingEvent_CL table.","A":"From Synapse Studio, select the workspace. From Monitor, select SQL requests.","B":"From Azure Monitor, run a Kusto query against the AzureDiagnostics table.","C":"From Synapse Studio, select the workspace. From Monitor, select Apache Sparks applications."},"exam_id":67,"topic":"2","discussion":[{"poster":"nefarious_smalls","comment_id":"612918","timestamp":"1686168240.0","content":"I think this is one is correct.","upvote_count":"10"},{"content":"Selected Answer: C\nCorrect","comment_id":"993736","upvote_count":"1","poster":"kkk5566","timestamp":"1724997180.0"},{"comment_id":"912507","timestamp":"1717299540.0","content":"Selected Answer: C\nCorrect","upvote_count":"1","poster":"Ankit_Az"},{"content":"The correct answer is C. From Synapse Studio, select the workspace. From Monitor, select Apache Spark applications.","timestamp":"1707536400.0","poster":"akk_1289","upvote_count":"2","comment_id":"803941"},{"comment_id":"661934","upvote_count":"4","poster":"pangas2567","content":"Selected Answer: C\nCorrect\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/monitoring/how-to-monitor-spark-applications","timestamp":"1694058000.0"},{"timestamp":"1691399700.0","content":"Selected Answer: C\nC is correct","poster":"Deeksha1234","comment_id":"643652","upvote_count":"2"}],"question_id":233,"answer_description":"","question_text":"You have an Azure Synapse Analytics job that uses Scala.\nYou need to view the status of the job.\nWhat should you do?","answer_ET":"C","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/76623-exam-dp-203-topic-2-question-76-discussion/","answers_community":["C (100%)"],"unix_timestamp":1654632240,"answer_images":[],"question_images":[],"timestamp":"2022-06-07 22:04:00"},{"id":"ZuDtvv5GL6CjwR5OdPfy","answer":"","question_text":"DRAG DROP -\nYou have an Azure Data Lake Storage Gen2 account that contains a JSON file for customers. The file contains two attributes named FirstName and LastName.\nYou need to copy the data from the JSON file to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values.\nYou create the following components:\n✑ A destination table in Azure Synapse\n✑ An Azure Blob storage container\n✑ A service principal\nIn which order should you perform the actions? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0026400001.png"],"timestamp":"2022-05-05 15:03:00","topic":"2","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0026400002.png"],"question_id":234,"exam_id":67,"discussion":[{"timestamp":"1667660580.0","upvote_count":"20","poster":"Feljoud","comment_id":"597316","content":"Similar to another question in this dump. Seems correct!"},{"upvote_count":"2","content":"Correct","poster":"Alongi","comment_id":"1203589","timestamp":"1730131260.0"},{"upvote_count":"2","comment_id":"993737","timestamp":"1709193240.0","content":"correct","poster":"kkk5566"},{"content":"correct","poster":"rzeng","timestamp":"1682671680.0","comment_id":"706287","upvote_count":"4"},{"comment_id":"652604","poster":"dom271219","comments":[{"upvote_count":"3","poster":"Karl_Cen","comment_id":"771090","content":"As mentioned earlier, the Azure Synapse connector uses Azure Blob storage as temporary storage to upload data between Azure Databricks and Azure Synapse\nso it means only before you loading data into ADLS, you need this temporary folder.\nhttps://learn.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse","timestamp":"1688960700.0"}],"timestamp":"1677516720.0","content":"\"Specify a temporary folder to stage the data\" must be before creating the DF : I am wrong ?","upvote_count":"1"},{"upvote_count":"2","comment_id":"643653","content":"correct","timestamp":"1675768860.0","poster":"Deeksha1234"},{"upvote_count":"1","timestamp":"1670450460.0","content":"correct","comment_id":"612916","poster":"nefarious_smalls"},{"upvote_count":"3","content":"answer is correct. Similar to a duplicated question in this question catalog.","timestamp":"1669814760.0","poster":"demirsamuel","comment_id":"609691"}],"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/75204-exam-dp-203-topic-2-question-77-discussion/","isMC":false,"answer_description":"Step 1: Mount the Data Lake Storage onto DBFS\nBegin with creating a file system in the Azure Data Lake Storage Gen2 account.\nStep 2: Read the file into a data frame.\nYou can load the json files as a data frame in Azure Databricks.\nStep 3: Perform transformations on the data frame.\nStep 4: Specify a temporary folder to stage the data\nSpecify a temporary folder to use while moving data between Azure Databricks and Azure Synapse.\nStep 5: Write the results to a table in Azure Synapse.\nYou upload the transformed data frame into Azure Synapse. You use the Azure Synapse connector for Azure Databricks to directly upload a dataframe as a table in a Azure Synapse.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse","answers_community":[],"unix_timestamp":1651755780},{"id":"04VI3Enu34kecVpGoAlW","answer_description":"","unix_timestamp":1662478860,"answer":"AB","isMC":true,"choices":{"D":"Create an Azure Data Factory trigger.","E":"From the UX Authoring canvas, select Publish.","F":"From the UX Authoring canvas, run Publish All.","B":"Create a Git repository.","C":"Create a GitHub action.","A":"From the UX Authoring canvas, select Set up code repository."},"answer_ET":"AB","answers_community":["AB (75%)","AF (16%)","9%"],"url":"https://www.examtopics.com/discussions/microsoft/view/80665-exam-dp-203-topic-2-question-78-discussion/","answer_images":[],"question_id":235,"timestamp":"2022-09-06 17:41:00","exam_id":67,"topic":"2","question_text":"You have an Azure data factory named ADF1.\nYou currently publish all pipeline authoring changes directly to ADF1.\nYou need to implement version control for the changes made to pipeline artifacts. The solution must ensure that you can apply version control to the resources currently defined in the UX Authoring canvas for ADF1.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","discussion":[{"upvote_count":"18","comment_id":"665431","content":"Selected Answer: AB\nThey are asking to \"implement version control\".\nB Create Git repo\nA From the UX Set up code repository","poster":"dom271219","timestamp":"1662816180.0"},{"comment_id":"832909","timestamp":"1678276140.0","content":"Selected Answer: AB\nTo implement version control for changes made to pipeline artifacts in ADF1 while ensuring that version control can be applied to the resources currently defined in the UX Authoring canvas, you should perform the following two actions:\n\nA. From the UX Authoring canvas, select Set up code repository: This will allow you to configure ADF1 to integrate with a version control system such as Git, which will enable you to track changes made to pipeline artifacts over time.\n\nB. Create a Git repository: This will provide the version control system needed to track changes made to pipeline artifacts in ADF1.\n\nTherefore, options A and B are the correct answers.\n\nC, D, E, and F are not relevant to implementing version control for changes made to pipeline artifacts in ADF1.","poster":"esaade","upvote_count":"8"},{"upvote_count":"1","poster":"renan_ineu","content":"Selected Answer: AB\nB - Repo must be created before setting it up\nA - Setup the repo in the Azure service\n(implementation ends here)","timestamp":"1726831380.0","comment_id":"1286784"},{"timestamp":"1716826380.0","upvote_count":"1","poster":"slamcity","content":"Selected Answer: AB\n(B)create git repo and conect to it from ADF (A)","comment_id":"1219675"},{"content":"Selected Answer: AB\nCorrect","comment_id":"993739","timestamp":"1693375260.0","poster":"kkk5566","upvote_count":"1"},{"upvote_count":"1","comment_id":"912508","timestamp":"1685677260.0","poster":"Ankit_Az","content":"Selected Answer: AB\nCorrect"},{"comment_id":"893921","timestamp":"1683723600.0","upvote_count":"3","poster":"Debasish93","content":"I think the answer should be AF as \"Set up code repository\" gives us the option of creating new repository if not already created so option B is redundant. More over we should not individually publish existing artifacts rather should go for \"Publish All\"."},{"comment_id":"893586","timestamp":"1683691740.0","poster":"azure_user11","content":"Selected Answer: AB\n\"ensuring that version control can be applied to the resources currently defined in the UX Authoring canvas\"\nWhen creating a Git repository this option is ticked by default, so all available resources at the time of the creation are imported into Git, no need to publish which is what the last answers are trying to imply.\nImport existing resources to repository Specifies whether to import existing data factory resources from the UX authoring canvas into a GitHub repository. Select the box to import your data factory resources into the associated Git repository in JSON format. This action exports each resource individually (that is, the linked services and datasets are exported into separate JSONs). When this box isn't selected, the existing resources aren't imported. Selected (default)","upvote_count":"1"},{"comment_id":"803942","upvote_count":"1","poster":"akk_1289","content":"The correct answers are B. Create a Git repository and A. From the UX Authoring canvas, select Set up code repository.","timestamp":"1676000580.0"},{"content":"Selected Answer: AB\noption A is correct because it allows you to set up a code repository to store and manage the changes made to pipeline artifacts in ADF1.\nOption B is correct because it allows you to create a Git repository, which is a version control system that stores the history of changes made to the pipeline artifacts. This allows you to easily roll back to a previous version or compare changes made over time.","poster":"[Removed]","upvote_count":"2","timestamp":"1673743500.0","comment_id":"776071"},{"upvote_count":"2","timestamp":"1669741800.0","content":"Selected Answer: AF\nSince there is no mention of GitHub or DevOps the solution that works for both is A & F","poster":"OldSchool","comment_id":"730655"},{"content":"Selected Answer: AE\nI did a setup of the version control for my test ADF instance in the following way:\n\nA. From the UX Authoring canvas, select Set up code repository.\nHere I configured a connection to the Azure DevOps organization, chose a project, and created a new repo.\n\nE. From the UX Authoring canvas, select Publish.","timestamp":"1667414040.0","comment_id":"709995","poster":"dmitriypo","upvote_count":"4"},{"content":"A & B: The documentation attached to this question states the first step is to set up a code repository from the UX and this question is around setting up version control, not saving your changes which is what F suggests","poster":"Titokyo","upvote_count":"3","timestamp":"1665661800.0","comment_id":"693852"},{"poster":"coolin","timestamp":"1663138800.0","upvote_count":"3","comment_id":"668690","content":"F A is the correct order. Save changes then set up code repos"},{"timestamp":"1662495480.0","content":"Selected Answer: AF\nShould be AF as we want to achieve the version control for the code changes.","upvote_count":"5","comment_id":"661628","poster":"anks84"},{"poster":"federc","upvote_count":"3","comment_id":"661400","content":"why not A and B ? I would set up the code repository after creating the git repo","comments":[{"comment_id":"826679","timestamp":"1677747240.0","content":"in my opinion, A & B resulted the same - repo created","upvote_count":"1","poster":"yuorrik"}],"timestamp":"1662478860.0"}],"question_images":[]}],"exam":{"isImplemented":true,"name":"DP-203","lastUpdated":"12 Apr 2025","id":67,"provider":"Microsoft","numberOfQuestions":384,"isMCOnly":false,"isBeta":false},"currentPage":47},"__N_SSP":true}