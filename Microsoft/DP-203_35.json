{"pageProps":{"questions":[{"id":"EnGbefZdRFbllvs8zqBb","url":"https://www.examtopics.com/discussions/microsoft/view/60832-exam-dp-203-topic-2-question-2-discussion/","topic":"2","answer_ET":"CF","timestamp":"2021-08-27 11:10:00","choices":{"B":"Implement Azure Stream Analytics user-defined functions (UDF).","F":"Implement query parallelization by partitioning the data input.","A":"Implement event ordering.","C":"Implement query parallelization by partitioning the data output.","D":"Scale the SU count for the job up.","E":"Scale the SU count for the job down."},"question_images":[],"answer_images":[],"exam_id":67,"isMC":true,"unix_timestamp":1630055400,"question_id":171,"answer_description":"","answers_community":["CF (50%)","DF (38%)","9%"],"question_text":"A company has a real-time data analysis solution that is hosted on Microsoft Azure. The solution uses Azure Event Hub to ingest data and an Azure Stream\nAnalytics cloud job to analyze the data. The cloud job is configured to use 120 Streaming Units (SU).\nYou need to optimize performance for the Azure Stream Analytics job.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","discussion":[{"content":"Partition input and output.\nREF: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization","comment_id":"436985","comments":[{"comment_id":"447314","timestamp":"1632008640.0","upvote_count":"14","poster":"kolakone","content":"Agree. And partitioning Input and output with same number of partitions gives the best performance optimization.."}],"upvote_count":"70","timestamp":"1630485960.0","poster":"manquak"},{"upvote_count":"15","comments":[{"content":"The stream analytics job is the consumer.","poster":"Boompiee","timestamp":"1652192220.0","comment_id":"599631","upvote_count":"2"},{"content":"Stream analytics ALWAYS has at least one output. There is no need to mention that. So correct answer is input and output","timestamp":"1637002620.0","poster":"nicolas1999","upvote_count":"4","comment_id":"478935"}],"poster":"Lio95","content":"No event consumer was mentioned. Therefore, partitioning output is not relevant. Answer is correct","comment_id":"448829","timestamp":"1632220020.0"},{"content":"Selected Answer: CD\nPartitioning the output data can improve write performance or manage how results are distributed, but it doesn’t directly impact the performance of data ingestion or processing within the job itself.","timestamp":"1737814620.0","poster":"sakis213","upvote_count":"1","comment_id":"1346489"},{"upvote_count":"2","comment_id":"1335500","timestamp":"1735813620.0","content":"Selected Answer: CF\nAs suggested in the following link: https://techcommunity.microsoft.com/blog/analyticsonazure/optimize-your-stream-analytics-job%E2%80%99s-performance-using-job-diagram-simulator/3652303\n\"One way to optimize a Stream Analytics job’s performance is to leverage parallelism in query\" and \"For a job to be parallel, you need to align partition keys between all inputs, query steps, and outputs\"","poster":"GiuseppeTanda"},{"content":"Selected Answer: CF\nC. Implement query parallelization by partitioning the data output.\n\nBy partitioning the data output, you can enable query parallelization. This allows the queries to be distributed across multiple partitions, which can significantly enhance performance.\n\nF. Implement query parallelization by partitioning the data input.","timestamp":"1733022000.0","comment_id":"1320428","upvote_count":"1","poster":"de_examtopics"},{"timestamp":"1720885440.0","poster":"Danweo","upvote_count":"1","content":"Selected Answer: DF\nPartitioning on both input and output can help, but we don't know if the output is a service that doesn't support partitioning like Power BI. Scaling up will always assign more resources at least.","comment_id":"1247372"},{"comment_id":"1244371","upvote_count":"2","timestamp":"1720447740.0","poster":"e56bb91","content":"Selected Answer: CF\nChatGPT 4o\nC. Implement query parallelization by partitioning the data output:\nOutput Partitioning: By partitioning the data output, you can ensure that the processing load is distributed evenly across multiple nodes, which can significantly improve performance by reducing bottlenecks in data writing.\nF. Implement query parallelization by partitioning the data input:\nInput Partitioning: Partitioning the data input allows the Stream Analytics job to process different partitions in parallel, leading to better utilization of the available streaming units and improved throughput."},{"poster":"Dusica","upvote_count":"1","content":"C and F > same partitions > embarrassingly parallel processing","comment_id":"1205110","timestamp":"1714570680.0"},{"content":"It says optimize performance, does not say that it is bad so adding SU may be unneccesary cost increase. Parallelization and embarrassingly parallel job is correct","poster":"Dusica","comment_id":"1201681","timestamp":"1714008720.0","upvote_count":"1"},{"content":"Answer is D & F","comment_id":"1187867","upvote_count":"1","poster":"Bhargava12","timestamp":"1712036760.0"},{"poster":"Elanche","comment_id":"1166951","upvote_count":"1","content":"D. Scale the SU count for the job up: Increasing the number of Streaming Units (SUs) can improve the performance of the Stream Analytics job by providing more processing power to handle the incoming data stream.\n\nC. Implement query parallelization by partitioning the data output: Partitioning the data output can help distribute the processing load across multiple partitions, allowing for parallel execution of queries and enhancing performance.","timestamp":"1709706000.0"},{"timestamp":"1706871660.0","poster":"Alongi","comment_id":"1138456","upvote_count":"1","content":"Selected Answer: CD\nC and D"},{"poster":"prshntdxt7","content":"Selected Answer: CD\nC. Implement query parallelization by partitioning the data output:\n\"Partitioning lets you divide data into subsets based on a partition key. If your input (for example Event Hubs) is partitioned by a key, it's highly recommended to specify this partition key when adding input to your Stream Analytics job. Scaling a Stream Analytics job takes advantage of partitions in the input and output. A Stream Analytics job can consume and write different partitions in parallel, which increases throughput.\"\n\nD. Scale the SU count for the job up:\n\"The total number of streaming units that can be used by a Stream Analytics job depends on the number of steps in the query defined for the job and the number of partitions for each step... All non-partitioned steps together can scale up to one streaming unit (SU V2s) for a Stream Analytics job. In addition, you can add 1 SU V2 for each partition in a partitioned step.\"","comment_id":"1133955","upvote_count":"1","timestamp":"1706435820.0"},{"comment_id":"1124405","content":"Selected Answer: CF\nAs there is no indication of any query parallelization currently, we have to choose to parallelize for both input and output as the first/correct answers.","poster":"sdg2844","upvote_count":"2","timestamp":"1705428300.0"},{"timestamp":"1703946480.0","poster":"Khadija10","comment_id":"1109783","upvote_count":"2","content":"Selected Answer: CF\nPartitioning lets you divide data into subsets based on a partition key. If your input (for example Event Hubs) is partitioned by a key, it's highly recommended to specify this partition key when adding input to your Stream Analytics job. Scaling a Stream Analytics job takes advantage of partitions in the input and output. A Stream Analytics job can consume and write different partitions in parallel, which increases throughput.\nRef: https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization"},{"upvote_count":"2","poster":"jongert","timestamp":"1703687580.0","comment_id":"1106905","content":"Selected Answer: CF\nAn embarrassingly parallel job allows the highest degree of parallelization. Looking at how the max number of stream units is calculated, it would not be useful to scale them up if you keep a bottleneck at the output. Unsure what a good reference value would be for the number of SUs, but 120 does not seem very low to me."},{"comment_id":"1095308","upvote_count":"1","poster":"d046bc0","content":"Selected Answer: CF\nScale the SU count for the job up - (ChatGPT) This will not necessarily improve the performance of your job, unless your query is CPU-bound or memory-bound. Scaling up the SU count will increase the amount of resources available for your job, but it will also increase the cost. You should first try to optimize your query by using parallelization and repartitioning techniques, and then scale up the SU count only if needed1","timestamp":"1702458360.0"},{"timestamp":"1702150500.0","comment_id":"1092003","content":"Selected Answer: DF\nChatgpt say DF :\nThe question in the image relates to optimizing the performance of an Azure Stream Analytics job. The correct actions would typically involve scaling the Streaming Units (SUs) appropriately based on the throughput needs and implementing query parallelization. In this context:\n\n- Scaling up the SU count (option D) would improve performance if the current SU allocation is insufficient.\n- Implementing query parallelization by partitioning the data input (option F) could also optimize performance as it would allow the job to process multiple data partitions concurrently.","upvote_count":"2","poster":"Momoanwar"},{"timestamp":"1696785660.0","content":"I would say DF is correct. Despite C being a correct option to optimize performance, we have no information about the output. If the output is Power BI, it does not support partition. Therefore we cannot state output partition without more information. Therefore best option will be SU","comment_id":"1028191","poster":"ellala","upvote_count":"3"},{"poster":"fahfouhi94","upvote_count":"3","timestamp":"1696059000.0","comment_id":"1021321","content":"Selected Answer: DF\nthe question is about actions should you perform, in case of power bi output , we cannot partition the stream analytics output.SO D & F"},{"content":"Selected Answer: CF\nImplement query parallelization by partitioning the data input (Option F): Parallelizing the query by partitioning the data input allows the Stream Analytics job to process multiple data streams concurrently, which can significantly improve performance, especially when dealing with a large volume of data.\n\nImplement query parallelization by partitioning the data output (Option C): Similar to partitioning the data input, partitioning the data output allows for parallel writing to the output sinks, which can also enhance performance.","comment_id":"999533","poster":"EliteAllen","timestamp":"1693919520.0","upvote_count":"3"},{"comment_id":"998430","upvote_count":"1","timestamp":"1693820160.0","poster":"kkk5566","content":"avoiding embarrassingly parallel jobs, I would go C &F"},{"upvote_count":"3","comment_id":"909343","poster":"dp_learner","timestamp":"1685360160.0","content":"\"An embarrassingly parallel job is the most scalable scenario in Azure Stream Analytics. It connects one partition of the input to one instance of the query to one partition of the output. This parallelism has the following requirements: \n...\n4. The number of input partitions must equal the number of output partitions.\" \n\n\nref : https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization"},{"upvote_count":"3","poster":"bakamon","timestamp":"1684904400.0","content":"Selected Answer: CF\ninput and output","comment_id":"905484"},{"poster":"rocky48","content":"Selected Answer: CF\nC. Implement query parallelization by partitioning the data output.\nF. Implement query parallelization by partitioning the data input.","timestamp":"1684342260.0","upvote_count":"3","comment_id":"900379"},{"timestamp":"1683391500.0","comment_id":"890883","poster":"dksks","upvote_count":"2","content":"Selected Answer: CD\nC. Implement query parallelization by partitioning the data output.\nD. Scale the SU count for the job up.\n\nExplanation:\n\nA higher SU count provides more processing power and can improve the performance of the Azure Stream Analytics job. Scaling up the job by increasing the SU count can reduce query latency and improve throughput.\n\nPartitioning the data output allows for query parallelization, which can improve the performance of the job. By dividing the output into partitions, the job can process data simultaneously, reducing the time required to complete the job."},{"content":"To optimize the performance of the Azure Stream Analytics job, you should perform the following two actions:\n\nC. Implement query parallelization by partitioning the data output. Partitioning the data output helps to distribute query processing across multiple partitions, which can improve performance for queries that require a large amount of processing power.\n\nD. Scale the SU count for the job up. Scaling up the number of Streaming Units (SU) will provide more processing power for the job, which can improve performance.\n\nTherefore, the correct answers are C and D. Implement query parallelization by partitioning the data output, and scale the SU count for the job up.","upvote_count":"3","timestamp":"1678164600.0","poster":"esaade","comment_id":"831560"},{"content":"C. Implement query parallelization by partitioning the data output.\nD. Scale the SU count for the job up.\n\nBy partitioning the data output, the query processing can be split into smaller, parallel tasks which can lead to better performance. Scaling up the SU count for the job increases the processing power available for the job, which can also lead to improved performance.\n\nNote: The specific optimizations required may vary based on the specific requirements and nature of the data analysis solution.","poster":"akk_1289","timestamp":"1675190520.0","upvote_count":"1","comment_id":"794504"},{"upvote_count":"4","poster":"VivekMadas","comment_id":"752119","timestamp":"1671617100.0","content":"Already 120 SU used (6 per node = 20 nodes) - Adding extra wont be any use.\nAnswer would be Partitioning Input & Output"},{"timestamp":"1671286920.0","poster":"vigilante89","content":"Selected Answer: CD\nD - Scale the SU count for the job up\nBecause the assigned streaming units might be low as compared to the streaming data analytics requirements.\n\nC - Implement parallization by partitioning the output data\nBecause any data to be analyzed should be partitioned for query optimization.","comment_id":"748110","upvote_count":"1"},{"upvote_count":"1","poster":"OldSchool","comment_id":"722056","content":"Selected Answer: CF\nBecause there is no mention of any bottleneck timestamping of stremed data my answer is CF","timestamp":"1668868500.0"},{"content":"My answer is CF.\n\nAs per the shared document we can't utilize parallelism with just partitioning the input when we don't partition the output. IF you can't utilize parallelism what is the point of partitioning the input \n\n\"Most of the outputs supported in Stream Analytics can take advantage of partitioning. If you use an output type that doesn't support partitioning your job won't be embarrassingly parallel. For Event Hubs output, ensure Partition key column is set to the same partition key used in the query. Refer to the output section for more details.\"","comment_id":"721990","timestamp":"1668861660.0","poster":"cosarac","upvote_count":"2"},{"comment_id":"704892","timestamp":"1666805340.0","upvote_count":"3","content":"The answer is CF.\nRead section \"Embarrassingly parallel jobs\" of the article:\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization","poster":"dmitriypo"},{"upvote_count":"4","comment_id":"639525","timestamp":"1659163260.0","poster":"Deeksha1234","content":"Selected Answer: DF\nD and F seems to be correct based on the reference article"},{"comments":[{"comment_id":"612689","upvote_count":"1","timestamp":"1654599000.0","poster":"Aditya0891","content":"A stream analytics job must include at least 1 input, 1 query and 1 output. Check this it's mentioned in the first part itself \"https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization\". So by default we need to take the output as well in consideration or else the whole streaming analytics won't work","comments":[{"comment_id":"612697","upvote_count":"1","content":"sorry my bad you are right, all the output don't have partition so we can't decide the output on our own. The answer should be DF","timestamp":"1654600380.0","poster":"Aditya0891"}]}],"content":"Selected Answer: DF\nAs the event consumer is not mentioned, partitioning output is not relevant. For instance, Power BI output doesn't currently support partitioning","comment_id":"612177","upvote_count":"4","poster":"HebaN","timestamp":"1654493100.0"},{"upvote_count":"1","comment_id":"597420","timestamp":"1651773600.0","poster":"Andushi","content":"Selected Answer: CF\nI agree with @manquak."},{"upvote_count":"4","poster":"DingDongSingSong","comment_id":"572485","timestamp":"1647893340.0","content":"I think the answer is correct. The two things you do is: 1. Scale up SU and 2. partition input. If this doesn't work, THEN you could partition output as well."},{"poster":"Dianova","content":"Selected Answer: DF\nI think answer is correct, because:\n Nothing is mentioned in the question about the output and some type of outputs do not support partitioning (like PowerBI), so it would be risky to assume that we can partition the output to implement Embarrassingly parallel jobs.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization#outputs\n\n Implementing query parallelization by partitioning the data input would be an optimization but the total number of SUs depends on the number of partitions, so the SUs would need to be scaled up.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization#calculate-the-maximum-streaming-units-of-a-job","comment_id":"548518","upvote_count":"10","timestamp":"1645010340.0"},{"poster":"PallaviPatel","upvote_count":"2","comment_id":"534665","timestamp":"1643371260.0","content":"Selected Answer: CF\nignore my previous answer C and F is correct."},{"poster":"PallaviPatel","upvote_count":"1","content":"Selected Answer: DF\ncorrect","comment_id":"534664","timestamp":"1643371080.0"},{"upvote_count":"1","poster":"assU2","timestamp":"1642713420.0","comment_id":"528819","content":"Selected Answer: CF\nPartitioning lets you divide data into subsets based on a partition key. If your input (for example Event Hubs) is partitioned by a key, it is highly recommended to specify this partition key when adding input to your Stream Analytics job. Scaling a Stream Analytics job takes advantage of partitions in the input and output. \n\nMore to say scaling is not an optimization"},{"timestamp":"1642713060.0","poster":"assU2","upvote_count":"1","comment_id":"528818","content":"Is scaling an optimization??"},{"upvote_count":"1","comment_id":"527251","poster":"DE_Sanjay","timestamp":"1642570620.0","content":"C & F Should be the right answer."},{"timestamp":"1642305960.0","comment_id":"524610","poster":"dev2dev","content":"Optimization is always about improving performance using existing resources. So definitly not increasing SKU or SU","upvote_count":"6"},{"timestamp":"1642120800.0","poster":"alex623","content":"I think the answer is to partitioning input and output, because the target is to optimize regardless of computing capacity (#SUs)","comments":[{"comment_id":"572486","upvote_count":"2","poster":"DingDongSingSong","timestamp":"1647893400.0","content":"who says optimization is regardless of computing capacity. Infact computing capacity increase is ONE of the ways to optimize performance."}],"upvote_count":"2","comment_id":"523220"},{"comment_id":"521547","content":"Selected Answer: CF\nShould always aim for Embarrassingly parallel jobs (partitioning input, job and output) https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization\n\nUpping the computing power of a resource (SUs in this case) should never be classed as 'optimisation' like the question asks.","timestamp":"1641906180.0","upvote_count":"6","comments":[{"comment_id":"524611","upvote_count":"1","timestamp":"1642306020.0","poster":"dev2dev","content":"I agree"},{"comment_id":"712030","timestamp":"1667686620.0","poster":"Igor85","upvote_count":"1","content":"indeed, ridiculously suboptimal code still can run fast, if you're allowed to access the computation power slider"}],"poster":"Jaws1990"},{"poster":"trietnv","upvote_count":"2","comment_id":"507014","content":"Selected Answer: BF\nChoosing the number of required SUs for a particular job depends on the partition configuration for \"the inputs\" and \"the query\" that's defined within the job. The Scale page allows you to set the right number of SUs. It is a best practice to allocate more SUs than needed.\nref: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption","timestamp":"1640171820.0"},{"upvote_count":"3","content":"The Answer Is Correct, You Scall Up Streaming Units And Partition Input So The Input Events Are More Efficient To Process.","comment_id":"502050","timestamp":"1639563720.0","poster":"Sayour"},{"content":"C and F","upvote_count":"1","comment_id":"501601","timestamp":"1639506660.0","poster":"m2shines"},{"content":"Selected Answer: CF\nPartition input and output is the correct answer even if output is not mentioned because stream analytics always have at least one output.","comments":[{"upvote_count":"1","content":"what if the output is poweBI? Power BI doesn't support partition","comment_id":"617582","poster":"Aditya0891","timestamp":"1655449860.0"}],"upvote_count":"1","timestamp":"1638870540.0","comment_id":"495847","poster":"rashjan"},{"comment_id":"477538","timestamp":"1636814820.0","upvote_count":"3","poster":"MALFOY","content":"Partition input and output."},{"poster":"RinkiiiiiV","comment_id":"456875","upvote_count":"5","content":"Provided Answers are correct","comments":[{"upvote_count":"2","comment_id":"458668","poster":"mahi18091998","content":"How do you know ?","timestamp":"1633601880.0"}],"timestamp":"1633322460.0"},{"content":"I think given answer is correct: Partitioning inputs creates larger amount of SU, which is crucial for Streaming Units utlilty. \nScaling up helps to maintain memoray usage at low leves, as it is recomended.","timestamp":"1631897460.0","poster":"Julius7000","upvote_count":"2","comment_id":"446712"},{"poster":"Liz42","content":"Not a lot of conversation here… do we agree that partitioning the input & output is the best solution?","upvote_count":"7","comment_id":"445505","timestamp":"1631745360.0"},{"poster":"[Removed]","content":"There has to be some information missing. The link describes how you count the streaming units. The questions give a number but does not tell us what we are working with.","timestamp":"1630055400.0","upvote_count":"6","comment_id":"432929"}],"answer":"CF"},{"id":"2fS90rDExprDRAnG0Non","exam_id":67,"unix_timestamp":1622644560,"answer_description":"The conditional split transformation routes data rows to different streams based on matching conditions. The conditional split transformation is similar to a CASE decision structure in a programming language. The transformation evaluates expressions, and based on the results, directs the data row to the specified stream.\nBox 1: dept=='ecommerce', dept=='retail', dept=='wholesale'\nFirst we put the condition. The order must match the stream labeling we define in Box 3.\nSyntax:\n<incomingStream>\nsplit(\n<conditionalExpression1>\n<conditionalExpression2>\n...\ndisjoint: {true | false}\n) ~> <splitTx>@(stream1, stream2, ..., <defaultStream>)\n\nBox 2: discount : false -\ndisjoint is false because the data goes to the first matching condition. All remaining rows matching the third condition go to output stream all.\nBox 3: ecommerce, retail, wholesale, all\n\nLabel the streams -\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split","question_id":172,"url":"https://www.examtopics.com/discussions/microsoft/view/54230-exam-dp-203-topic-2-question-20-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0018500001.png"],"answer":"","isMC":false,"question_text":"DRAG DROP -\nYou need to create an Azure Data Factory pipeline to process data for the following three departments at your company: Ecommerce, retail, and wholesale. The solution must ensure that data can also be processed for the entire company.\nHow should you complete the Data Factory data flow script? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0018400002.png"],"timestamp":"2021-06-02 16:36:00","discussion":[{"content":"I think \"disjoint\" should be True, so that data can be sent to all matching conditions. In this way the \"all\" output can get the data from every department, which ensures that \"data can also be processed by the entire company\".","comments":[{"content":"agree with you, disjount = true","poster":"gssd4scoder","upvote_count":"3","comment_id":"467788","timestamp":"1635221880.0"},{"poster":"DataSaM","timestamp":"1688383800.0","comment_id":"941780","content":"Disagree, all is like an else","upvote_count":"3"},{"poster":"MrityunjayPrabhat","comment_id":"665377","upvote_count":"6","comments":[{"poster":"kkk5566","upvote_count":"2","timestamp":"1693200180.0","content":"disjoint is false because the data goes to the first matching condition rather than all matching conditions.","comment_id":"991812"}],"content":"All is not defined in split so it has to be false. Refer \nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split#:~:text=CleanData%0A%20%20%20%20split(%0A%20%20%20%20%20%20%20%20year%20%3C%201960%2C%0A%09%20%20%20%20year%20%3E%201980%2C%0A%09%20%20%20%20disjoint%3A%20false%0A%20%20%20%20)%20~%3E%20SplitByYear%40(moviesBefore1960%2C%20moviesAfter1980%2C%20AllOtherMovies)","timestamp":"1662811440.0"},{"comment_id":"460484","timestamp":"1633943520.0","content":"yes it's True :Disjoint=True","upvote_count":"4","poster":"jed_elhak"},{"poster":"rav009","timestamp":"1632144060.0","comment_id":"448249","content":"agree, disjoint=true means the record will go through all the condition.","upvote_count":"4"},{"timestamp":"1624261680.0","content":"I concur with @Alekx42 thought. Since we want to process for each dept (3 streams), then we must ensure we can still process for ALL depts at the same time (4th or default stream), hence DISJOINT:TRUE. Else, DISJOINT:FALSE.","poster":"Steviyke","comment_id":"386885","upvote_count":"9"}],"poster":"Alekx42","comment_id":"372788","timestamp":"1622644560.0","upvote_count":"85"},{"poster":"mayank","upvote_count":"47","timestamp":"1622742600.0","content":"As per the link provided in the explanation disjoint:false looks correct. I believe you must go through the link https://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split and choose you answer for disjoint wisely . I will go with \"False\"","comments":[{"comment_id":"539404","poster":"dev2dev","timestamp":"1643864400.0","content":"you also need to read question to understand requirement. I will choose disjoint: true","upvote_count":"4"},{"upvote_count":"2","content":"From the link you've posted:\ndisjoint is false because the data goes to the first matching condition rather than all matching conditions. \nSo the correct answer is True, considering we have to \"duplicate\" records for the ALL category.","poster":"auwia","comment_id":"929897","timestamp":"1687379040.0"}],"comment_id":"373785"},{"upvote_count":"2","timestamp":"1722028140.0","poster":"f2a9aa5","comments":[{"content":"typo: replace default by disjoint","comment_id":"1255903","poster":"f2a9aa5","timestamp":"1722028560.0","upvote_count":"1"}],"content":"Guys, Guys, Guys.....\n\nThe clue is in the options given. \n\nIf default: true was the right answer, then options A and F would be the same. Either of them could have fulfilled the criteria.\n\nIf only one of them is right (and that is what we expect) then the order matters. And order only matters when default: false.\n\nThis also means that 'all' is slightly misleading. Refer back to the question: it does not imply all of the data needs to be available, just that the 'entire company can process'. Which is still okay if the 'all' had everything but ecommerce, retail and wholesale.\n\nFinal point: If default: true was the right answer, options B and C would be the same. Either of the them could have worked.\n\nConclusion: default: false.","comment_id":"1255901"},{"upvote_count":"1","comment_id":"1247992","timestamp":"1720998540.0","poster":"evangelist","content":"disjoint: true: If set to true, the row will be sent to all matching conditions. This means that a single row can appear in multiple output streams if it matches multiple conditions.\ndisjoint: false: If set to false, the row will be sent to the first matching condition only. Once a row matches a condition, it will not be evaluated against subsequent conditions."},{"upvote_count":"2","poster":"alphilla","comment_id":"1098783","timestamp":"1702807740.0","content":"Guys Disjoint is True 110% and I will tell you why.\n\ndisjoint: false means that rows will be directed to the first branch whose condition is satisfied, and subsequent conditions are ignored.\nThis might not fulfill the requirement because you want to process data for multiple departments, and with disjoint: false, a row would go to the first department branch it satisfies, ignoring the other departments.\n\nDisjoint TRUE is more appropriate because it fulfills the requirement of processing data for individual departments (Ecommerce, retail, and wholesale) while also handling data for the entire company. Because all rows will match 2 conditions: \n1st conditon. They will have one of the three depts\n2nd Condition. They will match the all condition\n\nThat's why it MUST BE TRUE."},{"poster":"kkk5566","timestamp":"1693200840.0","content":"False is correct","comment_id":"991819","upvote_count":"2"},{"poster":"orionduo","upvote_count":"1","timestamp":"1687402380.0","content":"I think the disjoint should be 'False'\nBy setting \"disjoint true\" for activities in a pipeline, you are essentially indicating that these activities are independent and can be executed concurrently. This can help improve the overall performance and efficiency of the pipeline by allowing for parallel execution of activities that do not have any interdependencies.","comment_id":"930093"},{"comments":[{"content":"The disjoint option in a split transformation determines whether the output streams are mutually exclusive or not. If disjoint is set to true, then each row of data can only be sent to one output stream. If disjoint is set to false, then a single row of data can be sent to multiple output streams.\n\nIn this case, setting disjoint to false allows for data to be processed for the entire company as well as for individual departments. This means that a single row of data can be sent to multiple output streams, allowing for processing at both the department and company level.","comment_id":"905926","upvote_count":"2","poster":"bakamon","timestamp":"1684935240.0"}],"content":"CleanData split(dept==‘ecommerce’, dept==‘retail’, dept==‘wholesale’) ~> SplitByDept@(disjoint: false)\n\nThis will split the data by department and allow for processing of data for the entire company as well as for individual departments.","poster":"bakamon","timestamp":"1684935120.0","upvote_count":"2","comment_id":"905923"},{"timestamp":"1678974780.0","comment_id":"841008","upvote_count":"4","poster":"markpumc","content":"disjoin = true if you want all , if disjoint = false, nothing in ALL split"},{"timestamp":"1675439460.0","content":"Disjoint=False","poster":"DPMishra","upvote_count":"1","comment_id":"797170"},{"poster":"DindaS","upvote_count":"4","content":"disjoint=false \n\nThe below example is a conditional split transformation named SplitByYear that takes in incoming stream CleanData. This transformation has two split conditions year < 1960 and year > 1980. disjoint is false because the data goes to the first matching condition rather than all matching conditions. Every row matching the first condition goes to output stream moviesBefore1960. All remaining rows matching the second condition go to output stream moviesAFter1980. All other rows flow through the default stream AllOtherMovies.\nfrom https://learn.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split","timestamp":"1674406560.0","comment_id":"784502"},{"content":"Given answer correct","poster":"nadahef","timestamp":"1672326180.0","upvote_count":"2","comment_id":"761166"},{"comment_id":"725258","timestamp":"1669220640.0","content":"The given answer is 100000% crct, don't confuse with others","poster":"Maddhy","upvote_count":"2"},{"upvote_count":"6","comment_id":"663747","poster":"Aslam208","timestamp":"1662651420.0","content":"Given answer is 100% correct"},{"content":"Everyone is discussing about disjoint. But if disjoint is true then there is no ordering required of ecommerce,retail,wholesale, all .so we can fill 1st option with 2 or 3 and 3rd option with 1 or 6.","poster":"kiranSargar","upvote_count":"2","comment_id":"614991","timestamp":"1654956900.0"},{"poster":"nefarious_smalls","comment_id":"599293","timestamp":"1652128260.0","content":"I think it should be disjoint is True based on microsofts example. it states that when disjoint is false each row will only go to the first matching condition. However in the example I believe each row will go to its matching department plus an aggregate stream that takes in every value regardless. Hence disjoint should be true","upvote_count":"1"},{"upvote_count":"2","content":"Definetely Disjoint=Trues as per Microsoft doc","poster":"Andushi","comment_id":"594403","timestamp":"1651229700.0"},{"content":"Answer: Disjoint=False \nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split\nExample\nThe below example is a conditional split transformation named SplitByYear that takes in incoming stream CleanData. This transformation has two split conditions year < 1960 and year > 1980. disjoint is false because the data goes to the first matching condition. Every row matching the first condition goes to output stream moviesBefore1960. All remaining rows matching the second condition go to output stream moviesAFter1980. All other rows flow through the default stream AllOtherMovies.","comments":[{"comment_id":"542157","timestamp":"1644210240.0","poster":"kilowd","upvote_count":"3","content":"Disjoint = True \nIf true then split on all matching conditions, if false then only split on the first matching condition."},{"comments":[{"comment_id":"582644","content":"sorry I meant to start off by saying \"The example has false\"","timestamp":"1649377920.0","upvote_count":"1","poster":"Onobhas01"}],"comment_id":"582642","poster":"Onobhas01","upvote_count":"3","timestamp":"1649377860.0","content":"The example has true as the data matches only one condition, it's either before 1960, after 1980 or Else... no two dataset matches more than one condition. But in the question they match more than one condition so disjoint has to be true."}],"timestamp":"1644209640.0","poster":"kilowd","upvote_count":"2","comment_id":"542150"},{"content":"I agree with yolap31172.","comment_id":"535497","timestamp":"1643465580.0","poster":"PallaviPatel","upvote_count":"1"},{"comments":[{"comments":[{"timestamp":"1644210120.0","comment_id":"542155","upvote_count":"2","poster":"kilowd","content":"dev2dev is correct ..."}],"timestamp":"1644209760.0","upvote_count":"1","poster":"kilowd","comment_id":"542153","content":"Its the other way round"}],"content":"What exactly \"disjoint\" mean? its basically saying weather you want to skip subsequent conditions once a condition is satisfied(disjoint = false) or you want to check all conditions (disjoint = true).","comment_id":"528206","timestamp":"1642662000.0","upvote_count":"4","poster":"dev2dev"},{"comment_id":"527776","timestamp":"1642613640.0","content":"If Disjoint=True, then split on all matching conditions;\nIf Disjoint=False, then only split on the first matching condition;\nSee: https://docs.varigence.com/biml-reference/language-reference/Varigence.Languages.Biml.DataFactory.AstAdfDataflowConditionalSplitTransformationNode","poster":"snna4","upvote_count":"2"},{"content":"I made test.\nI select \"All matching conditions\" (disjoint=true)\nthen all rows are returned to default output (\"All\" in this question)\nSo, the correct answer is disjount = true","poster":"marcin1212","comment_id":"505064","upvote_count":"9","timestamp":"1639945680.0"},{"content":"Disjoint should be false: \nCleanData\n split(\n year < 1960,\n year > 1980,\n disjoint: false\n ) ~> SplitByYear@(moviesBefore1960, moviesAfter1980, AllOtherMovies)\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split","poster":"avijitd","upvote_count":"3","comment_id":"499722","timestamp":"1639267620.0"},{"content":"The correct answer is Disjoint = True \nbecause Disjoint = False would be used if the goal was to bucket \"all other depts\" instead of \"all as in everything, whatever the dpt is\" as it is here requested.\n\ncf \"Conditional split transformation in mapping data flow\" at\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split","poster":"clement_","comment_id":"486718","timestamp":"1637850720.0","upvote_count":"4"},{"upvote_count":"1","content":"Sorry, tiny correction: The current ExamTopic answer of (disjoint=false) is incorrect. Disjoint should be set to TRUE. There is a need to ‘disjoin’ (evaluate more than just the first condition, in fact ALL conditions need to be evaluated) from the first condition.","timestamp":"1636064520.0","comment_id":"472786","poster":"Sasha_in_San_Francisco"},{"upvote_count":"3","poster":"Sasha_in_San_Francisco","content":"I figured it out. Disjoint must be manually set to True to process more than the first condition. Its default value is False or “First matching condition”.\nIn this url,\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split\nthe screenshot under “In the Service UI” should have shown the option [x] All matching conditions, which sets the ‘disjoint’ option to ‘True’.\nI believe the answer is correct, BUT the Microsoft documentation is poorly written.","comment_id":"472776","timestamp":"1636063260.0"},{"upvote_count":"2","comment_id":"471776","content":"disjoint = false is absolutely correct please look into the below link.\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split","timestamp":"1635878460.0","poster":"hryniewka"},{"content":"Disjoint- If true then split on all matching conditions, if false then only split on the first matching condition. Therefore, it should be set to True.","poster":"azurearmy","comment_id":"467146","upvote_count":"2","timestamp":"1635112800.0"},{"upvote_count":"1","content":"I think Disjoin=True because false = other dept (!= ecommerce,!=retail,!=...) but true = all match condition (=ecommerce,=retail,=...)","comment_id":"464917","poster":"nummpetch","timestamp":"1634704920.0"},{"timestamp":"1634066040.0","comments":[{"upvote_count":"4","comments":[{"comment_id":"505686","upvote_count":"6","poster":"onyerleft","content":"Yes! The conditions are mutually exclusive, so it doesn't matter how disjoint is set; the results will be the same. In your example, a movie from 1992 will be split to 'moviesAfter1980' AND 'moviesAfter1990' if disjoint = true. If disjoint = false, a movie from 1992 will only be sent to the 'moviesAfter1980' output.","timestamp":"1640037000.0"}],"content":"Agree! For this scenario, there is no way that one department match more than 2 conditions, so there is no difference for disjoint = true or false.\n\nIf condition as\nCleanData\n split(\n year < 1960,\n year > 1980,\n year > 1990\n disjoint: true\n ) ~> SplitByYear@(moviesBefore1960, moviesAfter1980, moviesAfter1990, AllOtherMovies)","timestamp":"1639969680.0","comment_id":"505199","poster":"Canary_2021"}],"content":"Looks like there's no consensus about disjoint. For me, it looks like it doesn't matter. There's no way given conditions can overlap (a record can only belong to one department) and the 'all' section will only be selected for \"rows that don't match any condition\", ie. a record that doesn't belong to any of specified departments. In other words, if a record has department = 'retail', it will NOT go to 'all', regardless of disjoint setting.","comment_id":"461237","upvote_count":"8","poster":"yolap31172"},{"timestamp":"1628981280.0","comments":[{"content":"They are split because it doesn't matter in this case. The conditions are mutually exclusive so it doesn't matter if it's matching on all or the first. Outputs are the same","upvote_count":"4","timestamp":"1640037060.0","poster":"onyerleft","comment_id":"505689"}],"poster":"brendy","comment_id":"424965","upvote_count":"3","content":"The top votes are split, any consensus?"},{"content":"Answer is correct. Refer below Microsoft doc\n https://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split","upvote_count":"2","timestamp":"1625909700.0","comment_id":"403248","poster":"Vaishnav"},{"upvote_count":"4","timestamp":"1624796580.0","poster":"escoins","comment_id":"392031","content":"The provided link handles with \"all other\", we have the situation here with \"all\". Therefore I think disjoint:true should be correct."}],"topic":"2","answer_ET":""},{"id":"ENPIidtXNsSu1D9hYgHK","discussion":[{"comment_id":"355849","content":"I think the correct order is:\n1) mount onto DBFS\n2) read into data frame\n3) transform data frame\n4) specify temporary folder\n5) write to table in SQL data warehouse\n\nAbout temporary folder, there is a note explain this:\nhttps://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse#load-data-into-azure-synapse\n\nDiscussions about this question:\nhttps://www.examtopics.com/discussions/microsoft/view/11653-exam-dp-200-topic-2-question-30-discussion/","comments":[{"timestamp":"1645875360.0","poster":"satyamkishoresingh","upvote_count":"3","content":"This order is absolutely correct.","comment_id":"432139"},{"upvote_count":"11","comments":[{"content":"hehe, those who understand sql dw = azure synapse :D","comments":[{"comment_id":"1000606","timestamp":"1709736000.0","content":"it is the incorrect answer","poster":"kkk5566","upvote_count":"1"}],"poster":"dev2dev","upvote_count":"36","timestamp":"1658322180.0","comment_id":"528498"}],"content":"OMG... the 5th step should be \"Write the results to a table in Azure synapse\". Who are those people \"liked\" this answer? Guys, just read the task.","timestamp":"1658247240.0","comment_id":"527805","poster":"snna4"},{"poster":"GameLift","upvote_count":"2","timestamp":"1646921880.0","content":"I agree, although, why do we need a temporary folder? We already have storage blob as temporary storage?","comment_id":"442474","comments":[{"content":"Databricks uses polybase to write to Synapse and thus we need to stage the file is required.","timestamp":"1671425880.0","upvote_count":"2","comment_id":"618504","poster":"gaganmay26"}]},{"comment_id":"393273","timestamp":"1640736000.0","poster":"andylop04","content":"Today I received this question in my exam. Only appeared the 5 options of this response. I only had to order, not choice. This solutions is the correct. Thanks sagga.","upvote_count":"41","comments":[{"poster":"KingIlo","upvote_count":"4","content":"Correct also received the only five options.\nAlso see: \nhttps://www.examtopics.com/discussions/microsoft/view/11653-exam-dp-200-topic-2-question-30-discussion/","comment_id":"480881","timestamp":"1652886120.0"}]},{"timestamp":"1636910940.0","content":"Hi sagga! Thank you. I do agree....","poster":"labasmuse","upvote_count":"2","comment_id":"357319","comments":[{"timestamp":"1640225100.0","content":"fix solution on site","upvote_count":"3","comment_id":"388385","poster":"InvisibleShadow"}]},{"upvote_count":"2","comment_id":"1196165","content":"the given answer is correct!","timestamp":"1729016220.0","poster":"tadenet"}],"poster":"sagga","upvote_count":"172","timestamp":"1636768440.0"},{"comment_id":"377536","upvote_count":"22","timestamp":"1638975060.0","comments":[{"content":"transformations on dataframe, not on the file.","upvote_count":"7","timestamp":"1684760400.0","comment_id":"724388","poster":"Tickxit"}],"poster":"Miris","content":"1) mount the data onto DBFS\n2) Read the file into a data frame\n3) Perform transformations on the file\n4) Specify a temporary folder to stage the data\n5) Write the results to a table in Azure synapse"},{"comment_id":"1092126","poster":"Momoanwar","upvote_count":"1","content":"Answer are correct, chatgpt say :\nTo accomplish the task in an Azure Databricks notebook, the logical sequence of actions would be:\n\n1. **Mount the Data Lake Storage onto DBFS**: This allows access to the JSON file stored in Azure Data Lake Storage using the Databricks File System.\n\n2. **Read the file into a data frame**: Use Spark to read the JSON file into a DataFrame for processing.\n\n3. **Perform transformations on the data frame**: Apply transformations to concatenate the FirstName and LastName fields to create a new column.\n\n4. **Specify a temporary folder to stage the data**: Before writing the data to Azure Synapse, it is a common practice to stage it in a temporary folder.\n\n5. **Write the results to a table in Azure Synapse**: Finally, write the transformed DataFrame to the destination table in Azure Synapse Analytics. \n\nThese steps would ensure the JSON file data is properly transformed and loaded into Azure Synapse Analytics for further use.","timestamp":"1717966500.0"},{"content":"Just remember the initials first: M.R.P.S.W then go to the details.","upvote_count":"1","timestamp":"1716466380.0","comment_id":"1078474","poster":"EliteAllen"},{"upvote_count":"3","comment_id":"905939","poster":"bakamon","content":"1. Mount the data lake storage onto DBFS.\n2. Read the file into a data frame.\n3. Perform transformations on the data frame.\n4. Specify a temporary folder to stage the data.\n5. Write the results to a table in Azure Synapse.\nThis will allow you to read the data from the JSON file into a data frame, perform the necessary transformations to concatenate the FirstName and LastName values, and then write the results to a table in Azure Synapse.","timestamp":"1700840760.0"},{"comment_id":"640166","poster":"Deeksha1234","timestamp":"1675188180.0","upvote_count":"1","content":"answer is correct, explained by the reference link in the given solution"},{"comments":[{"timestamp":"1671488760.0","content":"I want to know the reason too!","comment_id":"618844","upvote_count":"1","poster":"Davico93"}],"upvote_count":"3","timestamp":"1667657880.0","poster":"carloalbe","comment_id":"597297","content":"I don not see the reason why \"specify temporary folder\" can not be both before or after the \"read and transformation phase\""},{"timestamp":"1666031340.0","poster":"Egocentric","comment_id":"587346","upvote_count":"1","content":"given answer is correct, after reading and rereading stand with the given answer"},{"poster":"Sandip4u","timestamp":"1657419840.0","upvote_count":"3","content":"I think the correct order is:\n1) mount onto DBFS\n2) read into data frame\n3) transform data frame\n4) specify temporary folder\n5) write to table in SQL data warehouse","comment_id":"520631"},{"timestamp":"1655728080.0","comment_id":"505478","content":"Here is my answer.\n1) Create a service principal - Not sure why this step is not a choice in this question. I don't thing need to mount onto DBFS, but you do need to assign permission to allow databricks talk with Data Lake and read file.\n2) Read the file into data frame\n3) Perform transformations on the data frame \n Data have been read into data from, so should transform data from data frame, not data file.\n4) Specify temporary folder to stage the data\n5) Write the results to a table in Azure Synapse\n\nI reviewed this online document. No any place mentioned that the data frame needs to be dropped. \nhttps://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse","upvote_count":"2","comments":[{"content":"you do not need to create a service principal, this is already exist","timestamp":"1662687660.0","comment_id":"563737","poster":"Gina8008","upvote_count":"1"}],"poster":"Canary_2021"},{"comment_id":"502101","poster":"Sayour","timestamp":"1655287740.0","content":"There Is A Contradiction Between Answers On The Drag & Drop And The Answers In The Steps Listing, And I Think The Correct Ones Are That In The Listing And Not The Drag & Drop.","upvote_count":"1"},{"poster":"VJPR","upvote_count":"1","timestamp":"1655212800.0","content":"1) Mount the data onto DBFS\n2) Read the file into a data frame\n3) Perform transformations \n4) Specify a temporary folder to stage the data\n5) Write the results to a table in Azure synapse","comment_id":"501461"},{"comment_id":"430391","upvote_count":"1","timestamp":"1645671120.0","content":"The given answer is correct, after read the link provided carefully several times. There's already a service principal. With that, it's no need to mount. You do need to drop the dataframe as the last step.","poster":"[Removed]","comments":[{"poster":"GameLift","comment_id":"442472","comments":[{"timestamp":"1668033840.0","poster":"nefarious_smalls","upvote_count":"1","comment_id":"599295","content":"Actually you can assign a service principal to any data bricks account and use OAuth to connect with its tenant id app secret, and app id. You can then mount the data lake to databricks."}],"timestamp":"1646921820.0","content":"Service Principal has nothing to do with DataBricks.","upvote_count":"4"}]},{"comments":[{"comment_id":"573946","poster":"hello2tomoki","comments":[{"comment_id":"1000603","timestamp":"1709735820.0","content":"Correct","poster":"kkk5566","upvote_count":"1"}],"upvote_count":"3","timestamp":"1663966080.0","content":"Step 1: Read the file into a data frame.\nYou can load the json files as a data frame in Azure Databricks.\nStep 2: Perform transformations on the data frame.\nStep 3:Specify a temporary folder to stage the data\nSpecify a temporary folder to use while moving data between Azure Databricks and Azure SQL Data Warehouse.\nStep 4: Write the results to a table in Azure synapse\nStep 5: Drop the data frame - Clean up resources. \n\nhttps://www.examtopics.com/discussions/microsoft/view/11653-exam-dp-200-topic-2-question-30-discussion/"},{"upvote_count":"6","timestamp":"1637208900.0","poster":"Wisenut","comment_id":"359964","content":"I believe you perform transformation on the data frame and not on the file"},{"poster":"ThiruthuvaRajan","content":"you should not perform transformation on the file.\nYou need not to drop the dataframe. \nsagga options are correct","comment_id":"371652","upvote_count":"3","timestamp":"1638349980.0"}],"timestamp":"1636807500.0","upvote_count":"5","poster":"labasmuse","content":"Correct solution: \nRead the file into a data frame\nPerform transformations on the file\nSpecify a temporary folder to stage the data\nWrite the results to a table in Azure synapse\nDrop the data frame","comment_id":"356267"}],"question_text":"DRAG DROP -\nYou have an Azure Data Lake Storage Gen2 account that contains a JSON file for customers. The file contains two attributes named FirstName and LastName.\nYou need to copy the data from the JSON file to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values.\nYou create the following components:\n✑ A destination table in Azure Synapse\n✑ An Azure Blob storage container\n✑ A service principal\nWhich five actions should you perform in sequence next in is Databricks notebook? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","isMC":false,"unix_timestamp":1620863640,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0018600004.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0018700001.png"],"answer":"","answer_description":"Step 1: Mount the Data Lake Storage onto DBFS\nBegin with creating a file system in the Azure Data Lake Storage Gen2 account.\nStep 2: Read the file into a data frame.\nYou can load the json files as a data frame in Azure Databricks.\nStep 3: Perform transformations on the data frame.\nStep 4: Specify a temporary folder to stage the data\nSpecify a temporary folder to use while moving data between Azure Databricks and Azure Synapse.\nStep 5: Write the results to a table in Azure Synapse.\nYou upload the transformed data frame into Azure Synapse. You use the Azure Synapse connector for Azure Databricks to directly upload a dataframe as a table in a Azure Synapse.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse","exam_id":67,"timestamp":"2021-05-13 01:54:00","url":"https://www.examtopics.com/discussions/microsoft/view/52573-exam-dp-203-topic-2-question-21-discussion/","topic":"2","question_id":173,"answer_ET":"","answers_community":[]},{"id":"fSVoyLTavK6N3vdFHGjE","unix_timestamp":1623660840,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0018900001.png"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0018800004.png"],"exam_id":67,"discussion":[{"timestamp":"1639479240.0","upvote_count":"52","poster":"Puneetgupta003","content":"ANswers are correct","comment_id":"381715"},{"comment_id":"1102986","upvote_count":"7","content":"Got this question today on the exam","timestamp":"1718998020.0","poster":"positivitypeople"},{"content":"Both correct","upvote_count":"1","timestamp":"1727617980.0","poster":"Alongi","comment_id":"1185559"},{"timestamp":"1709736480.0","content":"Answers are correct","poster":"kkk5566","comment_id":"1000614","upvote_count":"1"},{"comment_id":"647046","timestamp":"1676447340.0","poster":"Deeksha1234","content":"correct","upvote_count":"4"},{"upvote_count":"2","content":"Answers are correct","poster":"StudentFromAus","comment_id":"617527","timestamp":"1671259440.0"},{"poster":"parx","comment_id":"583511","timestamp":"1665353700.0","content":"Correct. https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison","upvote_count":"4"},{"upvote_count":"2","comments":[{"upvote_count":"10","timestamp":"1650953280.0","comment_id":"467828","poster":"aaaaaaaan","content":"because we also wanna do backfill with past data. Technically, the event-based trigger will also allows ADF to find all the old files from the source which ADF hasn't processed yet (and we could add a datetime filter when loading the data) but ADF is gonna choke on so many past events from experience. With tumbling windows, the trigger will kick off for each 30 minutes slices of the time span, emulating batch loads. be very careful when doing backfill with a tumbling window, by default, ADF will start 50 concurrent pipelines, it can be pricey, change the settings in advanced panel of the trigger creation form."}],"content":"Why can't we use an event-based trigger here?","comment_id":"467151","timestamp":"1650838440.0","poster":"azurearmy"},{"poster":"belha","comment_id":"394550","comments":[{"comment_id":"399001","poster":"captainbee","timestamp":"1641378780.0","content":"As the solution says, you cannot use the Delay with Schedule.","upvote_count":"7"}],"timestamp":"1640862360.0","content":"not schedule ?","upvote_count":"2"},{"timestamp":"1640616420.0","comment_id":"392052","content":"why not schedule trigger?","poster":"escoins","upvote_count":"1","comments":[{"comment_id":"441149","content":"Schedule trigger would not work because backfill is only possible with Tumbling window trigger. In this case, we need to use trigger for old data.","timestamp":"1646706060.0","poster":"Podavenna","upvote_count":"8"}]}],"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/55302-exam-dp-203-topic-2-question-22-discussion/","timestamp":"2021-06-14 10:54:00","answers_community":[],"question_id":174,"answer":"","answer_description":"Box 1: Tumbling window -\nTo be able to use the Delay parameter we select Tumbling window.\nBox 2:\nRecurrence: 30 minutes, not 32 minutes\nDelay: 2 minutes.\nThe amount of time to delay the start of data processing for the window. The pipeline run is started after the expected execution time plus the amount of delay.\nThe delay defines how long the trigger waits past the due time before triggering a new run. The delay doesn't alter the window startTime.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger","question_text":"HOTSPOT -\nYou build an Azure Data Factory pipeline to move data from an Azure Data Lake Storage Gen2 container to a database in an Azure Synapse Analytics dedicated\nSQL pool.\nData in the container is stored in the following folder structure.\n/in/{YYYY}/{MM}/{DD}/{HH}/{mm}\nThe earliest folder is /in/2021/01/01/00/00. The latest folder is /in/2021/01/15/01/45.\nYou need to configure a pipeline trigger to meet the following requirements:\n✑ Existing data must be loaded.\n✑ Data must be loaded every 30 minutes.\n✑ Late-arriving data of up to two minutes must be included in the load for the time at which the data should have arrived.\nHow should you configure the pipeline trigger? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","topic":"2","isMC":false},{"id":"Uitifec27sgw0G5CzV3e","question_id":175,"topic":"2","timestamp":"2021-06-06 18:46:00","unix_timestamp":1622997960,"discussion":[{"comments":[{"timestamp":"1638870060.0","content":"Agreed. So easy that even ExamTopics got it right.","upvote_count":"171","comments":[{"content":"I badly needed a laugh after studying...LOL","poster":"eskimolight","timestamp":"1707024840.0","upvote_count":"3","comment_id":"971650"},{"poster":"shaileshutd","timestamp":"1685302260.0","upvote_count":"4","comment_id":"729631","content":"super like for this comment"},{"poster":"[Removed]","content":"The best comment ever :)","comment_id":"459799","upvote_count":"14","timestamp":"1649547000.0"},{"comment_id":"468360","upvote_count":"5","poster":"gssd4scoder","timestamp":"1651032300.0","content":"lol :)"}],"comment_id":"376602","poster":"captainbee"}],"timestamp":"1638816360.0","content":"Answer is correct","comment_id":"376217","upvote_count":"70","poster":"Sunnyb"},{"upvote_count":"6","content":"Got this question today on the exam","poster":"positivitypeople","timestamp":"1718998020.0","comment_id":"1102985"},{"timestamp":"1727619540.0","content":"I really hope this question comes up on the exam","comment_id":"1185573","upvote_count":"1","poster":"Alongi"},{"poster":"kkk5566","content":"correct","upvote_count":"1","timestamp":"1709736540.0","comment_id":"1000617"},{"upvote_count":"2","comment_id":"761180","poster":"nadahef","timestamp":"1688044500.0","content":"We agee on this"},{"timestamp":"1675189740.0","poster":"Deeksha1234","content":"Correct solution","upvote_count":"2","comment_id":"640180"},{"upvote_count":"4","comment_id":"600456","poster":"Boompiee","timestamp":"1668237060.0","content":"Correct. Question so easy I wonder if it was really in the exam."},{"timestamp":"1660975380.0","poster":"paras_gadhiya","content":"COrreECT","upvote_count":"2","comment_id":"551635"},{"content":"correct docs link is https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-real-time-fraud-detection","comment_id":"539417","poster":"dev2dev","timestamp":"1659498420.0","upvote_count":"2"},{"content":"Right Answer. \nAnswer to 3rd drop down is already in the question.","comments":[{"content":"also the 1st is in the question","timestamp":"1666076820.0","comment_id":"587516","poster":"Massy","upvote_count":"1"}],"timestamp":"1641976440.0","comment_id":"404426","upvote_count":"2","poster":"Palee"}],"exam_id":67,"answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0019000004.png"],"answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-power-bi-dashboard","question_text":"HOTSPOT -\nYou are designing a near real-time dashboard solution that will visualize streaming data from remote sensors that connect to the internet. The streaming data must be aggregated to show the average value of each 10-second interval. The data will be discarded after being displayed in the dashboard.\nThe solution will use Azure Stream Analytics and must meet the following requirements:\n✑ Minimize latency from an Azure Event hub to the dashboard.\n✑ Minimize the required storage.\n✑ Minimize development effort.\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point\nHot Area:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/54741-exam-dp-203-topic-2-question-23-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0019100001.png"],"isMC":false,"answers_community":[],"answer_ET":""}],"exam":{"id":67,"isImplemented":true,"provider":"Microsoft","isBeta":false,"name":"DP-203","isMCOnly":false,"numberOfQuestions":384,"lastUpdated":"12 Apr 2025"},"currentPage":35},"__N_SSP":true}