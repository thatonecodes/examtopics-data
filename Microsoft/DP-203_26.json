{"pageProps":{"questions":[{"id":"uXaChHZPiWnk10Yqky0e","answer_images":["https://img.examtopics.com/dp-203/image303.png"],"question_text":"DRAG DROP\n-\n\nYou have an Azure Synapse Analytics workspace named Workspace1.\n\nYou perform the following changes:\n\n• Implement source control for Workspace1.\n• Create a branch named Feature based on the collaboration branch.\n• Switch to the Feature branch.\n• Modify Workspace1.\n\nYou need to publish the changes to Azure Synapse.\n\nFrom which branch should you perform each change? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point\n\n//IMG//","isMC":false,"answer_ET":"","answers_community":[],"exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/108583-exam-dp-203-topic-2-question-105-discussion/","question_images":["https://img.examtopics.com/dp-203/image302.png"],"timestamp":"2023-05-05 21:40:00","answer_description":"","question_id":126,"unix_timestamp":1683315600,"answer":"","topic":"2","discussion":[{"content":"Correct! It's a easy one.","comment_id":"893362","poster":"henryphchan","timestamp":"1683657600.0","upvote_count":"10","comments":[{"content":"Yup also there no branch named publish","upvote_count":"1","timestamp":"1718696040.0","comment_id":"1232301","poster":"nifesimi"}]},{"comment_id":"993861","upvote_count":"1","content":"Correct","poster":"kkk5566","timestamp":"1693384860.0"},{"upvote_count":"3","timestamp":"1686963180.0","content":"Seems correct.","comment_id":"925680","poster":"vctrhugo"},{"timestamp":"1683315600.0","comment_id":"890286","upvote_count":"3","content":"Answer is correct","poster":"shakes103"}]},{"id":"S5FveAhIjoA4SolltpYd","timestamp":"2023-06-11 10:30:00","question_images":[],"question_text":"You have two Azure Blob Storage accounts named account1 and account2.\n\nYou plan to create an Azure Data Factory pipeline that will use scheduled intervals to replicate newly created or modified blobs from account1 to account2.\n\nYou need to recommend a solution to implement the pipeline. The solution must meet the following requirements:\n• Ensure that the pipeline only copies blobs that were created or modified since the most recent replication event.\n• Minimize the effort to create the pipeline.\n\nWhat should you recommend?","topic":"2","answer_ET":"D","unix_timestamp":1686472200,"answer_description":"","answer":"D","discussion":[{"poster":"Sabbath","timestamp":"1702290600.0","content":"Selected Answer: D\nJust use Built-in copy task, according to: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool","upvote_count":"13","comment_id":"920514"},{"comment_id":"925681","upvote_count":"6","content":"Selected Answer: D\n\"[...] use the Copy Data tool to create a pipeline that incrementally copies new and changed files only, from Azure Blob storage to Azure Blob storage. It uses LastModifiedDate to determine which files to copy.\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool","timestamp":"1702781760.0","poster":"vctrhugo"},{"poster":"9370d83","comment_id":"1395466","content":"Selected Answer: A\nWhile the Built-in copy task is useful for simpler scenarios, it does not inherently support metadata-driven incremental copy, which is key to meeting the requirements.","timestamp":"1741908480.0","upvote_count":"1"},{"timestamp":"1740387300.0","comment_id":"1360926","content":"Selected Answer: A\nThis option provides a minimal effort solution and can automatically track changes.\nwhy not D :\nThe Built-in copy task is a straightforward approach in Azure Data Factory to copy data from one source to a destination. However, this does not inherently include metadata tracking like the metadata-driven copy task does.","upvote_count":"1","poster":"Pey1nkh"},{"upvote_count":"1","content":"Selected Answer: D\ncorrect according to doc","poster":"swathi_rs","comment_id":"1202615","timestamp":"1729948800.0"},{"upvote_count":"1","timestamp":"1729900380.0","content":"vote for D","comment_id":"1202291","poster":"Dusica"},{"upvote_count":"3","content":"A. Run the Copy Data tool and select Metadata-driven copy task.\n\nExplanation:\n\nThe Metadata-driven copy task in Azure Data Factory allows you to dynamically define and manage data movement and transformation tasks based on metadata. This includes copying only blobs that were created or modified since the most recent replication event, which aligns with the requirements.\nThis solution minimizes the effort to create the pipeline, as you can configure the replication task based on metadata properties such as the creation or modification timestamp of the blobs.\nThe Built-in copy task also provides a solution for data movement but may require more manual effort to configure the incremental replication based on timestamps","comment_id":"1190278","poster":"Elanche","timestamp":"1728200880.0"},{"upvote_count":"1","comment_id":"1148333","content":"Selected Answer: D\nThe built-in copy task leads you to create a pipeline within five minutes to replicate data without learning about entities. The metadata driven copy task to ease your journey of creating parameterized pipelines and external control table in order to manage to copy large amounts of objects (for example, thousands of tables) at scale.","poster":"Azure_2023","timestamp":"1723472160.0"},{"content":"Selected Answer: A\nMetadata-driven copy task (A): The Metadata-driven copy task in the Copy Data tool of Azure Data Factory allows you to replicate data based on changes in metadata. This includes the ability to only copy blobs that were created or modified since the last replication event.\nGiven the requirements to replicate blobs based on changes in metadata and to minimize effort, the Metadata-driven copy task is a suitable choice.","upvote_count":"3","comments":[{"upvote_count":"3","poster":"ExamKiller42","comment_id":"1154608","timestamp":"1724140200.0","content":"From the documentation https://learn.microsoft.com/en-us/azure/data-factory/copy-data-tool-metadata-driven it looks like the Metadata-driven copy task retreives metaData from control table in order to perform the copy task from multiple sources to multiple destinations. That is not the requirement here.\nThe built-in copy task checks one source folder and looks at the metaData of the files present there and copies only the files that were newly added or modified since the last run: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool\nSo the answer has to be - D. Run the Copy Data tool and select Built-in copy task."},{"timestamp":"1722810660.0","upvote_count":"1","poster":"j888","comment_id":"1140611","content":"I agree A is more appropriate."}],"comment_id":"1118537","timestamp":"1720606980.0","poster":"dakku987"},{"content":"A is correct because of the requirement \"Minimize the effort to create the pipeline.\"","timestamp":"1717715160.0","comments":[{"poster":"Lewiasskick","comment_id":"1116197","upvote_count":"1","timestamp":"1720381680.0","content":"metadriven is for copy huge amounts of objects (for example, thousands of tables) or load data from large variety of sources"}],"poster":"MarkJoh","comment_id":"1089882","upvote_count":"3"},{"content":"Selected Answer: D\nD is correct","comment_id":"993869","poster":"kkk5566","upvote_count":"1","timestamp":"1709203860.0"},{"upvote_count":"2","comment_id":"931578","content":"Selected Answer: D\nCreate a data factory.\nUse the Built-in Copy Data tool to create a pipeline.\nMonitor the pipeline and activity runs.","timestamp":"1703341500.0","poster":"auwia"}],"exam_id":67,"question_id":127,"answer_images":[],"isMC":true,"answers_community":["D (83%)","A (17%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/111881-exam-dp-203-topic-2-question-106-discussion/","choices":{"B":"Create a pipeline that contains a Data Flow activity.","D":"Run the Copy Data tool and select Built-in copy task.","A":"Run the Copy Data tool and select Metadata-driven copy task.","C":"Create a pipeline that contains a flowlet."}},{"id":"mYDzxWnptCBaRWIwg9ig","timestamp":"2023-06-23 14:26:00","question_text":"You have an Azure Data Factory pipeline named pipeline1 that contains a data flow activity named activity1.\n\nYou need to run pipeline1.\n\nWhich runtime will be used to run activity1?","answers_community":["A (100%)"],"topic":"2","isMC":true,"unix_timestamp":1687523160,"answer_ET":"A","answer_description":"","choices":{"A":"Azure Integration runtime","B":"Self-hosted integration runtime","C":"SSIS integration runtime"},"answer_images":[],"exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/113092-exam-dp-203-topic-2-question-107-discussion/","question_images":[],"answer":"A","discussion":[{"comment_id":"993871","upvote_count":"6","content":"Selected Answer: A\ncorrect","timestamp":"1709203920.0","poster":"kkk5566"},{"content":"question is incomplete.","timestamp":"1729863360.0","poster":"Dusica","upvote_count":"5","comment_id":"1202028"},{"content":"A is correct; look at the link:\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime","timestamp":"1729900620.0","comment_id":"1202294","poster":"Dusica","upvote_count":"1"},{"timestamp":"1728402660.0","comment_id":"1191664","content":"Why answer is A?","upvote_count":"1","poster":"Koe24"},{"comment_id":"1119802","timestamp":"1720697100.0","upvote_count":"3","content":"I got this question today in exam 11-jan-2024","poster":"dakku987"},{"timestamp":"1703341560.0","content":"Selected Answer: A\nProbably the correct answer.","upvote_count":"2","poster":"auwia","comment_id":"931579"}],"question_id":128},{"id":"p6E4gMxpE6wtuC67ajrZ","exam_id":67,"isMC":false,"answers_community":[],"unix_timestamp":1686637920,"url":"https://www.examtopics.com/discussions/microsoft/view/112033-exam-dp-203-topic-2-question-108-discussion/","topic":"2","question_text":"HOTSPOT\n-\n\nYou have an Azure subscription that contains an Azure Synapse Analytics workspace named workspace1. Workspace1 contains a dedicated SQL pool named SQLPool1 and an Apache Spark pool named sparkpool1. Sparkpool1 contains a DataFrame named pyspark_df.\n\nYou need to write the contents of pyspark_df to a table in SQLPool1 by using a PySpark notebook.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","question_images":["https://img.examtopics.com/dp-203/image316.png"],"answer_images":["https://img.examtopics.com/dp-203/image317.png"],"question_id":129,"timestamp":"2023-06-13 08:32:00","answer_description":"","answer_ET":"","discussion":[{"upvote_count":"8","timestamp":"1702456320.0","poster":"Azure_2023","content":"Correct\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export?tabs=scala%2Cscala1%2Cscala2%2Cscala3%2Cscala4%2Cscala5","comments":[{"upvote_count":"2","poster":"efeee333","timestamp":"1724477160.0","comment_id":"1157711","content":"agreed"}],"comment_id":"922000"},{"upvote_count":"3","timestamp":"1709204400.0","poster":"kkk5566","content":"%%spark\n&&\ndf.write.synapsesql","comment_id":"993878"},{"content":"Correct, also according to this link\nhttps://microsoftlearning.github.io/DP-203-Data-Engineer/Instructions/Labs/LAB_04_data_warehouse_using_apache_spark.html\n\n*******\n %%spark\n // Make sure the name of the dedcated SQL pool (SQLPool01 below) matches the name of your SQL pool.\n val df = spark.sqlContext.sql(\"select * from top_purchases\")\n df.write.synapsesql(\"SQLPool01.wwi.TopPurchases\", Constants.INTERNAL)","upvote_count":"2","comment_id":"947150","poster":"alegiordx","timestamp":"1704805800.0"}],"answer":""},{"id":"9yudak057ZnD301jDFYq","answer_images":[],"question_text":"You have an Azure data factory named ADF1 and an Azure Synapse Analytics workspace that contains a pipeline named SynPipeLine1. SynPipeLine1 includes a Notebook activity.\n\nYou create a pipeline in ADF1 named ADFPipeline1.\n\nYou need to invoke SynPipeLine1 from ADFPipeline1.\n\nWhich type of activity should you use?","unix_timestamp":1686439560,"url":"https://www.examtopics.com/discussions/microsoft/view/111855-exam-dp-203-topic-2-question-109-discussion/","question_images":[],"timestamp":"2023-06-11 01:26:00","answer_ET":"A","choices":{"D":"Notebook","A":"Web","B":"Spark","C":"Custom"},"answers_community":["A (100%)"],"exam_id":67,"topic":"2","isMC":true,"answer_description":"","answer":"A","discussion":[{"timestamp":"1687189560.0","poster":"ludaka","upvote_count":"8","content":"Selected Answer: A\nWeb Activity \nhttps://learn.microsoft.com/en-us/azure/data-factory/solution-template-synapse-notebook","comment_id":"927639"},{"comments":[{"timestamp":"1733478660.0","poster":"f7c717f","content":"why not notebook?","upvote_count":"1","comment_id":"1322682"}],"upvote_count":"7","poster":"auwia","comment_id":"931585","timestamp":"1687523700.0","content":"Selected Answer: A\nTo invoke a Synapse pipeline from a Data Factory pipeline, you should use a Web activity."},{"upvote_count":"2","timestamp":"1705046700.0","poster":"Happynewyear1001","comment_id":"1120543","content":"Selected Answer: A\nTo invoke a Synapse Analytics pipeline from an Azure Data Factory pipeline, you should use the \"Web\" activity. This activity is specifically designed for making HTTP requests and can be used to trigger the execution of pipelines in other services, such as Azure Synapse Analytics."},{"poster":"jsav1","timestamp":"1704320340.0","comment_id":"1113177","content":"Selected Answer: A\nCorrect","upvote_count":"1"},{"comment_id":"993881","upvote_count":"1","content":"Selected Answer: A\nis correct","timestamp":"1693386300.0","poster":"kkk5566"},{"poster":"Mani_V","content":"its a notebook activity \nhttps://learn.microsoft.com/en-us/azure/data-factory/solution-template-synapse-notebook","upvote_count":"1","comment_id":"935933","timestamp":"1687913100.0"},{"poster":"vctrhugo","timestamp":"1687898100.0","upvote_count":"2","content":"Selected Answer: A\nWeb calls a Synapse pipeline with a notebook activity.","comment_id":"935823"},{"poster":"[Removed]","upvote_count":"4","timestamp":"1686439560.0","comment_id":"920312","content":"Selected Answer: A\nWeb Activity as per this article.\n\nhttps://fnuson.medium.com/invoke-synapse-notebook-spark-job-by-azure-data-factory-adf-fc19cef89bdd"}],"question_id":130}],"exam":{"name":"DP-203","id":67,"provider":"Microsoft","numberOfQuestions":384,"isBeta":false,"isImplemented":true,"lastUpdated":"12 Apr 2025","isMCOnly":false},"currentPage":26},"__N_SSP":true}