{"pageProps":{"questions":[{"id":"x0ebcvP8h0WRs1YFwho7","unix_timestamp":1682838240,"timestamp":"2023-04-30 09:04:00","url":"https://www.examtopics.com/discussions/microsoft/view/107937-exam-dp-420-topic-2-question-9-discussion/","topic":"2","answer_description":"Overriding the default consistency level only applies to reads within the SDK client. An account configured for strong consistency by default will still write and replicate data synchronously to every region in the account. When the SDK client instance or request overrides this with Session or weaker consistency, reads will be performed using a single replica.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels","discussion":[{"poster":"TRUESON","upvote_count":"7","content":"C\n\nOverriding the default consistency level only applies to reads within the SDK client. An account configured for strong consistency by default will still write and replicate data synchronously to every region in the account. https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-manage-consistency?tabs=portal%2Cdotnetv2%2Capi-async#override-the-default-consistency-level","timestamp":"1698656640.0","comment_id":"884942"},{"timestamp":"1719642840.0","content":"Selected Answer: C\nC, Overriding the default consistency level only applies to reads","comment_id":"1108487","poster":"Blubb1860","upvote_count":"2"},{"timestamp":"1702328100.0","poster":"azuredemo2022three","content":"C. Writes will use the strong consistency level.","upvote_count":"4","comment_id":"920921"}],"answer":"C","question_images":[],"question_text":"You have an Azure Cosmos DB Core (SQL) API account named account1 that has a single read-write region and one additional read region. Account1 uses the strong default consistency level.\nYou have an application that uses the eventual consistency level when submitting requests to account1.\nHow will writes from the application be handled?","answers_community":["C (100%)"],"question_id":61,"answer_images":[],"choices":{"B":"Azure Cosmos DB will reject writes from the application.","D":"The write order is not guaranteed during replication.","C":"Writes will use the strong consistency level.","A":"Writes will use the eventual consistency level."},"isMC":true,"answer_ET":"C","exam_id":69},{"id":"9IOHco9QfwieBUSxdgNv","answer":"A","question_text":"You have an Azure Cosmos DB Core (SQL) API account that uses a custom conflict resolution policy. The account has a registered merge procedure that throws a runtime exception.\nThe runtime exception prevents conflicts from being resolved.\nYou need to use an Azure function to resolve the conflicts.\nWhat should you use?","topic":"3","url":"https://www.examtopics.com/discussions/microsoft/view/77531-exam-dp-420-topic-3-question-1-discussion/","isMC":true,"timestamp":"2022-07-15 15:11:00","answers_community":["A (83%)","Other"],"answer_images":[],"choices":{"D":"a function that receives items pushed from the conflicts feed and is triggered by an Azure Cosmos DB trigger","C":"a function that pulls items from the change feed and is triggered by a timer trigger","A":"a function that pulls items from the conflicts feed and is triggered by a timer trigger","B":"a function that receives items pushed from the change feed and is triggered by an Azure Cosmos DB trigger"},"question_id":62,"unix_timestamp":1657890660,"answer_description":"","answer_ET":"A","discussion":[{"content":"Selected Answer: A\nCorrect answer should be \"A\" - we need to resolve conflicts, hence we definitely need to read the conflicts feed, hence answers B and C are immediately eliminated as they are pull from the changes feed, answer D mentions an Azure Cosmos DB trigger, but this is only for the change feed, not for the conflicts feed, hence A is the correct answer since there is no trigger mechanism for the conflict feed in Cosmos DB","poster":"ognamala","comment_id":"647140","upvote_count":"15","timestamp":"1676467320.0"},{"upvote_count":"1","timestamp":"1719643260.0","poster":"Blubb1860","comment_id":"1108494","content":"Selected Answer: D\nConflict feed and CosmosDB trigger"},{"poster":"Bviljoen","content":"Answer is A:\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/conflict-resolution-policies#conflict-resolution-policies\n\nIf you configure your container with the custom resolution option, and you fail to register a merge procedure on the container or the merge procedure throws an exception at runtime, the conflicts are written to the conflicts feed. Your application then needs to manually resolve the conflicts in the conflicts feed. To learn more, see examples of how to use the custom resolution policy and how to use the conflicts feed.","upvote_count":"1","comment_id":"1022080","timestamp":"1711956540.0"},{"poster":"virgilpza","content":"Correct answer is A","timestamp":"1693635660.0","upvote_count":"1","comment_id":"826644"},{"timestamp":"1679808240.0","content":"Selected Answer: A\nCorrect answer is A","poster":"klepper","upvote_count":"1","comment_id":"679431"},{"timestamp":"1678037880.0","comment_id":"660318","poster":"Shiggi","content":"Selected Answer: A\nCorrect answer is A: all conflicts related data is stored in the conflicts feed, you have to orchestrate the function using a timer","upvote_count":"2"},{"comment_id":"650148","poster":"remz","timestamp":"1677058080.0","content":"Selected Answer: A\nAnswer A","upvote_count":"1"},{"comments":[{"comments":[{"poster":"ognamala","comment_id":"647147","upvote_count":"1","timestamp":"1676467560.0","content":"Correct answer is A"}],"content":"Actually, ignore this answer - C is retrieving from the change feed not the conflicts feed","timestamp":"1676467200.0","comment_id":"647139","upvote_count":"1","poster":"ognamala"}],"upvote_count":"1","comment_id":"647136","timestamp":"1676466840.0","poster":"ognamala","content":"Selected Answer: C\nAs grada explained very well, the answer should be C"},{"content":"Selected Answer: B\nThe Azure Cosmos DB Trigger uses the Azure Cosmos DB Change Feed to listen for inserts and updates across partitions. The change feed publishes inserts and updates, not deletions.","timestamp":"1674598380.0","comment_id":"636230","poster":"mybiai","upvote_count":"2"},{"upvote_count":"4","comment_id":"635087","poster":"avocacao","content":"Answer should be A. Need to read from conflict feed but there is no trigger mechanism for the conflict feed in Cosmos DB.","timestamp":"1674382380.0"},{"comment_id":"631758","timestamp":"1673795460.0","poster":"grada","comments":[{"upvote_count":"1","comment_id":"1112784","poster":"xRiot007","timestamp":"1720005900.0","content":"You can use a timer trigger as well."}],"content":"The correct answer is C, only change feed triggers are available, and conflict feed needs to be manually queried from a timer-triggered function.\n\nSource: https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-cosmosdb-v2-trigger?tabs=in-process%2Cextensionv4&pivots=programming-language-csharp#attributes\n\nNothing conflict-feed-related there, only change-feed-related. Conflict feed needs to be manually triggered like this: https://docs.microsoft.com/en-us/azure/cosmos-db/sql/how-to-manage-conflicts?tabs=dotnetv3%2Capi-async%2Casync#read-from-conflict-feed","upvote_count":"2"}],"question_images":[],"exam_id":69},{"id":"aXPAolJ2MmAXMvvLZ3mx","answer_ET":"B","timestamp":"2022-09-05 19:31:00","url":"https://www.examtopics.com/discussions/microsoft/view/80399-exam-dp-420-topic-3-question-10-discussion/","answer_images":[],"isMC":true,"question_images":[],"discussion":[{"comment_id":"660403","upvote_count":"8","timestamp":"1678044660.0","comments":[{"timestamp":"1705056780.0","content":"But a Document databases can use analytical store.","upvote_count":"1","comment_id":"949612","poster":"XiangRongChang"}],"content":"Answer is B\nStream Analytics doesn't create containers in your database. Instead, it requires you to create them up front. You can then control the billing costs of Azure Cosmos DB containers, and u dont need to use Analytical Store becosue we dont have ColumnStore","poster":"IDATA"},{"comment_id":"1108502","poster":"Blubb1860","timestamp":"1719643620.0","content":"Strange and most likely incomplete or wrong question","upvote_count":"1"},{"poster":"Garyn","content":"To use an Azure Cosmos DB Core (SQL) API database (db1) as an output for Azure Stream Analytics while minimizing costs, you should follow these steps:\n\nB. In db1, create containers that have a custom indexing policy and analytical store disabled.\n\nHere's why this is the correct option:\n\nCustom Indexing Policy: Creating containers with a custom indexing policy allows you to optimize the indexing behavior based on your specific requirements. You can choose to exclude specific properties from indexing if they are not needed for querying. This can help reduce indexing costs.\n\nAnalytical Store Disabled: The analytical store is used for historical data analysis, and enabling it can add extra cost. If you're primarily using Stream Analytics for real-time data ingestion and processing, you may not need the analytical store. Disabling it can help minimize costs.","upvote_count":"2","comment_id":"1019214","timestamp":"1711579680.0"},{"comment_id":"827682","timestamp":"1693711620.0","content":"Selected Answer: B\n\"Stream Analytics doesn't create containers in your database. Instead, it requires you to create them beforehand. You can then control the billing costs of Azure Cosmos DB containers. You can also tune the performance, consistency, and capacity of your containers directly by using the Azure Cosmos DB APIs.\"\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-documentdb-output#basics-of-azure-cosmos-db-as-an-output-target","upvote_count":"4","poster":"basiltomato"},{"comment_id":"693966","poster":"susejzepol","content":"Selected Answer: B\nI Think that B is the correct answer because you don't need the Analytical Store.","timestamp":"1681395420.0","upvote_count":"2"},{"upvote_count":"2","poster":"TimSss","timestamp":"1680096840.0","comment_id":"682736","content":"Selected Answer: B\nsure it's not A, but usure about B"},{"upvote_count":"2","comment_id":"669124","timestamp":"1678812900.0","content":"Selected Answer: B\nDefinitely not A","poster":"essdeecee"}],"choices":{"D":"In account1, enable a dedicated gateway","C":"In db1, create containers that have an automatic indexing policy and analytical store enabled","A":"In account1, add a private endpoint","B":"In db1, create containers that have a custom indexing policy and analytical store disabled"},"topic":"3","answer_description":"","answers_community":["B (100%)"],"exam_id":69,"question_id":63,"answer":"B","question_text":"You have a database named db1 in an Azure Cosmos DB Core (SQL) API account named account1.\nYou need to write JSON data to db1 by using Azure Stream Analytics. The solution must minimize costs.\nWhich should you do before you can use db1 as an output of Stream Analytics?","unix_timestamp":1662399060},{"id":"u7d3L5VVYmYubIkApPys","timestamp":"2023-04-27 14:45:00","answer_images":[],"question_id":64,"answer_ET":"B","choices":{"B":"Azure Data Factory","C":"Database Migration Assistant","A":"Azure Migrate"},"answers_community":["B (100%)"],"answer":"B","exam_id":69,"isMC":true,"unix_timestamp":1682599500,"url":"https://www.examtopics.com/discussions/microsoft/view/107689-exam-dp-420-topic-3-question-11-discussion/","answer_description":"","question_text":"You have a database named db1 in an Azure Cosmos DB Core (SQL) API account.\nYou have a third-party application that is exposed through a REST API.\nYou need to migrate data from the application to a container in db1 on a weekly basis.\nWhat should you use?","topic":"3","question_images":[],"discussion":[{"poster":"[Removed]","upvote_count":"5","content":"Selected Answer: B\nCorrect Answer is B. \nAzure Migrate is a service designed for migrating on-premises virtual machines and database Migration Assistant is a tool that helps you assess and migrate databases to Azure, but it is not designed for migrating data from a third-party application exposed through a REST API","timestamp":"1714221900.0","comment_id":"882639"},{"poster":"azuredemo2022three","content":"Selected Answer: B\nWith Azure Data Factory, you can create a pipeline that connects to the third-party application's REST API as a data source and configures the destination to be the container in db1. By scheduling the pipeline to run weekly, you can migrate the data from the application to the specified container in db1 on a recurring basis.","timestamp":"1719770940.0","comment_id":"939309","upvote_count":"4"}]},{"id":"2eOuqYxRJchHOyOE2VrC","timestamp":"2022-10-30 19:35:00","answer_ET":"","answer":"","exam_id":69,"isMC":false,"topic":"3","question_id":65,"answers_community":[],"question_text":"HOTSPOT -\nYou have an Apache Spark pool in Azure Synapse Analytics that runs the following Python code in a notebook.\n//IMG//\n\nFor each of the following statements. select Yes if the statement is true. Otherwise. select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"Box 1: No -\nStreaming ג€\" Append Output Mode is an outputMode in which only the new rows in the streaming DataFrame/Dataset will be written to the sink.\nThis is the default mode. Use append as output mode outputMode(\"append\") when you want to output only new rows to the output sink.\nNote:\nStreaming ג€\" Complete Output Mode is an OutputMode in which all the rows in the streaming DataFrame/Dataset will be written to the sink every time there are some updates.\nStreaming ג€\" Update Output Mode is an outputMode in which only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates.\n\nBox 2: No -\nStructured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.\n\nBox 3: Yes -\nSynapse Apache Spark also allows you to ingest data into Azure Cosmos DB. It is important to note that data is always ingested into Azure Cosmos DB containers through the transactional store. When Synapse Link is enabled, any new inserts, updates, and deletes are then automatically synced to the analytical store.\nReference:\nhttps://sparkbyexamples.com/spark/spark-streaming-outputmode/ https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html https://docs.microsoft.com/en-us/azure/synapse-analytics/synapse-link/how-to-query-analytical-store-spark","url":"https://www.examtopics.com/discussions/microsoft/view/86682-exam-dp-420-topic-3-question-12-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04276/0008700002.jpg"],"unix_timestamp":1667154900,"discussion":[{"timestamp":"1707587880.0","comment_id":"977868","content":"Answer \nN: Append mode activate, updates won't be recorded.\nN: Streaming\nN: It doesn't need","upvote_count":"6","poster":"maxequiel"},{"content":"Answer YNN","timestamp":"1702840200.0","comment_id":"926152","upvote_count":"5","poster":"azuredemo2022three"},{"poster":"b890yc","timestamp":"1711596900.0","upvote_count":"1","comment_id":"1019321","content":"Yes - New and updated orders will be added to contoso-erp.orders - The format is cosmos.oltp.changeFeed which will process all inserts and updates.\n\nNo - The code performs bulk data ingestion from contoso-app - It is not bulk data ingestion rather it is change feed.\n\nNo - Both contoso-app and contoso-erp have Analytical store enabled - The format is cosmos.oltp which will write into the original container (not into the lease container) so it is not required to enable Analytical store on the container"},{"upvote_count":"1","poster":"Garyn","timestamp":"1711580220.0","content":"Please update the answer. Right answer is below\nAnswer\nN: Append mode activate, updates won't be recorded.\nN: Streaming\nN: It doesn't need","comment_id":"1019218"},{"timestamp":"1682873460.0","poster":"Nath2","upvote_count":"5","content":"And Question 1 is \"Yes\" - as this is reading from the change feed and so gets all the inserts and updates.","comment_id":"707976","comments":[{"timestamp":"1720008720.0","poster":"xRiot007","content":"Correct, but this is not what the bullet point is asking. \nIt says that updates and inserts are added to orders, which is FALSE. \n\nThe stream is reading inserts and updates for contoso-app. \nIt only adds the inserts for contoso-erp, due to mode Append. \nReading data from a stream does not guarantee it will processed in any way, shape or form.","upvote_count":"2","comment_id":"1112830"},{"content":"Since the append mode is enabled, updates will not be recorded, only new records are added so your answer is partially right/wrong. I would go with 'No' for Question 1.","comment_id":"779354","timestamp":"1689623280.0","poster":"Bharat","upvote_count":"4"}]},{"comment_id":"707966","poster":"Nath2","comments":[{"upvote_count":"2","timestamp":"1689623340.0","comment_id":"779355","poster":"Bharat","content":"I agree. For the third question, it should be a \"No\""}],"upvote_count":"3","timestamp":"1682872500.0","content":"This is linked to this documentation:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/synapse-link/how-to-query-analytical-store-spark-3#load-streaming-dataframe-from-azure-cosmos-db-container\n\nAnd so doesn't need the analytical store to be enabled, so \"No\" for the third question."}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04276/0008600001.jpg","https://www.examtopics.com/assets/media/exam-media/04276/0008700001.jpg"]}],"exam":{"lastUpdated":"12 Apr 2025","isBeta":false,"numberOfQuestions":147,"isImplemented":true,"name":"DP-420","provider":"Microsoft","isMCOnly":false,"id":69},"currentPage":13},"__N_SSP":true}