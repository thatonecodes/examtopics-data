{"pageProps":{"questions":[{"id":"oUVtp84Z9VeDbwWJ90nj","url":"https://www.examtopics.com/discussions/microsoft/view/138102-exam-dp-600-topic-1-question-87-discussion/","topic":"1","timestamp":"2024-04-07 23:05:00","question_images":[],"question_text":"You have a Microsoft Power BI semantic model.\n\nYou need to identify any surrogate key columns in the model that have the Summarize By property set to a value other than to None. The solution must minimize effort.\n\nWhat should you use?","question_id":166,"answers_community":["D (88%)","6%"],"isMC":true,"discussion":[{"comment_id":"1198389","content":"Selected Answer: D\nBest Practice Analyzer should be able to identify these with some rules","timestamp":"1729320960.0","poster":"Nefirs","upvote_count":"13"},{"content":"Selected Answer: C\nModel view in Microsoft Power BI Desktop: This tool allows you to visually inspect and manage the relationships and properties of tables and columns in your Power BI model. You can easily identify surrogate key columns and check their Summarize By property.","upvote_count":"1","poster":"Rataxe","comment_id":"1413970","timestamp":"1743401520.0"},{"content":"Selected Answer: D\nThe Best Practice Analyzer (BPA) in Tabular Editor can be configured to check for specific properties and configurations in your model. This includes identifying columns with certain \"Summarize By\" settings.\nYou can create or use existing BPA rules to quickly identify any surrogate key columns that do not have the \"Summarize By\" property set to \"None,\" thus minimizing the manual effort required.","poster":"282b85d","upvote_count":"1","timestamp":"1732821600.0","comment_id":"1220447"},{"content":"D (although it can be done it B, but not as efficiently as D)\nTo identify surrogate key columns with the 'Summarize By' property set to a value other than 'None,' the Best Practice Analyzer in Tabular Editor is the most efficient tool. The Best Practice Analyzer can analyze the entire model and provide a report on all columns that do not meet a specified best practice, such as having the 'Summarize By' property set correctly for surrogate key columns. Here's how you would proceed:\nOpen your Power BI model in Tabular Editor.\nGo to the Advanced Scripting window.\nWrite or use an existing script that checks the 'Summarize By' property of each column.\nExecute the script to get a report on the surrogate key columns that do not have their 'Summarize By' property set to 'None'.\n\nYou can then review and adjust the properties of the columns directly within the Tabular Editor.","poster":"rlo123","upvote_count":"2","timestamp":"1731783060.0","comment_id":"1212512"},{"comment_id":"1210404","content":"Selected Answer: D\nI go with D) Best Practice Analyzer","upvote_count":"1","timestamp":"1731453720.0","poster":"stilferx"},{"upvote_count":"1","comments":[{"poster":"harshalt10","comments":[],"comment_id":"1192568","timestamp":"1728519660.0","upvote_count":"9","content":"The question is about identifying, not changing. I think it should be D."}],"timestamp":"1728335100.0","comment_id":"1191205","content":"Selected Answer: B\nIt should be B\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/model-explorer#anatomy-of-model-explorer\n\nhttps://www.purplefrogsystems.com/2021/08/how-to-change-the-summarization-of-multiple-columns-in-power-bi/#:~:text=Once%20you've%20selected%20the,That's%20it!","poster":"neoverma"}],"answer_ET":"D","answer_description":"","choices":{"D":"Best Practice Analyzer in Tabular Editor","A":"DAX Formatter in DAX Studio","B":"Model explorer in Microsoft Power BI Desktop","C":"Model view in Microsoft Power BI Desktop"},"answer_images":[],"exam_id":71,"answer":"D","unix_timestamp":1712523900},{"id":"PSIiZUeSWsE5jnXoeJXa","answers_community":[],"timestamp":"2024-04-06 01:16:00","unix_timestamp":1712358960,"answer_description":"","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/138005-exam-dp-600-topic-1-question-88-discussion/","question_images":["https://img.examtopics.com/dp-600/image107.png"],"topic":"1","question_text":"DRAG DROP\n-\n\nYou have a Fabric tenant that contains a Microsoft Power BI report named Report1.\n\nReport1 is slow to render. You suspect that an inefficient DAX query is being executed.\n\nYou need to identify the slowest DAX query, and then review how long the query spends in the formula engine as compared to the storage engine.\n\nWhich five actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n\n//IMG//","answer_ET":"","answer":"","question_id":167,"discussion":[{"comment_id":"1190138","upvote_count":"47","timestamp":"1712358960.0","poster":"554b579","content":"Good Video: https://www.youtube.com/watch?v=C5HBhlLUFsE&t=176s\nFrom performance analyzer capture code. \nSort duration descending by DAX query time.\nCopy the first query over to Dax Studio\nEnable query and server timings and run the query\nView Server timings"},{"poster":"calvintcy","content":"I took the test today, and this question was included. The options 'Sort the Duration (ms) column in descending order' was not there. Hence the answer:\n-From Performance analyzer, capture a recording.\n-Sort the Duration (ms) column in descending order by DAX query time.\n-Copy the first query to Dax Studio.\n-Enable Query Timings and Server Timings. Run the query.\n-View the Server Timings tab.","comment_id":"1230820","upvote_count":"13","timestamp":"1718438460.0"},{"comment_id":"1294830","content":"From Performance analyzer, capture a recording - This will start recording the performance of the report.\nEnable Query Timings and Server Timings. Run the query - This step captures detailed timings of the query execution on both the formula engine and storage engine.\nView the Query Timings tab - Here you can analyze how long the query takes in different phases of execution.\nSort the Duration (ms) column in descending order by DAX query time - This will help identify the slowest DAX query.\nCopy the first query to DAX Studio - Once the slowest query is identified, you can further analyze it using DAX Studio for optimization.","upvote_count":"1","poster":"AbhiShar","timestamp":"1728410400.0"},{"comment_id":"1210407","content":"IMHO, \n\n1. Start Recording\n2. Sort in descending order\n3. Copy to DAX studio (the problematic query)\n4. Enable query t and server t (to gather additional info)\n5. View Server Timings (to understand it is storage engine or formula engine)\n\nIt is taken from the YouTube video below.","poster":"stilferx","upvote_count":"3","timestamp":"1715550660.0"},{"comment_id":"1200359","content":"Here are the steps you should follow to identify the slowest DAX query and review how long the query spends in the formula engine compared to the storage engine:\n\n1.Open the report in Power BI Desktop. Go to the 'View' tab and click on 'Performance Analyzer' and Click on 'Start Recording', then 'Refresh Visuals' \n2.Review the DAX query and duration in the performance analyzer pane. Sort the Duration (ms) column in descending order\n3.Copy the first query to DAX Studio \n4.In DAX Studio, enable Query Timings and Server Timings. Run the query\n5.View the Server Timings tab to see data for the formula engine (FE) and the storage engine (SE)","upvote_count":"3","timestamp":"1713815160.0","poster":"hello2tomoki"}],"answer_images":["https://img.examtopics.com/dp-600/image108.png"],"exam_id":71},{"id":"lOYipqUNa8Q2MKYqMVpu","topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/139059-exam-dp-600-topic-1-question-89-discussion/","answer":"C","timestamp":"2024-04-18 07:59:00","answers_community":["C (100%)"],"unix_timestamp":1713419940,"isMC":true,"choices":{"A":"Microsoft Power BI Desktop","C":"Tabular Editor","B":"ALM Toolkit","D":"DAX Studio"},"question_text":"You have a Fabric tenant that contains a semantic model. The model contains 15 tables.\n\nYou need to programmatically change each column that ends in the word Key to meet the following requirements:\n\n• Hide the column.\n• Set Nullable to False\n• Set Summarize By to None.\n• Set Available in MDX to False.\n• Mark the column as a key column.\n\nWhat should you use?","exam_id":71,"question_images":[],"question_id":168,"discussion":[{"timestamp":"1731456120.0","upvote_count":"6","comment_id":"1210409","content":"Selected Answer: C\nIMHO, C) Tabulr Editor is good.\n\nHere's why the other options are not ideal for this scenario:\n\n1) Microsoft Power BI Desktop: doesn't offer functionalities for bulk programmatic changes based on naming conventions.\n2) ALM Toolkit: it's not specifically designed for modifying the structure of semantic models within the tool itself.\n3) DAX Studio doesn't provide functionalities for directly modifying model structure or properties en masse.","poster":"stilferx"},{"timestamp":"1729576140.0","comment_id":"1199986","upvote_count":"3","poster":"Nefirs","content":"Selected Answer: C\nTabular Editor seems most correct for me"},{"upvote_count":"1","poster":"dp600","comments":[{"poster":"andrewkravchuk97","upvote_count":"9","comment_id":"1199840","timestamp":"1729548540.0","content":"Tabular Editor allows you to use C# scripts to automate these changes. In PBI Desktop you can do this only manually"},{"comment_id":"1199987","upvote_count":"3","content":"yes, in theory, however, not programmatically, only manually","poster":"Nefirs","timestamp":"1729576200.0"}],"comment_id":"1197731","timestamp":"1729231140.0","content":"Why option A is not correct? You can make all this options by using PBI Desktop... Am I wrong?"}],"answer_description":"","answer_ET":"C"},{"id":"QoPY3LLKTQqZ946Pt0nD","answer":"C","answer_images":[],"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/133448-exam-dp-600-topic-1-question-9-discussion/","unix_timestamp":1707484140,"question_images":["https://img.examtopics.com/dp-600/image4.png","https://img.examtopics.com/dp-600/image5.png"],"question_id":169,"answer_ET":"C","choices":{"C":"a lakehouse","D":"an external Hive metastore","A":"a data lake","B":"a warehouse"},"answer_description":"","discussion":[{"comment_id":"1208048","upvote_count":"15","poster":"stilferx","content":"Selected Answer: C\nIMHO,\nmy answer is C. Because of that: \"\"\"Semi-structured and unstructured data\"\"\" in the question","timestamp":"1715118240.0"},{"comment_id":"1193012","timestamp":"1712754780.0","poster":"VAzureD","upvote_count":"11","content":"Selected Answer: C\nC. to lakehouse\nAs a concept it exists in Fabric and is the most logical response for the given requirements,\nTechnical Requirements -\nSemi-structured and unstructured data\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\n\nA. a data lake\nAs an answer it could be valid, but it is not really a concept that we have in Fabric.\n\nB. a warehouse\nIt is not valid, because there is unstructured data.\n\nD. an external Hive metastore\nHas no sense.\nThe Hive Metastore is a centralized repository that stores metadata related to the tables, partitions, columns, schemas, and other data structures used by Hive and Spark.\nContains information about the PHYSICAL location of the data, the schemas of the tables, and the relationships between them."},{"poster":"NRezgui","timestamp":"1735117500.0","content":"Selected Answer: C\na lakehouse","upvote_count":"1","comment_id":"1331466"},{"comment_id":"1312423","upvote_count":"1","poster":"Rakesh16","timestamp":"1731650160.0","content":"Selected Answer: C\nLakehouse is the answer.\n\nWarehouse not possible since data is structured and unstructured."},{"content":"Selected Answer: C\nSemi-structured and unstructured data","comment_id":"1203979","upvote_count":"1","poster":"rmeng","timestamp":"1714389360.0"},{"content":"Selected Answer: C\nSemi-structured and unstructured data","comment_id":"1192738","timestamp":"1712729460.0","poster":"GPerez73","upvote_count":"2"},{"poster":"a_51","upvote_count":"1","timestamp":"1710870300.0","content":"Selected Answer: C\nLakehouse makes the most sense.","comment_id":"1177582"},{"poster":"TashaP","upvote_count":"1","timestamp":"1708876080.0","content":"lakehouse 100%","comment_id":"1158926"},{"timestamp":"1708504620.0","poster":"David_Webb","comment_id":"1155370","content":"In the technical requirement, it stated \"Semi-structured and unstructured data\" for the AnalyticsPOC data store. Thus, it must be a lakehouse.","upvote_count":"4"},{"poster":"SamuComqi","comment_id":"1153095","timestamp":"1708242780.0","content":"Selected Answer: C\nC. a lakehouse\n\nThe data store must handle semi-structured and unstructured data, therefore a Lakehouse should be the optimal solution supporting read access with T-SQL and Python.","upvote_count":"3"},{"timestamp":"1708206600.0","upvote_count":"1","poster":"Momoanwar","comment_id":"1152850","content":"Selected Answer: C\nAnalytic=lakehouse"},{"comment_id":"1148215","content":"Selected Answer: C\n\"Read access by using T-SQL or Python\nSemi-structured and unstructured data\"","upvote_count":"4","timestamp":"1707749220.0","poster":"Nicofr"},{"poster":"theseon","timestamp":"1707484140.0","comment_id":"1145485","upvote_count":"2","content":"Selected Answer: C\nC. a lakehouse"}],"exam_id":71,"question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nLitware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.\n\nExisting Environment -\n\nFabric Environment -\nLitware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.\n\nAvailable Data -\nLitware has data that must be analyzed as shown in the following table.\n//IMG//\n\nThe Product data contains a single table and the following columns.\n//IMG//\n\nThe customer satisfaction data contains the following tables:\n\nSurvey -\n\nQuestion -\n\nResponse -\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.\n\nUser Problems -\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.\nProduct data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.\n\nRequirements -\n\nPlanned Changes -\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity\nThe following three workspaces will be created:\nAnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store\nDataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake\nDataSciPOC: Will contain all the notebooks and reports created by the data scientists\nThe following will be created in the AnalyticsPOC workspace:\nA data store (type to be decided)\n\nA custom semantic model -\n\nA default semantic model -\n\nInteractive reports -\nThe data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers’ discretion.\nAll the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.\n\nTechnical Requirements -\nThe data store must support the following:\nRead access by using T-SQL or Python\nSemi-structured and unstructured data\nRow-level security (RLS) for users executing T-SQL queries\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\nData will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model\nThe data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model\nThe dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\nThe product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:\nList prices that are less than or equal to 50 are in the low pricing group.\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\nList prices that are greater than 1,000 are in the high pricing group.\n\nSecurity Requirements -\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\nFabric administrators will be the workspace administrators.\nThe data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.\nThe analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.\nThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\nThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.\nThe date dimension must be available to all users of the data store.\nThe principle of least privilege must be followed.\nBoth the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:\nFabricAdmins: Fabric administrators\nAnalyticsTeam: All the members of the analytics team\nDataAnalysts: The data analysts on the analytics team\nDataScientists: The data scientists on the analytics team\nDataEngineers: The data engineers on the analytics team\nAnalyticsEngineers: The analytics engineers on the analytics team\n\nReport Requirements -\nThe data analysts must create a customer satisfaction report that meets the following requirements:\nEnables a user to select a product to filter customer survey responses to only those who have purchased that product.\nDisplays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.\nShows data as soon as the data is updated in the data store.\nEnsures that the report and the semantic model only contain data from the current and previous year.\nEnsures that the report respects any table-level security specified in the source data store.\nMinimizes the execution time of report queries.\nWhich type of data store should you recommend in the AnalyticsPOC workspace?","timestamp":"2024-02-09 14:09:00","topic":"1","isMC":true},{"id":"aFczoJ7U7x0EICFr2160","answer_description":"","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/138007-exam-dp-600-topic-1-question-90-discussion/","answer_ET":"","answer_images":["https://img.examtopics.com/dp-600/image110.png"],"discussion":[{"timestamp":"1714107780.0","comment_id":"1202387","content":"CALCULATE\nSELECTEDMEASURE\nmore info refer to:\nhttps://www.sqlbi.com/articles/using-calculation-groups-to-selectively-replace-measures-in-dax-expressions/","upvote_count":"21","poster":"4371883"},{"comment_id":"1210410","content":"IMHO, Calculate / SELECTEDMEASURE,\n\nSource: https://learn.microsoft.com/en-us/dax/selectedmeasure-function-dax#example","poster":"stilferx","upvote_count":"7","timestamp":"1715551560.0"},{"comment_id":"1315118","upvote_count":"1","poster":"mghf61","content":"since DatesMTD is selecting a range of values from month to the current date, it should be SELECTEDMEASRE \n\nSELECTEDVALE is for single value not a range","timestamp":"1732089360.0"},{"comment_id":"1190146","upvote_count":"6","comments":[{"content":"Don't confuse people, the answer is correct. Calculate + selectedmeasure() is a standard combination when it comes to calculation items. When Calc. Item is applied dax engine replaces selectedmeasure() with measure reference.","poster":"andrewkravchuk97","comment_id":"1199841","upvote_count":"32","timestamp":"1713737760.0"}],"poster":"554b579","timestamp":"1712359800.0","content":"CALCULATE(Selectedvalue(), DATESMTD('Date'[Date])) combines two DAX functions to achieve a specific result\nSelectedvalue():\nThis function returns the value of the currently selected item in a column.\nIt is commonly used in scenarios where you want to retrieve a single value from a column (such as a slicer selection).\nDATESMTD('Date'[Date]):\nThe DATESMTD function calculates the month-to-date (MTD) total for a given expression.\nIt considers all dates from the beginning of the month up to the current date defined by the filter context.\nIn this case, it operates on the 'Date'[Date] column.\nOverall Purpose:\nThe entire expression calculates the MTD value for the currently selected item (such as a date) based on the date context.\nIt’s useful for creating dynamic MTD calculations that adjust automatically based on user selections.\nFor example, if you have a measure called “Sales Amount” and you want to calculate the MTD sales amount based on the selected date, this expression would give you that value."}],"timestamp":"2024-04-06 01:30:00","question_text":"HOTSPOT\n-\n\nYou have a Microsoft Power BI semantic model.\n\nYou plan to implement calculation groups.\n\nYou need to create a calculation item that will change the context from the selected date to month-to-date (MTD).\n\nHow should you complete the DAX expression? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","unix_timestamp":1712359800,"answer":"","answers_community":[],"topic":"1","question_images":["https://img.examtopics.com/dp-600/image109.png"],"exam_id":71,"question_id":170}],"exam":{"id":71,"provider":"Microsoft","name":"DP-600","isImplemented":true,"lastUpdated":"12 Apr 2025","isMCOnly":false,"numberOfQuestions":179,"isBeta":false},"currentPage":34},"__N_SSP":true}