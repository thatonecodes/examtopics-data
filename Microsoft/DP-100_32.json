{"pageProps":{"questions":[{"id":"Zt8VwIpS066FF8GLxPP9","exam_id":64,"question_text":"HOTSPOT\n-\n\nYou create an Azure Machine Learning workspace. You use the Azure Machine Learning SDK for Python.\n\nYou must create a dataset from remote paths. The dataset must be reusable within the workspace.\n\nYou need to create the dataset.\n\nHow should you complete the following code segment? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_description":"","answer":"","timestamp":"2023-04-03 10:46:00","answer_ET":"","answer_images":["https://img.examtopics.com/dp-100/image399.png"],"discussion":[{"timestamp":"1729698840.0","content":"given answer is correct ðŸ˜„","comment_id":"878570","upvote_count":"6","poster":"labriji"},{"timestamp":"1727945160.0","comment_id":"859695","poster":"hammamse","upvote_count":"2","content":"given answer is true"}],"question_id":156,"question_images":["https://img.examtopics.com/dp-100/image398.png"],"unix_timestamp":1680511560,"topic":"2","answers_community":[],"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/104950-exam-dp-100-topic-2-question-79-discussion/"},{"id":"vlpGcJMlUWtAdtmoOwHZ","answer_ET":"C","answer_description":"Use the Convert to ARFF module in Azure Machine Learning Studio, to convert datasets and results in Azure Machine Learning to the attribute-relation file format used by the Weka toolset. This format is known as ARFF.\nThe ARFF data specification for Weka supports multiple machine learning tasks, including data preprocessing, classification, and feature selection. In this format, data is organized by entites and their attributes, and is contained in a single text file.\nReference:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/convert-to-arff","answers_community":["C (100%)"],"question_images":[],"discussion":[{"upvote_count":"13","poster":"chaudha4","comments":[{"timestamp":"1669527840.0","poster":"Saurabhjain507","content":"thank you for your comment!","comment_id":"607883","upvote_count":"1"}],"comment_id":"345600","content":"This applies only to ML Studio (classic) - it won't get asked. Skip this one !!","timestamp":"1635531540.0"},{"content":"Selected Answer: C\nWEKA can use any format but ARFF is the native data format of course it is the answer.","upvote_count":"1","comment_id":"1155270","timestamp":"1724211900.0","poster":"evangelist"},{"upvote_count":"1","comment_id":"909121","content":"Selected Answer: C\nConvert to ARFF","timestamp":"1701249480.0","poster":"krishna1818"},{"poster":"duytran216","content":"Correct. ARFF is Weka extension.","comment_id":"304900","timestamp":"1630972440.0","upvote_count":"3"},{"upvote_count":"1","poster":"ipindado2020","comment_id":"215441","content":"Agree with C","timestamp":"1620491760.0"}],"unix_timestamp":1604860560,"timestamp":"2020-11-08 19:36:00","answer_images":[],"question_id":157,"choices":{"B":"Convert to Dataset","C":"Convert to ARFF","D":"Convert to SVMLight","A":"Convert to CSV"},"exam_id":64,"url":"https://www.examtopics.com/discussions/microsoft/view/36484-exam-dp-100-topic-2-question-8-discussion/","question_text":"You are moving a large dataset from Azure Machine Learning Studio to a Weka environment.\nYou need to format the data for the Weka environment.\nWhich module should you use?","topic":"2","answer":"C","isMC":true},{"id":"M9Wn5sHQz4yHa2WJdo0A","answer":"","discussion":[{"content":"It's \n1. Confusion Matrix \n2. Prediction vs True\n\nThe calibration curve plots a model's confidence in its predictions against the proportion of positive samples at each confidence level (https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2#calibration-curve)\n\nThe predicted vs. true chart plots the relationship between the target feature (true/actual values) and the model's predictions (https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2#predicted-vs-true)","comment_id":"1157739","timestamp":"1724482980.0","upvote_count":"2","poster":"Plb2"},{"comment_id":"1117606","poster":"Tin_Tin","content":"1. Confusion matrix\n2. Predicted vs. true\nhttps://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2","timestamp":"1720532100.0","upvote_count":"1"},{"content":"* for sure we use the \"confusion matrix\" for classification problem ðŸ˜„\n\n* for regression, we dont use \"confusion matrix\", we use\n>> \"Calibration curve\": shows the relationship between the predicted values and the actual values.\n>> \"Predicted vs. true\": visual representation of the model's performance.\ni think both are correct ðŸ˜…","poster":"labriji","timestamp":"1698076920.0","comment_id":"878583","upvote_count":"4"}],"answers_community":[],"question_text":"HOTSPOT\n-\n\nYou train classification and regression models by using automated machine learning.\n\nYou must evaluate automated machine learning experiment results. The results include how a classification model is making systematic errors in its predictions and the relationship between the target feature and the regression modelâ€™s predictions. You must use charts generated by automated machine learning.\n\nYou need to choose a chart type for each model type.\n\nWhich chart types should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","isMC":false,"answer_ET":"","answer_images":["https://img.examtopics.com/dp-100/image401.png"],"exam_id":64,"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/107223-exam-dp-100-topic-2-question-80-discussion/","unix_timestamp":1682265720,"topic":"2","timestamp":"2023-04-23 18:02:00","question_id":158,"question_images":["https://img.examtopics.com/dp-100/image400.png"]},{"id":"Brhwv0WMP3cjpcHrgAyD","answer":"","answer_images":["https://img.examtopics.com/dp-100/image403.png"],"question_text":"HOTSPOT\n-\n\nYou create an Azure Data Lake Storage Gen2 storage account named storage1containing a file system named fs1 and a folder named folder1.\n\nThe contents of folder1 must be accessible from jobs on compute targets in the Azure Machine Learning workspace.\n\nYou need to construct a URI to reference folder1.\n\nHow should you construct the URI? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","exam_id":64,"answer_ET":"","topic":"2","question_id":159,"answers_community":[],"isMC":false,"question_images":["https://img.examtopics.com/dp-100/image402.png"],"answer_description":"","discussion":[{"poster":"ajay0011","upvote_count":"9","comments":[{"upvote_count":"2","poster":"PI_Team","timestamp":"1706179080.0","content":"Correct. \n\nThe URI abfss://<filesystem>@<storageaccount>.dfs.core.windows.net/<directory> points to a folder named folder1 in a file system named fs1 inside an Azure Data Lake Storage Gen2 storage account named storage1. The URI is using the ABFSS (Azure Blob File System Service) protocol to access the storage account.\n\nHere is a breakdown of the URI:\n\nabfss: The scheme identifier for the ABFSS protocol.\n<filesystem>: The name of the file system.\n@: The delimiter between the file system name and the storage account name.\n<storageaccount>: The name of the storage account.\n.dfs.core.windows.net: The domain name for the Azure Data Lake Storage Gen2 service.\n<directory>: The path to the folder.","comment_id":"962523"}],"timestamp":"1696387020.0","content":"This is a URI (Uniform Resource Identifier) that points to a folder named \"folder1\" in a file system named \"fs1\" inside an Azure Data Lake Storage Gen2 storage account named \"storage1\".\n\nThe URI is using the ABFSS (Azure Blob File System Service) protocol to access the storage account. \nabfss://<filesystem>@<storageaccount>.dfs.core.windows.net/<directory>","comment_id":"860558"},{"poster":"Kanwal001","upvote_count":"5","comment_id":"992470","content":"On exam 28 Aug 2023","timestamp":"1709148960.0"},{"upvote_count":"1","poster":"evangelist","content":"Correct Selections:\nabfss\nstorage1.dfs.core.windows.net/fs1/folder1\nExplanation:\nabfss: This is the Azure Blob File System Secure (ABFSS) protocol, used for accessing data in Azure Data Lake Storage Gen2.\nstorage1.dfs.core.windows.net: This is the domain for accessing the Data Lake Storage Gen2 account named storage1.\nfs1/folder1: This is the path within the file system fs1 to the folder folder1.","comment_id":"1222544","timestamp":"1733044860.0"},{"content":"abfss is correct","upvote_count":"1","timestamp":"1728580980.0","poster":"sai384957324","comment_id":"1193182"},{"timestamp":"1705073760.0","poster":"damaldon","comment_id":"949844","content":"correct, for ADLS Gen2:\nabfss://<file_system>@<account_name>.dfs.core.windows.net/<path>","upvote_count":"1"},{"comment_id":"932103","poster":"pranav33","content":"Correct. \nTo construct the URI for referencing the contents of `folder1` in the Azure Data Lake Storage Gen2 storage account, you would need to combine the storage account name, file system name, and folder name in a specific format. Here's how you should construct the URI:\n\n```\nabfss://<storage-account-name>.dfs.core.windows.net/<file-system-name>/<folder-name>\n```\n\nIn this case, the correct options for constructing the URI would be:\n\n- `storage-account-name`: storage1\n- `file-system-name`: fs1\n- `folder-name`: folder1\n\nSo the correct URI would be:\n\n```\nabfss://storage1.dfs.core.windows.net/fs1/folder1\n```\n\nMake sure to replace `<storage-account-name>`, `<file-system-name>`, and `<folder-name>` with the actual names of your storage account, file system, and folder respectively.","timestamp":"1703390880.0","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/microsoft/view/105043-exam-dp-100-topic-2-question-81-discussion/","unix_timestamp":1680575820,"timestamp":"2023-04-04 04:37:00"},{"id":"BwG7Zsv3SaZfgwE21PnD","discussion":[{"content":"First one is event grid\nhttps://learn.microsoft.com/en-us/azure/data-explorer/create-event-grid-connection?tabs=portal-adx%2Cportal-2\nthe last one is logic apps:\nhttps://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-trigger-published-pipeline?view=azureml-api-1","timestamp":"1697278680.0","poster":"SoftAI","comment_id":"870117","upvote_count":"10"},{"upvote_count":"7","poster":"chaymat","comment_id":"871661","timestamp":"1697452620.0","content":"Event Grid, Logic Apps"},{"poster":"sanctafrax","content":"Given answer seems correct. Event grid needs to be filled explicitly (by someone), it does not happen by itself. you need some kind of function. Therefore detecting whether or not there is new data can be done by a timed based triggered function which pushes an event to an event grid. Then trigger the retraining based on the event grid entry.","timestamp":"1738331040.0","upvote_count":"1","comment_id":"1349535"},{"poster":"evangelist","content":"The other way around, 1: event grid to monitor event of new data 2: triggering functions to re-train the model","upvote_count":"1","timestamp":"1733045400.0","comment_id":"1222546"},{"comment_id":"992471","poster":"Kanwal001","timestamp":"1709149020.0","content":"On exam 28 Aug 2023","upvote_count":"6"},{"content":"Event Hubs: Used for high-volume, streaming data ingestion.\n\nEvent Grid: Used for reactive programming based on event-driven architectures, distributing event notifications.","timestamp":"1707406380.0","poster":"MarinaMijailovic","comment_id":"975292","upvote_count":"1"},{"timestamp":"1706181300.0","upvote_count":"5","comment_id":"962543","poster":"PI_Team","content":"Identify when new data is uploaded: You can use Event Grid to subscribe to events that are emitted when new data is uploaded to Azure Blob Storage. Event Grid will send an event to your subscription whenever a new blob is created or modified.\n\nTrigger re-training: Logic Apps can be a good choice for linking data to blob storage and then to ML Studio. Logic Apps are a more complex solution than Functions, but they offer more flexibility and integration capabilities.\n\nI think Logic Apps would be a better choice over Functions for triggering as the question asks for linking different services together. As far as I know, Logic Apps can be used to integrate with other Azure services, such as Azure Notification Hubs and Azure Service Bus and if we need to integrate with other Azure services, then Logic Apps may be a better choice.\n\nSaM"},{"upvote_count":"1","comments":[{"upvote_count":"1","poster":"DaniloMagone","timestamp":"1730646540.0","comment_id":"1206132","content":"\"You need to minimize development and coding.\" So it has to be Logic Apps over Functions"}],"poster":"Norasit","timestamp":"1703467560.0","comment_id":"932991","content":"Trigger re-training can be use Functions and Logic Apps.\nIn this problem it sound like simple process so I think Functions is better than Logic Apps."},{"upvote_count":"2","content":"Azure Event Grid: Azure Event Grid can be used to monitor events in Azure Blob Storage. You can configure Event Grid to trigger an event when new data is uploaded to Azure Blob Storage.\n\n Azure Functions: Azure Functions is a serverless compute service that can be used to run your re-training code in response to the event triggered by Azure Event Grid. You can write your re-training logic as a function and configure it to be executed when the event is received.\n\nBy combining Azure Event Grid and Azure Functions, you can create an automated re-training pipeline that triggers the re-training process whenever new data is uploaded to Azure Blob Storage, without the need for extensive coding or manual intervention.","comment_id":"908414","comments":[{"content":"it is better to mention that you use ChatGPT","upvote_count":"4","timestamp":"1702213200.0","comment_id":"919965","poster":"snegnik"}],"timestamp":"1701164820.0","poster":"rishi_ram"},{"timestamp":"1698077520.0","upvote_count":"4","comment_id":"878592","poster":"labriji","content":"Identify when new data is uploaded >> Event Grid\nTrigger re-training >> Azure Logic Apps \n\nbasta ðŸ˜…"},{"comment_id":"860567","upvote_count":"2","content":"Answer is reverse.\nyou can configure Azure Blob Storage to trigger an event in Azure Event Grid when new data is uploaded. Then, you can create an Azure Function that listens for the event and initiates the re-training process","timestamp":"1696387620.0","poster":"ajay0011"},{"content":"I think this is Event Grid + Event Grid?","timestamp":"1696276440.0","poster":"hammamse","comment_id":"859306","upvote_count":"2"},{"content":"Could also be Event Grid + Event Grid","upvote_count":"3","poster":"Tommo565","comment_id":"852953","timestamp":"1695886020.0"},{"upvote_count":"5","comment_id":"847987","poster":"Tommo565","content":"I think this is Event Grid + Logic Apps?","timestamp":"1695451980.0"}],"answer":"","answers_community":[],"question_text":"HOTSPOT\n-\n\nYou train a model by using Azure Machine Learning. You use Azure Blob Storage to store production data.\n\nThe model must be re-trained when new data is uploaded to Azure Blob Storage. You need to minimize development and coding.\n\nYou need to configure Azure services to develop a re-training solution.\n\nWhich Azure services should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","isMC":false,"answer_ET":"","exam_id":64,"answer_images":["https://img.examtopics.com/dp-100/image405.png"],"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/103650-exam-dp-100-topic-2-question-82-discussion/","unix_timestamp":1679561580,"timestamp":"2023-03-23 09:53:00","topic":"2","question_id":160,"question_images":["https://img.examtopics.com/dp-100/image404.png"]}],"exam":{"name":"DP-100","lastUpdated":"12 Apr 2025","isBeta":false,"id":64,"isMCOnly":false,"isImplemented":true,"numberOfQuestions":512,"provider":"Microsoft"},"currentPage":32},"__N_SSP":true}