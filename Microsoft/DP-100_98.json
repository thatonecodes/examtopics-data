{"pageProps":{"questions":[{"id":"KJkBwsY4plC5kfTcFutP","answer":"D","topic":"5","isMC":true,"unix_timestamp":1589220660,"url":"https://www.examtopics.com/discussions/microsoft/view/20314-exam-dp-100-topic-5-question-9-discussion/","timestamp":"2020-05-11 20:11:00","answers_community":["D (82%)","B (18%)"],"exam_id":64,"discussion":[{"content":"Since a new column\\feature is being added, I believe the answer Apply SQL Transform","upvote_count":"50","timestamp":"1589220660.0","poster":"VickyM","comment_id":"87259"},{"comment_id":"97319","poster":"jmonk","timestamp":"1590649260.0","upvote_count":"20","content":"The correct answer is D. Apply SQL Transfer\nEditing the metadata allows you to rename or change the data type of existing columns. It does not allow you to add a column and set it's value to \"London\"."},{"poster":"evangelist","content":"Selected Answer: D\nTo add a new feature with a constant value like \"London\" for all rows, the most efficient way is to use SQL. The \"Apply SQL Transformation\" module allows you to execute SQL queries on your dataset, which can easily add a new column with a constant value.","comment_id":"1235764","timestamp":"1719135780.0","upvote_count":"1"},{"content":"Selected Answer: D\nApply SQL Transformation: This module allows you to use SQL queries to manipulate and transform your dataset. You can easily add a new column and populate it with the desired value using an SQL statement.","timestamp":"1716005520.0","comment_id":"1213126","upvote_count":"1","poster":"evangelist"},{"poster":"BR_CS","content":"This question appears at least twice and is wrong both times. i checked. Edit Metadata cannot add a new column and assign a value. There is, however, a distinct \"Add column\" component.","comment_id":"986454","timestamp":"1692618180.0","upvote_count":"1"},{"timestamp":"1691574060.0","upvote_count":"1","poster":"phdykd","comment_id":"976430","content":"d is answer"},{"poster":"phdykd","content":"B is answer","comment_id":"820044","upvote_count":"1","timestamp":"1677205140.0"},{"timestamp":"1671048480.0","comment_id":"745407","upvote_count":"1","content":"Selected Answer: D\nAnswer is D. The new column is Data not Metadata.","poster":"michaelmorar"},{"comment_id":"725027","timestamp":"1669195020.0","upvote_count":"3","poster":"taer","content":"Selected Answer: D\nshould be D"},{"upvote_count":"1","comment_id":"615290","timestamp":"1655034300.0","content":"Select *, 'London' as City from t1","poster":"ning"},{"comments":[{"poster":"silva_831","upvote_count":"1","content":"Thank you guys for doing practice to verify the answer. Appreciate it.","comment_id":"720270","timestamp":"1668667200.0"}],"comment_id":"584887","poster":"pancman","upvote_count":"2","content":"Selected Answer: D\nCorrect answer is D. I have checked this on Azure Designer Studio to make sure. Edit metadata allows you to alter existing columns. However, it doesn't allow you to add a new column or set the value of it. However, you can add a new column and change its value using Apply SQL Transformation.","timestamp":"1649792100.0"},{"content":"D is the sure shot.","timestamp":"1648160340.0","comment_id":"574718","poster":"Thornehead","upvote_count":"1"},{"poster":"synapse","timestamp":"1647080640.0","comment_id":"566049","content":"Selected Answer: D\nD is the correct answer","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nI think B is correct, Marking columns as features is what the Edit metadata module can do,\nCreating new column is not mentioned in the question, It is populate!","poster":"dija123","timestamp":"1639505460.0","comment_id":"501592","comments":[{"content":"\"You must add a feature named CityName ...\" = You must create a new column named CItyName","upvote_count":"1","timestamp":"1649832240.0","comment_id":"585049","poster":"David_Tadeu"}]},{"poster":"dushmantha","content":"The answer should be \"Apply SQL Transformation\". I have checked it. The metadata does not allow to add new columns","comment_id":"435183","timestamp":"1630305660.0","upvote_count":"3"}],"question_images":[],"question_id":486,"answer_images":[],"question_text":"You are performing feature engineering on a dataset.\nYou must add a feature named CityName and populate the column value with the text London.\nYou need to add the new feature to the dataset.\nWhich Azure Machine Learning Studio module should you use?","choices":{"D":"Apply SQL Transformation","B":"Edit Metadata","C":"Preprocess Text","A":"Extract N-Gram Features from Text"},"answer_ET":"D","answer_description":""},{"id":"FrsRV22hpF4JrbPyUcMy","answers_community":["A (100%)"],"answer_description":"","unix_timestamp":1679302620,"exam_id":64,"discussion":[{"poster":"Lion007","upvote_count":"1","timestamp":"1719771420.0","comment_id":"1110797","content":"Selected Answer: A\nThe Correct answer is: A. Event Grid subscription\n\nJustification:\nEvent Grid subscription: is the most suitable choice for creating event-driven workflows in Azure. When a training run is completed in an Azure ML workspace, Event Grid can be used to trigger a workflow automatically. This approach requires minimal administrative effort, as you can subscribe to specific events (like training run completion) and respond to them without constant polling or manual intervention.\n\nWrong Answers:\n\nB. Azure Automation runbook: is more suitable for scenarios where you need to automate complex, multi-step processes and often require more administrative effort to set up triggers based on specific events in Azure Machine Learning.\n\nC. Event Hubs Capture: is designed to automatically capture the streaming data in Event Hubs and save it to a storage account. \n\nD. Event Hubs consumer: is part of the Event Hubs service and is used to read and process the stream of events. It is not a tool for triggering workflows based on specific events within Azure Machine Learning workspaces. It requires more effort to configure."},{"upvote_count":"1","timestamp":"1695193020.0","comment_id":"844667","poster":"Jin_22","content":"To configure an event-driven workflow to automatically trigger upon completion of training runs in the workspace, you should use Azure Event Grid subscription. Azure Machine Learning emits the following event types: Model registered, Model deployed, Run completed, and Dataset drift detected. When an event is triggered, the Event Grid service sends data about that event to subscribing endpoint. You can set up event-driven applications, processes, or CI/CD workflows based on Azure Machine Learning events, such as failure notification emails or ML pipeline runs, when certain conditions are detected by Azure Event Grid"}],"choices":{"D":"Event Hubs consumer","B":"Azure Automation runbook","A":"Event Grid subscription","C":"Event Hubs Capture"},"answer_images":[],"timestamp":"2023-03-20 09:57:00","question_id":487,"answer_ET":"A","topic":"6","url":"https://www.examtopics.com/discussions/microsoft/view/103348-exam-dp-100-topic-6-question-1-discussion/","answer":"A","isMC":true,"question_text":"You create an Azure Machine Learning workspace.\n\nYou must configure an event-driven workflow to automatically trigger upon completion of training runs in the workspace. The solution must minimize the administrative effort to configure the trigger.\n\nYou need to configure an Azure service to automatically trigger the workflow.\n\nWhich Azure service should you use?","question_images":[]},{"id":"GqDP3RdJnr1v5U9UgIRg","isMC":true,"answer_description":"Post batch normalization statistics (PBN) is the Microsoft Cognitive Toolkit (CNTK) version of how to evaluate the population mean and variance of Batch\nNormalization which could be used in inference Original Paper.\nIn CNTK, custom networks are defined using the BrainScriptNetworkBuilder and described in the CNTK network description language \"BrainScript.\"\nScenario:\nLocal penalty detection models must be written by using BrainScript.\nReference:\nhttps://docs.microsoft.com/en-us/cognitive-toolkit/post-batch-normalization-statistics","answers_community":["C (100%)"],"question_images":[],"question_text":"You need to implement a scaling strategy for the local penalty detection data.\nWhich normalization type should you use?","timestamp":"2020-01-16 05:24:00","discussion":[{"upvote_count":"78","poster":"huyennguyen","content":"Both the question and answer are difficult to follow.","comment_id":"39603","timestamp":"1594866240.0"},{"upvote_count":"9","comments":[{"timestamp":"1638084720.0","content":"any reference for this?","comment_id":"368522","poster":"prashantjoge","upvote_count":"2"}],"comment_id":"255388","content":"In case study they have also mentioned \n- All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow\n- The images and videos will have varying sizes and formats\n\nSo Batch normalization is usefull to speedup the process where as Cosine normalization is usefull to handle varying sizes and formats of input data.","timestamp":"1625024700.0","poster":"satishgunjal"},{"poster":"GHill1982","upvote_count":"1","timestamp":"1721237580.0","comment_id":"1125256","content":"Selected Answer: C\nThe best normalization type to use in this case is batch normalization. Batch normalization is a technique that reduces the internal covariate shift of the inputs to each layer of a neural network, making the training faster and more stable. Batch normalization also has the benefit of regularizing the model and reducing the need for dropout."},{"timestamp":"1701715800.0","poster":"snegnik","content":"Terminology from Cognitive Toolkit and Synapse Analytics. It seems doesn't relevant for DP-100 test","comment_id":"914820","upvote_count":"1"},{"upvote_count":"2","comment_id":"617650","timestamp":"1671277200.0","poster":"ning","content":"DNN normalization?? I really do not expect this kind of questions ...\nThe most common one is batch, and weight is kind of a batch with some improvements ...\nFor other two, I do not know ..."},{"poster":"[Removed]","comment_id":"595797","timestamp":"1667359200.0","upvote_count":"2","content":"is this question really for DP-100? it seems more suitable for AI-102."},{"poster":"ranjsi01","upvote_count":"2","comments":[{"comment_id":"617881","content":"The images and videos will have varying sizes and formats. Normalization mean put them into the same dimension and same format images / videos before further processing","poster":"ning","timestamp":"1671307320.0","upvote_count":"1"}],"timestamp":"1658830260.0","comment_id":"532868","content":"any easy way to understand this ?"},{"content":"\"Local penalty detection models must be written by using BrainScript.\" BrainScript is used in Microsoft Cognitive Toolkit (CNTK) and it's network definition only supports batch normalization. So C is correct.\n\nhttps://docs.microsoft.com/en-us/cognitive-toolkit/batchnormalization","comment_id":"476710","poster":"spaceykacey","timestamp":"1652330400.0","upvote_count":"3"},{"content":"Not able to follow question and answer. This question will take atleast 15 mins to read and summarize :-).","comment_id":"399144","poster":"gaint","timestamp":"1641390720.0","upvote_count":"5"},{"upvote_count":"1","timestamp":"1625024520.0","poster":"satishgunjal","content":"So Batch normalization is usefull to speedup the process where as normalization is use full to handle varying sizes and formats of input data.","comment_id":"255386"},{"upvote_count":"1","poster":"dmadhup","timestamp":"1603126320.0","comment_id":"76471","content":"Answer: C"}],"choices":{"B":"Weight","C":"Batch","A":"Streaming","D":"Cosine"},"topic":"6","question_id":488,"answer_ET":"C","exam_id":64,"url":"https://www.examtopics.com/discussions/microsoft/view/12106-exam-dp-100-topic-6-question-1-discussion/","unix_timestamp":1579148640,"answer_images":[],"answer":"C"},{"id":"jLbBtKnDLiMQ48A3wB2u","exam_id":64,"url":"https://www.examtopics.com/discussions/microsoft/view/120693-exam-dp-100-topic-6-question-2-discussion/","question_text":"HOTSPOT\n-\n\nYou plan to implement an Azure Machine Learning solution.\n\nYou have the following requirements:\n\n• Run a Jupyter notebook to interactively train a machine learning model.\n• Deploy assets and workflows for machine learning proof of concept by using scripting rather than custom programming.\n\nYou need to select a development technique for each requirement.\n\nWhich development technique should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","isMC":false,"answer_images":["https://img.examtopics.com/dp-100/image537.png"],"answers_community":[],"answer_description":"","answer":"","discussion":[{"content":"I wonder if an actual question like this in the exam can have two correct answers?","timestamp":"1729849800.0","upvote_count":"1","poster":"jefimija","comment_id":"1302826"},{"poster":"sl_mslconsulting","timestamp":"1717536120.0","comment_id":"1224370","upvote_count":"1","content":"Train models using SDK:https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-model?view=azureml-api-2&tabs=python\n\nusing studio: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-ui?view=azureml-api-2\n\nI will pick SDK for the first requirement as you need that to submit a job in a Notebook"},{"upvote_count":"1","content":"bard says the answeres are correct","timestamp":"1699395060.0","poster":"ferren","comment_id":"1065190"},{"timestamp":"1696273920.0","content":"On September 4, 2023 exam.","comment_id":"1023403","upvote_count":"4","poster":"A_PL300"},{"timestamp":"1694633400.0","content":"For the requirement to run a Jupyter notebook to interactively train a machine learning model, you can use the Azure Machine Learning Python SDK. This SDK provides an interactive environment for training machine learning models and it integrates well with Jupyter notebooks.\n\nFor deploying assets and workflows for machine learning proof of concept by using scripting rather than custom programming, you can use the Azure CLI. It allows you to manage Azure resources, including Machine Learning assets, using scripts which is ideal for proof of concept deployments.","comments":[{"upvote_count":"2","comments":[{"poster":"PI_Team","comments":[{"comment_id":"1159309","timestamp":"1708915740.0","upvote_count":"1","poster":"deyoz","content":"we can also write code via notebook in azure ml studio. I believe both sdk and ml studio are correct options."}],"content":"the question specifically mentions using a Jupyter notebook, it does imply that you’ll be writing code. In this case, using the Azure Machine Learning Python SDK would be more appropriate.","timestamp":"1702311660.0","upvote_count":"1","comment_id":"1093664"}],"poster":"InversaRadice","timestamp":"1702233120.0","comment_id":"1092742","content":"well question ask to interactively train so AML studio is correct"}],"poster":"PI_Team","comment_id":"1006951","upvote_count":"3"}],"question_id":489,"question_images":["https://img.examtopics.com/dp-100/image536.png"],"topic":"6","answer_ET":"","unix_timestamp":1694633400,"timestamp":"2023-09-13 21:30:00"},{"id":"dzG9sxbKIENxM8LreMXC","answer":"","answers_community":[],"question_id":490,"topic":"6","question_text":"HOTSPOT -\nYou need to use the Python language to build a sampling strategy for the global penalty detection models.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","exam_id":64,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04274/0032500001.png"],"answer_ET":"","discussion":[{"upvote_count":"12","comment_id":"368528","content":"TF supports static computational graph while pytorch supports dynamic Computational Graph. So the answer to the first question is pytorch since we are asked to use dynamic runtime graph computation\nthe 2nd and 4th option are as described in the given solution\nThe 3rd option is confusing, since SGD is offered by pytorch and gradient descent optimizer is offered by tensorflow. I will go with SGD, because it goes with the rest of the answers even though there is this \"All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow\"","poster":"prashantjoge","timestamp":"1653717600.0"},{"poster":"dzzz","content":"Box3: train.GradientDescentOptimizer belongs to TensorFlow, but the other boxes use Pytorch. \nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/train/GradientDescentOptimizer","upvote_count":"7","timestamp":"1640693100.0","comment_id":"254033"},{"upvote_count":"1","comment_id":"968180","poster":"phdykd","content":"-import pytorch as deeplearninglib\nf-train sampler = deeplearninglib.WeightedRandomSampler.(penalty video dataset)\nh-optimizer = deeplearninglib.optim. SGD(model. parameters).Ir=0,01)\nk-model = deeplearninglib.nn.parallel. DistributedDataParallelCPU(model)\nThese options support the requirements of dynamic runtime graph computation, handling imbalance in the penalty detection classes, applying Stochastic Gradient Descent (SGD) optimizer, and employing parallel computations for the model respectively.","timestamp":"1722435720.0"},{"timestamp":"1708811520.0","poster":"phdykd","upvote_count":"1","content":"Box 1: A) import pytorch as deeplearninglib\n\nExplanation: Since the feature mentioned, dynamic runtime graph computation, is a feature of PyTorch, we should import PyTorch in this case.\n\nBox 2: C) train_sampler= deeplearninglib.WeightedRnadomSampler.(penalty_video_dataset)\n\nExplanation: A sampling strategy is required for the global penalty detection models. The WeightedRandomSampler allows for weighted sampling, which may be useful for ensuring that rarer samples are not overlooked in the training process.\n\nBox 3: A) optimizer= deeplearninglib.optim.SGD(model.parameters().lr=0.01)\n\nExplanation: The SGD optimizer is mentioned specifically for the penalty detection models, and the learning rate is set to 0.01.\n\nBox 4: A) model= deeplearninglib.parallel.DistributedDataParallel(model)\n\nExplanation: The DistributedDataParallel module allows for parallel processing of a single model across multiple devices or nodes, which can significantly speed up the training process. This is useful for the global penalty detection models, which are mentioned to have slow inference times.","comment_id":"820978"},{"content":"No clue, the only thing I know of is that \nDistributedSampler, Optim.SGD, and nn.Parallel ... are all pytouch packages or classes ...","timestamp":"1686995520.0","comment_id":"617654","upvote_count":"1","poster":"ning"},{"comment_id":"595800","poster":"[Removed]","content":"is this question really for DP-100?? seems like it is for AI-102","upvote_count":"4","timestamp":"1682990520.0"},{"upvote_count":"5","timestamp":"1664278980.0","comment_id":"452380","poster":"frida321","content":"so hard to answer"},{"content":"its all messed up ......","poster":"ckkobe24","upvote_count":"3","comment_id":"450608","timestamp":"1663985880.0"},{"comment_id":"413765","poster":"YipingRuan","content":"Why Box 4 uses CPU?","timestamp":"1658742180.0","upvote_count":"1"},{"comment_id":"386745","poster":"andre999","content":"Box 2 is not correct either, it says 'deeplearming' instead of 'deeplearning'...","upvote_count":"1","timestamp":"1655784120.0"},{"poster":"luca2712","timestamp":"1642854840.0","upvote_count":"3","content":"I think, box3: optimizer = deeplearninglib.optim.SGD(model.parameters().lr=0,01)\n\nhttps://analyticsindiamag.com/how-ml-frameworks-like-tensorflow-and-pytorch-handle-gradient-descent/","comment_id":"273715","comments":[{"timestamp":"1646709660.0","upvote_count":"1","comment_id":"305484","poster":"wjrmffldrhrl","content":"In this case say \"All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow.\""}]},{"poster":"lucho94","upvote_count":"1","comment_id":"259481","timestamp":"1641310080.0","content":"Which is the correct one?"},{"upvote_count":"1","poster":"wahaha","timestamp":"1639987380.0","comment_id":"248439","comments":[{"timestamp":"1640688540.0","comment_id":"253984","content":"we need to use dynamic runtime graph computation thus pytorch","upvote_count":"8","poster":"kurasaki"},{"timestamp":"1662538560.0","comment_id":"440796","upvote_count":"3","poster":"sim39","content":"I might be wrong, but I think the \"to.device()\" code reveals that it must be PyTorch"}],"content":"why pytorch not tensorflow? they both support Python"}],"answer_description":"Box 1: import pytorch as deeplearninglib\nBox 2: ..DistributedSampler(Sampler)..\nDistributedSampler(Sampler):\nSampler that restricts data loading to a subset of the dataset.\nIt is especially useful in conjunction with class:`torch.nn.parallel.DistributedDataParallel`. In such case, each process can pass a DistributedSampler instance as a\nDataLoader sampler, and load a subset of the original dataset that is exclusive to it.\nScenario: Sampling must guarantee mutual and collective exclusively between local and global segmentation models that share the same features.\nBox 3: optimizer = deeplearninglib.train. GradientDescentOptimizer(learning_rate=0.10)\nIncorrect Answers: ..SGD..\nScenario: All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow.\nBox 4: .. nn.parallel.DistributedDataParallel..\nDistributedSampler(Sampler): The sampler that restricts data loading to a subset of the dataset.\nIt is especially useful in conjunction with :class:`torch.nn.parallel.DistributedDataParallel`.\nReference:\nhttps://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0032400001.png"],"timestamp":"2020-11-11 20:33:00","isMC":false,"unix_timestamp":1605123180,"url":"https://www.examtopics.com/discussions/microsoft/view/36794-exam-dp-100-topic-6-question-2-discussion/"}],"exam":{"isBeta":false,"isMCOnly":false,"lastUpdated":"12 Apr 2025","name":"DP-100","id":64,"isImplemented":true,"numberOfQuestions":512,"provider":"Microsoft"},"currentPage":98},"__N_SSP":true}