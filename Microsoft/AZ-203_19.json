{"pageProps":{"questions":[{"id":"RuOHKFQwv8Jcev84KlJ3","question_id":91,"answer_images":[],"discussion":[{"timestamp":"1594538220.0","comment_id":"132787","poster":"chaudh","content":"why not C?\nThe REST API for uploading is already supported. Also, coping with PowerShell or CLI seems not to be good choices for \"real time\" requirement.","comments":[{"comment_id":"136770","poster":"Bluediamond","timestamp":"1594937640.0","upvote_count":"3","content":"I tend to agree with you. Now if B said something like create a PS runbook that is set on a webhook connected to storage account then that would make sense."},{"content":"I think it's C too. As it uses storage REST API to upload to Container1, it's easy to do the same to put into Container2 at the same time to make it ' in real time'","upvote_count":"3","timestamp":"1595373660.0","poster":"rrongcheng","comment_id":"140698"}],"upvote_count":"12"},{"upvote_count":"2","poster":"altafpatel1984","comment_id":"485951","timestamp":"1637758560.0","content":"If Start-AzureStorageBlobCopy is triggered by Event Grid then it make sense, which is not mentioned here so C is correct.\n\nFor following question, since Event Grid is mentioned with Start-AzureStorageBlobCopy, that make sense:\nhttps://www.examtopics.com/discussions/microsoft/view/37714-exam-az-204-topic-3-question-24-discussion/"},{"comment_id":"266428","timestamp":"1610556240.0","poster":"Mehabooba","content":"The Put Blob operation creates a new block, page, or append blob, or updates the content of an existing block blob. Updating an existing block blob overwrites any existing metadata on the blob.\n\n\nThe difference between the two is likely AzCopy is operating a bulk mode and Start-AzureStorageBlobCopy is operating serially due to piping the blobs in one by one, sending request to initiate the transfer","upvote_count":"1"},{"timestamp":"1609926060.0","comments":[{"content":"Sorry, my fault. This are different questions....","comment_id":"260894","poster":"Chilred","upvote_count":"4","timestamp":"1609926180.0"}],"poster":"Chilred","upvote_count":"1","content":"There is a similar question in az204: https://www.examtopics.com/discussions/microsoft/view/37714-exam-az-204-topic-3-question-7-discussion/\n\nIn both the option C is available. So in choice between B and C, I would go with C","comment_id":"260892"},{"poster":"kirannep","timestamp":"1602514620.0","upvote_count":"3","comment_id":"198505","content":"Anyone saying put, The Put Blob operation creates a new block, page, or append blob, or updates the content of an existing block blob. Answer is B because you want to copy in realtime even if it is ps script."},{"poster":"Larry616","content":"Answer should be C \nB only makes sense if it was \"Create a PS script and set it up by AZ Functions or Webjobs\", but it's \"Run PS ...\" which means triggered manually and couldn't meet the \"real time\" requirement","upvote_count":"2","comment_id":"164015","timestamp":"1598144460.0"},{"timestamp":"1595389380.0","poster":"xsify","comments":[{"content":"B doesn't meet the 'real-time' requirement. It has to be done by code.","timestamp":"1595995380.0","comment_id":"146226","poster":"rrongcheng","upvote_count":"1"},{"comment_id":"233060","timestamp":"1606919220.0","content":"It uses a storage REST API to upload media, so the block type is a Block Blob, not a Page Blob (used for VM) and Append Blob (used to append data like logs)\nSo I think the correct answer is C.\nFor a block blob, the request body contains the content of the blob (see https://docs.microsoft.com/en-us/rest/api/storageservices/put-blob#request-body)","upvote_count":"1","poster":"Kobee"}],"content":"Copy with Put Blob (C) requires more step, for example, Put Blob on page blob and append blob only initialize the blob, additional step is needed to copy the blob. So i think the answers is B.\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/put-blob","upvote_count":"4","comment_id":"140833"}],"question_images":[],"answer_description":"The Start-AzureStorageBlobCopy cmdlet starts to copy a blob.\n\nExample 1: Copy a named blob -\nC:\\PS>Start-AzureStorageBlobCopy -SrcBlob \"ContosoPlanning2015\" -DestContainer \"ContosoArchives\" -SrcContainer \"ContosoUploads\"\nThis command starts the copy operation of the blob named ContosoPlanning2015 from the container named ContosoUploads to the container named\nContosoArchives.\nReferences:\nhttps://docs.microsoft.com/en-us/powershell/module/azure.storage/start-azurestorageblobcopy?view=azurermps-6.13.0","topic":"3","timestamp":"2020-07-12 09:17:00","url":"https://www.examtopics.com/discussions/microsoft/view/25477-exam-az-203-topic-3-question-13-discussion/","choices":{"C":"Copy blobs to Container2 by using the Put Blob operation of the Blob Service REST API.","B":"Run the Azure PowerShell command Start-AzureStorageBlobCopy.","A":"Download the blob to a virtual machine and then upload the blob to Container2.","D":"Use AzCopy with the Snapshot switch blobs to Container2."},"question_text":"You develop an app that allows users to upload photos and videos to Azure storage. The app uses a storage REST API call to upload the media to a blob storage account named Account1. You have blob storage containers named Container1 and Container2.\nUploading of videos occurs on an irregular basis.\nYou need to copy specific blobs from Container1 to Container2 in real time when specific requirements are met, excluding backup blob copies.\nWhat should you do?","isMC":true,"unix_timestamp":1594538220,"answer":"B","exam_id":47,"answer_ET":"B","answers_community":[]},{"id":"woInTmu0S31DiqK2bGIo","question_id":92,"question_text":"HOTSPOT -\nYou have an app that stores player scores for an online game. The app stores data in Azure tables using a class named PlayerScore as the table entity. The table is populated with 100,000 records.\nYou are reviewing the following section of code that is intended to retrieve 20 records where the player score exceeds 15,000. (Line numbers are included for reference only.)\n//IMG//\n\nYou have the following code. (Line numbers are included for reference only.)\n//IMG//\n\nYou store customer information in an Azure Cosmos database. The following data already exists in the database:\n//IMG//\n\nYou develop the following code. (Line numbers are included for reference only.)\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:","exam_id":47,"answers_community":[],"topic":"3","isMC":false,"answer":"Explanation","discussion":[{"poster":"Daltonic75","upvote_count":"28","timestamp":"1583936880.0","comment_id":"62473","content":"https://www.itexams.com/static/img/exams/Microsoft-AZ-203-1.0/xmlfile-177_1.png","comments":[{"upvote_count":"19","comment_id":"132793","timestamp":"1594538580.0","poster":"chaudh","content":"https://vceguide.com/wp-content/uploads/2020/05/Microsoft-AZ-203-date-05-08-2020_Page_143_Image_0001.jpg"},{"content":"Daltonic75, I don't know who you are. But clearly, examtopics should pay you for every subscriptions they receive as you did 50% of their work. Thank you.","comments":[{"timestamp":"1606054500.0","content":"The link provided by him is not relevant here :)\nIt is meant for other question.","upvote_count":"6","poster":"bhushan_786","comment_id":"225003"},{"upvote_count":"1","timestamp":"1638736200.0","poster":"altafpatel1984","comment_id":"494639","content":"but this time he did wrong job ! posted different image."}],"comment_id":"64638","timestamp":"1584353340.0","upvote_count":"68","poster":"SilverSea"}]},{"upvote_count":"15","timestamp":"1597662780.0","content":"No,Yes,No,Yes","comment_id":"159907","poster":"Ric0720"},{"content":"NO \nYES.\nYES. All record are retuned to the client. The máximum value are 1000. If so, tokenccontinuation returned and we can use: ExecuteQuerySegmentedAsync\nYES. By default PartitionKey and RowKey are already included by default in the select clause.\n\nhttps://www.examtopics.com/discussions/microsoft/view/16363-exam-az-203-topic-24-question-13-discussion/","timestamp":"1612632360.0","poster":"Juanlu","upvote_count":"1","comment_id":"284997"},{"content":"In my opinion, all answer are correct as the solution mention, so: YES, NO, NO, NO !","upvote_count":"1","comment_id":"284994","timestamp":"1612632000.0","poster":"Juanlu"},{"comment_id":"188622","poster":"NPaul_92","timestamp":"1601240820.0","content":"https://vceguide.com/where-the-player-score-exceeds-15000-line-numbers-are-included-for-reference-only/","upvote_count":"2"},{"content":"Third box should be NO. The client will get not all records, but those fitting the query filter.","timestamp":"1593520920.0","upvote_count":"12","comment_id":"123506","poster":"nikos2001","comments":[{"comment_id":"125975","timestamp":"1593845400.0","content":"Box 3 is No. If .Take() is not specified, a maximum of 1000 records are returned to the client. There is no way all records are returned to the client, what if we have 20 mil records?\nReference: https://docs.microsoft.com/bs-latn-ba/java/api/com.microsoft.azure.storage.table.tablequery.take?view=azure-java-legacy","upvote_count":"7","poster":"godsbane"}]},{"timestamp":"1592112060.0","poster":"Praveen24by7","content":"The code queries the Azure table and retrieves the TimePlayed property from the table - No\nThe code will display a maximum of twenty records - Yes\nAll records will be sent to the client. The client will display records for scores greater than or equal to 15,000. - Yes\nThe scoreItem.Key property of the KeyValuePairs that ExecuteQuery returns will contain a value for PlayerID. - Yes","comments":[{"poster":"pranay","upvote_count":"5","comment_id":"128699","comments":[{"upvote_count":"1","comment_id":"247141","content":"Then explain why","poster":"Chilred","timestamp":"1608278940.0"}],"content":"Third answer should be NO","timestamp":"1594105500.0"}],"comment_id":"109885","upvote_count":"4"},{"poster":"saeza100","comment_id":"100710","timestamp":"1591089660.0","upvote_count":"1","content":"Box4 should be \"No\" , we select just \"Score\" with \"Query\"","comments":[{"comment_id":"128703","upvote_count":"1","timestamp":"1594105560.0","content":"PlayerId is a rowKey so it will be returned anyhow. You don't need to mention rowKey and PartitionKey.","poster":"pranay"},{"poster":"Juanlu","content":"yes, but as I know, Partitionkey and Rowkey are also included by default, so de answer should be YES.","timestamp":"1612631820.0","comment_id":"284991","upvote_count":"1"}]},{"timestamp":"1590508740.0","content":"options \n\n[Yes|No]The code queries the Azure table and retrieves the TimePlayed property from the table\n[Yes|No]The code will display a maximum of twenty records\n[Yes|No]All records will be sent to the client. The client will display records for scores greater than or equal to 15,000.\n[Yes|No]The scoreItem.Key property of the KeyValuePairs that ExecuteQuery returns will contain a value for PlayerID.","comment_id":"96222","upvote_count":"6","poster":"lau13"},{"upvote_count":"4","poster":"realbart","timestamp":"1585692720.0","comment_id":"69961","content":"Not all records, but only 20 records will not be sent to the client but written to the console"},{"timestamp":"1584157380.0","upvote_count":"8","comment_id":"63710","content":"Box 3: should be false, please suggest","comments":[{"timestamp":"1584309720.0","upvote_count":"1","content":"Why do you think so? I think it is correct.","comment_id":"64511","poster":"VMCoder","comments":[{"poster":"LEOPOLD","content":"Same conclusion. I thought it would be deferred execution and iteration over the query with execute it so you have a query with .take 20 in it and when iterating you dont get all records sent to the client. (I sure hope thats what happens. Imagine that you had 2000000000 records in that DB all sent to client :D)","upvote_count":"5","comment_id":"70022","timestamp":"1585721160.0"},{"poster":"khrystyna","timestamp":"1585867980.0","upvote_count":"2","comment_id":"70560","content":"It can be because of method order:\nSelect - get all records to client\nWhere - in memory filtering on client side\nTake - return just 20 records"}]},{"content":"the partition key is the game id, and select is not on the partition key. In such a case all records are retrieved, called Table Scan, and therefore Client will filter records from the data fetched.","upvote_count":"2","comment_id":"157995","poster":"Ummara","timestamp":"1597398480.0"},{"poster":"Ummara","timestamp":"1597398600.0","upvote_count":"2","comment_id":"157996","content":"A Table Scan does not include the PartitionKey and is very inefficient because it searches all of the partitions that make up your table in turn for any matching entities. It will perform a table scan regardless of whether or not your filter uses the RowKey. For example: $filter=LastName eq 'Jones'\n\nhttps://docs.microsoft.com/en-us/azure/storage/tables/table-storage-design-for-query"}],"poster":"sagnikmukh"}],"timestamp":"2020-02-29 15:36:00","question_images":["https://www.examtopics.com/assets/media/exam-media/02838/0012200001.png","https://www.examtopics.com/assets/media/exam-media/02838/0012300001.png","https://www.examtopics.com/assets/media/exam-media/02838/0012300002.png","https://www.examtopics.com/assets/media/exam-media/02838/0012300003.png"],"answer_ET":"Explanation","answer_description":"Box 1: No -\n\nBox 2: Yes -\nThe TableQuery.Take method defines the upper bound for the number of entities the query returns.\nExample:\nquery.Take(10);\n\nBox 3: Yes -\n\nBox 4: Yes -\nReferences:\nhttps://www.vkinfotek.com/azureqa/how-do-i-query-azure-table-storage-using-tablequery-class.html","answer_images":[],"unix_timestamp":1582986960,"url":"https://www.examtopics.com/discussions/microsoft/view/15027-exam-az-203-topic-3-question-14-discussion/"},{"id":"4QILX1tiL8rWI5gGWaON","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/23180-exam-az-203-topic-3-question-15-discussion/","choices":{"A":"Yes","B":"No"},"topic":"3","discussion":[{"content":"Redis Cache is better for this but Posgre would also work (although it performs in this case worse regarding scalability). However the question is misleading since it wants you to select B (No) but it would be a valid choice although inefficient...","comments":[{"timestamp":"1606972860.0","poster":"pm_ufa","upvote_count":"3","content":"I totally agree, PgSQL by itself is a viable option, but i think extra conditions like `access to the same session state data for multiple readers and a single writer` and `Save full HTTP responses for concurrent requests` are meant to invalidate it","comment_id":"233685"},{"comment_id":"236210","timestamp":"1607236200.0","poster":"bhushan_786","comments":[{"poster":"m_siri","content":"the provided answer is correct\nref: https://docs.microsoft.com/en-us/azure/architecture/best-practices/caching#caching-session-state-and-html-output","upvote_count":"2","timestamp":"1608830820.0","comment_id":"251673"}],"content":"So what should be answer to it? Yes or No??","upvote_count":"1"}],"upvote_count":"18","timestamp":"1592212500.0","poster":"Drunken","comment_id":"110694"}],"answer_ET":"B","answers_community":[],"timestamp":"2020-06-15 11:15:00","question_images":[],"question_id":93,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou are developing and deploying several ASP.Net web applications to Azure App Service. You plan to save session state information and HTML output. You must use a storage mechanism with the following requirements:\n✑ Share session state across all ASP.NET web applications\n✑ Support controlled, concurrent access to the same session state data for multiple readers and a single writer\n✑ Save full HTTP responses for concurrent requests\nYou need to store the information.\nProposed Solution: Deploy and configure an Azure Database for PostgreSQL. Update the web applications.\nDoes the solution meet the goal?","exam_id":47,"answer_description":"","answer_images":[],"unix_timestamp":1592212500,"answer":"B"},{"id":"cUhEMZQ6tYSYDqiLN3d8","timestamp":"2019-11-24 18:13:00","answer_ET":"B","answer":"B","choices":{"A":"Yes","B":"No"},"unix_timestamp":1574615580,"question_id":94,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou are developing and deploying several ASP.Net web applications to Azure App Service. You plan to save session state information and HTML output. You must use a storage mechanism with the following requirements:\n✑ Share session state across all ASP.NET web applications\n✑ Support controlled, concurrent access to the same session state data for multiple readers and a single writer\n✑ Save full HTTP responses for concurrent requests\nYou need to store the information.\nProposed Solution: Deploy and configure Azure Cache for Redis. Update the web applications.\nDoes the solution meet the goal?","url":"https://www.examtopics.com/discussions/microsoft/view/9019-exam-az-203-topic-3-question-16-discussion/","question_images":[],"answer_description":"","discussion":[{"upvote_count":"65","timestamp":"1574615580.0","comment_id":"24108","content":"Wrong key - it should be YES: https://docs.microsoft.com/en-us/azure/architecture/best-practices/caching?source=docs#caching-session-state-and-html-output","poster":"EYIT"},{"content":"One more vote for the A answer!!","upvote_count":"17","poster":"gclopes","comment_id":"56883","timestamp":"1582987500.0"},{"content":"Correct Answer- Yes","upvote_count":"2","poster":"ss22","timestamp":"1613049300.0","comment_id":"288305"},{"poster":"goudigubba","upvote_count":"1","comment_id":"287430","content":"Its a simple question yet the answer is given as wrong. No point of providing wrong answers.","timestamp":"1612945320.0"},{"poster":"bashirk","content":"Correct Ans. Yes","comment_id":"160722","upvote_count":"2","timestamp":"1597738620.0"},{"content":"Today I got 900, I had this question, my answer: Yes","timestamp":"1596892200.0","upvote_count":"7","comment_id":"153052","poster":"hertino","comments":[{"timestamp":"1597107180.0","content":"Hey hertino, if you have the list of all the question with answer. Could you please share it","comment_id":"154918","poster":"Anamika7","upvote_count":"14"}]},{"comment_id":"106702","content":"YES\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/caching","timestamp":"1591785060.0","upvote_count":"1","poster":"mcampos"},{"poster":"AK89","timestamp":"1590510660.0","content":"It is YES","comment_id":"96250","upvote_count":"2"},{"comment_id":"84611","poster":"Dayakar","upvote_count":"5","timestamp":"1588779360.0","content":"It should be \"Yes\", because using redis cache store the session information and use it for later point of time."},{"upvote_count":"4","poster":"dollarpo7","timestamp":"1588177920.0","content":"Answer is Yes","comment_id":"81322"},{"upvote_count":"12","poster":"heero","content":"Answer: A","timestamp":"1582814580.0","comment_id":"56045"},{"upvote_count":"11","content":"Correct answer is A (Yes)","timestamp":"1579878120.0","poster":"Khang","comment_id":"42364"},{"poster":"shanky_007","upvote_count":"6","content":"correct Ans: Yes","timestamp":"1576913640.0","comment_id":"31439"},{"upvote_count":"13","timestamp":"1574673960.0","comment_id":"24239","poster":"mohanadzaidi","content":"Should be Yes ! thanks EYIT"}],"isMC":true,"answers_community":[],"topic":"3","answer_images":[],"exam_id":47},{"id":"eGSot3m9IPwvNtoevbsc","answers_community":[],"choices":{"A":"Yes","B":"No"},"url":"https://www.examtopics.com/discussions/microsoft/view/23125-exam-az-203-topic-3-question-17-discussion/","answer_images":[],"exam_id":47,"isMC":true,"question_id":95,"answer_description":"References:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-programming-guide","discussion":[{"timestamp":"1598149380.0","comment_id":"164049","comments":[{"comment_id":"279188","poster":"Brak","timestamp":"1611909660.0","content":"Wrong. See my response above.","upvote_count":"1"},{"upvote_count":"4","content":"This is wrong. You're confusing Partition Keys with creating partitions.","poster":"clarionprogrammer","timestamp":"1618498800.0","comment_id":"336373"}],"poster":"Larry616","upvote_count":"12","content":"Answer should be B-No\nBecause total partitions has a max# of 32, but in the question it uses machine idetifier as partation key, there're 2000 * 5 = 10,000 machines here\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions\nThe number of partitions is specified at creation and must be between 2 and 32"},{"content":"1. Partition Key is not mapped 1:1 to partition. 2. Store not accumulate data per each device. Each device send data separately. 3. 2MB per 24 hours are not sent at once. 4. Event Hub supposed to take events/messages from big amount of devices. 5. Event still a message (small one). You can put information you need to store in Blob. 6. To store message in Blob with Event Hub is easy. Just switch on capture setting.","poster":"Secure01","upvote_count":"8","comments":[{"upvote_count":"1","content":"Correct. Event Hub + Capture is a no-code solution that can scale.","comment_id":"279189","timestamp":"1611909780.0","poster":"Brak"}],"timestamp":"1606227960.0","comment_id":"226734"},{"content":"Got it on 03/2022, I chose A. Yes","upvote_count":"2","timestamp":"1647142860.0","poster":"meoukg","comment_id":"566519"},{"comments":[{"upvote_count":"1","content":"Yes is the correct answer\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-dedicated-overview#event-hubs-dedicated-quotas-and-limits","poster":"Esward","comment_id":"801662","timestamp":"1675830780.0"}],"comment_id":"262333","poster":"RahulKate","timestamp":"1610083020.0","content":"Partitions - 2000 per CU - Dedicated Plan - So we can have 2K+ partitions in Event Hub\nSize allowed - Yes\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-dedicated-overview#event-hubs-dedicated-quotas-and-limits","upvote_count":"2"},{"comment_id":"150166","timestamp":"1596515580.0","upvote_count":"7","content":"It is not Event Hub for the following reasons:\n- \"The event data has information about what happened but doesn't have the data that triggered the event.\" https://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services\n- \"Maximum size of Event Hubs event: 1 MB (standard)\". From the question \"A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.\" That means the max size from one store could be 2MB to 10MB.\n\nService Bus is correct for the following reasons:\n- \"A message is raw data produced by a service to be consumed or stored elsewhere.\"\n- \"Order processing and financial transactions\"\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services","poster":"ExamStudent123"},{"comments":[{"content":"In this like MS is talking about using device identity for partition key under Event Hub:\nThe temperature sensors discussed earlier each emit a distinct stream that will be kept together using the Event Hub partitioning model, using the identity of the device as the partitioning key.","timestamp":"1595296860.0","upvote_count":"1","comment_id":"139970","poster":"LTiwana"}],"comment_id":"125989","content":"Answer should be NO.\nhttps://azure.microsoft.com/es-es/blog/events-data-points-and-messages-choosing-the-right-azure-messaging-service-for-your-data/","upvote_count":"5","poster":"godsbane","timestamp":"1593846780.0"},{"upvote_count":"1","content":"but, if we should transfer each sale from POS (not the entire daily batch at once), I'd go with Service Bus","poster":"nickbilak","timestamp":"1592129940.0","comment_id":"110027"},{"content":"scratch that, eventdatabatch.tryadd is not for service bus, but for event hub. SB has 256K limit. then the right solution could be event grid triggered event for consumers upon blob upload, that event will have id/link to the blob with 2MB of POS data. Event hub can have only up to 32 partitions, so answer A has no sense.","comment_id":"110025","poster":"nickbilak","timestamp":"1592129580.0","comments":[{"poster":"Brak","timestamp":"1611909600.0","upvote_count":"1","content":"32 is the limit for physical partitions. Each device id is hashed to choose a physical partition.","comment_id":"279187"}],"upvote_count":"2"},{"comment_id":"110009","upvote_count":"5","poster":"nickbilak","timestamp":"1592127420.0","content":"I think Service Bus is correct as it's used for financial transactions (with guaranteed delivery) and allows batches up to 4MB - https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicebus.messaging.eventdatabatch.tryadd?view=azure-dotnet#remarks"},{"upvote_count":"2","timestamp":"1592127120.0","poster":"nickbilak","comment_id":"110007","content":"The event hub message size limit is 1MB. How can be answer A be correct, if the requirement is 2MB of data?","comments":[{"comments":[{"content":"I think it's Yes.\nIn this linked doc, it gives an example at the end. The example use Event hubs to 'Capture file' to Storage, and then 'Capture event' to next step.","poster":"rrongcheng","comment_id":"140927","timestamp":"1595400660.0","upvote_count":"3"}],"upvote_count":"6","content":"But the question mentions 2MB data every 24 hours and doesn't specify if the data needs to batched or streamed (Event Hub accepts both).\n\nIt's confusion, but the answer might be correct. Here is a good comparison between the three services:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services","comment_id":"139961","poster":"LTiwana","timestamp":"1595296080.0"}]}],"timestamp":"2020-06-14 11:32:00","topic":"3","answer_ET":"A","unix_timestamp":1592127120,"answer":"A","question_images":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.\nYou must store the device in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Event Hub. Configure the machine identifier as the partition key and enable capture.\nDoes the solution meet the goal?"}],"exam":{"name":"AZ-203","isImplemented":true,"isMCOnly":false,"numberOfQuestions":144,"provider":"Microsoft","id":47,"isBeta":false,"lastUpdated":"12 Apr 2025"},"currentPage":19},"__N_SSP":true}