{"pageProps":{"questions":[{"id":"JsYOo64P8C5Al09pMtHl","timestamp":"2021-12-10 10:10:00","isMC":true,"answer_images":[],"question_images":[],"question_id":56,"choices":{"A":"Type 0","B":"Type 1","C":"Type 2","D":"Type 3"},"answer_ET":"C","answer_description":"","unix_timestamp":1639127400,"question_text":"You are designing a dimension table for a data warehouse. The table will track the value of the dimension attributes over time and preserve the history of the data by adding new rows as the data changes.\nWhich type of slowly changing dimension (SCD) should you use?","answers_community":["C (100%)"],"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/67534-exam-dp-203-topic-1-question-43-discussion/","answer":"C","discussion":[{"content":"Correct","upvote_count":"18","comment_id":"498462","poster":"gf2tw","timestamp":"1639127400.0"},{"upvote_count":"11","poster":"virendrapsingh","comment_id":"609247","timestamp":"1653920100.0","content":"Kind of question that teacher leaves in paper for one free mark."},{"comment_id":"1364738","upvote_count":"1","poster":"Stuartsen","content":"Selected Answer: C\nsince adding a new row","timestamp":"1741065960.0"},{"upvote_count":"1","poster":"EmnCours","content":"Selected Answer: C\nCorrect Answer: C","timestamp":"1732771500.0","comment_id":"1319031"},{"upvote_count":"1","poster":"hassexat","timestamp":"1694069220.0","content":"Selected Answer: C\nPreserve history of changes... Type 2 is correct","comment_id":"1001242"},{"content":"Selected Answer: C\nSCD2 is correct","poster":"kkk5566","comment_id":"997625","timestamp":"1693745580.0","upvote_count":"1"},{"poster":"akhil5432","upvote_count":"1","content":"Selected Answer: C\nType 2","comment_id":"974447","timestamp":"1691390040.0"},{"comment_id":"792047","poster":"GodfreyMbizo","upvote_count":"3","content":"Type 2 is correct","timestamp":"1675024800.0"},{"upvote_count":"3","timestamp":"1672899600.0","comment_id":"766286","content":"Type 2- Mainitains Row for each version on dimension data.","poster":"LokeshJ"},{"poster":"dimbrici","comment_id":"728031","upvote_count":"3","timestamp":"1669536660.0","content":"Selected Answer: C\nCorrect"},{"timestamp":"1668159120.0","upvote_count":"4","comment_id":"715915","content":"Selected Answer: C\nCorrect:\ntype 1 - does not keep history of previuos values\ntype 3 - updates all rows, not required","poster":"kl8585"},{"timestamp":"1658766720.0","upvote_count":"1","poster":"Deeksha1234","content":"right answer given","comment_id":"636825"},{"comment_id":"625624","poster":"NikeJDI","content":"C is right answer","timestamp":"1656660120.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1655642820.0","poster":"Remedios79","content":"correct","comment_id":"618690"},{"poster":"SandipSingha","upvote_count":"1","content":"correct","timestamp":"1652083740.0","comment_id":"598938"},{"upvote_count":"1","timestamp":"1652082060.0","comment_id":"598929","poster":"SandipSingha","content":"correct"},{"content":"Correct","timestamp":"1651638480.0","poster":"AZ9997989798979789798979789797","upvote_count":"1","comment_id":"596504"},{"upvote_count":"2","content":"Selected Answer: C\nCorrect!","poster":"Onobhas01","timestamp":"1648737600.0","comment_id":"579007"},{"upvote_count":"1","content":"Selected Answer: C\nCorrect","timestamp":"1645502880.0","poster":"surya610","comment_id":"553434"},{"content":"Selected Answer: C\ncorrect","comment_id":"534537","poster":"PallaviPatel","upvote_count":"1","timestamp":"1643358660.0"},{"timestamp":"1641636000.0","upvote_count":"1","comment_id":"519438","poster":"saupats","content":"correct"},{"upvote_count":"1","content":"correct","comment_id":"516444","poster":"ANath","timestamp":"1641290880.0"}],"exam_id":67},{"id":"0uWikbQByDPtyMcfzOVD","answers_community":[],"topic":"1","answer_description":"Step 1: Create an external data source that uses the abfs location\nCreate External Data Source to reference Azure Data Lake Store Gen 1 or 2\nStep 2: Create an external file format and set the First_Row option.\nCreate External File Format.\nStep 3: Use CREATE EXTERNAL TABLE AS SELECT (CETAS) and configure the reject options to specify reject values or percentages\nTo use PolyBase, you must create external tables to reference your external data.\nUse reject options.\nNote: REJECT options don't apply at the time this CREATE EXTERNAL TABLE AS SELECT statement is run. Instead, they're specified here so that the database can use them at a later time when it imports data from the external table. Later, when the CREATE TABLE AS SELECT statement selects data from the external table, the database will use the reject options to determine the number or percentage of rows that can fail to import before it stops the import.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql","url":"https://www.examtopics.com/discussions/microsoft/view/82714-exam-dp-203-topic-1-question-44-discussion/","question_text":"DRAG DROP -\nYou have data stored in thousands of CSV files in Azure Data Lake Storage Gen2. Each file has a header row followed by a properly formatted carriage return (/ r) and line feed (/n).\nYou are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase.\nYou need to skip the header row when you import the files into the data warehouse. Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nNOTE: Each correct selection is worth one point\nSelect and Place:\n//IMG//","discussion":[{"upvote_count":"39","poster":"sunil_smile","timestamp":"1679173200.0","content":"1) create database scoped credentials\n2) create external source\n3) create file format\n4) create external table (it not supports CTAS)","comment_id":"672683","comments":[{"poster":"RMK2000","comment_id":"1335880","timestamp":"1735877340.0","upvote_count":"2","content":"Please see this. Given answer is correct.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/load-data-overview#3-prepare-the-data-for-loading"},{"comment_id":"937037","content":"It supports, it is a dedicated SQL pool (means not severless), reading the question:\nYou are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase.\n--> provided answers are correct in my opinion.","upvote_count":"6","timestamp":"1703797080.0","poster":"auwia"}]},{"upvote_count":"19","poster":"OldSchool","comment_id":"721968","content":"Because it's saying \"You have data stored in thousands of CSV files in Azure Data Lake Storage Gen2\" and \"You are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase\" assumption is that we already have database credentials, so the answer is:\n1) create external source\n2) create file format\n3) create external table","timestamp":"1684489560.0","comments":[{"timestamp":"1700744040.0","poster":"Rob77","content":"No, CETAS is not used for loading Azure Synapse Analytics. It's used to export data from and not to!","comment_id":"904826","upvote_count":"5"}]},{"comment_id":"1400581","timestamp":"1742400420.0","content":"1) create database scoped credentials\n2) create external source\n3) create file format","poster":"imatheushenrique","upvote_count":"1"},{"timestamp":"1728970080.0","poster":"dgerok","content":"The provided answer is correct.\n1) source\n2) file format\n3) CETAS - this is dedicated SQL Pool. So, it is preferred. While external table is the only option for SERVERLESS SQL Pool...\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=azure-sqldw-latest&tabs=powershell#examples","comment_id":"1195825","upvote_count":"1"},{"comment_id":"1186105","timestamp":"1727694420.0","content":"The correct answer are : \n1) create database scoped credentials\n2) create external source\n3) create file format\nWhy ? : Focus on this below statement in question. \nBefore building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics.\nExplanation: You just need to specify about prerequisite database object ( Not Loading pattern which is about Creating External Table )","upvote_count":"9","poster":"ankeshpatel2112"},{"content":"I think the answer (A, B, D) is very clearly explained on this page: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables","poster":"hydmt07","upvote_count":"2","timestamp":"1717982400.0","comment_id":"1092269"},{"poster":"ellala","content":"I believe the only reason why \"create database scoped credential\" is not a right answer is because it should be a managed identity instead of a service principal. Service principals are used for applications outside of the Azure Environment (such as SQL Server, as some comments here refer to SQL Server documentation). But since we are using Synapse Analytics environment, we use managed identities. \n\nCheck the link: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=powershell#g-use-create-external-table-as-select-with-a-view-as-the-source\n\nAnd if you want to compare to SQL Server documentation, where indeed they use Service principal keys (which is not the situation we are given in this question):\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-azure-data-lake-store","upvote_count":"2","comment_id":"1027818","timestamp":"1712564580.0"},{"timestamp":"1712488500.0","comment_id":"1027266","upvote_count":"2","content":"The provided Answer is correct check yourself, goto F section in the following link\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=azure-sqldw-latest&tabs=powershell#examples","poster":"pperf","comments":[{"timestamp":"1730201220.0","content":"F section shows all the options given in this questions :)\nmeaning including CETAS and data credential also","upvote_count":"1","comment_id":"1203933","poster":"SushilJinder"}]},{"upvote_count":"1","timestamp":"1709896620.0","poster":"[Removed]","content":"Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics - Loading pattern is CETAS, so answer \n\nDSC\nDS\nEFF","comment_id":"1002304"},{"content":"source ,format ,external","timestamp":"1709477760.0","upvote_count":"3","comment_id":"997628","poster":"kkk5566"},{"content":"The answer is correct, \n- Create database scoped credential: \"This step is required only for Kerberos-secured Hadoop clusters.\"\nIn this case, the previous step does not apply.","comment_id":"964828","timestamp":"1706374560.0","upvote_count":"1","poster":"eladioyovera"},{"timestamp":"1703370060.0","content":"1. Create database scoped credential\n2. Create external data source\n3. Create external file format\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-azure-blob-storage","comment_id":"931926","poster":"vctrhugo","upvote_count":"5"},{"upvote_count":"1","timestamp":"1697532360.0","content":"the given answer is correct imo; \"'PolyBase loads can be run using CTAS or INSERT INTO. CTAS will minimize transaction logging and is the fastest way to load your data. \"'\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool#use-polybase-to-load-and-export-data-quickly","comment_id":"872493","poster":"mamahani","comments":[{"comment_id":"872501","poster":"mamahani","upvote_count":"3","timestamp":"1697532780.0","content":"sorry, wrong link: https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16\n\"'PolyBase can now use CETAS to create an external table and then export, in parallel, the result of a Transact-SQL SELECT statement to Azure Data Lake Storage Gen2, Azure Storage Account V2, and S3-compatible object storage.\"'\n\"'Creates an external table and then exports, in parallel, the results of a Transact-SQL SELECT statement.\n\nFor Azure Synapse Analytics and Analytics Platform System, Hadoop or Azure Blob storage are supported.\"'"}]},{"comment_id":"796015","content":"When using serverless SQL pool, CETAS is used to create an external table and export query results to Azure Storage Blob or Azure Data Lake Storage Gen2 and we need to implement a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics, so:\n1) create database scoped credentials\n2) create external source\n3) create file format","poster":"olegjdll","upvote_count":"6","timestamp":"1690971660.0"},{"timestamp":"1689886440.0","poster":"Rakrah","upvote_count":"1","comment_id":"782824","content":"In this question clearly stating that, \n\"Before building the loading pattern, you need to prepare the required database objects\"\nSo Database objects list \n1) Data Source \n2) Data File Format\n3) Table ( need to set up skip the header row)\nMy Answer is 1) Create external source; 2) Create File Format ; 3) Create External Table"},{"comment_id":"767533","timestamp":"1688633280.0","upvote_count":"3","content":"CTAS for external table is to write the result of the query (select) in a destination folder.\nSo the good answer for this question is :\n) create database scoped credentials\n2) create external source\n3) create file format","poster":"aws123"},{"comments":[{"timestamp":"1697532180.0","upvote_count":"1","poster":"mamahani","comment_id":"872491","comments":[{"comment_id":"872499","content":"sorry, wrong link: https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16 \n\"'PolyBase can now use CETAS to create an external table and then export, in parallel, the result of a Transact-SQL SELECT statement to Azure Data Lake Storage Gen2, Azure Storage Account V2, and S3-compatible object storage.\"'\n\"'Creates an external table and then exports, in parallel, the results of a Transact-SQL SELECT statement.\n\nFor Azure Synapse Analytics and Analytics Platform System, Hadoop or Azure Blob storage are supported.\"'","poster":"mamahani","upvote_count":"1","timestamp":"1697532660.0"}],"content":"yes it is an option PolyBase loads can be run using CTAS or INSERT INTO. CTAS will minimize transaction logging and is the fastest way to load your data. \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool#use-polybase-to-load-and-export-data-quickly"}],"poster":"rohanb1986","timestamp":"1687748820.0","comment_id":"757070","content":"Should be -\n1) create database scoped credentials\n2) create external source\n3) create file format\n\nAs per https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16\n- Check under : Create external tables for Azure Blob Storage - CTAS is not an option","upvote_count":"2"},{"timestamp":"1685188200.0","content":"What is azure active directory application? is it managed identity?","comment_id":"728262","upvote_count":"2","poster":"Bro111"},{"upvote_count":"9","poster":"kl8585","timestamp":"1683792480.0","comments":[{"comment_id":"872492","timestamp":"1697532240.0","content":"yes you do use CTAS with polybase , see ms docs \"'PolyBase loads can be run using CTAS or INSERT INTO. CTAS will minimize transaction logging and is the fastest way to load your data. \"'\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool#use-polybase-to-load-and-export-data-quickly","poster":"mamahani","upvote_count":"1"},{"content":"In the link below they make it very clear.\nFirst, database scoped.\n2nd External Data source,\n3rd, File format.\n\nOnly after that you can create an external table in polybase\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16","poster":"RBKasemodel","comments":[{"comment_id":"717919","content":"thank you!","timestamp":"1684057680.0","upvote_count":"1","poster":"kl8585"},{"comments":[{"poster":"fionacanderson","timestamp":"1693224900.0","comment_id":"824893","upvote_count":"2","content":"you didnt scroll down the page far enough"}],"upvote_count":"2","comment_id":"730747","timestamp":"1685378880.0","content":"The first step is only required only for Kerberos-secured Hadoop clusters.","poster":"Billybob0604"}],"comment_id":"717441","upvote_count":"5","timestamp":"1683993000.0"}],"content":"I have many doubts on this question. Two points on which i focused:\n 1) The question states \"Before building the loading pattern, you need to prepare the required database objects\", so it doesn't seem to fit with CETAS statement because is asking about actions to do BEFORE implementing the loading pattern\n 2) With PolyBase you don't use CETAS statement, you should use CET with LOCATION option to create external table from Data Lake\n\nGiven these points, i will go with:\n - Create database scoped credentials\n - Create external data source\n - Create file format\n\nI will appreciate anyone confirming or confuting this answer. Thank you!","comment_id":"715941"},{"poster":"Igor85","timestamp":"1683202800.0","upvote_count":"5","content":"again, a question that is lacking precision. it suggests choosing three actions whereas all 4 makes sense, there is no indication that database scoped credentials are already in place","comment_id":"711208"},{"upvote_count":"1","comment_id":"706395","content":"the giving solution is correct","timestamp":"1682681160.0","poster":"allagowf"},{"timestamp":"1680538260.0","upvote_count":"5","comment_id":"685651","content":"1) create database scoped credentials\n2) create external source\n3) create file format\n4) create external table (it not supports CTAS)","poster":"smsme323"}],"isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0008800001.jpg"],"unix_timestamp":1663527600,"answer":"","timestamp":"2022-09-18 21:00:00","answer_ET":"","exam_id":67,"question_id":57,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0008800002.jpg"]},{"id":"c1B88dUPrbxt5ktxHCGT","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0009000001.png"],"unix_timestamp":1639127160,"url":"https://www.examtopics.com/discussions/microsoft/view/67531-exam-dp-203-topic-1-question-45-discussion/","question_text":"HOTSPOT -\nYou are building an Azure Synapse Analytics dedicated SQL pool that will contain a fact table for transactions from the first half of the year 2020.\nYou need to ensure that the table meets the following requirements:\n✑ Minimizes the processing time to delete data that is older than 10 years\n✑ Minimizes the I/O for queries that use year-to-date values\nHow should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2021-12-10 10:06:00","answer_description":"Box 1: PARTITION -\nRANGE RIGHT FOR VALUES is used with PARTITION.\nPart 2: [TransactionDateID]\nPartition on the date column.\nExample: Creating a RANGE RIGHT partition function on a datetime column\nThe following partition function partitions a table or index into 12 partitions, one for each month of a year's worth of values in a datetime column.\nCREATE PARTITION FUNCTION [myDateRangePF1] (datetime)\nAS RANGE RIGHT FOR VALUES ('20030201', '20030301', '20030401',\n'20030501', '20030601', '20030701', '20030801',\n'20030901', '20031001', '20031101', '20031201');\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-partition-function-transact-sql","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0009100001.jpg"],"discussion":[{"comment_id":"498456","content":"Correct","poster":"gf2tw","upvote_count":"25","timestamp":"1670663160.0"},{"timestamp":"1699881420.0","upvote_count":"7","content":"Correct answer, giveaway is \"RANGE RIGHT\"","comment_id":"717321","poster":"OldSchool"},{"content":"Have a small doubt.\nWe are creating partition on what field?\nShouldn't it be the columnstore index.","upvote_count":"1","comment_id":"1025108","timestamp":"1728070080.0","comments":[{"comment_id":"1336540","poster":"hypersam","timestamp":"1736024700.0","content":"on \"TransactionDateId\" field, which is the second answer","upvote_count":"1"}],"poster":"M_Anas_007"},{"comment_id":"997630","content":"Partition","upvote_count":"1","timestamp":"1725368220.0","poster":"kkk5566"},{"content":"correct answer given","timestamp":"1706561640.0","poster":"GodfreyMbizo","upvote_count":"5","comment_id":"792055"},{"content":"ans is correct","poster":"Deeksha1234","upvote_count":"3","comment_id":"636830","timestamp":"1690303500.0"},{"content":"I can see Keyword \"Range right for values\" pointing to \"Partition\", then \"TransactionDateID\"\n is the column on which partition needs to be done, rather than the TransactionID.","upvote_count":"4","timestamp":"1688196780.0","comment_id":"625627","poster":"NikeJDI"},{"content":"How are we ensuring \"Minimizes the processing time to delete data that is older than 10 years\"?","upvote_count":"2","comments":[{"upvote_count":"4","timestamp":"1686115440.0","comment_id":"612590","poster":"Aditya0891","comments":[{"timestamp":"1696012440.0","poster":"allagowf","comment_id":"682969","upvote_count":"3","content":"it's a date_ID and it's correct it will be linked to a date table via the date_ID to get the date"}],"content":"while deleting we can use switch partition. It is efficient than delete statement, so partition by date column in good but the question says the TransactionDate as int field which is wrong. It should be date type"}],"comment_id":"596024","poster":"gabdu","timestamp":"1683028680.0"},{"timestamp":"1678287840.0","upvote_count":"2","content":"Correct","comment_id":"563336","poster":"wwdba"},{"poster":"PallaviPatel","comment_id":"534575","timestamp":"1674899100.0","content":"correct","upvote_count":"2"},{"upvote_count":"2","content":"correct","timestamp":"1673172360.0","comment_id":"519441","poster":"saupats"}],"answer":"","answers_community":[],"answer_ET":"","isMC":false,"question_id":58,"exam_id":67,"topic":"1"},{"id":"A0kWDTFGrrhjeHZ4N365","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0009300001.jpg"],"unix_timestamp":1639127160,"url":"https://www.examtopics.com/discussions/microsoft/view/67530-exam-dp-203-topic-1-question-46-discussion/","question_text":"You are performing exploratory analysis of the bus fare data in an Azure Data Lake Storage Gen2 account by using an Azure Synapse Analytics serverless SQL pool.\nYou execute the Transact-SQL query shown in the following exhibit.\n//IMG//\n\nWhat do the query results include?","timestamp":"2021-12-10 10:06:00","answer_description":"","discussion":[{"upvote_count":"19","comment_id":"498455","comments":[{"comments":[{"timestamp":"1707207000.0","poster":"Gikan","comment_id":"1141918","content":"And therefore the D is not exactly true.","upvote_count":"1"}],"comment_id":"1141917","content":"Just a tricky question to think about:\nIs the csv/bufare/tripdata_2020_something/something/something.csv file will be loaded?\nI think YES :) But it is not matched with answer A, because not ONLY the tripdata_2020 folder will be loaded.","timestamp":"1707206940.0","poster":"Gikan","upvote_count":"1"}],"timestamp":"1639127160.0","content":"Correct","poster":"gf2tw"},{"upvote_count":"5","timestamp":"1675492380.0","poster":"akshaynag95","content":"Selected Answer: D\nD is the correct Answer","comment_id":"797655"},{"timestamp":"1732772460.0","upvote_count":"1","comment_id":"1319033","content":"Selected Answer: D\nCorrect Answer: D","poster":"EmnCours"},{"comment_id":"1001248","content":"Selected Answer: D\nCorrect is D","timestamp":"1694069520.0","upvote_count":"1","poster":"hassexat"},{"comment_id":"997632","content":"Selected Answer: D\nD is correct","timestamp":"1693745820.0","upvote_count":"1","poster":"kkk5566"},{"upvote_count":"3","poster":"henryphchan","timestamp":"1683042780.0","content":"Selected Answer: D\ncorrect","comment_id":"887639"},{"content":"Selected Answer: D\nD is the correct Answer","timestamp":"1675350900.0","comment_id":"796156","upvote_count":"3","poster":"SHENOOOO"},{"comment_id":"789474","upvote_count":"2","timestamp":"1674812040.0","comments":[{"comment_id":"800199","content":"Only file, there is no / after 2020 in the exercise.","upvote_count":"2","timestamp":"1675712640.0","poster":"Anton2020"}],"content":"Selected Answer: D\nSorry but I don't understand.\nFile or Directory that start with \"tripdata_2020\" can selected.\n/tripdata_2020/a.csv\n/tripdata_2020_a_b.csv\n/tripdata_2020/2020/1/1/myfile.csv\nSo question is very not clear.\nD question is partially correct","poster":"panda_azzurro"},{"poster":"Stokstaartje420","content":"Answer D is correct. Would be great if it was also correct grammatically.","timestamp":"1674715260.0","upvote_count":"4","comment_id":"788444"},{"comment_id":"715945","timestamp":"1668161580.0","content":"Selected Answer: D\nCorrect","poster":"kl8585","upvote_count":"1"},{"poster":"GauravPurandare","content":"Selected Answer: D\nCSV that have file names that beginning with \"tripdata_2020\" .","upvote_count":"2","comment_id":"644844","timestamp":"1660116900.0"},{"content":"D is correct","comment_id":"636832","timestamp":"1658767680.0","upvote_count":"1","poster":"Deeksha1234"},{"poster":"objecto","comment_id":"615161","content":"Selected Answer: D\nCorrect","timestamp":"1655011380.0","upvote_count":"2"},{"poster":"Rrk07","comment_id":"607926","content":"D is correct","timestamp":"1653630120.0","upvote_count":"1"},{"content":"on this one you need to pay attention to wording","timestamp":"1650197040.0","poster":"Egocentric","comment_id":"587203","upvote_count":"3"},{"comment_id":"583148","poster":"jskibick","upvote_count":"1","content":"Selected Answer: D\nD all good","timestamp":"1649483280.0"},{"content":"Selected Answer: D\nD is correct","upvote_count":"1","comment_id":"581653","timestamp":"1649228220.0","poster":"sarapaisley"},{"upvote_count":"1","content":"Selected Answer: D\nCorrect","poster":"SebK","comment_id":"573153","timestamp":"1647973500.0"},{"comment_id":"572451","comments":[{"poster":"Aditya0891","upvote_count":"2","content":"Beacuase option C states that all csv files including 'tripdata_2020\". You can see where the where the wild card is mentioned, it's after tripdata_2020(*) so option D is correct all csv files starting with tripdata_2020. Tripdata_2020 should come first than the wild card can take anything and the file should end with .csv","timestamp":"1654579860.0","comment_id":"612591"}],"upvote_count":"1","timestamp":"1647889380.0","poster":"DingDongSingSong","content":"Why is option C not correct, when the code has \"tripdata_2020*.csv\" which means that a wild card is used with \"tripdata_2020\" csv files. So, example tripdata_2020A.csv, tripdata_2020B.csv, tripdata_2020YZ.csv, all 3 would be queried. Option D does not make sense, even gramatically"},{"upvote_count":"2","poster":"PallaviPatel","timestamp":"1643363340.0","content":"Selected Answer: D\ncorrect","comment_id":"534579"},{"comment_id":"521212","poster":"anto69","timestamp":"1641863700.0","content":"No doubts is correct, no doubts is ans D","upvote_count":"1"},{"upvote_count":"1","comment_id":"499974","timestamp":"1639309140.0","content":"Why not B?","comments":[{"poster":"Nifl91","content":"Because of the .csv at the end","upvote_count":"3","timestamp":"1639433460.0","comment_id":"500924"}],"poster":"duds19"}],"answer_images":[],"answer":"D","choices":{"D":"Only CSV that have file names that beginning with \"tripdata_2020\".","B":"All files that have file names that beginning with \"tripdata_2020\".","C":"All CSV files that have file names that contain \"tripdata_2020\".","A":"Only CSV files in the tripdata_2020 subfolder."},"answers_community":["D (100%)"],"answer_ET":"D","isMC":true,"question_id":59,"exam_id":67,"topic":"1"},{"id":"6jxhvUtmCv2AULPHw0Xj","isMC":false,"exam_id":67,"answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0009400001.png","https://www.examtopics.com/assets/media/exam-media/04259/0009400002.png","https://www.examtopics.com/assets/media/exam-media/04259/0009500001.jpg"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0009500002.jpg"],"answer":"","question_id":60,"timestamp":"2022-04-27 12:12:00","answer_description":"Box 1: select -\n\nBox 2: explode -\n\nBop 3: alias -\npyspark.sql.Column.alias returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).\nReference:\nhttps://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html https://docs.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/explode","topic":"1","answer_ET":"","question_text":"DRAG DROP -\nYou use PySpark in Azure Databricks to parse the following JSON input.\n//IMG//\n\nYou need to output the data in the following tabular format.\n//IMG//\n\nHow should you complete the PySpark code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the spit bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","discussion":[{"poster":"kb8bo","timestamp":"1686919740.0","upvote_count":"24","content":"The final line with the blank looks incorrect... surely it should be:\nexplode(\"persons.dogs\").alias(\"dog\"))\n\n(Assuming this, the answer is correct, otherwise I don't think it makes any sense).","comment_id":"617260"},{"comment_id":"826824","content":"ah \"persons\".alias(\"persons\") what a fun and useful and nice alias","upvote_count":"18","poster":"urassi","timestamp":"1709381400.0","comments":[{"poster":"hypersam","upvote_count":"2","content":"actually if you don't use alias after explode(\"persons\"), the column name would be \"col\" so it's mandatory here.","comment_id":"1336547","timestamp":"1736026260.0"}]},{"poster":"kkk5566","timestamp":"1725368340.0","upvote_count":"2","content":"syntax is correct","comment_id":"997635"},{"timestamp":"1710480780.0","poster":"esaade","upvote_count":"12","content":"dbutils.fs.put(\"/tmp/source.json\", source_json, True)\nsource_df = spark.read.option(\"multiline\", \"true\").json(\"/tmp/source.json\")\npersons = source_df.select(explode(\"persons\").alias(\"persons\"))\npersons_dogs = persons.select(col(\"persons.name\").alias(\"owner\"), col(\"persons.age\").alias(\"age\"), explode(col(\"persons.dog\")).alias(\"dog_name\"))\npersons_dogs.display()","comment_id":"839562"},{"comment_id":"636835","upvote_count":"4","poster":"Deeksha1234","timestamp":"1690304100.0","content":"Correct"},{"comments":[{"poster":"Anton2020","timestamp":"1708718580.0","comment_id":"819683","upvote_count":"3","content":"The column name in the json is dogs, not dog"}],"comment_id":"635112","poster":"Dicer","timestamp":"1690019640.0","content":"Correct, but last .alias(\"dog\") is quite unnecessary because the column name is alredy 'dog'. I guess that is for safety measurement.","upvote_count":"5"},{"poster":"galacaw","timestamp":"1682590320.0","content":"Correct","comment_id":"593030","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/microsoft/view/74667-exam-dp-203-topic-1-question-47-discussion/","unix_timestamp":1651054320}],"exam":{"isBeta":false,"name":"DP-203","isImplemented":true,"id":67,"numberOfQuestions":384,"provider":"Microsoft","isMCOnly":false,"lastUpdated":"12 Apr 2025"},"currentPage":12},"__N_SSP":true}