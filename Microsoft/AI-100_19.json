{"pageProps":{"questions":[{"id":"3EPhXYNE5XLtIa8y3GCc","answer":"C","answers_community":[],"unix_timestamp":1597055340,"question_id":91,"topic":"2","question_text":"You build an internal application that uses the Computer Vision API.\nYou need to ensure that only specific employees can access the application.\nWhat should you include in the solution?","choices":{"D":"a multi-service subscription key","B":"user principals in Azure Active Directory (Azure AD)","C":"service principals in Azure Active Directory (Azure AD)","A":"a single-service subscription key"},"question_images":[],"answer_images":[],"answer_ET":"C","timestamp":"2020-08-10 12:29:00","url":"https://www.examtopics.com/discussions/microsoft/view/27906-exam-ai-100-topic-2-question-49-discussion/","discussion":[{"comments":[{"upvote_count":"1","timestamp":"1597853820.0","comment_id":"161653","content":"I think the correct answer is B as well.","poster":"DexRT"}],"poster":"examfailure","content":"Why wouldn't the answer be B? The question is asking about granting user access to the application. The Service Principal is used to deploy the application. I'm not granting access to the SP.","upvote_count":"12","comment_id":"154420","timestamp":"1597055340.0"},{"comment_id":"928242","timestamp":"1687246500.0","content":"To ensure that only specific employees can access the internal application that uses the Computer Vision API, you should include the following in the solution:\n\nB. User principals in Azure Active Directory (Azure AD)\n\nExplanation:\n\nUser principals in Azure Active Directory (Azure AD) provide a way to authenticate and authorize specific employees to access resources and applications. By configuring the internal application to use Azure AD authentication, you can enforce access control based on user identities.","upvote_count":"1","poster":"rveney"},{"upvote_count":"4","content":"Service principles is CORRECT - \n\"An application that needs to deploy or configure resources through Azure Resource Manager must be represented by its own identity. Just as a user is represented by a security principal called a user principal, an app is represented by a service principal.\n\nThe service principal identity allows you to delegate only the necessary permissions to the app. For example, a configuration management app might use Azure Resource Manager to inventory Azure resources. In this scenario, you would register the app in your directory, grant the \"reader\" role to the app's service principal, and limit the configuration management app to read-only access.\"\n\nhttps://docs.microsoft.com/en-us/azure-stack/operator/azure-stack-create-service-principals?view=azs-2102&tabs=az1%2Caz2&pivots=state-connected","comment_id":"367041","poster":"allanm","timestamp":"1622025120.0"},{"content":"Well - I am not sure now. I selected the service principle and I think it should have been B","poster":"PinkUnicorns","comment_id":"358263","upvote_count":"1","timestamp":"1621126800.0"},{"content":"The service principal object defines what the app can actually do in the specific tenant, who can access the app, and what resources the app can access.","upvote_count":"2","timestamp":"1613561760.0","poster":"saloni_kalra","comment_id":"292482"},{"timestamp":"1603196160.0","comment_id":"203180","upvote_count":"3","poster":"sayak17","content":"The service principal object defines what the app can actually do in the specific tenant, \"who can access the app\", and what resources the app can access.\n\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals#service-principal-object"},{"upvote_count":"3","content":"I think service principal is the correct answer, since it is an internal application, it needs to have service principal created to register to Azure, then assign RBAC to the registered service, the answer link also shows that.","poster":"CeliaZhou","timestamp":"1601124000.0","comment_id":"187659"}],"answer_description":"The app requires an Azure service principal account to deploy services to your Azure subscription. A service principal lets you delegate specific permissions to an app using role-based access control.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/logo-detector-mobile","isMC":true,"exam_id":39},{"id":"YDJhlA7IZl2F6kjKNZSt","question_id":92,"answer_ET":"B","discussion":[{"comment_id":"930261","content":"B. No\n\nModifying the custom time interval for the training phase of App1 will not resolve the issue of the PersonGroup object for Ben Smith reaching its capacity and being unable to accept additional entries. Changing the training interval will only affect the training process of the Face API, which is unrelated to the capacity of a PersonGroup object. To enable additional entries for Ben Smith, you would need to either increase the capacity of the PersonGroup object or create a new PersonGroup object for Ben Smith with a higher entry limit.","upvote_count":"1","timestamp":"1687419900.0","poster":"rveney"},{"content":"all solutions are wrong except for migration","comment_id":"429964","timestamp":"1629723060.0","upvote_count":"1","poster":"dijaa"},{"upvote_count":"3","poster":"berserkguts","timestamp":"1622444160.0","content":"this was in the AI-100 exam i took today, May 31","comment_id":"370734"},{"upvote_count":"2","timestamp":"1613265180.0","poster":"Cornholioz","comments":[{"timestamp":"1613265540.0","comment_id":"289893","content":"Rethinking because the Customize Time Interval applies only if entries are migrated to LargePersonGroup.","poster":"Cornholioz","upvote_count":"2"}],"comment_id":"289892","content":"Looks like this can be a possible solution too. Besides LargePersonGroup which is also correct.\n\nI'm choosing Yes for Custom Interval and for LargePersonGroup.\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/how-to-use-large-scale#step-31-customize-time-interval"}],"choices":{"A":"Yes","B":"No"},"timestamp":"2021-02-14 02:13:00","isMC":true,"answers_community":[],"topic":"2","unix_timestamp":1613265180,"answer_description":"Instead, use a LargePersonGroup. LargePersonGroup and LargeFaceList are collectively referred to as large-scale operations. LargePersonGroup can contain up to 1 million persons, each with a maximum of 248 faces. LargeFaceList can contain up to 1 million faces. The large-scale operations are similar to the conventional PersonGroup and FaceList but have some differences because of the new architecture.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/how-to-use-large-scale","exam_id":39,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/44643-exam-ai-100-topic-2-question-5-discussion/","answer":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an app named App1 that uses the Face API.\nApp1 contains several PersonGroup objects.\nYou discover that a PersonGroup object for an individual named Ben Smith cannot accept additional entries. The PersonGroup object for Ben Smith contains\n10,000 entries.\nYou need to ensure that additional entries can be added to the PersonGroup object for Ben Smith. The solution must ensure that Ben Smith can be identified by all the entries.\nSolution: You modify the custom time interval for the training phase of App1.\nDoes this meet the goal?","answer_images":[]},{"id":"Ytm0dLoKvuStMzxrMIc8","choices":{"A":"Microsoft SQL Server on a Microsoft Azure virtual machine","C":"Microsoft Azure Data Lake Store","B":"Microsoft Azure Database for MySQL","D":"Microsoft Azure Cosmos DB"},"answer_description":"Row-level security is supported by SQL Server, Azure SQL Database, and Azure SQL Data Warehouse.\nReferences:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-2017","question_text":"You plan to deploy two AI applications named AI1 and AI2. The data for the applications will be stored in a relational database.\nYou need to ensure that the users of AI1 and AI2 can see only data in each user's respective geographic region. The solution must be enforced at the database level by using row-level security.\nWhich database solution should you use to store the application data?","timestamp":"2020-09-15 10:26:00","url":"https://www.examtopics.com/discussions/microsoft/view/31321-exam-ai-100-topic-2-question-50-discussion/","isMC":true,"discussion":[{"content":"I think the answer being looked for is SQL on a VM because of the line solution must be enforced at the database level... which kinda warrants the view-workaround for MySQL too but hey.\n\nAnother poorly constructed question.\n\nAlso, how is this related to AI??? Like many others.","timestamp":"1612923660.0","poster":"Cornholioz","upvote_count":"7","comment_id":"287283"},{"comment_id":"201921","upvote_count":"5","poster":"vilas94","timestamp":"1603013400.0","content":"Azure database for MySQL still doesn't support row-level security"},{"content":"To ensure that the users of AI1 and AI2 can see only data in their respective geographic regions and enforce row-level security at the database level, the most suitable database solution to store the application data would be:\n\nA. Microsoft SQL Server on a Microsoft Azure virtual machine\n\nExplanation:\n\nMicrosoft SQL Server on a Microsoft Azure virtual machine provides a fully managed relational database solution. It offers robust security features, including row-level security (RLS), which allows you to restrict data access at the row level based on specific conditions.\n\nWith SQL Server on Azure virtual machines, you can implement RLS to enforce data access restrictions based on the user's geographic region. By defining appropriate security predicates, you can ensure that users of AI1 and AI2 can only see the data that belongs to their respective regions.","comment_id":"928243","poster":"rveney","upvote_count":"1","timestamp":"1687246620.0"},{"comments":[{"upvote_count":"2","content":"It says in this article: \"Mysql doesn't natively support row level security on tables. However, you can sort of implement it with views. So, just create a view on your table that exposes only the rows you want a given client to see. Then, only provide that client access to those views, and not the underlying tables.\"https://stackoverflow.com/questions/5527129/mysql-how-to-do-row-level-security-like-oracles-virtual-private-database","comment_id":"198022","poster":"CeliaZhou","timestamp":"1602459300.0"}],"upvote_count":"1","content":"Tricky question since both sql on a vm and mysql support row-level security, but since the criteria is the region, azure mysql offers better integration with azure metadata that enable to distinguish the region, so I should go for MySql","poster":"jadepe","comment_id":"179752","timestamp":"1600158360.0"}],"answer":"A","topic":"2","unix_timestamp":1600158360,"exam_id":39,"answers_community":[],"question_images":[],"answer_ET":"A","answer_images":[],"question_id":93},{"id":"r3kpcM6NvowhxN6TAuLM","question_images":[],"answers_community":[],"answer_description":"Generally, Data Lake will be a bit more expensive although they are in close range of each other. Blob storage has more options for pricing depending upon things like how frequently you need to access your data (cold vs hot storage). Data Lake is priced on volume, so it will go up as you reach certain tiers of volume.\nReferences:\nhttp://blog.pragmaticworks.com/azure-data-lake-vs-azure-blob-storage-in-data-warehousing","unix_timestamp":1600164780,"question_id":94,"topic":"2","timestamp":"2020-09-15 12:13:00","isMC":true,"discussion":[{"timestamp":"1600164780.0","poster":"jadepe","comments":[{"poster":"Rajuuu","content":"Question mentions ‘Minimise Cost’ which can be addressed by Blob storage .","timestamp":"1607407380.0","comment_id":"237942","upvote_count":"3"}],"upvote_count":"9","comment_id":"179804","content":"Problem with blob is that blob container size is limited to 500 Tb whereas Data Lake is not, so given the ratio at which new data is appended, I think Data Lake is the right choice"},{"upvote_count":"1","content":"To store more than 2 TB of new data daily and minimize costs, the most suitable data storage service would be:\n\nD. Azure Data Lake Storage\n\nExplanation:\n\nAzure Data Lake Storage is designed to handle large volumes of data, making it a suitable choice for storing over 2 TB of new data daily. It provides scalable storage capacity and is optimized for big data workloads. Additionally, it supports storing and processing data in various formats, including JSON.","comment_id":"928245","poster":"rveney","timestamp":"1687246740.0"},{"timestamp":"1612924200.0","comments":[{"content":"Going back on my statement a little bit because although ADL is based on Blobs, there would be additional costs for ADL because it is more feature-rich than Blobs.\nActually, it is just wrong to give both Blob and Data Lake as options with just this much info in the question.","comments":[{"timestamp":"1622025840.0","upvote_count":"1","content":"That is why it's a trick question. Both ADL and blob are in there to confuse you. The key is in the requirement \"Minimise cost\". Since ADL builds off the blob concept, it will be cheaper than the data lake and so blob storage is the answer.","comment_id":"367044","poster":"allanm"}],"comment_id":"292988","poster":"Cornholioz","timestamp":"1613610300.0","upvote_count":"2"}],"upvote_count":"2","comment_id":"287285","content":"Costs are the same for blob and data lake storage. I'll go with the JSON storage being better with Lake.","poster":"Cornholioz"},{"timestamp":"1609287180.0","poster":"aceking","content":"wrong answer, this should be data lake","upvote_count":"1","comments":[{"comment_id":"375833","timestamp":"1622969940.0","content":"@aceking... kindly also justify your answer","poster":"anwar1","upvote_count":"1"},{"poster":"kjbalamurugan","timestamp":"1610156700.0","content":"For storing json data, azure data lake is the best choice. But, low-cost is given, so, it should be azure blob storage.","comment_id":"262938","upvote_count":"1"}],"comment_id":"255245"},{"comment_id":"181075","poster":"jdadon","content":"Azure data lake benefit from the same storage tier as a blob (the article you are referring to is 2 years old)","upvote_count":"1","timestamp":"1600358340.0"}],"answer_images":[],"choices":{"D":"Azure Data Lake Storage","A":"Azure Manage Disks","B":"Azure Blob Storage","C":"Azure File Storage"},"exam_id":39,"answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/31333-exam-ai-100-topic-2-question-51-discussion/","answer_ET":"B","question_text":"You are designing an AI workflow that will aggregate data stored in Azure as JSON documents.\nYou expect to store more than 2 TB of new data daily.\nYou need to choose the data storage service for the data. The solution must minimize costs.\nWhich data storage service should you choose?"},{"id":"Xraz94TNjwBn2bzK02zm","choices":{"A":"Yes","B":"No"},"timestamp":"2020-09-02 03:20:00","answer_images":[],"answer":"B","topic":"2","answers_community":[],"answer_ET":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an application that uses an Azure Kubernetes Service (AKS) cluster.\nYou are troubleshooting a node issue.\nYou need to connect to an AKS node by using SSH.\nSolution: You run the kubect1 command, and then you create an SSH connection.\nDoes this meet the goal?","isMC":true,"exam_id":39,"url":"https://www.examtopics.com/discussions/microsoft/view/30302-exam-ai-100-topic-2-question-52-discussion/","unix_timestamp":1599009600,"answer_description":"","question_id":95,"question_images":[],"discussion":[{"comment_id":"180069","timestamp":"1600212240.0","content":"Throughout the lifecycle of your Azure Kubernetes Service (AKS) cluster, you may need to access an AKS node. This access could be for maintenance, log collection, or other troubleshooting operations. You can access AKS nodes using SSH, including Windows Server nodes. You can also connect to Windows Server nodes using remote desktop protocol (RDP) connections. For security purposes, the AKS nodes aren't exposed to the internet. To SSH to the AKS nodes, you use the private IP address.\nDetails:https://docs.microsoft.com/en-us/azure/aks/ssh","upvote_count":"5","comments":[{"poster":"lollo1234","timestamp":"1623584580.0","comment_id":"381081","upvote_count":"3","content":"The last sentence on your link says \"For security purposes, the AKS nodes aren't exposed to the internet. To SSH to the AKS nodes, you use kubectl debug or the private IP address.\" so maybe the answer to this question should be yes."}],"poster":"Ana007"},{"comment_id":"928247","content":"B. No\n\nExplanation:\nThe solution does not meet the goal. Running the \"kubect1\" command (which seems to have a typo and should be \"kubectl\") does not directly establish an SSH connection to an AKS node. The \"kubectl\" command is used to interact with the Kubernetes cluster and manage its resources, but it does not provide a built-in way to establish an SSH connection to the nodes.","poster":"rveney","upvote_count":"1","timestamp":"1687246980.0"}]}],"exam":{"name":"AI-100","provider":"Microsoft","isImplemented":true,"id":39,"numberOfQuestions":206,"lastUpdated":"12 Apr 2025","isBeta":false,"isMCOnly":false},"currentPage":19},"__N_SSP":true}