{"pageProps":{"questions":[{"id":"tIoLO3sQKwLHGQMk99tj","answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0043600001.png","https://www.examtopics.com/assets/media/exam-media/04274/0043700001.png"],"timestamp":"2021-07-11 14:24:00","topic":"5","isMC":false,"answers_community":[],"question_id":456,"unix_timestamp":1626006240,"exam_id":64,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04274/0043800001.jpg"],"discussion":[{"comment_id":"403962","timestamp":"1626006240.0","content":"On exam 2021/7/10","poster":"ljljljlj","comments":[{"poster":"azayra","content":"You passed?","upvote_count":"6","timestamp":"1626424140.0","comment_id":"407717"}],"upvote_count":"8"},{"timestamp":"1730710320.0","comment_id":"1306843","poster":"jefimija","content":"Linear Regression is not incorrect, maybe two options are correct.","upvote_count":"1"},{"timestamp":"1728752460.0","content":"Would linear regression not be the right answer for first drop-down?","comment_id":"1296592","poster":"AzureGeek79","upvote_count":"1"},{"timestamp":"1685884140.0","poster":"snegnik","content":"the table is not needed for the second question.","upvote_count":"1","comment_id":"914633"},{"upvote_count":"4","comment_id":"615747","timestamp":"1655116800.0","comments":[{"content":"https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/linear-regression?view=azureml-api-2#create-a-regression-model-using-online-gradient-descent","timestamp":"1682571900.0","poster":"ZoeJ","upvote_count":"1","comment_id":"882242"}],"content":"Yes, the given answer is correct:\n\"Parameter Range: If you want the algorithm to find the best parameters for you, set Create trainer mode option to Parameter Range. You can then specify multiple values for the algorithm to try.\"","poster":"ning"},{"poster":"Tsardoz","upvote_count":"2","comment_id":"524733","content":"Not sure about parameter range. The only way this would work is if you had prior knwledge of the coefficients eg. by looking at the Bayesian results. Otherwise increasing number of epochs would be the most sensible approach.","timestamp":"1642321140.0"}],"answer_ET":"","question_text":"HOTSPOT -\nYou are developing a linear regression model in Azure Machine Learning Studio. You run an experiment to compare different algorithms.\nThe following image displays the results dataset output:\n//IMG//\n\nUse the drop-down menus to select the answer choice that answers each question based on the information presented in the image.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"Box 1: Boosted Decision Tree Regression\nMean absolute error (MAE) measures how close the predictions are to the actual outcomes; thus, a lower score is better.\nBox 2:\nOnline Gradient Descent: If you want the algorithm to find the best parameters for you, set Create trainer mode option to Parameter Range. You can then specify multiple values for the algorithm to try.\nReference:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/evaluate-model https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/linear-regression","url":"https://www.examtopics.com/discussions/microsoft/view/57626-exam-dp-100-topic-5-question-15-discussion/"},{"id":"1DAPWJojkN3ySVo9u5vN","answer_ET":"","question_text":"HOTSPOT -\nYou are using a decision tree algorithm. You have trained a model that generalizes well at a tree depth equal to 10.\nYou need to select the bias and variance properties of the model with varying tree depth values.\nWhich properties should you select for each tree depth? To answer, select the appropriate options in the answer area.\nHot Area:\n//IMG//","topic":"5","answers_community":[],"answer_description":"In decision trees, the depth of the tree determines the variance. A complicated decision tree (e.g. deep) has low bias and high variance.\nNote: In statistics and machine learning, the biasג€\"variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. Increasing the bias will decrease the variance. Increasing the variance will decrease the bias.\nReference:\nhttps://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/","answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/30979-exam-dp-100-topic-5-question-16-discussion/","timestamp":"2020-09-10 05:26:00","question_id":457,"question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0043900001.png"],"discussion":[{"upvote_count":"7","content":"Low depth means under fitting and higher depth means over fitting. So the selections are correct","timestamp":"1646038920.0","poster":"dushmantha","comment_id":"435196"},{"content":"\"bias: underfitting\nvariance: overfitting\n\nReference: https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff","timestamp":"1722934560.0","poster":"Matt2000","comment_id":"1142048","upvote_count":"1"},{"comment_id":"820065","content":"In decision trees, the depth of the tree determines the variance. A complicated decision tree (e.g. deep) has low bias and high variance. Increasing the bias will decrease the variance. Increasing the variance will decrease the bias.","timestamp":"1692839640.0","upvote_count":"2","poster":"phdykd"},{"upvote_count":"3","timestamp":"1670935560.0","poster":"ning","content":"Correct!\n\nThe bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n\nThe variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).","comment_id":"615750"}],"unix_timestamp":1599708360,"isMC":false,"exam_id":64,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04274/0044000001.png"]},{"id":"Y8qlkfBDMkDGO5TChgEA","topic":"5","url":"https://www.examtopics.com/discussions/microsoft/view/20351-exam-dp-100-topic-5-question-17-discussion/","unix_timestamp":1589253600,"question_id":458,"answer_description":"Box 1: Split data -\n\nBox 2: Partition and Sample -\nBox 3: Two-Class Boosted Decision Tree\nBox 4: Tune Model Hyperparameters\nIntegrated train and tune: You configure a set of parameters to use, and then let the module iterate over multiple combinations, measuring accuracy until it finds a\n\"best\" model. With most learner modules, you can choose which parameters should be changed during the training process, and which should remain fixed.\nWe recommend that you use Cross-Validate Model to establish the goodness of the model given the specified parameters. Use Tune Model Hyperparameters to identify the optimal parameters.\nReference:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/partition-and-sample","answer_ET":"","timestamp":"2020-05-12 05:20:00","answer":"","exam_id":64,"answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0044100001.png"],"question_text":"DRAG DROP -\nYou have a model with a large difference between the training and validation error values.\nYou must create a new model and perform cross-validation.\nYou need to identify a parameter set for the new model using Azure Machine Learning Studio.\nWhich module you should use for each step? To answer, drag the appropriate modules to the correct steps. Each module may be used once or more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/04274/0044100002.png"],"discussion":[{"upvote_count":"98","comment_id":"133978","timestamp":"1594647900.0","poster":"priyalnish","comments":[{"poster":"Gitty","timestamp":"1597277760.0","content":"correct","comment_id":"156813","upvote_count":"2"},{"content":"Why is 3 and 4 have the same answer?","comment_id":"400239","timestamp":"1625595540.0","comments":[{"comment_id":"413597","content":"Train, evaluate, and compare\nThe same Tune Model Hyperparameters module trains all the models that correspond to the parameter set,","poster":"YipingRuan","timestamp":"1627186200.0","upvote_count":"1"}],"upvote_count":"2","poster":"jay2323"},{"comment_id":"390996","poster":"SnowCheetah","upvote_count":"1","timestamp":"1624690920.0","content":"This is a correct Answer"},{"upvote_count":"2","timestamp":"1627657620.0","content":"Thanks for the link. These answers are accurate based on the documentation.","comment_id":"417496","poster":"VJPrakash"}],"content":"According to below link;\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-parameters-optimize\n1. Two-Class Boosted Decision Tree\n2. Partition and Sample\n3. Tune Model Hyperparameters\n4. Tune Model Hyperparameters"},{"upvote_count":"9","comment_id":"87420","poster":"Yilu","timestamp":"1589253600.0","content":"box 1 and 4 got swapped"},{"comment_id":"1310106","timestamp":"1731332640.0","comments":[{"timestamp":"1731333120.0","upvote_count":"1","content":"Ignore this is wrong. Given answer is correct -> Split, Part, Boost, Tune","comment_id":"1310118","poster":"jl420"}],"upvote_count":"1","content":"Step Module\nDefine the parameter scope - Tune Model Hyperparameters\nDefine the cross-validation settings - Partition and Sample\nDefine the metric - Tune Model Hyperparameters\nTrain, evaluate, and compare - Two-Class Boosted Decision Tree","poster":"jl420"},{"comment_id":"983655","timestamp":"1692277860.0","upvote_count":"2","poster":"BR_CS","content":"The answers in the comments seem to make no sense, just like the answers shown. Was the image changed?"},{"content":"I think this is an out-dated question","poster":"ZoeJ","comment_id":"882247","timestamp":"1682572500.0","upvote_count":"2"},{"content":"how come the answers below say selecting the model first? shouldn't we split the data first and feed in the training data to the model?","timestamp":"1611175800.0","upvote_count":"2","poster":"ck1729","comment_id":"272255"},{"content":"https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-parameters-optimize\nbox 1: Boosted Decision Tree\nbox 2: Partition and Sample\nbox 3: Tune Model Hyperparameters \nbox 4:","comment_id":"122857","poster":"kath3624","upvote_count":"4","comments":[{"upvote_count":"1","comment_id":"316813","timestamp":"1616385660.0","poster":"dev2dev","content":"4th also hyperparmeters too"}],"timestamp":"1593442080.0"},{"timestamp":"1593107280.0","comment_id":"119672","poster":"pepmir","upvote_count":"3","content":"Tune Hyperparams belongs to Train Module. So 4 is correct."},{"comment_id":"92605","timestamp":"1589959920.0","poster":"davo123","content":"Box 1 should be Two Class Boosted?","comments":[{"poster":"abofficial","upvote_count":"6","content":"I think box 1 should be tune hyperparameters.. take note of the keyword 'parameter scope'","comment_id":"221617","timestamp":"1605680400.0"}],"upvote_count":"2"}],"isMC":false},{"id":"NdPsVRX3e9p3ouFeDBFV","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0044300001.png","https://www.examtopics.com/assets/media/exam-media/04274/0044300002.png"],"discussion":[{"upvote_count":"99","comment_id":"90824","timestamp":"1621280400.0","content":"Graph 1 is negative skew and Graph 2 is positive skew.\n\nA left-skewed distribution has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA right-skewed distribution has a long right tail. Right-skewed distributions are also called positive-skew distributions. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\nhttps://www.statisticshowto.com/probability-and-statistics/skewed-distribution/","poster":"VickyM"},{"comment_id":"92104","content":"Graph 1 is negative skew.","upvote_count":"12","timestamp":"1621429500.0","poster":"Zhuo"},{"poster":"InversaRadice","comment_id":"1092619","content":"given answer is correct","upvote_count":"1","timestamp":"1733846340.0"},{"poster":"james2033","content":"https://en.wikipedia.org/wiki/Skewness","upvote_count":"1","comment_id":"1026980","timestamp":"1728252900.0"},{"poster":"Peeking","upvote_count":"2","comment_id":"836760","timestamp":"1710226260.0","content":"Graph 1 = Negatively skewed or Left-tailed\nGraph 2 = Positively skewed or right-tailed.\nhttps://corporatefinanceinstitute.com/resources/data-science/negatively-skewed-distribution/"},{"poster":"phdykd","content":"G1: Negative, G2: Positive","upvote_count":"1","comment_id":"820077","timestamp":"1708745820.0"},{"content":"The microsoft reference (https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/compute-elementary-statistics) is technically correct.\n\"Negative skew values means the distribution is skewed to the left. 0 denotes the normal distribution. Positive skewness values mean the distribution is skewed to the right.\"\n\nThis is validated by Wiki (https://en.wikipedia.org/wiki/Skewness#positive_skew):\nnegative skew: The left tail is longer; the mass of the distribution is concentrated on the right of the figure. The distribution is said to be left-skewed, left-tailed, or skewed to the left,...\npositive skew: The right tail is longer; the mass of the distribution is concentrated on the left of the figure. The distribution is said to be right-skewed, right-tailed, or skewed to the right,...\n\nErgo, the answers are interchanged. G1: should be negative skew and G2: positive. See the figure from the wiki link or the link provided by VickyM.\n\nAt times like this, I am starting to doubt the competence of the answers provided.","timestamp":"1667641440.0","poster":"NormanDS","upvote_count":"2","comment_id":"472990"},{"content":"The answer is quite opposite.\nThe first one is negative and the second one is positive\n\n\nhttps://en.wikipedia.org/wiki/Skewness","poster":"saurabhk1","upvote_count":"5","timestamp":"1646470500.0","comment_id":"304080"},{"timestamp":"1643388720.0","content":"Wrong answer","poster":"Shankar_102","upvote_count":"3","comment_id":"278700"},{"upvote_count":"1","poster":"deepakconsult","timestamp":"1630126200.0","content":"Reverse: https://en.wikipedia.org/wiki/","comment_id":"168937"},{"content":"Graph 1 = negative\nGraph 2 = positive","upvote_count":"7","poster":"Nugi","comment_id":"157971","timestamp":"1628932560.0"},{"timestamp":"1628484780.0","content":"Graph 1 is negative skewed…Tail is negative \ngraph 2 is positive skewed.","poster":"Rajuuu","comment_id":"153369","upvote_count":"5"},{"content":"G1: Negative\nG2: Positive. As line is extending farther away from 0 on +ve line","poster":"pepmir","comment_id":"117847","upvote_count":"5","timestamp":"1624483080.0"},{"content":"Graph 1 is indeed negative skew https://en.wikipedia.org/wiki/Skewness","poster":"jsnels86","comment_id":"96118","upvote_count":"5","timestamp":"1622036820.0"},{"content":"The mentioned answer is wrong.\nThe correct answer is\ngraph-1--> negatively skewed\nGraph-2-> positively skewed\nreference: https://www.statisticshowto.com/probability-and-statistics/skewed-distribution/","timestamp":"1621761600.0","upvote_count":"6","poster":"ajithvajrala","comment_id":"94251"},{"timestamp":"1621402860.0","content":"I agree, I think Graph 1 is negatively skewed and Graph 2 is positively skewed.","poster":"James_James","upvote_count":"7","comment_id":"91833"},{"poster":"WTT","comment_id":"87624","upvote_count":"7","timestamp":"1620824580.0","content":"isn't Graph 1 positive skew?"}],"isMC":false,"exam_id":64,"url":"https://www.examtopics.com/discussions/microsoft/view/20376-exam-dp-100-topic-5-question-18-discussion/","question_text":"HOTSPOT -\nYou are analyzing the asymmetry in a statistical distribution.\nThe following image contains two density curves that show the probability distribution of two datasets.\n//IMG//\n\nUse the drop-down menus to select the answer choice that answers each question based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_id":459,"answer_ET":"","topic":"5","answers_community":[],"unix_timestamp":1589288580,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04274/0044400001.png"],"answer_description":"Box 1: Positive skew -\nPositive skew values means the distribution is skewed to the right.\n\nBox 2: Negative skew -\nNegative skewness values mean the distribution is skewed to the left.\nReference:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/compute-elementary-statistics","answer":"","timestamp":"2020-05-12 15:03:00"},{"id":"3viIAI8atnRxjBHGmA9O","topic":"5","question_id":460,"answer_ET":"BC","isMC":true,"choices":{"E":"Add an additional dense layer with 64 input units.","A":"Add an additional dense layer with 512 input units.","C":"Use training data augmentation.","B":"Add L1/L2 regularization.","D":"Reduce the amount of training data."},"question_images":[],"discussion":[{"timestamp":"1590501000.0","upvote_count":"25","poster":"jsnels86","comment_id":"96126","content":"I agree, B and C should be the correct answers"},{"comment_id":"87428","upvote_count":"16","content":"adding more training records should decrease the overfitting.","comments":[{"upvote_count":"47","content":"Answer should be B and C","comments":[{"upvote_count":"1","comment_id":"320260","timestamp":"1616682360.0","poster":"kty","content":"I agree"}],"timestamp":"1589597340.0","poster":"Yilu","comment_id":"89727"}],"timestamp":"1589255220.0","poster":"Yilu"},{"upvote_count":"1","content":"Selected Answer: BC\nExplanation:\nB. L1/L2 regularization helps prevent overfitting by adding a penalty term to the loss function, discouraging the model from relying too heavily on any particular feature.\nC. Data augmentation increases the diversity of your training set by applying random (but realistic) transformations to the existing images, which helps the model generalize better and reduce overfitting.\nThese two techniques are commonly used to address overfitting in deep learning models, especially CNNs for image classification.","comment_id":"1235769","timestamp":"1719136020.0","poster":"evangelist"},{"content":"B. Add L1/L2 regularization.\n\nC. Use training data augmentation.\n\nThese methods directly address the problem of overfitting by either penalizing overly complex models or by making the training data more diverse and challenging for the model.","upvote_count":"1","timestamp":"1716006780.0","comment_id":"1213136","poster":"evangelist"},{"timestamp":"1707217740.0","content":"This reference might be useful: https://towardsdatascience.com/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d","comment_id":"1142064","poster":"Matt2000","upvote_count":"1"},{"content":"The two actions that can help reduce overfitting and converge the model to an optimal fit are:\n\nB. Add L1/L2 regularization: Regularization techniques can help to reduce overfitting in a neural network. L1/L2 regularization adds a penalty term to the loss function, which encourages the model to learn simpler and smoother weight values. This, in turn, helps to prevent overfitting.\n\nC. Use training data augmentation: Data augmentation is a technique that can be used to artificially increase the size of the training dataset by creating new examples from existing data. This can help the model to generalize better and reduce overfitting. Common data augmentation techniques for image data include random rotations, flips, and translations.\n\nOptions A and E suggest adding additional dense layers, which can increase the complexity of the model and potentially exacerbate overfitting. Option D suggests reducing the amount of training data, which can lead to underfitting and poor generalization performance. Therefore, options B and C are the best choices for reducing overfitting and improving model performance.","upvote_count":"2","poster":"phdykd","comment_id":"820080","timestamp":"1677209940.0"},{"upvote_count":"2","poster":"ning","comment_id":"615765","timestamp":"1655118540.0","content":"Selected Answer: BC\nRegulation\nIncrease data through data argumentation"},{"timestamp":"1638984360.0","comment_id":"496997","poster":"dija123","content":"Selected Answer: BC\nI agree with B and C","upvote_count":"5"},{"poster":"saurabhk1","comment_id":"304083","upvote_count":"7","content":"Answer should be B and C","timestamp":"1614934740.0"},{"content":"Regularisation and data augmentation are correct. Dropouts and early termination are also correct but not in the options.","upvote_count":"4","poster":"Neuron","comment_id":"282206","timestamp":"1612300860.0"},{"poster":"aziti","comment_id":"255004","upvote_count":"4","timestamp":"1609269060.0","content":"During dropout, we are not actually reducing the training data but rather dropping the neurons to help them memorize less and not overfit.\nhttps://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html\nso the answer is B and C"},{"timestamp":"1608407520.0","content":"Answer should be B,C","poster":"Sud3962","comment_id":"248176","upvote_count":"2"},{"comment_id":"217680","poster":"Pucha","timestamp":"1605156060.0","content":"Yes BC should be correct, image data augmentation - generating more training data artificially to expand the learning of algo","upvote_count":"3"},{"timestamp":"1600616340.0","content":"and to support the argument for C:\n\"Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\nTraining deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\"\nRef: https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/","upvote_count":"5","poster":"BICube","comment_id":"183109"},{"timestamp":"1600575180.0","content":"Yes, BC are correct.","upvote_count":"2","comment_id":"182644","poster":"hima618"},{"timestamp":"1596112560.0","content":"BC are right answers. To reduce overfitting in DL model, you either increase training data volume or reduce complexity of the model","comment_id":"147417","upvote_count":"4","poster":"rr200"},{"content":"The answer is definitely B and C","timestamp":"1594802040.0","poster":"Timeless_Faceless","upvote_count":"4","comment_id":"135561"}],"exam_id":64,"question_text":"You are a data scientist building a deep convolutional neural network (CNN) for image classification.\nThe CNN model you build shows signs of overfitting.\nYou need to reduce overfitting and converge the model to an optimal fit.\nWhich two actions should you perform? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/20352-exam-dp-100-topic-5-question-19-discussion/","answer_images":[],"unix_timestamp":1589255220,"answers_community":["BC (100%)"],"timestamp":"2020-05-12 05:47:00","answer":"BC"}],"exam":{"lastUpdated":"12 Apr 2025","isBeta":false,"name":"DP-100","numberOfQuestions":512,"isImplemented":true,"provider":"Microsoft","id":64,"isMCOnly":false},"currentPage":92},"__N_SSP":true}