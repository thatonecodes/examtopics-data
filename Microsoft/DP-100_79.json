{"pageProps":{"questions":[{"id":"0M8JOqEibLEIeZinuUzq","answer_images":[],"timestamp":"2022-09-09 07:49:00","question_id":391,"choices":{"D":"Azure Container Instance (ACI) compute target","B":"Azure Machine Learning compute cluster","A":"Azure Machine Learning compute instance","C":"Azure Kubernetes Service (AKS)-based inference cluster"},"answer_ET":"B","unix_timestamp":1662702540,"discussion":[{"upvote_count":"9","comment_id":"774020","content":"I think the confusion here is real-time vs batch. A real-time inference pipeline should use AKS. This is a BATCH inference pipeline. The answer is B.","timestamp":"1673576280.0","comments":[{"poster":"manualrg","content":"I agree the answer is B. Indeed, in the example notebook: Create a Batch Inferencing Service\nhttps://github.com/MicrosoftLearning/mslearn-dp100/blob/main/10%20-%20Create%20a%20Batch%20Inferencing%20Service.ipynb\nExisting compute cluster resource is checked and set as pipeline compute target:\ntry:\n # Check for existing compute target\n inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n...\nparallel_run_config = ParallelRunConfig(\n source_directory=experiment_folder,\n entry_script=\"batch_diabetes.py\",\n mini_batch_size=\"5\",\n error_threshold=10,\n output_action=\"append_row\",\n environment=batch_env,\n compute_target=inference_cluster,\n node_count=2)","timestamp":"1674993480.0","upvote_count":"2","comment_id":"791603"}],"poster":"BTAB"},{"poster":"avinyc","timestamp":"1736378400.0","upvote_count":"1","comment_id":"1338117","content":"Selected Answer: B\nAzure Machine Learning compute clusters are designed to handle batch workloads efficiently, which aligns with the requirement of processing multiple data files"},{"comment_id":"1313951","poster":"nposteraro","timestamp":"1731927240.0","content":"I think it's C: https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target?view=azureml-api-2#compute-targets-for-inference","upvote_count":"1"},{"timestamp":"1717915260.0","upvote_count":"1","comment_id":"1227094","poster":"evangelist","content":"Selected Answer: B\nC is for real time B for batch"},{"timestamp":"1717025040.0","poster":"sl_mslconsulting","comment_id":"1221322","content":"Selected Answer: C\nbased on the SDK V2 Doc: https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target?view=azureml-api-2#compute-targets-for-inference","upvote_count":"1"},{"poster":"PI_Team","timestamp":"1692887640.0","content":"Selected Answer: B\nAzure Kubernetes Service (AKS) is a managed Kubernetes service that simplifies the deployment and management of a Kubernetes cluster in Azure1. An AKS-based inference cluster can be used to deploy machine learning models for real-time inferencing, but it is not the best choice for running batch inference jobs. For batch inference, you would want to use a compute target that can distribute the processing of large amounts of data across multiple nodes in the cloud","comment_id":"989269","upvote_count":"1"},{"upvote_count":"3","content":"I think the answer B is indeed correct! There is no wording about a \"production-level\" or \"heavy-workload\" deployment (this would always hint to AKS), and according to the documentation: \n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target\n\na compute-cluster supports job-scheduling aka scheduling to run every night, and scales up and down depending on the configuration.","poster":"Crusader2k13","comment_id":"745929","timestamp":"1671099780.0"},{"content":"Yes, Answer should be AKS","poster":"silva_831","upvote_count":"1","timestamp":"1668577680.0","comment_id":"719349"},{"comment_id":"702582","timestamp":"1666574580.0","upvote_count":"3","poster":"JTWang","content":"Answer should be AKS.\n\nCompute targets for inference:\n1.Local web service\n2.Azure Machine Learning endpoints\n3.Azure Machine Learning Kubernetes\n4.Azure Container Instances (SDK/CLI v1 only)\n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target","comments":[{"content":"I agree with you","comment_id":"882144","upvote_count":"1","poster":"ZoeJ","timestamp":"1682558520.0"}]},{"timestamp":"1666089300.0","comment_id":"698120","content":"Selected Answer: C\nIt should be aks !","upvote_count":"3","poster":"amokrane_mancer"},{"timestamp":"1662702540.0","comment_id":"664259","upvote_count":"2","content":"why not AKS?","poster":"claps92"}],"url":"https://www.examtopics.com/discussions/microsoft/view/81322-exam-dp-100-topic-4-question-32-discussion/","topic":"4","question_images":[],"isMC":true,"exam_id":64,"answer_description":"","answer":"B","answers_community":["B (60%)","C (40%)"],"question_text":"You train and register a machine learning model. You create a batch inference pipeline that uses the model to generate predictions from multiple data files.\nYou must publish the batch inference pipeline as a service that can be scheduled to run every night.\nYou need to select an appropriate compute target for the inference service.\nWhich compute target should you use?"},{"id":"wzHUf1tkqEjkS3deXw8R","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0040700001.jpg"],"exam_id":64,"isMC":false,"question_text":"DRAG DROP -\nYou train and register a model by using the Azure Machine Learning SDK on a local workstation. Python 3.6 and Visual Studio Code are installed on the workstation.\nWhen you try to deploy the model into production as an Azure Kubernetes Service (AKS)-based web service, you experience an error in the scoring script that causes deployment to fail.\nYou need to debug the service on the local workstation before deploying the service to production.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","answer_ET":"","unix_timestamp":1662598860,"topic":"4","answer_description":"Step 1: Install Docker on the workstation\nPrerequisites include having a working Docker installation on your local system.\nBuild or download the dockerfile to the compute node.\nStep 2: Create an AksWebservice deployment configuration and deploy the model to it\nTo deploy a model to Azure Kubernetes Service, create a deployment configuration that describes the compute resources needed.\n# If deploying to a cluster configured for dev/test, ensure that it was created with enough\n# cores and memory to handle this deployment configuration. Note that memory is also used by\n# things such as dependencies and AML components.\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1) service = Model.deploy(ws, \"myservice\", [model], inference_config, deployment_config, aks_target) service.wait_for_deployment(show_output = True) print(service.state) print(service.get_logs())\nStep 3: Create a LocalWebservice deployment configuration for the service and deploy the model to it\nTo deploy locally, modify your code to use LocalWebservice.deploy_configuration() to create a deployment configuration. Then use Model.deploy() to deploy the service.\nStep 4: Debug and modify the scoring script as necessary. Use the reload() method of the service after each modification.\nDuring local testing, you may need to update the score.py file to add logging or attempt to resolve any problems that you've discovered. To reload changes to the score.py file, use reload(). For example, the following code reloads the script for the service, and then sends data to it.\nIncorrect Answers:\nâœ‘ AciWebservice: The types of web services that can be deployed are LocalWebservice, which will deploy a model locally, and AciWebservice and\nAksWebservice, which will deploy a model to Azure Container Instances (ACI) and Azure Kubernetes Service (AKS), respectively.\nReference:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service https://docs.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-deployment-local","question_id":392,"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/81061-exam-dp-100-topic-4-question-33-discussion/","discussion":[{"comments":[{"comment_id":"1141704","content":"I agree with this answer but i want to know if the following sequence is also correct:\n1. Create localwebservice\n2. debug and modify the scoring script\n3. install docker\n4. Create Akswebserice","poster":"deyoz","upvote_count":"3","timestamp":"1722903060.0"}],"comment_id":"702672","upvote_count":"33","timestamp":"1682307540.0","content":"My answer:\n1.Install Docker on the workstation\nCreate a LocalWebservice deployment configuration for the service and deploy the mode to it\n3.Debug and modify the scroing script as necessary. use the reload() method of the service after earch modification.\n4.Creae an AksWebservice deployment configuration for the service and deploy the model to it\n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-managed-online-endpoints?tabs=python","poster":"JTWang"},{"comment_id":"663009","poster":"chevyli","timestamp":"1678244460.0","comments":[{"poster":"Johlec","upvote_count":"2","content":"agree with you, you redeploy to aks after finished to debug.","timestamp":"1682081160.0","comment_id":"700871"}],"content":"Logically, \"Create AksWebService\" should be the last step.","upvote_count":"8"},{"timestamp":"1692826980.0","comment_id":"819898","content":"Install Docker on the workstation \nCreate a LocalWebservice deployment configuration for the service and deploy the model to it (C)\nDebug and modify the scoring script as necessary. Use the reload() method of the service after each modification \nCreate an AksWebservice deployment configuration and deploy the model to it","upvote_count":"2","poster":"phdykd"}],"timestamp":"2022-09-08 03:01:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/04274/0040800001.jpg"]},{"id":"WZu4hZhsKo73ZWKMHKOa","answer_images":["https://www.examtopics.com/assets/media/exam-media/04274/0041000001.png","https://www.examtopics.com/assets/media/exam-media/04274/0041100001.jpg","https://www.examtopics.com/assets/media/exam-media/04274/0041200001.jpg"],"unix_timestamp":1662553920,"answers_community":[],"answer":"","answer_ET":"","question_text":"DRAG DROP -\nYou create an Azure Machine Learning workspace and a new Azure DevOps organization. You register a model in the workspace and deploy the model to the target environment.\nAll new versions of the model registered in the workspace must automatically be deployed to the target environment.\nYou need to configure Azure Pipelines to deploy the model.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","answer_description":"Step 1: Create an Azure DevOps project\nStep 2: Create a release pipeline\n1. Sign in to your Azure DevOps organization and navigate to your project.\n2. Go to Pipelines, and then select New pipeline.\nStep 3: Install the Machine Learning extension for Azure Pipelines\nYou must install and configure the Azure CLI and ML extension.\nStep 4: Create a service connection\nHow to set up your service connection\n\nSelect AzureMLWorkspace for the scope level, then fill in the following subsequent parameters.\n\nNote: How to enable model triggering in a release pipeline\nâœ‘ Go to your release pipeline and add a new artifact. Click on AzureML Model artifact then select the appropriate AzureML service connection and select from the available models in your workspace.\nâœ‘ Enable the deployment trigger on your model artifact as shown here. Every time a new version of that model is registered, a release pipeline will be triggered.\nReference:\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml https://docs.microsoft.com/en-us/azure/devops/pipelines/targets/azure-machine-learning","topic":"4","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0040900002.png"],"discussion":[{"content":"From CI/CD pipeline perspective, the e2e process should be:\n1. Create a project.\n2. Create service connections.\n3. Create build pipeline.\n4. Create release pipeline.\nI don't think it's necessary to install ml extension as a separate step since it's in pipeline script code.\n\nhttps://www.azuredevopslabs.com/labs/vstsextend/aml/","upvote_count":"21","poster":"bbigwolf","comments":[{"timestamp":"1682559360.0","upvote_count":"3","poster":"ZoeJ","content":"https://docs.microsoft.com/en-us/azure/devops/pipelines/targets/azure-machine-learning?view=azure-devops\nI agree with you","comment_id":"882149"},{"content":"The extension is for Azure pipelines, part of Azure devops, and this reference says it is a prerequisite for the steps you describe here: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-devops-machine-learning?view=azureml-api-2&viewFallbackFrom=azure-devops&tabs=arm","upvote_count":"1","timestamp":"1707130020.0","poster":"Matt2000","comment_id":"1140984"}],"timestamp":"1664075280.0","comment_id":"678371"},{"upvote_count":"14","comment_id":"662467","comments":[{"upvote_count":"1","content":"Why would install ML extension be the last step? @bbigwolf's answer seems correct.","timestamp":"1673862180.0","comment_id":"777512","poster":"[Removed]"},{"upvote_count":"1","comment_id":"1152704","timestamp":"1708188840.0","content":"Agree but 3) should be replaced by \"create build pipeline\" that correlates with steps 4 and 5 from azure website article. The main task is to configure azure pipelines. so create release pipeline will be the discarded step.\nThe main task is to configure azure pipelines. so create release pipeline will be the discarded step.","poster":"vprowerty"}],"poster":"giusecozza","timestamp":"1662553920.0","content":"Looking at the doc below, the correct sequence should be:\n1) create devops project [STEP 2]\n2) create service connection [STEP 3]\n3) create release pipeline [STEP 4]\n4) install SDK extension [STEP 6]\n\nhttps://docs.microsoft.com/en-us/azure/devops/pipelines/targets/azure-machine-learning?view=azure-devops"},{"comment_id":"1297871","timestamp":"1728950880.0","upvote_count":"4","content":"1. Create an Azure DevOps project: This is the first step, where you set up a new Azure DevOps project to manage your build and release pipelines.\n 2. Install the Machine Learning extension for Azure Pipelines: This step ensures that the necessary tools are in place for working with Azure Machine Learning in your DevOps environment.\n 3. Create a service connection: This creates the link between Azure DevOps and your Azure Machine Learning workspace, allowing the pipelines to interact with your models and other resources.\n 4. Create a release pipeline: This pipeline is used to automate the deployment of the model to the target environment whenever a new model version is registered.","poster":"brzhanyu"},{"comment_id":"1221459","content":"To configure Azure Pipelines to deploy a model from an Azure Machine Learning workspace, you should follow a specific sequence of actions. Here's the correct order based on the provided options:\n\nCreate an Azure DevOps project: This is the initial step where you set up the project in Azure DevOps.\nInstall the Machine Learning extension for Azure Pipelines: This step ensures that you have the necessary tools to work with Azure Machine Learning in your pipeline.\nCreate a service connection: This step involves creating a connection to the Azure Machine Learning workspace so that the pipeline can interact with it.\nCreate a build pipeline: This is where you define the build process, which includes steps to register the model.\nCreate a release pipeline: Finally, set up the release pipeline to deploy the registered model to the target environment.","upvote_count":"1","poster":"jessyMIH","timestamp":"1717056900.0"},{"content":"On exam 2023-03-27","upvote_count":"3","poster":"esimsek","timestamp":"1679939100.0","comment_id":"852337"},{"timestamp":"1677196920.0","upvote_count":"3","content":"Create a service connection: A service connection is required to connect Azure DevOps to Azure Machine Learning workspace.\nInstall the Machine learning extension for Azure pipelines: This extension is required to use the Azure Machine Learning tasks in the release pipeline.\nCreate a build pipeline: This step is optional, but it can be useful to create a build pipeline to build and package the model.\nCreate a release pipeline: This step is essential to create a release pipeline that deploys the model to the target environment. \n\nCreate an Azure DevOps project is not necessary since you have already created an Azure DevOps organization.","comment_id":"819916","poster":"phdykd"}],"exam_id":64,"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/80913-exam-dp-100-topic-4-question-34-discussion/","timestamp":"2022-09-07 14:32:00","question_id":393},{"id":"zTQ8qEr5kcxSALFFazkq","topic":"4","url":"https://www.examtopics.com/discussions/microsoft/view/86307-exam-dp-100-topic-4-question-35-discussion/","answer_images":[],"question_text":"You use the Azure Machine Learning designer to create and run a training pipeline.\nThe pipeline must be run every night to inference predictions from a large volume of files. The folder where the files will be stored is defined as a dataset.\nYou need to publish the pipeline as a REST service that can be used for the nightly inferencing run.\nWhat should you do?","question_images":[],"answers_community":["A (100%)"],"exam_id":64,"answer_description":"Azure Machine Learning Batch Inference targets large inference jobs that are not time-sensitive. Batch Inference provides cost-effective inference compute scaling, with unparalleled throughput for asynchronous applications. It is optimized for high-throughput, fire-and-forget inference over large collections of data.\nYou can submit a batch inference job by pipeline_run, or through REST calls with a published pipeline.\nReference:\nhttps://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/parallel-run/README.md","question_id":394,"answer":"A","choices":{"B":"Set the compute target for the pipeline to an inference cluster","D":"Clone the pipeline","A":"Create a batch inference pipeline","C":"Create a real-time inference pipeline"},"discussion":[{"timestamp":"1720947480.0","content":"Correct","upvote_count":"1","comment_id":"775258","poster":"BTAB"},{"upvote_count":"2","comment_id":"702683","poster":"JTWang","timestamp":"1713931440.0","content":"Selected Answer: A\nCorrect."}],"timestamp":"2022-10-24 06:04:00","answer_ET":"A","unix_timestamp":1666584240,"isMC":true},{"id":"WbzYzf2uXtVeCeoXVBvH","timestamp":"2023-03-27 10:36:00","question_images":["https://img.examtopics.com/dp-100/image437.png"],"question_text":"HOTSPOT\n-\n\nYou create an Azure Machine Learning model to include model files and a scoring script.\n\nYou must deploy the model. The deployment solution must meet the following requirements:\n\nâ€¢ Provide near real-time inferencing.\nâ€¢ Enable endpoint and deployment level cost estimates.\nâ€¢ Support logging to Azure Log Analytics.\n\nYou need to configure the deployment solution.\n\nWhat should you configure? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","discussion":[{"upvote_count":"1","comment_id":"1313955","poster":"nposteraro","content":"For Endpoint type the Managed Online Endpoint is the right answer. I was not convinced but then I read this: https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2#managed-online-endpoints-vs-kubernetes-online-endpoints","timestamp":"1731927960.0"},{"upvote_count":"3","poster":"evangelist","comment_id":"1227103","timestamp":"1717916100.0","content":"Endpoint type: Kubernetes online\nDeployment component: Azure Kubernetes Service (AKS) cluster"},{"content":"answers are correct:\n\nA Docker image can be used for real-time inferencing of ML models, depending on the deployment solution and the requirements. For example, you can use a Docker image to create a web service that exposes your model via a REST API and responds to requests with predictions in near real-time1. Alternatively, you can use a Docker image to deploy your model to a cloud platform such as Amazon SageMaker2 or Azure Machine Learning3 that provides managed online endpoints for real-time inferencing. However, using a Docker image for real-time inferencing may also introduce some challenges, such as ensuring the compatibility and security of the image, synchronizing the image with the online feature store, and scaling the image to handle the traffic and latency demands4. Therefore, you should carefully evaluate the trade-offs and best practices of using a Docker image for real-time inferencing of ML models.","poster":"deyoz","comment_id":"1154462","timestamp":"1708400280.0","upvote_count":"1"},{"timestamp":"1708266180.0","poster":"vprowerty","comment_id":"1153323","upvote_count":"1","content":"Deployment component asnwer might be docker image! Because:\n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=azure-cli#define-the-deployment\nDefine the deployment: \nA deployment is a set of resources required for hosting the model that does the actual inferencing. To deploy a model, you must have: (among others)\n- An environment in which your model runs. The environment can be a Docker image with Conda dependencies or a Dockerfile."},{"content":"both managed online endpoints and AKS-based online endpoints can be used to deploy machine learning models for real-time inferencing in Azure Machine Learning. Managed online endpoints are fully managed by Azure Machine Learning and provide a simple and cost-effective way to deploy models for real-time inferencing. They also provide out-of-the-box monitoring and logging powered by Azure Monitor and Log Analytics, which includes key metrics and log tables for endpoints and deployments. On the other hand, AKS-based online endpoints provide more flexibility and control, but require more user responsibility to set up and manage. \n\nSo the only reason I would go for managed online is the mentioning of Azure Log Analytics directly. \n\nSecond one should be Azure Kubernetes Service (AKS) for deployment. \n\nSaM","comment_id":"989807","upvote_count":"1","poster":"PI_Team","timestamp":"1692951180.0"},{"timestamp":"1685871120.0","content":"To enable endpoint and deployment level cost estimates, you can use managed online endpoints. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way. They take care of serving, scaling, securing, and monitoring your models, freeing you from the underlying infrastructure management.\n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2#managed-online-endpoints-vs-kubernetes-online-endpoints","poster":"snegnik","upvote_count":"2","comment_id":"914301"},{"upvote_count":"2","comments":[{"comment_id":"882158","timestamp":"1682561160.0","content":"https://learn.microsoft.com/en-us/azure/aks/monitor-aks#analyze-log-data-with-log-analytics\nThere is a 'Analyze log data with Log Analytics' part, maybe this is the evidence that we can choose AKS for the second question. If anyone find better evidence please let me know","upvote_count":"1","poster":"ZoeJ"}],"poster":"ZoeJ","content":"https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints?view=azureml-api-2\nmanaged online can provide deployment level cost estimates","comment_id":"882155","timestamp":"1682560740.0"},{"upvote_count":"2","content":"I would go for EndpointType: managed online, Deployment component: AKS cluster","poster":"sap_dg","comment_id":"853872","timestamp":"1680051480.0"},{"timestamp":"1679939280.0","content":"Was on exam 2023-03-27","upvote_count":"4","comment_id":"852341","poster":"esimsek"},{"comment_id":"851931","upvote_count":"1","poster":"esimsek","timestamp":"1679906160.0","content":"Is it correct?"}],"isMC":false,"answer":"","answer_images":["https://img.examtopics.com/dp-100/image438.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/104052-exam-dp-100-topic-4-question-36-discussion/","answer_ET":"","topic":"4","answer_description":"","unix_timestamp":1679906160,"answers_community":[],"exam_id":64,"question_id":395}],"exam":{"numberOfQuestions":512,"provider":"Microsoft","name":"DP-100","isMCOnly":false,"lastUpdated":"12 Apr 2025","id":64,"isBeta":false,"isImplemented":true},"currentPage":79},"__N_SSP":true}