{"pageProps":{"questions":[{"id":"y7I9IXEwYlejYqm62239","exam_id":67,"question_id":186,"answers_community":[],"isMC":false,"timestamp":"2021-05-14 00:58:00","answer_description":"Box 1: Azure Stream Analytics -\n\nBox 2: Hopping -\nHopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap and be emitted more often than the window size. Events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size.\n\nBox 3: Point within polygon -\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions","discussion":[{"comment_id":"373772","upvote_count":"90","timestamp":"1622740800.0","content":"You do not need a Window function. You just process the data and perform the geospatial check as it arrives. See the same example here:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios","poster":"Alekx42","comments":[{"content":"That's what I thought, there's no reporting over time periods. It's just a case of when this happens, ping it off.","timestamp":"1623056520.0","upvote_count":"7","comment_id":"376646","poster":"captainbee"},{"upvote_count":"8","content":"It works, But not the most const effective.","timestamp":"1640792280.0","poster":"isuru89","comment_id":"512327"},{"upvote_count":"1","content":"The constraint is: \"a message is added to another event hub for processing within 30 seconds\".\nThe example you provided specifies that: \"Devices can emit their ID and location every minute through a stream called DeviceStreamInput\" so does DeviceStreamInput only emit every minute or can you adapt it? Else you would need a window.","comment_id":"1025513","timestamp":"1696499520.0","poster":"Vanq69"}]},{"upvote_count":"85","content":"1. Azure Stream Analytics\n2. No Window\n3. Point within Polygon","comment_id":"383162","timestamp":"1623826200.0","poster":"JackArmitage"},{"upvote_count":"4","timestamp":"1719067440.0","poster":"Pisha","comment_id":"1235447","content":"Got this question in exam 22/06/2024. but only 3 options in Window without tumbling"},{"upvote_count":"1","timestamp":"1717161900.0","comment_id":"1222179","poster":"KarlGardnerDataEngineering","content":"I see that people are mentioning tumbling window here for this question. Would the query look something like this?\n\nSELECT Collect() as allEvents \nINTO Output\nFROM DeviceStreamInput \nJOIN GeoFenceReference\nON ST_WITHIN(DeviceStreamInput.GeoPosition, SiteReferenceInput.Geofence) = 0\nWHERE DeviceStreamInput.DeviceID = GeoFenceReference.DeviceID\nGROUP BY TumblingWindow(second, 30)"},{"content":"1. Azure Stream Analytics\n2. Sliding (change on event) or No window for me\n3. Point within Polygon","timestamp":"1702726980.0","poster":"blazy001","comment_id":"1098117","upvote_count":"1"},{"comment_id":"1092142","timestamp":"1702163580.0","content":"Wrong, chatgpt :\n- **Service**: Azure Stream Analytics – This service is ideal for processing real-time streaming data from IoT devices.\n- **Window**: No window – As you're processing each GPS event as it arrives, you don't need to aggregate over a time window but process each event individually to check if it's within the expected area.\n- **Analysis type**: Point within polygon – This analysis type is used for geospatial analytics, where you're checking if a point (the GPS position) is within a predefined polygon (the expected geographical area for the vehicle).","upvote_count":"2","poster":"Momoanwar"},{"content":"It is clear tumbling window guys. \nrequirements:- \n1) new data created in every 1 min\n2) if any out of range problem is there then need to send with in 30 sec.\nso window size is 1 min, we will have 1 min of data., so we will not miss data.\nhop size will be 30 sec, so we can process the data in 30 sec and report the problem if any.","timestamp":"1691977260.0","comment_id":"980388","poster":"ThisAlreadyTaken","upvote_count":"3"},{"timestamp":"1685036520.0","content":"Correct answers are:\n1. Azure Stream analytics\n2. Tumbling window\n3. Point within Polygon","comment_id":"906815","upvote_count":"8","poster":"janaki"},{"content":"It sounds like you want to use Azure Stream Analytics for this task. Stream Analytics is a real-time analytics service that allows you to analyze and process high volumes of streaming data from various sources, such as Azure Event Hubs.\n\nFor the window, you should use a Tumbling window. A tumbling window is a fixed-sized, non-overlapping window of data. It is well-suited for this scenario because you want to process the data once per minute, and a tumbling window with a size of 1 minute would allow you to do this.\n\nFor the analysis type, you should use Point within polygon. This analysis type allows you to determine whether a GPS position falls within a specific geographical area. You can use the CSV file in the Azure Data Lake Storage Gen2 container to define the expected geographical areas for each vehicle.","upvote_count":"9","comment_id":"769040","timestamp":"1673144640.0","poster":"akk_1289"},{"comment_id":"749256","content":"Choosing no window is totally wrong. No window means you have to process a msg everytime a event happened. It's costly. But a session window can be trigged when there is a event and can combine all the events in maxduration size(here is 30s) to add a packet of events at one time. And other types of windows are not suitable for the situation here.","timestamp":"1671403260.0","poster":"youngbug","upvote_count":"9"},{"comments":[{"poster":"semauni","comment_id":"968060","timestamp":"1690804860.0","content":"Why exactly? That means that every point is going to fall inside two events, so you'll be pinged two times.","upvote_count":"3"}],"timestamp":"1667202900.0","poster":"hoangcv","upvote_count":"2","content":"I think the answer is correct, should be hopping window with window size is 30 seconds and hope size is 1 minute.","comment_id":"708268"},{"comment_id":"640697","poster":"Deeksha1234","upvote_count":"3","content":"Answer is correct, agree to the point made by VyshakhUnnikrishnan","timestamp":"1659364680.0"},{"content":"when a GPS position is outside the expected area, a message is added to another event hub for processing within 30 seconds. - this has to be taken in to account \nNo Window","poster":"vishal10","timestamp":"1659094680.0","comment_id":"639186","upvote_count":"3"},{"poster":"vishal10","content":"1. Azure Stream Analytics\n2. No Window\n3. Point within Polygon","comment_id":"638314","upvote_count":"2","timestamp":"1658951760.0"},{"comment_id":"602975","poster":"Aurelkb","timestamp":"1652800800.0","content":"i thing it is correct","upvote_count":"2"},{"poster":"nefarious_smalls","upvote_count":"3","content":"I am not sure about tumbling window here. I think it would work but what would be the requirement to send an event out every thirty seconds with no data i.e no vehicle has traveled outside the geographic area. In my opinion, a no window could send data to another event hub as needed.","timestamp":"1652374380.0","comment_id":"600738"},{"content":"It's Tumbling.\nNo Window: it should run 500 times per minute (500 messages are sent per minute) -> costly.\nTumbling Window: if we configure 15s -> it runs 4 times per minute -> much better. \nHopping Window: Some messages are processed twice (don't care allowing duplication or not) -> costly.","comment_id":"575326","timestamp":"1648258200.0","upvote_count":"12","poster":"Khiem"},{"comment_id":"545962","upvote_count":"7","content":"Trumbling is the right answer because a message is added to another event hub for processing within 30 seconds -> Tumbling","timestamp":"1644680280.0","poster":"adel182ff"},{"content":"You need a window function to broadcast the messages every 30 seconds for the past one minute. It needs to geofence and update the second output with the location. \n1. Azure Stream Analytics\n2. Hopping Window\n3. Point within Polygon\nRef: https://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios\n\nlook for the text \"The following query joins the device stream with the geofence reference data and calculates the number of requests per region on a time window of 15 minutes every minute.\"\nThis is a similar example","comments":[{"upvote_count":"2","content":"Trumbling is the right answer because a message is added to another event hub for processing within 30 seconds -> Tumbling","poster":"adel182ff","comment_id":"545963","timestamp":"1644680340.0"}],"timestamp":"1644522300.0","upvote_count":"11","poster":"VyshakhUnnikrishnan","comment_id":"544823"},{"timestamp":"1641313380.0","content":"you use hopping to calculate moving average like this case. its correct","poster":"VeroDon","upvote_count":"1","comments":[{"upvote_count":"2","poster":"kamil_k","comment_id":"568526","timestamp":"1647363000.0","content":"In this case though you are not calculating aggregates, but instead sending messages when vehicle is detected to be outside of its designated area. CSV file is used as reference input to check events against. And you don't want to send a message for the same event multiple times."}],"comment_id":"516799"},{"upvote_count":"11","comments":[{"timestamp":"1653420060.0","comment_id":"606877","upvote_count":"1","comments":[{"poster":"sdokmak","comment_id":"606879","upvote_count":"1","timestamp":"1653420360.0","content":"or sent to another event hub for processing, not the csv :)\npoint is I think it's a play with words to trick us."}],"poster":"sdokmak","content":"The wording is strange, 30 seconds for the script to update csv file, not to do with the reference data's time window."},{"timestamp":"1641370980.0","poster":"lukeonline","upvote_count":"4","content":"sounds feasible, I agree on that.\n1. Azure Stream Analytics \n2. Tumbling Window - 30 sec (less costs than no window and no need of hopping window)\n3. Point within Polygon","comment_id":"517268","comments":[{"comments":[{"content":"Correct answer is Hoping window. It's sending the event every 30 sec for the last 1 minute. Ref. https://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios","comment_id":"570530","comments":[{"content":"without going to your mentioned link I can say your answer is wrong. Where in the question did you see \"It's sending the event every 30 sec for the last 1 minute.\"? It's just sending the data to a different hub to be processed within 30 seconds when it is outside the expected area, also every question will not have the same usecase as shown in the document. The document use case is different and this use case is different.","upvote_count":"2","timestamp":"1655526660.0","comment_id":"618085","poster":"Aditya0891"}],"upvote_count":"1","timestamp":"1647608820.0","poster":"Yohannesmulu"}],"poster":"Yohannesmulu","content":"Correct answer is Hoping window. It's sending the even every 30 sec for the last 2 minute. Ref. https://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios","upvote_count":"1","timestamp":"1647608700.0","comment_id":"570528"}]}],"timestamp":"1640792160.0","content":"The expected resolution is within 30 seconds. If you don't use windowing triggering happens for each event. which is not cost-effective. So you can use a tumbling window of 30 seconds which is definitely you don't need to trigger same event twice. Aldo don't get confused with 1 minute here it's just a distractor and that's why most people has gone with a hopping window by only considering numbers. So 100% its definitely tumbling window, stream analytics using a point in polygon analysis","poster":"isuru89","comment_id":"512325"},{"upvote_count":"2","timestamp":"1633152660.0","poster":"rikku33","comment_id":"455873","content":"Window: Session"},{"upvote_count":"2","comment_id":"427350","content":"answers are correct: Hopping is correct\nSELECT count(*) as NumberOfRequests, RegionsRefDataInput.RegionName \nFROM UserRequestStreamDataInput\nJOIN RegionsRefDataInput \nON st_within(UserRequestStreamDataInput.FromLocation, RegionsRefDataInput.Geofence) = 1\nGROUP BY RegionsRefDataInput.RegionName, hoppingwindow(minute, 1, 15)","poster":"Amalbenrebai","timestamp":"1629364800.0"},{"comment_id":"403387","content":"I would say Tumbling window as minimizing cost is a requirement as well. No window indicates you will recalculate if the point is inside the polygon every time a car moves. A tumbling window will only perform the calculation once every 30 seconds.","comments":[{"comments":[{"content":"From 500 vehicles. Assuming a uniform distribution you would get 250 events per 30 second window. Better to process 250 events once, or 1 event 250 times?","timestamp":"1634841120.0","upvote_count":"2","comment_id":"465818","poster":"cwiggins"}],"comment_id":"428550","upvote_count":"3","timestamp":"1629530520.0","content":"Question says data from the vehicles sent to azure event hub only once every minute so this isn't valid reasoning","poster":"GeneralZhukov"}],"timestamp":"1625926800.0","poster":"hs28974","upvote_count":"4"},{"timestamp":"1625578860.0","poster":"Newfton","comment_id":"400029","upvote_count":"2","content":"The explanation for Hopping Window only states what a hopping window is, not why is the correct answer here. It does not make sense in this question, I think it should be No Window."},{"upvote_count":"2","content":"How will the CSV file be read though? I thought Azure Stream Analytics can only load reference from Blob or Azure SQL?","poster":"Peterlustig2049","comment_id":"393567","timestamp":"1624949100.0","comments":[{"comment_id":"467865","content":"blob is a generic term, adls gen2 can contain tables, files and blob container\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data#configure-blob-reference-data","timestamp":"1635230880.0","upvote_count":"2","poster":"aaaaaaaan"}]},{"timestamp":"1624089240.0","upvote_count":"10","comment_id":"385328","content":"1. Azure Stream Analytics\n2. No Window \n3. Point within Polygon\n\nNo Window because you can write a query that joins the device stream with the geofence reference data and generates an alert every time a device is outside of an allowed building.\n\nSELECT DeviceStreamInput.DeviceID, SiteReferenceInput.SiteID, SiteReferenceInput.SiteName \nINTO Output\nFROM DeviceStreamInput \nJOIN SiteReferenceInput\nON st_within(DeviceStreamInput.GeoPosition, SiteReferenceInput.Geofence) = 0\nWHERE DeviceStreamInput.DeviceID = SiteReferenceInput.AllowedDeviceID\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios#generate-alerts-with-geofence","poster":"eng1"},{"content":"I would say No window, because Azure streaming service will have to respond when a vehicule is outside an area (by event), no window since we don't want it to calculate a metric here no mean, no sum.","comment_id":"377526","upvote_count":"4","timestamp":"1623155640.0","poster":"nas28"},{"content":"Answers :\n1) Azure streams analytics\n2) Hopping windows\n3) Point within Polygon\n\nExplained clearly about fencing\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios","comment_id":"375640","comments":[{"upvote_count":"3","content":"Really? Your reference is for \"the number of requests per region on a time window of 15 minutes every minute.\"","timestamp":"1633995360.0","poster":"YipingRuan","comment_id":"460836"}],"upvote_count":"6","poster":"ThiruthuvaRajan","timestamp":"1622956200.0"},{"comments":[{"poster":"captainbee","upvote_count":"1","timestamp":"1624369080.0","comment_id":"388021","content":"But hopping is for reporting at set intervals? Not for when an event happens."}],"poster":"Whiz_01","upvote_count":"6","timestamp":"1621612380.0","content":"Hopping is in the answer. The event is only triggered when a condition is met. Which means we will have overlapping events.","comment_id":"363150"},{"comments":[{"timestamp":"1621335960.0","poster":"alain2","comment_id":"360394","content":"yes, tumbling window makes more sense","upvote_count":"2"}],"content":"isn't it tumbling window?","upvote_count":"9","comment_id":"356701","poster":"sagga","timestamp":"1620946680.0"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0020500001.png"],"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/52650-exam-dp-203-topic-2-question-33-discussion/","answer":"","question_text":"HOTSPOT -\nYou are designing a monitoring solution for a fleet of 500 vehicles. Each vehicle has a GPS tracking device that sends data to an Azure event hub once per minute.\nYou have a CSV file in an Azure Data Lake Storage Gen2 container. The file maintains the expected geographical area in which each vehicle should be.\nYou need to ensure that when a GPS position is outside the expected area, a message is added to another event hub for processing within 30 seconds. The solution must minimize cost.\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0020400001.png"],"unix_timestamp":1620946680,"topic":"2"},{"id":"uiyYT6RaLacadRAFFqJP","question_text":"You are designing an Azure Databricks table. The table will ingest an average of 20 million streaming events per day.\nYou need to persist the events in the table for use in incremental load pipeline jobs in Azure Databricks. The solution must minimize storage costs and incremental load times.\nWhat should you include in the solution?","url":"https://www.examtopics.com/discussions/microsoft/view/53677-exam-dp-203-topic-2-question-34-discussion/","answer_images":[],"choices":{"D":"Use a JSON format for physical data storage.","B":"Sink to Azure Queue storage.","C":"Include a watermark column.","A":"Partition by DateTime fields."},"exam_id":67,"topic":"2","question_id":187,"answer_description":"","isMC":true,"timestamp":"2021-05-27 20:12:00","unix_timestamp":1622139120,"answer":"B","discussion":[{"upvote_count":"29","timestamp":"1622139120.0","content":"The ABS-AQS source is deprecated. For new streams, we recommend using Auto Loader instead.","poster":"bc5468521","comment_id":"368212"},{"comment_id":"436950","poster":"manquak","timestamp":"1630482960.0","content":"Why not partition by date? What does the auto loader have to do with streaming jobs?","upvote_count":"17"},{"poster":"20b1837","content":"Selected Answer: C\nWatermark.\nFor those saying it can't be watermark please consider that watermark has different meaning based on the context. I.e. different concept when used in a streaming or loading context. Please see the below link for why watermark is used and how it is used for incremental loading.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview","timestamp":"1742547060.0","upvote_count":"1","comment_id":"1401483"},{"comment_id":"1360118","upvote_count":"2","poster":"Pey1nkh","timestamp":"1740226560.0","content":"Selected Answer: A\nPartitioning by DateTime is the most effective approach to minimize storage costs and speed up incremental load times."},{"timestamp":"1733026380.0","poster":"de_examtopics","upvote_count":"3","comment_id":"1320447","content":"Selected Answer: A\nB. Sink to Azure Queue storage: Queue storage is primarily for messaging and not ideal for storing large volumes of streaming data efficiently.\n\nC. Include a watermark column: While a watermark column is useful for processing streaming data, it does not directly address storage costs or load times.\n\nD. Use a JSON format for physical data storage: JSON can be easy to work with, but it tends to use more storage space compared to more efficient formats like Parquet, which can negatively impact storage costs."},{"content":"Selected Answer: A\nPartitioning by DateTime fields helps in organizing the data efficiently, which can significantly reduce the time required for incremental loads. It allows you to quickly access and process only the relevant partitions, rather than scanning the entire dataset1.\nIncluding a watermark column (Option C) is also important for managing late-arriving data and ensuring that only the most recent data is processed. However, it doesn’t directly address storage costs or incremental load times2.\nSinking to Azure Queue storage (Option B) is not suitable for this scenario as it is more appropriate for message queuing rather than persistent storage for large volumes of data3.\nUsing a JSON format for physical data storage (Option D) is not recommended because JSON is not optimized for storage efficiency or query performance. Instead, using a columnar storage format like Parquet or Delta Lake would be more efficient4.","comment_id":"1280629","upvote_count":"2","timestamp":"1725834480.0","poster":"a85becd"},{"content":"Selected Answer: C\nA watermark column is essential for implementing event time-based processing in streaming data scenarios. It helps track the progress of event ingestion and ensures that only the latest data is processed, thereby enabling efficient incremental loading.","poster":"Alongi","upvote_count":"1","comment_id":"1203552","timestamp":"1714306140.0"},{"poster":"Alongi","timestamp":"1711732200.0","upvote_count":"2","comment_id":"1185595","content":"Selected Answer: C\nWatermark column could reduce the storage"},{"poster":"Azure_2023","comment_id":"1140347","timestamp":"1707066600.0","upvote_count":"2","content":"Selected Answer: A\nWell, Azure Queue Storage is a service for storing large numbers of messages. You access messages from anywhere in the world via authenticated calls using HTTP or HTTPS. A queue message can be up to 64 KB in size. A queue may contain millions of messages, up to the total capacity limit of a storage account. Queues are commonly used to create a backlog of work to process asynchronously, like in the Web-Queue-Worker architectural style.\n\nI believe the correct answer is A."},{"poster":"j888","upvote_count":"1","content":"Incremental key and time stamp matching watermark behaviour\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview","comments":[{"content":"A. more likely a better answer.\n\nA. Partition by DateTime fields:\n\nReason: Partitioning the table by date-related fields (e.g., year, month, day) allows efficient filtering during incremental load jobs. Queries can easily scan only relevant partitions for new data, significantly reducing processing time and associated costs.\nC. Include a watermark column:\n\nReason: A watermark column helps track the progress of data processing, allowing incremental jobs to focus on data newer than the last processed watermark. This ensures efficient updates without reprocessing already loaded data.","upvote_count":"3","poster":"j888","timestamp":"1708588620.0","comment_id":"1156222"}],"comment_id":"1136333","timestamp":"1706667780.0"},{"content":"Selected Answer: A\nchat gpt \no design an efficient Azure Databricks table for ingesting an average of 20 million streaming events per day while minimizing storage costs and incremental load times, you should consider the following:\n\nA. Partition by DateTime fields.\n\nExplanation:\n\nPartitioning by DateTime fields is a common practice for time-series data in Azure","comment_id":"1107649","poster":"dakku987","upvote_count":"1","timestamp":"1703757360.0"},{"comment_id":"1000651","timestamp":"1694006280.0","content":"Selected Answer: B\nshould be B","upvote_count":"2","poster":"kkk5566"},{"timestamp":"1691482020.0","content":"Selected Answer: B\noption B","upvote_count":"1","poster":"akhil5432","comment_id":"975322"},{"upvote_count":"1","poster":"vctrhugo","content":"Sinking to Azure Queue storage is not necessary for persisting the events in the Azure Databricks table. Azure Queue storage is typically used for decoupling and asynchronous messaging scenarios and may not directly contribute to minimizing storage costs or incremental load times for the Databricks table.","timestamp":"1687812180.0","comment_id":"934785"},{"poster":"auwia","timestamp":"1687415940.0","comment_id":"930215","upvote_count":"3","content":"Selected Answer: B\nProbably it is B:\nPartition by date&time is not the best, immagine events with each single partition because of (day, hour, minute, second) => the requirement is clear, minimiuze the space, etc..\nYou use Watermark when you need to reduce the amount of state data to improve latency during a long-running steaming operation.\nJSON I would exclude because how it is formulated.\nMy answer is B, even if it's deprecated, it's clear that this question is an old one, but looking at the commnents, we can still get in the exam."},{"poster":"dksks","timestamp":"1683448560.0","upvote_count":"2","content":"Selected Answer: A\nA. Partition by DateTime fields: Partitioning the table on frequently used columns such as DateTime fields can improve query performance and reduce incremental load times. Partitioning by DateTime can help to reduce the amount of data scanned during query execution and facilitate incremental loading.","comment_id":"891249"},{"upvote_count":"2","poster":"hiyoww","content":"is the question outdated?","timestamp":"1679980440.0","comment_id":"852803"},{"content":"Selected Answer: A\nIm sure it is A. Partition by DateTime!!","upvote_count":"1","poster":"haidebelognime","comment_id":"806350","timestamp":"1676207640.0"},{"poster":"kckalahasthi","timestamp":"1670563080.0","content":"https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/aqs","upvote_count":"2","comment_id":"739813"},{"timestamp":"1668977760.0","poster":"Igor85","comment_id":"723003","upvote_count":"4","content":"question is deprecated, AutoLoader is the way to do the incremental loads"},{"upvote_count":"4","timestamp":"1661408580.0","content":"As per requirement: \"You need to persist the events in the table for use in incremental load pipeline jobs in Azure Databricks. \n\nWha I understood from this is, dataset which will stored would be used by Databricks and load type is incremental. Considering this, I see \"watermark column\" makes more sense.","comment_id":"651671","poster":"Rajashekharc"},{"timestamp":"1659364920.0","upvote_count":"2","poster":"Deeksha1234","content":"B is correct","comment_id":"640702"},{"poster":"Aurelkb","upvote_count":"2","comment_id":"602977","timestamp":"1652800860.0","content":"Selected Answer: B\nCorret"},{"poster":"kamil_k","comment_id":"568532","content":"Do we want to persist this data in a table or in a message queue? From what the question asks it has to be a table. Why would we use queue storage for this task?","timestamp":"1647363780.0","upvote_count":"4"},{"timestamp":"1640120640.0","upvote_count":"10","content":"Selected Answer: B\nA. Partition by DataTime field\nEach partition will generate a file. Loading latency may reduce, but feel storage cost will increase because generate more folders and files for different partition. Is it right???\nB. Sink to Azure Queue Storage. \nRead this document. Spark table files are stored in DBFS. Mount Azure Blob storage containers to Databricks File System (DBFS). If The Databricks ABS-AQS provides these two benefits, sounds like it is a correct answer.\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/aqs\nC. Include a watermark column: For sure it is not correct\nWatermarks define how long your aggregate should wait around for data delay.\nD. User a Json format for physical data storage. - ???\nDon't find any documents to compare physical data storage of JSON, CSV, and Parquet.","poster":"Canary_2021","comment_id":"506438"},{"poster":"berserksap","upvote_count":"3","content":"I think ABS-AQS would be right answer though deprecated. If not it would be water mark or auto loader. As per my understanding Water mark columns use more space and hit the API accordingly many times when compared to ABS-AQS and is costly.","comment_id":"470248","timestamp":"1635600120.0"},{"upvote_count":"1","content":"Why not C ??","poster":"kilowd","timestamp":"1634721300.0","comment_id":"465052"},{"timestamp":"1630919820.0","poster":"Amalbenrebai","upvote_count":"4","comment_id":"440214","content":"I hesitate between A OR C"},{"upvote_count":"2","poster":"belha","timestamp":"1625050380.0","content":"TRUE ???","comment_id":"394608"}],"answer_ET":"B","question_images":[],"answers_community":["B (51%)","A (37%)","11%"]},{"id":"P7p9xfUxtDzZ6O9vtVT3","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0020900001.png"],"answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0020800001.png"],"discussion":[{"comment_id":"366047","content":"1/14 = 0.07\n6% = 0.06\n\nshould be lowered.","timestamp":"1621910640.0","comments":[{"upvote_count":"1","comment_id":"1314981","content":"I guess it should be lowered. MS documentation says \"but the execution of concurrent jobs reaches a node's limits, scale up by increasing the number of concurrent jobs \" but in this case limit is 14 and only 2 jobs are running means not enough input to process. Just because the limit is 14, the system will not create 14 concurrent jobs it does only when the throughput is increases.","poster":"DixonDavis","timestamp":"1732065000.0"},{"upvote_count":"5","poster":"shachar_ash","content":"The question mentions 2/14 which is 0.14, therefore it can be increased.","timestamp":"1653589980.0","comment_id":"607764"},{"upvote_count":"4","content":"Why is this the calculation you make? I see 6% utilization, so 94% to go, so the amount can be raised.","timestamp":"1690806180.0","comment_id":"968094","poster":"semauni"},{"content":"0.06/2 = 0.03\n0.03 * 14 = 0.42 = maximally 42% of cpu for all jobs\nisn't this better?","upvote_count":"10","comment_id":"526565","poster":"romanzdk","timestamp":"1642507080.0"}],"poster":"Sunnyb","upvote_count":"34"},{"upvote_count":"25","comments":[{"upvote_count":"1","timestamp":"1707701520.0","poster":"moneytime","comment_id":"1147769","content":"In essence, the 6% of the CPU depicts a low resource usage, therefore the concurrent jobs limit should be increased."},{"content":"In essence, the 6% of the CPU depicts a low resource usage, therefore the concurrent jobs limit.","upvote_count":"1","timestamp":"1707701400.0","comment_id":"1147767","poster":"moneytime"}],"timestamp":"1623938400.0","content":"\"We recommend that you increase the concurrent jobs limit only when you see low resource usage with the default values on each node.\"\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime","poster":"MirandaL","comment_id":"384240"},{"poster":"Pey1nkh","upvote_count":"1","comment_id":"1360120","timestamp":"1740227160.0","content":"Raise!! \nYou should raise the number of concurrent jobs, as the system is currently underutilized in terms of both CPU and memory, and increasing the concurrent jobs will help better use available capacity."},{"upvote_count":"2","timestamp":"1729340760.0","comment_id":"1300008","poster":"Thameur01","content":"Based on the description of the integration runtime configuration and node details in the image, here are the correct choices for the statements:\n\n1. **If the X-M node becomes unavailable, all executed pipelines will:**\n - Since there is only one node (1/1) and high availability is disabled, if the X-M node becomes unavailable, **all executed pipelines will fail until the node comes back online**.\n\n2. **The number of concurrent jobs and the CPU usage indicate that the Concurrent Jobs (Running/Limit) value should be:**\n - The current concurrent jobs are 2/14, and the CPU utilization is only at 6%, indicating that the system has plenty of resources available to handle more concurrent jobs. Therefore, the Concurrent Jobs (Running/Limit) value **should be raised**.\n\nThese answers reflect the situation presented in the image. Let me know if you need further clarification!"},{"content":"The answer provided by edba is explaining the question as well as the answer very well. The second question is asking what should we do with the LIMIT of the concurrent jobs which is currently set to 14. Since only 2 jobs are running, the LIMIT of concurrent jobs can be reduced from 14, so the question is asking about what should be do with the limit of concurrent jobs and not about what should we do with the actual concurrent jobs that can be run.\n\nCheck the below link\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#scale-considerations","poster":"learnwell","comment_id":"1240873","timestamp":"1719935220.0","upvote_count":"2"},{"poster":"saqib839","upvote_count":"3","content":"So the math is simple we have 2 concurrent processes running causing 6% CPU utilization. The concurrent process limit is 14 so if we multiply both sides (2 process = 6%) we get (14process = 42%) which is under utilized. We should raise concurrent process limit to 26 which gives us cpu utilization of 76%. \nALL THESE CALCULATIONS ARE DONE CONSIDERING EACH CONCURRENT PROCESS CAUSE SAME CPU UTILIZATION WHICH IS 3%.","comments":[{"upvote_count":"1","comment_id":"1184512","content":"100% / 3% = 33.33 SHOULD BE THE NEW LIMIT -> RAISED","poster":"gplusplus","timestamp":"1711593480.0"}],"comment_id":"1139238","timestamp":"1706964600.0"},{"poster":"j888","comment_id":"1136341","timestamp":"1706668620.0","upvote_count":"1","content":"Agreed on the answer for the first part but not the second.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime\n\"We recommend that you increase the concurrent jobs limit only when you see low resource usage with the default values on each node.\"\nI think the answer should be raised"},{"comment_id":"1126059","content":"Got this question on my exam on january 17, I wasn't sure but I put the same answers as the dump","poster":"ELJORDAN23","upvote_count":"2","timestamp":"1705595940.0"},{"comment_id":"1092157","upvote_count":"4","content":"Chatgpt :\nGiven that high availability is not enabled for the self-hosted integration runtime, the correct answer for the first statement is:\n\n- **Fail until the node comes back online.**\n\nFor the second statement regarding the number of concurrent jobs, considering that the CPU utilization is quite low at 6%, and there is a significant difference between the number of running jobs (2) and the limit (14), the correct answer should be:\n\n- **Left as is.**\n\nThere is no indication from the given data that the concurrent jobs limit needs to be adjusted, as the system is currently underutilized.","poster":"Momoanwar","timestamp":"1702165080.0"},{"upvote_count":"2","poster":"phydev","timestamp":"1698693120.0","comment_id":"1058209","content":"ChatGPT says: *it should be raised* because there's currently a very low CPU utilization (only 6%) and two concurrent jobs running out of a limit of 14. The fact that the CPU utilization is quite low suggests that your integration runtime has available processing capacity."},{"timestamp":"1696501920.0","upvote_count":"1","content":"I don't know why there is this one standard bs answer with lowered everywhere.\nSo only 2 concurrent jobs are running out of 14 possible and CPU usage is at 6%, does it not make sense to raise the concurrent jobs to be even 14/14 and still have only 42% CPU usage. Or is this question aiming at something else?","comment_id":"1025534","poster":"Vanq69"},{"comment_id":"1002340","timestamp":"1694167560.0","content":"Left as it is","upvote_count":"2","poster":"[Removed]"},{"content":"be lowered","comment_id":"1000662","poster":"kkk5566","timestamp":"1694007120.0","upvote_count":"1"},{"content":"Based on the information provided, the CPU Utilization is 6% and the Concurrent Jobs (Running/Limit) is 2/14. This indicates that the integration runtime is utilizing only 6% of the available CPU capacity and currently running 2 out of a maximum limit of 14 concurrent jobs.\n\nGiven this information, the appropriate answer choice for the completion statement would be:\n\nConcurrent Job should be scaled up\n\nSince the current CPU utilization is relatively low at 6% and there is still capacity available for running additional jobs, scaling up the concurrent job limit would allow for more jobs to run simultaneously and make better use of the available resources.","timestamp":"1687573800.0","upvote_count":"5","poster":"Omkarrokee","comment_id":"932111"},{"content":"We are talking about max number of job running in parallel! \nIf you have available resource of course it is recommanded to raise up the current limit to afford future load. \nAlso Microsoft recommned that:\nhttps://learn.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime\nWe recommend that you increase the concurrent jobs limit only when you see low resource usage with the default values on each node. I think this is the case, also the question doesn't tell you it's mandatory, what should! So I think we should follow recommandation and raise up the limit.","comment_id":"930241","poster":"auwia","timestamp":"1687418280.0","upvote_count":"2"},{"comment_id":"905085","content":"when the explanation is \"scale up by increasing the number\" then why the answer is \"Lowered\"???","upvote_count":"3","poster":"pavankr","timestamp":"1684860120.0"},{"upvote_count":"3","poster":"norbitek","content":"I would leave it as it is.\nSee:\nhttps://learn.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime\n\n\"The default value of the concurrent jobs limit is set based on the machine size. The factors used to calculate this value depend on the amount of RAM and the number of CPU cores of the machine. So the more cores and the more memory, the higher the default limit of concurrent jobs.\n\nYou scale out by increasing the number of nodes. When you increase the number of nodes, the concurrent jobs limit is the sum of the concurrent job limit values of all the available nodes. For example, if one node lets you run a maximum of twelve concurrent jobs, then adding three more similar nodes lets you run a maximum of 48 concurrent jobs (that is, 4 x 12). We recommend that you increase the concurrent jobs limit only when you see low resource usage with the default values on each node.\"","timestamp":"1673699760.0","comment_id":"775384"},{"upvote_count":"1","poster":"martcerv","comment_id":"754270","timestamp":"1671803520.0","content":"the concurrent jobs limit is the sum of the concurrent job limit values of all the available nodes"},{"timestamp":"1668425820.0","comment_id":"717916","content":"Here is my thinking of this. High availability is False, so no scaling. The 2nd Q is what should be done with the number of concurent jobs, not scaling up CPU. Since there are only 2 running jobs of possible 14 and CPU itilization is only 6% the number of concurent jobs should be Increased. If left as is we are overspending, if decreased we are still overspending even more since CPU utilization will be lovered too.","comments":[{"poster":"OldSchool","comment_id":"732397","timestamp":"1669882980.0","upvote_count":"1","content":"When the processor and available RAM aren't well utilized, but the execution of concurrent jobs reaches a node's limits, scale up by increasing the number of concurrent jobs that a node can run.\nhttps://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#scale-up"}],"poster":"OldSchool","upvote_count":"3"},{"timestamp":"1647365580.0","comment_id":"568559","upvote_count":"1","poster":"kamil_k","content":"according to this article https://docs.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime#:~:text=When%20you%20increase%20the%20number,is%2C%204%20x%2012).\nit is not advisable to touch the default calculated limits unless we encounter issues.."},{"comment_id":"544832","poster":"VyshakhUnnikrishnan","content":"The CPU is only used 6% with 2 parallel jobs running. This gives the opportunity for the cluster to scale up the number of concurrent jobs. The number of parallel/concurrent jobs should hence be increase","comments":[{"upvote_count":"1","comment_id":"568561","timestamp":"1647365940.0","content":"This question is poorly written, it doesn't paint the whole picture. You would need to monitor resource utilisation over a prolonged period of time e.g. 24 hours to see what happens at peak times when you can have all 14 spots taken. Each job can take different amount of compute power. For instance you may find that at some times one jobs consumes 50% CPU.","poster":"kamil_k"}],"timestamp":"1644523320.0","upvote_count":"4"},{"upvote_count":"19","content":"It sounds many people were confused regarding 2nd question. after check the link below, I think it means to lower or raise limit concurrent jobs. Apparently 14 is too high as usage is 2 only, so it should be \"lowered\". ref:https://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#scale-considerations","comments":[{"timestamp":"1670089980.0","comment_id":"734612","content":"Yes, this is the right explanation. the question is about lowering the limit not the running jobs.","upvote_count":"4","poster":"Billybob0604"},{"comment_id":"606888","timestamp":"1653422100.0","poster":"sdokmak","content":"maybe they worded the question poorly deliberately to test you on what you can scale up or down, which would be the limit not the number of jobs.","upvote_count":"3"},{"timestamp":"1668840780.0","content":"agree with this; practically paying more capacity then they need","upvote_count":"1","comment_id":"721815","poster":"Dusica"}],"comment_id":"510714","timestamp":"1640653680.0","poster":"edba"},{"timestamp":"1640041980.0","content":"1) Fail until the node comes back online\n2) left as is\n\nFor all those who are extrapolating CPU based on the 2 jobs that are running - you have no idea what the other 12 concurrent jobs could look like. You could have one additional job that maxes the CPU. You could have 12 easy jobs that bring it up to 10% utilization. Since we don't know, leave things as they are until one of the values becomes a bottleneck.","poster":"onyerleft","comment_id":"505729","comments":[{"content":"You're right, a professional would say that, but I thinks Ms is making it easier. I would say also left as is, but that they have to monitor","timestamp":"1656119040.0","comment_id":"621919","poster":"Davico93","upvote_count":"1"}],"upvote_count":"6"},{"content":"The question is poorly written. The value is expressed as a ratio, so lowering the value consists in increasing the denominator which is the Limit. Which is what I assume is done in practice when your 14/14 jobs would only consume 42% of your CPU.","timestamp":"1637859000.0","poster":"clement_","upvote_count":"2","comment_id":"486813"},{"comment_id":"444189","upvote_count":"4","poster":"Marcus1612","timestamp":"1631566860.0","content":"There is no reason to scale down the concurency limit, because there is no cost impact. So, from an administration perspective, there is no benefit. On the other hand, if the resources were toped, you should scale up the node, but it is not the case in this scenario. The answer is keep this configuration as is."},{"timestamp":"1629915180.0","content":"left as-is\nWhen the processor and available RAM aren't well utilized, but the execution of concurrent jobs reaches a node's limits, scale up by increasing the number of concurrent jobs that a node can run.\nso there is no need to scale it up as we did not reach the limit and there is no benefit from scale it down.","upvote_count":"5","comment_id":"431688","poster":"A1000"},{"poster":"Jacob_Wang","upvote_count":"2","timestamp":"1625360640.0","comment_id":"397906","content":"It might be the ratio. For instance, 2/14 might should be lowered to 2/20."},{"timestamp":"1624291500.0","upvote_count":"3","poster":"saty_nl","comment_id":"387260","content":"Concurrent jobs limit must be raised, as we are under-utilizing the provisioned capacity."},{"upvote_count":"1","comment_id":"383641","poster":"damaldon","content":"A) is correct because of HA is set to FALSE\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability","timestamp":"1623869880.0"},{"poster":"terajuana","upvote_count":"1","timestamp":"1623396000.0","content":"the limit should be left as is to allow capacity for more jobs - a single job could use 20% CPU if it is running intensive work. The pricing model isn't by concurrency so there is no budget rationale to lower it.","comment_id":"379531"},{"poster":"bc5468521","timestamp":"1622139960.0","content":"2 jobs/node, but the CPU is not fully utilized; based on the workload, don't need too many concurrent jobs, so lower to 1 job/node","upvote_count":"1","comment_id":"368227"},{"upvote_count":"8","comment_id":"361283","timestamp":"1621421580.0","comments":[{"comment_id":"373781","timestamp":"1622741760.0","content":"I agree with your explaination. \nI think lowering the limit makes no sense: the system is underloaded, why should you limit the parallelism that you could have when many jobs eventually get executed at the same time?\nMaintaining the current value could be an option: there are no issues with the current configuration with respect to the maximum concurrent job value.\nIncreasing the value is good if we take as true your hypotesis that every job requires the same CPU %.","upvote_count":"5","poster":"Alekx42"}],"poster":"dfdsfdsfsd","content":"I might be misunderstanding this but the way I look at it is that if 2 concurrent jobs use 6% of the CPU, then 1 job requires 3% CPU and you could have approximately 100/3=33 concurrent jobs. So you can raise the limit. What makes me insecure is that I imagine not every job would be equal in CPU-load."},{"timestamp":"1621270560.0","upvote_count":"1","comments":[{"comment_id":"359720","upvote_count":"1","timestamp":"1621270740.0","poster":"YellowSky002","content":"I meant the scalability of nodes should be lowered..."}],"content":"✑ CPU Utilization: 6%\n✑ Concurrent Jobs (Running/Limit): 2/14\nI am also confused but I tend to adjust the explanation because the system still has very low utilization 6% and only 2 out of 14 concurrent jobs are there... Hence I might think it should be lowered...\nCan you please explain why both of you think it should be raised?","poster":"YellowSky002","comment_id":"359715"},{"timestamp":"1620836700.0","content":"Concurrent jobs limit should be raised , no?","comments":[{"timestamp":"1620996240.0","upvote_count":"3","content":"for me, it should be raised. I don't find explanation in the given link... :(","poster":"MacronfromFrance","comment_id":"357195"},{"comment_id":"360409","poster":"alain2","content":"IMO, it should be lowered because:\n. Concurrent Jobs (Running/Limit): 2/14\n. CPU Utilization: 6%","timestamp":"1621336680.0","upvote_count":"1"},{"upvote_count":"14","timestamp":"1623167700.0","content":"If you eat 1 ice cream a day, but you buy 5 new ones every day -- should you increase the amount of ice cream you buy, or lower it? This is the same. You are paying for 14 concurrent jobs, but you are only using 2. You are only using 6 % of the CPU you have purchased, so you are paying for 94 % that you do not use.","comment_id":"377664","poster":"Preben","comments":[{"comments":[{"content":"I understand your point of view, and I understood the question in the same way you did at first. But after reading carefully the sentence it asks (as you said) about the limit value (or the settings) of concurrent jobs, knowing that you only use 6% of your CPU with only 2 concurrent jobs.\nTherefore, considering the waste of resources, \"lowered\" is, imo, the correct answer here (although the formulation of the question is a bit confusing, I admit).","timestamp":"1625821740.0","comment_id":"402577","upvote_count":"5","poster":"Banach"}],"content":"The question is about the action w.r.t. cuncurrent jobs value. Cuncurrent jobs should be raised to make full use of resources. Also, (if possible) the resources should be lowered so that it is not wasted. I think the choice of answer raised/lowered should be based on the context and the context here is about the cuncurrent jobs, not resources. Hence, I think raised would be correct.","poster":"bsa_2021","upvote_count":"6","timestamp":"1624166700.0","comment_id":"385968"},{"content":"data factory pricing is based on activity runs and not concurrency","comment_id":"379521","timestamp":"1623395880.0","upvote_count":"2","poster":"terajuana"}]}],"poster":"tanza","upvote_count":"10","comment_id":"355641"}],"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/52539-exam-dp-203-topic-2-question-35-discussion/","answer_description":"Box 1: fail until the node comes back online\nWe see: High Availability Enabled: False\nNote: Higher availability of the self-hosted integration runtime so that it's no longer the single point of failure in your big data solution or cloud data integration with\nData Factory.\n\nBox 2: lowered -\nWe see:\nConcurrent Jobs (Running/Limit): 2/14\nCPU Utilization: 6%\nNote: When the processor and available RAM aren't well utilized, but the execution of concurrent jobs reaches a node's limits, scale up by increasing the number of concurrent jobs that a node can run\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime","unix_timestamp":1620836700,"answers_community":[],"exam_id":67,"question_id":188,"question_text":"HOTSPOT -\nYou have a self-hosted integration runtime in Azure Data Factory.\nThe current status of the integration runtime has the following configurations:\n✑ Status: Running\n✑ Type: Self-Hosted\n✑ Version: 4.4.7292.1\n✑ Running / Registered Node(s): 1/1\n✑ High Availability Enabled: False\n✑ Linked Count: 0\n✑ Queue Length: 0\n✑ Average Queue Duration. 0.00s\nThe integration runtime has the following node details:\n✑ Name: X-M\n✑ Status: Running\n✑ Version: 4.4.7292.1\n✑ Available Memory: 7697MB\n✑ CPU Utilization: 6%\n✑ Network (In/Out): 1.21KBps/0.83KBps\n✑ Concurrent Jobs (Running/Limit): 2/14\n✑ Role: Dispatcher/Worker\n✑ Credential Status: In Sync\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","isMC":false,"timestamp":"2021-05-12 18:25:00","answer":""},{"id":"rS0d20QcWDHzrgUjHZEa","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/54675-exam-dp-203-topic-2-question-36-discussion/","choices":{"A":"Enable container services for workspace1.","C":"Set Cluster Mode to High Concurrency.","B":"Upgrade workspace1 to the Premium pricing tier.","D":"Create a cluster policy in workspace1."},"topic":"2","answer_description":"","discussion":[{"poster":"ROBERSONWM","timestamp":"1645985940.0","upvote_count":"19","comment_id":"433225","content":"B is the correct answer.\n\nAutomated (job) clusters always use optimized autoscaling. The type of autoscaling performed on all-purpose clusters depends on the workspace configuration.\n\nStandard autoscaling is used by all-purpose clusters in workspaces in the Standard pricing tier. Optimized autoscaling is used by all-purpose clusters in the Azure Databricks Premium Plan.\nhttps://docs.databricks.com/clusters/cluster-config-best-practices.html"},{"timestamp":"1657003680.0","upvote_count":"7","poster":"lukeonline","comment_id":"517281","content":"Selected Answer: B\nWe definitely need \"Optimized Autoscaling\" (not Standard Autoscaling) which is only part of Premium Plan. \n\nReason: We need to scale down after 3 min underutilization and Standard Autoscaling only allows scaling down after at least 10 minutes.\n\nStandard autoscaling: \"Scales down only when the cluster is completely idle and it has been underutilized for the last 10 minutes.\"\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"},{"comment_id":"1360122","poster":"Pey1nkh","content":"Selected Answer: B\nnaughty Microsoft :)! the right action is Cluster Policy but it require the Premium tier. If you're on the Standard tier, you cannot implement this solution until you upgrade to Premium.\nso the answer is \nB. Upgrade workspace1 to the Premium pricing tier.","timestamp":"1740227700.0","upvote_count":"1"},{"poster":"moneytime","upvote_count":"2","comment_id":"1147782","timestamp":"1723420440.0","content":"B and C are both required to do the job .But .the FIRST thing to do before creating the AUTO-SCALING CLUSTER POLICY s to migrate or upgrade to premium tier where AUTO-SCALING is supported or enabled.This explains why B is the valid answer .\nIn conclusion C is part of the process ,however not the first thing to do"},{"content":"Selected Answer: B\nB is correct","comment_id":"1000665","timestamp":"1709739180.0","poster":"kkk5566","upvote_count":"1"},{"content":"Selected Answer: B\nOption B","upvote_count":"1","poster":"akhil5432","timestamp":"1707387360.0","comment_id":"975329"},{"upvote_count":"2","poster":"auwia","comments":[{"upvote_count":"3","poster":"auwia","content":"False, Cluster policies require the Premium plan. :) So B is the correct answer.","timestamp":"1703779380.0","comment_id":"936735"}],"comment_id":"930282","content":"Selected Answer: D\nI've finally found a valid answer:\nhttps://learn.microsoft.com/en-us/azure/databricks/administration-guide/clusters/policies","timestamp":"1703240220.0"},{"comments":[{"comment_id":"926996","poster":"JG1984","content":"Cluster policies are available only in the Premium pricing tier of Azure Data bricks, and not in the Standard pricing tier.","upvote_count":"3","timestamp":"1702955520.0"}],"comment_id":"880077","timestamp":"1698218700.0","content":"B doesn't minimize the costs. To support autoscaling all-purpose clusters in Azure Databricks, you need to create a cluster policy that specifies the auto-scaling settings. The cluster policy allows you to specify when to add or remove workers based on the workload on the cluster.\n\nFor this scenario, the cluster policy should be configured to automatically scale down workers when the cluster is underutilized for three minutes. This will help to minimize costs by reducing the number of idle workers. The policy should also be configured to scale to the maximum number of workers quickly to minimize the time it takes to process workloads.\n\nEnabling container services for workspace1 (option A) is not necessary for autoscaling all-purpose clusters. Upgrading workspace1 to the Premium pricing tier (option B) may not be necessary and may not be cost-effective depending on your specific requirements. Setting Cluster Mode to High Concurrency (option C) is not related to autoscaling all-purpose clusters.","upvote_count":"5","poster":"Rossana"},{"upvote_count":"1","content":"B is correct","poster":"Deeksha1234","comment_id":"640709","timestamp":"1675270260.0"},{"poster":"Aurelkb","comment_id":"602978","content":"Selected Answer: B\ncorrect","upvote_count":"2","timestamp":"1668705660.0"},{"poster":"Egocentric","upvote_count":"1","comment_id":"587369","timestamp":"1666036380.0","content":"B is the correct answer"},{"poster":"Jaws1990","timestamp":"1657790040.0","comment_id":"523498","upvote_count":"6","comments":[{"timestamp":"1680414900.0","upvote_count":"1","comments":[{"comment_id":"723038","poster":"Igor85","content":"no difference anymore between Standard and Premium, indeed","upvote_count":"1","timestamp":"1684612080.0","comments":[{"comment_id":"1193750","poster":"lcss27","upvote_count":"1","content":"https://www.databricks.com/product/pricing/platform-addons\nCluster Policy obliga available on Premium","timestamp":"1728648720.0"}]},{"upvote_count":"1","comment_id":"722498","content":"as Jaws1990 says it is available on both on the link. I has green for both types","timestamp":"1684567080.0","poster":"cosarac"}],"content":"the autoscaling is under the premuim plan not the standrd one and this is clear in the link you shared.","comment_id":"684625","poster":"allagowf"}],"content":"Not sure if this is a valid question anymore. This link shows that the standard pricing tier supports optimised autoscaling. \n\nhttps://databricks.com/product/azure-pricing"},{"upvote_count":"3","comment_id":"507252","content":"Selected Answer: B\nThey need to use Optimized autoscaling for adapting requirements.\n- Optimized autoscaling is used by all-purpose clusters in the Azure Databricks Premium Plan.\n- On job clusters, scales down if the cluster is underutilized over the last 40 seconds.\n- On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds.\nreference:\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure","timestamp":"1655908560.0","poster":"trietnv"},{"comments":[{"timestamp":"1655841840.0","comment_id":"506472","upvote_count":"1","poster":"Canary_2021","content":"A .Enable Databricks Container Service only when you need to use customer containers, so it is not a correct answer.\n\nI vote C to be the correct Answer."}],"timestamp":"1655841060.0","upvote_count":"1","comment_id":"506459","content":"1. Both standard and premium pricing tire support Autopilot Cluster. Autopilot support Autoscaling and Terminate after X minutes of inactivity.\n2. 'Cluster Policies' is only supported by premium pricing tire. Control cost by limiting per cluster maximum cost.\n3. standard pricing tire is cheaper than premium pricing tire. \nBase on these 3 items, I don't figure out why it has to upgrade to Premium pricing tier.","poster":"Canary_2021"},{"content":"Answer B is correct. One has to check on the documentation. There are two autoscaling solutions:\nstandard autoscaling (Standard Tier) and optimized autoscaling (Premimum Tier).\n\nSince there is a requirement of downscaling after three minutes of underutilization, only optimized autoscaling can offer such a solution.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure#optimized-autoscaling\n\nOn all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds.","poster":"Larrave","upvote_count":"6","comment_id":"496290","timestamp":"1654623660.0"},{"content":"Why is the answer not D? Autoscaling is available in the Standard pricing tier. Since \"costs\" is also a factor in this question, why upgrade to premium?","comment_id":"470230","upvote_count":"5","poster":"certstowinirl","timestamp":"1651322100.0"},{"poster":"brendy","upvote_count":"2","comments":[{"content":"Not sure, what about the cost factor and premium doesn’t minimise cost.","poster":"Sudheer_K","upvote_count":"1","comment_id":"451080","timestamp":"1648159080.0"}],"comment_id":"424838","timestamp":"1644859800.0","content":"Is this correct?"},{"comment_id":"375533","comments":[{"comment_id":"375534","content":"please ignore this, it was meant for the question before","timestamp":"1638759480.0","upvote_count":"10","poster":"husseyn"}],"content":"Concurent Jobs should be raised - There is less cpu utilization","poster":"husseyn","upvote_count":"3","timestamp":"1638759420.0"}],"question_id":189,"answers_community":["B (88%)","12%"],"question_text":"You have an Azure Databricks workspace named workspace1 in the Standard pricing tier.\nYou need to configure workspace1 to support autoscaling all-purpose clusters. The solution must meet the following requirements:\n✑ Automatically scale down workers when the cluster is underutilized for three minutes.\n✑ Minimize the time it takes to scale to the maximum number of workers.\n✑ Minimize costs.\nWhat should you do first?","answer":"B","answer_images":[],"exam_id":67,"isMC":true,"answer_ET":"B","timestamp":"2021-06-06 02:57:00","unix_timestamp":1622941020},{"id":"PoZvZQOnrLlchmmiLO7b","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are designing an Azure Stream Analytics solution that will analyze Twitter data.\nYou need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.\nSolution: You use a tumbling window, and you set the window size to 10 seconds.\nDoes this meet the goal?","answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/54801-exam-dp-203-topic-2-question-37-discussion/","exam_id":67,"discussion":[{"content":"correct answer","timestamp":"1623066600.0","comment_id":"376742","poster":"Prabagar","upvote_count":"37"},{"upvote_count":"1","content":"Selected Answer: A\nTumbling Window\nHopping Window\nSliding Window\nSession Window","comment_id":"1243729","timestamp":"1720335780.0","poster":"evangelist"},{"poster":"ELJORDAN23","content":"Selected Answer: A\nGot this question on my exam on january 17, answer A is correct.","upvote_count":"1","timestamp":"1705596060.0","comment_id":"1126060"},{"upvote_count":"1","poster":"positivitypeople","comment_id":"1102981","content":"Got this question today on the exam","timestamp":"1703193960.0"},{"timestamp":"1694007240.0","comment_id":"1000666","upvote_count":"1","content":"correct \nhttps://learn.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics","poster":"kkk5566"},{"timestamp":"1659373680.0","content":"correct","upvote_count":"1","comment_id":"640768","poster":"Deeksha1234"},{"comments":[{"content":"Both are correct. A Hopping window with hop-size = window-size is identical to a Tumbling window.","poster":"kmrrch","upvote_count":"5","timestamp":"1665519420.0","comment_id":"692386"}],"content":"this question appears at topic 2 question 18 and it said the correct answer was hopping window with 10'' window... so, what's the right correct answer?","timestamp":"1657472580.0","upvote_count":"2","poster":"praticewizards","comment_id":"629648"},{"content":"Selected Answer: A\ncorrect","poster":"sarapaisley","timestamp":"1649404380.0","upvote_count":"2","comment_id":"582753"},{"timestamp":"1645548420.0","content":"correct \"D cholo","poster":"agar","upvote_count":"1","comment_id":"553834"},{"content":"quite trivial, yes - correct answer: https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics#:~:text=Tumbling%20windows%20are%20a%20series,into%2010%2Dsecond%20tumbling%20windows.","comment_id":"533511","upvote_count":"1","timestamp":"1643258700.0","poster":"anto69"},{"upvote_count":"1","poster":"Teraflow","timestamp":"1641397560.0","content":"A is correct","comment_id":"517637"},{"comment_id":"517290","timestamp":"1641373080.0","poster":"lukeonline","content":"Selected Answer: A\ncorrect","upvote_count":"1"},{"comment_id":"506613","content":"Selected Answer: A\nA is Correct Answer","poster":"Canary_2021","timestamp":"1640140920.0","upvote_count":"1"},{"poster":"rashjan","timestamp":"1638887700.0","comment_id":"496096","content":"Selected Answer: A\ncorrect","upvote_count":"1"},{"poster":"paoloscott","upvote_count":"1","comment_id":"478565","content":"Correct answer !","timestamp":"1636967760.0"},{"content":"correct","upvote_count":"1","poster":"AnandEMani","comment_id":"459588","timestamp":"1633773600.0"},{"timestamp":"1633077300.0","poster":"hugoborda","comment_id":"455413","content":"Answer is correct","upvote_count":"1"},{"poster":"damaldon","content":"Fully agree","comment_id":"383654","timestamp":"1623871200.0","upvote_count":"2"}],"choices":{"B":"No","A":"Yes"},"question_id":190,"answer":"A","topic":"2","timestamp":"2021-06-07 13:50:00","answer_description":"","answers_community":["A (100%)"],"unix_timestamp":1623066600,"answer_ET":"A","isMC":true}],"exam":{"isImplemented":true,"isMCOnly":false,"lastUpdated":"12 Apr 2025","id":67,"provider":"Microsoft","isBeta":false,"numberOfQuestions":384,"name":"DP-203"},"currentPage":38},"__N_SSP":true}