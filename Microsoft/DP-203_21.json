{"pageProps":{"questions":[{"id":"HihmP7RX2YWHF3OL1XQ3","question_id":101,"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/108552-exam-dp-203-topic-1-question-84-discussion/","choices":{"B":"ORC","D":"HIVE","A":"Parquet","C":"JSON"},"timestamp":"2023-05-05 18:10:00","answer":"A","exam_id":67,"answer_ET":"A","question_text":"You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1.\n\nYou plan to create a database named DB1 in Pool1.\n\nYou need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool.\n\nWhich format should you use for the tables in DB1?","answers_community":["A (100%)"],"discussion":[{"content":"always pick parquet first","poster":"kam1122","timestamp":"1702085340.0","upvote_count":"5","comment_id":"1091393"},{"timestamp":"1732909740.0","poster":"EmnCours","upvote_count":"1","comment_id":"1319893","content":"Selected Answer: A\nSelected Answer: A"},{"content":"Selected Answer: A\nis correct","comment_id":"998402","poster":"kkk5566","timestamp":"1693818600.0","upvote_count":"1"},{"content":"Selected Answer: A\nparquet. CSV , delta also possible but not an option here.","upvote_count":"2","poster":"Deeksha1234","timestamp":"1692138180.0","comment_id":"982034"},{"timestamp":"1691404140.0","comment_id":"974585","poster":"akhil5432","upvote_count":"1","content":"Selected Answer: A\nparquet"},{"comment_id":"907926","poster":"Ankit_Az","upvote_count":"1","content":"Selected Answer: A\nCorrect","timestamp":"1685186400.0"},{"content":"Selected Answer: A\nParquet is the correct answer","upvote_count":"1","timestamp":"1684275720.0","poster":"rocky48","comment_id":"899598"},{"comment_id":"894212","poster":"aemilka","timestamp":"1683738720.0","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\n\nSupported formats for serverless pool: Delimited/CSV, Parquet, Delta Lake \nSo Parquet is the correct answer","upvote_count":"2"},{"upvote_count":"2","comment_id":"890192","poster":"henryphchan","content":"Selected Answer: A\nParquet is supported by serverless SQL pool\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files","timestamp":"1683306600.0"},{"timestamp":"1683303060.0","poster":"makkelijkzat","comment_id":"890149","upvote_count":"1","content":"Selected Answer: A\ncorrect"},{"upvote_count":"1","timestamp":"1683303000.0","poster":"makkelijkzat","comment_id":"890148","content":"correct"}],"unix_timestamp":1683303000,"question_images":[],"topic":"1","answer_description":"","answer_images":[]},{"id":"lRAEkj3L9ZkAueZIN4qY","answers_community":["AD (73%)","BE (20%)","7%"],"exam_id":67,"timestamp":"2023-06-08 14:37:00","url":"https://www.examtopics.com/discussions/microsoft/view/111548-exam-dp-203-topic-1-question-85-discussion/","discussion":[{"poster":"orionduo","upvote_count":"11","timestamp":"1687314240.0","content":"Correct.\nQuery acceleration supports CSV and JSON formatted data as input to each request.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration","comment_id":"928972"},{"content":"Selected Answer: AD\nQuery acceleration supports CSV and JSON formatted data as input to each request.","comment_id":"920842","poster":"vctrhugo","upvote_count":"6","timestamp":"1686501120.0"},{"content":"Selected Answer: BE\nB & E options","timestamp":"1742479920.0","comment_id":"1401075","upvote_count":"1","poster":"sachin_mt"},{"timestamp":"1742017020.0","content":"Selected Answer: AD\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration","upvote_count":"1","poster":"Moli62","comment_id":"1397184"},{"upvote_count":"1","content":"Selected Answer: BE\nParquet is a columnar format, allowing efficient data retrieval and compression.\nAvro is optimized for efficient binary serialization, making it suitable for query acceleration.","poster":"jxs221","timestamp":"1741304520.0","comment_id":"1366076"},{"timestamp":"1739971080.0","content":"Selected Answer: D\nis this related to DP-300 or not","comment_id":"1358757","poster":"angelvenkovicch","upvote_count":"1"},{"poster":"samirarian","comment_id":"1354896","upvote_count":"1","timestamp":"1739261880.0","content":"Selected Answer: BE\nThese file formats are optimized for analytics and support features like columnar storage, which is beneficial for query acceleration, making them more efficient for querying in big data analytics workloads.\n\nParquet: A columnar storage format that is widely used for analytics, providing fast query performance.\nAvro: A row-based storage format that also supports efficient querying, particularly with its schema support and compact data format.\nSo, Parquet and Avro are the correct answers."},{"poster":"EmnCours","timestamp":"1732909920.0","content":"Selected Answer: AD\nCorrect Answer: AD","comment_id":"1319895","upvote_count":"1"},{"content":"Selected Answer: AD\nA and D","poster":"MohamedBI12","upvote_count":"1","comment_id":"1162044","timestamp":"1709162820.0"},{"poster":"moize","upvote_count":"3","comment_id":"1115391","content":"https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration","timestamp":"1704568620.0"},{"upvote_count":"1","timestamp":"1695379080.0","poster":"OldSchool","comment_id":"1013975","content":"Selected Answer: AD\nQuery acceleration supports CSV and JSON formatted data as input to each request."},{"comment_id":"998403","timestamp":"1693818600.0","content":"Selected Answer: AD\nCSV and JSON","upvote_count":"1","poster":"kkk5566"},{"poster":"aga444","comment_id":"921250","upvote_count":"1","content":"Parquet and CSV","timestamp":"1686558420.0"},{"timestamp":"1686227820.0","poster":"IanKwok81","upvote_count":"4","comment_id":"918252","content":"Correct. https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration"}],"unix_timestamp":1686227820,"question_text":"You have an Azure Data Lake Storage Gen2 account named storage1.\n\nYou plan to implement query acceleration for storage1.\n\nWhich two file types support query acceleration? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.","answer_images":[],"isMC":true,"answer_description":"","question_id":102,"answer":"AD","topic":"1","answer_ET":"AD","choices":{"B":"Apache Parquet","C":"XML","D":"CSV","E":"Avro","A":"JSON"},"question_images":[]},{"id":"BJLxixO7W9JRvbcfFjQV","choices":{"D":"CSV","C":"PARQUET","A":"JSON","B":"DELTA"},"answer_description":"","exam_id":67,"isMC":true,"answer_images":[],"question_id":103,"answer":"D","timestamp":"2023-06-08 14:55:00","answers_community":["D (90%)","10%"],"topic":"1","question_text":"You have an Azure subscription that contains the resources shown in the following table.\n\n//IMG//\n\n\nYou need to read the files in storage1 by using ad-hoc queries and the OPENROWSET function. The solution must ensure that each rowset contains a single JSON record.\n\nTo what should you set the FORMAT option of the OPENROWSET function?","discussion":[{"poster":"Zak_Zakaria","comment_id":"956917","comments":[{"upvote_count":"3","comment_id":"1302660","timestamp":"1729806000.0","content":"To return product data from a folder containing multiple JSON files in this format, you could use the following SQL query:\n\nSQL\n\nCopy\nSELECT doc\nFROM\n OPENROWSET(\n BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n FORMAT = 'csv',\n FIELDTERMINATOR ='0x0b',\n FIELDQUOTE = '0x0b',\n ROWTERMINATOR = '0x0b'\n ) WITH (doc NVARCHAR(MAX)) as rows\nOPENROWSET has no specific format for JSON files, so you must use csv format with FIELDTERMINATOR, FIELDQUOTE, and ROWTERMINATOR set to 0x0b, and a schema that includes a single NVARCHAR(MAX) column. The result of this query is a rowset containing a single column of JSON documents, like this:","poster":"humma"}],"timestamp":"1689793080.0","content":"Selected Answer: D\nIt should be D normally. \nI'll appreciate it if the one who curates the right answer and when the majority doesn't agree with his choice, brings some explanation so we can discuss and understand why he chooses it. \nIt's not the first time I see a total disagreement without any explanation from the ones who select the right answers.","upvote_count":"13"},{"comment_id":"1058881","poster":"phydev","content":"Selected Answer: D\nWas on my exam today (31.10.2023).","timestamp":"1698761820.0","upvote_count":"12"},{"poster":"JT16","comment_id":"1408066","upvote_count":"1","content":"Selected Answer: A\nSince the files in storage1 are publicly accessible JSON files, setting the FORMAT option to JSON allows Azure Synapse Analytics to correctly parse and process each file as individual JSON records.\n\nThis ensures that each rowset corresponds to a single JSON record, aligning with the goal of ad-hoc queries via OPENROWSET.","timestamp":"1742772180.0"},{"upvote_count":"1","timestamp":"1742018400.0","comment_id":"1397488","content":"Selected Answer: D\nhttps://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files","poster":"Moli62"},{"timestamp":"1739263260.0","poster":"samirarian","upvote_count":"1","comment_id":"1354910","content":"Selected Answer: A\nSELECT *\nFROM OPENROWSET(\n BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n FORMAT = 'JSON'\n) AS doc"},{"comments":[{"timestamp":"1736138820.0","content":"This is wrong, just try it using serverless sql pool and it gives error: Invalid or unknown format type 'JSON'.","poster":"hypersam","comment_id":"1336962","upvote_count":"2"}],"timestamp":"1733410920.0","comment_id":"1322402","poster":"moize","upvote_count":"2","content":"Selected Answer: A\nPour garantir que chaque ensemble de lignes contient un seul enregistrement JSON lorsque vous utilisez la fonction OPENROWSET pour lire des fichiers dans Azure Data Lake Storage Gen2, vous devez définir l'option FORMAT sur A. JSON.\n\nL'option D. CSV ne serait pas correcte dans ce contexte, car elle est utilisée pour lire des fichiers CSV, et non des fichiers JSON. Utiliser FORMAT = 'JSON' permet de s'assurer que chaque ligne de la sortie correspond à un enregistrement JSON unique.\nVoici un exemple de requête :\nSELECT *\nFROM OPENROWSET(\n BULK 'https://storage1.dfs.core.windows.net/container1/folder1/*.json',\n FORMAT = 'JSON'\n) AS rows;"},{"timestamp":"1732910100.0","upvote_count":"1","comment_id":"1319898","content":"Selected Answer: D\nCorrect Answer: D","poster":"EmnCours"},{"comment_id":"1278927","upvote_count":"2","poster":"ahana1074","timestamp":"1725540060.0","comments":[{"timestamp":"1729806120.0","poster":"humma","content":"OPENROWSET has no specific format for JSON files, so you must use csv format with FIELDTERMINATOR, FIELDQUOTE, and ROWTERMINATOR set to 0x0b, and a schema that includes a single NVARCHAR(MAX) column. The result of this query is a rowset containing a single column of JSON documents, like this:\nhttps://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files","upvote_count":"2","comment_id":"1302661"}],"content":"This allows the SQL pool to treat each JSON object as a row and parse the file accordingly.\n\nThe correct answer is:\nA. JSON"},{"timestamp":"1723407000.0","content":"Selected Answer: A\nA \n\nOPENROWSET support JSON format in Synapse. In traditional SQL Server on-prem JSON format wasn't supported so we used CSV method. Synapse support JSON OPENROWSET, json object is treated as separate row, no need for CSV workaround, it would complicate the process requiring additional parsing record","upvote_count":"2","poster":"AAhmadH","comment_id":"1264276"},{"timestamp":"1722190200.0","content":"A.\nOPENROWSET(BULK) is a table-valued function that can read data from any file on the local drive or network, if SQL Server has read access to that location. It returns a table with a single column that contains the contents of the file.\nEg..\nSELECT BulkColumn\nFROM OPENROWSET(BULK 'C:\\JSON\\Books\\book.json', SINGLE_CLOB) as j;","upvote_count":"1","poster":"vik_sahani","comment_id":"1256967"},{"comment_id":"1235676","upvote_count":"1","poster":"Prt1nov89","timestamp":"1719109860.0","content":"You have three choices for input files that contain the target data for querying. Valid values are:\n\n 'CSV' - Includes any delimited text file with row/column separators. Any character can be used as a field separator, such as TSV: FIELDTERMINATOR = tab.\n\n 'PARQUET' - Binary file in Parquet format\n\n 'DELTA' - A set of Parquet files organized in Delta Lake (preview) format\n\nValues with blank spaces are not valid, e.g. 'CSV ' is not a valid value.\n\nSo, its D"},{"poster":"ageorgieva","content":"Selected Answer: D\nGemini says it is a CSV:\nBased on the information provided in the image, the format option of the OPENROWSET function should be set to D. CSV.\n\nHere's why:\n\nThe data files in Azure Blob storage are JSON files.\nHowever, SQL Server doesn't natively support the JSON format for the OPENROWSET function.\nAs a workaround, you can specify the format option as CSV and configure the field terminator and fieldquote options appropriately to process each line of the JSON file as a single record.\nEven though the data is in JSON format, choosing CSV as the format option allows you to read the data into a SQL table using OPENROWSET.","upvote_count":"5","comment_id":"1234527","timestamp":"1718982360.0"},{"poster":"qadeeros","timestamp":"1716178920.0","comment_id":"1214114","upvote_count":"3","content":"chatgpt says option A"},{"content":"The easiest way to see to the content of your JSON file is to provide the file URL to the OPENROWSET function, specify csv FORMAT, and set values 0x0b for fieldterminator and fieldquote. If you need to read line-delimited JSON files, then this is enough.","poster":"Elanche","timestamp":"1711971420.0","upvote_count":"1","comment_id":"1187380"},{"poster":"Elanche","comment_id":"1187379","content":"select top 10 *\nfrom openrowset(\n bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.jsonl',\n format = 'csv',\n fieldterminator ='0x0b',\n fieldquote = '0x0b'\n ) with (doc nvarchar(max)) as rows\ngo\nselect top 10 *\nfrom openrowset(\n bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.json',\n format = 'csv',\n fieldterminator ='0x0b',\n fieldquote = '0x0b',\n rowterminator = '0x0b' --> You need to override rowterminator to read classic JSON\n ) with (doc nvarchar(max)) as rows","upvote_count":"1","timestamp":"1711971240.0"},{"content":"Selected Answer: D\nOPENROWSET doesn't have a JSON option","timestamp":"1709233320.0","poster":"s_unsworth","upvote_count":"1","comment_id":"1162942"},{"comment_id":"1142278","poster":"falzar","content":"I checked this on another dump site and chatgpt. Both said the answer is A.","timestamp":"1707231600.0","upvote_count":"2"},{"comment_id":"1117735","timestamp":"1704822960.0","content":"Bonne réponse est D. Se référer au lien : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-files","poster":"moize","upvote_count":"2"},{"content":"Selected Answer: D\nPlease refer: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-documents","timestamp":"1695574320.0","comment_id":"1016039","poster":"MJamesP","upvote_count":"4"},{"content":"Selected Answer: D\nno json format, using CSV","comment_id":"998405","timestamp":"1693818720.0","poster":"kkk5566","upvote_count":"7"},{"content":"Selected Answer: D\nIgnore my previous comment.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-files","upvote_count":"4","comment_id":"992411","timestamp":"1693241520.0","poster":"susbhat"},{"content":"CSV -> D is the correct answer","upvote_count":"2","comment_id":"990184","timestamp":"1692978360.0","poster":"kdp203"},{"timestamp":"1692355140.0","poster":"lfss","upvote_count":"1","content":"D is the correct","comment_id":"984386"},{"poster":"mmoayed","upvote_count":"1","comment_id":"983516","timestamp":"1692269340.0","content":"If most of all answered D, but the system says A, Who should I take then ?","comments":[{"timestamp":"1693241460.0","comment_id":"992409","poster":"susbhat","upvote_count":"4","content":"There is no filetype as JSON. We have to use CSV and then query:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-files"}]},{"poster":"Deeksha1234","upvote_count":"1","comment_id":"982037","timestamp":"1692138360.0","content":"Selected Answer: D\nD is correct"},{"content":"Selected Answer: D\nExactly! OPENROWSET has no JSON files, so use FORMAT = 'csv' for the querying.","timestamp":"1690220640.0","comment_id":"961888","poster":"Andrew_Chen","upvote_count":"1"},{"upvote_count":"1","poster":"auwia","timestamp":"1687939680.0","comment_id":"936311","content":"Selected Answer: D\nIt should be D."},{"poster":"andjurovicela","timestamp":"1686817800.0","upvote_count":"4","comment_id":"923883","content":"Selected Answer: D\nD is the correct answer, indeed: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files"},{"comment_id":"921432","timestamp":"1686571080.0","poster":"T4321","upvote_count":"2","content":"Selected Answer: D\nCorrect answer is 'D'"},{"upvote_count":"2","timestamp":"1686501240.0","comment_id":"920845","poster":"vctrhugo","content":"Selected Answer: D\nThe easiest way to see to the content of your JSON file is to specify csv FORMAT."},{"upvote_count":"1","content":"Selected Answer: D\nNo Format for JSON to achieve this CSV is the right answer","comment_id":"919266","timestamp":"1686311640.0","poster":"abdallaissa"},{"timestamp":"1686311520.0","poster":"abdallaissa","upvote_count":"1","comment_id":"919263","content":"To Read Json the format should be CSV"},{"timestamp":"1686296940.0","comment_id":"919060","poster":"sammieyuen","content":"Should it be format= 'csv'","upvote_count":"1"},{"timestamp":"1686228900.0","comment_id":"918281","poster":"IanKwok81","upvote_count":"1","content":"correct"}],"question_images":["https://img.examtopics.com/dp-203/image308.png"],"unix_timestamp":1686228900,"url":"https://www.examtopics.com/discussions/microsoft/view/111553-exam-dp-203-topic-1-question-86-discussion/","answer_ET":"D"},{"id":"5Mor92QdhvLirAGaN6uf","question_id":104,"answer_description":"","question_images":["https://img.examtopics.com/dp-203/image309.png","https://img.examtopics.com/dp-203/image310.png"],"discussion":[{"timestamp":"1687950420.0","content":"Provided answers are correct:\n1. Yes:\nAzure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog. When customers want to persist the Hive catalog metadata outside of the workspace, and share catalog objects with other computational engines outside of the workspace, such as HDInsight and Azure Databricks, they can connect to an external Hive Metastore. Only Azure SQL Database and Azure Database for MySQL are supported as an external Hive Metastore. \n\n2. Yes:\nAnd currently we only support User-Password authentication. \n\n3. No:\n And currently we only support User-Password authentication. ==> STORAGE BLOB CONTRIBUTOR is an Azure RBAC (Role-Based Access Control) ==> NOT COMPATIBLE (it is supported User-Password authentication ONLY).\n\nref.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore","poster":"auwia","comment_id":"936533","upvote_count":"18"},{"poster":"aga444","timestamp":"1686559320.0","comment_id":"921260","upvote_count":"12","content":"No-Yes-Yes","comments":[{"comments":[{"timestamp":"1687948980.0","content":"Sorry I was wrong.","poster":"auwia","comment_id":"936501","upvote_count":"10"}],"upvote_count":"3","poster":"auwia","content":"I confirm the first No:\nThe first statement, \"The shared catalog objects can be stored in Azure Database for MySQL,\" is not true because Azure Database for MySQL is not the appropriate storage option for shared catalog objects in Azure Synapse Analytics. The shared catalog objects, which include metadata and schema information, are typically stored in a centralized metadata store such as the Apache Hive Metastore. Azure Synapse Analytics supports using an Azure SQL Database or an Azure SQL Data Warehouse (now called Azure Synapse SQL) as the metadata store, but Azure Database for MySQL is not a supported option for this purpose.","timestamp":"1687351440.0","comment_id":"929514"}]},{"comment_id":"1308993","timestamp":"1731127740.0","content":"3.No\nIf the underlying data of your Hive tables is stored in the workspace primary storage account, you don't need to do extra settings. It will just work as long as you followed storage setting up instructions during workspace creation.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore#workspace-primary-storage-account","poster":"seranvijay","upvote_count":"1"},{"upvote_count":"2","poster":"rocky48","content":"Yes, Yes , No\n1. Yes:\nAzure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog. When customers want to persist the Hive catalog metadata outside of the workspace, and share catalog objects with other computational engines outside of the workspace, such as HDInsight and Azure Databricks, they can connect to an external Hive Metastore. Only Azure SQL Database and Azure Database for MySQL are supported as an external Hive Metastore.\n\n2. Yes:\nAnd currently we only support User-Password authentication.\n\n3. No:\nAnd currently we only support User-Password authentication. ==> STORAGE BLOB CONTRIBUTOR is an Azure RBAC (Role-Based Access Control) ==> NOT COMPATIBLE (it is supported User-Password authentication ONLY).\n\nref.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore","timestamp":"1707801120.0","comment_id":"1148891"},{"content":"I got this question yesterday on my exam","comment_id":"1144568","timestamp":"1707403680.0","upvote_count":"3","poster":"wel_fardeheb"},{"upvote_count":"1","content":"Looks like the Storage Blob Data Contributor role does not need to be assigned.\n\nWorkspace primary storage account\nIf the underlying data of your Hive tables is stored in the workspace primary storage account, you don't need to do extra settings. It will just work as long as you followed storage setting up instructions during workspace creation.\n\nOther ADLS Gen 2 account\nIf the underlying data of your Hive catalogs is stored in another ADLS Gen 2 account, you need to make sure the users who run Spark queries have Storage Blob Data Contributor role on the ADLS Gen2 storage account.","poster":"Bill_Walker","comment_id":"1137795","timestamp":"1706808840.0"},{"poster":"ELJORDAN23","content":"Got this question on my exam on january 17, answers are correct.","timestamp":"1705595400.0","comment_id":"1126050","upvote_count":"5"},{"upvote_count":"4","comment_id":"1101848","poster":"ExamDestroyer69","content":"Confusing discussion section, no consensus","timestamp":"1703098320.0"},{"poster":"kkk5566","comment_id":"998407","timestamp":"1693818840.0","comments":[{"timestamp":"1698891600.0","upvote_count":"1","content":"Shouldn't WS1 have blob data contributor access?","comment_id":"1060198","poster":"ahmadsayeed"}],"upvote_count":"1","content":"correct"},{"comment_id":"982039","content":"given answer is correct \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore","upvote_count":"2","timestamp":"1692138780.0","poster":"Deeksha1234"},{"poster":"[Removed]","timestamp":"1691714400.0","upvote_count":"1","comment_id":"978160","content":"Yes , yes , no https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"},{"content":"Correct order should be Yes, No, Yes","comment_id":"944139","upvote_count":"2","timestamp":"1688593560.0","poster":"pavankr"},{"timestamp":"1688310780.0","poster":"DataSaM","comment_id":"941003","upvote_count":"1","content":"https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"},{"comment_id":"939033","upvote_count":"1","comments":[{"timestamp":"1691741580.0","upvote_count":"2","poster":"[Removed]","comment_id":"978446","content":"Yes, Yes , No you can check the document link"}],"content":"What are the correct answers???","poster":"Albeeliu","timestamp":"1688123280.0"},{"comment_id":"931001","upvote_count":"6","poster":"Paulkuzzio","timestamp":"1687465200.0","content":"Only Azure SQL Database and Azure Database for MySQL are supported as an external Hive Metastore. And currently we only support User-Password authentication. https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"},{"poster":"sridat","content":"A ''Blob Data Contributor\" role must be assigned to user in order to access the files in blob storage. So it's a \"Yes\"\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/assign-azure-role-data-access?tabs=portal","comment_id":"920412","upvote_count":"6","timestamp":"1686458700.0"}],"timestamp":"2023-06-11 06:45:00","exam_id":67,"answer":"","topic":"1","answer_images":["https://img.examtopics.com/dp-203/image311.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/111867-exam-dp-203-topic-1-question-87-discussion/","question_text":"HOTSPOT\n-\n\nYou have an Azure subscription that contains the Azure Synapse Analytics workspaces shown in the following table.\n\n//IMG//\n\n\nEach workspace must read and write data to datalake1.\n\nEach workspace contains an unused Apache Spark pool.\n\nYou plan to configure each Spark pool to share catalog objects that reference datalake1.\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","unix_timestamp":1686458700,"answer_ET":"","isMC":false,"answers_community":[]},{"id":"aCzHXKb54hwyJ3M9ryVn","url":"https://www.examtopics.com/discussions/microsoft/view/111554-exam-dp-203-topic-1-question-88-discussion/","unix_timestamp":1686229500,"question_id":105,"isMC":false,"answers_community":[],"question_text":"DRAG DROP\n-\n\nYou have a data warehouse.\n\nYou need to implement a slowly changing dimension (SCD) named Product that will include three columns named ProductName, ProductColor, and ProductSize. The solution must meet the following requirements:\n\n• Prevent changes to the values stored in ProductName.\n• Retain only the current and the last values in ProductSize.\n• Retain all the current and previous values in ProductColor.\n\nWhich type of SCD should you implement for each column? To answer, drag the appropriate types to the correct columns. Each type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","timestamp":"2023-06-08 15:05:00","answer":"","answer_images":["https://img.examtopics.com/dp-203/image313.png"],"topic":"1","answer_ET":"","question_images":["https://img.examtopics.com/dp-203/image312.png"],"exam_id":67,"answer_description":"","discussion":[{"comment_id":"920372","content":"Product name -type 0\ncolor -type 2\nsize -type 3","timestamp":"1686451920.0","poster":"Ram0202","upvote_count":"116"},{"upvote_count":"41","content":"ProductName - type 0, as no changes are done. Color - type 3, as with type 3 we have one column for the current value and one for the previous so only these two are preserved. Size - type 2, as it inserts a new row for every change, so we get all historical values.","timestamp":"1686818400.0","poster":"andjurovicela","comments":[{"upvote_count":"21","poster":"hiyoww","timestamp":"1689686880.0","content":"Agree. beware that the order of ProductSize, ProductColor in the question, not same as in the graph.\nProduct name -type 0\ncolor -type 3\nsize -type 2","comment_id":"955480"}],"comment_id":"923897"},{"timestamp":"1739274720.0","upvote_count":"2","content":"Column Requirement SCD Type\nProductName Prevent changes (No updates allowed) SCD Type 0\nProductSize Retain only the current and last value SCD Type 3\nProductColor Retain all historical values SCD Type 2","comment_id":"1354977","poster":"samirarian"},{"timestamp":"1731957240.0","poster":"Piantoni","content":"ProductName: Type 0\nColor: Type 2\nSize: Type3","comment_id":"1314169","upvote_count":"1"},{"upvote_count":"2","comment_id":"1149028","timestamp":"1707815820.0","comments":[{"upvote_count":"5","timestamp":"1707816480.0","content":"I made a typo. \nMy answer should be as follow\nProductName: 0\nColor: 2\nSize: 3","poster":"j888","comment_id":"1149035"}],"poster":"j888","content":"Type 1: Overwrite the existing data with new data, without maintaining a history. This is suitable when you want to prevent changes to the values stored in a column.\n\nType 2: Keep a history of changes by adding new rows for each change. This is suitable for retaining all the current and previous values in a column.\n\nType 3: Keep a limited history, typically by adding new columns to the dimension table to store both the current and previous values. This allows you to track changes but in a more compact way compared to Type 2.\nTherefore I believe the answer is 0 3 2"},{"upvote_count":"5","poster":"rocky48","timestamp":"1707801360.0","content":"ProductName: To prevent changes to the values stored in ProductName, we should use Type 0. This means that the value remains static and does not change over time.\nProductColor: Since we need to retain all the current and previous values in ProductColor, we should use Type 2. This type allows us to track historical changes by creating new rows for each change and maintaining a history of values.\nProductSize: To retain only the current and last values in ProductSize, we should use Type 3. This type keeps track of the current value and the previous value in separate columns.\nTherefore, the correct SCD types for the three columns are as follows:\n\nProductName: Type 0\nProductColor: Type 2\nProductSize: Type 3","comment_id":"1148893"},{"content":"Product : Type 0 :No change (This eliminates possibilities of Type 1,2 and other higher dimensions \nColour : Type 1 or 2 ;There is a change which can be rowise(type 1) or column wise(Type 2)\nSize :Type 3 ; All changes means both rowise and column wise","comment_id":"1138671","poster":"moneytime","upvote_count":"1","timestamp":"1706887680.0"},{"content":"For the given requirements:\n\n- **Product Name**: Since changes must be prevented, this would be a Type 0 SCD, as it maintains the original value without any changes.\n- **Product Size**: To retain only the current and the last values, you would use a Type 3 SCD, which keeps the original value and adds a new column for the current value.\n- **Product Color**: To retain all the current and previous values, a Type 2 SCD is used, as it tracks historical data by creating a new record for each change.\n\nSo, you would apply:\n\n- Type 0 for ProductName\n- Type 3 for ProductSize\n- Type 2 for ProductColor","comment_id":"1089781","poster":"Momoanwar","upvote_count":"6","timestamp":"1701897120.0"},{"upvote_count":"5","comments":[{"timestamp":"1696435380.0","content":"I meant Name: 0\nColor: 3\nSize: 2","comment_id":"1024970","poster":"Vanq69","upvote_count":"5"}],"content":"Name: 0 Fixed Dimension\nColor: 2 Row Versioning (current + last value)\nSize: 3 Previous Value column (current value + all previous values in extra column)","poster":"Vanq69","comment_id":"1024969","timestamp":"1696435320.0"},{"comment_id":"1003936","upvote_count":"3","poster":"kkk5566","timestamp":"1694345220.0","content":"SCD0-1-2"},{"upvote_count":"2","timestamp":"1694072280.0","poster":"hassexat","comment_id":"1001300","content":"Product Name - Type 0\nColor - Type 2\nSize - Type 3"},{"comments":[{"comment_id":"1000287","content":"ProductName - 0\nProductColor - 3\nProductSize - 2","comments":[{"content":"The requirements and blanks are out of order in the question","comment_id":"1000291","timestamp":"1693982340.0","upvote_count":"2","poster":"AvSUN"},{"comment_id":"1085051","timestamp":"1701417720.0","upvote_count":"4","poster":"Lucasmh","content":"ProductColor cannot be Type3 since it indicates that it has to preserve all current and previous values so Type3\nIt only preserves the current and previous value without maintaining a complete history.\nFor me it is:\nProductName: Type0\nColor: Type2\nSize: Type3"}],"timestamp":"1693982280.0","poster":"AvSUN","upvote_count":"2"}],"timestamp":"1693982220.0","poster":"AvSUN","comment_id":"1000281","content":"The answers are 0, 3, 2","upvote_count":"2"},{"poster":"Deeksha1234","upvote_count":"4","content":"correct answer is - type 0, type 3,type 2","timestamp":"1692139020.0","comment_id":"982042"},{"comment_id":"974590","poster":"akhil5432","upvote_count":"3","content":"type 0\ntype 3\ntype 2","timestamp":"1691404380.0"},{"content":"Answer is \nProduct name -type 0\ncolor -type 2\nsize -type 3\n\nType 0 – Fixed Dimension\nNo changes allowed, dimension never changes\n\nA Type 1 SCD always reflects the latest values, and when changes in source data are detected, the dimension table data is overwritten.\nType 2 SCD\nA Type 2 SCD supports versioning of dimension members. Often the source system doesn't store versions, so the data warehouse load process detects and manages changes in a dimension table. \nType 3 SCD\nA Type 3 SCD supports storing two versions of a dimension member as separate columns. The table includes a column for the current value of a member plus either the original or previous value of the member. So Type 3 uses additional columns to track one key instance of history, rather than storing additional rows to track each change like in a Type 2 SCD.","poster":"ravigolu","upvote_count":"9","timestamp":"1688217840.0","comment_id":"939981"},{"comment_id":"929615","upvote_count":"3","timestamp":"1687357020.0","content":"Answer are corrects as reported in the solution: 0, 1, and 2.","poster":"auwia"},{"comment_id":"929596","comments":[{"poster":"auwia","upvote_count":"2","content":"• Retain only the current and the last values in ProductSize. => TYPE 1 (current and last, DOESN'T means all the history, but as written only the last ... meaning the current).\n\nhttps://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/5-load-slowly-changing-dimensions","comment_id":"929607","timestamp":"1687356840.0"}],"upvote_count":"4","poster":"auwia","timestamp":"1687356120.0","content":"• Prevent changes to the values stored in ProductName. => TYPE 0\n• Retain only the current and the last values in ProductSize. => TYPE 2 (current and last, menas all the history)\n• Retain all the current and previous values in ProductColor. => TYPE 3 (it includes a column for the previous value)"},{"poster":"HimaC5991","content":"my answer is 0,2,3","upvote_count":"4","timestamp":"1687009740.0","comment_id":"925997"},{"comment_id":"920420","timestamp":"1686459600.0","content":"ProductSize is Type 2 since it maintains current and last record. Type 1 can only have current value.","comments":[{"comment_id":"922293","timestamp":"1686665760.0","poster":"wendyy","content":"Retain ONLY the current and the last values in ProductSize. type2 will include all changes. type 3 is correct.","upvote_count":"5"}],"poster":"sridat","upvote_count":"2"},{"content":"Correct answer is type 0, type 2, type 3","poster":"mehroosali","upvote_count":"2","comment_id":"920290","timestamp":"1686435540.0"},{"timestamp":"1686312360.0","upvote_count":"3","poster":"abdallaissa","comment_id":"919277","content":"Correct!"},{"content":"Should be ProductName Type0, ProductColor Type2, ProductSize Type1","comment_id":"918297","upvote_count":"3","timestamp":"1686229500.0","comments":[{"upvote_count":"2","poster":"RoyP654","timestamp":"1686290100.0","comment_id":"918943","content":"ProductSize = Type3? retain only the current and last value on the same row"},{"timestamp":"1686374700.0","upvote_count":"2","content":"ProductSize Type3","poster":"IanKwok81","comment_id":"919810"}],"poster":"IanKwok81"}]}],"exam":{"id":67,"provider":"Microsoft","name":"DP-203","isImplemented":true,"isMCOnly":false,"numberOfQuestions":384,"isBeta":false,"lastUpdated":"12 Apr 2025"},"currentPage":21},"__N_SSP":true}