{"pageProps":{"questions":[{"id":"PARU4hAsqA4FWVtVhm7s","question_id":151,"unix_timestamp":1670872860,"answer_ET":"CD","topic":"1","question_text":"This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nContoso, Ltd. is a company that sells enriched financial data to a variety of external customers.\nContoso has a main office in Los Angeles and two branch offices in New York and Seattle.\n\nExisting Environment -\n\nData Infrastructure -\nContoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.\nThe data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.\nContoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.\nUsers frequently query the Synapse Analytics workspace by using Transact-SQL.\n\nUser Problems -\nContoso identifies the following user issues:\nSome users indicate that the visuals in Power BI reports are slow to render when making filter selections.\nUsers indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.\nUsers indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.\n\nPlanned Changes -\nContoso plans to implement the following changes:\nInto the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.\nBuild a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.\nProvide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.\nImplement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.\nDeploy prebuilt datasets to Power BI to simplify the query experience of the customers.\nProvide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.\nYou need to recommend a solution for the customer workspace to support the planned changes.\nWhich two configurations should you include in the recommendation? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer_images":[],"isMC":true,"timestamp":"2022-12-12 20:21:00","discussion":[{"timestamp":"1670872860.0","poster":"nbagchi","comment_id":"743250","comments":[{"upvote_count":"2","poster":"Cococo","content":"Agree - CD - \"When you create a report in Power BI Desktop, the data in that report is stored in a data model. When you publish a report to the Power BI service, the data model is also published to the service as a dataset at the same time. When you share the report with others, you can give them Build permission for the dataset that the report is built on, so they can discover and reuse it for their own reports, dashboards, etc.\"","timestamp":"1684994460.0","comment_id":"906423"}],"content":"I think correct answer is C,D\nQuestion mentions- 'Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.' Since customers will create their own reports, they will need access to the underlying dataset, so Build permissions will have to be provided.","upvote_count":"12"},{"poster":"dev2dev","content":"Selected Answer: AB\nA and B\nRequirement is to make findata available for everyone.","timestamp":"1682865060.0","upvote_count":"1","comment_id":"885332"},{"upvote_count":"1","comment_id":"871656","poster":"DarioReymago","content":"Selected Answer: CD\ncorrect answer is C,D","timestamp":"1681640880.0"},{"timestamp":"1679932140.0","comments":[{"poster":"Lesucks","upvote_count":"1","comment_id":"884746","content":"https://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-admin-across-workspaces","timestamp":"1682808060.0"}],"upvote_count":"2","content":"I think it is B and D.\nDon't they all need at least a power bi pro licence in order to read the data / and share it?\nIn order to share it with users with free licenses, designers need to publish that content to a group workspace backed by a Premium capacity. Premium capacity provides the benefit of unlimited content sharing.\nIs it really possible to \"set use datasets across workspaces to Enabled\"? Where is that option.","comment_id":"852236","poster":"NickWerbung"},{"timestamp":"1675246380.0","comments":[{"upvote_count":"1","timestamp":"1675246440.0","comment_id":"794989","content":"People do need* build permissions","poster":"MaxExams"}],"comment_id":"794988","poster":"MaxExams","content":"I think the answer is BD\n\n\"You discover that the refresh process of the Power BI model occasionally times out\" \nFor Power BI Pro workspaces, data refreshes must complete in less than 2 hours. On Premium, the limit is 5 hours so B would a solution.\n\nThey are now going to provide customers access to their own workspace (meaning they haven't already) Hence D - \"Use Datasets across workspaces\" needs to be enabled.\n\nPeople do not build permission, but looking at the question, they seem to already have this access so it isn't something that would be included in the solution which is what the question is asking.","upvote_count":"4"},{"poster":"taza31","upvote_count":"3","comment_id":"786392","timestamp":"1674554700.0","content":"correct answer is C,D"},{"poster":"Az301301X","timestamp":"1672991700.0","content":"Selected Answer: CD\nI agree. I would go for C and D.","comment_id":"767392","upvote_count":"2"},{"poster":"Saffar","upvote_count":"3","timestamp":"1672977540.0","comment_id":"767252","content":"Selected Answer: CD\nit should be C,D"},{"comment_id":"767251","timestamp":"1672977480.0","content":"it should be C,D","poster":"Saffar","upvote_count":"2"},{"comment_id":"752684","timestamp":"1671650880.0","poster":"MICC","upvote_count":"4","content":"I believe it should be C,D, \nIn PBI Admin portal, Tenant settings --> Workspace settings --> Use datasets across workspaces\nit requires users have the required Build permission when enabling \"use datasets across workspaces\""}],"answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/91272-exam-dp-500-topic-1-question-7-discussion/","exam_id":70,"choices":{"B":"Configure the FinData workspace to use a Power BI Premium capacity.","D":"Set Use datasets across workspaces to Enabled.","C":"Grant the Build permission for the financial data to each customer.","A":"Publish the financial data to the web."},"answer":"CD","answers_community":["CD (86%)","14%"]},{"id":"lfAMFWkPecCt68ZOHt55","topic":"1","isMC":true,"discussion":[{"poster":"DarioReymago","timestamp":"1680602940.0","content":"Selected Answer: D\nI understand that all query could be sent to source, for this reason it is folded","comment_id":"860869","upvote_count":"1"},{"comment_id":"771055","upvote_count":"3","content":"Answer is correct,\nnon of the steps added to query prevent folding, which means folding should be working fine in all steps\n\nSteps that prevent folding: \nhttps://learn.microsoft.com/en-us/power-query/power-query-folding#transformations-that-prevent-folding","timestamp":"1673324520.0","poster":"Saffar"},{"poster":"AshwinN1992","upvote_count":"2","comment_id":"762120","content":"Please confirm","timestamp":"1672417020.0"}],"question_text":"You are running a diagnostic against a query as shown in the following exhibit.\n//IMG//\n\nWhat can you identify from the diagnostics query?","choices":{"C":"Elevated permissions are being used to query records.","A":"Some query steps are folding.","B":"The query is timing out.","D":"All the query steps are folding."},"question_images":["https://img.examtopics.com/dp-500/image84.png"],"question_id":152,"answer_ET":"D","url":"https://www.examtopics.com/discussions/microsoft/view/93287-exam-dp-500-topic-1-question-70-discussion/","answer_images":[],"answer_description":"","exam_id":70,"answer":"D","answers_community":["D (100%)"],"unix_timestamp":1672417020,"timestamp":"2022-12-30 17:17:00"},{"id":"YQ316t1tQzszDTyAf0J1","discussion":[{"content":"Apache\nDedicated\nServerless","poster":"Eltooth","upvote_count":"3","comment_id":"934645","timestamp":"1687798260.0"},{"timestamp":"1680658140.0","upvote_count":"1","poster":"DarioReymago","comment_id":"861667","content":"I think this is ok, but not sure"},{"timestamp":"1679977080.0","comment_id":"852758","poster":"solref","content":"it ok! :)","upvote_count":"1"},{"upvote_count":"1","timestamp":"1674294960.0","comment_id":"783156","content":"I believe this is correct https://www.royalcyber.com/resources/blogs/dedicated-sql-pool-vs-serverless-sql/","poster":"per_ing"}],"question_text":"DRAG DROP -\nYou are configuring Azure Synapse Analytics pools to support the Azure Active Directory groups shown in the following table.\n//IMG//\n\nWhich type of pool should each group use? To answer, drag the appropriate pool types to the groups. Each pool type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\n//IMG//","answers_community":[],"question_id":153,"answer_images":["https://img.examtopics.com/dp-500/image87.png"],"topic":"1","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/94193-exam-dp-500-topic-1-question-71-discussion/","unix_timestamp":1673021100,"timestamp":"2023-01-06 17:05:00","answer_description":"","exam_id":70,"question_images":["https://img.examtopics.com/dp-500/image85.png","https://img.examtopics.com/dp-500/image86.png"],"isMC":false,"answer":""},{"id":"opLaDd12fY9G0jU0yDRO","answer_images":["https://img.examtopics.com/dp-500/image89.png"],"question_id":154,"discussion":[{"poster":"MaryemSB","timestamp":"1702460460.0","comment_id":"1095334","upvote_count":"1","content":"correct answer"},{"content":"This is correct :)","comment_id":"956243","timestamp":"1689744540.0","poster":"Cata23","upvote_count":"2"},{"timestamp":"1687798380.0","comment_id":"934646","poster":"Eltooth","upvote_count":"3","content":"Create\nApply\nDefine\nPublish"},{"upvote_count":"2","content":"is corrrrrrect","comment_id":"861670","poster":"DarioReymago","timestamp":"1680658440.0"},{"poster":"stfglv","comment_id":"774538","content":"the correct sequence is: 1. define the RangeStart and RangeEnd parameters 2. apply customer filter to the Date column 3. define incremental refresh policy 4. publish the model\n\nsource: https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-configure","timestamp":"1673617440.0","upvote_count":"3"}],"question_text":"DRAG DROP -\nYou have a Power BI dataset. The dataset contains data that is updated frequently.\nYou need to improve the performance of the dataset by using incremental refreshes.\nWhich four actions should you perform in sequence to enable the incremental refreshes? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n//IMG//","unix_timestamp":1673617440,"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/95044-exam-dp-500-topic-1-question-72-discussion/","answer_ET":"","answer":"","exam_id":70,"answers_community":[],"isMC":false,"timestamp":"2023-01-13 14:44:00","question_images":["https://img.examtopics.com/dp-500/image88.png"]},{"id":"HcBU5UQEzyrP2QUz64Ut","discussion":[{"upvote_count":"3","timestamp":"1702891440.0","comment_id":"1099567","content":"Selected Answer: DE\nIt's in the docs: https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models#enable-large-semantic-models\n\n--> Publish the model as a semantic model to the service.\n--> Invoke a refresh to load historical data based on the incremental refresh policy.","poster":"salvalcaraz"},{"comment_id":"861674","upvote_count":"2","poster":"DarioReymago","content":"Selected Answer: BD\nin practice B y D is the best solution","timestamp":"1680659040.0"},{"comments":[{"content":"C is definitely wrong please read below as per the logic provided below comments in documentation negate it, anyhow a smaller dataset could have a larger in memory size and to support mroe than 10 GB one needs to definitely enable Large Dataset storage format which is not there in options.\nTo safeguard the performance of the system, an additional SKU-specific hard ceiling for max offline dataset size is applied, regardless of the configured value. The additional SKU-specific hard ceiling in the below table does not apply to Power BI datasets stored in large dataset storage format.\n\nEM1/A1 EM2/A2 EM3/A3 P1/A4 P2/A5 P3/A6 P4/A7 P5/A8\nHard ceiling for Max Offline Dataset Size 3 GB 5 GB 6 GB 10 GB 10 GB 10 GB 10 GB\n\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-workloads#max-offline-dataset-size","poster":"Qordata","comment_id":"1023635","timestamp":"1696312200.0","upvote_count":"1"}],"comment_id":"853996","timestamp":"1680063540.0","upvote_count":"1","poster":"Az301301X","content":"Selected Answer: CE\nChat GPT:\nThe correct options to ensure successful publishing of a Power BI Premium dataset that is expected to consume 50 GB of memory are:\n\nC. Increase the Max Offline Dataset Size setting: Power BI Premium capacity has a default limit of 10 GB for the maximum offline dataset size that can be published to the service. You can increase this limit to accommodate larger datasets. In this case, you should increase the limit to 50 GB to support the dataset.\n\nE. Publish the complete dataset: To ensure that the complete dataset is published successfully, you should not publish an initial dataset that is less than 10 GB, as suggested in option B. Also, restarting the capacity, as suggested in option A, is not necessary in this scenario. Invoking a refresh to load historical data based on the incremental refresh policy, as suggested in option D, is optional and does not affect the publishing process.\n\nTherefore, the correct actions to perform are to increase the Max Offline Dataset Size setting to 50 GB and publish the complete dataset."},{"content":"Selected Answer: DE\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models","poster":"solref","comment_id":"852786","timestamp":"1679979180.0","upvote_count":"3"},{"timestamp":"1673617920.0","upvote_count":"2","content":"We need to differentiate between the limit for a dataset and limit for a data model that will be uploaded to the service. As this link (https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models) states, for Premium the option to enable large datasets does not affect the PBI Desktop model upload size, which is still limited to 10 GB. \"Instead, datasets can grow beyond that limit in the service on refresh.\" . In conclusion, until you upload it it has to be smaller than 10GB and afterwards it can grow beyond that. Incremental refresh is a must obviously.","comments":[{"poster":"ThariCD","comments":[{"content":"Complete dataset is of 50 GB size so you could publish the complete data set hence E is wrong.\nFirst you publish the dataset where you had configured the incremental refresh in powerbi desktop file and it's size is less than 10 GB which makes option B correct\nThan you do what is mentioned in option D which would make the dataset to consume 50GB of memory maybe but for that you would have to enable large dataset storage format keep that in mind","upvote_count":"1","timestamp":"1696312500.0","comment_id":"1023640","poster":"Qordata"}],"timestamp":"1676018940.0","content":"The link you mention also includes the steps to enable a large dataset for a new model published to the service, which are: \n1. Create a model in Power BI Desktop. If your dataset will become larger and progressively consume more memory, be sure to configure Incremental Refresh\n2. Publish the model as a dataset to the service \n3. In the service > dataset > Settings, expand Large dataset storage format, set the slider to On and select Apply\n\nDoesn't that mean that D & E are the correct answers? The incremental refresh is obvious from step 1 but doesn't step 2 mean you should publish the complete dataset? Nowhere in those steps does it say you need to first publish 10 GB of your dataset.","upvote_count":"2","comment_id":"804139"}],"poster":"stfglv","comment_id":"774545"},{"upvote_count":"4","poster":"Saffar","timestamp":"1673325060.0","content":"Selected Answer: BD\nI think BD is correct.","comment_id":"771061"},{"upvote_count":"4","timestamp":"1672462200.0","content":"Selected Answer: BD\nBD is correct.","poster":"cherious","comment_id":"762488"},{"upvote_count":"2","content":"I think D & E are the correct answers. \nSee the section \"Enable large Datasets\" at https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models","timestamp":"1672314120.0","poster":"Maazi","comment_id":"760950"},{"content":"Selected Answer: BD\nD + B is correct","timestamp":"1672064760.0","comment_id":"757540","poster":"jeroen12345","upvote_count":"3"}],"timestamp":"2022-12-26 15:26:00","answer_description":"","question_images":[],"answer":"BD","choices":{"D":"Invoke a refresh to load historical data based on the incremental refresh policy.","C":"Increase the Max Offline Dataset Size setting.","A":"Restart the capacity.","B":"Publish an initial dataset that is less than 10 GB.","E":"Publish the complete dataset."},"topic":"1","unix_timestamp":1672064760,"url":"https://www.examtopics.com/discussions/microsoft/view/92879-exam-dp-500-topic-1-question-73-discussion/","answer_ET":"BD","question_text":"You develop a solution that uses a Power BI Premium capacity. The capacity contains a dataset that is expected to consume 50 GB of memory.\nWhich two actions should you perform to ensure that you can publish the model successfully to the Power BI service? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer_images":[],"answers_community":["BD (65%)","DE (30%)","5%"],"exam_id":70,"question_id":155,"isMC":true}],"exam":{"provider":"Microsoft","name":"DP-500","id":70,"isImplemented":true,"numberOfQuestions":183,"isMCOnly":false,"isBeta":false,"lastUpdated":"12 Apr 2025"},"currentPage":31},"__N_SSP":true}