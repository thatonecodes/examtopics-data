{"pageProps":{"questions":[{"id":"DNN1UnHWjcTE7zkoFm8Q","choices":{"A":"Yes","B":"No"},"unix_timestamp":1650813120,"question_images":[],"discussion":[{"timestamp":"1666624320.0","upvote_count":"13","comment_id":"591132","content":"Selected Answer: A\nThe correct answer is \"A. Yes\"\nYou can execute R code in a notebook, and then call it from Data Factory.\nYou can check it at \"Databricks Notebook activity\" header:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data\nAnd also:\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/sparkr/overview","comments":[{"upvote_count":"12","timestamp":"1666624740.0","poster":"juanlu46","content":"I'm Sorry, in the statement there isn't mention to \"Data factory\", but you can use a Databrick's job also, therefore the solution meet the goal.\nhttps://docs.microsoft.com/en-us/azure/databricks/jobs#--run-a-job","comment_id":"591133"},{"poster":"bp_a_user","comment_id":"885319","upvote_count":"2","timestamp":"1698682860.0","content":"...but where is the ingest done?"}],"poster":"juanlu46"},{"upvote_count":"2","timestamp":"1730530200.0","comment_id":"1205299","content":"well it is a trick question and I would hate to get it. \nIt says execute DBr job which can be only executed from ADF","poster":"Dusica"},{"comment_id":"1118414","upvote_count":"1","content":"Selected Answer: A\nA. Yes\n\nExplanation:\n\nScheduling an Azure Databricks job that executes an R notebook and then inserts the data into the data warehouse is a valid solution that meets the goal. Azure Databricks is a cloud-based platform that integrates with Apache Spark, providing a collaborative environment for big data analytics and machine learning. It supports multiple programming languages, including R.","poster":"dakku987","timestamp":"1720597800.0"},{"poster":"ExamDestroyer69","timestamp":"1720121760.0","content":"Selected Answer: A\n**VARIATIONS OF THIS QUESTION**\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. **NO**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse. **YES**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse. **NO**\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse. **YES**","upvote_count":"3","comment_id":"1114087"},{"poster":"kkk5566","timestamp":"1709822160.0","comment_id":"1001558","content":"Selected Answer: A\nYes, this solution would meet the goal.","upvote_count":"1"},{"content":"Yes, this solution would meet the goal. An Azure Databricks job can be scheduled to run on a regular basis, such as daily, and can execute an R notebook that reads data from Azure Data Lake Storage, transforms the data using R code, and then writes the transformed data to the data warehouse in Azure Synapse Analytics.","upvote_count":"3","poster":"esaade","timestamp":"1694083380.0","comment_id":"831892"},{"upvote_count":"3","comment_id":"766514","content":"Selected Answer: A\nshould be yes, you can schedule notebook directly from databricks","poster":"vrodriguesp","timestamp":"1688544900.0"},{"upvote_count":"1","content":"Selected Answer: A\nHas to be Yes","poster":"lemonpotato","timestamp":"1687903260.0","comment_id":"759179"},{"upvote_count":"1","content":"The Answer is A. You can only execute R notebook in Databricks and not in Data Factory. The key word here is Databricks.","timestamp":"1685822100.0","comment_id":"734715","poster":"XiltroX"},{"poster":"greenlever","comment_id":"695352","content":"Selected Answer: A\n1. extract data from Azure Data Lake Storage Gen2 into Azure Databricks, \n2. run transformations on the data in Azure Databricks, \n3. load the transformed data into Azure Synapse Analytics.","upvote_count":"2","timestamp":"1681556760.0"},{"comment_id":"643616","content":"Selected Answer: A\nyes, its possible","poster":"Deeksha1234","upvote_count":"1","timestamp":"1675758900.0"},{"comment_id":"609915","timestamp":"1669852200.0","content":"Selected Answer: A\nI go for A as well","poster":"demirsamuel","upvote_count":"2"},{"content":"You have an Azure subscription that includes the following resources:\n\nVNet1, a virtual network\n\nSubnet1, a subnet in VNet1\n\nWebApp1, a web app application service\n\nNSG1, a network security group\n\nYou create an application security group named ASG1.\n\nWhich resource can use ASG1?\n\nSelecione somente uma resposta.\n\nVNet1\n\nSubnet1\n\nWebApp1\n\nNSG1","comments":[{"comment_id":"706096","content":"the anwser is : VNet1","upvote_count":"1","timestamp":"1682655180.0","poster":"allagowf"}],"timestamp":"1669833480.0","upvote_count":"2","poster":"observador081","comment_id":"609811"},{"poster":"cuongthh","content":"Selected Answer: A\nI go for A.","upvote_count":"2","comment_id":"609619","timestamp":"1669804500.0"},{"upvote_count":"2","poster":"HoangTr","content":"I go for A.\nDatabrick should have an option to trigger the job on selected schedule, it doesn't need data factory to trigger.","timestamp":"1669368420.0","comment_id":"607103"},{"poster":"KHawk","comments":[{"timestamp":"1671940920.0","poster":"Davico93","comment_id":"621927","upvote_count":"1","content":"you made me doubt about it"}],"timestamp":"1667725200.0","content":"I would go for No. You can create a Spark Submit Job to run R Code but as shown in the second link, Databricks Utilities is not supoorted which would be necessary in my opinion to connect to Data Lake\n\nhttps://docs.microsoft.com/en-us/azure/databricks/jobs\n\nWhat do you think ?\nhttps://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/examples#spark-submit-api-example-r","upvote_count":"2","comment_id":"597598"},{"upvote_count":"2","timestamp":"1667380860.0","comment_id":"595932","poster":"Andushi","content":"Selected Answer: A\nThe solution meet the goal"}],"answer":"A","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/74335-exam-dp-203-topic-2-question-65-discussion/","exam_id":67,"answers_community":["A (100%)"],"topic":"2","answer_description":"","question_id":221,"timestamp":"2022-04-24 17:12:00","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse.\nDoes this meet the goal?","answer_ET":"A"},{"id":"79J8PzFqjFQaxHKNNGw9","choices":{"C":"alter row","A":"new branch","B":"unpivot","D":"flatten"},"question_id":222,"url":"https://www.examtopics.com/discussions/microsoft/view/74336-exam-dp-203-topic-2-question-66-discussion/","discussion":[{"timestamp":"1650837120.0","content":"Correct","poster":"gauravgogs","comment_id":"591244","upvote_count":"7"},{"timestamp":"1650813720.0","content":"Selected Answer: D\nIs correct\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-flatten","upvote_count":"6","comment_id":"591134","poster":"juanlu46"},{"poster":"606a82e","comment_id":"1255900","content":"Selected Answer: D\nflatten for JSON","timestamp":"1722027720.0","upvote_count":"1"},{"comment_id":"1001559","poster":"kkk5566","content":"Selected Answer: D\nCorrect","upvote_count":"2","timestamp":"1694090220.0"},{"comment_id":"643617","poster":"Deeksha1234","timestamp":"1659854220.0","upvote_count":"4","content":"Selected Answer: D\nD is correct"}],"answer_images":[],"exam_id":67,"unix_timestamp":1650813720,"answer_ET":"D","answer_description":"","question_text":"You plan to create an Azure Data Factory pipeline that will include a mapping data flow.\nYou have JSON data containing objects that have nested arrays.\nYou need to transform the JSON-formatted data into a tabular dataset. The dataset must have one row for each item in the arrays.\nWhich transformation method should you use in the mapping data flow?","timestamp":"2022-04-24 17:22:00","topic":"2","question_images":[],"isMC":true,"answer":"D","answers_community":["D (100%)"]},{"id":"3lvFC83RxG2DcBH9yypl","choices":{"D":"a five-minute Tumbling window","B":"a five-minute Session window","C":"a five-minute Hopping window that has a one-minute hop","A":"a five-minute Sliding window"},"question_images":[],"question_id":223,"answers_community":["D (100%)"],"discussion":[{"poster":"Remedios79","timestamp":"1687373760.0","content":"corrett. It would be corret also a hopping window with hop and size both to 5 seconds","comment_id":"620014","upvote_count":"8"},{"poster":"Kavya_sri","comment_id":"1091238","upvote_count":"1","timestamp":"1733680680.0","content":"correct"},{"timestamp":"1725712620.0","content":"Selected Answer: D\nrepeated","poster":"kkk5566","comment_id":"1001560","upvote_count":"2"},{"content":"Selected Answer: D\ncorrect","upvote_count":"3","poster":"Kezzah","timestamp":"1692578460.0","comment_id":"649555"},{"upvote_count":"2","timestamp":"1691390280.0","content":"Selected Answer: D\ncorrect","comment_id":"643618","poster":"Deeksha1234"},{"timestamp":"1686170580.0","content":"correct","poster":"nefarious_smalls","upvote_count":"2","comment_id":"612927"},{"poster":"juanlu46","timestamp":"1682422200.0","content":"Selected Answer: D\nIs correct","comment_id":"591619","upvote_count":"3"}],"answer_ET":"D","question_text":"You use Azure Stream Analytics to receive Twitter data from Azure Event Hubs and to output the data to an Azure Blob storage account.\nYou need to output the count of tweets during the last five minutes every five minutes. Each tweet must only be counted once.\nWhich windowing function should you use?","timestamp":"2022-04-25 13:30:00","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/74446-exam-dp-203-topic-2-question-67-discussion/","answer":"D","answer_description":"","topic":"2","unix_timestamp":1650886200,"exam_id":67,"answer_images":[]},{"id":"LVUxRVyiEqdIbMHY9FCi","question_text":"You are planning a streaming data solution that will use Azure Databricks. The solution will stream sales transaction data from an online store. The solution has the following specifications:\nThe output data will contain items purchased, quantity, line total sales amount, and line total tax amount.\n//IMG//\n\n✑ Line total sales amount and line total tax amount will be aggregated in Databricks.\n✑ Sales transactions will never be updated. Instead, new rows will be added to adjust a sale.\nYou need to recommend an output mode for the dataset that will be processed by using Structured Streaming. The solution must minimize duplicate data.\nWhat should you recommend?","answer_ET":"C","discussion":[{"poster":"[Removed]","upvote_count":"18","comment_id":"779518","content":"Selected Answer: C\nUsing chatgpt : Append","timestamp":"1674008520.0"},{"content":"Selected Answer: A\nI think Update is correct, because \" new rows will be added to adjust a sale\" , that means that in the course of a day you must update de daily import with the new sales, the group by process generates new amounts, keep in mind that when it say \"sales transactions will never be updated\" its about the online store, not the aggregated rows.","comment_id":"595018","upvote_count":"18","timestamp":"1651323840.0","poster":"necktru","comments":[{"content":"Sales transactions will never be updated. Instead, new rows will be added to adjust a sale.","comment_id":"935766","poster":"vctrhugo","timestamp":"1687893900.0","upvote_count":"8"}]},{"content":"Selected Answer: C\nWe need to append new entries only, the question describes updates that are not done on existing row entries, but by adding new rows of duplicate transactions","upvote_count":"1","poster":"Danweo","comment_id":"1248366","timestamp":"1721049000.0"},{"upvote_count":"1","timestamp":"1718739360.0","comment_id":"1232584","content":"Selected Answer: C\nappend...","poster":"ageorgieva"},{"timestamp":"1714028880.0","poster":"Dusica","comment_id":"1201802","comments":[{"poster":"jpgsa11","upvote_count":"1","timestamp":"1716020280.0","content":"Exacly","comment_id":"1213214"}],"upvote_count":"4","content":"Pay attention to the \"adjust\" word. That is like in double entry accounting. Lines are added positive or negative, then queries are used to produce final numeric value (aggregating).\nIt is C"},{"comment_id":"1199014","timestamp":"1713600900.0","content":"Selected Answer: A\nIt's Update","poster":"Alongi","upvote_count":"1"},{"comment_id":"1153974","content":"Selected Answer: A\nI think 'A. Update' is correct.\nFrom what I understand, \"Sales transactions will never be updated. Instead, new rows will be added to adjust a sale.\" means that the input stream will have new rows reflecting the corrected sales transaction. If we use \"append\" output mode we will have duplicates in the target table, corresponding to both the original transaction as well as the new corrected transaction.\nInstead, we can use the forEachBatch method and \"update\" output mode to merge each microBatch to the target table, updating old transactions if they match or inserting new ones if they don't. This would minimize duplicate data as well as allow for line sales amount and tax amount to be aggregated correctly in the target table.","timestamp":"1708351500.0","poster":"ExamKiller42","comments":[{"comment_id":"1153977","content":"https://docs.databricks.com/en/structured-streaming/delta-lake.html#upsert-from-streaming-queries-using-foreachbatch","timestamp":"1708351740.0","poster":"ExamKiller42","upvote_count":"1"}],"upvote_count":"2"},{"content":"Selected Answer: C\nAppend: This mode adds new rows to the output sink for each received event. It's perfect for your scenario where sales transactions are never updated but adjusted by adding new rows. This guarantees no duplicate data due to updates, minimizing duplicates.\nUpdate: This mode updates existing rows in the output sink if a matching key is found. Since you mentioned no updates occur, using this mode would lead to unnecessary operations and potential inconsistencies.\nComplete: This mode writes the entire dataset to the output sink for each micro-batch interval. This is unnecessary and inefficient for your scenario since only new rows need to be added, and it potentially duplicates data across micro-batches.","poster":"Azure_2023","upvote_count":"4","comment_id":"1147094","timestamp":"1707642720.0"},{"upvote_count":"1","timestamp":"1706867880.0","comment_id":"1138403","content":"Append. The update does not add the new rows","poster":"j888"},{"timestamp":"1704154200.0","upvote_count":"1","comment_id":"1111512","content":"Selected Answer: C\nAppend: you are only adding new rows and existing rows do not need to be updated","poster":"jsav1"},{"comment_id":"1108011","upvote_count":"1","timestamp":"1703785200.0","poster":"dakku987","content":"Selected Answer: C\nwhen you see new rows will be added to APPEND is always the answers"},{"upvote_count":"1","poster":"d046bc0","timestamp":"1702670400.0","comment_id":"1097665","content":"Selected Answer: A\n(ChatGPT) The Append output mode is used when new rows are added to the result table. This mode is suitable for scenarios where the output table is a summary of the input data, and the input data is not updated"},{"upvote_count":"1","poster":"dawoodiee","comment_id":"1017623","timestamp":"1695724560.0","content":"Sales transactions will NEVER be updated.\n\nAppend."},{"poster":"[Removed]","content":"Selected Answer: A\nUpdate","timestamp":"1694216160.0","upvote_count":"1","comment_id":"1002762"},{"timestamp":"1694095080.0","poster":"EliteAllen","upvote_count":"1","content":"Selected Answer: C\nC. Append\nThis mode is used when you are always adding new records to the output data. Given that sales transactions will never be updated and new rows will be added to adjust a sale, this mode seems to be the most suitable. It will also help in minimizing duplicate data since it only adds new records and does not modify existing ones.","comment_id":"1001623"},{"comment_id":"1001564","comments":[{"comment_id":"1001569","upvote_count":"2","timestamp":"1694090580.0","poster":"kkk5566","content":"ignore it"}],"upvote_count":"1","content":"Selected Answer: A\nA is correct","timestamp":"1694090400.0","poster":"kkk5566"},{"upvote_count":"1","content":"Selected Answer: C\nAppend is the right choice . Update is for modifications and append is to add new rows","poster":"Tightbot","timestamp":"1692464280.0","comment_id":"985330"},{"comment_id":"979295","comments":[{"timestamp":"1694665440.0","poster":"wanchihh","comment_id":"1007167","upvote_count":"1","content":"The question specifically stated \"Sales transactions will never be updated\"."}],"content":"Selected Answer: A\nWhen say adjusted, it means update because we need to reaggregate to get the latest total amount after adjustment, also there is a hint to minimise duplicates.. Append will ignore the updated state records only emit new records","upvote_count":"1","timestamp":"1691833860.0","poster":"[Removed]"},{"content":"The requirement says \"not to update\", your answer says \"update\"??????","upvote_count":"2","poster":"pavankr","comment_id":"944693","timestamp":"1688649660.0"},{"timestamp":"1686777300.0","comment_id":"923573","comments":[{"poster":"MarkJoh","content":"This is definitely the correct answer. The statement \"Sales transactions will never be updated. Instead, new rows will be added to adjust a sale.\" is not about the actions that \"you do\", it's about what the incoming rows will look like. There will never be an update of a row, if an update is needed, a new row will come in as an adjustment. So, it's kind of a trick question. The answer is Append.","upvote_count":"1","comment_id":"1089713","timestamp":"1701891600.0"}],"poster":"vctrhugo","upvote_count":"4","content":"Selected Answer: C\nC. Append\n\nFor the given scenario, where sales transactions are never updated but new rows are added to adjust a sale, the recommended output mode for the dataset processed by using Structured Streaming in Azure Databricks is \"Append\".\n\nThe \"Append\" output mode ensures that only new rows are added to the output data as they arrive in the streaming data source. It appends the new rows to the existing result without modifying or updating previously processed data. This mode is suitable when you want to continuously append new records to the output data without duplicating or modifying existing data.\n\nIn this case, as new rows are added to adjust a sale, the \"Append\" mode will capture these new rows and include them in the output data, allowing you to aggregate the line total sales amount and line total tax amount in Databricks while minimizing duplicate data."},{"comment_id":"912500","poster":"Ankit_Az","timestamp":"1685675940.0","content":"I feel Append is correct here","upvote_count":"3"},{"comment_id":"908192","content":"It's Append as the 3rd instruction says\nSales transactions will never be updated. Instead, new rows will be added to adjust a sale.\nSo it's not UPDATE but an APPEND","timestamp":"1685221680.0","poster":"janaki","upvote_count":"4"},{"timestamp":"1678193400.0","comment_id":"831901","poster":"esaade","content":"Selected Answer: C\nI would recommend using the \"Append\" output mode for the dataset processed by using Structured Streaming in this scenario.\n\nThe \"Append\" output mode is appropriate when the output dataset is a set of new records and does not include any updates or deletions. It will only append new rows to the output dataset, which means there will be no duplicate data created as a result of the streaming data solution. Since the solution will never update existing rows, but rather add new rows, the \"Append\" mode is the best choice to meet the requirements.","upvote_count":"7"},{"comment_id":"802016","poster":"Rakrah","content":"Very Correct Answer is \"APPEND\" MODE - Because Sales transaction never be updated using Update Mode, would not provide any benefits, rather \"Append\" mode will be add new row to the output dataset and correctly aggregate the line total sales amount and line total tax amount without any duplicates. So Append mode 200% meet the requirement.","upvote_count":"5","timestamp":"1675859580.0"},{"comments":[{"poster":"kkk5566","content":"\"Line total sales amount and line total tax amount will be aggregated in Databricks.\"","timestamp":"1693285560.0","comment_id":"992793","upvote_count":"1","comments":[{"upvote_count":"1","poster":"kkk5566","comment_id":"992794","timestamp":"1693285620.0","content":"Append is correct."}]}],"upvote_count":"1","timestamp":"1675025520.0","content":"Update mode is the answer:\nIt involves writing the data records that are either new or for which the old value is updated. So this mode can be used when it is required to have the “upsert” mode of operation doing some aggregation. If no aggregation is applied, the update mode works the same as the append mode.\nhttps://medium.com/analytics-vidhya/spark-streaming-output-modes-600c689b6bf9","poster":"Okea","comment_id":"792054"},{"upvote_count":"3","comment_id":"777168","content":"Selected Answer: C\nNew rows will be added suggest \"Append\", correct is C for me","timestamp":"1673824920.0","poster":"agold96"},{"upvote_count":"1","comment_id":"764419","comments":[{"timestamp":"1672741680.0","comment_id":"764424","poster":"hanzocuk","content":"Sorry made a typo, C: Append....","upvote_count":"1","comments":[{"upvote_count":"2","comment_id":"764441","content":"Moderator please dont include any of above, I feel it could mislead people as I am not even sure myself...\n\n1) transactions never updated -> suggests Append\n2) new rows added -> suggests Append.... to adjust a sale -> suggests Update\n3) rows minimized - suggests Update\n\nThis is altogether poorly formulated... I think as a whole A: Update is a better choice","timestamp":"1672742220.0","poster":"hanzocuk"}]}],"content":"Correct is A\n\nFocus on the task-> \"Sales >>transactions will never be updated<<. Instead, >>new rows will be added<< to adjust a sale\" (Yes, very poorly formulated as usual, who is responsible for this adjustment??)\n\nFrom spark docs -> \"append: Only the new rows in the streaming DataFrame/Dataset will be written to the sink\"\n\nupdate would be similar to append if no aggregations were involved, but in our case we have aggregations.\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode:","timestamp":"1672741560.0","poster":"hanzocuk"},{"timestamp":"1672581240.0","poster":"vikasptl07","comment_id":"763206","content":"As per below article answer should be complete mode\nhttps://medium.com/analytics-vidhya/spark-streaming-output-modes-600c689b6bf9","upvote_count":"3"},{"content":"update in outputmode in streaming does not work without watermark on timestamp ,append is the anwer","poster":"vikasptl07","timestamp":"1672309740.0","comment_id":"760882","upvote_count":"1"},{"upvote_count":"2","timestamp":"1669330440.0","content":"the fact that aggregations are mentioned here is clearly pointing to 'update', without them it would be obviously 'append' mode","poster":"Igor85","comment_id":"726252"},{"upvote_count":"1","content":"APPEND","poster":"Dusica","comment_id":"722113","timestamp":"1668876360.0"},{"upvote_count":"4","comment_id":"708708","timestamp":"1667247480.0","poster":"bokLuci","content":"Selected Answer: C\nCertainly 'C'. Line total will be aggregated at reporting time and those aggs will be run on the cumulative delta of sales transaction amounts. It's normal design of transactional delta for end of period reporting. \nYou don't have anything to Update, you are only appending the delta from the previous transaction."},{"comment_id":"706250","content":"A - update, this reduce duplicate data","poster":"rzeng","upvote_count":"2","timestamp":"1666943640.0"},{"upvote_count":"2","comment_id":"645786","content":"Selected Answer: A\nI think this should be \"Update\" mode because the key is \"minimize duplicates\". Please check https://sparkbyexamples.com/spark/spark-streaming-outputmode/","timestamp":"1660289160.0","poster":"ads5891"},{"comments":[{"comment_id":"648284","upvote_count":"1","poster":"Genere","content":"Mode\n\nExample\n\nNotes\n\nComplete\n\n.outputMode(\"complete\")\n\nThe entire updated Result Table is written to the sink. The individual sink implementation decides how to handle writing the entire table.\n\nAppend\n\n.outputMode(\"append\")\n\nOnly the new rows appended to the Result Table since the last trigger are written to the sink.\n\nUpdate\n\n.outputMode(\"update\")\n\nOnly the rows in the Result Table that were updated since the last trigger will be outputted to the sink. Since Spark 2.1.1","timestamp":"1660797240.0"}],"upvote_count":"1","timestamp":"1660124940.0","comment_id":"644895","content":"Selected Answer: C\nI'm still not convinced about 'update'. The DB docs doesn't even mention append for writeStream outputmodes, just complete and append: https://docs.databricks.com/delta/delta-streaming.html\n Lmk if I'm missing something!","poster":"Fidel_104"},{"poster":"wendyy","content":"Update is Correct. \n\"The output data will contain items purchased, quantity, line total sales amount, and line total tax amount\" means output data has been aggregated, they are not Sales transactions(input). You don't want the aggredated data has duplicates, so update is correct.","timestamp":"1660074300.0","comment_id":"644653","upvote_count":"1"},{"comment_id":"643628","poster":"Deeksha1234","content":"Selected Answer: C\nsorry, but when I check all the documentation available , I find only append and complete modes for sink, append will add only new rows so the answer should be append.","upvote_count":"5","timestamp":"1659858720.0"},{"comment_id":"643621","content":"Update - to minimize duplicates and Output can be updated (given that transactions can't be updated but we need to find a solution for output)","upvote_count":"1","timestamp":"1659854760.0","poster":"Deeksha1234"},{"content":"Selected Answer: A\nUpdate is Correct - \"The solution must minimize duplicate data\"","comment_id":"629306","upvote_count":"3","timestamp":"1657392000.0","poster":"dsp17"},{"timestamp":"1656277800.0","content":"Selected Answer: C\nAppend looks good to me","upvote_count":"1","poster":"Saim8711","comment_id":"622818"},{"comment_id":"620018","timestamp":"1655838540.0","poster":"Remedios79","content":"For me is C, append, as it would be like INSERT in SQL statement","upvote_count":"2"},{"comment_id":"604003","timestamp":"1652972880.0","upvote_count":"1","content":"Selected Answer: C\nCorrect answer should be \"Append\". Detail here: https://docs.microsoft.com/en-us/azure/stream-analytics/sql-database-upsert","comments":[{"timestamp":"1655567520.0","poster":"Aditya0891","content":"check the question first. It talks about Azure Databricks and not Stream Analytics","comment_id":"618316","upvote_count":"1"}],"poster":"upliftinghut"},{"poster":"Rickmundo","comment_id":"600721","content":"For me it's not clear why it is update instead of append. Does have anyone a explanation?","comments":[{"poster":"Davico93","content":"The key word is \"OUTPUT\" and this is updated, transactions are \"INPUT\" and this won't be updated","timestamp":"1656123240.0","upvote_count":"3","comment_id":"621929"}],"upvote_count":"1","timestamp":"1652372940.0"},{"poster":"oakmm","comment_id":"593421","content":"should it be \"complete\" output mode?","upvote_count":"2","timestamp":"1651100160.0"},{"comment_id":"591347","upvote_count":"2","content":"Initially I thought append as well but the line that says \"minimise duplicates\" indicates to me that you don't want to continually add records if you can update existing ones","timestamp":"1650854460.0","poster":"chuckas"},{"content":"Why not append?","poster":"jackttt","upvote_count":"3","timestamp":"1650713160.0","comment_id":"590571"}],"choices":{"A":"Update","B":"Complete","C":"Append"},"answer_images":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0025200002.png"],"timestamp":"2022-04-23 13:26:00","answer":"C","unix_timestamp":1650713160,"question_id":224,"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/74226-exam-dp-203-topic-2-question-68-discussion/","isMC":true,"answer_description":"","exam_id":67,"answers_community":["C (64%)","A (36%)"]},{"id":"YyIECfMTaK5OssW69FPW","question_text":"You have an enterprise data warehouse in Azure Synapse Analytics named DW1 on a server named Server1.\nYou need to determine the size of the transaction log file for each distribution of DW1.\nWhat should you do?","choices":{"A":"On DW1, execute a query against the sys.database_files dynamic management view.","C":"Execute a query against the logs of DW1 by using the Get-AzOperationalInsightsSearchResult PowerShell cmdlet.","D":"On the master database, execute a query against the sys.dm_pdw_nodes_os_performance_counters dynamic management view.","B":"From Azure Monitor in the Azure portal, execute a query against the logs of DW1."},"answer_ET":"D","discussion":[{"content":"The question asks for transaction log size on each distribution. The correct answer is D: Link below: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor\n-- Transaction log size\nSELECT\n instance_name as distribution_db,\n cntr_value*1.0/1048576 as log_file_size_used_GB,\n pdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'","timestamp":"1654854960.0","comments":[{"poster":"learnwell","timestamp":"1719722580.0","comment_id":"1239522","content":"This is correct. The explanation is given in the link provided above.","upvote_count":"1"},{"poster":"Davico93","comment_id":"622182","timestamp":"1656171960.0","upvote_count":"5","content":"but you don't need it from master, just DW1"}],"poster":"Saransundar","comment_id":"614457","upvote_count":"24"},{"timestamp":"1656278460.0","poster":"Saim8711","comment_id":"622820","upvote_count":"11","content":"Selected Answer: D\nD is totally correct. Link has this very clearly mentioned\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"},{"comment_id":"1304056","upvote_count":"1","poster":"obeheshti","content":"Selected Answer: A\nA","timestamp":"1730134200.0"},{"poster":"renan_ineu","upvote_count":"1","content":"Selected Answer: D\n```\nSELECT\n instance_name as distribution_db, pdw_node_id,\n cntr_value*1.0/1048576 as log_file_size_used_GB\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE instance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\n```\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size","timestamp":"1726839900.0","comment_id":"1286852"},{"content":"ans-: D because the Reference document\n from Xam topic i checked","upvote_count":"1","comment_id":"1285658","poster":"ahana1074","timestamp":"1726650540.0"},{"upvote_count":"1","poster":"e56bb91","comment_id":"1244520","content":"Selected Answer: D\nChatGPT 4o\nThe sys.dm_pdw_nodes_os_performance_counters dynamic management view provides performance counter information for each node in your Synapse Analytics instance. This includes metrics related to the transaction log file.","timestamp":"1720463160.0"},{"upvote_count":"1","poster":"e56bb91","content":"ChatGPT:\nSELECT\n name,\n type_desc,\n size * 8 / 1024 AS size_in_MB\nFROM\n sys.database_files\nWHERE\n type = 1; -- Type 1 corresponds to log files","timestamp":"1719784800.0","comment_id":"1239865"},{"poster":"tadenet","timestamp":"1713386520.0","content":"Selected Answer: A\nsys.dm_pdw_nodes_os_performance_counters DMV in Azure Synapse Analytics does not provide information about the size of the transaction log file for each distribution of the data warehouse; it provides CPU utilization, memory usage, disk I/O rates, and network traffic at the node level. \nTo obtain information about the size of transaction log files, we can use sys.dm_db_file_space_usage or sys.database_files.","comment_id":"1197480","upvote_count":"1"},{"comment_id":"1095817","upvote_count":"2","timestamp":"1702500780.0","poster":"Sachmett","content":"Selected Answer: A\nTable sys.dm_pdw_nodes_os_performance_counter contains information about current size of file log each distribution. \nYou can use sys.database_files to determine size of file log of DW1 (each distribiution the same).","comments":[{"comment_id":"1138406","poster":"j888","upvote_count":"1","timestamp":"1706868300.0","content":"Agreed with A."}]},{"content":"Selected Answer: D\nShould be D","upvote_count":"1","poster":"kkk5566","comment_id":"1001567","timestamp":"1694090580.0"},{"content":"Selected Answer: D\nA is wrong it applies for SQL server and non distributed non MPP database.. question clearly says per distribution and synapse","comment_id":"979298","timestamp":"1691834160.0","poster":"[Removed]","upvote_count":"1"},{"comment_id":"944703","upvote_count":"1","timestamp":"1688650200.0","content":"the question is about distribution, so D should be answer.","poster":"pavankr"},{"comment_id":"935770","content":"-- Transaction log size\nSELECT\n instance_name as distribution_db,\n cntr_value*1.0/1048576 as log_file_size_used_GB,\n pdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size","poster":"vctrhugo","comments":[{"timestamp":"1687894140.0","poster":"vctrhugo","comment_id":"935772","content":"This query returns the transaction log size on each distribution.","upvote_count":"1"}],"timestamp":"1687894080.0","upvote_count":"2"},{"upvote_count":"2","comment_id":"931272","poster":"auwia","timestamp":"1687501440.0","content":"Selected Answer: D\nProbably A and D are correct, but I would choise D, because it's clearly described as the question:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"},{"comment_id":"923575","timestamp":"1686777540.0","content":"Selected Answer: D\nMonitor transaction log size\nThe following query returns the transaction log size on each distribution. If one of the log files is reaching 160 GB, you should consider scaling up your instance or limiting your transaction size.\n\n-- Transaction log size\nSELECT\n instance_name as distribution_db,\n cntr_value*1.0/1048576 as log_file_size_used_GB,\n pdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size","poster":"vctrhugo","upvote_count":"1"},{"timestamp":"1685122080.0","poster":"TestingCRM","comment_id":"907523","upvote_count":"1","content":"D. See this article https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size"},{"poster":"agold96","comment_id":"777172","content":"Selected Answer: A\nAccording to the documentation:\n\"For information about the current log file size, its maximum size, and the autogrow option for the file, you can also use the size, max_size, and growth columns for that log file in sys.database_files.\"\nA seems enough, I am not sure it gives the results for each distribution but it seems so.","timestamp":"1673825280.0","upvote_count":"2"},{"timestamp":"1669620540.0","upvote_count":"1","poster":"cokey","content":"Selected Answer: D\ni think \"D\"","comment_id":"728849"},{"upvote_count":"2","poster":"allagowf","content":"Selected Answer: D\nAnswer is On the master database, execute a query against the sys.dm_pdw_nodes_os_performance_counters dynamic management view.\n\nThe following query returns the transaction log size on each distribution. If one of the log files is reaching 160 GB, you should consider scaling up your instance or limiting your transaction size.\n\n-- Transaction log size\nSELECT\ninstance_name as distribution_db, cntr_value*1.0/1048576 as log_file_size_used_GB, pdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-manage-monitor","comment_id":"706110","timestamp":"1666931280.0"},{"content":"Selected Answer: D\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size","upvote_count":"2","poster":"ads5891","timestamp":"1660290000.0","comment_id":"645790"},{"upvote_count":"1","timestamp":"1660074480.0","comment_id":"644654","content":"DW is a distributed system, and you can run view queries on any node. So it doesn't matter on the master database.","poster":"youngbug"},{"content":"I cannot find any correct answer if the question is correct. Someone said D, but how can you run it in master database? you should execute it in DW1. you will get error message if you run it in master database \"Invalid object name 'sys.dm_pdw_nodes_os_performance_counters'.\" it is correct if change answer D to execute on DW1.","upvote_count":"3","comment_id":"642068","poster":"zxc01","timestamp":"1659556860.0"},{"comments":[{"timestamp":"1671444600.0","upvote_count":"1","content":"A is also close, but D wil give exact answer (log size for each distribution). Not sure if same can be achieved using A.","comment_id":"749709","poster":"MS2710"}],"comment_id":"621154","content":"D is totally correct. Link has this very clearly mentioned\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor\n\nThe following query returns the transaction log size on each distribution. If one of the log files is reaching 160 GB, you should consider scaling up your instance or limiting your transaction size.","timestamp":"1656004560.0","upvote_count":"3","poster":"Saim8711"},{"upvote_count":"2","content":"Selected Answer: D\ni agree with Saransundar","poster":"jihenTR13","comment_id":"618237","timestamp":"1655553480.0"},{"upvote_count":"3","timestamp":"1651125840.0","content":"Selected Answer: A\nA: Provided source gives the solution","comment_id":"593570","poster":"Feljoud"}],"answer_images":[],"question_images":[],"timestamp":"2022-04-28 08:04:00","answer":"D","unix_timestamp":1651125840,"topic":"2","question_id":225,"url":"https://www.examtopics.com/discussions/microsoft/view/74724-exam-dp-203-topic-2-question-69-discussion/","isMC":true,"answer_description":"","exam_id":67,"answers_community":["D (74%)","A (26%)"]}],"exam":{"isMCOnly":false,"provider":"Microsoft","name":"DP-203","isBeta":false,"lastUpdated":"12 Apr 2025","isImplemented":true,"numberOfQuestions":384,"id":67},"currentPage":45},"__N_SSP":true}