{"pageProps":{"questions":[{"id":"fVlraaWIYpUUzOXra3Fr","exam_id":48,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/37295-exam-az-204-topic-3-question-18-discussion/","choices":{"B":"a value containing the collection name","D":"a concatenation of multiple property values with a random suffix appended","A":"a single property value that does not appear frequently in the documents","C":"a single property value that appears frequently in the documents","E":"a hash suffix appended to a property value"},"answer_ET":"DE","answer":"DE","isMC":true,"answers_community":["DE (94%)","6%"],"question_text":"You are developing an Azure Cosmos DB solution by using the Azure Cosmos DB SQL API. The data includes millions of documents. Each document may contain hundreds of properties.\nThe properties of the documents do not contain distinct values for partitioning. Azure Cosmos DB must scale individual containers in the database to meet the performance needs of the application by spreading the workload evenly across all partitions over time.\nYou need to select a partition key.\nWhich two partition keys can you use? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","answer_description":"","unix_timestamp":1605779820,"timestamp":"2020-11-19 10:57:00","question_id":171,"question_images":[],"discussion":[{"poster":"TEMPKAKAM","upvote_count":"74","content":"The given answer is correct","timestamp":"1605779820.0","comment_id":"222664"},{"timestamp":"1622372940.0","upvote_count":"18","poster":"mlantonis","content":"D and E\n\nReference:\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/synthetic-partition-keys","comment_id":"370123"},{"timestamp":"1737969600.0","poster":"wafa_chaari","comment_id":"1347327","content":"Selected Answer: DE\nyou must construct a synthetic partition key. 1-by concatenating multiple property values into a single partitionkey (expl: name=azure , date= 2024--> partitionkey= azure2024) \n2-use a partition key with a random suffix ( expl azure1 , azure3..)","upvote_count":"1"},{"poster":"Vichu_1607","comment_id":"1307723","timestamp":"1730881860.0","upvote_count":"1","content":"Selected Answer: DE\nD. a concatenation of multiple property values with a random suffix appended\nE. a hash suffix appended to a property value"},{"poster":"jobolesonihal","timestamp":"1711054200.0","comment_id":"1179637","content":"Selected Answer: CE\nCopilot says C and E\nWith D - random suffice for each item may be an overkill.","upvote_count":"1"},{"timestamp":"1701091800.0","comment_id":"1081583","poster":"JoseManel","content":"Selected Answer: DE\nD. a concatenation of multiple property values with a random suffix appended Most Voted\nE. a hash suffix appended to a property value Most Voted","upvote_count":"2"},{"comment_id":"950010","poster":"[Removed]","upvote_count":"1","content":"Given answer is incorrect. Correct answer: AE","comments":[{"poster":"Net_IT","comment_id":"1045685","upvote_count":"1","content":"That is not the given answer? It is D and E.","timestamp":"1697529780.0"}],"timestamp":"1689179940.0"},{"content":"Selected Answer: DE\nIt's the best practice to have a partition key with many distinct values, such as hundreds or thousands. The goal is to distribute your data and workload evenly across the items associated with these partition key values. If such a property doesn’t exist in your data, you can construct a synthetic partition key","timestamp":"1674934080.0","upvote_count":"2","comment_id":"791001","poster":"narenazure"},{"timestamp":"1670429220.0","poster":"mdg3501","comment_id":"738095","content":"got this on 2022-12-7","upvote_count":"5"},{"content":"Selected Answer: DE\nD. a concatenation of multiple property values with a random suffix appended Most Voted\nE. a hash suffix appended to a property value Most Voted","poster":"OPT_001122","timestamp":"1668867720.0","upvote_count":"2","comment_id":"722047"},{"comment_id":"718773","poster":"TheExamMaster2020","content":"Did my exam on 15th November 2022. This question was on it.","upvote_count":"4","timestamp":"1668520080.0"},{"content":"at option E : a hash value will always deliver the same result on the same data. It is not a random value as stated in the proposed solution. \nSince A,B and C fall off, and two answers must be chosen still I would go for D,E.","poster":"coffecold","comment_id":"705232","upvote_count":"2","timestamp":"1666846440.0"},{"comment_id":"618161","content":"Selected Answer: DE\nIf no property in the document data will have unique values, you need to make one. \nThis is called a synthetic partition key. \nThese sorts of keys are made by adding a unique suffix at the end of some property. \nOne other way is to create a property that will have the hashed data + a random suffix. \nThe objective is to have a property that is random enough so that you can rely on it to be your key.","upvote_count":"6","timestamp":"1655544420.0","poster":"xRiot007"},{"upvote_count":"1","comment_id":"614524","poster":"Eltooth","timestamp":"1654862280.0","content":"Selected Answer: DE\nD & E are the correct answers."},{"upvote_count":"2","content":"Got this on 20 Apr 2022","poster":"Rini100","timestamp":"1650443040.0","comment_id":"588513"},{"poster":"oescm","comment_id":"542294","upvote_count":"6","timestamp":"1644231060.0","content":"Got this one 02/2022. Went with highly voted answer."},{"comment_id":"529073","content":"Got this in the exam 01/22","upvote_count":"6","poster":"Mev4953","timestamp":"1642759200.0"},{"upvote_count":"1","comment_id":"507923","content":"the given answer is correct.","timestamp":"1640267520.0","poster":"tramlong888"},{"poster":"sanjayrawat","content":"seems, the given answer is correct.","comment_id":"458647","timestamp":"1633600080.0","upvote_count":"1"},{"content":"\"The properties of the documents do not contain distinct values for partitioning.\" based on this statement, no single property in document can be used for partition, so only possibility is D / E, though in reality, I think you might be able to use something else","poster":"ning","upvote_count":"4","comment_id":"422098","timestamp":"1628510040.0"},{"poster":"francis6170","content":"got this in the exam :)","comment_id":"376121","timestamp":"1622991360.0","upvote_count":"5","comments":[{"comment_id":"420147","timestamp":"1628152080.0","upvote_count":"1","poster":"carlos0808","content":"About how many questions that are present here, appeared in the exam?"},{"timestamp":"1627725060.0","content":"Go Fun yourself","upvote_count":"2","poster":"cool_tool","comment_id":"417803"}]},{"upvote_count":"3","poster":"glam","timestamp":"1620964020.0","content":"D. a concatenation of multiple property values with a random suffix appended\nE. a hash suffix appended to a property value","comment_id":"356866"},{"poster":"AfroYeti","comment_id":"273867","timestamp":"1611335040.0","comments":[{"comment_id":"335723","comments":[{"comment_id":"336539","upvote_count":"3","content":"nm... I thought 'C' was worded poorly....\nE is right.... The link explains it.\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/synthetic-partition-keys","poster":"clarionprogrammer","timestamp":"1618512480.0"}],"content":"That is why E is wrong. It should be C. C was intended to say \"[A] single property value (with high cardinality) that appears frequently in the documents.\"","timestamp":"1618423020.0","poster":"clarionprogrammer","upvote_count":"1"},{"comment_id":"425984","timestamp":"1629142560.0","poster":"ning","content":"Does not say you have to modify the field directly, you can create a new field concatenate the old field value and the hash","upvote_count":"1"}],"content":"Just unfortunate that E will give you roblems when trying to find your records","upvote_count":"1"},{"content":"The given answer is correct.","timestamp":"1607867220.0","comment_id":"242610","poster":"Tealon","upvote_count":"9"}],"topic":"3"},{"id":"73ejEWD8leASh0rM8NPl","topic":"3","discussion":[{"timestamp":"1622480400.0","comments":[{"content":"Agreed","timestamp":"1675172040.0","comment_id":"794215","upvote_count":"1","poster":"Esward"},{"content":"The first box is not correct, the name of database is database and not salesOrder !","comments":[{"comment_id":"754376","content":"SalesOrders is the databaseID, not the name !","timestamp":"1671815460.0","poster":"chettir01","comments":[{"upvote_count":"1","content":"// New instance of Database class referencing the server-side database\n// The name of instance is database2, and we need an ID to create it !\nDatabase database2 = await client.CreateDatabaseIfNotExistsAsync(\n id: \"adventureworks-2\"\n);","poster":"chettir01","comment_id":"754380","timestamp":"1671815700.0"}],"upvote_count":"1"},{"poster":"Vladimir_Gajinov","comment_id":"1004485","upvote_count":"1","content":"You are right but in Azure Cosmos DB, the database name is typically the same as the database ID, but spaces in the ID are replaced with hyphens (\"-\").","timestamp":"1694412360.0"}],"timestamp":"1671815340.0","upvote_count":"1","comment_id":"754374","poster":"chettir01"},{"timestamp":"1622480460.0","content":"Reference:\n\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.cosmosclient.createdatabaseifnotexistsasync\n\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.database.createcontainerasync\n \nhttps://docs.microsoft.com/en-us/dotnet/api/azure.cosmos.cosmoscontainer.createitemasync","poster":"mlantonis","comment_id":"371241","upvote_count":"7"}],"comment_id":"371240","poster":"mlantonis","content":"Box 1: Yes\nThe createDatabaseIfNotExistsAsync method checks if a database exists, and if it doesn't, create it. (Line 22)\nThe Database.CreateContainerAsync method creates a container as an asynchronous operation in the Azure Cosmos service. (Line 23 and 24)\n\nBox 2: Yes\nThe CosmosContainer.CreateItemAsync method creates an item as an asynchronous operation in the Azure Cosmos service. (Line 26 and 28)\n\nBox 3: Yes\nThe CosmosContainer.CreateItemAsync method creates an item as an asynchronous operation in the Azure Cosmos service. (Line 30)","upvote_count":"91"},{"comment_id":"427024","upvote_count":"7","content":"Line 21 is tricky, it assumes the database is already created as it calls DeleteStreamAsync. I'm confused.","comments":[{"poster":"warchoon","comment_id":"821274","upvote_count":"1","timestamp":"1677311760.0","content":"It returns ResponseMessage which is IDisposable"},{"comment_id":"648762","timestamp":"1660889640.0","content":"// Delete a Database resource where database_id is the ID property of the Database resource you wish to delete.\nDatabase database = this.cosmosClient.GetDatabase(database_id);\nawait database.DeleteStreamAsync();\n\nThat is deleted so the create if not exist will surely create the DB","upvote_count":"1","poster":"Knightie"},{"timestamp":"1665126240.0","comment_id":"688406","content":"\"using\" is equal to a \"try-finaly\" with noting in finaly so the null exeption is already been taken care of . https://stackoverflow.com/questions/2522822/will-dispose-be-called-in-a-using-statement-with-a-null-object","upvote_count":"2","poster":"coffecold"}],"timestamp":"1629313440.0","poster":"AOE"},{"upvote_count":"4","timestamp":"1692636180.0","poster":"ReyPirata","content":"On my exam 2023-08-20. Scored 925\nYes\nYes\nYes","comment_id":"986722"},{"comment_id":"956465","comments":[{"comment_id":"986168","timestamp":"1692584280.0","upvote_count":"3","comments":[{"poster":"Christian_garcia_martin","comment_id":"1254340","upvote_count":"2","timestamp":"1721819280.0","content":"exactly"}],"content":"I think you are confused! It said container2 has 1 item. So box3 - Yes is correct","poster":"macobuzi"}],"timestamp":"1689758520.0","upvote_count":"1","content":"Box 1: Yes\nBox 2: Yes\nBox 3: No - container2.Create is called only once","poster":"ks321"},{"content":"On my exam 2023-02-25","timestamp":"1677618420.0","upvote_count":"1","comment_id":"825286","poster":"Videira"},{"poster":"Lucifer14","content":"In 28-12-2022 exam","upvote_count":"3","comment_id":"762479","timestamp":"1672460400.0"},{"content":"Did my exam on 15th November 2022. This question was on it.","comment_id":"718774","poster":"TheExamMaster2020","timestamp":"1668520140.0","upvote_count":"2"},{"comments":[{"upvote_count":"1","poster":"chettir01","comments":[{"comment_id":"754378","content":"SalesOrders is the databaseID, not the name !","poster":"chettir01","upvote_count":"1","timestamp":"1671815520.0"}],"timestamp":"1671815400.0","content":"No, it's not correct, The first box is not correct, the name of database on the code is database and not salesOrder !","comment_id":"754375"}],"content":"yes yes yes","comment_id":"687858","timestamp":"1665062760.0","poster":"sam5678","upvote_count":"1"},{"comments":[{"poster":"AymanAkk","content":"there is no name of database, that s a variable called database of type Databse and it will receive the newly created database with name salesOrder !","upvote_count":"4","comment_id":"755798","timestamp":"1671983220.0"}],"comment_id":"614527","poster":"Eltooth","timestamp":"1654862700.0","upvote_count":"1","content":"Answer is correct:\nYes\nYes \nYes"},{"timestamp":"1624336380.0","comment_id":"387591","upvote_count":"2","poster":"nargzul","content":"I'm just curious, how do you know that Container 1 contains 2 items and container2 contains 1 item and not the opposite? I see that we are partitionning on the account number, but not sure to understand how the partition is made?","comments":[{"comment_id":"388087","upvote_count":"5","content":"see Lines 26 & 28 contain items for Container 1\nOnly Line 30 containers an item for Container 2","timestamp":"1624375380.0","poster":"BroGood"},{"content":"container1 is called two times adding the items, container2 just one.","upvote_count":"6","timestamp":"1624384680.0","comment_id":"388189","poster":"ariel_dev"},{"comment_id":"648761","poster":"Knightie","upvote_count":"1","timestamp":"1660889580.0","content":"container 1 will have 2 partitions, container 2 only 1 partition because partition key is the account number. But container 1 will still have 2 items, 1 item per partition. Hope this help."}]},{"upvote_count":"1","timestamp":"1624159260.0","content":"Correct Answer","poster":"AlokSingh","comment_id":"385912"}],"timestamp":"2021-05-31 19:00:00","isMC":false,"answer_description":"Box 1: Yes -\nThe createDatabaseIfNotExistsAsync method checks if a database exists, and if it doesn't, create it.\nThe Database.CreateContainerAsync method creates a container as an asynchronous operation in the Azure Cosmos service.\n\nBox 2: Yes -\nThe CosmosContainer.CreateItemAsync method creates an item as an asynchronous operation in the Azure Cosmos service.\n\nBox 3: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.cosmosclient.createdatabaseifnotexistsasync https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.database.createcontainerasync https://docs.microsoft.com/en-us/dotnet/api/azure.cosmos.cosmoscontainer.createitemasync","url":"https://www.examtopics.com/discussions/microsoft/view/53970-exam-az-204-topic-3-question-19-discussion/","question_text":"HOTSPOT -\nYou are developing an Azure-hosted e-commerce web application. The application will use Azure Cosmos DB to store sales orders. You are using the latest SDK to manage the sales orders in the database.\nYou create a new Azure Cosmos DB instance. You include a valid endpoint and valid authorization key to an appSettings.json file in the code project.\nYou are evaluating the following application code: (Line number are included for reference only.)\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answers_community":[],"answer_ET":"","answer":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04273/0024500002.jpg"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04273/0024400001.png","https://www.examtopics.com/assets/media/exam-media/04273/0024500001.jpg"],"unix_timestamp":1622480400,"exam_id":48,"question_id":172},{"id":"EHFSbiqxSAt37ZPW7w5R","answer":"B","topic":"3","question_images":[],"discussion":[{"content":"Answer is correct","comment_id":"222301","comments":[{"upvote_count":"5","timestamp":"1681724640.0","content":"received 2023-04-17 went given answer, score 926","poster":"surprise0011","comment_id":"872557"},{"timestamp":"1704197040.0","content":"Got this today.\nWent with answer here.\nScore 927","poster":"130nk3r5","comment_id":"1111840","upvote_count":"1"}],"timestamp":"1605738540.0","poster":"homimi6115","upvote_count":"45"},{"poster":"mlantonis","comment_id":"364549","timestamp":"1621774500.0","upvote_count":"27","content":"Correct Answer: B\n\n- Standard priority: The rehydration request will be processed in the order it was received and may take up to 15 hours.\n- High priority: The rehydration request will be prioritized over Standard requests and may finish in under 1 hour for objects under ten GB in size.\n\nReference:\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers?tabs=azure-portal#archive-access-tier\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-rehydration?tabs=azure-portal"},{"content":"Selected Answer: B\nData in the archive tier can take up to 15 hours to rehydrate","timestamp":"1737725220.0","upvote_count":"2","comment_id":"1346132","poster":"wafa_chaari"},{"comment_id":"1307252","upvote_count":"2","content":"Selected Answer: B\nB. between one and 15 hours\n\nExplanation:\nArchive Tier Retrieval Time: The retrieval time for data stored in the archive tier typically ranges from one hour to 15 hours. This is the time it takes to rehydrate the data from the archive tier to an online tier (hot or cool) where it can be accessed.\nService-Level Agreement (SLA): When documenting the SLA for data recovery, you should account for the maximum retrieval time. Therefore, the minimum SLA for data recovery from the archive tier should be between one and 15 hours.","poster":"Vichu_1607","timestamp":"1730794980.0"},{"comment_id":"1270469","upvote_count":"1","timestamp":"1724296380.0","content":"Selected Answer: A\nRetrieving from the Archive tier takes 12 to 48 hours","poster":"IntegrationTeam"},{"poster":"manopeydakon","content":"The SLA for data recovery from the archive tier in Azure Blob storage is indeed between zero and 60 minutes. Option B (\"between one and 15 hours\") is not accurate in the context of Azure Blob storage archive tier recovery. The correct answer should be:\n\nD. between zero and 60 minutes","upvote_count":"1","timestamp":"1704801300.0","comment_id":"1117451"},{"upvote_count":"1","comment_id":"938092","timestamp":"1688042280.0","content":"Selected Answer: B\nGot this on 6/28/2023 and passed with 850. Went with answer.","poster":"JH81"},{"upvote_count":"1","content":"Got this 2023-05-12.\nEasy one.\nMy case:\ncase: You need to configure authorization.\ncase: You need to ensure the app does not time out and processes the blob data\ncase study: VanArsdel Inc Canada","comments":[{"upvote_count":"2","poster":"methkill","comment_id":"1084237","content":"These are some random bot anwers.","timestamp":"1701339720.0"}],"poster":"aragones","timestamp":"1683914820.0","comment_id":"896131"},{"upvote_count":"1","timestamp":"1680161520.0","comment_id":"855372","content":"Question was in Exam 2023-03-30","poster":"Saluk_DE"},{"comment_id":"844174","timestamp":"1679256060.0","poster":"sarmaria","upvote_count":"2","content":"Got this question in the exam on 16/03/2023. Went with proposed solution. Make sure to prepare for case studies. I got city and lights case study."},{"timestamp":"1668853140.0","upvote_count":"1","poster":"OPT_001122","content":"Selected Answer: B\nB. between one and 15 hour","comment_id":"721926"},{"upvote_count":"1","poster":"elequiel","timestamp":"1668439380.0","content":"Selected Answer: B\nCorrect Answer: B,","comment_id":"718061"},{"poster":"winston_45","upvote_count":"4","comment_id":"651758","content":"This question is sooo open for interpretation: \"For small objects, a high priority rehydrate may retrieve the object from archive in under 1 hour.\". \n\nWhere is this 15 hours coming from?","timestamp":"1661425140.0"},{"upvote_count":"2","poster":"AZAdam22","timestamp":"1658134080.0","content":"Selected Answer: B\nB - Because it takes between one and 15 hours to recover data from the archive tier.","comment_id":"632942"},{"poster":"Eltooth","upvote_count":"2","comment_id":"614006","timestamp":"1654780560.0","content":"Selected Answer: B\nB is correct answer.\nGen 1 storage used to be up to 24 hours, now gen 2 is up to 15 hours. \n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/archive-rehydrate-overview#rehydration-priority"},{"upvote_count":"1","timestamp":"1647142380.0","poster":"meoukg","comment_id":"566513","content":"Got it on 03/2022, I chose B. between one and 15 hours"},{"content":"Selected Answer: B\nCorrect Answer is between 1-15 hours","timestamp":"1643377380.0","comment_id":"534735","upvote_count":"1","poster":"KiranAtShinde"},{"content":"Selected Answer: B\nAnswer is correct","timestamp":"1642611900.0","upvote_count":"1","comment_id":"527759","poster":"sertes"},{"upvote_count":"1","comment_id":"483688","content":"Selected Answer: B\nThe answer is correct. Per the provided reference URL https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview?tabs=azure-portal\n\nArchive tier-An offline tier optimized for storing data that is rarely accessed, and that has flexible latency requirements, on the order of hours. Data in the Archive tier should be stored for a minium of 108 days.","timestamp":"1637530500.0","poster":"ucsdmiami2020"},{"content":"Answer is correct","timestamp":"1621510800.0","poster":"UnknowMan","comment_id":"362097","upvote_count":"1"},{"poster":"glam","comment_id":"356879","upvote_count":"3","content":"correct.","timestamp":"1620965640.0"},{"timestamp":"1611805080.0","upvote_count":"2","poster":"Brak","comments":[{"content":"Minimum SLA is the SLA you promise as a developer to your end user of the application. Standard priority is the default rehydration option for archive. High priority will cost more and is usually reserved for use in emergency data restoration situations. For regular use the SLA has to go with Standard priority.","comment_id":"279409","comments":[{"upvote_count":"2","comment_id":"316995","content":"Exactly. In addition the official docs states: \"High priority may take longer than 1 hour, depending on blob size and current demand. High priority requests are guaranteed to be prioritized over Standard priority requests.\" We don't know the blob size and the current demand so we can't exclude it can be over 1 hour even in the case of high priority","timestamp":"1616404200.0","poster":"rdemontis"}],"upvote_count":"4","poster":"Archimedes","timestamp":"1611936600.0"},{"timestamp":"1612494840.0","poster":"Amrit862","content":"given answer is correct.\nref: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-rehydration?tabs=azure-portal#rehydrate-an-archived-blob-to-an-online-tier","comment_id":"283895","upvote_count":"1"},{"comment_id":"357514","timestamp":"1621039500.0","comments":[{"upvote_count":"1","comment_id":"481976","poster":"altafpatel1984","timestamp":"1637343720.0","content":"and Standard priority is the default rehydration option. so B is correct"}],"upvote_count":"1","poster":"TakumaK","content":"So.. are you confused by that the \"minimum SLA\" should mean the high rehydrate-priority? If then, can you explain what SLA would be for \"Standard rehydrate-priority\"?"}],"comment_id":"278262","content":"The question asks for \"minimum SLA\", which is <1 hour using high priority rehydrate. Answer is wrong."},{"content":"Correct Answer is between 1-15 hours","comment_id":"263240","poster":"RaviKS","upvote_count":"4","timestamp":"1610200440.0"},{"comment_id":"242628","timestamp":"1607868420.0","content":"The given answer is correct.","upvote_count":"1","poster":"Tealon"},{"poster":"Cornholioz","timestamp":"1606508760.0","comment_id":"229228","content":"Where does it provide the SLA as 1-15 hours? Standard rehydrate-priority takes up to 15. High rehydrate-priority takes <1 hour.\nI'll go with given answer but unclear where it specifically lists 1-15 in the documentation or blogs.","comments":[{"upvote_count":"2","comment_id":"250527","content":"You're right, it doesn't explicitly state an SLA per se; but it does say- Standard rehydrate-priority... ...retrievals taking up to 15 hours. It also says it's the new name for what Archive was provided over the past 2 years (default option) for CopyBlob requests.\nAssuming that this case (although doesn't state it) uses the Standard rehydrate-priority and not the High rehydrate-priority, I'll go with 1-15 hours instead of <1hr.","timestamp":"1608679380.0","poster":"Cornholioz"}],"upvote_count":"3"}],"question_id":173,"answer_description":"","answer_images":[],"exam_id":48,"question_text":"You are building a website that uses Azure Blob storage for data storage. You configure Azure Blob storage lifecycle to move all blobs to the archive tier after 30 days.\nCustomers have requested a service-level agreement (SLA) for viewing data older than 30 days.\nYou need to document the minimum SLA for data recovery.\nWhich SLA should you use?","unix_timestamp":1605738540,"isMC":true,"answer_ET":"B","timestamp":"2020-11-18 23:29:00","choices":{"B":"between one and 15 hours","D":"between zero and 60 minutes","A":"at least two days","C":"at least one day"},"url":"https://www.examtopics.com/discussions/microsoft/view/37262-exam-az-204-topic-3-question-2-discussion/","answers_community":["B (93%)","7%"]},{"id":"RVcwirv3j3JsAbtKyjZI","topic":"3","answer":"","question_text":"DRAG DROP -\nYou develop an Azure solution that uses Cosmos DB.\nThe current Cosmos DB container must be replicated and must use a partition key that is optimized for queries.\nYou need to implement a change feed processor solution.\nWhich change feed processor components should you use? To answer, drag the appropriate components to the correct requirements. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view the content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","discussion":[{"comment_id":"394671","comments":[{"content":"You are right. The given answer is correct.","timestamp":"1625352360.0","upvote_count":"11","poster":"TakumaK","comment_id":"397862","comments":[{"upvote_count":"19","content":"And your given answer, that MattXu is right, is also correct.","poster":"john4p","comments":[{"content":"that observation is correct, john","timestamp":"1639398600.0","upvote_count":"15","poster":"dbobspurfpoo","comments":[{"comment_id":"1077937","poster":"Stann07","timestamp":"1700698200.0","upvote_count":"2","content":"dbobspurfpoo, you are right to say that the observation of john is correct."},{"poster":"john4p","content":"Thank you for the confirmation.","upvote_count":"13","comment_id":"501984","timestamp":"1639556760.0","comments":[{"upvote_count":"4","timestamp":"1669147260.0","poster":"rafael0","comment_id":"724628","content":"Thank you too for your confirmation sir"},{"timestamp":"1675939140.0","comment_id":"803091","poster":"GrumpyVic","upvote_count":"5","content":"Affirming that the answer is incorrect would be incorrect, if I am correct."},{"upvote_count":"2","content":"That's funny :D","comment_id":"761083","timestamp":"1672322160.0","poster":"rotimislaw"},{"upvote_count":"1","timestamp":"1697575920.0","content":"You are welcome.","comment_id":"1046360","poster":"riyachavan"},{"poster":"heisenberg33","comment_id":"536894","content":"Thank you for the confirmation of that observation on the given answer. You are right, the given answer is correct.","upvote_count":"12","timestamp":"1643628240.0"}]}],"comment_id":"500584"}],"timestamp":"1638889080.0","comment_id":"496121"}]}],"content":"The given answer is correct.","timestamp":"1625054580.0","poster":"MattXu","upvote_count":"60"},{"timestamp":"1664947500.0","poster":"gmishra88","content":"Microsoft has obscrure names in documentation and has nothing to do with the azure component itself. \"Delegate\", \"Host component\", \"Compute instance\". How's remembering this from one page they wrote mean anything. We do not call any of them these when we implement the change feed processor","comment_id":"686594","upvote_count":"14"},{"upvote_count":"1","content":"correct\nThe change feed processor has four main components:\n\nThe monitored container: The monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.\n\nThe lease container: The lease container acts as state storage and coordinates the processing of the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.\n\nThe compute instance(host): A compute instance hosts the change feed processor to listen for changes. Depending on the platform, it might be represented by a virtual machine (VM), a Kubernetes pod, an Azure App Service instance, or an actual physical machine. The compute instance has a unique identifier that's called the instance name throughout this article.\n\nThe delegate: The delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.","poster":"wafa_chaari","comment_id":"1347333","timestamp":"1737970800.0"},{"upvote_count":"1","comment_id":"1249838","timestamp":"1721236800.0","content":"Got this Q on 07/07. Went with given answer.","poster":"royalbaby"},{"timestamp":"1695774900.0","poster":"Dixavado","comment_id":"1018296","content":"It was on my exam today (2023-09-26) I went with the examtopics answer - score 850","upvote_count":"3"},{"timestamp":"1695377220.0","comment_id":"1013946","upvote_count":"2","poster":"shekhar11","comments":[{"comment_id":"1017988","content":"me too today","poster":"nikipediaa","timestamp":"1695745680.0","upvote_count":"1"}],"content":"Question in my exam 22sept 2023"},{"upvote_count":"2","comment_id":"1004625","poster":"Tarajee","timestamp":"1694428020.0","content":"On my exam 2023sept"},{"timestamp":"1691998500.0","comment_id":"980573","upvote_count":"5","poster":"kayvg","content":"1. Monitored container\n> You want to track changes of the data you store\n\n2. Lease container\n> A lease container is responsible for maintaining information about which workers are processing which data from the Monitored Container\n\n3. Host\n> They represent the worker instances responsible for processing changes from the change feed\n\n4. Delegates\n> Methods you define to handle actual changes detected by the change feed (so when the change feed receives changes, it invokes delegate methods)\n\nThe difference between a monitored container and a host is that a host actually distributes work to delegates, while a monitored container tracks changes in data and gives that info to the change feed processor."},{"content":"please provide correct answers","comment_id":"969705","upvote_count":"1","poster":"KrishTeam","timestamp":"1690953240.0"},{"content":"The discussions are meant to help prepare for the exam so please try to be helpful","upvote_count":"2","poster":"unraval","comment_id":"961492","timestamp":"1690195980.0"},{"content":"People are commenting here to be funny, but can they care to provide an explanation?","poster":"unraval","comment_id":"961490","upvote_count":"2","timestamp":"1690195920.0"},{"content":"Received this on 15th of June 2023. Went with the given answer.","comment_id":"923858","poster":"Chris2349","upvote_count":"2","timestamp":"1686816480.0"},{"timestamp":"1677618480.0","content":"On my exam 2023-02-25","poster":"Videira","upvote_count":"3","comment_id":"825288"},{"comment_id":"738097","timestamp":"1670429220.0","poster":"mdg3501","upvote_count":"3","content":"got this on 2022-12-7"},{"comment_id":"718776","upvote_count":"5","content":"Did my exam on 15th November 2022. This question was on it.","poster":"TheExamMaster2020","comments":[{"timestamp":"1670902740.0","upvote_count":"2","content":"I also Got same in 5 dec exam.","comment_id":"743567","poster":"BennyJai"}],"timestamp":"1668520140.0"},{"comment_id":"581667","upvote_count":"6","timestamp":"1649229540.0","content":"Answer is correct.\n\n*Note that the \"Host\" Component should be called Compute Instance instead.\n \nhttps://docs.microsoft.com/en-us/azure/cosmos-db/change-feed-processor","poster":"herrmutig"},{"timestamp":"1646859480.0","poster":"petitbilly","comment_id":"564338","upvote_count":"2","content":"Got it in exam 03/22"},{"content":"Got this one 02/2022. Went with the given answer","comment_id":"542297","timestamp":"1644231120.0","upvote_count":"5","poster":"oescm"},{"poster":"lugospod","timestamp":"1642603020.0","content":"Got similar this one 01/2022 but it was in the CASE study. Exact same offered options...","comment_id":"527591","upvote_count":"5"},{"comment_id":"416682","timestamp":"1627541640.0","upvote_count":"5","content":"Answer is correct","poster":"MK22"}],"url":"https://www.examtopics.com/discussions/microsoft/view/56321-exam-az-204-topic-3-question-20-discussion/","isMC":false,"answer_description":"Box 1: The monitored container -\nThe monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.\n\nBox 2: The lease container -\nThe lease container acts as a state storage and coordinates processing the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.\nBox 3: The host: A host is an application instance that uses the change feed processor to listen for changes. Multiple instances with the same lease configuration can run in parallel, but each instance should have a different instance name.\n\nBox 4: The delegate -\nThe delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/change-feed-processor","question_images":["https://www.examtopics.com/assets/media/exam-media/04273/0024700001.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04273/0024700002.png"],"answer_ET":"","unix_timestamp":1625054580,"question_id":174,"exam_id":48,"timestamp":"2021-06-30 14:03:00","answers_community":[]},{"id":"vdwmK6BVXSeR4s8Os8Sv","answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=/azure/storage/blobs/toc.json","discussion":[{"comment_id":"686626","content":"So, because it is ZRS, and that does not support arhive tier, it cannot be moved to archive tier even though the questions mention the red-herring key-word \"infrequently accessed\" (which triggers feelings for archive tier). For no logically apparent reason Microsoft decided not to support archive tier in ZRS and unfortunately I have to remember that Microsoft \"feature\"?","timestamp":"1664949540.0","upvote_count":"41","comments":[{"timestamp":"1723529700.0","content":"Archive data is physically disconnected from the web, if it has to be zone-redundant then it needs to be online, so it cannot be archived.","comment_id":"1265012","poster":"0cc50bf","upvote_count":"1"},{"timestamp":"1668919620.0","comment_id":"722392","content":"ZRS, and that does not support arhive tier - this is key point, Thanks","upvote_count":"13","poster":"OPT_001122"},{"poster":"surprise0011","timestamp":"1681619040.0","comment_id":"871454","upvote_count":"2","content":"also GZRS and RA-GZRS does not support archieve tier\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#archive-access-tier"},{"comment_id":"983591","timestamp":"1692274560.0","poster":"jakobste","content":"I assume they wanted to charge people more money for ZRS and thus no Archive tier.","upvote_count":"1"}],"poster":"gmishra88"},{"content":"Correct answer. Got this in exam on 30/12/2023. \nCase study: Contoso Ltd.\nTotal questions: 46\nTime: 1:40 minutes\nScore: 940\n\n43 questions from Exam Topics. Just 3 questions outside of it.","timestamp":"1704022260.0","comment_id":"1110486","upvote_count":"11","poster":"Mick1z8"},{"comment_id":"1347335","poster":"wafa_chaari","content":"correct.\nThe archive tier for Blob Storage isn't currently supported for ZRS, GZRS, or RA-GZRS accounts. Unmanaged disks don't support ZRS or GZRS.\n\nGeneral-purpose v2 storage accounts support the latest Azure Storage features and incorporate all of the functionality of general-purpose v1 and Blob storage accounts. General-purpose v2 accounts are recommended for most storage scenarios. General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. General-purpose v2 accounts support default account access tiers of hot or cool and blob level tiering between hot, cool, or archive.","timestamp":"1737971340.0","upvote_count":"1"},{"poster":"chrisbele","content":"The answer is correct!","upvote_count":"1","timestamp":"1732378080.0","comment_id":"1316726"},{"content":"Got it on 20 April 2024...Marks > 900...All questions from examtopics 400 questions...\nanswer is correct...","upvote_count":"4","timestamp":"1714287240.0","comment_id":"1203412","poster":"neelkanths"},{"poster":"Dixavado","timestamp":"1695774900.0","content":"It was on my exam today (2023-09-26) I went with the examtopics answer - score 850","comment_id":"1018297","upvote_count":"2"},{"comment_id":"967150","poster":"applepie","timestamp":"1690723380.0","upvote_count":"1","content":"got this question today, go with the provided answer - 7/30/2023, score 895/1000"},{"timestamp":"1676881440.0","upvote_count":"3","comment_id":"815005","content":"On exam 20-02-2023","poster":"Priya0703"},{"content":"1. Implement storage V2\n2. Set lifecycle management policy to move to cool tier\n\nwhich means given answers are correct","comment_id":"780964","timestamp":"1674120840.0","poster":"Esward","comments":[{"poster":"Hasti123","timestamp":"1695293340.0","content":"Why not Blob storage ?","upvote_count":"2","comment_id":"1013000","comments":[{"comment_id":"1144195","timestamp":"1707376140.0","upvote_count":"2","poster":"Woksi","content":"Blob Storage is not a type of *Account*"}]}],"upvote_count":"2"},{"upvote_count":"11","content":"Got in exam. go with given answer","poster":"18Marks","comments":[{"upvote_count":"3","comments":[{"comment_id":"857506","timestamp":"1680311820.0","content":"were all the questions from the exam topic?","upvote_count":"3","poster":"adilkhan"}],"poster":"TonyMel","timestamp":"1679634540.0","content":"correct, in 2023Mar24, score: 904/1000","comment_id":"848979"}],"timestamp":"1670685960.0","comment_id":"741067"},{"poster":"Alluru","comment_id":"695516","content":"Given answer is correct. Data retention policy relay on cool tier","upvote_count":"4","timestamp":"1665849840.0"},{"content":"Microsoft plays on the unfortunate choice of words in their documentation: \"Rarely used\", \"Infrequently used\". The difference I cannot find","poster":"gmishra88","upvote_count":"6","timestamp":"1664947860.0","comment_id":"686601"},{"poster":"OPT_001122","timestamp":"1664270760.0","comment_id":"680573","comments":[{"upvote_count":"5","timestamp":"1668210480.0","poster":"micro9000","comment_id":"716363","content":"Yup, the answer is correct \nBased on this: https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview?source=recommendations\nOnly storage accounts that are configured for LRS, GRS, or RA-GRS support moving blobs to the archive tier. The archive tier isn't supported for ZRS, GZRS, or RA-GZRS accounts. For more information about redundancy configurations for Azure Storage, see Azure Storage redundancy."}],"upvote_count":"1","content":"Given answer is correct or not?"},{"timestamp":"1663672560.0","comment_id":"674075","upvote_count":"3","content":"Only storage accounts that are configured for LRS, GRS, or RA-GRS support moving blobs to the Archive tier. The Archive tier isn't supported for ZRS, GZRS, or RA-GZRS accounts.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview?source=recommendations","poster":"Enigma___"},{"timestamp":"1662452340.0","poster":"finnishr","comment_id":"660980","upvote_count":"2","content":"Correct. Can't pick the archive option since ZRS needs to be used."},{"poster":"RochaG2","content":"Data in all tiers, including the Archive tier, is always copied from the primary to the secondary during geo-replication. The Archive tier for Blob Storage is currently supported for LRS, GRS, and RA-GRS accounts, but not for ZRS, GZRS, or RA-GZRS accounts. \n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy","comment_id":"658600","upvote_count":"2","timestamp":"1662217440.0"},{"timestamp":"1651155840.0","comments":[{"timestamp":"1672674780.0","poster":"CellCS","comment_id":"763824","upvote_count":"1","content":"here is en-us link : https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview"}],"content":"https://docs.microsoft.com/ko-kr/azure/storage/common/storage-account-overview","poster":"sghaha","upvote_count":"1","comment_id":"593868"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04273/0024800004.png"],"answer":"","timestamp":"2022-04-28 16:24:00","answers_community":[],"topic":"3","url":"https://www.examtopics.com/discussions/microsoft/view/74782-exam-az-204-topic-3-question-21-discussion/","question_text":"HOTSPOT -\nYou are developing a web application that will use Azure Storage. Older data will be less frequently used than more recent data.\nYou need to configure data storage for the application. You have the following requirements:\n✑ Retain copies of data for five years.\n✑ Minimize costs associated with storing data that is over one year old.\n✑ Implement Zone Redundant Storage for application data.\nWhat should you do? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_ET":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04273/0024900001.png"],"unix_timestamp":1651155840,"isMC":false,"exam_id":48,"question_id":175}],"exam":{"id":48,"provider":"Microsoft","lastUpdated":"12 Apr 2025","name":"AZ-204","isMCOnly":false,"isBeta":false,"numberOfQuestions":452,"isImplemented":true},"currentPage":35},"__N_SSP":true}