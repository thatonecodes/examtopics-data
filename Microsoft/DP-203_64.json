{"pageProps":{"questions":[{"id":"Do8AMqIUi1t7mLDVCarU","question_text":"HOTSPOT -\nYou have an Azure Data Factory pipeline that has the activities shown in the following exhibit.\n//IMG//\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_id":316,"exam_id":67,"discussion":[{"comments":[{"content":"This was on my test on 26-Jun with 930+ score, I chose Succeeded for both. Test it for first one I'm damn sure its succeed and for second one also kind of sure","comment_id":"1237659","upvote_count":"2","timestamp":"1719425100.0","poster":"Sr18"},{"upvote_count":"31","poster":"XiltroX","content":"The second answer should be \"Succeeded\". You are providing false information to other members. The reason why it is a success is because Set Variable 2 happened because of the failure of Web 1. Therefore, this red pipeline is deedmed a success.","comments":[{"comment_id":"1017315","content":"You are incorrect.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-else-block","timestamp":"1695690660.0","upvote_count":"6","poster":"wanchihh"},{"content":"Yes, as Wanchihh mentions, you are wrong.\nUpper branch status are fail => skipped => skipped. According to the logic in the url below, this is deemed as a failed pipeline.","poster":"y154707","comment_id":"1063646","comments":[{"comment_id":"1202516","upvote_count":"1","content":"where did you find skipped?","timestamp":"1714124160.0","poster":"Dusica"}],"upvote_count":"2","timestamp":"1699259820.0"}],"comment_id":"735151","timestamp":"1670164980.0"},{"upvote_count":"11","comment_id":"578652","content":"Second should also be succeeded.","timestamp":"1648700880.0","poster":"Avi_Bdj"},{"upvote_count":"4","poster":"a03","timestamp":"1639560060.0","comment_id":"502009","content":"Agree. Second is \"Fail\" because Success connector presented."},{"content":"I just tested it myself. Provided answers are correct","upvote_count":"12","poster":"HaBroNounen","comment_id":"514977","timestamp":"1641128220.0"}],"timestamp":"1639420620.0","content":"The answers are correct.\n\nThe second question is \"failed\" because web1 has both a success and failed path. web1 would have to have only a failed path for the second question to be considered successful.","poster":"ItHYMeRIsh","comment_id":"500819","upvote_count":"48"},{"comment_id":"499518","poster":"RajBathani","timestamp":"1639240800.0","content":"The second answer should be Succeeded as 'Set Variable 2' has failed dependency on Web1.","upvote_count":"39"},{"poster":"20b1837","comment_id":"1401886","upvote_count":"1","content":"The answer given is correct. Please take notice of the colour of the dependency from set variable to stored procedure.. It is on success, NOT skip dependency. This is a GOTCHA question. If you review the MS documentation the pattern shown that looks like this has a grey line (SKIP DEPENDENCY) which is different to this example.\nOnly when skip dependency is used in a pattern like this would both examples be success.","timestamp":"1742642460.0"},{"upvote_count":"1","content":"both are succeded","poster":"Karoubi","comment_id":"1263546","timestamp":"1723299360.0"},{"comment_id":"1260063","poster":"Kryor","timestamp":"1722635940.0","upvote_count":"1","content":"Both Succeed"},{"upvote_count":"1","poster":"7082935","comment_id":"1259509","timestamp":"1722535920.0","content":"The arrow between \"Set Variable 1\" and \"Stored Procedure 1\" is green which indicates a \"succeed dependency\". (a grey arrow indicates skipped). This means the pipeline will fail on answer 2 since Stored Procedure 1 will never execute."},{"content":"as of skip - it is used in debug mode","timestamp":"1714124520.0","upvote_count":"1","comment_id":"1202521","poster":"Dusica"},{"poster":"Alongi","comment_id":"1196754","timestamp":"1713293820.0","upvote_count":"1","content":"Both Success"},{"poster":"dakku987","upvote_count":"1","content":"Both are success 100% sure about this","timestamp":"1704803820.0","comment_id":"1117473"},{"poster":"dakku987","content":"I think both are successful\n\nbcz i think when web activity fail it will pass to the set variable and the purpose of the set varible will beCOMPLETED so pipeline will be success","upvote_count":"1","timestamp":"1704203580.0","comment_id":"1111942"},{"comment_id":"1075447","content":"Both should be succeed \nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#summary-table","upvote_count":"2","timestamp":"1700488620.0","poster":"Abdulwahab1983"},{"timestamp":"1699808760.0","comment_id":"1068686","poster":"Abdulwahab1983","content":"Do If Skip Else block\nIn this approach, customer defines the business logic, and defines both the Upon Failure path, and Upon Success path, with a dummy Upon Skipped activity attached. This approach renders pipeline succeeds, if Upon Failure path succeeds.","upvote_count":"1"},{"poster":"kkk5566","timestamp":"1693472100.0","content":"1. Success and 2. Failed","upvote_count":"2","comment_id":"994947"},{"poster":"chryckie","content":"The answer is correct! It's actually pretty neat how ADF determines that.\n\nIf an activity fails but there was a subsequent OnSuccess activity that never runs, it's a fail. To handle that, you also need an OnSkipped activity to follow the OnSuccess activity in case it never ran!\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-else-block","timestamp":"1682562600.0","comments":[{"poster":"Vanq69","content":"There are so many wrong high voted answers. READ THIS.","comment_id":"1041113","upvote_count":"3","timestamp":"1697058540.0"},{"comment_id":"1018086","timestamp":"1695752220.0","poster":"JoannaMar","upvote_count":"2","content":"Thanks @chryckie for this explanation. Finally it's clear!"}],"comment_id":"882171","upvote_count":"12"},{"comment_id":"863330","timestamp":"1680816720.0","content":"second box should be succeeded\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-skip-else-block","upvote_count":"4","poster":"AHUI"},{"comment_id":"777937","upvote_count":"6","timestamp":"1673887020.0","content":"Using this microsfot doc: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#try-catch-block\nthat claims\n\"\"We determine pipeline success and failures as follows:\n-)Evaluate outcome for all leaves activities. If a leaf activity was skipped, we evaluate its parent activity instead\n-)Pipeline result is success if and only if all nodes evaluated succeed\"\"\n\nI used this logic\n\nWhen web1 activity fails: node setVariable2 succeeds and setVariable1 is skipped and its parent node web1 failed; overall pipeline fails","poster":"vrodriguesp"},{"content":"In any scenario pipeline will show success status, cause we are catching the failure","poster":"csd","timestamp":"1660406220.0","comment_id":"646388","upvote_count":"2"},{"comment_id":"623515","upvote_count":"2","content":"The answers are correct.","timestamp":"1656368520.0","poster":"StudentFromAus"},{"comment_id":"502797","comments":[{"content":"Updated: Correct ans as 1. Success and 2. Failed\nThe failure dependency means this pipeline reports success.\nBut, the presence of the success path alongside the failure path changes the outcome reported by the pipeline: Web-1 fails, Set-var-1 is skipped, and Set-var-2 succeeds --> The pipeline reports failure.","comment_id":"502800","poster":"datnguye","upvote_count":"15","comments":[{"upvote_count":"1","poster":"Remedios79","comment_id":"625977","timestamp":"1656743580.0","content":"I agree with you too"},{"comments":[{"poster":"ROLLINGROCKS","content":"This is all you need for the right answer. Its well explained in the link.","timestamp":"1659161820.0","upvote_count":"1","comment_id":"639508"}],"poster":"ladywhiteadder","content":"See https://docs.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-else-block","timestamp":"1648814520.0","comment_id":"579464","upvote_count":"6"},{"poster":"Yohannesmulu","content":"Agreed!","timestamp":"1649290560.0","comment_id":"582085","upvote_count":"1"}],"timestamp":"1639646640.0"}],"timestamp":"1639645980.0","poster":"datnguye","upvote_count":"14","content":"It should be Suceeded in both.\nThe reference article says: The failure dependency means this pipeline reports success."}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0035900001.png","https://www.examtopics.com/assets/media/exam-media/04259/0036000001.jpg"],"isMC":false,"timestamp":"2021-12-11 17:40:00","topic":"4","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0035800001.jpg","https://www.examtopics.com/assets/media/exam-media/04259/0035800002.png"],"answers_community":[],"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/67642-exam-dp-203-topic-4-question-22-discussion/","answer_ET":"","answer_description":"Box 1: succeed -\n\nBox 2: failed -\nExample:\nNow let's say we have a pipeline with 3 activities, where Activity1 has a success path to Activity2 and a failure path to Activity3. If Activity1 fails and Activity3 succeeds, the pipeline will fail. The presence of the success path alongside the failure path changes the outcome reported by the pipeline, even though the activity executions from the pipeline are the same as the previous scenario.\n\nActivity1 fails, Activity2 is skipped, and Activity3 succeeds. The pipeline reports failure.\nReference:\nhttps://datasavvy.me/2021/02/18/azure-data-factory-activity-failures-and-pipeline-outcomes/","unix_timestamp":1639240800},{"id":"KhJRqwFC82XT4hzreB0s","answer_ET":"DE","answers_community":["DE (99%)","1%"],"timestamp":"2021-12-12 00:53:00","answer_description":"","question_images":[],"unix_timestamp":1639266780,"choices":{"E":"Azure Databricks","B":"Azure HDInsight","A":"Azure Synapse Analytics","D":"Azure Data Factory","C":"Azure Machine Learning"},"question_text":"You have several Azure Data Factory pipelines that contain a mix of the following types of activities:\n✑ Wrangling data flow\n✑ Notebook\n✑ Copy\n✑ Jar\nWhich two Azure services should you use to debug the activities? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point","answer_images":[],"exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/67680-exam-dp-203-topic-4-question-23-discussion/","question_id":317,"answer":"DE","isMC":true,"discussion":[{"upvote_count":"36","comment_id":"500319","comments":[{"timestamp":"1719425340.0","comment_id":"1237668","poster":"Sr18","upvote_count":"2","content":"This was on my test on 26-Jun with 930+ score, I chose above and its correct 100%"}],"poster":"KrishIC","content":"Selected Answer: DE\nNotebook- azure databricks, managing activities in pipeline-datafactroy","timestamp":"1639360620.0"},{"content":"Selected Answer: DE\nD & E; Databricks for Wrangling and Notebooks; ADF for Copy and Jar","comment_id":"522305","upvote_count":"16","poster":"ElHomo2222","timestamp":"1642005540.0","comments":[{"content":"Wrangling and Copy = ADF \nJar and Notbooks = Databricks","upvote_count":"12","comment_id":"543537","poster":"kilowd","timestamp":"1644383040.0"}]},{"content":"Selected Answer: DE\nChatGPT confirms Data Factory Monitor for Copy and Wrangling + Databricks for Notebook and Jar.","poster":"renan_ineu","comment_id":"1283971","timestamp":"1726388040.0","upvote_count":"1"},{"poster":"7082935","content":"Selected Answer: DE\nExamtopics answers are incorrect.\n\n1. Why would you use Synapse Analytics to debug something that it outside of its own system (i.e. Data Factory Pipelines).\n2. Azure Machine Learning describes a type of logic you can implement. It is not an \"environment\" where you can debug a Data Factory Pipeline.","upvote_count":"1","comment_id":"1259979","timestamp":"1722620880.0"},{"comment_id":"1206462","content":"Selected Answer: DE\nI found this question on my exam 30/04/2024, and I put DE. I passed the exam with a high score, but I'm not sure if the answer is correct.","upvote_count":"1","timestamp":"1714819620.0","poster":"Alongi"},{"poster":"swathi_rs","upvote_count":"1","timestamp":"1714439400.0","comment_id":"1204268","content":"Selected Answer: DE\nnotebook-> databricks\ncopy -> adf"},{"comment_id":"1187973","poster":"MBRSDG","content":"Selected Answer: DE\nJust a detail: there's another question on examtopics, perfectly identical, with solution DE ...","timestamp":"1712054640.0","upvote_count":"1"},{"comment_id":"1123192","upvote_count":"1","timestamp":"1705308720.0","poster":"Azure_2023","content":"Selected Answer: DE\nhe two Azure services you should use to debug the activities are Azure Data Factory and Azure Databricks.\n\nAzure Data Factory provides a comprehensive debugging experience for all types of activities, including wrangling data flows, notebooks, and copy activities. You can use the Data Factory UI or command-line tools to step through activities, inspect data, and identify errors.\n\nAzure Databricks is a cloud-based platform for big data processing that offers a rich debugging experience for Jar activities. You can use Databricks notebooks to debug Jar code, inspect variables, and set breakpoints."},{"timestamp":"1693472340.0","content":"Selected Answer: DE\nis correct","upvote_count":"1","poster":"kkk5566","comment_id":"994950"},{"timestamp":"1687850400.0","poster":"auwia","content":"Selected Answer: DE\nWrangling and Copy = ADF\nJar and Notbooks = Databricks","upvote_count":"2","comment_id":"935093"},{"comment_id":"929924","upvote_count":"3","content":"Selected Answer: DE\nD. Azure Data Factory: Azure Data Factory itself provides debugging capabilities for its activities. You can monitor and debug the execution of pipeline activities directly within the Azure Data Factory interface. It allows you to view activity run details, input/output data, logs, and diagnose any errors or issues encountered during execution.\n\nE. Azure Databricks: Azure Databricks is a powerful analytics platform that integrates well with Azure Data Factory. You can use it to debug and analyze Notebook activities within the Data Factory pipelines. Azure Databricks provides an interactive environment to run and debug notebooks, allowing you to inspect intermediate data, execute code step-by-step, and troubleshoot any issues.","timestamp":"1687381200.0","poster":"vctrhugo"},{"comment_id":"907570","content":"Selected Answer: DE\nD - Azure Data Factory\nE - Azure Databricks","upvote_count":"3","timestamp":"1685130540.0","poster":"janaki"},{"content":"You \"de-bug\" the activity with ML??? Seriously??? come on man??? from where you are getting these answers???","upvote_count":"2","comment_id":"907563","poster":"pavankr","timestamp":"1685128920.0"},{"timestamp":"1682840640.0","content":"Selected Answer: DE\nD & E are correct","upvote_count":"2","poster":"Mohamedali.Cintellic","comment_id":"884965"},{"poster":"vrodriguesp","timestamp":"1673887260.0","content":"Selected Answer: DE\nNotebook on azure databricks, rest on pipeline data factroy. No sense for AandC","upvote_count":"3","comment_id":"777944"},{"upvote_count":"4","poster":"nicky87654","content":"Selected Answer: DE\nWrangling and Copy = ADF\nJar and Notbooks = Databricks","comment_id":"765128","timestamp":"1672792440.0"},{"timestamp":"1660379100.0","comment_id":"646221","poster":"Deeksha1234","content":"Selected Answer: DE\nshould be DE","upvote_count":"2"},{"content":"Couldn't be AD?","upvote_count":"1","comment_id":"645801","poster":"martinamartina","timestamp":"1660291140.0"},{"content":"Selected Answer: DE\nDE - correct","upvote_count":"1","poster":"dsp17","comment_id":"632146","timestamp":"1657973280.0"},{"content":"Selected Answer: DE\nWrangling and Copy -> ADF\nJar and Notbooks -> Databricks","comment_id":"631449","poster":"dsp17","timestamp":"1657815720.0","upvote_count":"1"},{"timestamp":"1656743940.0","comment_id":"625981","content":"Selected Answer: DE\nabsolutely D&E","poster":"Remedios79","upvote_count":"2"},{"timestamp":"1656743880.0","content":"D and E absolutely!!","upvote_count":"1","comment_id":"625980","poster":"Remedios79"},{"content":"Selected Answer: DE\nAnswer is D&E","poster":"VenkataPolepalli","timestamp":"1653971460.0","upvote_count":"2","comment_id":"609540"},{"upvote_count":"2","content":"Selected Answer: DE\nNotebook, jar: databriks\nManaging activities/pipelines: ADF","comment_id":"598675","poster":"Lucky_me","timestamp":"1652037480.0"},{"timestamp":"1647694620.0","upvote_count":"1","comment_id":"571053","content":"Selected Answer: AC\nAzure synapse analytics já contempla o ADF e ADB.","poster":"rafaelptu"},{"poster":"kanak01","timestamp":"1644325020.0","upvote_count":"1","content":"Selected Answer: DE\nData Factory & Databricks","comment_id":"543042"},{"upvote_count":"1","content":"Selected Answer: DE\nD and E are correct. comments given by edba are valid.","comment_id":"528176","poster":"PallaviPatel","timestamp":"1642659120.0"},{"poster":"Raghu108","comment_id":"527856","upvote_count":"2","content":"Selected Answer: DE\nIt should be DE","timestamp":"1642621020.0"},{"comment_id":"512881","timestamp":"1640824140.0","upvote_count":"8","content":"I think answer should be D & E as the following reasons: 1. Data wangling is only supported by ADF not Synapse Analytics. 2.Jar activity requires Databricks. https://docs.microsoft.com/en-us/azure/data-factory/wrangling-overview https://docs.microsoft.com/en-us/azure/data-factory/transform-data-databricks-jar","poster":"edba"},{"timestamp":"1639266780.0","poster":"alexleonvalencia","upvote_count":"6","content":"Selected Answer: DE\nCreo la respuesta correcta es D/E","comment_id":"499718"}],"topic":"4"},{"id":"2ndLKqVMSzznrFPr9lKP","url":"https://www.examtopics.com/discussions/microsoft/view/74281-exam-dp-203-topic-4-question-24-discussion/","answer_ET":"D","answer":"D","timestamp":"2022-04-24 05:45:00","question_text":"You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a database named DB1. DB1 contains a fact table named Table1.\nYou need to identify the extent of the data skew in Table1.\nWhat should you do in Synapse Studio?","exam_id":67,"choices":{"D":"Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats.","A":"Connect to the built-in pool and run sys.dm_pdw_nodes_db_partition_stats.","B":"Connect to Pool1 and run DBCC CHECKALLOC.","C":"Connect to the built-in pool and run DBCC CHECKALLOC."},"question_id":318,"answer_description":"","unix_timestamp":1650771900,"answer_images":[],"answers_community":["D (86%)","14%"],"question_images":[],"discussion":[{"comments":[{"comment_id":"994952","upvote_count":"3","content":"Question 8 topic 4","timestamp":"1693472460.0","poster":"kkk5566"}],"poster":"Lotusss","timestamp":"1650771900.0","content":"Correct. See Question 12 topic 4","upvote_count":"10","comment_id":"590866"},{"upvote_count":"1","poster":"f7c717f","content":"Selected Answer: D\nBecause \"This is correct because querying sys.dm_pdw_nodes_db_partition_stats in the dedicated SQL pool (Pool1) will provide information about the distribution of data across the nodes and partitions, which is essential for identifying data skew in Table1.\"","comment_id":"1321100","timestamp":"1733173740.0"},{"comment_id":"994951","poster":"kkk5566","content":"Selected Answer: D\nCorrect. See Question 12 topic 4","timestamp":"1693472400.0","upvote_count":"1"},{"comment_id":"980873","poster":"Matt2000","timestamp":"1692023280.0","upvote_count":"1","content":"Does sys.dm_pdw_nodes_db_partition_stats exist? I found, however, found a reference for sys.dm_db_partition_stats that seems to do the trick."},{"comment_id":"942203","timestamp":"1688416080.0","upvote_count":"2","poster":"niaspa","content":"Selected Answer: D\nD .See Question 12 topic 4"},{"upvote_count":"2","timestamp":"1687381620.0","content":"Selected Answer: D\nD. Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats.\n\nBy connecting to Pool1, which represents the dedicated SQL pool, and querying the sys.dm_pdw_nodes_db_partition_stats system view, you can obtain information about the distribution of data across the compute nodes in the SQL pool. This view provides details on the number of rows and the size of data partitions on each node, allowing you to identify any significant data skew in Table1.","poster":"vctrhugo","comment_id":"929932"},{"comment_id":"911493","timestamp":"1685557020.0","upvote_count":"1","content":"D correct","poster":"bulutfet"},{"timestamp":"1685130060.0","comments":[{"poster":"janaki","content":"Sorry, option D is correct.","comment_id":"907568","upvote_count":"3","timestamp":"1685130300.0"},{"content":"How could you use built-in (serverless) to query dedicated pool?","poster":"vctrhugo","upvote_count":"2","timestamp":"1687381560.0","comment_id":"929931"}],"comment_id":"907567","upvote_count":"1","content":"Selected Answer: A\nOption A is correct","poster":"janaki"},{"content":"correct","upvote_count":"3","timestamp":"1660379160.0","poster":"Deeksha1234","comment_id":"646223"}],"topic":"4","isMC":true},{"id":"dOZ5ua34wtrLeBgXqCwD","answer_description":"","choices":{"B":"Cache used percentage","D":"CPU percentage","C":"Data IO percentage","A":"Local tempdb percentage"},"discussion":[{"content":"Selected Answer: B\nIs correct","poster":"juanlu46","comments":[{"content":"why it is not D,C?","comment_id":"1321102","timestamp":"1733174040.0","poster":"f7c717f","upvote_count":"1"}],"timestamp":"1650907860.0","upvote_count":"7","comment_id":"591883"},{"content":"Selected Answer: B\nrepeted","upvote_count":"1","comment_id":"994953","poster":"kkk5566","timestamp":"1693472580.0"},{"poster":"vctrhugo","comment_id":"929934","upvote_count":"2","content":"Selected Answer: B\nRepeated question.","timestamp":"1687381680.0"},{"poster":"StudentFromAus","upvote_count":"4","content":"Selected Answer: B\nFor already used queries, we need to monitor the adaptive caching","comment_id":"623519","timestamp":"1656369120.0"}],"timestamp":"2022-04-25 19:31:00","topic":"4","question_text":"You manage an enterprise data warehouse in Azure Synapse Analytics.\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\nYou need to monitor resource utilization to determine the source of the performance issues.\nWhich metric should you monitor?","url":"https://www.examtopics.com/discussions/microsoft/view/74509-exam-dp-203-topic-4-question-25-discussion/","answer_ET":"B","answer_images":[],"answers_community":["B (100%)"],"question_id":319,"answer":"B","exam_id":67,"question_images":[],"isMC":true,"unix_timestamp":1650907860},{"id":"SNzAOKIeGZu7Ye8xqXhn","timestamp":"2022-04-25 19:31:00","exam_id":67,"topic":"4","answer_description":"","answers_community":["D (100%)"],"answer_ET":"D","isMC":true,"answer":"D","url":"https://www.examtopics.com/discussions/microsoft/view/74510-exam-dp-203-topic-4-question-26-discussion/","answer_images":[],"discussion":[{"comment_id":"591884","content":"Selected Answer: D\nCorrect!","timestamp":"1666719060.0","upvote_count":"7","poster":"juanlu46"},{"content":"Selected Answer: D\nI found this question on my exam 30/04/2024, and I put D. I passed the exam with a high score, but I'm not sure if the answer is correct.","poster":"Alongi","timestamp":"1730724480.0","comment_id":"1206463","upvote_count":"2"},{"poster":"kkk5566","content":"Selected Answer: D\nrepeted","upvote_count":"1","comment_id":"994955","timestamp":"1709204640.0"},{"timestamp":"1703200080.0","comment_id":"929936","content":"Selected Answer: D\nIf you see anything above 45 days involving logs on ADF, it won't be ADF itself.","poster":"vctrhugo","upvote_count":"4"},{"comment_id":"787146","poster":"Jerrie86","timestamp":"1690241040.0","upvote_count":"1","content":"Asking to monitor Pipeline failures and D is activity runs. so Cant be D. Looks like they are missing an answer here"},{"poster":"dom271219","timestamp":"1677598440.0","content":"Selected Answer: D\nRedundant question","upvote_count":"1","comment_id":"652954"},{"comment_id":"646224","timestamp":"1676284080.0","content":"Selected Answer: D\ncorrect","poster":"Deeksha1234","upvote_count":"3"}],"question_id":320,"unix_timestamp":1650907860,"question_text":"You have an Azure data factory.\nYou need to examine the pipeline failures from the last 180 days.\nWhat should you use?","question_images":[],"choices":{"B":"Pipeline runs in the Azure Data Factory user experience","C":"the Resource health blade for the Data Factory resource","A":"the Activity log blade for the Data Factory resource","D":"Azure Data Factory activity runs in Azure Monitor"}}],"exam":{"provider":"Microsoft","name":"DP-203","isImplemented":true,"id":67,"isMCOnly":false,"numberOfQuestions":384,"lastUpdated":"12 Apr 2025","isBeta":false},"currentPage":64},"__N_SSP":true}