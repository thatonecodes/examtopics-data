{"pageProps":{"questions":[{"id":"AZ7zAqGq3SH6r9djuQfe","exam_id":67,"topic":"1","question_text":"You build a data warehouse in an Azure Synapse Analytics dedicated SQL pool.\nAnalysts write a complex SELECT query that contains multiple JOIN and CASE statements to transform data for use in inventory reports. The inventory reports will use the data and additional WHERE parameters depending on the report. The reports will be produced once daily.\nYou need to implement a solution to make the dataset available for the reports. The solution must minimize query times.\nWhat should you implement?","answer":"B","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/67460-exam-dp-203-topic-1-question-25-discussion/","isMC":true,"answer_images":[],"unix_timestamp":1639088640,"answers_community":["B (97%)","3%"],"question_images":[],"discussion":[{"poster":"ANath","timestamp":"1644269640.0","upvote_count":"27","content":"B is correct.\n\nMaterialized view and result set caching\n\nThese two features in dedicated SQL pool are used for query performance tuning. Result set caching is used for getting high concurrency and fast response from repetitive queries against static data.\n\nTo use the cached result, the form of the cache requesting query must match with the query that produced the cache. In addition, the cached result must apply to the entire query.\n\nMaterialized views allow data changes in the base tables. Data in materialized views can be applied to a piece of a query. This support allows the same materialized views to be used by different queries that share some computation for faster performance.","comment_id":"542689"},{"poster":"Canary_2021","timestamp":"1639584600.0","comment_id":"502322","content":"Selected Answer: B\nB is the correct answer.\nA materialized view is a database object that contains the results of a query. A materialized view is not simply a window on the base table. It is actually a separate object holding data in itself. So query data against a materialized view with different filters should be quick.\nDifference Between View and Materialized View: \nhttps://techdifferences.com/difference-between-view-and-materialized-view.html","upvote_count":"14"},{"poster":"20b1837","comment_id":"1400946","content":"Selected Answer: A\nB is incorrect. A materialized view MUST contains one of the following a select that contains an aggregate function and/or a Group by clause. See the below article\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-materialized-view-as-select-transact-sql?toc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Ftoc.json&bc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Fbreadcrumb%2Ftoc.json&view=azure-sqldw-latest&preserve-view=true","upvote_count":"1","timestamp":"1742459400.0"},{"timestamp":"1735930800.0","upvote_count":"1","poster":"hypersam","comment_id":"1336119","content":"Selected Answer: B\nalso why C is incorrect: \n\"When cached results are used: There is an exact match between the new query and the previous query that generated the result set cache.\"\nbut in our case we need additional WHERE clause\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching#when-cached-results-are-used"},{"upvote_count":"1","content":"Selected Answer: B\nCorrect Answer: B","comment_id":"1318456","timestamp":"1732688220.0","poster":"EmnCours"},{"upvote_count":"1","content":"Selected Answer: B\nAs materalized views stores data. we can write once and can read from it mutiple times","poster":"ff5037f","timestamp":"1729799460.0","comment_id":"1302640"},{"poster":"ELJORDAN23","timestamp":"1705594920.0","comment_id":"1126042","upvote_count":"3","content":"Selected Answer: B\nGot this question on my exam on january 17, B is correct"},{"upvote_count":"2","poster":"Joanna0","comment_id":"1117865","timestamp":"1704836760.0","content":"Selected Answer: B\nMaterialized Views:\n\nCreate materialized views that store the results of the complex SELECT queries. Materialized views are precomputed views stored as tables, and they can significantly reduce query times by avoiding the need to recompute the results every time the query is executed."},{"upvote_count":"1","comment_id":"1112654","content":"A. an ordered clustered columnstore index => this will impact the where clause so it is a valid option\nB. a materialized => can't be used since there is no aggregation\nC. result set caching => We don't know if the output query respects the limitation (10 gb ) so no\nD. a replicated table => sizes of lookups tables not mentioned so even if it is a possible solution it's not a suggested approach","timestamp":"1704277440.0","poster":"ll94"},{"upvote_count":"4","poster":"phydev","timestamp":"1698761100.0","content":"Selected Answer: B\nWas on my exam today (31.10.2023).","comment_id":"1058850"},{"comment_id":"997592","timestamp":"1693743120.0","content":"Selected Answer: B\nMaterialized view","upvote_count":"1","poster":"kkk5566"},{"timestamp":"1673020260.0","content":"There is no information that this query aggregates data.\n\n\"SELECT list in the materialized view definition needs to meet at least one of these two criteria:\n\nThe SELECT list contains an aggregate function.\nGROUP BY is used in the Materialized view definition and all columns in GROUP BY are included in the SELECT list. Up to 32 columns can be used in the GROUP BY clause.\"\n\nI'm not sure that B is correct answer.\nUnfortunately I cannot see better answer","comment_id":"767856","comments":[{"content":"A properly designed materialized view provides :\nReduce the execution time for complex queries with JOINs and aggregate functions. The more complex the query, the higher the potential for execution-time saving. The most benefit is gained when a query's computation cost is high and the resulting data set is small. So the right answer is B","upvote_count":"1","poster":"ck8.kakade","timestamp":"1722040140.0","comment_id":"1255924"}],"poster":"norbitek","upvote_count":"3"},{"timestamp":"1658678040.0","upvote_count":"2","content":"B is correct","comment_id":"636116","poster":"Deepshikha1228"},{"timestamp":"1656427200.0","upvote_count":"1","poster":"SKN0865","comment_id":"624026","content":"B is correct acc to: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-materialized-views\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-materialized-views"},{"upvote_count":"2","poster":"SandipSingha","content":"B materialized view","comment_id":"598858","timestamp":"1652074860.0"},{"content":"B is correct without a doubt","comment_id":"586984","poster":"Egocentric","timestamp":"1650148620.0","upvote_count":"2"},{"upvote_count":"1","comments":[{"content":"I was thinking on the same level initially but there are multiple tables informed and apply column store indexes an all tables would not ensure good results materialized view would store the results of complex calculations and it would be faster to just query those results i believe even if extra where clauses are applied","upvote_count":"3","comment_id":"621993","timestamp":"1656138900.0","poster":"uzairahm"}],"comment_id":"572358","poster":"DingDongSingSong","timestamp":"1647877680.0","content":"Why isn't the answer \"A\" when the query may have additional WHERE parameters depending on the report. That mean's the query isn't static and will change depending on the report. A clustered columstore index would provide a bettery query performance in case of a complex query where query isn't static."},{"poster":"PallaviPatel","content":"Selected Answer: B\nB correct.","timestamp":"1643274000.0","comment_id":"533621","upvote_count":"1"},{"comment_id":"513202","upvote_count":"3","timestamp":"1640857920.0","poster":"VeroDon","content":"Selected Answer: B\nCorrect"},{"content":"B is correct","upvote_count":"1","comment_id":"510333","timestamp":"1640612100.0","poster":"Mahesh_mm"},{"timestamp":"1639665180.0","poster":"bad_atitude","upvote_count":"2","content":"B materialized view","comment_id":"502983"},{"poster":"alexleonvalencia","upvote_count":"5","content":"Respuesta Correcta B, Una vista materializada.","comment_id":"498089","timestamp":"1639088640.0"}],"choices":{"A":"an ordered clustered columnstore index","B":"a materialized view","D":"a replicated table","C":"result set caching"},"timestamp":"2021-12-09 23:24:00","question_id":36,"answer_ET":"B"},{"id":"jy9TcolTt7zXPApKptWs","answer":"D","choices":{"B":"ORC","C":"JSON","D":"Parquet","A":"CSV"},"question_id":37,"discussion":[{"comment_id":"501430","content":"Both A and D are correct \n\"For each Spark external table based on Parquet or CSV and located in Azure Storage, an external table is created in a serverless SQL pool database. As such, you can shut down your Spark pools and still query Spark external tables from serverless SQL pool.\"","upvote_count":"24","poster":"KevinSames","timestamp":"1639493100.0"},{"timestamp":"1666341120.0","poster":"ZIMARAKI","content":"Selected Answer: D\n\"For each Spark external table based on Parquet or CSV and located in Azure Storage, an external table is created in a serverless SQL pool database. As such, you can shut down your Spark pools and still query Spark external tables from serverless SQL pool.\"\n\nSo A and D. Parquet are faster so D","comments":[{"upvote_count":"3","content":"But they never asked about faster, so why it cant be A","timestamp":"1668555660.0","comments":[{"comment_id":"780214","content":"In this business, time is money.","upvote_count":"19","timestamp":"1674059340.0","poster":"shakes103"}],"comment_id":"719185","poster":"Jiviify"}],"upvote_count":"5","comment_id":"700663"},{"poster":"EmnCours","upvote_count":"1","timestamp":"1732688340.0","comment_id":"1318457","content":"Selected Answer: D\nCorrect Answer: D"},{"timestamp":"1716456960.0","upvote_count":"1","content":"Selected Answer: D\nBoth A and D, but for priority i decide D","comment_id":"1216434","poster":"sergio_eduardo"},{"upvote_count":"2","content":"Selected Answer: D\nPrefer to D","timestamp":"1693743240.0","poster":"kkk5566","comment_id":"997595"},{"poster":"ExamWiner","content":"Selected Answer: D\nTipos de archivo adecuados para consultas analíticas\n- Si tiene cargas de trabajo basadas en Hive o Presto, vaya con ORC.\n- Si tiene cargas de trabajo basadas en Spark o Drill, vaya con Parquet.","comment_id":"923650","upvote_count":"3","timestamp":"1686795420.0"},{"upvote_count":"2","content":"Selected Answer: D\nboth A and D; D - Parquet is faster","timestamp":"1660491420.0","comment_id":"646811","poster":"Deeksha1234"},{"comment_id":"629574","poster":"jainparag1","content":"Option D as the explanation suggests. Parquet is always faster than CSV being columnar data store.","timestamp":"1657459920.0","upvote_count":"3"},{"content":"both A and D are okay, but Paraquet is faster than CSV, so answer is D.","timestamp":"1656142980.0","upvote_count":"2","comment_id":"622037","poster":"Dicer"},{"poster":"Rrk07","timestamp":"1653627000.0","comment_id":"607911","content":"Both A & D are correct , as the explanation also suggest same.","upvote_count":"2"},{"comments":[{"comment_id":"605546","upvote_count":"1","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/metadata/database","timestamp":"1653227640.0","poster":"RehanRajput"}],"comment_id":"605543","upvote_count":"2","content":"Both A and D","timestamp":"1653227400.0","poster":"RehanRajput"},{"content":"Selected Answer: D\nLooks correct to me","timestamp":"1651062900.0","upvote_count":"2","poster":"MatiCiri","comment_id":"593153"},{"upvote_count":"2","content":"JSON is also supported by Serverless SQL Pool but it is kinda complicated. Why is it not selected?","timestamp":"1647144300.0","poster":"AhmedDaffaie","comment_id":"566536"},{"comment_id":"554602","poster":"Ajitk27","upvote_count":"2","content":"Selected Answer: D\nLooks correct to me","timestamp":"1645630260.0"},{"comment_id":"550954","timestamp":"1645279500.0","upvote_count":"1","content":"Selected Answer: D\nCorrect","poster":"VijayMore"},{"comment_id":"533624","content":"Selected Answer: D\nBoth A and D are correct. as CSV and Parquet are correct answers.","timestamp":"1643274180.0","poster":"PallaviPatel","upvote_count":"1"},{"comment_id":"510382","upvote_count":"4","timestamp":"1640615580.0","content":"Parquet and CSV are correct","poster":"Mahesh_mm"},{"poster":"Nifl91","timestamp":"1639097940.0","content":"I think A and D are both correct answers.","comment_id":"498151","upvote_count":"3"},{"comment_id":"498087","content":"Respuesta Correcta Parquet.","upvote_count":"1","poster":"alexleonvalencia","timestamp":"1639088520.0"}],"question_images":[],"answer_images":[],"exam_id":67,"question_text":"You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1.\nYou plan to create a database named DB1 in Pool1.\nYou need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool.\nWhich format should you use for the tables in DB1?","timestamp":"2021-12-09 23:22:00","url":"https://www.examtopics.com/discussions/microsoft/view/67459-exam-dp-203-topic-1-question-26-discussion/","unix_timestamp":1639088520,"answer_description":"","topic":"1","answer_ET":"D","answers_community":["D (100%)"],"isMC":true},{"id":"EdLwbtb65bjCJaon1BkG","timestamp":"2021-12-09 23:26:00","answer":"D","unix_timestamp":1639088760,"question_text":"You are planning a solution to aggregate streaming data that originates in Apache Kafka and is output to Azure Data Lake Storage Gen2. The developers who will implement the stream processing solution use Java.\nWhich service should you recommend using to process the streaming data?","discussion":[{"content":"Correct!","poster":"Nifl91","upvote_count":"24","comment_id":"498162","timestamp":"1670634120.0"},{"content":"Selected Answer: A\nAnswer list is very strange. Except for JAR deployment in Databricks Job, there is almost no space for Java as a language. Scala is another syntax need training for regular java dev to adopt","poster":"davidkhala","comment_id":"1326050","upvote_count":"1","timestamp":"1734075360.0"},{"upvote_count":"2","timestamp":"1725365760.0","comment_id":"997596","content":"Selected Answer: D\nDataBricks with Java lang","poster":"kkk5566"},{"timestamp":"1718534820.0","upvote_count":"1","comment_id":"925073","content":"Selected Answer: D\nD is correct.","poster":"auwia"},{"comment_id":"837619","content":"D is the correct one for sure.","poster":"Mohitsain","timestamp":"1710302580.0","upvote_count":"2"},{"poster":"Fernando_Caemerer","content":"Selected Answer: D\nD is correct","upvote_count":"2","timestamp":"1702211820.0","comment_id":"740968"},{"timestamp":"1694680920.0","comment_id":"668788","content":"correct","poster":"Aaashu","upvote_count":"2"},{"upvote_count":"3","comment_id":"636132","timestamp":"1690215480.0","poster":"Deepshikha1228","content":"D is correct"},{"content":"Azure Databrics as the question is clearly asking the support for Java programming.","comment_id":"629575","upvote_count":"3","poster":"jainparag1","timestamp":"1688996040.0"},{"timestamp":"1681188960.0","poster":"NewTuanAnh","comments":[{"comment_id":"592487","poster":"Muishkin","content":"Yes Azure stream Analytics for streaming data?","timestamp":"1682517420.0","upvote_count":"1"},{"content":"I see, Azure Stream Analytics does not associate with Java","poster":"NewTuanAnh","timestamp":"1681189020.0","comments":[{"poster":"sdokmak","timestamp":"1684836000.0","upvote_count":"1","content":"or databricks","comments":[{"comment_id":"605963","poster":"sdokmak","content":"kafka*","timestamp":"1684836060.0","upvote_count":"2"}],"comment_id":"605962"}],"comment_id":"584055","upvote_count":"2"}],"upvote_count":"1","content":"why not C: Azure Stream Analytics?","comment_id":"584053"},{"content":"Selected Answer: D\ncorrect.","upvote_count":"2","poster":"PallaviPatel","timestamp":"1674810300.0","comment_id":"533626"},{"upvote_count":"3","content":"Answer is correct","comment_id":"510341","timestamp":"1672148520.0","poster":"Mahesh_mm"},{"content":"Respuesta correcta Azure DataBricks.","timestamp":"1670624760.0","comment_id":"498090","poster":"alexleonvalencia","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/microsoft/view/67461-exam-dp-203-topic-1-question-27-discussion/","question_images":[],"answer_images":[],"exam_id":67,"answers_community":["D (88%)","13%"],"question_id":38,"choices":{"C":"Azure Stream Analytics","A":"Azure Event Hubs","B":"Azure Data Factory","D":"Azure Databricks"},"answer_ET":"D","topic":"1","answer_description":"","isMC":true},{"id":"oBldoQimZ3WhjOJ46fgg","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/67462-exam-dp-203-topic-1-question-28-discussion/","answer_ET":"D","unix_timestamp":1639088940,"timestamp":"2021-12-09 23:29:00","choices":{"B":"Convert the files to Avro","D":"Merge the files","C":"Compress the files","A":"Convert the files to JSON"},"isMC":true,"question_id":39,"question_text":"You plan to implement an Azure Data Lake Storage Gen2 container that will contain CSV files. The size of the files will vary based on the number of events that occur per hour.\nFile sizes range from 4 KB to 5 GB.\nYou need to ensure that the files stored in the container are optimized for batch processing.\nWhat should you do?","answers_community":["D (75%)","13%","10%"],"answer":"D","answer_images":[],"exam_id":67,"question_images":[],"topic":"1","discussion":[{"comments":[{"comment_id":"925081","content":"Option B: Convert the files to Avro (WRONG FOR ME)\nWhile converting the files to Avro is a valid option for optimizing data storage and processing, it may not be the most suitable choice in this specific scenario. Avro is a binary serialization format that is efficient for compact storage and fast data processing. It provides schema evolution support and is widely used in big data processing frameworks like Apache Hadoop and Apache Spark.\n\nHowever, in the given scenario, the files are already in CSV format. Converting them to Avro would require additional processing and potentially introduce complexity. Avro is better suited for scenarios where data is generated or consumed by systems that natively support Avro or for cases where schema evolution is a critical requirement.\n\nOn the other hand, merging the files (Option D) is a more straightforward and common approach to optimize batch processing. It helps reduce the overhead associated with managing a large number of small files, improves data scanning efficiency, and enhances overall processing performance. Merging files is a recommended practice to achieve better performance and cost efficiency in scenarios where file sizes vary.","timestamp":"1686912900.0","poster":"auwia","upvote_count":"17"},{"comment_id":"1275697","timestamp":"1725125520.0","poster":"Bouhdy","content":"Avro is often used when schema evolution and efficient serialization are needed, but merging files is the primary solution for optimizing batch processing when dealing with a large number of small files.\n\nASNWER IS D !","upvote_count":"1"},{"timestamp":"1651238760.0","comment_id":"594487","poster":"Massy","content":"I can understand why you say not merge, but why avro?","comments":[{"upvote_count":"2","comment_id":"664032","timestamp":"1662672300.0","poster":"anks84","content":"Because we need to ensure files stored in the container are optimized for batch processing. converting the files to AVRO would be suitable for optimized for batch processing. So, the answer is \"Convert to AVRO\""}],"upvote_count":"4"},{"poster":"bhrz","content":"The information about the file size is already given which is between 5KB to 5GB. So option D seems to be correct.","timestamp":"1663310760.0","upvote_count":"3","comment_id":"670549"},{"content":"hi , please clarify if we can conclude this as convert to avro as the right option? thank you very much!","upvote_count":"1","comment_id":"1098815","timestamp":"1702810260.0","poster":"SenMia"}],"timestamp":"1726815360.0","upvote_count":"51","comment_id":"513269","poster":"VeroDon","content":"You can not merge the files if u don't know how many files exist in ADLS2. In this case, you could easily create a file larger than 100 GB in size and decrease performance. so B is the correct answer. Convert to AVRO"},{"timestamp":"1726815360.0","comment_id":"514281","content":"Selected Answer: D\nIf you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256 MB to 100 GB in size). \nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#optimize-for-data-ingest","poster":"Canary_2021","upvote_count":"27"},{"timestamp":"1743087000.0","comment_id":"1410937","content":"Selected Answer: B\nI choose B,\n\nTo optimize the files stored in your Azure Data Lake Storage Gen2 container for batch processing, the best option is to convert the files to Avro (Option B). Here's why:\n\nAvro is a binary format that is highly efficient for both storage and processing. It supports schema evolution, which is beneficial for handling changes in data structure over time1.\nCompression: Avro files can be compressed, reducing storage costs and improving read/write performance.\nSplittable: Avro files are splittable, which means they can be processed in parallel, enhancing batch processing efficiency.\nWhile compressing the files (Option C) can also help reduce storage size and improve transfer speeds, converting to Avro provides additional benefits like schema support and better performance for large-scale data processing.","upvote_count":"1","poster":"Jolyboy"},{"content":"Selected Answer: D\nCopilot says Nerge the files","poster":"Januaz","timestamp":"1742461740.0","upvote_count":"1","comment_id":"1400970"},{"comment_id":"1356543","poster":"IMadnan","upvote_count":"1","content":"Selected Answer: D\nmerging the files (Option D)","timestamp":"1739556120.0"},{"content":"Selected Answer: D\nso Confusing for me C and D are seem correct for me","timestamp":"1737989760.0","upvote_count":"1","comment_id":"1347445","poster":"samianae"},{"upvote_count":"1","content":"Selected Answer: D\nLarger files lead to better performance and reduced costs.\n\nTypically, analytics engines such as HDInsight have a per-file overhead that involves tasks such as listing, checking access, and performing various metadata operations. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256 MB to 100 GB in size). Some engines and applications might have trouble efficiently processing files that are greater than 100 GB in size.\n\nIncreasing file size can also reduce transaction costs. Read and write operations are billed in 4 megabyte increments so you're charged for operation whether or not the file contains 4 megabytes or only a few kilobytes. For pricing information, see Azure Data Lake Storage pricing.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#file-size","poster":"RMK2000","timestamp":"1735669020.0","comment_id":"1334977"},{"content":"Selected Answer: D\nPour optimiser les fichiers CSV stockés dans Azure Data Lake Storage Gen2 pour le traitement par lots, la meilleure option est de fusionner les fichiers (option D).\n\nFusionner les fichiers permet de réduire le nombre de petits fichiers, ce qui améliore les performances de traitement par lots en réduisant la surcharge liée à la gestion de nombreux petits fichiers","upvote_count":"1","comment_id":"1319827","timestamp":"1732902660.0","poster":"moize"},{"comment_id":"1318460","poster":"EmnCours","content":"Selected Answer: D\nCorrect Answer: D","timestamp":"1732689360.0","upvote_count":"1"},{"content":"Selected Answer: D\nD. Merge the files\nExplanation:\nFile Merging: Merging small files into larger ones helps to reduce the overhead associated with processing many small files, which can negatively impact performance. Large files are typically more efficient for batch processing in distributed systems like Azure Data Lake and Azure Databricks because they minimize the number of tasks and the amount of metadata the system needs to manage.\n\nCompressing the Files (Option C Incorrect): While compression can reduce storage costs and improve I/O efficiency, it doesn't address the issue of small file sizes, which can still lead to inefficiencies in distributed processing.\n\nConverting to Avro (Option B Incorrect) or JSON (Option A Incorrect): Converting to a different file format like Avro or JSON could be beneficial for specific use cases, especially where schema evolution or specific query optimizations are required. However, this does not address the fundamental issue of optimizing file size for batch processing.\n\nTherefore, merging the files (Option D) is the most direct and effective way to optimize for batch processing in this scenario.","comment_id":"1268695","upvote_count":"1","timestamp":"1726815360.0","poster":"roopansh.gupta2"},{"upvote_count":"1","content":"Compress the files is my choise","comment_id":"1219993","timestamp":"1716873060.0","poster":"d39f475"},{"content":"pay attention to the \"number of events that occur per hour.\" That means it is streaming and you can group small files with streaming engines - it is D","comment_id":"1204403","upvote_count":"1","poster":"Dusica","timestamp":"1714463460.0"},{"comment_id":"1197808","poster":"f214eb2","upvote_count":"1","content":"Selected Answer: A\nConvert to AVRO is Legit","timestamp":"1713430800.0"},{"upvote_count":"1","timestamp":"1713143580.0","comment_id":"1195740","poster":"Charley92","content":"Selected Answer: C\nC. Compress the files\n\nCompression is beneficial for batch processing as it can significantly reduce the file size, which leads to faster transfer rates and can improve performance during batch processing tasks. It’s particularly effective for large files, making it easier to handle and process them efficiently"},{"timestamp":"1713117600.0","content":"option d","upvote_count":"1","poster":"da257c2","comment_id":"1195641"},{"upvote_count":"3","poster":"Khadija10","comment_id":"1136847","timestamp":"1706711520.0","content":"Selected Answer: C\nThe answer is compress the files\n- we can't merge because we don't know how many files we will receive, and the performance will be decreased if the size of the merged files is too large.\n- converting the files to Avro will require additional processing and if we receive too many files it may cause complexity. \n- compressing the CSV files is the best choice in our scenario, compressing the files is a common practice for optimizing batch processing. It helps reduce storage space, minimize data transfer times, and improve overall performance."},{"upvote_count":"5","timestamp":"1706045700.0","poster":"alphilla","content":"Crazy how OPTION D (which is wrong) has such a high ammount of votes. \nI'm going for C.compress files. Guys you don't know how many files you have, you can't design a system where this kind of randomness can make your system to fail.","comment_id":"1130064"},{"poster":"Joanna0","timestamp":"1704837840.0","upvote_count":"1","content":"Selected Answer: B\nBinary Serialization:\n\nAvro uses a compact binary format, making it more efficient in terms of storage and transmission compared to plain text formats like CSV. This can be advantageous for batch processing scenarios, especially when dealing with large volumes of data.\nSchema Evolution:\n\nAvro supports schema evolution, allowing you to change the schema of your data without requiring modifications to the entire dataset or affecting backward compatibility. This flexibility is beneficial in scenarios where your data schema may evolve over time.\nCompression:\n\nWhile Avro itself is a binary format that provides some level of compression, you can further enhance compression by applying additional compression algorithms. This is particularly useful when dealing with large files, and it helps to reduce storage costs and improve data transfer efficiency.","comment_id":"1117876"},{"poster":"ll94","comment_id":"1112673","content":"Selected Answer: C\nA. Convert the files to JSON => no sense\nB. Convert the files to Avro => my understanding is that the format of the file csv is given, so no\nC. Compress the files => for batch processing it's a win and it's the only option that you can assume true given the available information\nD. Merge the files => this can be true but not knowing how many files there is big issue","upvote_count":"5","timestamp":"1704278400.0"},{"upvote_count":"1","timestamp":"1703259240.0","poster":"jongert","comment_id":"1103457","content":"Selected Answer: B\nAVRO is binary format, so it will be optimized for batch processing. \nProblem with merging files is that it is still CSV, String typed which has to be parsed when processing later. Therefore, it would not qualify as being optimized for batch processing."},{"comment_id":"1103304","upvote_count":"1","content":"compress the files","timestamp":"1703240940.0","poster":"lisa710"},{"comment_id":"1098817","timestamp":"1702810440.0","upvote_count":"1","poster":"SenMia","content":"the confusing point in this question is that we will not know how many files are expected in an hour, if that's the case. will merging files really be helpful?"},{"timestamp":"1694763120.0","poster":"Moo925","content":"Selected Answer: C\ncompressing the CSV files is the most practical and efficient way to optimize them for batch processing, especially when dealing with varying file sizes","comment_id":"1008237","upvote_count":"2"},{"timestamp":"1693798920.0","upvote_count":"4","poster":"Ranjan6214","comment_id":"998167","content":"Selected Answer: D\nSometimes, data pipelines have limited control over the raw data, which has lots of small files. In general, we recommend that your system have some sort of process to aggregate small files into larger ones for use by downstream applications. If you're processing data in real time, you can use a real time streaming engine (such as Azure Stream Analytics or Spark Streaming) together with a message broker (such as Event Hubs or Apache Kafka) to store your data as larger files. As you aggregate small files into larger ones, consider saving them in a read-optimized format such as Apache Parquet for downstream processing.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#optimize-for-data-ingest"},{"timestamp":"1693743420.0","upvote_count":"1","comment_id":"997597","poster":"kkk5566","content":"Selected Answer: D\nShould be D"},{"upvote_count":"1","poster":"sidh_r","comment_id":"986480","content":"Selected Answer: B\nMerging might not be the best one since the size of files are varying, we wouldn't be able to come with a simple approach to merge. If we have to merge then again we would cause some overhead, hence converting to avro seems to be a best choice","timestamp":"1692620220.0"},{"timestamp":"1692422100.0","upvote_count":"1","content":"avro is correct ans. Merging might help when many small sized files are there however, it'll introduce one more challenge to handle schema variations. If too many large sized files need to merged that'll be difficult.","comment_id":"984978","poster":"vijay007123"},{"upvote_count":"1","comment_id":"972263","poster":"akhil5432","content":"Selected Answer: C\nopetion c","timestamp":"1691161380.0"},{"content":"Selected Answer: D\nD. Merge the files\nTo optimize the files stored in the Azure Data Lake Storage Gen2 container for batch processing, you should merge the files. Merging smaller files into larger files is a common optimization technique in data processing scenarios.\n\nHaving a large number of small files can introduce overhead in terms of file management, metadata processing, and data scanning. By merging the smaller files into larger files, you can reduce this overhead and improve the efficiency of batch processing operations.\n\nMerging the files is especially beneficial when dealing with varying file sizes, as it helps to create a more balanced distribution of data across the files and reduces the impact of small files on processing performance.\n\nTherefore, in this scenario, merging the files would be the recommended approach to optimize the files for batch processing.","timestamp":"1686912780.0","comment_id":"925080","poster":"auwia","upvote_count":"4"},{"upvote_count":"1","comment_id":"894958","poster":"mamahani","timestamp":"1683802440.0","content":"Selected Answer: D\nD: merge the files"},{"comment_id":"892807","upvote_count":"1","timestamp":"1683610740.0","poster":"janaki","content":"The best option is to compress the files. Azure Data Lake Storage Gen2 supports a variety of compression codecs, including GZIP, DEFLATE, and Snappy, which can help to reduce file sizes and improve batch processing times."},{"comment_id":"885646","poster":"steveo123","timestamp":"1682891340.0","content":"Selected Answer: D\n\"larger files lead to better performace\"","upvote_count":"1"},{"content":"Selected Answer: B\nAVRO files are highly compact and efficient, which makes them ideal for batch processing in distributed computing environments. They are also splittable, which means they can be easily divided into smaller chunks to be processed in parallel, further enhancing batch processing performance.","comment_id":"883217","poster":"rocky48","upvote_count":"2","timestamp":"1682654220.0"},{"poster":"Mohamedali.Cintellic","content":"Selected Answer: B\nAvro is better","timestamp":"1682617320.0","comment_id":"882896","upvote_count":"1"},{"comment_id":"882894","upvote_count":"1","content":"Selected Answer: B\nyou can't just merge blindly, Avro is more compressed version that's why it's B","poster":"Mohamedali.Cintellic","timestamp":"1682617260.0"},{"poster":"mamahani","comment_id":"870127","content":"Id go for merge files; i found interesting study on comparison of the data processing and computing files depending on their formats, and according to it \"In general computation of uncompressed data\nwas faster than compressed data with the exception of Json data\nIt is understandable why compressed data were slow to process as the data\nwill need to be uncompressed before processing and will therefore add additional overheads to the computation time\nAlthought uncompressed Abvro and CSV jobs were fast, the CSV \nappears to be the fastest; \"\nsince compression makes batch processing worse, it must be either convert to avro or merge; but since uncompressed csv performs better than uncompressed avro, it must be d, so merge files; here the full article for those interested in details https://www.researchgate.net/publication/288039494_Monitoring_WLCG_with_lambda-architecture_a_new_scalable_data_store_and_analytics_platform_for_monitoring_at_petabyte_scale","upvote_count":"6","timestamp":"1681468440.0"},{"upvote_count":"2","content":"chat gpt answer : \n\nTo optimize the files for batch processing in Azure Data Lake Storage Gen2, I recommend compressing the files (option C). Compressing the files will reduce their size, making them easier to transfer and process in batches, which is particularly important for larger files. Compression can be done using a variety of algorithms, such as Gzip or Snappy, which are supported by Azure Data Lake Storage Gen2.\n\nConverting the files to JSON (option A) or Avro (option B) is not necessary for optimizing the files for batch processing, as CSV files are already well-suited for batch processing. While JSON and Avro are also file formats that can be used for batch processing, they are not necessary in this case and may add unnecessary complexity.\n\nMerging the files (option D) may not be necessary either, as long as the files are small enough to be processed efficiently in batches. However, if the number of files is very large and processing them individually is too slow, merging them into larger files can be a good strategy to improve processing performance.","poster":"djangoo","comment_id":"858271","timestamp":"1680380340.0"},{"comment_id":"850659","poster":"rajaanto","upvote_count":"1","timestamp":"1679795460.0","content":"As per ChatGPT \"By converting CSV files to the Avro format, you can take advantage of these benefits and optimize your data for batch processing in Azure Data Lake Storage Gen2.\""},{"timestamp":"1678807320.0","content":"Selected Answer: C\nThe best option for optimizing CSV files stored in an Azure Data Lake Storage Gen2 container for batch processing would be to compress the files, option C.\nBatch processing typically involves processing large amounts of data at once, and reducing the size of the data can significantly improve processing times and reduce costs. Compressing the files can help reduce their size and optimize them for batch processing. The compression method used will depend on the specific requirements of the batch processing workload, but options such as gzip, bzip2, or Snappy can be effective for compressing CSV files.","comment_id":"838973","poster":"esaade","upvote_count":"1"},{"comments":[{"comment_id":"833333","poster":"Musafaynou","content":"For optimizing batch processing, compressing the CSV files can be an effective solution. By compressing the files, you can reduce the amount of data that needs to be read from or written to storage, which can improve performance and reduce costs. Additionally, compressing the files can help reduce network latency, which is especially important when working with large files.","timestamp":"1678305180.0","upvote_count":"1"}],"upvote_count":"2","timestamp":"1678118100.0","poster":"pravash02","content":"Why Can't option C -\n\nAs the most effective way to optimize the storage of CSV files in Azure Data Lake Storage Gen2 container for batch processing is to compress the files. Compressing the files reduces their size, which makes it easier and faster to transfer and process them in batches. This can also result in cost savings by reducing the amount of storage required.\n\nConverting the files to JSON or Avro is not necessarily the most effective solution as it would require additional processing to convert the files back to CSV format for batch processing. Merging the files could also cause issues with batch processing, as the larger file sizes could impact the performance of batch processing jobs.\n\nTherefore, compressing the files is the best option for optimizing storage and batch processing of CSV files in Azure Data Lake Storage Gen2 container.","comment_id":"830968"},{"timestamp":"1677511680.0","comments":[{"poster":"Musafaynou","timestamp":"1678305120.0","upvote_count":"1","comment_id":"833331","content":"the size of the files can vary widely and Avro files can be difficult to work with if their schema changes frequently."}],"comment_id":"823889","content":"Selected Answer: D\nFile size\nLarger files lead to better performance and reduced costs.\n\nTypically, analytics engines such as HDInsight have a per-file overhead that involves tasks such as listing, checking access, and performing various metadata operations. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256 MB to 100 GB in size). Some engines and applications might have trouble efficiently processing files that are greater than 100 GB in size.\n\nIncreasing file size can also reduce transaction costs. Read and write operations are billed in 4-megabyte increments so you're charged for operation whether or not the file contains 4 megabytes or only a few kilobytes.","upvote_count":"1","poster":"Simhamed2015"},{"timestamp":"1675535400.0","content":"Selected Answer: B\nAlthough merging would have been optimal, I will go with B as the question dictates that it wants to process in batch and Avro is optimized for batch.","comment_id":"798250","poster":"Anshuman_B","upvote_count":"1"},{"poster":"akshaynag95","content":"Selected Answer: D\nLarger files lead to better performance and reduced costs.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#optimize-for-data-ingest","timestamp":"1675488600.0","upvote_count":"1","comment_id":"797625"},{"poster":"akk_1289","timestamp":"1675053060.0","upvote_count":"1","comment_id":"792393","content":"Convert the files to Avro"},{"upvote_count":"1","comment_id":"783762","timestamp":"1674337920.0","content":"I believe it should be D( merge files) as per Microsoft guidelines","poster":"DindaS"},{"upvote_count":"1","comment_id":"758622","poster":"Yamarh","timestamp":"1672151280.0","content":"B is the answer. Avro supports batch and is very relevant for streaming. Avro stores its schema in JSON format making it easy to read and interpret by any program. Data itsel is stored in binary format by doing it compact and efficient."},{"poster":"Billybob0604","content":"Thing is that files can only be merged if their structure is the same. It doesn't say that anywhere meaning you just can't say wheter merge is an option. avro is common for stream processing","comment_id":"729539","upvote_count":"3","timestamp":"1669661940.0"},{"poster":"Rrk07","content":"Merge the files is correct answer","upvote_count":"1","timestamp":"1669482300.0","comment_id":"727702"},{"poster":"Selma97","content":"For me, batch brocessing means that the files are having the same structure and merge them is the best way, if the question is about optimizing for stream processing the correct answer would be B. But for batch processing the correct answer is D in my opinion.","upvote_count":"3","comment_id":"715040","timestamp":"1668068580.0"},{"content":"Option D\nFrom Microsoft page - \nFile size\n\nLarger files lead to better performance and reduced costs.\nTypically, analytics engines such as HDInsight have a per-file overhead that involves tasks such as listing, checking access, and performing various metadata operations. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256 MB to 100 GB in size). Some engines and applications might have trouble efficiently processing files that are greater than 100 GB in size.","timestamp":"1663991820.0","upvote_count":"1","comment_id":"677591","poster":"KilaspSG"},{"comment_id":"657174","content":"Selected Answer: D\nCorrect Answer: D\n\nIf you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256 MB to 100 GB in size).\n\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#optimize-for-data-ingest\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#file-size","poster":"proserv","timestamp":"1662106920.0","upvote_count":"2"},{"timestamp":"1661428860.0","upvote_count":"1","content":"Selected Answer: B\nB\nAvro is good file format for data exchange. It has a data storage which is very compact, fast and eficient for analytics. It supports batch and is very relevant for streaming.\n\nhttps://www.adaltas.com/en/2020/07/23/benchmark-study-of-different-file-format/","comment_id":"651783","poster":"yyyhhh"},{"poster":"Deeksha1234","timestamp":"1660491540.0","content":"Selected Answer: D\nD is correct","upvote_count":"2","comment_id":"646813"},{"comment_id":"639584","poster":"ROLLINGROCKS","upvote_count":"1","timestamp":"1659168180.0","content":"The question does not explicitly say that the files have the same structure, so we don't know a priori if the files can be merged."},{"upvote_count":"3","comment_id":"637420","poster":"Janisys","timestamp":"1658840340.0","content":"Selected answer: B\nCopy files in text (CSV) format from an on-premises file system and write to Azure Blob storage in Avro format. The source: https://docs.microsoft.com/en-us/azure/data-factory/copy-activity-overview"},{"comment_id":"636599","timestamp":"1658741520.0","poster":"Deepshikha1228","content":"It should be D","upvote_count":"1"},{"comment_id":"604948","upvote_count":"1","poster":"SAYAK7","content":"Selected Answer: D\nBatch can support JSON or AVRO, you should input one file by merging them all.","comments":[{"timestamp":"1653432000.0","comment_id":"606937","comments":[{"timestamp":"1653432060.0","comment_id":"606938","content":"B*, AVRO is faster than JSON","upvote_count":"1","poster":"sdokmak"}],"upvote_count":"1","poster":"sdokmak","content":"They're CSV so you're saying answer is A"}],"timestamp":"1653150600.0"},{"poster":"RehanRajput","timestamp":"1650705960.0","content":"Selected Answer: D\nYou need to make sure that the files in the container are optimized for BATCH PROCESSING. In case of batch processing it makes sense to merge files as to reduce the amount of IO Listing operations. \n\nB would have been correct if we had to optimize for stream processing.","comment_id":"590512","upvote_count":"6"},{"content":"Conversion makes an additional load, so not an good idea to convert into Avro rather than merging is easier","poster":"Karthikj18","timestamp":"1648990020.0","upvote_count":"1","comment_id":"580268"},{"upvote_count":"2","poster":"SebK","content":"Selected Answer: D\nmerge files","timestamp":"1647893520.0","comment_id":"572487"},{"timestamp":"1643706780.0","comment_id":"537746","upvote_count":"8","content":"This question makes me very confused.\nIt says the file size depends on the number of events per hour, so i guess there is a file generated every hour. In the worst case, we have 5GB * 24h, which is greater than 100GB...\nBut why is AVRO a good choice??","poster":"adfgasd"},{"comment_id":"533634","content":"Selected Answer: D\nmerge files is correct.","poster":"PallaviPatel","upvote_count":"3","timestamp":"1643275140.0"},{"poster":"vincetita","content":"Selected Answer: D\nSmall-sized files will hurt performance. Optimal file size: 256MB to 100GB","comment_id":"529003","upvote_count":"2","timestamp":"1642752300.0"},{"content":"Selected Answer: D\nIn my opinion for better batch processing files should be not bigger than 100GB but as big as possible.","poster":"Tomi1234","timestamp":"1640870400.0","comment_id":"513389","upvote_count":"8"},{"comment_id":"513267","upvote_count":"1","timestamp":"1640861640.0","poster":"VeroDon","content":"One example of batch processing is transforming a large set of flat, semi-structured CSV or JSON files into a schematized and structured format that is ready for further querying. Typically the data is converted from the raw formats used for ingestion (such as CSV) into binary formats that are more performant for querying because they store data in a columnar format, and often provide indexes and inline statistics about the data.\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/batch-processing"},{"poster":"edba","timestamp":"1640616000.0","comment_id":"510388","upvote_count":"5","content":"I think it shall be D as well. Please check the link below. https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#file-size"},{"timestamp":"1640613540.0","poster":"Mahesh_mm","comments":[{"timestamp":"1640791320.0","content":"Consider using the Avro file format in cases where your I/O patterns are more write heavy, or the query patterns favor retrieving multiple rows of records in their entirety. For example, the Avro format works well with a message bus such as Event Hub or Kafka that write multiple events/messages in succession.","poster":"SabaJamal2010AtGmail","comment_id":"512320","upvote_count":"1"}],"upvote_count":"2","comment_id":"510354","content":"B is correct"},{"poster":"didixuecoding","timestamp":"1640195760.0","upvote_count":"2","content":"Correct Answer should be D: Merge the files","comment_id":"507279","comments":[{"upvote_count":"2","poster":"corebit","content":"Please explain why it is D.","comment_id":"509904","timestamp":"1640562240.0"}]},{"content":"Respuesta correcta Avro","poster":"alexleonvalencia","comment_id":"498093","upvote_count":"1","timestamp":"1639088940.0"}]},{"id":"UHJZeyE8DzBqbUYEzm7B","timestamp":"2021-12-09 23:30:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0006500001.png","https://www.examtopics.com/assets/media/exam-media/04259/0006600001.png"],"answer_description":"Box 1: moved to cool storage -\nThe ManagementPolicyBaseBlob.TierToCool property gets or sets the function to tier blobs to cool storage. Support blobs currently at Hot tier.\n\nBox 2: container1/contoso.csv -\nAs defined by prefixMatch.\nprefixMatch: An array of strings for prefixes to be matched. Each rule can define up to 10 case-senstive prefixes. A prefix string must start with a container name.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.management.storage.fluent.models.managementpolicybaseblob.tiertocool","question_id":40,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0006700001.jpg"],"isMC":false,"answers_community":[],"exam_id":67,"discussion":[{"timestamp":"1671202860.0","upvote_count":"42","comment_id":"502997","comments":[{"comment_id":"503551","poster":"adfgasd","content":"why the .csv?","upvote_count":"3","timestamp":"1671271920.0","comments":[{"content":"It matches anything that starts with \"container1/contoso\" and the csv in the answer is the only one that matches.","timestamp":"1671633060.0","comment_id":"506192","upvote_count":"19","poster":"Lewistrick"}]}],"content":"correct","poster":"bad_atitude"},{"comment_id":"498094","timestamp":"1670625000.0","upvote_count":"8","poster":"alexleonvalencia","content":"Respuesta Cool Tier & Container1/contoso.csv"},{"poster":"kwokeric97","comment_id":"1078178","timestamp":"1732346760.0","comments":[{"timestamp":"1733071440.0","poster":"data_guy","comment_id":"1085355","upvote_count":"2","content":"It's not empty: the value \"30\" is on the next line"}],"content":"The tireToCool values is empty, should this rule being skipped? \nShould the data keep in hot storage at day 30, and being deleted at day 60? \n\n\"tierToCool\": {\n \"daysAfterModificationGreaterThan\":\n\nhttps://stackoverflow.com/questions/62368999/skip-rule-creation-if-parameter-is-empty-in-azure-storage-life-cycle-management","upvote_count":"1"},{"poster":"kkk5566","timestamp":"1725365940.0","content":"Move to Cool Tier & Container1/contoso.csv","comment_id":"997598","upvote_count":"3"},{"comment_id":"894280","content":"Move to cold storage is the right answer, but the statement, has to be \"files which are idle for more than 30 days have to be moved to a cold storage\", If the files get modified before the 30 day period, say weekly, it remains in the hot storage tier.","upvote_count":"1","timestamp":"1715366460.0","poster":"darth_vader_007"},{"upvote_count":"1","comment_id":"746716","content":"ah yes move to Cool storage and Container1/contoso.csv \nbecause prefixmatch is Container1/contoso","poster":"tembal","timestamp":"1702694340.0"},{"content":"Cool Tier & Container1/contoso.csv","poster":"Deeksha1234","comment_id":"646821","upvote_count":"2","timestamp":"1692027720.0"},{"timestamp":"1690278000.0","upvote_count":"2","poster":"Deepshikha1228","content":"given answer is correct","comment_id":"636605"},{"upvote_count":"1","content":"I think the responses do not match the question. There is no policy for cold storage here (it only says delete after 60 days) and as far as I know there is no such thing as a default duration for moving things to the cold storage if lifecycle is enabled\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-policy-configure?tabs=azure-portal","timestamp":"1688110380.0","comment_id":"625094","poster":"azure900test","comments":[{"upvote_count":"3","content":"sorry overlooked the cold tier policy, please ignore","comment_id":"625095","timestamp":"1688110500.0","poster":"azure900test"}]},{"upvote_count":"2","timestamp":"1673044740.0","content":"shouldn't the question be greater than 60 days?","comment_id":"518659","poster":"AJ01","comments":[{"comment_id":"534682","poster":"stunner85_","upvote_count":"4","timestamp":"1674908760.0","content":"The files get deleted after 60 days but after 30 days they are moved to the cool storage."}]},{"content":"correct","timestamp":"1672151760.0","upvote_count":"3","poster":"Mahesh_mm","comment_id":"510385"}],"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/67463-exam-dp-203-topic-1-question-29-discussion/","question_text":"HOTSPOT -\nYou store files in an Azure Data Lake Storage Gen2 container. The container has the storage policy shown in the following exhibit.\n//IMG//\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","topic":"1","answer":"","unix_timestamp":1639089000}],"exam":{"isImplemented":true,"provider":"Microsoft","isMCOnly":false,"isBeta":false,"name":"DP-203","lastUpdated":"12 Apr 2025","numberOfQuestions":384,"id":67},"currentPage":8},"__N_SSP":true}