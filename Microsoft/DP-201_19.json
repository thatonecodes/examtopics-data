{"pageProps":{"questions":[{"id":"R3bWigNv3BkvKGp2avwS","answer_images":[],"choices":{"C":"Azure Event Hubs","D":"Azure Data Lake Storage Gen2","A":"Azure Databricks","B":"Azure Data Factory"},"isMC":true,"answer_ET":"AC","answers_community":[],"unix_timestamp":1617440880,"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/48907-exam-dp-201-topic-2-question-3-discussion/","question_id":91,"answer_description":"You connect a data ingestion system with Azure Databricks to stream data into an Apache Spark cluster in near real-time. You set up data ingestion system using\nAzure Event Hubs and then connect it to Azure Databricks to process the messages coming through.\nNote: Azure Event Hubs is a highly scalable data streaming platform and event ingestion service, capable of receiving and processing millions of events per second. Event Hubs can process and store events, data, or telemetry produced by distributed software and devices. Data sent to an event hub can be transformed and stored using any real-time analytics provider or batching/storage adapters.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-databricks/databricks-stream-from-eventhubs","discussion":[{"content":"Correct answer.","comment_id":"329390","upvote_count":"11","timestamp":"1617692580.0","poster":"maynard13x8"},{"timestamp":"1621842960.0","poster":"cadio30","content":"Propose solution is correct","upvote_count":"2","comment_id":"365401"},{"comment_id":"327247","upvote_count":"1","poster":"bdloko","timestamp":"1617440880.0","content":"A: for realtime analysis.\nB: for stream injection."}],"exam_id":66,"timestamp":"2021-04-03 11:08:00","question_images":[],"answer":"AC","question_text":"You need to design a telemetry data solution that supports the analysis of log files in real time.\nWhich two Azure services should you include in the solution? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point."},{"id":"pOFe0o7oQgI2rwhEdS9X","exam_id":66,"topic":"2","unix_timestamp":1594719660,"timestamp":"2020-07-14 11:41:00","question_text":"You have a Windows-based solution that analyzes scientific data. You are designing a cloud-based solution that performs real-time analysis of the data.\nYou need to design the logical flow for the solution.\nWhich two actions should you recommend? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","url":"https://www.examtopics.com/discussions/microsoft/view/25694-exam-dp-201-topic-2-question-30-discussion/","answer":"CF","discussion":[{"content":"CF is the correct one, no confusion","upvote_count":"36","timestamp":"1596533880.0","poster":"anupit","comment_id":"150324"},{"content":"CF correct","timestamp":"1598007000.0","comment_id":"162883","upvote_count":"15","poster":"Arsa"},{"poster":"cadio30","comment_id":"366195","content":"As stated C and F are the correct solutions though if it is needed to configure it sequentially then F and C are the appropriate steps to accomplish it.\n\nEvent Hub > Azure Stream Analytics (Cloud) > Power BI (output for real time analysis)","timestamp":"1621926540.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1614514980.0","content":"we can't o/p azure data lake storage as we have a better option Power BI as we can run Power BI directly on ASA","comment_id":"300763","poster":"Deepu1987"},{"poster":"Deepu1987","upvote_count":"1","content":"The 1st step would be read option F & then option C in order to understand the logical flow \nD cannot be included as per the given scenario.","comment_id":"300761","timestamp":"1614514800.0"},{"upvote_count":"1","poster":"spiitr","comments":[{"upvote_count":"1","poster":"spiitr","comment_id":"292792","timestamp":"1613588220.0","content":"I guess these are two actions or steps part of same solution. If so CF is correct. First action is F and then perform C."},{"timestamp":"1614078840.0","upvote_count":"1","content":"Yes for real time analysis input through event hub and output to BI","comment_id":"297340","poster":"Sriniv"}],"content":"Why not CD? and how do you meet \"performs real-time analysis of the data.\" through event hub?","comment_id":"292789","timestamp":"1613588040.0"},{"content":"Why is the output to DLake v2 option wrong? Thanks","comments":[{"upvote_count":"1","content":"I think that data lake cannot be tread as last step to consume data, there would be needed a third step to consume data from DL. PowerBI visualization can be threated as final data consumption","poster":"maciejt","timestamp":"1617783360.0","comment_id":"330171"}],"upvote_count":"5","poster":"JMCun","comment_id":"265842","timestamp":"1610486160.0"},{"timestamp":"1595653680.0","comment_id":"143117","content":"The answer should be DF","upvote_count":"1","poster":"ExamW","comments":[{"upvote_count":"40","timestamp":"1595750880.0","poster":"ExamW","content":"Ignore this. CF is correct","comment_id":"143914"}]},{"comments":[{"upvote_count":"2","poster":"ExamW","timestamp":"1595653800.0","content":"https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-power-bi-dashboard It seems you can...","comment_id":"143118"},{"timestamp":"1602182820.0","content":"Direct Query works in real time when you have Power BI Premium subcription","upvote_count":"2","poster":"knightkkd","comment_id":"196290"}],"comment_id":"134725","poster":"Israel2","content":"Could it be D? Can you run Power BI directly on Streaming Analytics? Or do you need direct query for real time?","timestamp":"1594719660.0","upvote_count":"2"}],"answers_community":[],"answer_images":[],"question_images":[],"isMC":true,"answer_description":"Stream Analytics has first-class integration with Azure data streams as inputs from three kinds of resources:\n✑ Azure Event Hubs\n✑ Azure IoT Hub\n✑ Azure Blob storage\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs","question_id":92,"answer_ET":"CF","choices":{"C":"Use an Azure Stream Analytics job in the cloud. Ingress data from the Azure Event Hub instance and build queries that output to Power BI.","B":"Use an Azure Stream Analytics job on an edge device. Ingress data from an Azure Data Factory instance and build queries that output to Power BI.","D":"Use an Azure Stream Analytics job in the cloud. Ingress data from an Azure Event Hub instance and build queries that output to Azure Data Lake Storage.","F":"Send data from the application to an Azure Event Hub instance.","A":"Send data from the application to an Azure Stream Analytics job.","E":"Send data from the application to Azure Data Lake Storage."}},{"id":"lkspcIqxQfXWFGKsQY9K","exam_id":66,"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/46705-exam-dp-201-topic-2-question-31-discussion/","discussion":[{"timestamp":"1615829040.0","content":"NOT ANY MORE IN DP-201","upvote_count":"6","comments":[{"upvote_count":"1","timestamp":"1622907000.0","comment_id":"375286","content":"are you sure? how do you know?","poster":"jacint"}],"comment_id":"311612","poster":"H_S"},{"upvote_count":"5","content":"Beeline used for running hive queries not for monitoring","timestamp":"1615554780.0","poster":"akram786","comment_id":"308861"},{"upvote_count":"1","content":"Beeline or Livy for triggering the Spark job.\nAzure Logic Apps for monitoring and updating status on the company intranet.","comment_id":"1106098","poster":"dakku987","timestamp":"1703601480.0"},{"upvote_count":"1","timestamp":"1624703640.0","poster":"seby","content":"Guys, then which is the answer ?","comment_id":"391119"},{"upvote_count":"2","timestamp":"1615974360.0","poster":"Makar","comment_id":"313105","content":"answer is right https://livy.incubator.apache.org/"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0017900003.jpg"],"answers_community":[],"answer_description":"Box 1: Livy -\nYou can use Livy to run interactive Spark shells or submit batch jobs to be run on Spark.\n\nBox 2: Beeline -\nApache Beeline can be used to run Apache Hive queries on HDInsight. You can use Beeline with Apache Spark.\nNote: Beeline is a Hive client that is included on the head nodes of your HDInsight cluster. Beeline uses JDBC to connect to HiveServer2, a service hosted on your\nHDInsight cluster. You can also use Beeline to access Hive on HDInsight remotely over the internet.\nReference:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-livy-rest-interface https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-use-hive-beeline","question_text":"DRAG DROP -\nYou are designing a Spark job that performs batch processing of daily web log traffic.\nWhen you deploy the job in the production environment, it must meet the following requirements:\n✑ Run once a day.\n✑ Display status information on the company intranet as the job runs.\nYou need to recommend technologies for triggering and monitoring jobs.\nWhich technologies should you recommend? To answer, drag the appropriate technologies to the correct locations. Each technology may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","timestamp":"2021-03-12 14:13:00","answer_ET":"","question_id":93,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0018000001.jpg"],"isMC":false,"unix_timestamp":1615554780,"topic":"2"},{"id":"IEzd05lnGosAUBbO0ClZ","unix_timestamp":1619997900,"discussion":[{"upvote_count":"7","comment_id":"348111","content":"Correct.","timestamp":"1619997900.0","poster":"VG2007"},{"content":"B is the answer","upvote_count":"1","timestamp":"1635656640.0","comment_id":"470511","poster":"PHULU"}],"timestamp":"2021-05-03 01:25:00","answer":"B","choices":{"D":"Create a cluster policy in workspace1.","B":"Create a pool in workspace1.","C":"Configure a global init script for workspace1.","A":"Upgrade workspace1 to the Premium pricing tier."},"isMC":true,"question_text":"You have an Azure Databricks workspace named workspace1 in the Standard pricing tier. Workspace1 contains an all-purpose cluster named cluster1.\nYou need to reduce the time it takes for cluster1 to start and scale up. The solution must minimize costs.\nWhat should you do first?","topic":"2","question_images":[],"answer_images":[],"answers_community":[],"answer_description":"Databricks Pools increase the productivity of both Data Engineers and Data Analysts. With Pools, Databricks customers eliminate slow cluster start and auto- scaling times. Data Engineers can reduce the time it takes to run short jobs in their data pipeline, thereby providing better SLAs to their downstream teams.\nReference:\nhttps://databricks.com/blog/2019/11/11/databricks-pools-speed-up-data-pipelines.html","url":"https://www.examtopics.com/discussions/microsoft/view/51620-exam-dp-201-topic-2-question-32-discussion/","answer_ET":"B","question_id":94,"exam_id":66},{"id":"AltTr5ja1J3g8e3fyMWl","unix_timestamp":1616422560,"topic":"2","answers_community":[],"isMC":false,"answer_description":"Box 1: Azure Stream Analytics -\nUsers can now ingest, process, view, and analyze real-time streaming data into a table directly from a database in Azure SQL Database. They do so in the Azure portal using Azure Stream Analytics.\nIn the Azure portal, you can select an events source (Event Hub/IoT Hub), view incoming real-time events, and select a table to store events.\nStream Analytics leverages versioning of reference data to augment streaming data with the reference data that was valid at the time the event was generated.\nThis ensures repeatability of results.\n\nBox 2: Replay -\nReference data is versioned, enabling to always get the same results, even when we ג€replayג€ the stream.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/stream-data-stream-analytics-integration https://azure.microsoft.com/en-us/updates/additional-support-for-managed-identity-and-new-features-in-azure-stream-analytics/","answer_ET":"","question_text":"HOTSPOT -\nYou are designing a solution to process data from multiple Azure event hubs in near real-time.\nOnce processed, the data will be written to an Azure SQL database.\nThe solution must meet the following requirements:\n✑ Support the auditing of resource and data changes.\n✑ Support data versioning and rollback.\nWhat should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0018300001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/47947-exam-dp-201-topic-2-question-33-discussion/","exam_id":66,"question_id":95,"discussion":[{"comment_id":"317221","timestamp":"1616422560.0","content":"Azure streaming analytics and replay are about job recovery.\nI would definitely go for Azure Databricks and Delta.\n\nhttps://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html","poster":"obz","comments":[{"timestamp":"1623146820.0","poster":"hichemck","comment_id":"377409","upvote_count":"1","content":"Also azure stream analytics does not support multiple inputs"},{"comment_id":"366211","timestamp":"1621927860.0","content":"Agree with this solution. Better to compare the delta table of Azure Databricks against the Azure Stream Analytics","upvote_count":"2","poster":"cadio30"}],"upvote_count":"38"},{"comment_id":"342430","timestamp":"1619342940.0","upvote_count":"8","poster":"aditya_064","content":"Data is to be written to Azure SQL Database, Azure Databricks doesn't support it as a sink - https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing. Therefore ASA is the only otpion left along with Replay feature. Replay may not be the best method but since ASA has to be included this will do. Also Azure SQL Database can handle rollbacks and auditing by itself. So given solution is correct."},{"poster":"dakku987","timestamp":"1703608020.0","content":"Azure synpse analytics and delta\n\nAzure Service to Use: Azure Synapse Analytics (formerly SQL Data Warehouse):\n\nAzure Synapse Analytics is a comprehensive analytics service that brings together big data and data warehousing. It supports near real-time analytics and integrates with Azure SQL Database.\nFeature to Use: Delta:\n\nDelta is a feature associated with Azure Synapse Analytics that provides capabilities like versioning, change tracking, and rollback options. It is designed to handle data versioning and changes efficiently.","upvote_count":"1","comment_id":"1106210"},{"poster":"nefarious_smalls","upvote_count":"1","timestamp":"1652809680.0","comment_id":"603001","content":"Job Replay is simply about recovering your stream to its prior state in the case of a service upgrade or any manual intervention in which you stop the stream. It has nothing to do with versioning or rolling back to a specific version. That is Delta."},{"content":"azure databricks and delta tables. Databricks can use delta feature to load required snapshot. ADB can load data to sql db.","poster":"muni53","upvote_count":"1","timestamp":"1632225540.0","comment_id":"448886"},{"comments":[{"comment_id":"416776","content":"https://www.sqlshack.com/load-data-into-azure-sql-database-from-azure-databricks/","poster":"Tracy_Anderson","timestamp":"1627556400.0","upvote_count":"1"}],"timestamp":"1626161460.0","content":"Azure databricks and Delta \nWe can push data from databricks to azure sql db using jdbc driver.","comment_id":"405239","upvote_count":"2","poster":"zarga"},{"poster":"VG2007","upvote_count":"3","timestamp":"1619998260.0","comment_id":"348115","content":"Not sure what is the confusion.. refrence links are already given in and looks correct ..\neference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/stream-data-stream-analytics-integration https://azure.microsoft.com/en-us/updates/additional-support-for-managed-identity-and-new-features-in-azure-stream-analytics/"},{"poster":"niwe","content":"The given answer, I think i correct\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-concepts-checkpoint-replay","comment_id":"337430","upvote_count":"2","comments":[{"timestamp":"1618799100.0","comment_id":"338558","upvote_count":"2","poster":"KRV","content":"I think the questions talks about data versioning and auditing data and date related changes and not about the job recovery , and hence I think Azure Databricks and Delta happens to be logically correct with corelation to the question asked for Data changes and not job related changes or recovery"}],"timestamp":"1618647960.0"}],"timestamp":"2021-03-22 15:16:00","answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0018200001.png"]}],"exam":{"numberOfQuestions":206,"isBeta":false,"name":"DP-201","lastUpdated":"12 Apr 2025","provider":"Microsoft","isImplemented":true,"isMCOnly":false,"id":66},"currentPage":19},"__N_SSP":true}