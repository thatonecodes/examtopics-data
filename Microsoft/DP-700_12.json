{"pageProps":{"questions":[{"id":"FBAV9Nvrw7orpSlYjiMo","exam_id":72,"question_text":"HOTSPOT -\nYou are building a data loading pattern for Fabric notebook workloads.\nYou have the following code segment:\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\n//IMG//","discussion":[{"poster":"8d6881f","timestamp":"1738161540.0","comment_id":"1348600","upvote_count":"9","content":"No No Yes.\nIf anything goes wrong writing the existing table or when writing a new table (.targettable) the merge will not be executed."},{"poster":"MultiCloudIronMan","timestamp":"1741357740.0","comment_id":"1366286","content":"NNY is correct","upvote_count":"3"},{"comment_id":"1352881","upvote_count":"3","timestamp":"1738913040.0","content":"No Yes Yes\nMerge operation always runs, the table is not overwritten if condition not matched","poster":"prabhjot"},{"content":"No; Yes: Yes\nMerge operation will always run, the matching_condition is part of the merge operation. But target table may not be overwritten if condition not met.","upvote_count":"3","comment_id":"1346382","poster":"4371883","timestamp":"1737793620.0"}],"question_images":["https://img.examtopics.com/dp-700/image39.png","https://img.examtopics.com/dp-700/image40.png"],"isMC":false,"unix_timestamp":1737793620,"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/155370-exam-dp-700-topic-2-question-11-discussion/","timestamp":"2025-01-25 09:27:00","topic":"2","answers_community":[],"answer_description":"","question_id":56,"answer_images":["https://img.examtopics.com/dp-700/image41.png"],"answer":""},{"id":"scT0Wx32WCtjKXXkHPSm","discussion":[{"poster":"4371883","content":"Agree with the answers","timestamp":"1737794280.0","upvote_count":"8","comment_id":"1346383"},{"timestamp":"1737112980.0","poster":"werfragt","comment_id":"1342110","content":"Is this correct? I would choose >= and <=.","upvote_count":"6","comments":[{"comments":[{"upvote_count":"1","comments":[{"content":"But don't you think, customer table are a dimensional table with a repeated customerId(non unique customerId)? that's why we need these conditions.","timestamp":"1742885580.0","poster":"d98a23f","comment_id":"1409918","upvote_count":"1","comments":[{"comment_id":"1409920","poster":"d98a23f","upvote_count":"1","content":"I mean, the customer need to be valid To datetime","timestamp":"1742886000.0"}]}],"comment_id":"1361565","poster":"DirectX","timestamp":"1740512940.0","content":"Of course that's not going to happen. Each order is linked to a customer (o.CustomerId = c.CustomerId). And new customer and old customer has different CustomerId value.\nThe answer is: >= and <="}],"content":"No, the order has to be *before* the 'new costumer' takes effect. Otherwise it could belong to two customers","comment_id":"1348605","poster":"8d6881f","upvote_count":"2","timestamp":"1738161960.0"}]}],"isMC":false,"unix_timestamp":1737112980,"question_text":"HOTSPOT -\nYou have a Fabric workspace that contains two lakehouses named Lakehouse1 and Lakehouse2. Lakehouse1 contains staging data in a Delta table named Orderlines. Lakehouse2 contains a Type 2 slowly changing dimension (SCD) dimension table named Dim_Customer.\nYou need to build a query that will combine data from Orderlines and Dim_Customer to create a new fact table named Fact_Orders. The new table must meet the following requirements:\nEnable the analysis of customer orders based on historical attributes.\nEnable the analysis of customer orders based on the current attributes.\nHow should you complete the statement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n//IMG//","answers_community":[],"exam_id":72,"answer_ET":"","question_id":57,"topic":"2","question_images":["https://img.examtopics.com/dp-700/image42.png"],"answer_images":["https://img.examtopics.com/dp-700/image43.png"],"answer_description":"","timestamp":"2025-01-17 12:23:00","url":"https://www.examtopics.com/discussions/microsoft/view/154783-exam-dp-700-topic-2-question-12-discussion/","answer":""},{"id":"qSBffQ22o6lVBANPwZAD","unix_timestamp":1734104580,"answer_images":[],"question_id":58,"answer_description":"","question_text":"You have a Fabric workspace that contains a lakehouse named Lakehouse1.\nIn an external data source, you have data files that are 500 GB each. A new file is added every day.\nYou need to ingest the data into Lakehouse1 without applying any transformations. The solution must meet the following requirements\nTrigger the process when a new file is added.\nProvide the highest throughput.\nWhich type of item should you use to ingest the data?","answer":"D","answer_ET":"D","choices":{"C":"Streaming dataset","B":"Dataflow Gen2","A":"Eventstream","D":"Data pipeline"},"question_images":[],"discussion":[{"content":"Selected Answer: D\nEventstream is designed for ingesting real-time or streaming data from sources like IoT devices or logs. Itâ€™s not optimized for batch processing or large files.","poster":"IshtarSQL","comment_id":"1326178","timestamp":"1734104580.0","upvote_count":"8"},{"content":"Selected Answer: D\nD. Data pipeline.\n\nData pipelines are designed to handle large volumes of data efficiently and can be configured to trigger the ingestion process automatically when new files are added to the external data source. They also provide high throughput, making them suitable for handling 500 GB files daily without applying any transformations.","poster":"fassil","timestamp":"1740671040.0","comment_id":"1362588","upvote_count":"3"},{"timestamp":"1739772000.0","comment_id":"1357604","upvote_count":"1","poster":"henryphchan","content":"Selected Answer: D\nI would prefer using data pipeline although it is a preview feature. Eventstream and streaming data are designed for realtime events"},{"comment_id":"1357283","content":"Selected Answer: D\nFor high-throughput, event-triggered ingestion of large files into a lakehouse without transformations, Data pipeline is the most appropriate and efficient item in Fabric.","upvote_count":"2","poster":"2e6975f","timestamp":"1739721480.0"},{"upvote_count":"2","content":"Selected Answer: C\nStreaming dataset is the only answer that ticks the requirements for storage trigger and high throughput.\nData Pipeline is not right as at 2025-Jan. The storage trigger is still in preview, so it doesn't satisfy the requirement. But it's probably the best option.","comment_id":"1346388","poster":"4371883","timestamp":"1737795480.0"}],"exam_id":72,"answers_community":["D (88%)","13%"],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/152972-exam-dp-700-topic-2-question-13-discussion/","topic":"2","timestamp":"2024-12-13 16:43:00"},{"id":"kN6IdCBLz6XhxPNEbI34","timestamp":"2025-01-26 02:49:00","answer_description":"","question_text":"You have a Fabric workspace that contains a lakehouse named Lakehouse1.\nIn an external data source, you have data files that are 500 GB each. A new file is added every day.\nYou need to ingest the data into Lakehouse1 without applying any transformations. The solution must meet the following requirements\nTrigger the process when a new file is added.\nProvide the highest throughput.\nWhich type of item should you use to ingest the data?","question_images":[],"answer_images":[],"discussion":[{"content":"Selected Answer: A\nsame as the previous question","comment_id":"1357607","upvote_count":"1","poster":"henryphchan","timestamp":"1739772060.0"},{"timestamp":"1739721660.0","comment_id":"1357285","poster":"2e6975f","upvote_count":"3","content":"Selected Answer: A\nFor high-throughput, event-triggered ingestion of large files into a lakehouse without transformations, Data pipeline is the most appropriate and efficient item in Fabric."},{"timestamp":"1738162320.0","content":"Selected Answer: A\nData Pipeline is not right as at 2025-Jan as the storage trigger is still in preview, so it doesn't satisfy the requirement. But it's probably the best option.","upvote_count":"1","poster":"8d6881f","comment_id":"1348609"}],"choices":{"D":"Dataflow Gen2","C":"KQL queryset","B":"Environment","A":"Data pipeline"},"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/155411-exam-dp-700-topic-2-question-14-discussion/","question_id":59,"answer_ET":"A","isMC":true,"topic":"2","answer":"A","exam_id":72,"unix_timestamp":1737856140},{"id":"Gkn2InFo7W3TB8sj6ZfI","answer_ET":"AD","answer":"AD","question_text":"You have a Fabric workspace that contains an eventhouse and a KQL database named Database1. Database1 has the following:\n\nA table named Table1 -\n\nA table named Table2 -\n\nAn update policy named Policy1 -\nPolicy1 sends data from Table1 to Table2.\nThe following is a sample of the data in Table2.\n\n//IMG//\n\nRecently, the following actions were performed on Table1:\nAn additional element named temperature was added to the StreamData column.\nThe data type of the Timestamp column was changed to date.\nThe data type of the DeviceId column was changed to string.\nYou plan to load additional records to Table2.\nWhich two records will load from Table1 to Table2? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","url":"https://www.examtopics.com/discussions/microsoft/view/152974-exam-dp-700-topic-2-question-15-discussion/","question_images":["https://img.examtopics.com/dp-700/image44.png"],"isMC":true,"discussion":[{"content":"Selected Answer: AD\nA and D have valid GUIds","timestamp":"1738913580.0","comment_id":"1352883","upvote_count":"7","poster":"prabhjot"},{"timestamp":"1734114360.0","comment_id":"1326216","content":"Selected Answer: BD\nRecord B loads because it conforms to the updated schema (string DeviceId, StreamData with temperature).\nRecord D loads because it conforms to the original schema (guid DeviceId, no temperature in StreamData).","comments":[{"upvote_count":"1","poster":"PetJoh422","timestamp":"1739440380.0","content":"This is incorrect because it was only updated in Table 1. So passing a string to table2 which requires a GUID will fail when it is not a valid guid.","comment_id":"1356052"}],"poster":"IshtarSQL","upvote_count":"5"},{"upvote_count":"1","poster":"Goye","content":"Selected Answer: AD\nPlease explain the answer","timestamp":"1741147920.0","comment_id":"1365257"},{"comments":[{"timestamp":"1741085940.0","poster":"DirectX","content":"I meant A and D. Answers with valid GUID","upvote_count":"1","comment_id":"1364863"}],"upvote_count":"1","timestamp":"1740513600.0","content":"B and D. \nBecause of GUID fields in the source data - The destination table schema has not been changed.\nThe third field with json data is probably of type nvarchar(max) which can accommodate any json structure.","comment_id":"1361570","poster":"DirectX"},{"upvote_count":"3","comment_id":"1350904","poster":"Golearn","content":"Selected Answer: AD\nA & C have valid GUIds in correct format and also have temperature column","comments":[{"timestamp":"1738660560.0","comment_id":"1351270","content":"A & D have valid GUIds","upvote_count":"1","poster":"Golearn"}],"timestamp":"1738591320.0"},{"poster":"18e18d0","content":"Selected Answer: AD\nRecords B and C will not load because of invalid GUIDs","timestamp":"1738331940.0","comment_id":"1349538","upvote_count":"3"}],"timestamp":"2024-12-13 19:26:00","question_id":60,"topic":"2","answer_description":"","choices":{"D":"","B":"","A":"","C":""},"answer_images":[],"exam_id":72,"unix_timestamp":1734114360,"answers_community":["AD (74%)","BD (26%)"]}],"exam":{"numberOfQuestions":97,"isImplemented":true,"name":"DP-700","provider":"Microsoft","lastUpdated":"12 Apr 2025","isBeta":false,"isMCOnly":false,"id":72},"currentPage":12},"__N_SSP":true}