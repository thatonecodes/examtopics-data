{"pageProps":{"questions":[{"id":"B9o2mHgCrALvdijBpzGA","exam_id":68,"timestamp":"2021-08-21 11:32:00","question_text":"HOTSPOT -\nFrom a website analytics system, you receive data extracts about user interactions such as downloads, link clicks, form submissions, and video plays.\nThe data contains the following columns:\n//IMG//\n\nYou need to design a star schema to support analytical queries of the data. The star schema will contain four tables including a date dimension.\nTo which table should you add each column? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0004600001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/60133-exam-dp-300-topic-1-question-14-discussion/","topic":"1","answer_description":"Box 1: DimEvent -\n\nBox 2: DimChannel -\nDimension tables describe business entities ג€\" the things you model. Entities can include products, people, places, and concepts including time itself. The most consistent table you'll find in a star schema is a date dimension table. A dimension table contains a key column (or columns) that acts as a unique identifier, and descriptive columns.\n\nBox 3: FactEvents -\nFact tables store observations or events, and can be sales orders, stock balances, exchange rates, temperatures, etc.\nReference:\nhttps://docs.microsoft.com/en-us/power-bi/guidance/star-schema","answer":"","discussion":[{"comment_id":"444841","poster":"Aggie0702","timestamp":"1631660400.0","upvote_count":"29","content":"Dimension tables support filtering and grouping, Fact tables support summarization, so the correct answer is: DimEvent, DimChannel, FactEvents."},{"comments":[{"content":"Now its correct but, its DP 203","upvote_count":"1","comment_id":"1239873","timestamp":"1719785280.0","poster":"Sr18"}],"timestamp":"1629738780.0","upvote_count":"11","comment_id":"430204","poster":"learnazureportal","content":"Answer is incorrect. The correct answer is: DimEvent, DimChannel, FactEvents"},{"upvote_count":"3","timestamp":"1668272880.0","content":"OLAP questions are not part of the DP-300 exam.","poster":"Ciupaz","comment_id":"716820"},{"content":"It's part of dp 300","poster":"gursimran_s","timestamp":"1654686180.0","upvote_count":"2","comment_id":"613197"},{"upvote_count":"9","timestamp":"1644303420.0","poster":"tesen_tolga","comment_id":"542895","content":"This is a DP-203 question."},{"comment_id":"505042","timestamp":"1639941120.0","upvote_count":"1","content":"Box 1: FactEvents - \nBox 2: DimChannel - \n \nReference: https://docs.microsoft.com/en-us/power-bi/guidance/star-schema","poster":"gt002"},{"comment_id":"503804","poster":"SQLHell","upvote_count":"3","timestamp":"1639761180.0","content":"Its not a DP-300 Question !!"},{"content":"is part of DP-300???","poster":"o2091","comment_id":"481204","timestamp":"1637277600.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1635330600.0","comment_id":"468495","content":"Another, can this be fixed please.","poster":"jerkyflexoff"},{"poster":"azure2022","timestamp":"1629698700.0","upvote_count":"5","content":"DimEvent\nDimChannel\nFactEvents","comment_id":"429698"},{"poster":"kamilsky","timestamp":"1629538320.0","upvote_count":"4","comment_id":"428592","content":"From my opinion 1 and 3 are opposite - Fact is number of events and Event category is DimEvent."}],"answers_community":[],"answer_ET":"","isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0004400001.png","https://www.examtopics.com/assets/media/exam-media/04275/0004500001.png"],"question_id":6,"unix_timestamp":1629538320},{"id":"yT9mOaKN6piAe7nFUeMe","question_text":"DRAG DROP -\nYou plan to create a table in an Azure Synapse Analytics dedicated SQL pool.\nData in the table will be retained for five years. Once a year, data that is older than five years will be deleted.\nYou need to ensure that the data is distributed evenly across partitions. The solutions must minimize the amount of time required to delete old data.\nHow should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all.\nYou may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","topic":"1","answer_description":"Box 1: HASH -\n\nBox 2: OrderDateKey -\nIn most cases, table partitions are created on a date column.\nA way to eliminate rollbacks is to use Metadata Only operations like partition switching for data management. For example, rather than execute a DELETE statement to delete all rows in a table where the order_date was in October of 2001, you could partition your data early. Then you can switch out the partition with data for an empty partition from another table.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool","url":"https://www.examtopics.com/discussions/microsoft/view/60921-exam-dp-300-topic-1-question-15-discussion/","unix_timestamp":1630170660,"answer":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0004800002.png"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0004800001.png"],"question_id":7,"timestamp":"2021-08-28 19:11:00","answers_community":[],"discussion":[{"poster":"learnazureportal","comments":[{"poster":"U_C","content":"I agree.\n\nHere is an example from MS:\n\nCREATE TABLE myTable ( \n l_orderkey bigint,\n l_partkey bigint,\n l_suppkey bigint,\n l_shipinstruct char(25), \n l_shipmode char(10), \n l_comment varchar(44)) \nWITH\n (\n DISTRIBUTION = HASH (l_orderkey), \n CLUSTERED COLUMNSTORE INDEX, \n PARTITION ( l_shipdate RANGE RIGHT FOR VALUES\n ( \n '1992-01-01','1992-02-01','1992-03-01','1992-04-01','1992-05-01',\n '1992-06-01','1992-07-01','1992-08-01','1992-09-01','1992-10-01',\n '1992-11-01','1992-12-01','1993-01-01','1993-02-01','1993-03-01'\n ))\n );","comments":[{"upvote_count":"4","comment_id":"460809","poster":"U_C","content":"FYI\n\nDISTRIBUTION = HASH ( distribution_column_name ) Assigns each row to one distribution by hashing the value stored in distribution_column_name. The algorithm is deterministic, which means it always hashes the same value to the same distribution. The distribution column should be defined as NOT NULL because all rows that have NULL are assigned to the same distribution.\n\nDISTRIBUTION = ROUND_ROBIN Distributes the rows evenly across all the distributions in a round-robin fashion. This behavior is the default for Azure Synapse Analytics.\n\nDISTRIBUTION = REPLICATE Stores one copy of the table on each Compute node. For Azure Synapse Analytics the table is stored on a distribution database on each Compute node. For Analytics Platform System (PDW), the table is stored in a SQL Server filegroup that spans the Compute node. This behavior is the default for Analytics Platform System (PDW).","timestamp":"1633988220.0"}],"timestamp":"1727327760.0","upvote_count":"5","comment_id":"460808"}],"upvote_count":"10","comment_id":"447344","content":"The provided answer is correct.","timestamp":"1726813260.0"},{"content":"DISTRIBUTION = ROUND_ROBIN Distributes the rows evenly across all the distributions in a round-robin fashion. This behavior is the default for Azure Synapse Analytics.","upvote_count":"6","poster":"Mladen_66","comments":[{"content":"distribution in a column just in hash","comment_id":"495277","timestamp":"1638807720.0","poster":"ramelas","upvote_count":"2"}],"comment_id":"434061","timestamp":"1630170660.0"},{"poster":"o2091","content":"is part of DP-300?","upvote_count":"3","comment_id":"481205","timestamp":"1637277600.0"},{"content":"from what i read \nfact = hash\ndimensions = round robin","comment_id":"469497","upvote_count":"2","timestamp":"1635462180.0","poster":"matongax"},{"timestamp":"1632743460.0","content":"(distribution_column_name) is only available in HASH\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7","comment_id":"452386","upvote_count":"5","poster":"captainpike"},{"content":"By default, tables are Round Robin distributed. This default makes it easy for users to start creating tables without having to decide how their tables should be distributed. Round Robin tables may perform sufficiently for some workloads. But, in most cases, a distribution column provides better performance.\n\nThe most common example of a table distributed by a column outperforming a Round Robin table is when two large fact tables are joined.","poster":"Aggie0702","comment_id":"444903","timestamp":"1631667780.0","upvote_count":"1"},{"timestamp":"1631664780.0","content":"A round-robin distributed table distributes table rows evenly across all distributions. The assignment of rows to distributions is random. Unlike hash-distributed tables, rows with equal values are not guaranteed to be assigned to the same distribution.","comment_id":"444868","upvote_count":"1","poster":"Aggie0702"},{"comment_id":"435517","upvote_count":"5","content":"Is this a DP-300 question?","timestamp":"1630328760.0","poster":"ovokpus","comments":[{"poster":"o2091","comment_id":"481200","timestamp":"1637277420.0","content":"I would say no, what do you think?","upvote_count":"2"}]},{"upvote_count":"3","poster":"maple580122","timestamp":"1630327380.0","comment_id":"435487","content":"The question requires to distribute evenly across \"partitions\". So it should be HASH."}],"isMC":false,"answer_ET":"","exam_id":68},{"id":"HnuG1m4uRVjsYPhelkzA","answer_description":"","topic":"1","question_images":[],"timestamp":"2021-10-01 21:03:00","answer_images":[],"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/63441-exam-dp-300-topic-1-question-16-discussion/","choices":{"C":"Parquet","A":"JSON","D":"ORC","B":"CSV"},"exam_id":68,"answer":"C","question_id":8,"discussion":[{"content":"Azure Synapse is not part of DP-300","poster":"SamBalbij","upvote_count":"11","comment_id":"457579","timestamp":"1664956440.0"},{"content":"Selected Answer: C\nanswer c","timestamp":"1744304220.0","upvote_count":"1","poster":"sincerebb","comment_id":"1559649"},{"timestamp":"1722510420.0","poster":"Socket","comment_id":"968984","content":"Selected Answer: C\nTo ensure that tables created in DB1 are available automatically as external tables to the built-in serverless SQL pool in Azure Synapse Analytics, you should use format C. Parquet.\n\nParquet is a columnar storage file format that is well-suited for big data processing frameworks like Apache Spark and is natively supported by Azure Synapse Analytics. When you create tables in DB1 using the Parquet format, the data can be directly accessed by the serverless SQL pool without requiring additional data movement or conversion.\n\nBy using Parquet as the format, you enable seamless integration between the Apache Spark pool (Pool1) and the serverless SQL pool, allowing both to work with the same data without any extra configuration or manual steps.\n\nOption A (JSON), Option B (CSV), and Option D (ORC) are valid file formats, but for the given scenario where you want to automatically access data from DB1 in the serverless SQL pool, Parquet is the most appropriate choice due to its native support in Azure Synapse Analytics.","upvote_count":"1"},{"upvote_count":"1","poster":"MS_KoolaidMan","comment_id":"904434","timestamp":"1716422340.0","content":"Selected Answer: C\nParquet"},{"content":"Actually , both B/CSV and C/Parquet are correct, because for each Spark external table based on Parquet or CSV and located in Azure Storage, an external table is created in a serverless SQL pool database.","comment_id":"620955","timestamp":"1687521960.0","poster":"Backy","upvote_count":"2"},{"timestamp":"1669921740.0","comment_id":"491883","content":"For each Spark external table based on Parquet or CSV and located in Azure Storage, an external table is created in a serverless SQL pool database. As such, you can shut down your Spark pools and still query Spark external tables from serverless SQL pool.","upvote_count":"2","poster":"CaptainJameson"},{"timestamp":"1666867020.0","upvote_count":"1","poster":"jerkyflexoff","comment_id":"468502","content":"Again Please fix this...."},{"comments":[{"poster":"learnazureportal","comment_id":"458015","timestamp":"1665027780.0","upvote_count":"5","content":"Yes, The Answer is correct."}],"comment_id":"455668","timestamp":"1664650980.0","upvote_count":"1","poster":"o2091","content":"Is it correct?"}],"question_text":"You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1.\nYou plan to create a database named DB1 in Pool1.\nYou need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool.\nWhich format should you use for the tables in DB1?","isMC":true,"unix_timestamp":1633114980,"answer_ET":"C"},{"id":"lbSkxzZCxl5HiG1PUhmc","answers_community":["C (100%)"],"answer_images":[],"unix_timestamp":1629714420,"discussion":[{"content":"Question FOR DP-203 , Not For DBA (DP-300)","upvote_count":"20","poster":"HichemZe","timestamp":"1677155220.0","comment_id":"429879"},{"content":"Selected Answer: C\nSee gt002's explaination.","poster":"MS_KoolaidMan","comment_id":"904436","upvote_count":"1","timestamp":"1732327620.0"},{"poster":"Jo123se","upvote_count":"2","timestamp":"1696893660.0","comment_id":"583532","content":"a lot of questions are not for DP-300. More for Data Engineer or Data Analytics"},{"timestamp":"1687320060.0","comment_id":"505844","content":"ANSWER CORRECT: Azure Stream Analytics\n\nAzure Stream Analytics Job - To create an Azure Stream Analytics job, follow the steps in the Get started using Azure Stream Analytics tutorial to:\n\nCreate an Event Hub input\nConfigure and start event generator application\nProvision a Stream Analytics job\nSpecify job input and query\nDedicated SQL pool - To create a new dedicated SQL pool, follow the steps in the Quickstart: Create a dedicated SQL pool.\nSpecify streaming output to point to your dedicated SQL pool","upvote_count":"4","poster":"gt002"}],"url":"https://www.examtopics.com/discussions/microsoft/view/60371-exam-dp-300-topic-1-question-17-discussion/","isMC":true,"question_images":[],"choices":{"C":"Azure Stream Analytics","A":"Azure SQL Database","B":"Azure Databricks"},"answer_ET":"C","question_text":"You are designing an anomaly detection solution for streaming data from an Azure IoT hub. The solution must meet the following requirements:\n✑ Send the output to an Azure Synapse.\n✑ Identify spikes and dips in time series data.\n✑ Minimize development and configuration effort.\nWhich should you include in the solution?","answer_description":"Anomalies can be identified by routing data via IoT Hub to a built-in ML model in Azure Stream Analytics\nReference:\nhttps://docs.microsoft.com/en-us/learn/modules/data-anomaly-detection-using-azure-iot-hub/ https://docs.microsoft.com/en-us/azure/stream-analytics/azure-synapse-analytics-output","answer":"C","exam_id":68,"topic":"1","question_id":9,"timestamp":"2021-08-23 12:27:00"},{"id":"8w9u4AnwGfrFHX2usB3O","answer_ET":"B","isMC":true,"answer_images":[],"discussion":[{"upvote_count":"23","content":"Question FOR DP-203 , Not For DBA (DP-300)","comment_id":"429880","poster":"HichemZe","timestamp":"1677155280.0"},{"timestamp":"1683461580.0","comment_id":"473929","content":"Answer is %<language>","upvote_count":"7","poster":"stdevops"},{"comment_id":"904438","poster":"MS_KoolaidMan","content":"Selected Answer: B\n%<language>","timestamp":"1732327680.0","upvote_count":"1"},{"content":"Are we sure that these questions do not appear in DP-300?","poster":"Itsalwaymethecap","comment_id":"628760","timestamp":"1704722280.0","upvote_count":"3"}],"answers_community":["B (100%)"],"unix_timestamp":1629714480,"choices":{"B":"%<language>","A":"\\\\[<language>]","C":"\\\\[<language>]","D":"@<language>"},"timestamp":"2021-08-23 12:28:00","exam_id":68,"question_images":[],"answer":"B","question_text":"You are creating a new notebook in Azure Databricks that will support R as the primary language but will also support Scala and SQL.\nWhich switch should you use to switch between languages?","topic":"1","answer_description":"You can override the default language by specifying the language magic command %<language> at the beginning of a cell. The supported magic commands are:\n%python, %r, %scala, and %sql.\nReference:\nhttps://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-use","url":"https://www.examtopics.com/discussions/microsoft/view/60372-exam-dp-300-topic-1-question-18-discussion/","question_id":10}],"exam":{"id":68,"lastUpdated":"12 Apr 2025","isImplemented":true,"isMCOnly":false,"provider":"Microsoft","numberOfQuestions":360,"name":"DP-300","isBeta":false},"currentPage":2},"__N_SSP":true}