{"pageProps":{"questions":[{"id":"wwKkYWJXkPrl34y9wysW","exam_id":65,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0015000001.png"],"unix_timestamp":1574926740,"discussion":[{"poster":"cnuusd","comments":[{"upvote_count":"5","comment_id":"51407","content":"totally agree","timestamp":"1581894840.0","poster":"avestabrzn"},{"timestamp":"1585289700.0","comments":[{"content":"Just the installation of the IR in the on-premise server. I guess it can be called configuration as well.","upvote_count":"1","comments":[{"poster":"BRW","timestamp":"1610898420.0","comment_id":"269606","upvote_count":"1","content":"You may need to create a db user to use in a link service in ADF."}],"timestamp":"1586731620.0","poster":"Yuri1101","comment_id":"73856"}],"upvote_count":"2","comment_id":"68510","content":"curious, what sort of configuration do we need in on-prem SQL Server instance? creating user account and so on?","poster":"zenomas"}],"comment_id":"36294","upvote_count":"71","content":"Create an Azure Data Factory\nConfigure a self-hosted integration runtime\nConfigure on-premises SQL Server Instance ................","timestamp":"1578397980.0"},{"comments":[{"timestamp":"1584361860.0","comment_id":"64706","comments":[{"poster":"zenomas","comments":[{"upvote_count":"1","poster":"visakh","timestamp":"1588994520.0","content":"Its not madatory. But its a security best practice to use VPN\n\nSee\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-movement-security-considerations","comment_id":"85913"}],"upvote_count":"1","timestamp":"1585362000.0","comment_id":"68704","content":"I think it is not required when the machine is internet-facing with connectivity to on-prem SQL Server."},{"content":"The company may have ExpressRoute setup.","poster":"cowtown","timestamp":"1593790740.0","comment_id":"125662","upvote_count":"3"}],"content":"\"The installation of a self-hosted integration runtime needs an on-premises machine or a virtual machine inside a private network.\" - https://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime\n\nImplies that a virtual private network is indeed required.","poster":"John123123","upvote_count":"8"}],"timestamp":"1574926740.0","content":"It's not necessary to Create a virtual private network (VPN) connection from on-premises to Microsoft Azure.","poster":"epgd","upvote_count":"23","comment_id":"24956"},{"timestamp":"1619761860.0","content":"Create an Azure Data Factory\nConfigure a self-hosted integration runtime\nConfigure on-premises SQL Server Instance\n\nThese steps leads to installation of gateway to the on-prem server that links to ADF (Self-Hosted IR)","poster":"cadio30","comment_id":"345913","upvote_count":"1"},{"comment_id":"319503","poster":"sonu_agrawal","timestamp":"1616616060.0","upvote_count":"1","content":"What is the correct answer for this?"},{"upvote_count":"4","comment_id":"262814","poster":"joguerra","content":"You don't need to pick \"Configure on-premises SQL Server Instance ... \" because self-host runtime already does that. Look at the tuturial, you don't need to touch anything related to SQL Server Instance, you simply create a linked service through the self-host IR. I would say it is redundant.\n\nCreating a VPN on the otherside, (although not mandatory) makes the solution more complete.","timestamp":"1610133540.0"},{"upvote_count":"3","poster":"syu31svc","comment_id":"229518","timestamp":"1606554060.0","content":"I would say the answer given is correct\nVPN to establish connection as the first step followed by creating Azure Data Factory and configure self-hosted integration runtime"},{"upvote_count":"2","content":"1: Deploy an Azure Data Factory\n2: From the on-premises network, install and configure a self-hosted runtime.\n3: Configure a linked service to connect to the SQL Server instance.","timestamp":"1600596960.0","comment_id":"182876","poster":"groy","comments":[{"upvote_count":"1","comment_id":"269595","timestamp":"1610897700.0","content":"Without link service there is no way to connect SQL Server from ADF. I guess this option is missing.","poster":"BRW"}]},{"upvote_count":"7","content":"1. Create an Azure Data Factory resource\n2. Configure a self-hosted integration runtime (configure means that you have ADF already running)\n3. Configure the on-premises SQL server instance with an integration runtime (means that you go into ADF and connect to the database using IR)\n\nYou don't need VPN - all communication from IR to ADF is over HTTPS","timestamp":"1599644340.0","poster":"ExamPwnr","comments":[{"timestamp":"1607420100.0","content":"hi to all, this is the right answer 4 sure. please see this link, all the steps are there: https://docs.microsoft.com/pt-pt/azure/machine-learning/team-data-science-process/move-sql-azure-adf\n\nvpn no needed, the IR onpremises uses https as default\n\nregards","comment_id":"238097","comments":[{"content":"Create an Azure Data Factory\nThe instructions for creating a new Azure Data Factory and a resource group in the Azure portal are provided Create an Azure Data Factory. Name the new ADF instance adfdsp and name the resource group created adfdsprg.\n\nInstall and configure Azure Data Factory Integration Runtime\nThe Integration Runtime is a customer-managed data integration infrastructure used by Azure Data Factory to provide data integration capabilities across different network environments. This runtime was formerly called \"Data Management Gateway\".\n\nTo set up, follow the instructions for creating a pipeline\n\nCreate linked services to connect to the data resources\nA linked service defines the information needed for Azure Data Factory to connect to a data resource. We have three resources in this scenario for which linked services are needed:\n\nOn-premises SQL Server\nAzure Blob Storage\nAzure SQL Database","poster":"dumpsm42","timestamp":"1607443740.0","upvote_count":"1","comment_id":"238479"}],"poster":"dumpsm42","upvote_count":"1"}],"comment_id":"176410"},{"comments":[{"upvote_count":"1","timestamp":"1621370400.0","content":"That's why you have ADF to use copy activity directly from table to blob, and you don't need to manually copy a backup to storage","comment_id":"360834","poster":"maciejt"}],"comment_id":"101688","poster":"Abhitm","timestamp":"1591195980.0","upvote_count":"4","content":"1: Create an Azure Data Factory\n2: Configure a self-hosted integration runtime\n3: Backup the DB and send it to Azure Blob storage"},{"content":"VPN is not a mandatory requirement for self-hosted IR. It is only needed when to perform data integration securely in a private network environment, which doesn't have a direct line-of-sight from the public cloud environment. (https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime).\nSo, the correct answers should be \n1. Create an ADF;\n2. Create a self-hosted IR;\n3. Install and configure self-hosted IR on on-premise server;","comment_id":"94319","upvote_count":"5","poster":"Luke97","timestamp":"1590235440.0"},{"content":"Is VPN really a necessity?","comment_id":"90955","comments":[{"content":"The installation of a self-hosted integration runtime needs an on-premises machine or a virtual machine inside a private network.","upvote_count":"1","comment_id":"150630","timestamp":"1596562440.0","poster":"kilowd"}],"upvote_count":"1","timestamp":"1589768880.0","poster":"TheCyanideLancer"},{"timestamp":"1585289760.0","poster":"zenomas","content":"Step 1 and 2 can be in any order.","comment_id":"68511","upvote_count":"2"},{"comment_id":"58590","poster":"AAJ","content":"Given answer is correct","timestamp":"1583309160.0","upvote_count":"5"},{"timestamp":"1582735560.0","upvote_count":"2","content":"correct VPN is not needed","poster":"MilindD","comments":[{"poster":"kilowd","timestamp":"1596562380.0","content":"The installation of a self-hosted integration runtime needs an on-premises machine or a virtual machine inside a private network.","upvote_count":"1","comment_id":"150628"}],"comment_id":"55565"},{"comment_id":"25357","content":"The answer could be\nStep 1: Create an Azure Data Factory\nYou need to create a data factory and start the Data Factory UI to create a pipeline in the data factory.\n\nStep 2: Configure a self-hosted integration runtime\n\nStep 3: Configure a self-hosted integration runtime. \n?Â¿?","upvote_count":"6","comments":[{"content":"Tu ere loko o k","comment_id":"195608","upvote_count":"9","timestamp":"1602113520.0","poster":"Kampai787"}],"poster":"epgd","timestamp":"1575104400.0"}],"isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0014900001.png"],"answer_description":"Step 1: Create a virtual private network (VPN) connection from on-premises to Microsoft Azure.\nYou can also use IPSec VPN or Azure ExpressRoute to further secure the communication channel between your on-premises network and Azure.\nAzure Virtual Network is a logical representation of your network in the cloud. You can connect an on-premises network to your virtual network by setting up IPSec\nVPN (site-to-site) or ExpressRoute (private peering).\nStep 2: Create an Azure Data Factory resource.\nStep 3: Configure a self-hosted integration runtime.\nYou create a self-hosted integration runtime and associate it with an on-premises machine with the SQL Server database. The self-hosted integration runtime is the component that copies data from the SQL Server database on your machine to Azure Blob storage.\nNote: A self-hosted integration runtime can run copy activities between a cloud data store and a data store in a private network, and it can dispatch transform activities against compute resources in an on-premises network or an Azure virtual network. The installation of a self-hosted integration runtime needs on an on- premises machine or a virtual machine (VM) inside a private network.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell","question_id":121,"question_text":"DRAG DROP -\nYour company manages on-premises Microsoft SQL Server pipelines by using a custom solution.\nThe data engineering team must implement a process to pull data from SQL Server and migrate it to Azure Blob storage. The process must orchestrate and manage the data lifecycle.\nYou need to configure Azure Data Factory to connect to the on-premises SQL Server database.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","answer_ET":"","timestamp":"2019-11-28 08:39:00","answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/9252-exam-dp-200-topic-2-question-5-discussion/","answers_community":[],"topic":"2"},{"id":"4cdvu6WUm1UOSPGr7p8w","answer_images":[],"choices":{"B":"interactive","A":"automated","C":"High Concurrency"},"question_images":[],"topic":"2","answer_description":"Azure Databricks has two types of clusters: interactive and automated. You use interactive clusters to analyze data collaboratively with interactive notebooks. You use automated clusters to run fast and robust automated jobs.\nExample: Scheduled batch workloads (data engineers running ETL jobs)\nThis scenario involves running batch job JARs and notebooks on a regular cadence through the Databricks platform.\nThe suggested best practice is to launch a new cluster for each run of critical jobs. This helps avoid any issues (failures, missing SLA, and so on) due to an existing workload (noisy neighbor) on a shared cluster.\nReference:\nhttps://docs.databricks.com/administration-guide/cloud-configurations/aws/cmbp.html#scenario-3-scheduled-batch-workloads-data-engineers-running-etl-jobs","unix_timestamp":1618895400,"url":"https://www.examtopics.com/discussions/microsoft/view/50524-exam-dp-200-topic-2-question-50-discussion/","discussion":[{"poster":"Simon2021","timestamp":"1623762480.0","upvote_count":"1","content":"The automated cluster type is called Job Cluster.","comment_id":"382680"},{"upvote_count":"1","content":"Automated cluster is correct answer","comment_id":"379872","timestamp":"1623422640.0","poster":"dumpi"},{"timestamp":"1622131680.0","content":"There's no such thing as automated cluster. \"Databricks makes a distinction between all-purpose clusters and job clusters\". https://docs.databricks.com/clusters/index.html","upvote_count":"2","comment_id":"368115","poster":"Rob77"},{"timestamp":"1620021360.0","poster":"cadio30","comment_id":"348303","upvote_count":"3","content":"The answer is \"automated\".\n\nReference: https://docs.databricks.com/clusters/index.html?_ga=2.1881073.1354805237.1620028392-1378612004.1617756501"},{"timestamp":"1619088660.0","poster":"Hassan_Mazhar_Khan","content":"It has discussed about type not mode what Nicks is referring to is mode. However question is about type","comment_id":"340984","upvote_count":"1"},{"poster":"Nicks_125281","timestamp":"1618895400.0","comment_id":"339334","content":"An automated Databricks cluster doesn't exists:\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure","comments":[{"comment_id":"373682","poster":"niwe","content":"So what is the answer?","timestamp":"1622732280.0","upvote_count":"3"}],"upvote_count":"2"}],"question_id":122,"answer_ET":"A","question_text":"You plan to perform batch processing in Azure Databricks once daily.\nWhich type of Databricks cluster should you use?","answers_community":[],"answer":"A","exam_id":65,"timestamp":"2021-04-20 07:10:00","isMC":true},{"id":"6jqokGbZVkVYUQ70sMYl","question_id":123,"answer":"C","discussion":[{"comment_id":"501475","poster":"ramelas","timestamp":"1639496940.0","comments":[{"upvote_count":"1","poster":"uzairahm","timestamp":"1656216840.0","comment_id":"622341","content":"It is a delta lake in destination so you could write the complete results to output as well but what would be the criteria of managing in delta lake is not clear so i would go with append as each five minutes interval count results would go in as a separate row in destination"}],"upvote_count":"1","content":"\"The solution will count new events in five-minute intervals and report only events that arrive during the interval.\"\nit is COMPLETE, not append"},{"content":"answer is CORRECT","comment_id":"361084","upvote_count":"4","poster":"memo43","timestamp":"1621405980.0"}],"choices":{"C":"append","A":"complete","B":"update"},"unix_timestamp":1621405980,"question_text":"You plan to build a structured streaming solution in Azure Databricks. The solution will count new events in five-minute intervals and report only events that arrive during the interval. The output will be sent to a Delta Lake table.\nWhich output mode should you use?","answer_ET":"C","answer_description":"Append Mode: Only new rows appended in the result table since the last trigger are written to external storage. This is applicable only for the queries where existing rows in the Result Table are not expected to change.\nIncorrect Answers:\nA: Complete Mode: The entire updated result table is written to external storage. It is up to the storage connector to decide how to handle the writing of the entire table.\nB: Update Mode: Only the rows that were updated in the result table since the last trigger are written to external storage. This is different from Complete Mode in that Update Mode outputs only the rows that have changed since the last trigger. If the query doesn't contain aggregations, it is equivalent to Append mode.\nReference:\nhttps://docs.databricks.com/getting-started/spark/streaming.html","exam_id":65,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/53086-exam-dp-200-topic-2-question-51-discussion/","isMC":true,"topic":"2","answer_images":[],"question_images":[],"timestamp":"2021-05-19 08:33:00"},{"id":"04aaEOsGTd3MBTaFT2s8","discussion":[{"comment_id":"360017","poster":"Raya2021","timestamp":"1621309920.0","content":"correct","upvote_count":"4"}],"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/52992-exam-dp-200-topic-2-question-52-discussion/","timestamp":"2021-05-18 05:52:00","unix_timestamp":1621309920,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0024700001.png"],"topic":"2","exam_id":65,"question_text":"HOTSPOT -\nYou are building an Azure Stream Analytics job to identify how much time a user spends interacting with a feature on a webpage.\nThe job receives events based on user actions on the webpage. Each row of data represents an event. Each event has a type of either 'start' or 'end'.\nYou need to calculate the duration between start and end events.\nHow should you complete the query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_ET":"","isMC":false,"answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0024600001.png"],"question_id":124,"answer_description":"Box 1: DATEDIFF -\nDATEDIFF function returns the count (as a signed integer value) of the specified datepart boundaries crossed between the specified startdate and enddate.\nSyntax: DATEDIFF ( datepart , startdate, enddate )\n\nBox 2: LAST -\nThe LAST function can be used to retrieve the last event within a specific condition. In this example, the condition is an event of type Start, partitioning the search by PARTITION BY user and feature. This way, every user and feature is treated independently when searching for the Start event. LIMIT DURATION limits the search back in time to 1 hour between the End and Start events.\nExample:\n\nSELECT -\n[user],\nfeature,\nDATEDIFF(\nsecond,\nLAST(Time) OVER (PARTITION BY [user], feature LIMIT DURATION(hour, 1) WHEN Event = 'start'),\n\nTime) as duration -\n\nFROM input TIMESTAMP BY Time -\n\nWHERE -\nEvent = 'end'\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns"},{"id":"CybyNqlkOE3K8enhwNNC","answer_description":"Box 1: DataErrorType -\nThe DataErrorType is InputDeserializerError.InvalidData.\n\nBox 2: Message -\nRetrieve the message.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/data-errors","discussion":[{"content":"Should be parse_json(properties_s).DataErrorType and parse_json(properties_s).Message\nExample: https://github.com/MicrosoftDocs/LogAnalyticsExamples/blob/master/ResourceTypes/StreamAnalytics/List-all-input-deserialization-errors.txt","poster":"MsIrene","comments":[{"comment_id":"348310","content":"as quoted, this is the appropriate answer","timestamp":"1620021960.0","poster":"cadio30","upvote_count":"2"},{"timestamp":"1625427060.0","poster":"captainbee","upvote_count":"2","content":"Oh thank Christ. When I revealed their answer I was about to explode","comment_id":"398611"},{"content":"with documents you provide, it shows first need .DataErrorType but after project TimeGenerated, we don't need to .Message. So, thank you for the good link, for second dropdown I will go for just Message.","timestamp":"1628615640.0","poster":"Ati1362","comment_id":"422890","upvote_count":"1"}],"timestamp":"1618059960.0","upvote_count":"26","comment_id":"332558"},{"poster":"vrmei","comment_id":"376261","upvote_count":"1","timestamp":"1623001860.0","content":"Given Answer is correct: \nNo need to user parse_json function. \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/media/stream-analytics-job-diagnostic-logs/logs-example.png","comments":[{"timestamp":"1624600440.0","upvote_count":"3","poster":"hello_there_","content":"... the link you provided shows a parse_json function","comment_id":"390161"}]},{"comment_id":"343447","content":"You should use the parse_json function\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-job-diagnostic-logs","upvote_count":"3","timestamp":"1619458080.0","poster":"Maddaa"}],"unix_timestamp":1618059960,"question_text":"HOTSPOT -\nYou have an Azure Stream Analytics job named ASA1.\nThe Diagnostic settings for ASA1 are configured to write errors to Log Analytics.\nASA1 reports an error, and the following message is sent to Log Analytics.\n//IMG//\n\nYou need to write a Kusto query language query to identify all instances of the error and return the message field.\nHow should you complete the query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/49802-exam-dp-200-topic-2-question-53-discussion/","question_id":125,"isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0024900001.png","https://www.examtopics.com/assets/media/exam-media/03872/0024900002.png"],"timestamp":"2021-04-10 15:06:00","exam_id":65,"answer":"","topic":"2","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0025000001.png"],"answers_community":[]}],"exam":{"numberOfQuestions":228,"id":65,"provider":"Microsoft","isMCOnly":false,"isImplemented":true,"isBeta":false,"lastUpdated":"12 Apr 2025","name":"DP-200"},"currentPage":25},"__N_SSP":true}