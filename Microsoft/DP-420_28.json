{"pageProps":{"questions":[{"id":"Bn2Ygr0qVu5Tp0OtWpMm","answer_images":["https://www.examtopics.com/assets/media/exam-media/04276/0013100001.jpg","https://www.examtopics.com/assets/media/exam-media/04276/0013100002.jpg"],"question_text":"You plan to create an Azure Cosmos DB Core (SQL) API account that will use customer-managed keys stored in Azure Key Vault.\nYou need to configure an access policy in Key Vault to allow Azure Cosmos DB access to the keys.\nWhich three permissions should you enable in the access policy? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/89318-exam-dp-420-topic-5-question-7-discussion/","question_images":[],"answer_description":"To Configure customer-managed keys for your Azure Cosmos account with Azure Key Vault:\nAdd an access policy to your Azure Key Vault instance:\n1. From the Azure portal, go to the Azure Key Vault instance that you plan to use to host your encryption keys. Select Access Policies from the left menu:\n\n2. Select + Add Access Policy.\n3. Under the Key permissions drop-down menu, select Get, Unwrap Key, and Wrap Key permissions:\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk","discussion":[{"upvote_count":"5","content":"A. Wrap Key\nB. Get\nG. Unwrap Key","poster":"azuredemo2022three","comment_id":"929779","timestamp":"1718991300.0"},{"poster":"Juba1711","comment_id":"730920","timestamp":"1701299160.0","content":"Correct answer\nhttps://learn.microsoft.com/en-us/azure/storage/common/customer-managed-keys-overview","upvote_count":"5"}],"answer":"ABG","timestamp":"2022-11-30 00:06:00","topic":"5","exam_id":69,"unix_timestamp":1669763160,"answer_ET":"ABG","choices":{"A":"Wrap Key","E":"Sign","C":"List","G":"Unwrap Key","F":"Verify","B":"Get","D":"Update"},"answers_community":[],"question_id":136},{"id":"kGoUGD76gbv9fOZ4Kzoj","question_images":[],"question_id":137,"question_text":"You need to configure an Apache Kafka instance to ingest data from an Azure Cosmos DB Core (SQL) API account. The data from a container named telemetry must be added to a Kafka topic named iot. The solution must store the data in a compact binary format.\nWhich three configuration items should you include in the solution? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","timestamp":"2022-08-15 16:06:00","answer":"ACD","exam_id":69,"answer_images":[],"discussion":[{"upvote_count":"10","timestamp":"1696000680.0","comments":[{"poster":"TRUESON","timestamp":"1714565400.0","comment_id":"886200","upvote_count":"2","content":"source is to get cosmosdb data to kafka, sink is to write kafka data to cosmosdb"},{"comment_id":"886206","timestamp":"1714565580.0","content":"for better understanding watch this video from 22:30 https://www.youtube.com/live/b9L_CTUaz5Y","upvote_count":"1","poster":"TRUESON"}],"content":"Selected Answer: ACD\nWe want to have data from cosmos to kafka so source, not sink","poster":"TimSss","comment_id":"682801"},{"upvote_count":"6","content":"Not sure, but shouldn't we have answer A and not F ?\n\nHere we intent to have the AzureCosmosDB as a source to export to Kafka as a sink - meaning that we should import the \"com.azure.cosmos.kafka.connect.source.CosmosDBSourceConnector\" as stated here https://docs.microsoft.com/en-us/azure/cosmos-db/sql/kafka-connector-source\n\"Kafka Connect for Azure Cosmos DB is a connector to read from and write data to Azure Cosmos DB. The Azure Cosmos DB source connector provides the capability to read data from the Azure Cosmos DB change feed and publish this data to a Kafka topic.\"","poster":"AscentAcademy","timestamp":"1692108360.0","comment_id":"647233"},{"timestamp":"1727484960.0","content":"Selected Answer: ACD\nTo configure an Apache Kafka instance to ingest data from an Azure Cosmos DB Core (SQL) API account and store it in a Kafka topic named \"iot\" while using a compact binary format, you should include the following configuration items:\n\nA. \"connector.class\": \"com.azure.cosmos.kafka.connect.source.CosmosDBSourceConnector\"\n\nThis specifies the source connector class for ingesting data from Azure Cosmos DB.\nB. \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\"\n\nThis configuration specifies the key converter. However, since you want to store data in a compact binary format, you should use Avro as the key and value converter instead of JSON.\nC. \"key.converter\": \"io.confluent.connect.avro.AvroConverter\"\n\nAvro is a compact binary format, and using this converter for both key and value will store data in Avro format.\nD. \"connect.cosmos.containers.topicmap\": \"iot#telemetry\"\n\nThis configuration maps the Cosmos DB container \"telemetry\" to the Kafka topic \"iot.\"\nSo, you should include configurations A, C, and D in the solution for your specific requirements.","poster":"Garyn","upvote_count":"4","comment_id":"1019309"},{"poster":"azuredemo2022three","comment_id":"934615","upvote_count":"1","content":"Selected Answer: ACD\nAnswer","timestamp":"1719417720.0"}],"topic":"5","unix_timestamp":1660572360,"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/78037-exam-dp-420-topic-5-question-8-discussion/","answer_ET":"ACD","answers_community":["ACD (100%)"],"choices":{"D":"\"connect.cosmos.containers.topicmap\": \"iot#telemetry\"","B":"\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\"","E":"\"connect.cosmos.containers.topicmap\": \"iot\"","A":"\"connector.class\": \"com.azure.cosmos.kafka.connect.source.CosmosDBSourceConnector\"","C":"\"key.converter\": \"io.confluent.connect.avro.AvroConverter\"","F":"\"connector.class\": \"com.azure.cosmos.kafka.connect.source.CosmosDBSinkConnector\""}},{"id":"LFVuVy2NGIYtjEBqlcrE","isMC":true,"question_images":[],"answer":"C","discussion":[{"upvote_count":"1","timestamp":"1737460080.0","comment_id":"1344126","content":"Selected Answer: B\nThe \"Write Throughput Budget\" setting in the Azure Cosmos DB sink of an Azure Data Factory data flow allows you to control the amount of Request Units (RUs) that your data flow operation can consume when writing data to Cosmos DB.","poster":"YellowSky002"},{"timestamp":"1718995140.0","comment_id":"929824","upvote_count":"4","poster":"azuredemo2022three","content":"C. Batch size determines the number of documents that are sent in each batch to the sink. By configuring an appropriate batch size, you can control the number of documents processed at a time and optimize the ingestion process."},{"content":"Selected Answer: C\nC is correct","upvote_count":"3","poster":"imando","timestamp":"1717416540.0","comment_id":"913519"}],"answer_images":[],"topic":"5","answer_description":"","question_text":"You are implementing an Azure Data Factory data flow that will use an Azure Cosmos DB (SQL API) sink to write a dataset. The data flow will use 2,000 Apache\nSpark partitions.\nYou need to ensure that the ingestion from each Spark partition is balanced to optimize throughput.\nWhich sink setting should you configure?","question_id":138,"unix_timestamp":1685794140,"exam_id":69,"choices":{"D":"Collection action","C":"Batch size","B":"Write throughput budget","A":"Throughput"},"answer_ET":"C","timestamp":"2023-06-03 14:09:00","url":"https://www.examtopics.com/discussions/microsoft/view/110991-exam-dp-420-topic-5-question-9-discussion/","answers_community":["C (75%)","B (25%)"]},{"id":"7wste0Iv2QIySJRfzgIR","unix_timestamp":1663497480,"url":"https://www.examtopics.com/discussions/microsoft/view/82634-exam-dp-420-topic-6-question-1-discussion/","timestamp":"2022-09-18 12:38:00","question_text":"You are troubleshooting the current issues caused by the application updates.\nWhich action can address the application updates issue without affecting the functionality of the application?","topic":"6","answer_images":[],"answer_description":"","answer_ET":"D","answers_community":["D (100%)"],"choices":{"C":"Set the default consistency level of account1 to bounded staleness.","A":"Enable time to live for the con-product container.","D":"Add a custom indexing policy to the con-product container.","B":"Set the default consistency level of account1 to strong."},"answer":"D","question_images":[],"question_id":139,"exam_id":69,"isMC":true,"discussion":[{"upvote_count":"14","poster":"DudeWheresMyCar","content":"Selected Answer: D\nFor 429's on updating documents it is recommended to create a custom index policy, and only contain the properties that are needed. \n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/sql/troubleshoot-request-rate-too-large?tabs=resource-specific#429s-on-create-replace-or-upsert-document-requests","comment_id":"672231","timestamp":"1695033480.0"},{"timestamp":"1719424200.0","upvote_count":"4","content":"Selected Answer: D\nAnswer","comment_id":"934688","poster":"azuredemo2022three"}]},{"id":"DnYVQA0lnV3ZdXuq5or9","choices":{"B":"the humidity","C":"the temperature","A":"the timestamp","D":"the device ID"},"topic":"6","answers_community":["D (100%)"],"question_images":[],"question_id":140,"isMC":true,"answer_description":"","unix_timestamp":1664466600,"question_text":"You need to select the partition key for con-iot1. The solution must meet the IoT telemetry requirements.\nWhat should you select?","answer_images":[],"answer_ET":"D","answer":"D","url":"https://www.examtopics.com/discussions/microsoft/view/84038-exam-dp-420-topic-6-question-2-discussion/","timestamp":"2022-09-29 17:50:00","discussion":[{"comment_id":"682815","timestamp":"1696002600.0","content":"correct","poster":"TimSss","upvote_count":"5"},{"upvote_count":"1","content":"Selected Answer: D\nrequirement is to have ability to get historical data for any device from con-iot1","timestamp":"1735286700.0","comment_id":"1332301","poster":"3709334"},{"comment_id":"935682","poster":"azuredemo2022three","timestamp":"1719511500.0","content":"Selected Answer: D\nAnswer","upvote_count":"4"}],"exam_id":69}],"exam":{"id":69,"name":"DP-420","isMCOnly":false,"lastUpdated":"12 Apr 2025","provider":"Microsoft","isImplemented":true,"numberOfQuestions":147,"isBeta":false},"currentPage":28},"__N_SSP":true}