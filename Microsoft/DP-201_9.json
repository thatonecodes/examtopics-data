{"pageProps":{"questions":[{"id":"HFL0jMAHLTKjYi1sE7ix","answer":"","topic":"1","answer_ET":"","question_id":41,"isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0009700001.jpg","https://www.examtopics.com/assets/media/exam-media/03774/0009800001.png"],"answer_description":"Box 1: vertices and edges -\nGremlin API is selected.\nYou can use the Gremlin language to create graph entities (vertices and edges), modify properties within those entities, perform queries and traversals, and delete entities.\n\nBox 2: US East -\nThe (US) West US is selected as the primary location and geo- redundancy is enabled.\nThe secondary location for West US is East US.\nNote: When a storage account is created, the customer chooses the primary location for their storage account. However, the secondary location for the storage account is fixed and customers do not have the ability to change this. The following table shows the current primary and secondary location pairings:\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/gremlin-support https://technet2.github.io/Wiki/blogs/windowsazurestorage/windows-azure-storage-redundancy-options-and-read-access-geo-redundant-storage.html","unix_timestamp":1617336240,"answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0009800002.png","https://www.examtopics.com/assets/media/exam-media/03774/0010000001.png"],"discussion":[{"comment_id":"326286","upvote_count":"2","content":"US West is selected in the picture - hence 2nd answer is US West.","poster":"Mittun","timestamp":"1617336240.0","comments":[{"timestamp":"1618013160.0","poster":"DongDuong","content":"Wrong, as explained The (US) West US is selected as the primary location and geo-redundancy is enabled. The secondary location for West US is East US. So East US is the correct answer.","comments":[{"poster":"suvenk","comment_id":"371864","upvote_count":"1","content":"Thats right they fall under the paired regions, hence it is US East.","timestamp":"1622548860.0"},{"poster":"cadio30","comment_id":"378009","content":"i agree with the explanation, some of the regions do have corresponding default secondary location whenever the geo-redundancy is enable and if there are no specified location in it.","timestamp":"1623217560.0","upvote_count":"1"}],"upvote_count":"27","comment_id":"332206"}]}],"exam_id":66,"question_text":"HOTSPOT -\nYou are evaluating the use of an Azure Cosmos DB account for a new database.\nThe proposed account will be configured as shown in the following exhibit.\n//IMG//\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2021-04-02 06:04:00","url":"https://www.examtopics.com/discussions/microsoft/view/48721-exam-dp-201-topic-1-question-46-discussion/"},{"id":"0AiMAxuM45TQ6mPzH2Ud","topic":"1","exam_id":66,"answer":"C","discussion":[{"content":"ARM template can only be used to create databricks workspace but not for the application(notebooks),cluster etc. whereas it can be used for stream analytics. Hence Stream Analytics should be the answer here","comment_id":"331021","timestamp":"1617872400.0","poster":"suman13","upvote_count":"32"},{"timestamp":"1617541020.0","poster":"KRV","comment_id":"327981","content":"Since the question clearly specifies to define calculations by using a SQL query as well as the points that minimum, maximum, and average sensor readings are to be calculated every five minutes\nusing Windows functions within the Azure stream analytics would be a straight and preffered option. \nOne can always use Azure Databricks , however for minimal code using SQL and windows functions , the best possible solution ideally should be Azure Stream Analytics.","upvote_count":"13"},{"poster":"Ankush1994","content":"Azure Databricks, running Apache Spark Streaming, picks up the messages in real time from IoT Hub, processes the data based on the business logic and sends the data to Serving layer for storage.","comment_id":"425231","upvote_count":"1","timestamp":"1629023220.0"},{"timestamp":"1624774080.0","comment_id":"391790","content":"In the link given there is an alternatives section which states for streaming Stream Analytics could be used as an alternative. So that is another plus to the argument that Stream Analytics should be the answer.","upvote_count":"1","poster":"tes"},{"upvote_count":"1","timestamp":"1624124280.0","poster":"davem0193","comment_id":"385663","content":"The architectural diagram provided as part of the solution clearly shows that it needs to be databricks although Stream analytics makes more sense. Solution provided is correct - it is databricks","comments":[{"content":"in the same page alternatives are provided and one is stream analytics. ARM template deploy of jobs are possible there. Where as DBR notebooks cannot be deployed through arm templates","comment_id":"391784","timestamp":"1624773600.0","upvote_count":"2","poster":"tes"}]},{"content":"D. Azure Stream Analytics is the appropriate solution for the requirements","timestamp":"1621836780.0","upvote_count":"4","comment_id":"365244","poster":"cadio30"},{"poster":"maciejt","comment_id":"328118","timestamp":"1617554820.0","upvote_count":"1","content":"Azure functions is also present in the architecture. Why incorrect answer then?"},{"timestamp":"1617444840.0","upvote_count":"3","poster":"SK1984","content":"Why not D. Azure Stream analytics ?","comment_id":"327285","comments":[{"comment_id":"329364","upvote_count":"2","comments":[{"poster":"maynard13x8","comment_id":"329367","content":"I would also choose asa (Azure stream analytics).","upvote_count":"3","timestamp":"1617691500.0"}],"content":"I think because asa jobs queries are not exactly sql. If that si not te case","poster":"maynard13x8","timestamp":"1617691380.0"}]}],"question_images":[],"answer_description":"Cosmos DB is ideally suited for IoT solutions. Cosmos DB can ingest device telemetry data at high rates.\n\nArchitecture -\n\n\nData flow -\n1. Events generated from IoT devices are sent to the analyze and transform layer through Azure IoT Hub as a stream of messages. Azure IoT Hub stores streams of data in partitions for a configurable amount of time.\n2. Azure Databricks, running Apache Spark Streaming, picks up the messages in real time from IoT Hub, processes the data based on the business logic and sends the data to Serving layer for storage. Spark Streaming can provide real time analytics such as calculating moving averages, min and max values over time periods.\n3. Device messages are stored in Cosmos DB as JSON documents.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/iot-using-cosmos-db","question_id":42,"url":"https://www.examtopics.com/discussions/microsoft/view/48917-exam-dp-201-topic-1-question-47-discussion/","question_text":"You are designing a streaming solution that must meet the following requirements:\n✑ Accept input data from an Azure IoT hub.\n✑ Write aggregated data to Azure Cosmos DB.\n✑ Calculate minimum, maximum, and average sensor readings every five minutes.\n✑ Define calculations by using a SQL query.\n✑ Deploy to multiple environments by using Azure Resource Manager templates.\nWhat should you include in the solution?","choices":{"C":"Azure Databricks","A":"Azure Functions","B":"Azure HDInsight with Spark Streaming","D":"Azure Stream Analytics"},"timestamp":"2021-04-03 12:14:00","unix_timestamp":1617444840,"answer_ET":"C","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0010100001.jpg"],"answers_community":[],"isMC":true},{"id":"0F8JCJJL0SgGzYF1y8PT","question_images":[],"answer_ET":"B","answer_description":"Azure Data Lake Storage implements an access control model that derives from HDFS, which in turn derives from the POSIX access control model.\nBlob container ACLs does not support the hierarchical namespace, so it must be disabled.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues","exam_id":66,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to store delimited text files in an Azure Data Lake Storage account that will be organized into department folders.\nYou need to configure data access so that users see only the files in their respective department folder.\nSolution: From the storage account, you enable a hierarchical namespace, and you use access control lists (ACLs).\nDoes this meet the goal?","unix_timestamp":1617519960,"url":"https://www.examtopics.com/discussions/microsoft/view/49018-exam-dp-201-topic-1-question-48-discussion/","answers_community":[],"isMC":true,"question_id":43,"topic":"1","choices":{"A":"Yes","B":"No"},"discussion":[{"comment_id":"327826","poster":"niwe","comments":[{"content":"Question has \"Enable\",please check","comment_id":"375813","poster":"azurenav","timestamp":"1622968740.0","upvote_count":"8"},{"content":"It says enable, please check the question. Answer is \"Yes\"","poster":"Sennkumar","timestamp":"1624312440.0","upvote_count":"7","comment_id":"387425"},{"poster":"cadio30","upvote_count":"6","content":"The link provided already stated that it requires to \"ENABLE\" the Hierarchical namespace which is included in the question. The appropriate answer is \"Yes\". Also this configuration is only available in Azure Data Lake Storage Gen 2 (azure storage blob)","comment_id":"365266","timestamp":"1621837800.0"},{"timestamp":"1619964240.0","content":"The question saying \"you enable a hierarchical namespace\" though. Did the question change?\nSolution: From the storage account, you enable a hierarchical namespace, and you use access control lists (ACLs).","poster":"Deasto","comments":[{"content":"Yes you are right question has change","comment_id":"349598","comments":[{"upvote_count":"7","poster":"Psycho","comment_id":"359688","comments":[{"poster":"BobFar","content":"right, the answer is YES","timestamp":"1622765040.0","comment_id":"373970","upvote_count":"5"}],"content":"So, the answer is Yes, right?","timestamp":"1621267920.0"},{"comment_id":"359692","content":"so, YES is the answer","timestamp":"1621268220.0","upvote_count":"9","poster":"crissw22"}],"timestamp":"1620145680.0","upvote_count":"5","poster":"niwe"}],"upvote_count":"7","comment_id":"347793"}],"timestamp":"1617519960.0","content":"Hierarchical namespace, must be enabled, so No.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-acl-dotnet","upvote_count":"12"},{"upvote_count":"1","poster":"satyamkishoresingh","content":"This should be Yes as solution Fit the problem statement , \nEnable Hierarchical namespace + ACL","timestamp":"1628825040.0","comment_id":"424049"},{"upvote_count":"1","comment_id":"391794","timestamp":"1624774440.0","poster":"tes","comments":[{"poster":"tes","timestamp":"1624774560.0","upvote_count":"1","content":"sorry ignore this, wrong answer. I cannot delete it. ACL is there in Gen1.","comment_id":"391796"}],"content":"you enable a hierarchical namespace = then the storage account becomes Gen2\nEnable ACL: Gen2 automatically has ACL\nso the answer is Yes"},{"comment_id":"344861","timestamp":"1619637420.0","upvote_count":"1","content":"B. No The answer","poster":"davita8"},{"upvote_count":"2","comment_id":"343108","poster":"Apox","comments":[],"timestamp":"1619427120.0","content":"The answer is \"NO\"\nHierarchical namespace must be enabled to have a folder structure and actually be an ADLS account (or else it is regular blob)\nHowever, it is correct to use ACLs, as this is the only mechanism to give \"finer grain\" level of access to directories and files. (except Shared Access Signature, but this would make more sense to use for external users for e.g. a limited amount of time) \nSource: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model"},{"poster":"aksoumi","upvote_count":"3","comment_id":"329014","comments":[],"timestamp":"1617652560.0","content":"Also, note below from the Azure Documentation. In order to create ADLS account you have to enable Hierarchical option, else it is not ADLS. Hence correct ANSWER is \"NO\"\n\"You'll create a Data Lake Storage Gen2 account the same way you create an Azure Blob store, but with one setting difference. In Advanced, in the Data Lake Storage Gen2 (preview) section, next to Hierarchical namespace, select Enabled.\""}],"timestamp":"2021-04-04 09:06:00","answer_images":[],"answer":"B"},{"id":"Z2PJebYPcoPLB43xAQuN","question_id":44,"answer":"B","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0010200003.png"],"answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-storage\nDesign data processing solutions","answer_ET":"B","topic":"1","choices":{"A":"Azure SQL Database","B":"Azure Storage","C":"Azure Cosmos DB","D":"Azure HDInsight"},"question_text":"You need to design a solution to support the storage of datasets. The solution must meet the following requirements:\n✑ Send email alerts when new datasets are added.\n✑ Control access to collections of datasets by using Azure Active Directory groups.\nSupport the storage of Microsoft Excel, Comma Separated Values (CSV), and zip files.\n//IMG//\n\nWhat should you include in the solution?","answers_community":[],"discussion":[{"content":"Agreed, the answer is correct.","poster":"KpKo","timestamp":"1622079600.0","comment_id":"367549","upvote_count":"5"},{"content":"The answer here should be CosmosDB. Access to datasets needs to be controlled. Storage account only provides container level access security at the most granular level which means all datasets will be available for anyone with access to storage account container and the respective blobs. That does not meet the access security requirement.","timestamp":"1648340040.0","comment_id":"575846","upvote_count":"1","poster":"DingDongSingSong"},{"content":"ACD are definitely not so B should be the answer","upvote_count":"1","timestamp":"1628825160.0","comment_id":"424051","poster":"satyamkishoresingh"},{"timestamp":"1621679400.0","content":"answer is CORRECT","upvote_count":"4","poster":"memo43","comment_id":"363584"}],"unix_timestamp":1621679400,"url":"https://www.examtopics.com/discussions/microsoft/view/53334-exam-dp-201-topic-1-question-49-discussion/","answer_images":[],"timestamp":"2021-05-22 12:30:00","isMC":true,"exam_id":66},{"id":"imO2O2kEVnrXTwwV968g","discussion":[{"poster":"Ankush1994","comment_id":"425173","timestamp":"1629016440.0","content":"Correct answer is\nNo\nbecuase \nSeparate data into shards by using horizontal partitioning","upvote_count":"1"},{"upvote_count":"1","poster":"osoroshi","comment_id":"302425","content":"harding can be performed and managed using (1) the elastic database tools libraries or (2) self-sharding. An elastic query is used to query or compile reports across many shards. Shards are typically databases within an elastic pool. You can think of elastic query as an efficient way for querying all databases of elastic pool at once, as long as databases share the common schema.","timestamp":"1614739560.0"},{"poster":"redalarm2000","content":"Ok i am confused as to the difference between question 4 and question 5 on this site. Question 4 says to use horizontal partitioning but Question 5 says it recommends to use horizontal partition and the wording is the same but they say that answer should be No still why?","comment_id":"268073","upvote_count":"2","comments":[{"content":"Answer : No\nApplicable solution : Horizontal partitioning (based on customerID not region i.e. using sharding concept)\nReference : https://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning","timestamp":"1611550740.0","comment_id":"275692","upvote_count":"6","poster":"Shanmahi"}],"timestamp":"1610729700.0"},{"content":"I don't understand why is not recommend horizontal ... Each shard could be the region, no?","poster":"fmunozse","timestamp":"1598164860.0","comments":[{"poster":"stijn5454","comment_id":"164871","upvote_count":"9","timestamp":"1598243940.0","content":"Horizontal partitioning splits one or more tables by row, usually within a single instance of a schema and a database server.\n\nSharding goes beyond this: it partitions the problematic table(s) in the same way, but it does this across potentially multiple instances of the schema. The obvious advantage would be that search load for the large partitioned table can now be split across multiple servers (logical or physical), not just multiple indexes on the same logical server. \n\nRef: https://en.wikipedia.org/wiki/Shard_(database_architecture)#Shards_compared_to_horizontal_partitioning"}],"upvote_count":"2","comment_id":"164195"}],"exam_id":66,"question_id":45,"unix_timestamp":1598164860,"url":"https://www.examtopics.com/discussions/microsoft/view/29334-exam-dp-201-topic-1-question-5-discussion/","answers_community":[],"answer_ET":"B","answer_description":"We should use Horizontal Partitioning through Sharding, not divide through regions.\nNote: Horizontal Partitioning - Sharding: Data is partitioned horizontally to distribute rows across a scaled out data tier. With this approach, the schema is identical on all participating databases. This approach is also called ג€shardingג€. Sharding can be performed and managed using (1) the elastic database tools libraries or\n(2) self-sharding. An elastic query is used to query or compile reports across many shards.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-query-overview","timestamp":"2020-08-23 08:41:00","answer":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are designing an Azure SQL Database that will use elastic pools. You plan to store data about customers in a table. Each record uses a value for\nCustomerID.\nYou need to recommend a strategy to partition data based on values in CustomerID.\nProposed Solution: Separate data into customer regions by using horizontal partitioning.\nDoes the solution meet the goal?","answer_images":[],"question_images":[],"choices":{"B":"No","A":"Yes"},"topic":"1","isMC":true}],"exam":{"lastUpdated":"12 Apr 2025","id":66,"isBeta":false,"provider":"Microsoft","numberOfQuestions":206,"name":"DP-201","isMCOnly":false,"isImplemented":true},"currentPage":9},"__N_SSP":true}