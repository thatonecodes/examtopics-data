{"pageProps":{"questions":[{"id":"MWVTtS7pXRhiduxqOKNP","exam_id":71,"isMC":false,"discussion":[{"poster":"Momoanwar","comments":[{"timestamp":"1709109540.0","comments":[{"upvote_count":"1","timestamp":"1724398740.0","comments":[{"comment_id":"1332061","comments":[{"timestamp":"1738493820.0","comment_id":"1350353","content":"The .parquet() format supports compression, but compression is not automatically applied unless specified (e.g., df.write.option(\"compression\", \"snappy\")).\nBy default, Parquet often uses Snappy compression, but this depends on the implementation.","poster":"goldy29","upvote_count":"2"}],"poster":"vimalan","timestamp":"1735237140.0","content":"agreed\nYes, the resulting file partitions will use file compression. When you save a DataFrame in Parquet format using the df.write.partitionBy(...).mode(\"overwrite\").parquet(...) method, Parquet files are automatically compressed by defaut","upvote_count":"2"}],"content":"Yup, agreed","comment_id":"1271168","poster":"Training_ND"}],"upvote_count":"7","comment_id":"1161405","content":"I think the same","poster":"Blue_MSBI"}],"comment_id":"1152939","upvote_count":"37","content":"I think yes yes yes. \nParquet= compression","timestamp":"1708214040.0"},{"poster":"estrelle2008","timestamp":"1708893780.0","comments":[{"timestamp":"1708894320.0","poster":"estrelle2008","comment_id":"1159180","upvote_count":"5","content":"additional: Parquet files are compressed by default, and you don’t need to take any additional actions to enable compression. When writing Parquet files, you can specify the desired compression codec (if needed) to further optimize storage and performance"}],"upvote_count":"8","comment_id":"1159168","content":"I think so too: YYY\ncode snippet according to Learn"},{"upvote_count":"1","timestamp":"1731650940.0","poster":"Rakesh16","comment_id":"1312456","content":"yes,yes,yes"},{"poster":"Mitchell12345","content":"Technically the results will not form a hierarchy of folders for EACH partition key right? Because for the day partition key, files are created. If it was phrased generically without the each-part, would've been better. Did someone have it on the exam?","comment_id":"1280872","timestamp":"1725885360.0","comments":[{"timestamp":"1734038880.0","upvote_count":"1","poster":"AdventureChick","comment_id":"1325874","content":"Yes, there is a level for each \"entity\" in the partition key. Even if you only had 1 file in a day, that file would be found under a \"day\" folder."}],"upvote_count":"1"},{"content":"Yes Yes Yes\nThe compression is optional parameter which uses 'snappy' compression by default. Unless we specifiy none, compression happens.","comment_id":"1223669","timestamp":"1717429740.0","upvote_count":"3","poster":"dev2dev"},{"upvote_count":"3","comments":[{"content":"Snappy is the default compression type of parquet, though. So it will create chunks of your file by default and compress them. If you want to have it all in one file, that's when you have to overwrite your default compression. So I disagree with the 'No'","poster":"vernillen","comment_id":"1225178","timestamp":"1717654800.0","upvote_count":"6"}],"timestamp":"1716850200.0","comment_id":"1219881","content":"Y-Y-N\nThe results will form a hierarchy of folders for each partition key:\nYes: When using partitionBy in Spark, the data is organized into a hierarchical directory structure based on the specified partition keys. Therefore, you will see directories like year=YYYY/month=MM/day=DD within the specified output path.\n\nThe resulting file partitions can be read in parallel across multiple nodes:\nYes: Parquet files are designed for efficient querying and support parallel processing. Spark can read these partitions in parallel, enabling distributed query execution across multiple nodes.\n\nThe resulting file partitions will use file compression:\nNo: While Parquet format supports compression, it is not enabled by default in the code snippet provided. Compression needs to be explicitly specified if required. For example, you could use .option(\"compression\", \"snappy\") to enable Snappy compression.","poster":"282b85d"},{"comment_id":"1208632","timestamp":"1715218740.0","upvote_count":"2","content":"IMHO, fully agree with colleagues below - Y -> Y -> Y","poster":"stilferx"},{"timestamp":"1713595020.0","poster":"Nefirs","comment_id":"1198985","content":"i think YYY as well","upvote_count":"2"},{"comment_id":"1160822","upvote_count":"1","content":"I think it should be NYY as there is no mention in the code to form a hierarchy. Please correct me if I'm wrong.","poster":"XiltroX","comments":[{"comment_id":"1179267","poster":"4fd861f","upvote_count":"4","content":"partitionBy will create it","timestamp":"1711026960.0"}],"timestamp":"1709051820.0"}],"answer_images":["https://img.examtopics.com/dp-600/image229.png"],"answer":"","answer_description":"","timestamp":"2024-02-18 00:54:00","question_text":"HOTSPOT -\nYou have a Fabric tenant that contains a lakehouse.\nYou are using a Fabric notebook to save a large DataFrame by using the following code. df.write.partitionBy(“year”, “month”, “day”).mode(“overwrite”).parquet(“Files/SalesOrder”)\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\n//IMG//","question_id":116,"unix_timestamp":1708214040,"url":"https://www.examtopics.com/discussions/microsoft/view/134096-exam-dp-600-topic-1-question-41-discussion/","answer_ET":"","question_images":["https://img.examtopics.com/dp-600/image43.png"],"topic":"1","answers_community":[]},{"id":"li0od1UC3mCgNpVgl44c","answer_ET":"C","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/134097-exam-dp-600-topic-1-question-42-discussion/","answer_description":"","answer_images":[],"topic":"1","question_images":["https://img.examtopics.com/dp-600/image45.png"],"question_id":117,"answer":"C","answers_community":["C (100%)"],"exam_id":71,"discussion":[{"comment_id":"1152943","upvote_count":"11","content":"Selected Answer: C\nCorrect","poster":"Momoanwar","timestamp":"1708214220.0"},{"poster":"Rakesh16","comment_id":"1312457","timestamp":"1731650940.0","upvote_count":"1","content":"Selected Answer: C\nUnpivot other columns"},{"content":"Selected Answer: C\nIMHO, \"C\",\n\nBecause of: Unpivot other columns\n This command unpivots unselected columns. Use this command in a query when not all columns are known. New columns added during a refresh operation are also unpivoted.\n\ndescription is here: https://support.microsoft.com/en-us/office/unpivot-columns-power-query-0f7bad4b-9ea1-49c1-9d95-f588221c7098#:~:text=Select%20the%20columns%20you%20do,Transform%20%3E%20Unpivot%20Only%20Selected%20Columns.","poster":"stilferx","comment_id":"1208639","upvote_count":"3","timestamp":"1715219640.0"},{"poster":"Nefirs","timestamp":"1713249000.0","comment_id":"1196399","content":"Selected Answer: C\nclearly C","upvote_count":"1"},{"comment_id":"1160824","poster":"XiltroX","content":"C is the correct answer","upvote_count":"3","timestamp":"1709051940.0"}],"choices":{"C":"Unpivot other columns","B":"Unpivot columns","E":"Remove other columns","A":"Group by","D":"Split column"},"unix_timestamp":1708214220,"timestamp":"2024-02-18 00:57:00","question_text":"You have a Fabric workspace named Workspace1 that contains a data flow named Dataflow1 contains a query that returns the data shown in the following exhibit.\n//IMG//\n\nYou need to transform the data columns into attribute-value pairs, where columns become rows.\nYou select the VendorID column.\nWhich transformation should you select from the context menu of the VendorID column?"},{"id":"A7E7nJ25RbmEhParcQgT","answers_community":["C (98%)","3%"],"discussion":[{"timestamp":"1708387440.0","poster":"Sanji931","comment_id":"1154365","upvote_count":"17","content":"Selected Answer: C\nAnswer C : Weekly.\nThe only way to do this is to set the schedule to ''Weekly'', set the days on Monday and Friday and add manually 6 Time of 4 hour intervals."},{"poster":"Rakesh16","content":"Selected Answer: C\nWeekly","upvote_count":"1","comment_id":"1312459","timestamp":"1731651000.0"},{"content":"Selected Answer: C\nWeekly","comment_id":"1277773","poster":"Bonga1","upvote_count":"1","timestamp":"1725386340.0"},{"comment_id":"1219883","upvote_count":"3","content":"Selected Answer: C\nSchedule Type: Weekly\nDays: Monday, Friday\nInterval: Every 4 hours","poster":"282b85d","timestamp":"1716850560.0"},{"comment_id":"1208642","upvote_count":"1","poster":"stilferx","content":"Selected Answer: C\nIMHO, \"C\" is good","timestamp":"1715219760.0"},{"poster":"Nefirs","content":"Selected Answer: C\nC - individual days can be selected only in Weekly schedule, and then add additional Times manually for each interval within a day.","timestamp":"1713249420.0","comment_id":"1196403","upvote_count":"2"},{"poster":"dvenkatesh","upvote_count":"2","comment_id":"1190962","timestamp":"1712495760.0","content":"I would pick D.\nTo ensure that the pipeline runs every four hours on Mondays and Fridays, you should set the Repeat for the schedule to \"4 hours\"."},{"comment_id":"1160827","upvote_count":"4","poster":"XiltroX","content":"Selected Answer: C\nThis is a no-brainer as the only way to make this work is to set the schedule as weekly and specify the pipeline to run on Mondays and Wednesdays.","timestamp":"1709052000.0"},{"poster":"Momoanwar","content":"Selected Answer: C\nWeekly","comment_id":"1152944","upvote_count":"3","timestamp":"1708214280.0"},{"upvote_count":"4","timestamp":"1707918120.0","comment_id":"1150248","poster":"Nicofr","content":"Selected Answer: C\nWeekly allow to choose the days and the time"},{"upvote_count":"3","poster":"fabric1","comment_id":"1149045","content":"Selected Answer: C\nHi IshtarSQL, \nthe selection of individual week days is only possible in the \"Weekly\" schedule option. Therefore, C is correct here.","timestamp":"1707817620.0"},{"timestamp":"1707510660.0","content":"Selected Answer: A\nTo ensure that the pipeline runs every four hours on Mondays and Fridays, you should set the \"Repeat\" frequency for the schedule to \"Daily\" and set the \"Interval\" to 4 hours. Then, you can specify the days of the week when the pipeline should run by selecting only Mondays and Fridays.","comment_id":"1145778","upvote_count":"1","poster":"IshtarSQL"}],"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/133470-exam-dp-600-topic-1-question-43-discussion/","timestamp":"2024-02-09 21:31:00","answer_images":[],"question_id":118,"answer":"C","answer_description":"","choices":{"C":"Weekly","A":"Daily","D":"Hourly","B":"By the minute"},"question_text":"You have a Fabric tenant that contains a data pipeline.\nYou need to ensure that the pipeline runs every four hours on Mondays and Fridays.\nTo what should you set Repeat for the schedule?","unix_timestamp":1707510660,"isMC":true,"exam_id":71,"answer_ET":"C","topic":"1"},{"id":"kGGa9NOmStG423lvjKen","topic":"1","exam_id":71,"question_text":"You have a Fabric tenant that contains a warehouse.\nSeveral times a day, the performance of all warehouse queries degrades. You suspect that Fabric is throttling the compute used by the warehouse.\nWhat should you use to identify whether throttling is occurring?","answer_images":[],"answer_description":"","question_images":[],"question_id":119,"unix_timestamp":1708214340,"url":"https://www.examtopics.com/discussions/microsoft/view/134098-exam-dp-600-topic-1-question-44-discussion/","timestamp":"2024-02-18 00:59:00","answer":"D","answers_community":["D (100%)"],"discussion":[{"poster":"XiltroX","upvote_count":"13","content":"Selected Answer: D\nFrom MS Learn: \nThe Microsoft Capacity Metrics app, also known as the metrics app, serves as a monitoring tool within the Microsoft Power BI service. It offers functionalities to track and analyze the resource utilization","timestamp":"1709052180.0","comment_id":"1160828"},{"timestamp":"1731651000.0","poster":"Rakesh16","comment_id":"1312460","content":"Selected Answer: D\nthe Microsoft Fabric Capacity Metrics app","upvote_count":"1"},{"upvote_count":"3","poster":"AndreaRosho1","content":"Selected Answer: D\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/compute-capacity-smoothing-throttling\n\"Just like most Warehouse operations, dynamic management views (DMVs) are also classified as background and covered by the \"Background Rejection\" policy. As a result, DMVs cannot be queried when capacity is throttled. Even though DMVs are not available, capacity admins can go to Microsoft Fabric Capacity Metrics app to understand the root cause.\"","comment_id":"1218294","timestamp":"1716638580.0"},{"upvote_count":"1","content":"Selected Answer: D\nIMHO, \"D\"\n\nBecause:\nTrack rejected operations\nThe Microsoft Fabric Capacity Metrics app drilldown allows admins to see operations that were rejected during a throttling event. \n\nHere: https://learn.microsoft.com/en-us/fabric/enterprise/throttling#track-rejected-operations","timestamp":"1715220060.0","comment_id":"1208648","poster":"stilferx"},{"poster":"prabhjot","upvote_count":"1","comment_id":"1189983","timestamp":"1712330460.0","content":"yes Ans D - https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app"},{"comment_id":"1152945","upvote_count":"2","timestamp":"1708214340.0","content":"Selected Answer: D\nMicrosoft Fabric Capacity Metrics app for trottling","poster":"Momoanwar"}],"isMC":true,"choices":{"C":"dynamic management views (DMVs)","A":"the Capacity settings","D":"the Microsoft Fabric Capacity Metrics app","B":"the Monitoring hub"},"answer_ET":"D"},{"id":"zLxTvcxQ7pUEKsFaTmyh","timestamp":"2024-02-18 01:30:00","url":"https://www.examtopics.com/discussions/microsoft/view/134102-exam-dp-600-topic-1-question-45-discussion/","exam_id":71,"question_text":"HOTSPOT -\nYou have a Fabric workspace that uses the default Spark starter pool and runtime version 1.2.\nYou plan to read a CSV file named Sales_raw.csv in a lakehouse, select columns, and save the data as a Delta table to the managed area of the lakehouse. Sales_raw.csv contains 12 columns.\nYou have the following code.\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\n//IMG//","isMC":false,"answer":"","answers_community":[],"topic":"1","answer_description":"","answer_ET":"","unix_timestamp":1708216200,"question_images":["https://img.examtopics.com/dp-600/image51.png","https://img.examtopics.com/dp-600/image52.png"],"discussion":[{"comment_id":"1170038","upvote_count":"37","poster":"metiii","content":"1. No, this is called filter pushdown / predicate pushdown / column pruning. This config is available when reading from a columnar type like parquet, I didn't find anything related to csv, I know that you can pushdown a predicate on csv to make it only read some rows in that case it works but it probably doesn't work for selecting columns so spark will read the entire file then filters the columns.\n2. Yes partitioning creates some overhead since Spark needs to create more files\n3. Yes infereSchema forces spark to read the file twice once for schema and once for data","timestamp":"1710042540.0"},{"timestamp":"1716898800.0","content":"N-N-Y\n•No: The Spark engine will initially read all columns from the CSV file because the .select() transformation is applied after the data has been read into memory. Therefore, all 12 columns from Sales_raw.csv are read before the selection of specific columns is applied.\n•No: Removing the partition might not necessarily reduce the execution time. While there might be some overhead in writing data to partitions, the overall impact on read performance, especially for large datasets, is usually beneficial. The query execution time for saving might be higher due to partitioning, but the read performance improvement usually outweighs this cost.\n•Yes: Adding inferSchema = 'true' will increase the execution time of the query because Spark will need to read through the entire dataset to determine the data types of each column. This extra pass over the data adds to the initial read time.","comments":[{"comment_id":"1323975","upvote_count":"1","poster":"Huepig","content":"This is not correct. Partitioning is not recommended because of both read and write overheads. Most often than not, partitioning results in “many small file” problem and data skew. Also, partitioning will result in reshuffle which causes longer write durations.\nIn this case, unless the csv is so massive that each “year” partition is greater 128mb as parquet, the partition will increase both read and write durations.\nTo add to this","timestamp":"1733738880.0"},{"timestamp":"1717401600.0","comment_id":"1223484","content":"This is correct","poster":"lelima","upvote_count":"1"}],"upvote_count":"20","comment_id":"1220222","poster":"282b85d"},{"poster":"testtaker45","content":"N, Y, Y\nLol, such a controversial question. \n1. The key word is read. Load will read all of them, it will only select the specified. When Spark does a .select(), it will read everything into mem first. \n2. Removing the partition would reduce compute requirements, making the query execute faster, but would likely increase the in memory footprint as ultimately more is loaded into memory, not just a part. \n3.infer_schema will try to guess the type, like int, date, etc, but it requires compute. This will slow down the query.\n\nI hope this helps someone!","timestamp":"1737213300.0","comment_id":"1342563","upvote_count":"1"},{"poster":"Rakesh16","content":"No,No,yes","comment_id":"1312461","upvote_count":"1","timestamp":"1731651000.0"},{"comment_id":"1250007","upvote_count":"1","poster":"Pegooli","content":"I'm going to Y-N-N","timestamp":"1721253240.0"},{"comment_id":"1230841","upvote_count":"11","timestamp":"1718440740.0","content":"I took the test today. This question was included, but the option 'Removing the partition will reduce the execution time of the query' has been replaced by 'Will the Year column replace the OrderDate column?'. My answer was No.","comments":[{"poster":"radamantes","timestamp":"1736339520.0","upvote_count":"1","comment_id":"1337939","content":"Did you approve? Most of question were in this site? Can you share?"}],"poster":"calvintcy"},{"poster":"mnc_1997","timestamp":"1716815760.0","comments":[{"poster":"mnc_1997","upvote_count":"2","comment_id":"1219587","timestamp":"1716816780.0","content":"also, what spark does is perform a lazy evaluation approach, it does not read each method(read,load,option) into memory. The actual reading happens when an action is performed such as(display,show,write). Spark will create a plan on how to execute the entire query and will optimize this plan for efficient execution."}],"comment_id":"1219570","content":"just tried it, it only writes the columns that were selected.\nAnswer: YNY","upvote_count":"2"},{"upvote_count":"6","poster":"stilferx","comment_id":"1208651","content":"IMHO, \n1. N\n2. N - arguable\n3. Y\n\n1 No - because it is CSV. It will be read in full (in contrast to parquet)\n2 No - well, maybe 0.5% slower due to creating a new files. But actually - no\n3 Yes - because infering schema - it is additional process","timestamp":"1715220660.0"},{"timestamp":"1715078040.0","poster":"vish9","comment_id":"1207838","content":"No, CSV will be read in full and then filtered. \nNo: Using the partition by clause in Spark's Delta format can impact write performance in several ways:\n\nIncreased Write Throughput: Partitioning your data can potentially increase write throughput by distributing the write workload across multiple partitions. This parallelism allows Spark to write data to different partitions concurrently, improving overall write performance, especially when dealing with large datasets.\nY. Infer schema will slow the performance","upvote_count":"5"},{"timestamp":"1714285740.0","content":"I would go with NYY.\n\nIt's a CSV it is a row format, I don't think you can separate it by columns before reading the entire content.\n\nPartitioning takes extra work, so it may slow down the proccess.\n\nInferSchema requires an extra scan of the document or I think so, so maybe, I will go with yes.","poster":"dp600","comment_id":"1203395","upvote_count":"2"},{"timestamp":"1714214940.0","upvote_count":"1","comment_id":"1203067","poster":"DilumD","content":"1. Yes: Reason: Select columns: The code selects specific columns from the DataFrame using the select method. The selected columns are \"SalesOrderNumber\", \"OrderDate\", \"CustomerName\", and \"UnitPrice\".\n2. Yes: Reason: removing the partitionBy will simplify the process. Partitioning data involves some overhead in organizing the data into separate folders/files based on the partitioning column.\n3. No: Reason: Potentially Slower: Enabling inferSchema generally results in a slightly slower initial read operation. This is because Spark needs to do an additional scan of a portion of your data to analyze and determine data types before loading it."},{"poster":"wellingtonluis","upvote_count":"5","content":"After read all file, engine will select just some. But, initially it runs the entire file.","comment_id":"1164999","timestamp":"1709490420.0"},{"timestamp":"1709052480.0","upvote_count":"11","content":"The answer is probably YNY\n1. Those are exactly the columns that are being read. So Yes\n2. Removing the PartitionBy line would not result in any performance changes. So NO\n3. Adding inferSchema as True WILL result in extra time in execution as it will make the engine go over the data twice (one to read data and the other time to read Schema). So YES.","comment_id":"1160832","poster":"XiltroX"},{"comment_id":"1159214","timestamp":"1708896300.0","upvote_count":"2","poster":"estrelle2008","content":"full of typos, this one.\nAnyhow, my guess:\nYNN\ninferSchema=true helps automatically determine column data types, but it needs a extra pass over the data, which comes with a slight query performance cost. So last statement = No"},{"comment_id":"1152956","content":"Its read not red.\nThis question is ambiguous would say : no no yes.\nFor the point 1 : with case sensitivity sales_raw is not Sales_raw","timestamp":"1708216200.0","poster":"Momoanwar","upvote_count":"5"}],"answer_images":["https://img.examtopics.com/dp-600/image230.png"],"question_id":120}],"exam":{"lastUpdated":"12 Apr 2025","isMCOnly":false,"provider":"Microsoft","name":"DP-600","isImplemented":true,"isBeta":false,"numberOfQuestions":179,"id":71},"currentPage":24},"__N_SSP":true}