{"pageProps":{"questions":[{"id":"lbI4U8R64KVJNzTzdMcV","question_text":"You manage a solution that uses Azure HDInsight clusters.\nYou need to implement a solution to monitor cluster performance and status.\nWhich technology should you use?","exam_id":66,"discussion":[{"comment_id":"279218","content":"good luck everyone!","upvote_count":"17","timestamp":"1611913440.0","poster":"someth1ng"},{"upvote_count":"5","timestamp":"1610766480.0","poster":"DannyDaj","content":"It also appeared in DP-200","comment_id":"268490"},{"content":"this is not the part of the exam anymore","upvote_count":"2","comment_id":"340894","timestamp":"1619080800.0","poster":"anamaster"},{"upvote_count":"1","timestamp":"1616280000.0","content":"C Ambari Rest API For sure","poster":"burkinofaso","comment_id":"315898","comments":[{"comment_id":"375555","timestamp":"1622944020.0","upvote_count":"1","poster":"BobFar","content":"Ambari Rest Api is for developer"}]}],"url":"https://www.examtopics.com/discussions/microsoft/view/42501-exam-dp-201-topic-4-question-16-discussion/","unix_timestamp":1610766480,"answer_ET":"E","question_id":176,"choices":{"B":"Azure HDInsight REST API","A":"Azure HDInsight.NET SDK","D":"Azure Log Analytics","E":"Ambari Web UI","C":"Ambari REST API"},"answer_images":[],"question_images":[],"answer":"E","answer_description":"Ambari is the recommended tool for monitoring utilization across the whole cluster. The Ambari dashboard shows easily glanceable widgets that display metrics such as CPU, network, YARN memory, and HDFS disk usage. The specific metrics shown depend on cluster type. The \"Hosts\" tab shows metrics for individual nodes so you can ensure the load on your cluster is evenly distributed. The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.\nReference:\nhttps://azure.microsoft.com/en-us/blog/monitoring-on-hdinsight-part-1-an-overview/ https://ambari.apache.org/","topic":"4","answers_community":[],"isMC":true,"timestamp":"2021-01-16 04:08:00"},{"id":"NS0Vlyqxn2qbL8GaqLyY","isMC":true,"answer_images":[],"answer_description":"User-Defined Restore Points -\nThis feature enables you to manually trigger snapshots to create restore points of your data warehouse before and after large modifications. This capability ensures that restore points are logically consistent, which provides additional data protection in case of any workload interruptions or user errors for quick recovery time.\nNote: A data warehouse restore is a new data warehouse that is created from a restore point of an existing or deleted data warehouse. Restoring your data warehouse is an essential part of any business continuity and disaster recovery strategy because it re-creates your data after accidental corruption or deletion.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/backup-and-restore","question_id":177,"choices":{"A":"Yes","B":"No"},"discussion":[{"poster":"Ash1602","comments":[{"poster":"D_Duke","comment_id":"225742","upvote_count":"4","content":"Although I've given you a thumb as I believe in real world your words make perfect sense, I must say that the question seems to be tricky as it states that the 'upload and corruption check shouldn't impact reporting', regardless of the restoration to a previous point. That way the answer is correct.","timestamp":"1606131120.0"}],"upvote_count":"9","timestamp":"1594208160.0","comment_id":"129678","content":"I think this should be NO, as rollback to restore point will affect the reporting and analytic processes...\nWhen the data is found corrupted, a rollback to previous restore point would happen, causing a lag.\nAnd even when the file is uploaded, the corrupted data would reside in the db until it is detected by corruption check and restored to previous point... that would also affect reporting...."},{"timestamp":"1595510580.0","upvote_count":"9","content":"why not ingest into staging layer, perform validations and decide whether to load into DWH?","poster":"cmihai","comment_id":"142036"},{"poster":"syu31svc","timestamp":"1607439120.0","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/backup-and-restore#user-defined-restore-points:\n\"This feature enables you to manually trigger snapshots to create restore points of your data warehouse before and after large modifications. This capability ensures that restore points are logically consistent, which provides additional data protection in case of any workload interruptions or user errors for quick recovery time\"\nAnswer is correct","comment_id":"238383","upvote_count":"9"}],"timestamp":"2020-07-08 13:36:00","question_images":[],"answer_ET":"A","answers_community":[],"exam_id":66,"url":"https://www.examtopics.com/discussions/microsoft/view/25089-exam-dp-201-topic-4-question-17-discussion/","unix_timestamp":1594208160,"topic":"4","answer":"A","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nA company is developing a solution to manage inventory data for a group of automotive repair shops. The solution will use Azure Synapse Analytics as the data store.\nShops will upload data every 10 days.\nData corruption checks must run each time data is uploaded. If corruption is detected, the corrupted data must be removed.\nYou need to ensure that upload processes and data corruption checks do not impact reporting and analytics processes that use the data warehouse.\nProposed solution: Create a user-defined restore point before data is uploaded. Delete the restore point after data corruption checks complete.\nDoes the solution meet the goal?"},{"id":"X0LpDYzoBCv3BsYYMB1a","answer":"B","answer_images":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nA company is developing a solution to manage inventory data for a group of automotive repair shops. The solution will use Azure Synapse Analytics as the data store.\nShops will upload data every 10 days.\nData corruption checks must run each time data is uploaded. If corruption is detected, the corrupted data must be removed.\nYou need to ensure that upload processes and data corruption checks do not impact reporting and analytics processes that use the data warehouse.\nProposed solution: Configure database-level auditing in Azure Synapse Analytics and set retention to 10 days.\nDoes the solution meet the goal?","timestamp":"2021-06-25 19:12:00","question_id":178,"url":"https://www.examtopics.com/discussions/microsoft/view/56077-exam-dp-201-topic-4-question-18-discussion/","topic":"4","answer_description":"Instead, create a user-defined restore point before data is uploaded. Delete the restore point after data corruption checks complete.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/backup-and-restore","unix_timestamp":1624641120,"answers_community":[],"choices":{"B":"No","A":"Yes"},"exam_id":66,"isMC":true,"answer_ET":"B","question_images":[],"discussion":[{"timestamp":"1624641120.0","content":"correct","upvote_count":"1","poster":"mjr","comment_id":"390668"}]},{"id":"Y38xrAP6KFOBR6p9hEm4","answers_community":[],"answer_images":[],"question_text":"You plan to implement an Azure Data Lake Gen2 storage account.\nYou need to ensure that the data lake will remain available if a data center fails in the primary Azure region. The solution must minimize costs.\nWhich type of replication should you use for the storage account?","timestamp":"2021-03-16 14:10:00","exam_id":66,"unix_timestamp":1615900200,"question_images":[],"choices":{"D":"geo-zone-redundant storage (GZRS)","A":"geo-redundant storage (GRS)","B":"zone-redundant storage (ZRS)","C":"locally-redundant storage (LRS)"},"url":"https://www.examtopics.com/discussions/microsoft/view/47326-exam-dp-201-topic-4-question-19-discussion/","topic":"4","isMC":true,"question_id":179,"discussion":[{"poster":"H_S","timestamp":"1615900200.0","comment_id":"312320","upvote_count":"35","content":"B. zone-redundant storage (ZRS)\nif a data center fails in the primary rigion, another datacenter in the same region could be a good solution and less expensive, do it's ZRS","comments":[{"comment_id":"357614","upvote_count":"6","content":"key words are \"fails in primary Azure region\" . ZRS is within same region so answer GRS is correct","timestamp":"1621057980.0","poster":"Hrabia","comments":[{"content":"IMO, key words are: \"a data center fails\" and \"minimize costs\" -> ZRS","upvote_count":"4","timestamp":"1621425840.0","poster":"alain2","comment_id":"361344","comments":[{"content":"I agree it says in the question that a \"data center fails in the primary Azure region\" not the full Azure region is down so the correct answer is B as cost needs to be reduced as well","poster":"uzairahm","timestamp":"1656135180.0","upvote_count":"1","comment_id":"621972"}]}]}]},{"poster":"Ous01","content":"A region consists of more than one data center. If a data center fails, it doesn't mean the whole region is down. ZRS should be the right answer.","upvote_count":"2","comment_id":"366774","timestamp":"1621992480.0"},{"content":"A is correct. To quote the provided link below: \"However, ZRS by itself may not protect your data against a regional disaster where multiple zones are permanently affected\". We can choose either GRS or GZRS. Since GRS is cheaper, it should be chosen.\nLink: https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy","timestamp":"1621735200.0","comment_id":"364046","poster":"toandm","upvote_count":"4","comments":[{"upvote_count":"3","content":"here is not mentioned regional disaster. you must focus to datacenter fails. So ZRS is enough.\n>>Zone-redundant storage (ZRS) copies your data synchronously across three Azure availability zones in the primary region.","timestamp":"1621791960.0","poster":"memo43","comment_id":"364874"}]},{"upvote_count":"4","content":"I think the given ans is correct, it should be GRS","poster":"Nidshi","comment_id":"350700","comments":[{"upvote_count":"1","content":"Correct answer is ZRS, as we have always more than one data center per region and we want to minimize costs.","comment_id":"488858","poster":"Larrave","timestamp":"1638078120.0"},{"timestamp":"1625640120.0","content":"Why? It says only a data centre has failed in the primary region, not that the whole region is fubared.","upvote_count":"1","poster":"captainbee","comment_id":"400556"}],"timestamp":"1620276240.0"}],"answer":"A","answer_ET":"A","answer_description":"Geo-redundant storage (GRS) copies your data synchronously three times within a single physical location in the primary region using LRS. It then copies your data asynchronously to a single physical location in the secondary region.\nIncorrect Answers:\nB: Zone-redundant storage (ZRS) copies your data synchronously across three Azure availability zones in the primary region. For applications requiring high availability, Microsoft recommends using ZRS in the primary region, and also replicating to a secondary region.\nC: Locally redundant storage (LRS) copies your data synchronously three times within a single physical location in the primary region. LRS is the least expensive replication option, but is not recommended for applications requiring high availability.\nD: GZRS is more expensive compared to GRS.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy"},{"id":"4NvDKU7BTUyDLfxh0Ssr","isMC":true,"answer_description":"Stream Analytics guarantees jobs in paired regions are updated in separate batches. As a result there is a sufficient time gap between the updates to identify potential breaking bugs and remediate them.\nCustomers are advised to deploy identical jobs to both paired regions.\nIn addition to Stream Analytics internal monitoring capabilities, customers are also advised to monitor the jobs as if both are production jobs. If a break is identified to be a result of the Stream Analytics service update, escalate appropriately and fail over any downstream consumers to the healthy job output. Escalation to support will prevent the paired region from being affected by the new deployment and maintain the integrity of the paired jobs.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-job-reliability","question_text":"You are developing a solution that performs real-time analysis of IoT data in the cloud.\nThe solution must remain available during Azure service updates.\nYou need to recommend a solution.\nWhich two actions should you recommend? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer_ET":"BC","exam_id":66,"answer":"BC","discussion":[{"upvote_count":"3","poster":"Psycho","content":"The given answer is correct","comment_id":"361411","timestamp":"1621430400.0"},{"comments":[{"comment_id":"267997","timestamp":"1610721600.0","poster":"ACSC","content":"Azure service updates don't occur in paired regions at the same time. If the regions are not paired they can be done at the same time. Given answer is correct.","upvote_count":"7"}],"poster":"AliSoley","timestamp":"1609745880.0","comment_id":"259198","content":"Why not AC, my thought is paired region Azure service update may happen in the same time. A said deploy it on seperate region that are not pair?","upvote_count":"1"},{"content":"why not BE ?","timestamp":"1600350420.0","comments":[{"poster":"ARahman","content":"E says \"to one region\", so the service will not be available in any other region if this one region is having an update.","upvote_count":"3","timestamp":"1600764360.0","comment_id":"184281"}],"comment_id":"180974","upvote_count":"1","poster":"kiemberaid"}],"answer_images":[],"unix_timestamp":1600350420,"url":"https://www.examtopics.com/discussions/microsoft/view/31470-exam-dp-201-topic-4-question-2-discussion/","timestamp":"2020-09-17 15:47:00","answers_community":[],"topic":"4","question_id":180,"question_images":[],"choices":{"A":"Deploy an Azure Stream Analytics job to two separate regions that are not in a pair.","C":"Monitor jobs in both regions for failure.","D":"Monitor jobs in the primary region for failure.","B":"Deploy an Azure Stream Analytics job to each region in a paired region.","E":"Deploy an Azure Stream Analytics job to one region in a paired region."}}],"exam":{"isImplemented":true,"lastUpdated":"12 Apr 2025","id":66,"provider":"Microsoft","name":"DP-201","numberOfQuestions":206,"isBeta":false,"isMCOnly":false},"currentPage":36},"__N_SSP":true}