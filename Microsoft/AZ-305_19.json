{"pageProps":{"questions":[{"id":"PfMAX0gSWxHX8S8Hff1n","exam_id":54,"answers_community":[],"topic":"2","question_id":91,"discussion":[{"poster":"IRISone","timestamp":"1673888940.0","content":"1. correct\n2. Should be COntainer Policy for immutable storage. A resource lock does not prevent removal of files and folders. Prevents deleting resource inside the resource group","upvote_count":"74","comment_id":"777972"},{"poster":"OPT_001122","upvote_count":"32","comment_id":"790555","timestamp":"1674906180.0","content":"1 is correct\n2 - Container access policy"},{"timestamp":"1731424320.0","poster":"[Removed]","content":"WRONG\n\n1. General purpose v2 with Hot access tier for blobs\n\n2. Container access policy","comment_id":"1310679","upvote_count":"3"},{"content":"1. GPv2 with hot access tier for blobs\n 2. Container access policy","comment_id":"1308278","poster":"Thanveer","upvote_count":"1","timestamp":"1730967420.0"},{"comments":[{"upvote_count":"1","content":"Storage account resource lock:\n\nReason against: While a resource lock can prevent the entire storage account or container from being deleted, it doesn't inherently enforce immutability of the data. Resource locks are more suited for protecting against accidental deletion or modification of the resource itself, not the data within it.\nThe Cool access tier is optimized for infrequently accessed data, which means lower storage costs but higher access charges. Since the data will be accessed daily, the higher access charges could lead to higher overall costs, which does not align with the requirement to minimize data access charges","timestamp":"1719073920.0","poster":"23169fd","comment_id":"1235504"}],"comment_id":"1235503","content":"Storage account type: General purpose v2 with Hot access tier for blobs: This option provides a balance between performance and cost, ideal for daily access.\nConfiguration to prevent modifications and deletions: Container access policy with an immutable blob policy: This ensures that data cannot be modified or deleted, strictly adhering to the requirement of read-only access until the data can be deleted after five years.","poster":"23169fd","timestamp":"1719073860.0","upvote_count":"2"},{"poster":"Lazylinux","content":"Storage account type: General purpose v2 with HOT access tier for blobs, it is most expensive in storage cost but CHEAPEST in access costs and hence meets requirements\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview\n\nOnce the data is written, the data can only be read. Modifications and deletion must be prevented. is called WORM and can only be done via policy and hence container access policy.\nFor those who say Storage resource lock is NOT correct because it can achieve the required goal however it fails to stop it being modified i.e. in order to delete the storage/blob you need to remove the lock from read and make it modify at that moment no guarantee that no one is accessing it and modifying it and other problem is resource lock are way to administrative as compared to Container policy (automated) i.e. who will remember to remove lock of storage acc and allow for delete without ever being modified after 5 years!! Good Luck :)","timestamp":"1714910460.0","comment_id":"1206873","upvote_count":"1"},{"timestamp":"1714808280.0","comment_id":"1206381","comments":[{"content":"I tend to disagree, please read my comments.. cool is more expensive in access charges than hot, resource lock will not allow you delete without remove the lock and hence too much administration","comment_id":"1206867","upvote_count":"1","poster":"Lazylinux","timestamp":"1714909920.0"}],"upvote_count":"1","content":"Storage account type: General purpose v2 with Cool access tier for blobs. This option is suitable because it allows storing data that is infrequently accessed, which helps minimize data access charges, and it supports blob storage which can handle datasets under 10 GB. \nConfiguration to prevent modifications and deletions: Storage account resource lock. This option prevents modifications or deletions of the storage account resources, ensuring that once written, the data can only be read as required.","poster":"Chenn"},{"upvote_count":"3","poster":"xRiot007","timestamp":"1707915720.0","content":"Box 2 is a container access policy (for immutable storage) \nA resource lock will only prevent the modification/deletion of the said resource, not of the data inside of that resource.","comment_id":"1150225"},{"comment_id":"868385","upvote_count":"9","poster":"NagaByrd","timestamp":"1681304760.0","content":"Same as Question #17\n\n1. GPv2 with hot access tier for blobs 2. Container access policy","comments":[{"comment_id":"1017950","timestamp":"1695743460.0","upvote_count":"4","content":"It's not the same question #17. This question have a different option (Premium Block blobs) and this is the correct answer, because access cost to Premium Block is cheaper than GPv2 hot tier","poster":"Elecktrus"}]},{"comment_id":"847327","timestamp":"1679505300.0","upvote_count":"13","content":"1. Storage account type:\nC. General purpose v2 with hot access tier for blobs\nThe hot access tier provides lower data access costs compared to the cool access tier, making it more suitable for minimizing charges when data is accessed daily. Although the cool tier has lower storage costs, the data access charges are higher, which would not be ideal for your scenario. Premium block blobs are meant for high-performance scenarios and are not necessary for a small dataset of less than 10 GB.\n\n2. Configuration to prevent modifications and deletions:\nB. Container access policy\nYou can create a container access policy with specific permissions (in this case, read-only) and set an expiry time of five years. This policy prevents modifications and deletions, while still allowing the data to be read. After five years, the policy will expire, and the data can be deleted but not modified. Storage account resource locks and container access level settings don't offer the same granularity of control over the data as the container access policy.","poster":"NotMeAnyWay"},{"comments":[{"content":"This is correct answer!!!\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview","comment_id":"853283","upvote_count":"1","timestamp":"1680009960.0","poster":"AzureMasterChamp"}],"upvote_count":"6","timestamp":"1677526560.0","content":"1. Premium block blobs\n2. Container Policy\nhttps://azure.microsoft.com/en-us/pricing/details/storage/blobs/\nOperations and data transfer","comment_id":"824081","poster":"Rams_84zO6n"},{"comments":[{"upvote_count":"2","timestamp":"1697242200.0","comment_id":"1043054","content":"The question mentioned \"The data will be accessed daily.\". Can not assume super high read frequency.","poster":"OrangeSG"}],"poster":"Rams_84zO6n","timestamp":"1677078480.0","content":"premium block blob, container access policy\nWe only need to minimize access charge, not storage cost. \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-block-blob-premium\nIn other words, to access same amount of data within a given time - 500 million reads /second. That is much faster than GPv2 hot tier milisecond access time","comment_id":"817963","upvote_count":"2"},{"comment_id":"812577","poster":"zellck","upvote_count":"2","content":"1. GPv2 with hot access tier for blobs\n2. Container access policy\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview\nImmutable storage for Azure Blob Storage enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data cannot be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes.","timestamp":"1676688360.0"},{"content":"For Data Storage and Access, Premium will be the cheapest. https://azure.microsoft.com/en-us/pricing/details/storage/blobs/","upvote_count":"2","comment_id":"809781","poster":"Amrx","timestamp":"1676481060.0"},{"content":"The second answer is wrong. A resource lock does exactly what the name suggests - it locks the resource itself. To prevent modification of the files, a container access policy is needed.","comment_id":"807301","poster":"PatFFM","upvote_count":"1","timestamp":"1676285820.0"},{"poster":"ed79","content":"Why not Premium block blobs and the storage amount is small. Access charges are cheapest in the scenario","comment_id":"796443","timestamp":"1675372080.0","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"798000","content":"Imho - GPV2 will be cheaper than Premium Block Blobs. You can check with Azure pricing calculator.","comments":[{"content":"Check it out Premium is cheaper for data access\nhttps://azure.microsoft.com/en-us/pricing/details/storage/blobs/?cdn=disable","comment_id":"800189","timestamp":"1675712100.0","upvote_count":"2","poster":"ed79"}],"timestamp":"1675518000.0","poster":"CallmeZdzisiek"}]},{"comment_id":"783998","timestamp":"1674370500.0","upvote_count":"5","poster":"LeeVee","content":"GPv2 and Container Policy"}],"answer_images":["https://img.examtopics.com/az-305/image167.png"],"isMC":false,"answer":"","question_images":["https://img.examtopics.com/az-305/image166.png"],"timestamp":"2023-01-16 18:09:00","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/95594-exam-az-305-topic-2-question-21-discussion/","answer_ET":"","question_text":"HOTSPOT\n-\n\nYou are planning an Azure Storage solution for sensitive data. The data will be accessed daily. The dataset is less than 10 GB.\n\nYou need to recommend a storage solution that meets the following requirements:\n\n• All the data written to storage must be retained for five years.\n• Once the data is written, the data can only be read. Modifications and deletion must be prevented.\n• After five years, the data can be deleted, but never modified.\n• Data access charges must be minimized.\n\nWhat should you recommend? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","unix_timestamp":1673888940},{"id":"zrF3hUC16MGyVCxrYns9","answer_description":"","answer_ET":"","exam_id":54,"question_text":"HOTSPOT\n-\n\nYou are designing a data analytics solution that will use Azure Synapse and Azure Data Lake Storage Gen2.\n\nYou need to recommend Azure Synapse pools to meet the following requirements:\n\n• Ingest data from Data Lake Storage into hash-distributed tables.\n• Implement query, and update data in Delta Lake.\n\nWhat should you recommend for each requirement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/95788-exam-az-305-topic-2-question-22-discussion/","topic":"2","question_id":92,"question_images":["https://img.examtopics.com/az-305/image168.png"],"answers_community":[],"answer":"","unix_timestamp":1674027900,"isMC":false,"timestamp":"2023-01-18 08:45:00","answer_images":["https://img.examtopics.com/az-305/image169.png"],"discussion":[{"timestamp":"1675619700.0","comment_id":"799051","poster":"saiyandjinn","comments":[{"upvote_count":"2","comments":[{"comment_id":"1167213","content":"Ah I take it back, Delta lake is also mentioned later, sry for the confusion.","timestamp":"1709734500.0","poster":"Fidel_104","upvote_count":"2"}],"content":"The question mentions 'Data Lake Storage', not Delta Lake - there is no explicit indication that the data is stored in a delta lake format. Therefore I don't think that the Spark pool is needed.\n\nNevertheless, Delta Lake is indeed a very confusing name for what is essentially a data format (\"optimized storage layer\").","timestamp":"1709734380.0","poster":"Fidel_104","comment_id":"1167209"},{"upvote_count":"1","timestamp":"1710918060.0","content":"Apache Spark pools in Azure Synapse enable data engineers to modify Delta Lake files\nTaken from:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format","poster":"WeepingMaplte","comment_id":"1177987"},{"upvote_count":"2","content":"Agree.\nFrom what I can find SQL pool can't update delta lake files only Apache Spark can do that, assuming article is accurate below;\n\nhttps://www.jamesserra.com/archive/2022/03/azure-synapse-and-delta-lake/#:~:text=Serverless%20SQL%20pools%20do%20not%20support%20updating%20delta,in%20Azure%20Synapse%20Analytics%20to%20update%20Delta%20Lake.","timestamp":"1675772820.0","poster":"RandomNickname","comment_id":"800872"}],"upvote_count":"36","content":"The second question is confusing, and I am not sure what the answer is\n- Can query delta lake with Serverless SQL pool but won't be able to update it. \n- Only Apache Spark pools support updates to Delta Lakes files. It can also be used to query long-time series as well if I understand the doc correctly...\n\nI think the answer to 2 is Apache Spark tools on that basis..."},{"upvote_count":"22","poster":"Liveroso","comments":[{"content":"How can you spent so much time to give explained answers, but you still get them wrong? First answer is correct, second one is Apache Spark pool.\n\nServerless SQL pool doesn't provides updates: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format. Do you see any information about updates there?\n\nUpdates are possible in Apache Spark: https://docs.delta.io/latest/delta-update.html\n\nBtw - what \"Apache Spark is best for big data analysis and ML tasks\" have in common with Delta Lake updates? Are you copying the answers from the ChatGPT? I have worked with Databricks for 2 years and Apache Spark is the right answer. Apache Spark can be also used for small scenarios as it's not that expensive and is often used by data engineers, not just big data engineers","comments":[{"comment_id":"982839","upvote_count":"11","timestamp":"1692208920.0","poster":"sawanti","content":"Last note - Hash-distributed tables are used for VERY LARGE FACT TABLES. As per documentation (https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute): Consider using a hash-distributed table when:\n\nThe table size on disk is more than 2 GB."}],"timestamp":"1692208800.0","poster":"sawanti","upvote_count":"30","comment_id":"982837"}],"comment_id":"789693","timestamp":"1674830460.0","content":"The answer is correct.\n\nAzure Synapse Analytics (also named SQL Data Warehouse) is a cloud-based analytics service that allows you to analyze large amounts of data using a combination of on-demand and provisioned resources. It offers several different options for working with data, including:\n\n- Dedicated SQL pool: It's best for big and complex tasks.\n\n- Serverless Apache Spark pool: It's best for big data analysis and machine learning tasks using Spark SQL and Spark DataFrames.\n\n- Serverless SQL pool: This is a service that automatically adjusts the amount of resources you use based on your needs. You only pay for what you use. It's best for small to medium-sized tasks and tasks that change often."},{"upvote_count":"1","timestamp":"1731424560.0","content":"WRONG\n\n1. A dedicated SQL pool\n\n2. A serverless Apache Spark pool","poster":"[Removed]","comment_id":"1310680"},{"upvote_count":"2","timestamp":"1728974040.0","poster":"c_h_r_i_s_","content":"1. Ingest data from Data Lake Storage into hash-distributed tables:\n • A dedicated SQL pool\nExplanation: Hash-distributed tables are a feature of dedicated SQL pools in Azure Synapse. They allow for efficient data distribution and parallel processing, which is ideal for large-scale data ingestion from Data Lake Storage.\n 2. Implement, query, and update data in Delta Lake:\n • A serverless Apache Spark pool\nExplanation: Serverless Apache Spark pools in Azure Synapse support Delta Lake, providing full read and write capabilities. They allow you to implement, query, and update Delta Lake tables effectively.\n\nAnswer Area:\n\n 1. A dedicated SQL pool\n 2. A serverless Apache Spark pool","comment_id":"1298012"},{"comment_id":"1281872","timestamp":"1726020480.0","poster":"Teerawee","upvote_count":"1","content":"dedicated SQL pool\nserverless Apache Spark pool"},{"upvote_count":"2","poster":"Len83","content":"This question appeared in the exam, August 2024, I gave this same answer for box 1 but answered Apache Spark Pool for box 2. I scored 870","timestamp":"1723199460.0","comment_id":"1262864"},{"poster":"Gaz_","comment_id":"1237976","upvote_count":"1","content":"From Copilot:\n\nTo meet the requirements for ingesting data from Data Lake Storage into hash-distributed tables, you should recommend A Dedicated SQL pool. This option is designed for large-scale, high-performance, and secure analytics on Azure.\n\nFor implementing, querying, and updating data in Delta Lake, you should recommend A serverless Apache Spark pool. This option allows you to run big data analytics and artificial intelligence workloads with Apache Spark, which is compatible with Delta Lake.\n\nThese recommendations align with Azure's best practices for performance and scalability when working with Synapse and Data Lake Storage Gen2. If you need further details or assistance with the setup, feel free to ask.","timestamp":"1719475620.0"},{"upvote_count":"2","comments":[{"poster":"23169fd","comment_id":"1235521","upvote_count":"2","content":"Requirement 1: Ingest data from Data Lake Storage into hash-distributed tables\nA dedicated SQL pool: This pool is specifically designed for high-performance data warehousing. It allows for the ingestion of large datasets into hash-distributed tables, optimizing performance and scalability. Hash distribution is a key feature of dedicated SQL pools to enhance query performance for large datasets.\nRecommendation: A dedicated SQL pool\n\nRequirement 2: Implement, query, and update data in Delta Lake\nA serverless Apache Spark pool: Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark. It is optimized for big data workloads and is best utilized with Apache Spark pools. The serverless Apache Spark pool in Azure Synapse provides a managed Spark environment, ideal for working with Delta Lake for querying, updating, and managing large datasets.","timestamp":"1719075720.0"}],"timestamp":"1719075720.0","content":"Ingest data from Data Lake Storage into hash-distributed tables: A dedicated SQL pool\nImplement, query, and update data in Delta Lake: A serverless Apache Spark poo","comment_id":"1235520","poster":"23169fd"},{"comment_id":"1206879","upvote_count":"1","poster":"Lazylinux","timestamp":"1714911000.0","content":"Box 1: is correct => A hash-distributed table distributes table rows across the Compute nodes by using a deterministic hash function to assign each row to one distribution.\nSince identical values always hash to the same distribution, SQL Analytics has built-in knowledge of the row locations. In dedicated SQL pool this knowledge is used to minimize data movement during queries, which improves query performance.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\nBox 2: should be Apache Spark pools \nin Azure Synapse enable data engineers to modify Delta Lake files\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format"},{"upvote_count":"1","timestamp":"1714808580.0","poster":"Chenn","content":"Ingest data from Data Lake Storage into hash-distributed tables: For this requirement, I recommend using a Dedicated SQL pool in Azure Synapse. This service is designed for large-scale data processing and supports creating hash-distributed tables to optimize query performance. \nImplement, query, and update data in Delta Lake: For this requirement, I recommend using a Serverless Apache Spark pool in Azure Synapse. This service provides capabilities for working with Delta Lake as it offers an analytics service that can handle big data processing tasks without the need to provision or manage clusters.","comment_id":"1206384"},{"upvote_count":"2","timestamp":"1711186740.0","content":"Dedicated SQL Pools\n\nPurpose: Dedicated SQL pools provide massive parallel processing (MPP) capabilities ideal for handling large volumes of data. They are optimized for complex queries over large datasets and are suitable for building enterprise-level, big data analytics solutions.\n\nSpark Pools\n\nPurpose: Spark pools in Azure Synapse provide a fully managed Apache Spark environment. They are designed to handle big data processing, analytics, and machine learning tasks. Spark pools can process data in various formats and from multiple sources, including Azure Data Lake Storage.","comment_id":"1180751","poster":"RanOlfati"},{"comment_id":"1173766","content":"from ChatGPT : For implementing, querying, and updating data in Delta Lake, the most suitable option among the ones you listed would be A serverless Apache Spark pool.\n\nHere's why:\n\n Integration with Delta Lake: Apache Spark is tightly integrated with Delta Lake, offering native support for reading from and writing to Delta tables. This integration ensures seamless compatibility and efficient data processing capabilities","upvote_count":"1","poster":"ahmedkmj","timestamp":"1710453120.0"},{"content":"OPTION 2: SERVERLESS APACHE SPARK POOL","timestamp":"1701017160.0","upvote_count":"3","comment_id":"1080825","poster":"Paul_white"},{"poster":"Exams_Prep_2021","comment_id":"1021124","upvote_count":"5","timestamp":"1696032600.0","content":"Got this on Sept. 29, 2023"},{"comment_id":"1016468","content":"I had question at 24th Sep 2023","upvote_count":"5","timestamp":"1695619380.0","poster":"Forex19"},{"upvote_count":"4","timestamp":"1695188280.0","comment_id":"1011946","content":"Serverless SQL pools don't support updating Delta Lake files. You can use serverless SQL pool to query the latest version of Delta Lake. Use Apache Spark pools in Synapse Analytics to update Delta Lake.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand?tabs=x80070002#delta-lake","poster":"salman_23_c4"},{"timestamp":"1692083820.0","content":"From MSFT docs: \n\nServerless SQL pools don't support updating Delta Lake files. You can use serverless SQL pool to query the latest version of Delta Lake. Use Apache Spark pools in Synapse Analytics to update Delta Lake.","poster":"calotta1","upvote_count":"6","comment_id":"981386"},{"content":"To meet the requirements of ingesting data from Data Lake Storage into hash-distributed tables and implementing query and update operations in Delta Lake, the recommended Azure Synapse pool options are as follows:\n\nIngest Data from Data Lake Storage into Hash-Distributable Tables:\n\nA dedicated SQL pool: This option allows you to leverage the power of the dedicated SQL pool (formerly SQL Data Warehouse) in Azure Synapse to perform high-performance ingest operations into hash-distributed tables. The dedicated SQL pool is optimized for large-scale data warehousing scenarios.\nImplement Query and Update Data in Delta Lake:\n\nA serverless Apache Spark pool: This option allows you to use Apache Spark as a serverless processing engine within Azure Synapse. Spark provides robust support for querying and updating data in Delta Lake, which is an open-source storage layer for reliable big data processing.","comment_id":"896083","upvote_count":"5","timestamp":"1683910740.0","comments":[{"upvote_count":"2","content":"It would be nice to also include a disclaimer saying that this is a response generated by ChatGPT or another similar tool.","comment_id":"1150227","poster":"xRiot007","timestamp":"1707915840.0"}],"poster":"Tr619899"},{"poster":"Bigbluee","timestamp":"1680785100.0","upvote_count":"2","content":"From the Delat Lake:\n\"Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.\"\n\nSo Delta Lake points to Apache Spark. In this case 2nd is Apache Spark Pool","comment_id":"862966"},{"timestamp":"1679505660.0","content":"1. Ingest data from Data Lake Storage into hash-distributed tables:\nA. A dedicated SQL pool\nA dedicated SQL pool in Azure Synapse provides the ability to create hash-distributed tables, which help distribute data evenly across multiple nodes and improve query performance. This option is well-suited for ingesting data from Data Lake Storage into hash-distributed tables.\n\n2. Implement query, and update data in Delta Lake:\nB. A serverless Apache Spark pool\nA serverless Apache Spark pool in Azure Synapse allows you to run Apache Spark jobs on-demand without having to manage the underlying infrastructure. This option is ideal for working with Delta Lake, as it provides native support for querying and updating data stored in Delta Lake format.","comment_id":"847331","poster":"NotMeAnyWay","upvote_count":"11"},{"timestamp":"1679243940.0","comment_id":"843973","upvote_count":"4","comments":[{"timestamp":"1683910860.0","poster":"betterthanlife","comment_id":"896084","upvote_count":"1","content":"Says it plain as do (what a shock!)\n\"Serverless SQL pools don't support updating Delta Lake files. You can use serverless SQL pool to query the latest version of Delta Lake. Use Apache Spark pools in Synapse Analytics to update Delta Lake.\""}],"content":"Second looks to be Apache spark pools as Serverless pool cannot update delta.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand?tabs=x80070002#delta-lake","poster":"Helice"},{"comment_id":"839590","timestamp":"1678861800.0","poster":"SD_Coordinator","upvote_count":"3","content":"For each requirement, I recommend the following Azure Synapse pools:\n\nIngest data from Data Lake Storage into hash-distributed tables: Use a \"Dedicated SQL Pool\" (formerly known as Azure Synapse Analytics SQL Data Warehouse). This pool provides the necessary performance and scaling capabilities to handle large-scale data ingestion and transformation. It also supports hash-distributed tables for better performance and query parallelism.\n\nImplement query and update data in Delta Lake: Use a \"Serverless Apache Spark Pool\". Apache Spark provides native support for Delta Lake, allowing you to query and update data efficiently. The serverless pool allows you to only pay for the resources you consume during job execution, offering cost efficiency for varying workloads."},{"timestamp":"1677489180.0","comment_id":"823459","content":"1- Dedicated SQL pools\n2- Serverless spark pools \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand?tabs=x80070002#delta-lake\n\"Serverless SQL pools don't support updating Delta Lake files. You can use serverless SQL pool to query the latest version of Delta Lake. Use Apache Spark pools in Synapse Analytics to update Delta Lake.\"\n\nSql serverless cannot update (using MERGE) delta tables","upvote_count":"2","poster":"latia6"},{"content":"1. Dedicated SQL pool\n2. Serverless SQL pool\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview\nEvery Azure Synapse Analytics workspace comes with serverless SQL pool endpoints that you can use to query data in the Azure Data Lake (Parquet, Delta Lake, delimited text formats), Azure Cosmos DB, or Dataverse.\n\nServerless SQL pool is a distributed data processing system, built for large-scale data and computational functions. Serverless SQL pool enables you to analyze your Big Data in seconds to minutes, depending on the workload. Thanks to built-in query execution fault-tolerance, the system provides high reliability and success rates even for long-running queries involving large data sets.\n\nServerless SQL pool is serverless, hence there's no infrastructure to setup or clusters to maintain. A default endpoint for this service is provided within every Azure Synapse workspace, so you can start querying data as soon as the workspace is created.","comments":[{"timestamp":"1685109240.0","content":"2. A Serverless Apache Spark pool\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand?tabs=x80070002#delta-lake\n\"Serverless SQL pools don't support updating Delta Lake files. You can use serverless SQL pool to query the latest version of Delta Lake. Use Apache Spark pools in Synapse Analytics to update Delta Lake.\"","comment_id":"907410","poster":"fahrulnizam","upvote_count":"1"}],"upvote_count":"3","comment_id":"822272","timestamp":"1677408720.0","poster":"zellck"},{"timestamp":"1676228040.0","content":"Answers are correct.\n- Dedicated SQL pool (To shard data into a hash-distributed table, dedicated SQL pool uses a hash function to deterministically assign each row to one distribution).\n- Serverless (Serverless SQL pool allows you to query your data lake files, while dedicated SQL pool allows you to query and ingest data from your data lake files).\nSee https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture","upvote_count":"1","poster":"4PHL","comment_id":"806692","comments":[{"content":"The difference is the \"update\" delta lake requirement - serverless wont do that. So your assertion doesnt seem right? See robmac17.","timestamp":"1676324340.0","poster":"np2021","upvote_count":"2","comment_id":"807825"}]},{"content":"why not apache spark?","poster":"Putra19","timestamp":"1676183220.0","comment_id":"806025","upvote_count":"1"},{"upvote_count":"4","content":"Second Question refers to Delta lake and Serverless can't update delta lake - https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand?tabs=x80070002#delta-lake","comment_id":"801342","timestamp":"1675798320.0","poster":"JBorne"},{"upvote_count":"3","comment_id":"796447","poster":"ed79","timestamp":"1675372200.0","comments":[{"timestamp":"1675559280.0","content":"Serverless SQL pools do not support updating delta lake files. Use Azure Databricks or Apache Spark pools in Azure Synapse Analytics to update Delta Lake.","comment_id":"798529","upvote_count":"3","poster":"robmac17"}],"content":"Box 2 says Delta Lake which is specific to Spark. This would be Spark"},{"comment_id":"789233","content":"Serverless SQL pool allows you to query your data lake files, while dedicated SQL pool allows you to query and ingest data from your data lake files.","poster":"KPVP","timestamp":"1674782340.0","upvote_count":"2"},{"upvote_count":"1","content":"Box1: Dedicated\nBox2: Serverless\n\nEvery Azure Synapse Analytics workspace comes with serverless SQL pool endpoints that you can use to query data in the Azure Data Lake (Parquet, Delta Lake, delimited text formats), Azure Cosmos DB, or Dataverse.\nServerless SQL pool is a query service over the data in your data lake. \nServerless SQL pool enables you to analyze your Big Data in seconds to minutes, depending on the workload. Thanks to built-in query execution fault-tolerance, the system provides high reliability and success rates even for long-running queries involving large data sets.\n\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is?context=%2Fazure%2Fsynapse-analytics%2Fcontext%2Fcontext","timestamp":"1674369060.0","poster":"yeanlingmedal71","comment_id":"783981"},{"poster":"Asten","content":"Serverless SQL pool can be used to query data in Azure data Lake.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview","upvote_count":"2","timestamp":"1674028020.0","comment_id":"779723","comments":[{"comment_id":"907417","content":"can be used to query but cannot update..\nso anwer for \n2. A Serverless Apache Spark pool\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand?tabs=x80070002#delta-lake\n\"Serverless SQL pools don't support updating Delta Lake files. You can use serverless SQL pool to query the latest version of Delta Lake. Use Apache Spark pools in Synapse Analytics to update Delta Lake.\"","upvote_count":"1","timestamp":"1685109600.0","poster":"fahrulnizam"}]},{"timestamp":"1674027900.0","upvote_count":"1","poster":"Asten","content":"Given answer ist correct.\nDedicated SQL pool refers to the data warehousing features that are available in Azure Synapse Analytics. ASA is the data analytics service for data warehaousing and big data.https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is","comment_id":"779717"}]},{"id":"yMeDg3HLUK3YHVSXUazP","unix_timestamp":1673884500,"exam_id":54,"question_text":"You have an on-premises storage solution.\n\nYou need to migrate the solution to Azure. The solution must support Hadoop Distributed File System (HDFS).\n\nWhat should you use?","answer_description":"","question_images":[],"answer_images":[],"answers_community":["A (100%)"],"answer":"A","topic":"2","choices":{"C":"Azure Data Share","B":"Azure NetApp Files","A":"Azure Data Lake Storage Gen2","D":"Azure Table storage"},"url":"https://www.examtopics.com/discussions/microsoft/view/95575-exam-az-305-topic-2-question-23-discussion/","question_id":93,"discussion":[{"upvote_count":"11","content":"Selected Answer: A\nAzure Data Lake Storage Gen2: This is a fully managed, cloud-native data lake that supports the HDFS protocol. It allows you to store and analyze large amounts of data in its native format, without the need to move or transform the data.","timestamp":"1673884500.0","poster":"[Removed]","comment_id":"777901"},{"upvote_count":"8","timestamp":"1679505720.0","comment_id":"847333","poster":"NotMeAnyWay","content":"Selected Answer: A\nA. Azure Data Lake Storage Gen2\n\nAzure Data Lake Storage Gen2 is the best choice for migrating your on-premises storage solution to Azure with support for Hadoop Distributed File System (HDFS). It is a highly scalable and cost-effective storage service designed for big data analytics, providing integration with Azure HDInsight, Azure Databricks, and other Azure services. It is built on Azure Blob Storage and combines the advantages of HDFS with Blob Storage, offering a hierarchical file system, fine-grained security, and high-performance analytics."},{"comment_id":"1310681","timestamp":"1731424620.0","upvote_count":"1","poster":"[Removed]","content":"Selected Answer: A\nA is correct"},{"comment_id":"1203942","timestamp":"1714384080.0","poster":"Lazylinux","upvote_count":"1","content":"Selected Answer: A\nGiven answer A is correct\nAzure Data Lake Storage Gen2: is designed for big data analytics, it combines the power of a high-performance file system with massive scale and economy to help you speed up your big data analytics. It allows the data to be organized in directories by date and time. It is the best choice for migrating your on-premises storage solution to Azure with support for Hadoop Distributed File System (HDFS). It is a highly scalable and cost-effective storage service designed for big data analytics, providing integration with Azure HDInsight, Azure Databricks, and other Azure services. It is built on Azure Blob Storage and combines the advantages of HDFS with Blob Storage, offering a hierarchical file system, fine-grained security, and high-performance analytics. Azure Data Lake Storage is optimized storage for big data analytics workloads."},{"timestamp":"1711215840.0","upvote_count":"1","comment_id":"1181066","content":"Selected Answer: A\n\"Azure Data Lake Storage Gen2 is primarily designed to work with Hadoop and all frameworks that use the Apache Hadoop Distributed File System (HDFS) as their data access layer.\"\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#hadoop-compatible-access","poster":"cris_exam"},{"poster":"arnaud_nauwynck","upvote_count":"1","comment_id":"992545","timestamp":"1693246620.0","content":"Azure DataLake Storage Gen2 has its own private API on top of https protocol, which is not compatible with HDFS internal protocol (used by NameNode and DataNode servers of the \"Distributed File System\" in hadoop) ...\nHowever, the java API class hadoop \"FileSystem\" has an implementation for abfs \nsee here : https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java#L133"},{"poster":"zellck","upvote_count":"4","comment_id":"812576","timestamp":"1676688060.0","content":"Selected Answer: A\nA is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#key-features-of-data-lake-storage-gen2\nHadoop compatible access: Data Lake Storage Gen2 allows you to manage and access data just as you would with a Hadoop Distributed File System (HDFS). The new ABFS driver (used to access data) is available within all Apache Hadoop environments. These environments include Azure HDInsight, Azure Databricks, and Azure Synapse Analytics."},{"poster":"RandomNickname","upvote_count":"4","timestamp":"1675185240.0","content":"Selected Answer: A\nCorrect\n\nhttps://learn.microsoft.com/en-us/azure/architecture/guide/hadoop/apache-hdfs-migration","comment_id":"794431"},{"poster":"OPT_001122","comment_id":"790564","content":"Selected Answer: A\nA. Azure Data Lake Storage Gen2","upvote_count":"1","timestamp":"1674906960.0","comments":[{"comment_id":"799894","content":"remember HDFS","timestamp":"1675697040.0","upvote_count":"2","poster":"OPT_001122"}]},{"poster":"armpro","content":"Selected Answer: A\nAzure Data Lake Gen 2","upvote_count":"2","timestamp":"1674399900.0","comment_id":"784417"},{"content":"Data Lake Storage Gen2 allows you to manage and access data just as you would with a Hadoop Distributed File System (HDFS)\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction","upvote_count":"2","poster":"francescoc","timestamp":"1673963160.0","comment_id":"778977"}],"timestamp":"2023-01-16 16:55:00","answer_ET":"A","isMC":true},{"id":"D6xgyiK2T90M0AAo3Vwc","answer_description":"","answers_community":[],"question_images":["https://img.examtopics.com/az-305/image176.png"],"answer_ET":"","question_text":"DRAG DROP\n-\n\nYou have an on-premises app named App1.\n\nCustomers use App1 to manage digital images.\n\nYou plan to migrate App1 to Azure.\n\nYou need to recommend a data storage solution for App1. The solution must meet the following image storage requirements:\n\n• Encrypt images at rest.\n• Allow files up to 50 MB.\n• Manage access to the images by using Azure Web Application Firewall (WAF) on Azure Front Door.\n\nThe solution must meet the following customer account requirements:\n\n• Support automatic scale out of the storage.\n• Maintain the availability of App1 if a datacenter fails.\n• Support reading and writing data from multiple Azure regions.\n\nWhich service should you include in the recommendation for each type of data? To answer, drag the appropriate services to the correct type of data. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct answer is worth one point.\n\n//IMG//","exam_id":54,"discussion":[{"comment_id":"862002","timestamp":"1680692040.0","upvote_count":"22","poster":"rex303","content":"Box 1 - Azure blob storage \nThe requirement to be accessible through a WAF limit the options to the Blob storage.\nBox 2 - Cosmos DB\nConcurrent writes from multiple regions make this the only option."},{"upvote_count":"17","content":"Box 1 - Image storage: A. Azure Blob Storage\n\nAzure Blob Storage is a suitable choice for storing digital images, as it supports encryption at rest, handles large file sizes (up to 50 MB or even larger), and can be used in conjunction with Azure Web Application Firewall (WAF) on Azure Front Door.\n\nBox 2 - Customer accounts: B. Azure Cosmos DB\n\nAzure Cosmos DB is a highly scalable, globally distributed, multi-model database service that supports automatic scale-out, ensures high availability even in the event of a datacenter failure, and allows for reading and writing data from multiple Azure regions. This makes it an ideal choice for storing customer account data in your scenario.","comment_id":"847337","poster":"NotMeAnyWay","timestamp":"1679505960.0"},{"timestamp":"1732365660.0","content":"1. Azure Blob storage\n2. Azure Cosmos DB","comment_id":"1316667","upvote_count":"1","poster":"Thanveer"},{"timestamp":"1731424680.0","poster":"[Removed]","upvote_count":"1","content":"CORRECT","comment_id":"1310682"},{"comment_id":"1203943","timestamp":"1714384200.0","content":"Given answer is correct","poster":"Lazylinux","upvote_count":"3"},{"poster":"JimmyYop","upvote_count":"10","timestamp":"1705854780.0","comment_id":"1127945","content":"appeared in Exam 01/2024"},{"content":"1. Azure Blob storage\n2. Azure Cosmos DB\n\nhttps://learn.microsoft.com/en-us/azure/frontdoor/scenario-storage-blobs\nAzure Front Door accelerates the delivery of static content from Azure Storage blobs, and enables a secure and scalable architecture. Static content delivery is useful for many different use cases, including website hosting and file delivery\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/introduction#guaranteed-speed-at-any-scale\n- Multi-region writes and data distribution to any Azure region with just a button","poster":"zellck","upvote_count":"8","comment_id":"818014","timestamp":"1677081840.0"},{"content":"correct","poster":"SH_22","upvote_count":"2","timestamp":"1677004440.0","comment_id":"816997"}],"question_id":94,"url":"https://www.examtopics.com/discussions/microsoft/view/100297-exam-az-305-topic-2-question-24-discussion/","answer_images":["https://img.examtopics.com/az-305/image177.png"],"answer":"","unix_timestamp":1677004440,"isMC":false,"timestamp":"2023-02-21 19:34:00","topic":"2"},{"id":"WNZwihlnTpw29g8foaDU","isMC":true,"answer":"A","discussion":[{"timestamp":"1676557620.0","comment_id":"810751","poster":"Coolfrenesie","comments":[{"content":"No SQL supports SQLquery ?","upvote_count":"5","timestamp":"1679822820.0","comment_id":"850858","comments":[{"comments":[{"poster":"xRiot007","content":"This is an unfortunate abbreviation, to say the least :) Should have called it SQL+ or something that shows SQL availability, plus other options.","upvote_count":"9","timestamp":"1707916260.0","comment_id":"1150231"}],"upvote_count":"39","poster":"Lonlystar","comment_id":"954849","timestamp":"1689649080.0","content":"It is not a \"No SQL\", it is a \"Not Only SQL\". \nHence, it does support SQL query. Hope it helps."},{"comment_id":"1225891","content":":))) it sounds funny.","timestamp":"1717730580.0","poster":"erikdran","upvote_count":"2"},{"poster":"jozir8","upvote_count":"9","content":"Yes, it does. \n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-dotnet-query-items\n- The Azure Cosmos DB for NoSQL supports the use of Structured Query Language (SQL) to perform queries on items in containers.","timestamp":"1681880520.0","comments":[{"content":"microsoft is weird","timestamp":"1682670360.0","poster":"GS300","upvote_count":"18","comments":[{"comment_id":"1334095","content":"NoSQL is not Microsoft proprietary","timestamp":"1735560660.0","poster":"Shri0024","upvote_count":"1"}],"comment_id":"883346"}],"comment_id":"874286"}],"poster":"Debosree"}],"content":"Selected Answer: A\ncosmos for the multi writer\npostgre is not good at reading","upvote_count":"11"},{"comment_id":"847339","poster":"NotMeAnyWay","content":"Selected Answer: A\n2 / 2\n\nA. Azure Cosmos DB for NoSQL\n\nAzure Cosmos DB is a globally distributed, multi-model database service that supports SQL commands, multi-master writes, and guarantees low latency read operations. It supports a variety of NoSQL data models including document, key-value, graph, and column-family. Azure Cosmos DB provides automatic and instant scalability, high availability, and low latency globally by replicating and synchronizing data across multiple Azure regions.\n\nOn the other hand, Azure SQL Database and Azure SQL Database Hyperscale are traditional relational database services that do not natively support multi-master writes.","timestamp":"1679506080.0","upvote_count":"8"},{"content":"Selected Answer: D\nConsidering the need for SQL support, multi-master writes, and low-latency reads, Azure Cosmos DB for PostgreSQL is the optimal choice, as it combines the familiarity of PostgreSQL with the distributed capabilities of Azure Cosmos DB.","comment_id":"1318625","upvote_count":"1","timestamp":"1732709460.0","poster":"robinhoHH"},{"comment_id":"1310683","timestamp":"1731424800.0","upvote_count":"1","poster":"[Removed]","content":"Selected Answer: A\nA is correct"},{"upvote_count":"3","comment_id":"1203945","timestamp":"1714384380.0","poster":"Lazylinux","content":"Selected Answer: A\nGiven answer A is correct\nCosmo DB for NOSQL or Cosmo DB SQL API provides the following features\n• Support SQL commands.\n• Support multi-master writes.\n• Guarantee low latency read operations."},{"comment_id":"1175006","content":"Why \"D - Azure Cosmos DB for PostgreSQL\" is incorrect ?\nAzure Cosmos DB for PostgreSQL fulfils all the requirements in the question","upvote_count":"1","poster":"Zein135","timestamp":"1710598620.0"},{"upvote_count":"2","timestamp":"1702078020.0","content":"Content aggregation is the main objective. Content can be structured or unstructured. NoSQL = \"Not Only SQL\" i.e. it can deal with any kind of content - structured or unstructured. Cosmos DB support multi master writes synchronously during replication and provide low latency for reads across regions as long as distributed DBs are nearer (local) to users who access it. So answer is Azure Cosmos DB for NoSQL.","comment_id":"1091370","poster":"BShelat"},{"comment_id":"965422","upvote_count":"3","content":"Although Azure Cosmos supports Multi Region Writes, Azure Cosmos for PostgreSQL does not.\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/introduction#fully-managed-resilient-database","timestamp":"1690534080.0","poster":"dave22339"},{"upvote_count":"2","poster":"jwu2023","timestamp":"1688168340.0","comment_id":"939458","content":"As for speed, NoSQL is generally faster than SQL, especially for key-value storage in our experiment"},{"comment_id":"813096","comments":[{"comment_id":"850860","timestamp":"1679823000.0","poster":"Debosree","upvote_count":"6","content":"nope . Here it is Cosmos DB for NoSQL earlier question had right optionAzure Cosmos DB for SQL API."}],"content":"Same as Question 10.\nhttps://www.examtopics.com/discussions/microsoft/view/67751-exam-az-305-topic-2-question-10-discussion","poster":"zellck","timestamp":"1676730600.0","upvote_count":"4"},{"upvote_count":"1","content":"Selected Answer: A\ncorrect answer","comment_id":"811149","timestamp":"1676585340.0","poster":"Alessandro365"}],"answer_images":[],"answer_ET":"A","answer_description":"","exam_id":54,"topic":"2","question_id":95,"choices":{"C":"Azure SQL Database Hyperscale","A":"Azure Cosmos DB for NoSQL","D":"Azure Cosmos DB for PostgreSQL","B":"Azure SQL Database that uses active geo-replication"},"question_text":"You are designing an application that will aggregate content for users.\n\nYou need to recommend a database solution for the application. The solution must meet the following requirements:\n\n• Support SQL commands.\n• Support multi-master writes.\n• Guarantee low latency read operations.\n\nWhat should you include in the recommendation?","unix_timestamp":1676557620,"url":"https://www.examtopics.com/discussions/microsoft/view/99419-exam-az-305-topic-2-question-25-discussion/","question_images":[],"timestamp":"2023-02-16 15:27:00","answers_community":["A (96%)","4%"]}],"exam":{"isImplemented":true,"isBeta":false,"id":54,"name":"AZ-305","isMCOnly":false,"numberOfQuestions":286,"provider":"Microsoft","lastUpdated":"12 Apr 2025"},"currentPage":19},"__N_SSP":true}