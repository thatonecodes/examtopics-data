{"pageProps":{"questions":[{"id":"CF6b7QDE876Qpim3uagc","answer_images":[],"choices":{"A":"\\DataSource\\SubjectArea\\YYYY\\WW\\FileData_YYYY_MM_DD.parquet","B":"\\DataSource\\SubjectArea\\YYYY-WW\\FileData_YYYY_MM_DD.parquet","C":"DataSource\\SubjectArea\\WW\\YYYY\\FileData_YYYY_MM_DD.parquet","D":"\\YYYY\\WW\\DataSource\\SubjectArea\\FileData_YYYY_MM_DD.parquet","E":"WW\\YYYY\\SubjectArea\\DataSource\\FileData_YYYY_MM_DD.parquet"},"answers_community":["A (90%)","10%"],"answer":"A","answer_description":"","timestamp":"2023-01-13 10:12:00","exam_id":67,"question_images":[],"isMC":true,"question_id":86,"question_text":"You are designing the folder structure for an Azure Data Lake Storage Gen2 account.\n\nYou identify the following usage patterns:\n\n• Users will query data by using Azure Synapse Analytics serverless SQL pools and Azure Synapse Analytics serverless Apache Spark pools.\n• Most queries will include a filter on the current year or week.\n• Data will be secured by data source.\n\nYou need to recommend a folder structure that meets the following requirements:\n\n• Supports the usage patterns\n• Simplifies folder security\n• Minimizes query times\n\nWhich folder structure should you recommend?","topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/94993-exam-dp-203-topic-1-question-70-discussion/","answer_ET":"A","unix_timestamp":1673601120,"discussion":[{"timestamp":"1673673720.0","comment_id":"775102","content":"Selected Answer: A\nA. \\DataSource\\SubjectArea\\YYYY\\WW\\FileData_YYYY_MM_DD.parquet\n\nThe recommended folder structure that best meets the requirements is option A. It separates data by data source, year and week. It allows for easy filtering of data by year or week, which aligns with the usage pattern where most queries include a filter on the current year or week.","poster":"nicky87654","upvote_count":"16"},{"comment_id":"1319544","upvote_count":"1","timestamp":"1732856400.0","content":"Selected Answer: A\nCorrect Answer: A","poster":"EmnCours"},{"timestamp":"1706422080.0","upvote_count":"2","comment_id":"1133869","content":"Selected Answer: A\nBased on the usage patterns and requirements, I recommend the following folder structure:\n\n\\DataSource\\SubjectArea\\YYYY\\WW\\FileData_YYYY_MM_DD.parquet\n\nThis structure is designed to support the usage patterns by organizing data by data source and subject area. The year and week folders will help optimize queries that filter by current year or week. This structure also simplifies folder security by grouping data by data source and subject area, making it easier to apply security policies. Finally, this structure minimizes query times by organizing data in a way that is optimized for the most common queries.","poster":"Charley92"},{"upvote_count":"1","comment_id":"1099791","content":"Selected Answer: B\nI believe it would be B. \n\n\"Minimises query time\" and \"Most queries will include a filter on the current year OR week.\"\n\nThe \"OR WEEK\" suggests that we may filter by only week and not the year.\n\nIn this event it would (to my knowledge) take longer to query through every single year to select the week you want as opposed to selecting all the folders containing the WW target value in their name.\n\nIf someone with query optimisation knowledge could confirm this below it would be appreciated.","poster":"ExamDestroyer69","timestamp":"1702910340.0"},{"comment_id":"998368","upvote_count":"1","content":"Selected Answer: B\nMinimizes query times && Most queries will include a filter on the current year OR week.\nIt is B.","comments":[{"content":"forhot it ,Option A can set permissions at the year level or at the week, should be A.","timestamp":"1693816140.0","poster":"kkk5566","comment_id":"998372","upvote_count":"2"}],"poster":"kkk5566","timestamp":"1693815780.0"},{"upvote_count":"2","poster":"Deeksha1234","content":"my opinion it should be A, as it can clearly filter on year or on week , both the options will be available.","timestamp":"1692059160.0","comment_id":"981184"},{"poster":"auwia","upvote_count":"1","content":"Selected Answer: B\nWe need to query by week too, so better YYYY-WWW.","timestamp":"1687932960.0","comment_id":"936224","comments":[{"timestamp":"1687980120.0","comments":[{"comment_id":"964685","content":"But why is it YYY-WW for you then instead of YYYY\\WW?","timestamp":"1690459620.0","upvote_count":"2","poster":"semauni"}],"comment_id":"937069","poster":"auwia","upvote_count":"1","content":"I got another good point in favour of B from the question, look the requirement: \n\"Most queries will include a filter on the current year OR week.\"\n(OR) let's suppose they ask you to give back the fourth week for example, you need to go year by year (folders) instead of have it in 1 page. Definitely for me it's option B."}]},{"timestamp":"1687235580.0","upvote_count":"2","content":"Option B seems right answer as we can directly access the given yyyy-ww .","poster":"VittalManikonda","comment_id":"928143"},{"comments":[{"comments":[{"content":"\"Most queries will include a filter on the current year OR week.\"","poster":"auwia","comment_id":"937071","upvote_count":"1","timestamp":"1687980240.0"}],"comment_id":"904951","content":"No, A it will give you additional separate column for week (WW).","poster":"Rob77","timestamp":"1684848480.0","upvote_count":"1"},{"content":"I would not trust using Chat GPT for studying a certification...","timestamp":"1686748920.0","poster":"DP203Cert2023","upvote_count":"5","comment_id":"923234"}],"content":"chat GPT: Based on the given usage patterns and requirements, the recommended folder structure would be option B: \n\n\\DataSource\\SubjectArea\\YYYY-WW\\FileData_YYYY_MM_DD.parquet\n\nThis structure allows for easy filtering of data by year and week, which aligns with the identified usage pattern of most queries filtering by the current year or week. It also organizes the data by data source and subject area, which simplifies folder security. By using a flat structure, with the data files directly under the year-week folder, query times can be minimized as the data is organized for efficient partition pruning.\n\nOption A is similar but includes an additional level of hierarchy for the year, which is unnecessary given the requirement to filter by year-week. Options C, D, and E do not follow a consistent hierarchy, making it difficult to navigate and locate specific data files.","timestamp":"1681989240.0","poster":"Rossana","upvote_count":"4","comment_id":"875510"},{"upvote_count":"1","comment_id":"797770","poster":"akshaynag95","timestamp":"1675502340.0","content":"Selected Answer: A\nA is correct answer"},{"poster":"vrodriguesp","timestamp":"1675353840.0","upvote_count":"1","content":"Selected Answer: A\nA is correct","comment_id":"796199"},{"poster":"DindaS","content":"The answer A is correct","upvote_count":"1","comment_id":"783813","timestamp":"1674343200.0"},{"upvote_count":"1","poster":"[Removed]","comment_id":"775428","timestamp":"1673701260.0","content":"Selected Answer: A\nGiven answer is correct"},{"content":"Selected Answer: A\nCorrect","upvote_count":"1","comment_id":"775249","timestamp":"1673692680.0","poster":"ZIMARAKI"},{"timestamp":"1673630040.0","content":"Selected Answer: A\nAnswer is correct.\nThe reason is that this folder structure allows for the data to be organized by data source and subject area, which can help with securing the data by data source. Additionally, it organizes the data by year and week, which can minimize query times for the queries that include a filter on the current year or week. And also the file name format is consistent with the folder structure, which makes it easy to understand where the data comes from.","upvote_count":"2","comment_id":"774706","poster":"MrWood47"},{"poster":"alexnicolita","content":"Selected Answer: A\nMy choice is A","comment_id":"774277","timestamp":"1673601120.0","upvote_count":"1"}]},{"id":"sbPSrfJXiWVNUdrejNZc","unix_timestamp":1673630580,"isMC":true,"answers_community":["D (84%)","C (16%)"],"exam_id":67,"answer_description":"","question_images":[],"answer_images":[],"answer":"D","question_text":"You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a table named table1.\n\nYou load 5 TB of data into table1.\n\nYou need to ensure that columnstore compression is maximized for table1.\n\nWhich statement should you execute?","timestamp":"2023-01-13 18:23:00","choices":{"A":"DBCC INDEXDEFRAG (pool1, table1)","B":"DBCC DBREINDEX (table1)","D":"ALTER INDEX ALL on table1 REBUILD","C":"ALTER INDEX ALL on table1 REORGANIZE"},"url":"https://www.examtopics.com/discussions/microsoft/view/95066-exam-dp-203-topic-1-question-71-discussion/","discussion":[{"comment_id":"774718","content":"Selected Answer: D\nD. ALTER INDEX ALL on table1 REBUILD\n\nThis statement will rebuild all indexes on table1, which can help to maximize columnstore compression. The other options are not appropriate for this task.\nDBCC INDEXDEFRAG (pool1, table1) is for defragmenting the indexes and DBCC DBREINDEX (table1) is for recreating the indexes. ALTER INDEX ALL on table1 REORGANIZE is for reorganizing the indexes.","poster":"MrWood47","timestamp":"1673630580.0","upvote_count":"30"},{"poster":"aemilka","timestamp":"1680033180.0","upvote_count":"7","comments":[{"upvote_count":"8","content":"As far as I can see, your quoted article does not refer to Azure Synapse Analytics dedicated SQL pool. I think rebuild is the only supported option for dedicated SQL as can be found here:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index","poster":"[Removed]","comment_id":"893873","comments":[{"poster":"aemilka","comment_id":"896621","upvote_count":"6","content":"Yes, I agree with you, I haven't noticed that the article does not apply to Synapse Analystics. \nD seems to be only possible answer.","timestamp":"1683976740.0"}],"timestamp":"1683719580.0"},{"comment_id":"1177393","upvote_count":"1","content":"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index\n\n// For a table with an ordered clustered columnstore index, ALTER INDEX REORGANIZE does not re-sort the data. To re-sort data, use ALTER INDEX REBUILD //\n\nThe question state \"columnstored\" table, so D","timestamp":"1710857220.0","poster":"Homer23"}],"comment_id":"853649","content":"Selected Answer: C\nReorganizing an index is less resource intensive than rebuilding an index. For that reason it should be your preferred index maintenance method, unless there is a specific reason to use index rebuild.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/indexes/reorganize-and-rebuild-indexes?view=sql-server-ver16"},{"timestamp":"1737398400.0","content":"Selected Answer: D\nDefinitely D. REORGANIZE is for light maintenance, to compact small rowgroups and clean up delta stores without a full rebuild. It's faster and less resource-intensive.\n\nREBUILD when the table has significant fragmentation or many delta stores. It recreates the entire index, providing MAXIMUM compression and performance improvement. Use for large-scale maintenance.","upvote_count":"1","comment_id":"1343799","poster":"JustImperius"},{"timestamp":"1732856700.0","upvote_count":"1","comment_id":"1319545","poster":"EmnCours","content":"Selected Answer: D\nCorrect Answer: D"},{"content":"Answer is C\nhttps://learn.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-query-performance?view=sql-server-ver16\nIf the number of deleted rows for several rowgroups is large enough to be merged into fewer rowgroups, reorganizing the columnstore increases the quality of the index and query performance improves. If your data deletion process usually empties entire rowgroups, consider using table partitioning","poster":"Okea","timestamp":"1730837280.0","comment_id":"1307545","upvote_count":"1"},{"poster":"renan_ineu","upvote_count":"2","content":"I saw all the documentations here and found another one that makes pretty clear the correct option is option D: \n\n\"By default, tables are defined as a clustered columnstore index. After a load completes, some of the data rows might not be compressed into the columnstore. There's a variety of reasons why this can happen. To learn more, see manage columnstore indexes.\nTo optimize query performance and columnstore compression after a load, rebuild the table to force the columnstore index to compress all the rows.\"\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store#optimize-columnstore-compression","timestamp":"1724588340.0","comment_id":"1272120"},{"poster":"Yoleri","timestamp":"1724313060.0","comment_id":"1270556","upvote_count":"1","content":"The answer is D. Why not C because ALTER INDEX ALL on table1 REORGANIZE is used for defragmenting the index, but it doesn’t achieve the same level of compression as a full rebuild."},{"timestamp":"1706422260.0","poster":"Charley92","upvote_count":"2","comment_id":"1133870","content":"Selected Answer: D\nTo maximize columnstore compression for table1 in Azure Synapse Analytics dedicated SQL pool, you should execute the following statement:\n\nALTER INDEX ALL on table1 REBUILD\n\nThe REBUILD option is used to rebuild all indexes on the table, which will maximize columnstore compression for table1. This option is recommended when a large amount of data has been added to the table, as in this case where 5 TB of data has been loaded into table1"},{"timestamp":"1698580380.0","comment_id":"1056786","content":"what is the different between \"Alter Index Rebuild\" or \"DBCC DBREINDEX\"","upvote_count":"2","poster":"ukivanlamlpi"},{"comment_id":"998375","poster":"kkk5566","timestamp":"1693816200.0","upvote_count":"1","content":"Selected Answer: D\nD. ALTER INDEX ALL on table1 REBUILD"},{"comment_id":"981865","upvote_count":"1","poster":"Deeksha1234","content":"Selected Answer: D\nD is correct","timestamp":"1692119700.0"},{"comment_id":"932938","comments":[{"upvote_count":"1","comment_id":"980110","content":"I agree. A rebuild can compress the data more efficiently within each combination of distribution and partition: It can open such existing columnstore segments and shuffle data within them (and the deltastore) to maximize compression for the resulting compressed columnstore segments. That is not possible when reorganizing. That process only changes compressed columnstore segments by physically deleting logically deleted rows and \ncombining small columnstore segments into larger ones.","timestamp":"1691939940.0","poster":"Matt2000"}],"poster":"vctrhugo","upvote_count":"1","content":"Selected Answer: D\nALTER INDEX REORGANIZE is used for rebuilding or reorganizing indexes, but it does not maximize columnstore compression.","timestamp":"1687641660.0"},{"upvote_count":"2","content":"Reorganize is for row store indexes. The question here clearly mentions column store indexes. Correct answer is D","poster":"Rajan191083","comment_id":"902152","timestamp":"1684518180.0"},{"poster":"MuhilMahil","content":"Selected Answer is C. \nreorganizing only help in optimizing compression and performance.","upvote_count":"1","timestamp":"1683560280.0","comment_id":"892300"},{"content":"Why not C?\nWhen reorganizing a columnstore index, the Database Engine compresses each closed row group in delta store into columnstore as a compressed row group. Starting with SQL Server 2016 (13.x) and in Azure SQL Database, the REORGANIZE command performs the following additional defragmentation optimizations online:\n\nPhysically removes rows from a row group when 10% or more of the rows have been logically deleted. For example, if a compressed row group of 1 million rows has 100,000 rows deleted, the Database Engine will remove the deleted rows and recompress the row group with 900,000 rows, reducing storage footprint.","timestamp":"1675173960.0","comment_id":"794254","upvote_count":"1","poster":"Vedjha"}],"question_id":87,"topic":"1","answer_ET":"D"},{"id":"mYAu22mqsWGZQSQDEWE6","answer_description":"","question_text":"You have an Azure Synapse Analytics dedicated SQL pool named pool1.\n\nYou plan to implement a star schema in pool and create a new table named DimCustomer by using the following code.\n\n//IMG//\n\n\nYou need to ensure that DimCustomer has the necessary columns to support a Type 2 slowly changing dimension (SCD).\n\nWhich two columns should you add? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.","answers_community":["BE (69%)","BD (31%)"],"question_id":88,"question_images":["https://img.examtopics.com/dp-203/image270.png"],"exam_id":67,"unix_timestamp":1680558840,"url":"https://www.examtopics.com/discussions/microsoft/view/105021-exam-dp-203-topic-1-question-72-discussion/","timestamp":"2023-04-03 23:54:00","answer_images":[],"isMC":true,"discussion":[{"comment_id":"863852","comments":[{"poster":"laurasscastro","comment_id":"896871","comments":[{"comment_id":"1057108","upvote_count":"3","content":"No, the 'CustomerKey' is the Surrogate Key. Moreover, a Business Key also already exists in DimCustomer table by the name 'CustomerSourceID'. So, B&E are the correct options.","poster":"phydev","timestamp":"1698608460.0"}],"content":"that's the business key, not the surrogate key. If a new record is generated, there would be a duplicate key. SK is necessary to identify the record","upvote_count":"8","timestamp":"1683993660.0"}],"poster":"aditya816","upvote_count":"18","timestamp":"1680870240.0","content":"Selected Answer: BE\nSurrogate is already there as customerkey column"},{"content":"Selected Answer: BD\nI think, there is already a column called InsertedDate, therefore E is not necessary. So we just need another column to track the end date, which is B. And RowID should be a surrogate key in this case.","upvote_count":"11","comments":[{"content":"The date of insertion and the expiration date from when to when is something else. You can insert data now, but either with future validity or with past validity (correcting errors, for example).\nSo options : BE","upvote_count":"7","timestamp":"1696771560.0","comment_id":"1028000","poster":"jiriz"}],"comment_id":"880008","poster":"[Removed]","timestamp":"1682402340.0"},{"upvote_count":"1","comments":[{"comment_id":"1354461","timestamp":"1739192580.0","content":"You're overcomplicating things. If it needs to be an identity, why is answer D just a BIGINT? Why is your surrogate key a BIGINT; nothing in the question mentions any volumes for you to be concerned of running out of values.\n\nSecondly, it is not good practice to \"kind-of\" implement a model because you have columns that seem similar. Inserted data and EffectiveStartDate serve different purposes. Are your EffectiveStartDate and InsertedDate the same for late arriving rows?","poster":"PreQL","upvote_count":"1"}],"timestamp":"1737399840.0","poster":"JustImperius","content":"Selected Answer: BD\nIts a bad question. Looking at the table creation query CustomerKey is not clearly a surrogate key due to the lack of IDENTITY or NEWID() etc. You cannot have a SCD 2 table without a surrogate key. \nSo assuming CustomerKey is not a surrogate key (due to the evidence provided in the query) you need that -> D \nWe only have one spot left. I would prefer to have both effective start and end date but if we had to choose the choice is effectiveEndDate because InsertedDate can act as the defacto effectiveStartDate -> B\n\nIf you can prove to me the CustomerKey is a surrogate key I will change my mind. The code says no. And before you tell me CustomerSourceId is the business key I would say the CustomerSourceId in the context of a DimCustomer table likely represents an identifier used to link the customer record to its original source system. But its really all conjecture...go with the code and the code says that there ain't no surrogate key present.","comment_id":"1343813"},{"poster":"monakl23","content":"Selected Answer: BE\nAnswer: BE.\nLink - https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types","timestamp":"1733150220.0","upvote_count":"1","comment_id":"1320983"},{"timestamp":"1732856820.0","content":"Selected Answer: BE\nCorrect Answer: BE","comment_id":"1319546","upvote_count":"1","poster":"EmnCours"},{"comment_id":"1278899","timestamp":"1725538260.0","content":"wrong answer by Xam topic correct answer with solution is-: [EffectiveStartDate] [datetime] NOT NULL: This column tracks when the current record started being valid, helping to define the validity period of the data.\n\n[EffectiveEndDate] [datetime] NOT NULL: This column marks the end of the record's validity period when a new version of the data is inserted.\n\nThese two columns will allow you to differentiate between active and historical records and handle Type 2 SCD effectively. BE","upvote_count":"1","poster":"ahana1074"},{"content":"We need a surrogate key.\n\nThe Inserteddate is NOT NULL, so will act as EffectiveStartDate. However, ModifiedDate is also NOT NULL, so it cannot work as EffectiveEndDate because the current record must have NULL for its EffectiveEndDate. So, we need EffectiveEndDate.\n\nAnswer: D,E.","poster":"f2a9aa5","timestamp":"1721933040.0","upvote_count":"1","comment_id":"1255184"},{"upvote_count":"1","timestamp":"1720861920.0","comment_id":"1247201","poster":"DanielCruz","content":"the right answer depends on a few topics not clear:\n- CustumerKey may be a surrogate key?\n- InsertedDate may be a the \"EffectiveStartDate\"\n- modifiedDate may be the \"EffectiveEndDate\""},{"upvote_count":"1","timestamp":"1716899100.0","content":"Selected Answer: BE\nThe subrogatekey is already","poster":"sergio_eduardo","comment_id":"1220223"},{"content":"Selected Answer: BD\nB --> end date is missing, and required by SCD2\nD --> (not E), RowID is required since in SCD2 you're adding the same CustomerID twice, even with a different end date. So, you need a way to uniquely identify a row in the table, that's going no longer to be the customer identifier in general.","comment_id":"1189156","poster":"MBRSDG","timestamp":"1712216220.0","upvote_count":"5"},{"poster":"AbhiJain1993","timestamp":"1711779720.0","content":"It should be BD. If CustomerKey was surrogate key then IDENTITY should have been mentioned in Column definition.","upvote_count":"2","comment_id":"1185906"},{"poster":"sdg2844","upvote_count":"2","comment_id":"1114158","timestamp":"1704416400.0","content":"Selected Answer: BE\nThere is already a hash key that serves as the surrogate, if I'm not mistaken. Inserted and modified are probably dates from the source data, not from the work being done here, so you need to add the start/end dates."},{"timestamp":"1696771740.0","content":"Selected Answer: BE\nThe date of insertion and the expiration date from when to when is something else. You can insert data now, but either with future validity or with past validity (correcting errors, for example).\nSo options : BE","poster":"jiriz","upvote_count":"4","comment_id":"1028005"},{"comment_id":"1001279","poster":"hassexat","timestamp":"1694071320.0","content":"Selected Answer: BE\nB and E","upvote_count":"1"},{"comment_id":"1000217","upvote_count":"2","poster":"AvSUN","content":"B and D we need a unique row identifier","timestamp":"1693980240.0"},{"comments":[{"upvote_count":"1","content":"after think twice ,B&E","poster":"kkk5566","timestamp":"1693816440.0","comment_id":"998378"}],"comment_id":"989758","poster":"kkk5566","upvote_count":"1","timestamp":"1692947580.0","content":"B and D ,its a star schema on which has a fact table include a customerID property."},{"poster":"Deeksha1234","timestamp":"1692120240.0","comment_id":"981874","content":"B and D makes more sense, since inserted date is there already","upvote_count":"1"},{"poster":"YikesYikes2023","comment_id":"941283","content":"Selected Answer: BE\nIf RowID was the surrogate, wouldn't it be an IDENTITY column? Therefore, it has to be B and E. Right? Please explain if this doesn't make sense make sense","timestamp":"1688337540.0","upvote_count":"2"},{"comment_id":"936255","timestamp":"1687935420.0","upvote_count":"3","poster":"auwia","content":"Selected Answer: BE\nhttps://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/\n\n\"For the SCD Type 2, we need to include three more attributes such as StartDate, EndDate and IsCurrent\"\nIsCurrentRow is already present! ... ;-)\nCustomerKey (in reality is the RowID that many guys wants to add here), \neffectiveEndDate will probably set to: 31.12.9999, (to justify the not null).\n\nMy final answer wil lbe : B and E."},{"poster":"_ry__","content":"what is the answer ?","comment_id":"936147","timestamp":"1687929480.0","upvote_count":"2"},{"comment_id":"909447","timestamp":"1685367360.0","upvote_count":"2","poster":"ArunMat","content":"Selected Answer: BE\nFor SCD Type 2 we need record valid from and to date i.e effective date to identify latest row for that id."},{"upvote_count":"3","comment_id":"888526","comments":[{"content":"OK seen elsewhere that typically would be e.g. Dec-31-9999","comment_id":"889871","poster":"jlad26","timestamp":"1683272400.0","upvote_count":"1"}],"poster":"jlad26","timestamp":"1683118200.0","content":"I'm confused by the NOT NULL for the EffectiveEndDate. What value is this column going to hold for the row that holds the current information ?"},{"content":"B and E","upvote_count":"2","comment_id":"861689","poster":"AmrNegm","timestamp":"1680660840.0"},{"content":"Should be BE","timestamp":"1680580740.0","poster":"wendyy","upvote_count":"2","comment_id":"860599"},{"comment_id":"860376","content":"Selected Answer: BE\nB and E. I don't think RowID is not needed, as there is already a surrogate key that exists with the CustomerKey column.","timestamp":"1680558840.0","upvote_count":"2","poster":"SteveMcD"}],"choices":{"C":"[PreviousModifiedDate] [datetime] NOT NULL","B":"[EffectiveEndDate] [datetime] NOT NULL","E":"[EffectiveStartDate] [datetime] NOT NULL","D":"[RowID] [bigint] NOT NULL","A":"[HistoricalSalesPerson] [nvarchar] (256) NOT NULL"},"answer":"BE","answer_ET":"BE","topic":"1"},{"id":"ZBGmq4jy9WtOiTxlTQnk","url":"https://www.examtopics.com/discussions/microsoft/view/104994-exam-dp-203-topic-1-question-73-discussion/","question_images":["https://img.examtopics.com/dp-203/image271.png"],"unix_timestamp":1680538320,"timestamp":"2023-04-03 18:12:00","answers_community":[],"answer_images":["https://img.examtopics.com/dp-203/image272.png"],"answer_description":"","question_text":"HOTSPOT\n-\n\nYou have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool.\nYou plan to deploy a solution that will analyze sales data and include the following:\n\n• A table named Country that will contain 195 rows\n• A table named Sales that will contain 100 million rows\n• A query to identify total sales by country and customer from the past 30 days\n\nYou need to create the tables. The solution must maximize query performance.\n\nHow should you complete the script? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","topic":"1","exam_id":67,"answer":"","answer_ET":"","discussion":[{"timestamp":"1680538320.0","comment_id":"860118","content":"Correct! 1. Hash(CustomerID) 2. Replicate","upvote_count":"21","poster":"FRANCIS_A_M","comments":[{"poster":"zafnad","comment_id":"894886","upvote_count":"2","timestamp":"1683796200.0","content":"Could you please explain why 1. Hash([CustomerID]) is correct, and 2. Hash([OrderDate]) is incorrect.","comments":[{"upvote_count":"7","comment_id":"901992","poster":"ajhak","timestamp":"1684504920.0","content":"It is hash because it is a fact table (you can tell because there is the \"total\" column being created which is numerical). Rule of thumb, never hash on a date field, so in this case you would hash on 'CustomerID'. You want the hash to have as many unique values as possible."},{"poster":"Spam_Account","upvote_count":"16","comment_id":"940989","content":"Don't hash on date, only partition on date","timestamp":"1688309640.0"},{"comment_id":"932945","poster":"vctrhugo","timestamp":"1687641960.0","content":"Never distribute on Date.","upvote_count":"5"}]}]},{"poster":"AnhHoang","comment_id":"1249734","upvote_count":"2","timestamp":"1721230320.0","content":"1. You would want hash distribution to improve query performance. You don't want to hash on Date column since it can cause bottlenecks if many people query on a same date (eg. getdate()), so hash on customerid is the way.\n2. You would want to replicate small tables across all distributions, so it can pick up any distribution and still have full data."},{"upvote_count":"2","poster":"kkk5566","timestamp":"1693816740.0","content":"1. Hash(CustomerID) \n2. Replicate","comment_id":"998382"},{"timestamp":"1692120600.0","upvote_count":"1","comment_id":"981875","poster":"Deeksha1234","content":"given answer is correct"},{"poster":"examtopicsofyannick","timestamp":"1690993380.0","content":"Correct. Hash on Sales Table(Fact) and Replicate on Country table(Dimension)","upvote_count":"2","comment_id":"970368"},{"content":"correct","timestamp":"1681067820.0","comment_id":"865758","poster":"nmnm22","upvote_count":"4"}],"isMC":false,"question_id":89},{"id":"ej032U1vyv6UJBEqSB4K","isMC":true,"answer":"A","timestamp":"2023-04-03 18:20:00","answer_ET":"A","answers_community":["A (96%)","4%"],"url":"https://www.examtopics.com/discussions/microsoft/view/104995-exam-dp-203-topic-1-question-74-discussion/","unix_timestamp":1680538800,"choices":{"D":"Use an Apache Hadoop external table and authenticate by using a service principal in Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra.","B":"Use a native external table and authenticate by using a storage account key.","A":"Use a native external table and authenticate by using a shared access signature (SAS).","C":"Use an Apache Hadoop external table and authenticate by using a shared access signature (SAS)."},"discussion":[{"poster":"FRANCIS_A_M","timestamp":"1680538800.0","comment_id":"860126","content":"Selected Answer: A\nCorrect! Serverless SQL Pools cannot use Hadoop, Only Native. Access Key Auth is never best practice therefore leaving only A as a viable answer.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","upvote_count":"34","comments":[{"upvote_count":"5","content":"thanks a lot for the good explanation","poster":"nmnm22","comment_id":"865760","timestamp":"1681067880.0"},{"comments":[{"upvote_count":"1","timestamp":"1737979080.0","poster":"hypersam","content":"Indeed, to narrow it down, it's shown in the table from this link, from the row \"Storage authentication\", the Native column only shows Shared Access Signature(SAS), Microsoft Entra passthrough, Managed identity, Custom application Microsoft Entra identity.","comment_id":"1347379"}],"upvote_count":"7","poster":"Rob77","content":"It's not about the best practice - there is no option to use storage keys... \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#security","comment_id":"904956","timestamp":"1684849320.0"}]},{"upvote_count":"5","timestamp":"1687642080.0","poster":"vctrhugo","comment_id":"932946","content":"Selected Answer: A\nThe other options provided (B, C, and D) are not the recommended configurations for maximizing performance in this scenario. Using a storage account key for authentication (option B) poses a security risk and should be avoided. Apache Hadoop external tables (options C and D) do not provide the same level of performance optimization as native external tables in Azure Synapse Analytics."},{"timestamp":"1739193300.0","comment_id":"1354467","poster":"samirarian","content":"Selected Answer: B\nWhy Native External Table?\n\nNative external tables (CREATE EXTERNAL TABLE) in Synapse serverless SQL pools are optimized for querying external data sources such as Azure Data Lake Storage Gen2.\nThey provide better query optimization than Apache Hadoop external tables.\nWhy Storage Account Key for Authentication?\n\nUsing the storage account key ensures fast and direct access to the storage account without requiring token exchanges or additional configurations.\nWhile a shared access signature (SAS) works for temporary scenarios, it isn't as straightforward for long-term performance and scalability.","upvote_count":"1"},{"comment_id":"1340414","upvote_count":"1","timestamp":"1736869620.0","poster":"moize","content":"Selected Answer: B\nPour optimiser les performances lors de la création d'une table externe dans un pool SQL sans serveur dans Azure Synapse Analytics, il est recommandé d'utiliser une table externe native et de s'authentifier à l'aide d'une clé de compte de stockage (option B)\nL'utilisation d'une table externe native permet de tirer parti des optimisations spécifiques à Azure Synapse pour les fichiers CSV. L'authentification avec une clé de compte de stockage est simple à configurer et offre de bonnes performances pour accéder aux données stockées dans Azure Data Lake Storage Gen2."},{"poster":"moize","comment_id":"1321951","content":"Selected Answer: A\nPour créer une table externe dans un pool SQL sans serveur dans Azure Synapse Analytics et optimiser les performances, vous devez utiliser une table externe native. Voici la configuration recommandée :\n\nA. Utilisez une table externe native et authentifiez-vous à l’aide d’une signature d’accès partagé (SAS).\n\nRaisons :\nTables Externes Natives : Les tables externes natives sont optimisées pour les pools SQL sans serveur dans Azure Synapse Analytics et offrent de meilleures performances par rapport aux tables externes Hadoop1.\nAuthentification avec SAS : Utiliser une signature d'accès partagé (SAS) est une méthode sécurisée et flexible pour accéder aux données stockées dans Azure Data Lake Storage Gen2.","upvote_count":"1","timestamp":"1733322840.0"},{"upvote_count":"1","comment_id":"1319547","content":"Selected Answer: A\nCorrect Answer: A","timestamp":"1732857120.0","poster":"EmnCours"},{"content":"A for sure","upvote_count":"1","comment_id":"1204920","poster":"Dusica","timestamp":"1714548180.0"},{"upvote_count":"1","content":"A is correct.\nNative external tables perform better than hadoop external table.\nAlso ,storage account is more secure with the scheme of SAS \n\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables","comment_id":"1137104","timestamp":"1706735580.0","poster":"moneytime"},{"timestamp":"1706423760.0","poster":"Charley92","upvote_count":"1","content":"Selected Answer: A\nTo maximize performance when creating an external table in a serverless SQL pool in Azure Synapse Analytics workspace1 that references CSV files stored in account1, you should use a native external table and authenticate by using a shared access signature (SAS).\n\nNative external tables are designed to read and export data in various data formats such as CSV and Parquet. They are available in serverless SQL pools and are in public preview in dedicated SQL pools. Using an SAS to authenticate provides a secure way to access the data in account1 without exposing the storage account key.","comment_id":"1133881"},{"upvote_count":"1","poster":"[Removed]","comment_id":"1002318","timestamp":"1694165640.0","content":"No support for storage acc key only ui, sas, sp, mi, apa"},{"poster":"kkk5566","upvote_count":"1","timestamp":"1693816860.0","content":"Selected Answer: A\nis correct","comment_id":"998384"},{"timestamp":"1692120900.0","upvote_count":"1","content":"Selected Answer: A\nA is correct","comment_id":"981884","poster":"Deeksha1234"}],"question_text":"You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account1 and an Azure Synapse Analytics workspace named workspace1.\n\nYou need to create an external table in a serverless SQL pool in workspace1. The external table will reference CSV files stored in account1. The solution must maximize performance.\n\nHow should you configure the external table?","question_images":[],"answer_images":[],"topic":"1","exam_id":67,"question_id":90,"answer_description":""}],"exam":{"id":67,"isImplemented":true,"name":"DP-203","numberOfQuestions":384,"isMCOnly":false,"lastUpdated":"12 Apr 2025","provider":"Microsoft","isBeta":false},"currentPage":18},"__N_SSP":true}