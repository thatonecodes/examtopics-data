{"pageProps":{"questions":[{"id":"MMqhLdK8rdrfD20RAbug","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/89228-exam-dp-203-topic-4-question-27-discussion/","topic":"4","unix_timestamp":1669715880,"question_images":[],"timestamp":"2022-11-29 10:58:00","answer_ET":"B","exam_id":67,"isMC":true,"answer":"B","answers_community":["B (100%)"],"answer_description":"","question_id":321,"choices":{"B":"Azure Stream Analytics Edge application using Microsoft Visual Studio","C":"Azure Analysis Services using Microsoft Visual Studio","A":"Azure Analysis Services using Azure PowerShell","D":"Azure Data Factory instance using Azure Portal"},"question_text":"A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\nThe company must be able to monitor the devices in real-time.\nYou need to design the solution.\nWhat should you recommend?","discussion":[{"timestamp":"1728375360.0","upvote_count":"1","poster":"Alongi","content":"Correct\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/quick-create-visual-studio-code","comment_id":"1191458"},{"comment_id":"1025786","content":"Q14 Topic4\nSame question but different options.","upvote_count":"2","timestamp":"1712329200.0","poster":"pperf"},{"timestamp":"1709204640.0","upvote_count":"2","poster":"kkk5566","comment_id":"994956","content":"Selected Answer: B\nrepeted"},{"timestamp":"1703200140.0","content":"Selected Answer: B\nAzure IoT Hub = Stream","poster":"vctrhugo","upvote_count":"1","comment_id":"929937"},{"content":"Selected Answer: B\nAzure Stream Analytics Edge application using Microsoft Visual Studio\n\nAzure Stream Analytics is a real-time data streaming service that allows you to analyze and process data streams in near real-time. The Stream Analytics Edge application can be deployed on IoT devices, such as those used to monitor manufacturing machinery, to enable real-time monitoring and analysis of the data generated by the devices. Stream Analytics Edge allows you to run Stream Analytics jobs on IoT","comment_id":"776621","poster":"nicky87654","timestamp":"1689421500.0","upvote_count":"4"},{"timestamp":"1686096240.0","upvote_count":"2","comment_id":"737367","poster":"Shanmahi","content":"Selected Answer: B\nReasons for choosing option B --> IoT devices, streaming data, real-time data requirement"},{"content":"Ans is D","upvote_count":"1","comment_id":"730198","comments":[{"upvote_count":"1","content":"Azure Data Factory is primarily a data integration service designed for orchestrating and managing data workflows, such as data movement and transformation.\n\nWhile ADF can be used for data ingestion and processing, it is not optimized for real-time scenarios. Azure Data Factory works based on scheduled or triggered data pipelines rather than real-time streaming data processing.","comment_id":"929938","timestamp":"1703200260.0","poster":"vctrhugo"}],"timestamp":"1685347080.0","poster":"MadhuMDLK1055"}]},{"id":"G9b9JG0Te13Gj6CWKnY7","choices":{"D":"sys.dm_db_column_store_row_group_physical_stats","B":"sys.dm_db_column_store_row_group_operational_stats","C":"sys.pdw_nodes_column_store_row_groups","A":"sys.pdw_nodes_column_store_segments"},"question_images":[],"answer_images":[],"exam_id":67,"unix_timestamp":1662563100,"isMC":true,"discussion":[{"comment_id":"695413","upvote_count":"8","poster":"greenlever","content":"Selected Answer: C\nhas a column for the total number of rows physically stored (including those marked as deleted) and a column for the number of rows marked as deleted. Use sys.pdw_nodes_column_store_row_groups to determine which row groups have a high percentage of deleted rows and should be rebuilt","timestamp":"1665840000.0"},{"comment_id":"1348736","content":"Selected Answer: C\n\"Use sys.pdw_nodes_column_store_row_groups to determine which row groups have a high percentage of deleted rows and should be rebuilt.\"\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-pdw-nodes-column-store-row-groups-transact-sql?view=aps-pdw-2016-au7","upvote_count":"1","poster":"hypersam","timestamp":"1738178940.0"},{"content":"Selected Answer: D\nD. sys.dm_db_column_store_row_group_physical_stats is most appropriate. This view provides information about the physical state of columnstore row groups, including the percentage of deleted rows.","comment_id":"1320993","upvote_count":"1","timestamp":"1733152020.0","poster":"de_examtopics"},{"poster":"e56bb91","upvote_count":"1","content":"Selected Answer: D\nChatGPT 4o code:\nWITH RowGroupStats AS (\n SELECT\n OBJECT_NAME(t.object_id) AS TableName,\n t.name AS IndexName,\n p.partition_number AS PartitionNumber,\n rg.row_group_id AS RowGroupID,\n rg.total_rows,\n rg.deleted_rows,\n rg.deleted_rows * 1.0 / NULLIF(rg.total_rows, 0) AS DeletedRowPercentage\n FROM\n sys.dm_db_column_store_row_group_physical_stats AS rg\n JOIN sys.partitions AS p ON rg.partition_id = p.partition_id\n JOIN sys.indexes AS t ON p.object_id = t.object_id AND p.index_id = t.index_id\n)\nSELECT\n TableName,\n IndexName,\n PartitionNumber,\n AVG(DeletedRowPercentage) AS AvgDeletedRowPercentage\nFROM\n RowGroupStats\nGROUP BY\n TableName,\n IndexName,\n PartitionNumber\nHAVING\n AVG(DeletedRowPercentage) > 0.1 -- Adjust this threshold as needed\nORDER BY\n AvgDeletedRowPercentage DESC;","comment_id":"1245028","timestamp":"1720547940.0"},{"comments":[{"comment_id":"1191461","timestamp":"1712564700.0","upvote_count":"2","poster":"Alongi","content":"Sorry, D is Correct !"}],"poster":"Alongi","timestamp":"1712564580.0","content":"Selected Answer: C\nThe system views starting with \"sys.dm_db_\" are specific to SQL Server and provide information about the database and server activities, while the system views starting with \"sys.pdw_nodes_\" are specific to Azure Synapse and provide information about the distribution and performance in the Parallel Data Warehouse distributed storage environment.\n\nAccording to that, C is correct.","upvote_count":"1","comment_id":"1191460"},{"timestamp":"1702805700.0","content":"Selected Answer: D\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-pdw-nodes-column-store-row-groups-transact-sql?view=aps-pdw-2016-au7","comment_id":"1098773","poster":"d046bc0","upvote_count":"2"},{"comments":[{"poster":"kkk5566","upvote_count":"4","timestamp":"1693473000.0","comment_id":"994961","content":"change to C\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-pdw-nodes-column-store-row-groups-transact-sql?view=aps-pdw-2016-au7"}],"poster":"kkk5566","content":"Selected Answer: D\nD is corrct","timestamp":"1693472700.0","comment_id":"994958","upvote_count":"2"},{"upvote_count":"1","comment_id":"965767","timestamp":"1690569780.0","poster":"andie123","content":"Selected Answer: D\nD is correct answer"},{"timestamp":"1687382040.0","poster":"vctrhugo","upvote_count":"2","content":"Selected Answer: C\nUse sys.pdw_nodes_column_store_row_groups to determine which row groups have a high percentage of deleted rows and should be rebuilt.\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-pdw-nodes-column-store-row-groups-transact-sql?view=aps-pdw-2016-au7","comment_id":"929943"},{"timestamp":"1668436020.0","content":"Selected Answer: C\nC is the correct Answer !","poster":"dimbrici","comment_id":"718016","upvote_count":"3"},{"comment_id":"662627","poster":"anks84","upvote_count":"3","timestamp":"1662563100.0","content":"Selected Answer: C\nC is the correct Answer !"}],"url":"https://www.examtopics.com/discussions/microsoft/view/80943-exam-dp-203-topic-4-question-28-discussion/","topic":"4","answers_community":["C (72%)","D (28%)"],"answer":"C","answer_ET":"C","question_id":322,"answer_description":"","question_text":"You have an Azure Synapse Analytics dedicated SQL pool named SA1 that contains a table named Table1.\nYou need to identify tables that have a high percentage of deleted rows.\nWhat should you run?","timestamp":"2022-09-07 17:05:00"},{"id":"LzqTi6Nx2pqiWYmrF3fX","isMC":true,"exam_id":67,"discussion":[{"comment_id":"882174","timestamp":"1682563200.0","content":"Selected Answer: C\nIt must be DWU percentage. e.g. 95% is bad and 99% is very bad, and you don't need to look at anything else.\n\nIf you looked at DWU used, what can you infer without also knowing the DWU limit (or DWU percentage)?","poster":"chryckie","upvote_count":"14"},{"content":"C. DWU percentage is the best metric to monitor to identify whether you must scale up to a higher service level to accommodate the current workloads in Azure Synapse Analytics. DWU percentage measures the percentage of Data Warehouse Units (DWUs) in use, which indicates how much processing power is being used. If the DWU percentage consistently exceeds a certain threshold, it may be necessary to scale up to a higher service level to accommodate the workload. DWU used, CPU percentage, and Data IO percentage are also important metrics to monitor, but they do not directly reflect the overall processing power available in the data warehouse.","comment_id":"842095","timestamp":"1679066580.0","upvote_count":"8","poster":"markpumc"},{"poster":"renan_ineu","content":"Selected Answer: C\n\"DWU used represents only a high-level representation of usage across the SQL pool and isn't meant to be a comprehensive indicator of utilization. To determine whether to scale up or down, consider all factors which can be impacted by DWU such as concurrency, memory, tempdb, and adaptive cache capacity.\"\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity\n\n\"DWU used percentage\nRepresents a high-level representation of usage across the SQL pool. Measured by taking the maximum between CPU percentage and Data IO percentage.\"\nSource: https://learn.microsoft.com/en-us/azure/azure-monitor/reference/supported-metrics/microsoft-synapse-workspaces-sqlpools-metrics\n\nAlthough a little bit confusing at first read, due to the answer misleading options, the correct answer is clearly C.","comment_id":"1276580","timestamp":"1725271440.0","upvote_count":"1"},{"poster":"evangelist","content":"Selected Answer: C\nDWU (Data Warehouse Unit) percentage is the best metric to monitor when determining if you need to scale up to a higher service level. It shows how much of the currently allocated compute resources are being used, indicating whether the current level is sufficient or if scaling up is necessary.","comment_id":"1248852","timestamp":"1721129460.0","upvote_count":"1"},{"content":"Selected Answer: A\nIf you are concerned about whether you need to scale up to a higher service level, then DWU used is the most important metric to monitor. If you are concerned about how your workload is using your resources, then DWU percentage can be a helpful secondary metric.","timestamp":"1705311360.0","upvote_count":"1","comment_id":"1123226","poster":"Azure_2023"},{"upvote_count":"1","poster":"Momoanwar","content":"Selected Answer: C\nChatgpt :\nKnowing the absolute number of DWUs used (option A) doesn't provide complete information unless you also know the total DWUs available. On the other hand, the DWU percentage directly indicates how much of the available compute capacity is being used, which is a more informative metric for deciding whether scaling is needed.\n\nTherefore, the best metric to monitor to identify whether you need to scale up to a higher service level to accommodate the current workloads would indeed be:\n\nC. DWU percentage","timestamp":"1703523240.0","comment_id":"1105411"},{"timestamp":"1697196180.0","poster":"jiriz","upvote_count":"1","content":"Selected Answer: C\nC - DWU Percentage","comment_id":"1042605"},{"comments":[{"comment_id":"994967","timestamp":"1693473420.0","poster":"kkk5566","upvote_count":"1","content":"DWU percentage is the percentage of Data Warehouse Units (DWUs) used by the data warehouse. It is calculated as the maximum of CPU percentage and Data IO percentage. If the DWU percentage is consistently high, it may indicate that you need to scale up to a higher service level to accommodate the current workloads"}],"poster":"kkk5566","timestamp":"1693473120.0","upvote_count":"1","comment_id":"994964","content":"Selected Answer: C\nC is the best."},{"upvote_count":"2","timestamp":"1687382160.0","comment_id":"929946","poster":"vctrhugo","content":"Selected Answer: C\nPercentage should be way more useful than units itself."},{"comment_id":"900831","poster":"henryphchan","timestamp":"1684389960.0","upvote_count":"4","content":"Selected Answer: C\ni vote for C because only the % used is meaningful"},{"poster":"rohitbinnani","timestamp":"1683344760.0","upvote_count":"4","comment_id":"890450","content":"Selected Answer: C\nI 100% agree with C. How will you know by a UNIT value if it's sufficient or not? You would need to check the percentage consumed out of total capacity, right? Hence, in my logical and design views it must be C --> DWU Percentage."},{"content":"Selected Answer: A\nDWU used is the metric to use, if only one best answer is expected. option A.","poster":"Shanmahi","comment_id":"737365","timestamp":"1670378580.0","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: A\nAs given in the document and explanation, DWU used = DWU limit * DWU percentage, it comprises limit and percentage.\nThe question also states that more than one answer may achieve the goal and we are supposed to select the best answer, I think DWU used gives the best metric.","comment_id":"731308","timestamp":"1669801560.0","poster":"shaileshutd"},{"content":"Which is the best one, DWU used or DWU percentage? We need to select one.","poster":"Tickxit","upvote_count":"1","timestamp":"1669021740.0","comment_id":"723327"},{"comment_id":"712589","poster":"CodingOwl","timestamp":"1667761620.0","content":"AC Both are answers","upvote_count":"1"},{"timestamp":"1663036560.0","poster":"Phund","upvote_count":"2","content":"should be both DWU metrics","comment_id":"667595"}],"topic":"4","answer_ET":"C","answers_community":["C (81%)","A (19%)"],"question_text":"You have an enterprise data warehouse in Azure Synapse Analytics.\nYou need to monitor the data warehouse to identify whether you must scale up to a higher service level to accommodate the current workloads.\nWhich is the best metric to monitor?\nMore than one answer choice may achieve the goal. Select the BEST answer.","answer_description":"","question_images":[],"choices":{"D":"Data IO percentage","A":"DWU used","C":"DWU percentage","B":"CPU percentage"},"url":"https://www.examtopics.com/discussions/microsoft/view/81913-exam-dp-203-topic-4-question-29-discussion/","timestamp":"2022-09-13 04:36:00","answer_images":[],"question_id":323,"unix_timestamp":1663036560,"answer":"C"},{"id":"u5dUmyIYYzHyNZS5jHzi","answer_ET":"B","isMC":true,"choices":{"D":"workspace logs","A":"notebook logs","B":"cluster event logs","C":"global init scripts logs"},"answer_description":"","question_id":324,"unix_timestamp":1625034120,"answers_community":["B (68%)","C (32%)"],"answer":"B","topic":"4","url":"https://www.examtopics.com/discussions/microsoft/view/56279-exam-dp-203-topic-4-question-3-discussion/","exam_id":67,"answer_images":[],"discussion":[{"poster":"Dizzystar","timestamp":"1651410480.0","upvote_count":"34","content":"I should say Cluster Event logs:\nAzure Databricks provides three kinds of logging of cluster-related activity:\n\nCluster event logs, which capture cluster lifecycle events, like creation, termination, configuration edits, and so on.\nApache Spark driver and worker logs, which you can use for debugging.\nCluster init-script logs, valuable for debugging init scripts.\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage#event-log","comment_id":"471294"},{"comment_id":"394472","timestamp":"1640852520.0","upvote_count":"11","poster":"dragos_dragos62000","content":"Correct"},{"comment_id":"1189835","upvote_count":"1","content":"B. cluster event logs.\n\nExplanation:\n\nCluster event logs provide information about the cluster's lifecycle events, including the initialization process.\nWhen you specify an additional library to install on the Databricks cluster, the installation process is part of the cluster initialization.\nReviewing the cluster event logs can help you determine whether the library installation process encountered any errors or issues that prevented the library from being installed successfully.\nAny errors or warnings during the library installation process would likely be logged in the cluster event logs, providing insights into the cause of the issue.","poster":"Elanche","timestamp":"1728120540.0"},{"poster":"be8a152","upvote_count":"1","content":"B. Cluster Event Logs","comment_id":"1142862","timestamp":"1722983760.0"},{"timestamp":"1720505400.0","upvote_count":"2","comment_id":"1117290","content":"Selected Answer: B\nits so simple yet you all are making it hard","poster":"dakku987"},{"content":"Selected Answer: B\nThe correct answer is B\n\nCluster event logs capture two init script events: INIT_SCRIPTS_STARTED and INIT_SCRIPTS_FINISHED, indicating which scripts are scheduled for execution and which have completed successfully. INIT_SCRIPTS_FINISHED also captures execution duration.\n\nhttps://docs.databricks.com/en/init-scripts/logs.html","comment_id":"1112725","timestamp":"1720000680.0","poster":"jongert","upvote_count":"2"},{"content":"Selected Answer: B\nChatGpt :\nif the library was to be installed through:\n- Standard Databricks library installation methods: Check the cluster event logs (B).\n- A global init script: Check the global init scripts logs (C).\nWithout additional context or explicit mention of an init script being used, option B is typically the more standard choice for initial troubleshooting.","comment_id":"1105400","upvote_count":"1","poster":"Momoanwar","timestamp":"1719325980.0"},{"upvote_count":"1","comment_id":"994876","poster":"kkk5566","timestamp":"1709197740.0","content":"Legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces starting February 21, 2023. On September 1st, 2023, Azure Databricks will disable legacy global init scripts for all workspaces."},{"content":"Selected Answer: C\nshould be C","comment_id":"994872","poster":"kkk5566","upvote_count":"1","timestamp":"1709197620.0"},{"content":"Selected Answer: B\nCluster event logs","upvote_count":"2","timestamp":"1704618000.0","poster":"[Removed]","comment_id":"945423"},{"content":"Selected Answer: B\nCluster event logs in Azure Databricks provide detailed information about the cluster's lifecycle events, including the installation and initialization of libraries. By reviewing the cluster event logs, you can examine the events related to library installation and determine if any errors or issues occurred during the process.","upvote_count":"3","comment_id":"937165","timestamp":"1703807760.0","poster":"vctrhugo"},{"upvote_count":"1","poster":"auwia","content":"Selected Answer: C\nCluster event logs do not log init script events for each cluster node; only one node is selected to represent them all.\n\nhttps://learn.microsoft.com/en-us/azure/databricks/clusters/init-scripts","comments":[{"comment_id":"976643","upvote_count":"1","poster":"bch9994","timestamp":"1707488880.0","content":"That's incorrect. It is a part of init scripts.\nSome examples of tasks performed by init scripts include:\n\nSet system properties and environment variables used by the JVM.\nModify Spark configuration parameters.\nModify the JVM system classpath in special cases.\nInstall packages and libraries not included in Databricks Runtime. To install Python packages, use the Azure Databricks pip binary located at /databricks/python/bin/pip to ensure that Python packages install into the Azure Databricks Python virtual environment rather than the system Python environment. For example, /databricks/python/bin/pip install <package-name>.\nhttps://learn.microsoft.com/en-us/azure/databricks/init-scripts/"}],"comment_id":"934502","timestamp":"1703604420.0"},{"poster":"aemilka","comments":[{"poster":"vctrhugo","upvote_count":"1","timestamp":"1703807880.0","comment_id":"937167","content":"There are two primary ways to install a library on a cluster:\n- Install a workspace library that has been already been uploaded to the workspace.\n- Install a library for use with a specific cluster only."}],"content":"Selected Answer: C\nAdditional libraries are installed in global init scripts, so correct answer is C.\n\nSome examples of tasks performed by init scripts include:\n- Install packages and libraries not included in Databricks Runtime. To install Python packages, use the Azure Databricks pip binary located at /databricks/python/bin/pip to ensure that Python packages install into the Azure Databricks Python virtual environment rather than the system Python environment. For example, /databricks/python/bin/pip install <package-name>.\n- Modify the JVM system classpath in special cases.\n- Set system properties and environment variables used by the JVM.\n- Modify Spark configuration parameters.\n\nref: https://learn.microsoft.com/en-us/azure/databricks/clusters/init-scripts","upvote_count":"2","timestamp":"1697739540.0","comment_id":"875010"},{"timestamp":"1696585140.0","comment_id":"862818","upvote_count":"2","poster":"kornat","content":"Selected Answer: C\ncorrect"},{"timestamp":"1694598240.0","poster":"esaade","content":"Selected Answer: B\nthe best option in this scenario would be to review the cluster event logs to identify the cause of the issue where an additional library is not found in the Azure Databricks cluster.","upvote_count":"3","comment_id":"837890"},{"timestamp":"1691562540.0","comment_id":"802973","upvote_count":"2","content":"Answer C.\nA global init script runs on every cluster created in your workspace. Global init scripts are useful when you want to enforce organization-wide library configurations or security screens. Only admins can create global init scripts. You can create them using either the UI or REST API.","poster":"lafita"},{"poster":"youngbug","upvote_count":"2","content":"Selected Answer: C\ncluster evnet logs only record start and finish event, so C is right, init script logs record the details of running.","timestamp":"1690251780.0","comment_id":"787265"},{"content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/azure/databricks/clusters/init-scripts:\n\nInit script start and finish events are captured in cluster event logs. Details are captured in cluster logs. Global init script create, edit, and delete events are also captured in account-level diagnostic logs.\n\nCluster event logs capture two init script events: INIT_SCRIPTS_STARTED and INIT_SCRIPTS_FINISHED, indicating which scripts are scheduled for execution and which have completed successfully. INIT_SCRIPTS_FINISHED also captures execution duration.\nGlobal init scripts are indicated in the log event details by the key \"global\" and cluster-scoped init scripts are indicated by the key \"cluster\".","timestamp":"1683885360.0","poster":"gerrie1979","comment_id":"716671","upvote_count":"2"},{"comment_id":"713178","poster":"dmitriypo","content":"Selected Answer: C\nAgree with the given answer - C\n\nDatabase customers use init scripts for various purposes such as installing custom libraries, launching background processes, or applying enterprise security policies.\n\nReference:\nhttps://www.databricks.com/blog/2018/08/30/introducing-cluster-scoped-init-scripts.html","timestamp":"1683471360.0","upvote_count":"2"},{"poster":"Raghu108","timestamp":"1658243760.0","upvote_count":"1","content":"My Answer is B","comment_id":"527765"},{"upvote_count":"6","comment_id":"512675","timestamp":"1656527280.0","poster":"edba","content":"I think answer is B - Cluster Event logs. Because there are 3 ways to install a new library ( https://docs.microsoft.com/en-us/azure/databricks/libraries/cluster-libraries#--install-a-library-on-a-cluster ), using init script is just one of them."},{"content":"B 'Cluster event logs' is the correct answer.","poster":"Canary_2021","timestamp":"1656374580.0","comment_id":"510731","upvote_count":"5"},{"upvote_count":"6","poster":"rashjan","timestamp":"1654663500.0","comment_id":"496601","content":"Selected Answer: B\nI would go with Cluster Event Logs."},{"content":"Shouldn't it be cluster-scoped Init scripts rather than global init scripts.","comment_id":"454647","upvote_count":"3","comments":[{"comment_id":"934506","upvote_count":"1","poster":"auwia","content":"Cluster-scoped: run on every cluster configured with the script. This is the recommended way to run an init script.\nGlobal: run on every cluster in the workspace. They can help you to enforce consistent cluster configurations across your workspace. Use them carefully because they can cause unanticipated impacts, like library conflicts.","timestamp":"1703604540.0"},{"comment_id":"479589","poster":"gf2tw","content":"Yep, the answer even specifies cluster-scoped init scripts so it seems somehow the question doesn't match up.","timestamp":"1652721240.0","upvote_count":"1"}],"timestamp":"1648607280.0","poster":"Sudheer_K"}],"question_text":"You create an Azure Databricks cluster and specify an additional library to install.\nWhen you attempt to load the library to a notebook, the library in not found.\nYou need to identify the cause of the issue.\nWhat should you review?","question_images":[],"timestamp":"2021-06-30 08:22:00"},{"id":"UntadqUwD3cRFAIYcAth","discussion":[{"poster":"kkk5566","upvote_count":"1","timestamp":"1725160080.0","comment_id":"995618","content":"Selected Answer: C\ncorrect"},{"content":"this question is not repeated as options are different. It could appear the first one or this.","timestamp":"1700578200.0","comment_id":"723641","poster":"maximilianogarcia6","upvote_count":"4"},{"timestamp":"1699004760.0","upvote_count":"2","content":"Question is a repeat","comment_id":"710392","poster":"AdarshKumarKhare"},{"content":"Selected Answer: C\nCorrect. Repeated question.","upvote_count":"3","poster":"sensaint","timestamp":"1698493260.0","comment_id":"706406"}],"unix_timestamp":1666957260,"topic":"4","answer":"C","answer_images":[],"isMC":true,"question_text":"A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\nThe company must be able to monitor the devices in real-time.\nYou need to design the solution.\nWhat should you recommend?","answer_description":"","timestamp":"2022-10-28 13:41:00","choices":{"A":"Azure Analysis Services using Azure PowerShell","D":"Azure Data Factory instance using Microsoft Visual Studio","B":"Azure Data Factory instance using Azure PowerShell","C":"Azure Stream Analytics cloud job using Azure Portal"},"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/86576-exam-dp-203-topic-4-question-30-discussion/","question_id":325,"answer_ET":"C","answers_community":["C (100%)"],"exam_id":67}],"exam":{"id":67,"isBeta":false,"lastUpdated":"12 Apr 2025","isImplemented":true,"provider":"Microsoft","isMCOnly":false,"numberOfQuestions":384,"name":"DP-203"},"currentPage":65},"__N_SSP":true}