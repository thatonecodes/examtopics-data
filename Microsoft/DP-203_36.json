{"pageProps":{"questions":[{"id":"g1TBjxO0ps4llytFjxiL","unix_timestamp":1623401040,"question_id":176,"answer_ET":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0019200002.png","https://www.examtopics.com/assets/media/exam-media/04259/0019300001.jpg"],"timestamp":"2021-06-11 10:44:00","question_text":"DRAG DROP -\nYou have an Azure Stream Analytics job that is a Stream Analytics project solution in Microsoft Visual Studio. The job accepts data generated by IoT devices in the JSON format.\nYou need to modify the job to accept data generated by the IoT devices in the Protobuf format.\nWhich three actions should you perform from Visual Studio on sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","isMC":false,"discussion":[{"comments":[{"poster":"17lan","comment_id":"1324887","timestamp":"1733898600.0","content":"Custom .net deserializer for Azure Stream Analytics will be retired on 30th September 2024. After that date, it won't be possible to use the feature. Please transition to a JSON, AVRO, or CSV built-in deserializer by that date.","upvote_count":"1"},{"content":"Absolutely: https://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer","timestamp":"1631299740.0","poster":"Marcus1612","comments":[{"poster":"kamil_k","upvote_count":"6","content":"this is a tricky question.. technically document here https://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer describes it as follows:\n1. Add custom deserializer project\n2. Add the MessageBodyProto class and the MessageBodyDeserializer class to your project (it's actually merged with point 1)\n3. add an Azure Stream Analytics project\n4. Configure a Stream Analytics job (including changing input.json)\n\nTechnically, the question asks for 3 steps so we might to either skip the \"add code\" as a separate step, and include point 4 as the last step (i.e. 1,3,4 above) or we stop at 3 and then the answer is as listed.. As usual, the description of the question is confusing.","timestamp":"1647330420.0","comment_id":"568208"}],"comment_id":"442657","upvote_count":"6"}],"comment_id":"401761","poster":"zarga","upvote_count":"66","timestamp":"1625738400.0","content":"The third one is wrong because the stream analytics application already exist in the project. \nThe goal is to modify the current stream analytics application in order to read protobuff data. \nI think the right answer is the first one in the list (update input.json file and reference dll)"},{"content":"1 Add an Azure Stream Analytics Customer Deserializer Project(.net) project to the Solution \n2 Add .net deseriliaizer Code to ProtoBuf to customer deserializer project\n3. Change the event Serialization format to protobuf in the input.json File of the job and reference the DLL.","upvote_count":"62","poster":"HaliBrickclay","timestamp":"1634536980.0","comment_id":"463900"},{"content":"1 Add an Azure Stream Analytics Customer Deserializer Project(.net) project to the Solution\n2 Add .net deseriliaizer Code to ProtoBuf to customer deserializer project\n3. Change the event Serialization format to protobuf in the input.json File of the job and reference the DLL.\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/custom-deserializer#configure-a-stream-analytics-job","poster":"seranvijay","upvote_count":"1","timestamp":"1731214920.0","comment_id":"1309308"},{"poster":"jpgsa11","comment_id":"1213039","content":"I bet even the Microsoft arquitect that turned goose farmer has nightmares with this question","timestamp":"1715976960.0","upvote_count":"8"},{"upvote_count":"4","timestamp":"1702162920.0","content":"Chatgpt :\nTo modify an Azure Stream Analytics job in Visual Studio to accept data in Protobuf format from IoT devices, you would typically need to:\n\n1. **Add an Azure Stream Analytics Custom Deserializer Project (.NET project) to the solution**: This sets up a project that can include the custom deserialization logic.\n\n2. **Add .NET deserializer code for Protobuf to the custom deserializer project**: Here, you would implement the Protobuf deserialization logic within the project you added in the previous step.\n\n3. **Change the Event Serialization Format to Protobuf in the input.json file of the job and reference the DLL**: Finally, you need to update the job configuration to use the custom deserializer by changing the serialization format and pointing it to the compiled DLL from your custom deserializer project. \n\nThese actions will enable the Azure Stream Analytics job to deserialize and process data in Protobuf format instead of JSON.","comment_id":"1092132","poster":"Momoanwar"},{"poster":"Lscranio","upvote_count":"1","timestamp":"1701747540.0","comment_id":"1088172","content":"1- Add Azure Stream Analyticst Custon Deserializer Project (.NET) project to the Solution;\n2 - Add an Azure Stream Analytics Application project to the solution;\n3 - Change... \"Iput.json\" is necessary your modification;"},{"content":"1. Add Azure Stream Analytics Custom Deserializer Project (.NET) 2. Add Azure Stream Analytics Application 3. Configure a Stream Analytics job in Input.json","timestamp":"1694004660.0","poster":"kkk5566","comment_id":"1000621","upvote_count":"1"},{"timestamp":"1684989900.0","upvote_count":"3","content":"Correct answer is:\nAdd an Azure Stream Analytics Customer Deserializer Project(.net) project to the Solution\nAdd .net deserializer Code to ProtoBuf to the customer deserializer project\nChange the event Serialization format to protobuf in the input.json File of the job and reference the DLL","comment_id":"906356","poster":"janaki"},{"timestamp":"1684936620.0","poster":"bakamon","content":"1. Add an Azure Stream Analytics Custom Deserializer Project (.NET) project to the solution.\n2. Add .NET deserializer code for Protobuf to the custom deserializer project.\n3. Change the Event Serialization Format to Protobuf in the input.json file of the job and reference the DLL.\nThis will allow you to create a custom deserializer project and add .NET deserializer code for Protobuf to it. Then, you can change the Event Serialization Format to Protobuf in the input.json file of the job and reference the DLL containing the custom deserializer code.","upvote_count":"2","comment_id":"905952"},{"comment_id":"879272","upvote_count":"3","content":"Chat GPT: B-C-A","poster":"Rossana","timestamp":"1682336400.0"},{"timestamp":"1679968620.0","comment_id":"852656","comments":[{"poster":"ELJORDAN23","timestamp":"1705416240.0","upvote_count":"2","content":"It seems that not anymore, I think that as of january 16 of 2024, there is nothing related to .NET and ProtoBuf in the learning path of DP-203","comment_id":"1124291"}],"upvote_count":"6","content":"guessing seem not for DP203, anyone agree?","poster":"hiyoww"},{"timestamp":"1678179180.0","content":"To modify the Azure Stream Analytics job to accept data generated by the IoT devices in the Protobuf format, follow these steps in sequence:\n\nAdd an Azure Stream Analytics Custom Deserializer Project (.NET) project to the solution.\nAdd .NET deserializer code for Protobuf to the custom deserializer project.\nChange the Event Serialization Format to Protobuf in the input.json file of the job and reference the DLL.","comment_id":"831678","poster":"esaade","upvote_count":"1"},{"timestamp":"1666336200.0","comment_id":"700599","content":"According to the documentation: \n1- Create a custom deserializer for protocol buffer.\n2- Add an Azure Stream Analytics project\n3- Configure a Stream Analytics job ( in here you specify reference the dll...) ==> Change the event Serialization format to protobuf in the input.json File of the job and reference the DLL.\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/custom-deserializer","comments":[],"poster":"zorko10","upvote_count":"3"},{"upvote_count":"1","content":"As stated in the documentation : \n1- Create a custom deserializer project\n2- Add an azure stream alaytics project\n3- Configure a stream analytics job, ( in this configuration, the dll is referenced) ==> update input.json file and reference dll\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/custom-deserializer","poster":"nadahef","timestamp":"1666334400.0","comment_id":"700569"},{"upvote_count":"1","content":"I am still practicing and have not seen the answer yet. Have put my answer as 3-2-1. Will see the right answer in the end.","poster":"ExamTopicsAshwin","comment_id":"687491","timestamp":"1665034140.0"},{"timestamp":"1658809560.0","poster":"Franz58","content":"As described in https://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer:\n1. Add Azure Stream Analytics Custom Deserializer Project (.NET)\n2. Add Azure Stream Analytics Application\n3. Configure a Stream Analytics job in Input.json","comments":[{"timestamp":"1687525440.0","upvote_count":"1","content":"Yes, I think your answer is correct following this link : https://learn.microsoft.com/en-us/azure/stream-analytics/custom-deserializer","comment_id":"931614","poster":"Paulkuzzio"}],"comment_id":"637144","upvote_count":"7"},{"upvote_count":"16","content":"Has this question come up in the DP-203 exam?","timestamp":"1636039260.0","comment_id":"472652","poster":"Johno1393"},{"upvote_count":"4","comment_id":"430404","content":"Third one should be the first action listed: Change file format in input.json","poster":"[Removed]","timestamp":"1629769440.0"},{"content":"Correct!","upvote_count":"1","poster":"Gowthamr02","timestamp":"1623401040.0","comment_id":"379593"}],"url":"https://www.examtopics.com/discussions/microsoft/view/55087-exam-dp-203-topic-2-question-24-discussion/","answer_description":"Step 1: Add an Azure Stream Analytics Custom Deserializer Project (.NET) project to the solution.\n\nCreate a custom deserializer -\n1. Open Visual Studio and select File > New > Project. Search for Stream Analytics and select Azure Stream Analytics Custom Deserializer Project (.NET). Give the project a name, like Protobuf Deserializer.\n\n2. In Solution Explorer, right-click your Protobuf Deserializer project and select Manage NuGet Packages from the menu. Then install the\nMicrosoft.Azure.StreamAnalytics and Google.Protobuf NuGet packages.\n3. Add the MessageBodyProto class and the MessageBodyDeserializer class to your project.\n4. Build the Protobuf Deserializer project.\nStep 2: Add .NET deserializer code for Protobuf to the custom deserializer project\nAzure Stream Analytics has built-in support for three data formats: JSON, CSV, and Avro. With custom .NET deserializers, you can read data from other formats such as Protocol Buffer, Bond and other user defined formats for both cloud and edge jobs.\nStep 3: Add an Azure Stream Analytics Application project to the solution\nAdd an Azure Stream Analytics project\n1. In Solution Explorer, right-click the Protobuf Deserializer solution and select Add > New Project. Under Azure Stream Analytics > Stream Analytics, choose\nAzure Stream Analytics Application. Name it ProtobufCloudDeserializer and select OK.\n2. Right-click References under the ProtobufCloudDeserializer Azure Stream Analytics project. Under Projects, add Protobuf Deserializer. It should be automatically populated for you.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0019200001.png"],"exam_id":67,"answer":"","topic":"2","answers_community":[]},{"id":"FWoPy3NYef0dUE6dIcI3","answer_images":[],"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/54531-exam-dp-203-topic-2-question-25-discussion/","choices":{"C":"Self-hosted integration runtime","B":"Azure-SSIS integration runtime","A":"Azure integration runtime"},"question_text":"You have an Azure Storage account and a data warehouse in Azure Synapse Analytics in the UK South region.\nYou need to copy blob data from the storage account to the data warehouse by using Azure Data Factory. The solution must meet the following requirements:\n✑ Ensure that the data remains in the UK South region at all times.\n✑ Minimize administrative effort.\nWhich type of integration runtime should you use?","discussion":[{"timestamp":"1657274580.0","comment_id":"401767","content":"A is the right answer (don't use autoresolve region)","comments":[{"comment_id":"725347","timestamp":"1700766660.0","content":"Here I found in the docs https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime.\nYou can set the location region of an Azure IR, in which case the activity execution or dispatch will happen in the selected region\n\nSelf-hosted integration runtime can achieve the same goal with higher administrative effort","upvote_count":"1","poster":"SomethingRight100"}],"poster":"zarga","upvote_count":"44"},{"content":"Should not this be option A??\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime\n\n\"If you have strict data compliance requirements and need ensure that data do not leave a certain geography, you can explicitly create an Azure IR in a certain region and point the Linked Service to this IR using ConnectVia property. For example, if you want to copy data from Blob in UK South to Azure Synapse Analytics in UK South and want to ensure data do not leave UK, create an Azure IR in UK South and link both Linked Services to this IR.\"","upvote_count":"14","comment_id":"387519","poster":"kishorenayak","comments":[{"upvote_count":"2","content":"Yes, it is Azure integration runtime - option A","poster":"janaki","timestamp":"1716612780.0","comment_id":"906369"},{"comment_id":"392698","content":"Yes it's option A","poster":"Dicupillo","timestamp":"1656402480.0","upvote_count":"2"}],"timestamp":"1655860200.0"},{"content":"Selected Answer: A\nA is correct","comment_id":"1000624","upvote_count":"1","poster":"kkk5566","timestamp":"1725627300.0"},{"comment_id":"831679","content":"To ensure that the data remains in the UK South region and minimize administrative effort while copying blob data from the storage account to the data warehouse by using Azure Data Factory, you should use the Azure integration runtime.","poster":"esaade","upvote_count":"3","timestamp":"1709801700.0"},{"comment_id":"640182","content":"Selected Answer: A\nA is correct","upvote_count":"2","poster":"Deeksha1234","timestamp":"1690821420.0"},{"content":"I think the first requirement isn't adding much to the equation, so it is primarily focussed on administration which is lowest with A","timestamp":"1688125020.0","upvote_count":"2","poster":"Fishy_Marcy","comment_id":"625224"},{"upvote_count":"2","timestamp":"1686977640.0","comment_id":"617529","content":"Selected Answer: A\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime","poster":"StudentFromAus"},{"comment_id":"515742","upvote_count":"4","timestamp":"1672751340.0","poster":"DrTaz","content":"Selected Answer: A\nWhy would I want to move data to a local host then back to cloud? That sounds a bit unwise, eh?"},{"content":"Selected Answer: A\nA is correct","poster":"rashjan","timestamp":"1670422620.0","comment_id":"496082","upvote_count":"1"},{"poster":"hsetin","upvote_count":"2","timestamp":"1661618280.0","content":"A it is.","comment_id":"433237"},{"timestamp":"1655826840.0","comment_id":"387253","upvote_count":"2","content":"Correct answer.","poster":"saty_nl"},{"comment_id":"383601","content":"fully agree","poster":"damaldon","upvote_count":"1","timestamp":"1655401080.0"},{"content":"A is correct","comment_id":"374385","poster":"Sunnyb","timestamp":"1654351380.0","upvote_count":"2"}],"question_images":[],"exam_id":67,"timestamp":"2021-06-04 16:03:00","isMC":true,"unix_timestamp":1622815380,"answer_description":"","question_id":177,"answer_ET":"A","topic":"2","answer":"A"},{"id":"NfMWGgE80HjUTatPiDbZ","question_id":178,"discussion":[{"upvote_count":"42","poster":"Sunnyb","comment_id":"374389","content":"Answer is correct","timestamp":"1638633840.0"},{"comment_id":"523470","poster":"Jaws1990","timestamp":"1657786080.0","upvote_count":"16","content":"Crap question. With that data, how are you supposed to link the stream data with the reference data."},{"poster":"KarlGardnerDataEngineering","upvote_count":"6","comment_id":"1221681","timestamp":"1732988520.0","content":"Why are some of these questions very simple and others are insanely hard?"},{"comment_id":"1000626","upvote_count":"1","content":"correct","timestamp":"1709737080.0","poster":"kkk5566"},{"upvote_count":"3","timestamp":"1696500600.0","comment_id":"861960","poster":"mafragias","content":"Stream, STream, Reference"},{"content":"Correct\nData stream input : is an unbounded sequence of events over time. \nReference Data input : Reference data is either completely static or changes slowly\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs","poster":"zorko10","comment_id":"700610","upvote_count":"6","timestamp":"1682061900.0"},{"timestamp":"1675190400.0","upvote_count":"2","comment_id":"640184","poster":"Deeksha1234","content":"correct"},{"comment_id":"620776","timestamp":"1671779280.0","comments":[{"comment_id":"708610","content":"HubA doesn't contain driver's name. It is Database1 that contains driver's name.","timestamp":"1682869500.0","upvote_count":"4","poster":"dmitriypo"}],"upvote_count":"3","content":"why hubA, that contains driver's name, should be processed as stream?? it's a dimension type and should be processed as reference in my opinion","poster":"wolf74"},{"upvote_count":"2","comment_id":"468943","poster":"gssd4scoder","timestamp":"1651109520.0","content":"Correct: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs"},{"upvote_count":"4","timestamp":"1646923920.0","comments":[{"poster":"[Removed]","timestamp":"1649547480.0","comment_id":"459802","content":"it could be real, he is asking if you can realize the main difference between the real-time data vs the reference data and so you can choose the best service for each one","upvote_count":"5"},{"content":"I mean they can't just ask hard questions where you need 5min to think about, no one would pass that. Also they want the certificate to be worth something but also have many people passing the exam and go for Azure instead of AWS, GCP.","upvote_count":"1","poster":"Vanq69","timestamp":"1712306760.0","comment_id":"1025478"},{"poster":"[Removed]","timestamp":"1649547540.0","comment_id":"459803","content":"I have tried some pretty easy question like this one before in prev exams","upvote_count":"3"}],"poster":"GameLift","content":"I doubt if these questions are really what they asking in the real exam.","comment_id":"442491"}],"timestamp":"2021-06-04 16:04:00","topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/54533-exam-dp-203-topic-2-question-26-discussion/","isMC":false,"answer":"","answer_ET":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0019700001.png"],"answers_community":[],"question_text":"HOTSPOT -\nYou have an Azure SQL database named Database1 and two Azure event hubs named HubA and HubB. The data consumed from each source is shown in the following table.\n//IMG//\n\nYou need to implement Azure Stream Analytics to calculate the average fare per mile by driver.\nHow should you configure the Stream Analytics input for each source? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"HubA: Stream -\n\nHubB: Stream -\n\nDatabase1: Reference -\nReference data (also known as a lookup table) is a finite data set that is static or slowly changing in nature, used to perform a lookup or to augment your data streams. For example, in an IoT scenario, you could store metadata about sensors (which don't change often) in reference data and join it with real time IoT data streams. Azure Stream Analytics loads reference data in memory to achieve low latency stream processing\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data","unix_timestamp":1622815440,"exam_id":67,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0019500001.png","https://www.examtopics.com/assets/media/exam-media/04259/0019600001.png"]},{"id":"rsheyPccYQBgl6Jo0Bay","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/55088-exam-dp-203-topic-2-question-27-discussion/","isMC":true,"answer_description":"","exam_id":67,"answer":"B","choices":{"C":"SELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, HoppingWindow(second, 10, 2)","D":"SELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SessionWindow(second, 5, 10)","B":"SELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, TumblingWindow(second, 10)","A":"SELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SlidingWindow(second, 10)"},"question_images":[],"discussion":[{"comment_id":"387254","timestamp":"1655826960.0","poster":"saty_nl","upvote_count":"25","content":"Correct answer."},{"comments":[{"poster":"sdokmak","upvote_count":"1","comment_id":"606647","content":"Not really, the other Count option doesn't overlap. But it does skip.","timestamp":"1684926780.0"}],"content":"keyword : do not overlap","timestamp":"1662814620.0","comment_id":"442497","poster":"GameLift","upvote_count":"5"},{"content":"Selected Answer: B\ncorrect","upvote_count":"1","poster":"kkk5566","timestamp":"1725627720.0","comment_id":"1000629"},{"upvote_count":"2","timestamp":"1716734820.0","poster":"joponlu","comment_id":"907455","content":"Selected Answer: B\nYes B is correct!!!"},{"content":"Selected Answer: B\nB is correct","poster":"cale","timestamp":"1712478960.0","upvote_count":"3","comment_id":"863657"},{"content":"Selected Answer: B\nCorrect","comment_id":"699289","poster":"igormmpinto","upvote_count":"2","timestamp":"1697742420.0"},{"comment_id":"684130","timestamp":"1696154820.0","content":"A,C are excluded easily by the AVG function, the D also excluded by the session size that is less the window size and this will result in overlapped windows.\nB is left alone Hahaha just want to describe a different method to answer exam questions.","poster":"allagowf","upvote_count":"4"},{"upvote_count":"3","timestamp":"1690821840.0","comment_id":"640185","poster":"Deeksha1234","content":"Selected Answer: B\nB is correct"},{"content":"Selected Answer: B\nB is correct","upvote_count":"2","poster":"rashjan","timestamp":"1670422740.0","comment_id":"496083"},{"poster":"gssd4scoder","content":"Correct: https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics","timestamp":"1666968420.0","comment_id":"469333","upvote_count":"2"},{"poster":"damaldon","content":"Correct, Tumbling Window is needed to use periodic time intervals","timestamp":"1655401440.0","upvote_count":"2","comment_id":"383606"},{"poster":"Gowthamr02","upvote_count":"2","content":"Correct!","timestamp":"1654937160.0","comment_id":"379598"}],"question_id":179,"answer_images":[],"answer_ET":"B","timestamp":"2021-06-11 10:46:00","unix_timestamp":1623401160,"question_text":"You have an Azure Stream Analytics job that receives clickstream data from an Azure event hub.\nYou need to define a query in the Stream Analytics job. The query must meet the following requirements:\n✑ Count the number of clicks within each 10-second window based on the country of a visitor.\n✑ Ensure that each click is NOT counted more than once.\nHow should you define the Query?","topic":"2"},{"id":"DSQBxnKaiA3wUcibOtO1","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0019900002.png"],"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/68346-exam-dp-203-topic-2-question-28-discussion/","question_text":"HOTSPOT -\nYou are building an Azure Analytics query that will receive input data from Azure IoT Hub and write the results to Azure Blob storage.\nYou need to calculate the difference in the number of readings per sensor per hour.\nHow should you complete the query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"poster":"ANath","upvote_count":"19","comment_id":"526653","timestamp":"1658143800.0","content":"The answer is correct"},{"comment_id":"771613","poster":"UristMcFarmer","timestamp":"1688999460.0","upvote_count":"10","content":"The question and answer do not match. The question asks for \"the difference in THE NUMBER OF READINGS per sensor per hour\". The answer given is to compute the difference between the current sensor reading and the sensor reading from an hour ago."},{"upvote_count":"8","content":"Correct.\n\nChoosing the Right Function:\n\nWhen you need to compare a value with its future occurrences: Use LEAD.\nWhen you need to analyze dependencies or trends based on past values: Use LAG.\nWhen you want to identify the maximum or minimum for a group based on a specific order: Use LAST.","timestamp":"1722781380.0","poster":"Azure_2023","comment_id":"1140320"},{"upvote_count":"1","timestamp":"1717967100.0","content":"Correct, cgatpgt :\nTo calculate the difference in the number of readings per sensor per hour using an Azure Stream Analytics query, you would use the LAG function to access the previous value and then calculate the difference. Here is how you would complete the query:\n\n- Use `LAG` to get the previous reading.\n- Use `LIMIT DURATION` to set the window of time for comparison, which in this case is per hour.\n\nThe completed query would look something like this:\n\n```sql\nSELECT sensorId, \n reading - LAG(reading) OVER (PARTITION BY sensorId LIMIT DURATION(hour, 1)) AS growth \nFROM input\n```\n\nThis query assumes `reading` is the column holding the sensor data and `sensorId` is the column to partition the data by each sensor. The `LAG` function gets the last reading for the same sensor from the previous hour, and then you subtract this value from the current reading to find the growth.","comment_id":"1092134","poster":"Momoanwar"},{"upvote_count":"1","timestamp":"1709737440.0","comment_id":"1000632","content":"should be correct","poster":"kkk5566"},{"upvote_count":"4","content":"LAG is the correct answer. Refer the below link. It mentions this example\nhttps://learn.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics","comment_id":"902746","timestamp":"1700517300.0","poster":"Rajan191083"},{"upvote_count":"4","timestamp":"1675190940.0","content":"right answer","comment_id":"640188","poster":"Deeksha1234"},{"comments":[{"upvote_count":"4","comment_id":"601015","content":"LAST is possible, however the code sample in this question does not include any WHEN syntax, so that will rule out LAST","comments":[{"comment_id":"956783","upvote_count":"2","poster":"UzairMir","content":"When clause is optional \nhttps://learn.microsoft.com/en-us/stream-analytics-query/last-azure-stream-analytics\nStill cannot understand the difference between LAG and LAST :(","comments":[{"upvote_count":"3","timestamp":"1707650640.0","comment_id":"978477","content":"LAST is the most recent event - literally opposite to the name.. that is why they take LAG which gives the previous event","poster":"[Removed]"}],"timestamp":"1705686480.0"}],"poster":"hbad","timestamp":"1668332640.0"}],"timestamp":"1667041560.0","poster":"Muishkin","comment_id":"594414","content":"what about LAST?","upvote_count":"1"},{"comments":[{"upvote_count":"6","poster":"bubububox","timestamp":"1656155880.0","comment_id":"509125","content":"yep, but the question here is unclear"}],"upvote_count":"5","poster":"onyerleft","timestamp":"1655757960.0","comment_id":"505715","content":"Answers as revealed are for computing the rate of growth per sensor. https://docs.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics#examples"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0019900001.png"],"exam_id":67,"isMC":false,"timestamp":"2021-12-20 23:46:00","unix_timestamp":1640040360,"answer_description":"Box 1: LAG -\nThe LAG analytic operator allows one to look up a ג€previousג€ event in an event stream, within certain constraints. It is very useful for computing the rate of growth of a variable, detecting when a variable crosses a threshold, or when a condition starts or stops being true.\n\nBox 2: LIMIT DURATION -\nExample: Compute the rate of growth, per sensor:\nSELECT sensorId,\ngrowth = reading -\nLAG(reading) OVER (PARTITION BY sensorId LIMIT DURATION(hour, 1))\n\nFROM input -\nReference:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics","question_id":180,"answer_ET":"","topic":"2","answer":""}],"exam":{"name":"DP-203","numberOfQuestions":384,"isImplemented":true,"isMCOnly":false,"lastUpdated":"12 Apr 2025","id":67,"provider":"Microsoft","isBeta":false},"currentPage":36},"__N_SSP":true}