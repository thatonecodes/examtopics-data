{"pageProps":{"questions":[{"id":"JdYF5rNYGb4ZVJvbUlHn","question_id":281,"discussion":[{"poster":"damaldon","content":"Correct!","upvote_count":"28","comment_id":"384359","timestamp":"1639770060.0"},{"comment_id":"384313","content":"Answer is correct. Dynamic data masking will limit the exposure of sensitive data.","timestamp":"1639764120.0","upvote_count":"11","poster":"saty_nl"},{"poster":"Dusica","timestamp":"1730542920.0","content":"B and C\nnobody mentions purview there","upvote_count":"1","comment_id":"1205375"},{"timestamp":"1729906620.0","content":"The answer is B and C\nSensitivity classification tags are part of Purview. Purview is a separate service, and an expensive one. The question does not mention Purview.","poster":"Dusica","upvote_count":"1","comment_id":"1202330"},{"poster":"kkk5566","timestamp":"1709209260.0","upvote_count":"2","content":"Selected Answer: AC\naudit aand sensitivity-classification labels","comment_id":"993951"},{"poster":"anks84","content":"Selected Answer: AC\nGiven Answers are correct !","comment_id":"661731","timestamp":"1678153200.0","upvote_count":"4"},{"comment_id":"643670","poster":"Deeksha1234","upvote_count":"3","timestamp":"1675771440.0","content":"correct"},{"timestamp":"1672258560.0","upvote_count":"2","comment_id":"624161","content":"Also for me is correct","poster":"Remedios79"},{"poster":"sparkchu","content":"log auditing & tracing is important for data governance, therefore necessary for any data solution.","upvote_count":"2","timestamp":"1664929020.0","comment_id":"580971"},{"comment_id":"496135","content":"Selected Answer: AC\ncorrect","timestamp":"1654607820.0","poster":"rashjan","upvote_count":"2"},{"content":"Correct: \"The solution needs to identify the users who executed queries, not to hide confidental information.\" thanks @DirectX from this discussion: https://www.examtopics.com/discussions/microsoft/view/51257-exam-dp-201-topic-3-question-32-discussion/","comment_id":"492253","upvote_count":"8","timestamp":"1654150260.0","poster":"rashjan"},{"comment_id":"444940","poster":"dduque10","upvote_count":"1","timestamp":"1647316680.0","comments":[{"poster":"Dizzystar","upvote_count":"1","timestamp":"1651342020.0","content":"wondering the same thing.","comment_id":"470356"},{"content":"Yes, the logs are used to identify the user who executed the query.","comment_id":"492254","upvote_count":"3","poster":"rashjan","timestamp":"1654150320.0"}],"content":"Is it really C correct?"}],"exam_id":67,"timestamp":"2021-06-17 18:02:00","answer_images":[],"isMC":true,"unix_timestamp":1623945720,"answer_description":"","answers_community":["AC (100%)"],"answer":"AC","url":"https://www.examtopics.com/discussions/microsoft/view/55525-exam-dp-203-topic-3-question-3-discussion/","choices":{"A":"sensitivity-classification labels applied to columns that contain confidential information","D":"dynamic data masking for columns that contain confidential information","B":"resource tags for databases that contain confidential information","C":"audit logs sent to a Log Analytics workspace"},"question_text":"You plan to create an Azure Synapse Analytics dedicated SQL pool.\nYou need to minimize the time it takes to identify queries that return confidential information as defined by the company's data privacy regulations and the users who executed the queues.\nWhich two components should you include in the solution? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","question_images":[],"answer_ET":"AC","topic":"3"},{"id":"t2XV5IoF2DMdQsd6TKp1","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0032800002.jpg"],"timestamp":"2022-09-07 12:55:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0032800001.jpg"],"isMC":false,"topic":"3","answer":"","answer_description":"Box 1: Azure SQL Database -\nUse external Hive Metastore for Synapse Spark Pool\nAzure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog.\nSet up linked service to Hive Metastore\nFollow below steps to set up a linked service to the external Hive Metastore in Synapse workspace.\n1. Open Synapse Studio, go to Manage > Linked services at left, click New to create a new linked service.\n2. Set up Hive Metastore linked service\n3. Choose Azure SQL Database or Azure Database for MySQL based on your database type, click Continue.\n4. Provide Name of the linked service. Record the name of the linked service, this info will be used to configure Spark shortly.\n5. You can either select Azure SQL Database/Azure Database for MySQL for the external Hive Metastore from Azure subscription list, or enter the info manually.\n6. Provide User name and Password to set up the connection.\n7. Test connection to verify the username and password.\n8. Click Create to create the linked service.\n\nBox 2: A Hive Metastore -\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore","question_id":282,"question_text":"HOTSPOT -\nYou have an Azure subscription that contains an Azure Databricks workspace named databricks1 and an Azure Synapse Analytics workspace named synapse1.\nThe synapse1 workspace contains an Apache Spark pool named pool1.\nYou need to share an Apache Hive catalog of pool1 with databricks1.\nWhat should you do? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_ET":"","unix_timestamp":1662548100,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/80883-exam-dp-203-topic-3-question-30-discussion/","exam_id":67,"discussion":[{"comment_id":"662370","poster":"federc","comments":[{"content":"scrath that, given anwers are correct. sql + hive metastore","poster":"federc","upvote_count":"21","comment_id":"662371","timestamp":"1678193880.0"},{"timestamp":"1726813380.0","content":"Only Azure SQL Database and Azure Database for MySQL are supported as an external Hive Metastore. And currently we only support User-Password authentication.","comment_id":"1178034","poster":"[Removed]","upvote_count":"2"}],"upvote_count":"10","timestamp":"1678193700.0","content":"I would say:\n1. sql - this is correct\n2. managed hive metastore"},{"upvote_count":"1","comment_id":"1202380","timestamp":"1729916280.0","poster":"Dusica","content":"It seems that it is Managed Hive Metastore; se the second green note in here:\nhttps://learn.microsoft.com/en-us/azure/hdinsight/share-hive-metastore-with-synapse"},{"poster":"Alongi","timestamp":"1729240980.0","content":"Correct for me","comment_id":"1197802","upvote_count":"1"},{"comment_id":"1065477","timestamp":"1715151780.0","poster":"matiandal","comments":[{"timestamp":"1715152020.0","upvote_count":"1","comment_id":"1065479","poster":"matiandal","content":"b2. hive metastore, not a managed ! ( sorry )\nWhy b2 have to be \" A Hive metastore\" and not a managed one"}],"content":"b1-sql\nb2. managed hive metastore\nWhy b2 a managed ? \n\nA Hive metastore is a central repository that stores metadata about the data stored in a Hive warehouse. A managed Hive metastore is a type of Hive metastore that is fully managed by Azure Databricks. It provides the following benefits over a self-managed Hive metastore:\n\nIt is automatically created and configured when you create a Databricks workspace.\nIt is automatically backed up and restored by Databricks.\nIt is automatically scaled and optimized by Databricks.\nIt is compatible with all Databricks features, such as Delta Lake, SQL Analytics, and Unity Catalog.\nA managed Hive metastore is recommended for most use cases, unless you have specific requirements that need a self-managed Hive metastore, such as:\n\nYou want to use an external metastore service, such as AWS Glue or Azure SQL Database.\nYou want to share the same metastore across multiple Databricks workspaces or other applications.","upvote_count":"1"},{"content":"1. sql db\n2. hive metastore","poster":"kkk5566","timestamp":"1709190780.0","upvote_count":"2","comment_id":"994770"},{"comment_id":"948727","upvote_count":"4","timestamp":"1704961320.0","poster":"andjurovicela","content":"1 - definitely correct per documentation TestingCRM provided.\n2 - I think the devil's in the detail here :/ documentation says \"Azure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog\". \nThe word managed may sway you towards the answer managed hive metasotre SERVICE but the docs don't mention \"service\" at all, which is why I would go with Hive metastore"},{"content":"1. sql - this is correct\n2. managed hive metastore\n\nSee https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore","comment_id":"913303","poster":"TestingCRM","timestamp":"1701588300.0","upvote_count":"1"}]},{"id":"YmB4OFqVqorAvsqOa4m5","discussion":[{"upvote_count":"44","comments":[{"timestamp":"1667022060.0","upvote_count":"4","comment_id":"706992","poster":"allagowf","content":"Agree no mention for tiering in the quetion so LRS is the best option to minimize the cost"},{"poster":"mamahani","upvote_count":"1","content":"its not the same as hot; see this microsoft article: https://azure.microsoft.com/nl-nl/blog/azure-premium-block-blob-storage-is-now-generally-available/\n\"'Premium Blob Storage is a new performance tier in Azure Blob Storage for block blobs and append blobs, complimenting the existing Hot, Cool, and Archive access tiers. \"'","comment_id":"883511","timestamp":"1682688360.0"}],"poster":"goxxx","content":"If u choose premium storage account, there is no possibility to choose tiers (hot, cool, archive), its always hot, sa LRS and lifecycle storage mngt","timestamp":"1663785180.0","comment_id":"675434"},{"poster":"dom271219","timestamp":"1662882240.0","comment_id":"665920","content":"The statement doesn't mention requirement for a tiercing storage archive nor cool nor hot before deletion.\nThen I think it is LRS and lifecycle storage mngt","upvote_count":"17"},{"timestamp":"1741493640.0","upvote_count":"1","content":"Azure Data Lake Storage Gen2 Premium accounts are optimized for high-performance workloads, but they do not currently support archive access tiers. Archive tiers are available in standard storage accounts for Azure Data Lake Storage Gen2, which are designed for cost-effective long-term storage of infrequently accessed data.","comment_id":"1367164","poster":"9370d83"},{"timestamp":"1726578000.0","comment_id":"1285238","content":"1. Premium does not support other than Hot. Azure does recommend LRS for lower costs.\n2. Lifecycle can be set in Premium and can manage the removal.\n\nNotes:\n- Soft delete is about recovery, not [programmed] removal.\n- I just tested what I wrote.\n- ChatGPT can be challenged if you feel it provided a wrong answer. And it does.\n\nRef.: https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview#known-issues-and-limitations","upvote_count":"1","poster":"renan_ineu"},{"upvote_count":"1","comment_id":"1133718","timestamp":"1706396940.0","content":"LRS and LCM","poster":"Azure_2023"},{"timestamp":"1703463300.0","poster":"Momoanwar","upvote_count":"1","comment_id":"1104938","content":"Chatgpt :\nTo minimize costs, select **The Archive access tier** since it is optimized for data that is rarely accessed and offers the lowest storage cost. For the deletion of blobs older than 365 days, you would use **Azure Storage lifecycle management** to automate the deletion process, reducing administrative effort."},{"poster":"AlfredPennyworth","upvote_count":"1","comment_id":"1098096","content":"For your Azure Data Lake Storage Gen2 Premium account, considering the requirements:\n\nTo minimize costs: Locally-redundant storage (LRS). This is cost-effective and provides high durability within a single region.\n\nTo delete blobs older than 365 days: Azure Automation runbooks. Since Azure Storage lifecycle management isn't applicable to Premium tier, automation runbooks can be used to programmatically delete older blobs, minimizing administrative effort.","timestamp":"1702724700.0"},{"comment_id":"1001551","poster":"hassexat","content":"LRS & Lifecycle","timestamp":"1694089860.0","upvote_count":"3"},{"comment_id":"994774","timestamp":"1693459080.0","upvote_count":"3","poster":"kkk5566","content":"LRS and LCM"},{"timestamp":"1692005700.0","upvote_count":"1","content":"As per the response from the Microsoft https://github.com/MicrosoftDocs/azure-docs/issues/100695 tiering is not supported for premium but delete through LCM is supported.. but still not clearly mentioned in this document https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview\n\nAnswer LRS and LCM","comment_id":"980647","poster":"[Removed]"},{"upvote_count":"1","poster":"pavankr","timestamp":"1688680680.0","comment_id":"945036","content":"Why you want to \"Archive\"???"},{"comment_id":"937160","poster":"vctrhugo","content":"LRS and data lifecycle. Even tho you can't switch data from tier-to-tier, you can still apply a rule to delete the BLOB once it reaches 365 days.","upvote_count":"2","timestamp":"1687988400.0"},{"upvote_count":"2","comments":[{"timestamp":"1687308840.0","poster":"JG1984","content":"Azure Blob Storage Lifecycle Management allows you to create rules to automatically delete blobs based on their age, reducing administrative effort and minimizing costs. This makes it a better option for meeting the requirements specified in your scenario.\nSoft delete is an option for protecting against accidental deletion of blobs, but it is not the best option for automatically deleting blobs that are older than 365 days. Soft delete works by retaining deleted blobs for a specified period of time, allowing you to recover them if needed. However, it does not automatically delete blobs based on their age.","upvote_count":"1","comment_id":"928915"}],"content":"According to \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview?tabs=azure-portal\n\"Data stored in a premium block blob storage account cannot be tiered to hot, cool, cold or archive by using Set Blob Tier or using Azure Blob Storage lifecycle management.\"\nSo answers are LRS and Soft delete\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/soft-delete-blob-overview\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/soft-delete-container-enable?tabs=azure-portal","poster":"BPW","timestamp":"1684644600.0","comment_id":"902888"},{"comment_id":"883512","comments":[{"content":"also in the documentation all the three tiers are greyed out for premium\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-feature-support-in-storage-accounts#premium-block-blob-accounts\nso you cannot possibly choose this as an answer;","poster":"mamahani","timestamp":"1682688600.0","comment_id":"883516","upvote_count":"1"}],"content":"áccording to microsoft: '\"Premium Blob Storage is a new performance tier in Azure Blob Storage for block blobs and append blobs, complimenting the existing Hot, Cool, and Archive access tiers. \"\"\nhttps://azure.microsoft.com/nl-nl/blog/azure-premium-block-blob-storage-is-now-generally-available/\nso the only two other options left are LRS and ZRS; LRS is cheaper; so it must be this one;","timestamp":"1682688420.0","poster":"mamahani","upvote_count":"1"},{"content":"I strongly doubt they didn't offer the whole question. The question is not clear.","timestamp":"1672016640.0","poster":"youngbug","comment_id":"756114","upvote_count":"1"},{"comment_id":"734771","upvote_count":"7","poster":"AzureJobsTillRetire","content":"Box1: Locally-redundant storage (LRS)\nIn the question, it specifically states that \"You need to deploy an Azure Data Lake Storage Gen2 Premium account\", and Azure Data Lake Storage Gen2 premium tier is neither an Archive access tier nor a Cool Access tier, and so those two options are out. Locally-redundant storage (LRS) is less expensive than Zone-redundant storage (ZRS), so we choose LRS. \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/premium-tier-for-data-lake-storage\n\nBox2: Azure Storage Lifecycle management\nWell explained in the answer already.","timestamp":"1670115540.0"}],"question_id":283,"exam_id":67,"timestamp":"2022-09-11 09:44:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0033100001.jpg"],"isMC":false,"unix_timestamp":1662882240,"answer_description":"Box 1: The Archive access tier -\nArchive tier - An offline tier optimized for storing data that is rarely accessed, and that has flexible latency requirements, on the order of hours. Data in the Archive tier should be stored for a minimum of 180 days.\nBox 2: Azure Storage lifecycle management\nWith the lifecycle management policy, you can:\n* Delete current versions of a blob, previous versions of a blob, or blob snapshots at the end of their lifecycles.\nTransition blobs from cool to hot immediately when they're accessed, to optimize for performance.\nTransition current versions of a blob, previous versions of a blob, or blob snapshots to a cooler storage tier if these objects haven't been accessed or modified for a period of time, to optimize for cost. In this scenario, the lifecycle management policy can move objects from hot to cool, from hot to archive, or from cool to archive.\nEtc.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview https://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview","answer":"","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/81625-exam-dp-203-topic-3-question-31-discussion/","question_text":"HOTSPOT -\nYou have an Azure subscription.\nYou need to deploy an Azure Data Lake Storage Gen2 Premium account. The solution must meet the following requirements:\n* Blobs that are older than 365 days must be deleted.\n* Administrative effort must be minimized.\n* Costs must be minimized.\nWhat should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0033000001.jpg"],"answer_ET":"","topic":"3"},{"id":"JNvH8PRV7Vr1UCw6uha8","question_text":"HOTSPOT -\nYou are designing an application that will use an Azure Data Lake Storage Gen 2 account to store petabytes of license plate photos from toll booths. The account will use zone-redundant storage (ZRS).\nYou identify the following usage patterns:\n* The data will be accessed several times a day during the first 30 days after the data is created. The data must meet an availability SLA of 99.9%.\n* After 90 days, the data will be accessed infrequently but must be available within 30 seconds.\n* After 365 days, the data will be accessed infrequently but must be available within five minutes.\nYou need to recommend a data retention solution. The solution must minimize costs.\nWhich access tier should you recommend for each time frame? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2022-09-02 12:14:00","url":"https://www.examtopics.com/discussions/microsoft/view/79373-exam-dp-203-topic-3-question-32-discussion/","topic":"3","answers_community":[],"answer_description":"Box 1: Hot -\nThe data will be accessed several times a day during the first 30 days after the data is created. The data must meet an availability SLA of 99.9%.\n\nBox 2: Cool -\nAfter 90 days, the data will be accessed infrequently but must be available within 30 seconds.\nData in the Cool tier should be stored for a minimum of 30 days.\nWhen your data is stored in an online access tier (either Hot or Cool), users can access it immediately. The Hot tier is the best choice for data that is in active use, while the Cool tier is ideal for data that is accessed less frequently, but that still must be available for reading and writing.\n\nBox 3: Cool -\nAfter 365 days, the data will be accessed infrequently but must be available within five minutes.\nIncorrect:\nNot Archive:\nWhile a blob is in the Archive access tier, it's considered to be offline and can't be read or modified. In order to read or modify data in an archived blob, you must first rehydrate the blob to an online tier, either the Hot or Cool tier.\n\nRehydration priority -\nWhen you rehydrate a blob, you can set the priority for the rehydration operation via the optional x-ms-rehydrate-priority header on a Set Blob Tier or Copy Blob operation. Rehydration priority options include:\nStandard priority: The rehydration request will be processed in the order it was received and may take up to 15 hours.\nHigh priority: The rehydration request will be prioritized over standard priority requests and may complete in less than one hour for objects under 10 GB in size.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview https://docs.microsoft.com/en-us/azure/storage/blobs/archive-rehydrate-overview","answer_ET":"","exam_id":67,"discussion":[{"timestamp":"1674343080.0","upvote_count":"10","comment_id":"783811","content":"Hot, Cool, Cool is correct.\nRef: https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview","poster":"OdogwuSaina"},{"content":"First 30 Days: Use the Hot tier for frequent access and meeting the 99.9% availability SLA.\n\nAfter 90 Days: Shift to the Cool tier, suitable for infrequent access with availability within 30 seconds.\n\nAfter 365 Days: Transition to the Archive tier for rare access and longer retrieval time.","timestamp":"1702724940.0","upvote_count":"1","comments":[{"content":"Archive requires several hours to retrieve a file, so it would not be a good choice for \"After 365\" which need to have a 5 minute response time.","comment_id":"1259954","upvote_count":"2","poster":"7082935","timestamp":"1722617700.0"}],"comment_id":"1098097","poster":"AlfredPennyworth"},{"poster":"hassexat","upvote_count":"2","timestamp":"1694089920.0","content":"Hot\nCool\nCool","comment_id":"1001552"},{"poster":"kkk5566","comment_id":"994777","content":"Hot, Cool, Cool is correct.","timestamp":"1693459260.0","upvote_count":"1"},{"timestamp":"1673992320.0","comment_id":"779356","poster":"Sima_al","comments":[{"comment_id":"826554","timestamp":"1677740160.0","content":"Cool has a respone time of Milliseconds. So Hot, Cool, Cool","poster":"shoottheduck","upvote_count":"8"},{"poster":"kkk5566","content":"https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#summary-of-access-tier-options","timestamp":"1693459320.0","upvote_count":"1","comment_id":"994778"}],"content":"1. Hot - because of the 99.9% availability. \n2. Hot - because Cool tier needs several minutes to give back an answer (but 30 sec. is asked for).\n3. Cool - because the answer is needed within 5 minutes. Thats what cool tier does.","upvote_count":"2"},{"timestamp":"1663947720.0","comments":[{"poster":"Marcohcm","upvote_count":"4","timestamp":"1665466320.0","comment_id":"691771","content":"Cool Tier provides 99.9% availability only on RA-GRS. For ZRS, it should be 99% .\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#summary-of-access-tier-options"},{"poster":"hanzocuk","comment_id":"764917","timestamp":"1672769040.0","upvote_count":"4","content":"Keep this in mind --> \"The data will be accessed several times a day during the first 30 days\". Cool tier is more expensive to read from.\nhot, cool, cool looks correct."}],"content":"I think that 'cool' tier is just enough, it provides availability on 99.9%","comment_id":"677283","poster":"gabrysr1997","upvote_count":"2"},{"content":"Correct!","comment_id":"657282","upvote_count":"2","timestamp":"1662113640.0","poster":"Strix"}],"unix_timestamp":1662113640,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0033300001.jpg"],"isMC":false,"answer":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0033500001.jpg"],"question_id":284},{"id":"byUIX0ForAWq6U4Vuozl","timestamp":"2023-01-15 04:22:00","question_images":["https://img.examtopics.com/dp-203/image268.png"],"exam_id":67,"answer":"","answer_description":"","discussion":[{"upvote_count":"8","poster":"aemilka","comment_id":"874839","content":"Correct. \n\nAzure Data Lake Storage Gen2 implements an access control model that supports both Azure role-based access control (Azure RBAC) and POSIX-like access control lists (ACLs). \nAzure RBAC scope are storage accounts and containers.\nACL scope are directories and files.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control","timestamp":"1697728560.0"},{"timestamp":"1690549620.0","content":"Correct","upvote_count":"8","poster":"SannPro","comment_id":"790695"},{"upvote_count":"1","timestamp":"1724919480.0","comment_id":"1162421","content":"Correct!\nRBAC and ACL","poster":"Alongi"},{"comment_id":"1005497","content":"1. Role-based access control (RBAC) rules\n2. Access control lists (ACLs)","timestamp":"1710234780.0","poster":"EliteAllen","upvote_count":"2"},{"timestamp":"1709191560.0","content":"correct","upvote_count":"2","poster":"kkk5566","comment_id":"994782"},{"upvote_count":"4","timestamp":"1689384120.0","poster":"Venub28","comment_id":"776133","content":"Given answer is correct"}],"isMC":false,"answer_ET":"","unix_timestamp":1673752920,"question_id":285,"question_text":"DRAG DROP\n-\n\nYou have an Azure Data Lake Storage Gen 2 account named storage1.\n\nYou need to recommend a solution for accessing the content in storage1. The solution must meet the following requirements:\n\n• List and read permissions must be granted at the storage account level.\n• Additional permissions can be applied to individual objects in storage1.\n• Security principals from Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra, must be used for authentication.\n\nWhat should you use? To answer, drag the appropriate components to the correct requirements. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_images":["https://img.examtopics.com/dp-203/image269.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/95364-exam-dp-203-topic-3-question-33-discussion/","topic":"3","answers_community":[]}],"exam":{"lastUpdated":"12 Apr 2025","numberOfQuestions":384,"isImplemented":true,"isMCOnly":false,"id":67,"name":"DP-203","isBeta":false,"provider":"Microsoft"},"currentPage":57},"__N_SSP":true}