{"pageProps":{"questions":[{"id":"mCBoFScrU9FvIiIT2RIA","question_text":"You have an Azure SQL database named sqldb1.\nYou need to minimize the possibility of Query Store transitioning to a read-only state.\nWhat should you do?","answers_community":["B (59%)","C (41%)"],"unix_timestamp":1597128900,"question_images":[],"answer_images":[],"answer":"B","timestamp":"2020-08-11 08:55:00","topic":"3","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/28027-exam-dp-300-topic-3-question-1-discussion/","exam_id":68,"answer_ET":"B","choices":{"A":"Double the value of Data Flush interval","D":"Decrease by half the value of Statistics Collection interval","B":"Decrease by half the value of Data Flush Interval","C":"Double the value of Statistics Collection Interval"},"answer_description":"","question_id":151,"discussion":[{"content":"From my point of view, the correct answer is C. (Double the value of Statistics Collection Interval). To avoid Query Store transitioning to read-only state, the runtime statistics for the Last Stale Query Threshold days must fit in the defined Max Size (MB). If the Statistics Collection Interval is increased then less space will be used for the persisted runtime statistics and there will be more free space for new data. The Data Flush Interval can delay the transitioning to read-only state, but not minimize the possibility it occurs.","comments":[{"timestamp":"1740696240.0","content":"Disagree.\nIt seems logical that doubling Statistics Collection Interval will make Query Store consume less storage. BUT - when you try to run this:\n\nALTER DATABASE [DB1] \nSET QUERY_STORE (INTERVAL_LENGTH_MINUTES = 120)\nGO\n\nIt fails with error:\nMsg 153, Level 16, State 6, Line 1\nInvalid usage of the option interval_length_minutes in the ALTER DATABASE statement.\n\nThe GUI doesn't have selection of 2 hours, too. \nYou can only set it to 30 minutes (or less), or to 1 day 1440 minutes.\nSo C is not the right answer.","upvote_count":"1","poster":"voodoo_sh","comment_id":"1362756"},{"poster":"Luke97","upvote_count":"3","comments":[{"content":"You're right - the only sensible answer is \"C\".","upvote_count":"1","comment_id":"313199","poster":"Raffer","timestamp":"1647518400.0"}],"timestamp":"1638913380.0","comment_id":"237660","content":"Agree. This should be the correct answer. I don't see how DFI make any influence to Max Size."}],"timestamp":"1633374840.0","upvote_count":"33","poster":"miguelmol","comment_id":"193151"},{"content":"in microsoft prep test ,this question says B. Decrease Data Flush Interval.","upvote_count":"11","comment_id":"469460","poster":"matongax","timestamp":"1666986480.0","comments":[{"upvote_count":"1","poster":"KingChuang","comment_id":"794870","content":"B is correct.\n\nWhile Query Store collects queries, execution plans, and statistics, its size in the database grows until this limit is reached. When that happens, Query Store automatically changes the operation mode to READ_ONLY and stops collecting new data, which means that your performance analysis is no longer accurate.\n\nMAX_STORAGE_SIZE_MB limit isn't strictly enforced. Storage size is checked only when Query Store writes data to disk. This interval is set by the DATA_FLUSH_INTERVAL_SECONDS option or the Management Studio Query Store dialog option Data Flush Interval.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/performance/manage-the-query-store?view=sql-server-ver16&tabs=ssms","timestamp":"1706766600.0"}]},{"timestamp":"1740696540.0","poster":"voodoo_sh","comment_id":"1362758","content":"Selected Answer: B\nB: Decrease by half the value of Data Flush interval --> to prevent in-memory max size.\n\nWhile Statistics Collection Interval has direct impact on Query Store size storage wise, you can't double the Statistics collection interval. GUI doesn't allow that, and setting INTERVAL_LENGTH_MINUTES to 120 in T-SQL also fails.","upvote_count":"1"},{"upvote_count":"1","comment_id":"1076738","content":"Selected Answer: B\nB is right answer","poster":"k6745","timestamp":"1732225200.0"},{"upvote_count":"3","poster":"ofzrgrz","timestamp":"1725844020.0","comment_id":"1002797","content":"It's B. Decreasing Data Flush is the main way to keep the Query Store outside of read-only, prevent in-memory max size, and when it writes to disk, can delete stale data if enabled. Data Flush can be halved/doubled with ease. Statistics Collection Interval cannot be doubled as easily."},{"content":"Selected Answer: C\nC is correct","comments":[{"upvote_count":"2","poster":"jtu363","content":"Please disregard comment","comment_id":"776147","timestamp":"1705291020.0"}],"timestamp":"1705290780.0","poster":"jtu363","comment_id":"776145","upvote_count":"2"},{"content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/sql/relational-databases/performance/manage-the-query-store?view=sql-server-ver16&tabs=ssms\n\nStale Query Threshold (Days): Time-based cleanup policy that controls the retention period of persisted runtime statistics and inactive queries, expressed in days. By default, Query Store is configured to keep the data for 30 days, which might be unnecessarily long for your scenario.\n\nAvoid keeping historical data that you don't plan to use. This practice reduces changes to read-only status. The size of Query Store data and the time to detect and mitigate the issue will be more predictable. Use Management Studio or the following script to configure time-based cleanup policy:\n\nSQL\n\nCopy\nALTER DATABASE [QueryStoreDB]\nSET QUERY_STORE (CLEANUP_POLICY = (STALE_QUERY_THRESHOLD_DAYS = 90));","timestamp":"1704230520.0","poster":"vittOS","upvote_count":"5","comment_id":"764008"},{"content":"B combined with SIZE_BASED_CLEANUP_MODE\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/performance/manage-the-query-store?view=sql-server-ver16&tabs=ssms#query-store-maximum-size","comment_id":"752647","timestamp":"1703184300.0","upvote_count":"1","poster":"OneplusOne"},{"timestamp":"1689534060.0","comment_id":"632332","upvote_count":"3","poster":"Backy","content":"Selected Answer: C\nC decreases the amount of collected data so less chances of reaching the limit"},{"poster":"eric0718","timestamp":"1682786280.0","comment_id":"594598","upvote_count":"3","content":"Selected Answer: B\nThe Max Size (MB) limit isn't strictly enforced. Storage size is checked only when Query Store writes data to disk. This interval is set by the Data Flush Interval (Minutes) option. If Query Store has breached the maximum size limit between storage size checks, it transitions to read-only mode. If Size Based Cleanup Mode is enabled, the cleanup mechanism to enforce the maximum size limit is also triggered."},{"timestamp":"1678715580.0","content":"Selected Answer: C\nC is correct","comment_id":"566885","upvote_count":"3","poster":"annaandalex2021"},{"poster":"thedatadetective","timestamp":"1674655140.0","comments":[{"comment_id":"579153","upvote_count":"5","content":"After reading through that, I think B.","poster":"cusman","timestamp":"1680294360.0"}],"upvote_count":"6","content":"Selected Answer: B\nThis clears up the confusion in MS documentation...it specifically states that Flush parameters are what controls Max DB store https://docs.microsoft.com/en-us/sql/relational-databases/performance/how-query-store-collects-data?view=sql-server-ver15#remarks","comment_id":"532172"},{"timestamp":"1673077140.0","comment_id":"518820","poster":"arrakis968","upvote_count":"1","content":"You can't really double or halve the Statistics Collection interval as that is a fixed value with a list of allowed values, see: sys.database_query_store_options You can't halve 15 minutes and can't double 1 hour. Flush Interval can be set to any arbitrary value above 1 minute. Makes sense to have the size checked more often. Tricky but very bad question."},{"comment_id":"512735","upvote_count":"3","poster":"CaptainJameson","timestamp":"1672347720.0","content":"Selected Answer: C\nCorrect answer is basically outlined in the explanation, only marked as incorrect ðŸ¤·â€â™‚ï¸"},{"upvote_count":"2","poster":"cura","comment_id":"481035","content":"Correct answer is B","timestamp":"1668805020.0"},{"content":"Answer is C according to a lot of forums exponing this question....","timestamp":"1666026540.0","comment_id":"463637","poster":"cura","upvote_count":"1"},{"comment_id":"422252","content":"C should be the correct answer","timestamp":"1660065840.0","upvote_count":"1","poster":"erssiws"},{"comment_id":"421804","poster":"learnazureportal","timestamp":"1659987780.0","upvote_count":"3","content":"The correct Answer is B. Just check the Query Store - database Properties and you can find in read-only state the option B is valid. for option C , after one hour we have ONE day (cannot add the custom value) . it means, we cannot double it. it would be X 24 more."},{"poster":"examtry","comment_id":"381036","upvote_count":"2","timestamp":"1655117220.0","content":"C: is the correct answer."},{"content":"Could be B as well...decreasing the flush interval would be mean...data would be written more frequently to the disk and more frequently it would check the size limit of query store; and more frequently it would be able to cleanup the query store (considering by default size based cleanup is enabled) which means more chances to avoid query store turning into read-only state","poster":"pm4certs","upvote_count":"2","comment_id":"333365","timestamp":"1649688420.0"},{"comment_id":"315647","upvote_count":"1","content":"Answer: C\n\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/best-practice-with-the-query-store","poster":"rmn900","timestamp":"1647783300.0"},{"timestamp":"1644355560.0","comment_id":"286463","content":"it should be C logically","poster":"Billhardy","upvote_count":"2"},{"comment_id":"281242","timestamp":"1643721000.0","upvote_count":"2","poster":"QiangQiang","content":"it should be C"},{"upvote_count":"3","comments":[{"timestamp":"1642989060.0","content":"Wouldn't that make the right answer C? By doubling the interval time, you lose granularity but save space. If you double the flush interval, I don't think that saves space although it could still help by triggering a cleanup. Either way, again a poor question as I wouldn't solve the problem without first considering the other QS settings/size.","comments":[{"upvote_count":"1","content":"correct","timestamp":"1644012780.0","poster":"QiangQiang","comment_id":"283739"}],"upvote_count":"2","comment_id":"274944","poster":"bnc"}],"content":"The answer should be A. If you decrease the data flush interval you are writing to disk more often. To accomplish the objective here we need to write to disk LESS often.","comment_id":"264265","timestamp":"1641847380.0","poster":"JohnCrawford"},{"poster":"BurhanUddin","timestamp":"1641693720.0","comment_id":"262943","upvote_count":"1","content":"So, B is the right answer?"},{"timestamp":"1641514860.0","upvote_count":"1","comment_id":"261375","poster":"JohnCrawford","content":"NKnab is correct, but the answer is backwards in its thinking. If you decrease the data flush INTERVAL you would be going from 15 minutes to 7 1/2 minutes. Instead you want to increase the time between flushes on the offchance that will allow enough stale information to be flushed that you don't hit the max size limit. Question should be focused on stale query threshold and auto cleanup instead. Very poorly written question."},{"upvote_count":"1","content":"What is the correct answer here?","comment_id":"260668","timestamp":"1641432780.0","poster":"BurhanUddin"},{"poster":"MediMedi","comment_id":"207296","content":"https://docs.microsoft.com/en-us/sql/relational-databases/performance/best-practice-with-the-query-store?view=sql-server-ver15#:~:text=If%20Query%20Store%20has%20breached,size%20limit%20is%20also%20triggered\nStatistics Collection Interval: Defines the level of granularity for the collected runtime statistic, expressed in minutes. The default is 60 minutes. Consider using a lower value if you require finer granularity or less time to detect and mitigate issues. Keep in mind that the value directly affects the size of Query Store data.","upvote_count":"2","timestamp":"1635359520.0"},{"upvote_count":"2","content":"Data Flush Interval (Minutes): It defines the frequency to persist collected runtime statistics to disk. It's expressed in minutes in the graphical user interface (GUI), but in Transact-SQL it's expressed in seconds. The default is 900 seconds, which is 15 minutes in the graphical user interface. Consider using a higher value if your workload doesn't generate a large number of different queries and plans, or if you can withstand longer time to persist data before a database shutdown.","poster":"Zhishan","comment_id":"179827","timestamp":"1631704980.0"},{"upvote_count":"2","comments":[{"content":"so in your opinion, the answer given is correct?","comments":[{"poster":"U_C","timestamp":"1647911940.0","content":"Yes, the answer is correct.\nStorage size is checked only when Query Store writes data to disk. This interval is set by the Data Flush Interval (Minutes) option.","comment_id":"316770","upvote_count":"1"}],"comment_id":"222858","poster":"mrsmjparker","timestamp":"1637333220.0","upvote_count":"1"}],"poster":"NKnab","comment_id":"155180","timestamp":"1628664900.0","content":"Storage size is checked only when Query Store writes data to disk. This interval is set by the Data Flush Interval (Minutes) option. If Query Store has breached the maximum size limit between storage size checks, it transitions to read-only mode"}]},{"id":"OP71w7bSWLvgftvQqJVC","answer_images":[],"answer_ET":"B","topic":"3","choices":{"C":"Azure HDInsight","B":"Azure Databricks","A":"Azure Data Lake Storage","D":"Azure Data Factory"},"discussion":[{"content":"Azure Databricks is correct. Minimize admon effort is a key word.","upvote_count":"10","comment_id":"476234","timestamp":"1668173880.0","comments":[{"comment_id":"525857","content":"Check this out \n\n\n\nAzure HDInsight offers pre-made, monitoring dashboards in the form of solutions that can be used to monitor the workloads running on your clusters. There are solutions for Apache Spark, Hadoop, Apache Kafka, live long and process (LLAP), Apache HBase, and Apache Storm available in the Azure Marketplace. Please see our documentation to learn how to install a monitoring solution. These solutions are workload-specific, allowing you to monitor metrics like central processing unit (CPU) time, available YARN memory, and logical disk writes across multiple clusters of a given type. Selecting a graph takes you to the query used to generate it, shown in the logs view.","timestamp":"1673966760.0","poster":"VinayakBudapanahalli","upvote_count":"5"}],"poster":"ss1516"},{"comment_id":"525856","poster":"VinayakBudapanahalli","timestamp":"1673966700.0","upvote_count":"5","content":"Selected Answer: C\nHDInsight monitoring solutions\nAzure HDInsight offers pre-made, monitoring dashboards in the form of solutions that can be used to monitor the workloads running on your clusters. There are solutions for Apache Spark, Hadoop, Apache Kafka, live long and process (LLAP), Apache HBase, and Apache Storm available in the Azure Marketplace. Please see our documentation to learn how to install a monitoring solution. These solutions are workload-specific, allowing you to monitor metrics like central processing unit (CPU) time, available YARN memory, and logical disk writes across multiple clusters of a given type. Selecting a graph takes you to the query used to generate it, shown in the logs view."},{"timestamp":"1725846120.0","poster":"ofzrgrz","content":"DP-203 question, you can skip.","upvote_count":"2","comment_id":"1002814"},{"content":"Azure DataBricks are not part of the DP-300 exam.","upvote_count":"2","timestamp":"1698514200.0","poster":"Ciupaz","comment_id":"706664"},{"poster":"GigaCaster","content":"Believe the answer is correct as they state that we need to analyze the already acquired Intrusion detection data and not get it","comment_id":"623955","timestamp":"1687954560.0","upvote_count":"2"},{"comment_id":"594809","timestamp":"1682822160.0","upvote_count":"2","content":"Selected Answer: B\nIntrusion detection is needed to monitor network or system activities for malicious activities or policy violations and produces electronic reports to a management station.\nIntrusion detection is needed to monitor network or system activities for malicious activities or policy violations and produces electronic reports to a management station.","poster":"eric0718"},{"content":"I believe this is a DP-203 question and outside the scope of DP-300","comment_id":"579433","poster":"cusman","timestamp":"1680346800.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1678707000.0","poster":"reachmymind","comment_id":"566756","content":"Selected Answer: B\nB. Azure Databricks\n\nThe key point is \"The solution must minimize administrative efforts.\""},{"poster":"reachmymind","timestamp":"1678706940.0","upvote_count":"2","comment_id":"566755","content":"Selected Answer: B\nB. Azure Databricks\n\nhttps://pages.databricks.com/rs/094-YMS-629/images/FY18_AA_Databricks_e-book_FINAL_032018.pdf\n\nStep 1: Ingest Intrusion detection system Data to Notebook\nStep 2: Enrich the Data to Get Additional Insights to IDS Dataset\nStep 3: Explore IDS Data by Capturing the Type of Attacks on the Network\nStep 4: Visualization\nStep 5: Model Creation"},{"upvote_count":"2","comment_id":"562919","content":"Selected Answer: B\nAzure Databricks\n\nhttps://azure.microsoft.com/es-es/blog/three-critical-analytics-use-cases-with-microsoft-azure-databricks/","poster":"yster","timestamp":"1678229340.0"},{"comment_id":"477775","upvote_count":"2","poster":"o2091","content":"Is B correct?","timestamp":"1668375300.0"}],"unix_timestamp":1636637880,"timestamp":"2021-11-11 14:38:00","question_text":"A company plans to use Apache Spark analytics to analyze intrusion detection data.\nYou need to recommend a solution to analyze network and system activity data for malicious activities and policy violations. The solution must minimize administrative efforts.\nWhat should you recommend?","question_images":[],"question_id":152,"isMC":true,"answers_community":["B (62%)","C (38%)"],"answer_description":"","answer":"B","exam_id":68,"url":"https://www.examtopics.com/discussions/microsoft/view/65833-exam-dp-300-topic-3-question-10-discussion/"},{"id":"OPkU6qjmwF9v24BdYI7k","question_id":153,"url":"https://www.examtopics.com/discussions/microsoft/view/64088-exam-dp-300-topic-3-question-11-discussion/","timestamp":"2021-10-14 15:30:00","question_text":"DRAG DROP -\nYour company analyzes images from security cameras and sends alerts to security teams that respond to unusual activity. The solution uses Azure Databricks.\nYou need to send Apache Spark level events, Spark Structured Streaming metrics, and application metrics to Azure Monitor.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions in the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0015800001.png"],"isMC":false,"answers_community":[],"answer":"","unix_timestamp":1634218200,"question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0015700001.png"],"answer_ET":"","topic":"3","exam_id":68,"answer_description":"Send application metrics using Dropwizard.\nSpark uses a configurable metrics system based on the Dropwizard Metrics Library.\nTo send application metrics from Azure Databricks application code to Azure Monitor, follow these steps:\nStep 1: Configure your Azure Databricks cluster to use the Databricksmonitoring library.\nPrerequisite: Configure your Azure Databricks cluster to use the monitoring library.\nStep 2: Build the spark-listeners-loganalytics-1.0-SNAPSHOT.jar JAR file\nStep 3: Create Dropwizard counters in your application code\nCreate Dropwizard gauges or counters in your application code\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs","discussion":[{"upvote_count":"6","comment_id":"462099","content":"is his related to DP-300?","poster":"SamBalbij","comments":[{"poster":"Sr18","upvote_count":"1","content":"Its DP-203, but answers are correct","timestamp":"1719593760.0","comment_id":"1238800"},{"poster":"cusman","content":"I believe this is outside the scope of DP-300 and belongs to DP-203","timestamp":"1648810920.0","comment_id":"579434","upvote_count":"6"},{"comment_id":"477780","timestamp":"1636839420.0","content":"looks DP-300, what do you think?","poster":"o2091","upvote_count":"1"}],"timestamp":"1634218200.0"},{"poster":"ofzrgrz","content":"DP-203 question, you can skip.","upvote_count":"1","comment_id":"1002815","timestamp":"1694223720.0"},{"content":"Azure DataBricks are out of scope of the DP-300 exam.","upvote_count":"2","comment_id":"706224","timestamp":"1666940760.0","poster":"Ciupaz"},{"content":"Answer is correct.\nhttps://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs","timestamp":"1651321800.0","comment_id":"594996","poster":"eric0718","upvote_count":"1"},{"content":"-> Configure the Databricks cluster to use the Databricks monitoring library.\n-> Build a spark-listner-lognalytics-1.0-SNAPSHOT.jar JAR file.\n-> Create Dropwizard counters in the application code.\n\nhttps://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs","timestamp":"1647171840.0","comment_id":"566772","poster":"reachmymind","upvote_count":"2"}]},{"id":"6WDPokreszDxu4341dGU","unix_timestamp":1636839480,"answer_ET":"C","answer_images":[],"question_images":[],"question_id":154,"answer":"C","isMC":true,"exam_id":68,"url":"https://www.examtopics.com/discussions/microsoft/view/65976-exam-dp-300-topic-3-question-12-discussion/","answers_community":[],"topic":"3","question_text":"You have an Azure data solution that contains an enterprise data warehouse in Azure Synapse Analytics named DW1.\nSeveral users execute adhoc queries to DW1 concurrently.\nYou regularly perform automated data loads to DW1.\nYou need to ensure that the automated data loads have enough memory available to complete quickly and successfully when the adhoc queries run.\nWhat should you do?","timestamp":"2021-11-13 22:38:00","answer_description":"The performance capacity of a query is determined by the user's resource class.\nSmaller resource classes reduce the maximum memory per query, but increase concurrency.\nLarger resource classes increase the maximum memory per query, but reduce concurrency.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/resource-classes-for-workload-management","discussion":[{"timestamp":"1725846120.0","comment_id":"1002816","upvote_count":"2","content":"DP-203 question, you can skip.","poster":"ofzrgrz"},{"comment_id":"697220","timestamp":"1697534520.0","poster":"Ciupaz","content":"Synapse Analytics is not part of the DP-300 exam.","upvote_count":"2"},{"timestamp":"1680346980.0","comment_id":"579435","content":"I believe this is outside the scope of DP-300 and belongs to DP-203","poster":"cusman","upvote_count":"2"},{"timestamp":"1668375480.0","comment_id":"477784","content":"Answer looks correct","poster":"o2091","upvote_count":"3"}],"choices":{"B":"Create sampled statistics to every column in each table of DW1.","C":"Assign a larger resource class to the automated data load queries.","A":"Assign a smaller resource class to the automated data load queries.","D":"Hash distribute the large fact tables in DW1 before performing the automated data loads."}},{"id":"eEWS2sRlDhyvKzSrGamy","unix_timestamp":1636839480,"answer_ET":"D","question_images":[],"answer_images":[],"question_id":155,"answer":"D","isMC":true,"exam_id":68,"url":"https://www.examtopics.com/discussions/microsoft/view/65977-exam-dp-300-topic-3-question-13-discussion/","topic":"3","answers_community":[],"question_text":"You are monitoring an Azure Stream Analytics job.\nYou discover that the Backlogged input Events metric is increasing slowly and is consistently non-zero.\nYou need to ensure that the job can handle all the events.\nWhat should you do?","timestamp":"2021-11-13 22:38:00","answer_description":"Backlogged Input Events: Number of input events that are backlogged. A non-zero value for this metric implies that your job isn't able to keep up with the number of incoming events. If this value is slowly increasing or consistently non-zero, you should scale out your job, by increasing the SUs.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-monitoring","choices":{"C":"Create an additional output stream for the existing input stream.","A":"Remove any named consumer groups from the connection and use $default.","B":"Change the compatibility level of the Stream Analytics job.","D":"Increase the number of streaming units (SUs)."},"discussion":[{"content":"DP-203","upvote_count":"7","poster":"cusman","timestamp":"1680346980.0","comment_id":"579436"},{"timestamp":"1725846120.0","poster":"ofzrgrz","comment_id":"1002817","content":"DP-203 question, you can skip.","upvote_count":"1"},{"content":"Azure Stream Analytics is out-of-scope of the DP-300 exam.","upvote_count":"2","timestamp":"1701359640.0","poster":"Ciupaz","comment_id":"731704"},{"poster":"o2091","timestamp":"1668375480.0","comment_id":"477785","content":"looks correct","upvote_count":"3"}]}],"exam":{"id":68,"numberOfQuestions":360,"isImplemented":true,"isBeta":false,"lastUpdated":"12 Apr 2025","isMCOnly":false,"name":"DP-300","provider":"Microsoft"},"currentPage":31},"__N_SSP":true}