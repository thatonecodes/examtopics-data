{"pageProps":{"questions":[{"id":"kixcJOcg2ZeGzpWPcWKO","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0002800001.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0002900001.png"],"question_text":"HOTSPOT -\nYou use Azure Data Factory to prepare data to be queried by Azure Synapse Analytics serverless SQL pools.\nFiles are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company.\nYou need to move the files to a different folder and transform the data to meet the following requirements:\n✑ Provide the fastest possible query times.\n✑ Automatically infer the schema from the underlying files.\nHow should you configure the Data Factory copy activity? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_ET":"","discussion":[{"poster":"alain2","comments":[{"content":"just want to add a bit more reference regarding copyBehavior in ADF plus info mentioned in Best Practice doc, so it shall be MergeFile first. \nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#file-system-as-sink","comment_id":"509170","timestamp":"1640447160.0","upvote_count":"12","poster":"edba"},{"timestamp":"1621975800.0","content":"The smaller the files, the negative the performance so Merge and Parquet seems to be the right answer.","poster":"Ameenymous","upvote_count":"25","comment_id":"366671"},{"upvote_count":"10","comment_id":"464446","timestamp":"1634617140.0","content":"Larger files lead to better performance and reduced costs.\n\nTypically, analytics engines such as HDInsight have a per-file overhead that involves tasks such as listing, checking access, and performing various metadata operations. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256 MB to 100 GB in size). S","poster":"kilowd"}],"comment_id":"360178","content":"1. Merge Files\n2. Parquet\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance","upvote_count":"171","timestamp":"1726814640.0"},{"content":"It should be \n1)Merge Files - Question clearly says \"initially ingested as 10 small json files\". There is no hint on hierarchy or partition information. so clearly we need to merge these files for better performance\n2) Parquet -> Always gives better performance for columnar based data","upvote_count":"12","timestamp":"1622525940.0","poster":"ThiruthuvaRajan","comment_id":"371590"},{"timestamp":"1742727540.0","content":"Copy Behavior → Merge Files\n\nSince there are 10 small JSON files, merging them into a single, optimized file will improve query performance.\n\nAzure Synapse Analytics serverless SQL pools perform better with fewer, larger files than many small files.\n\nFlatten hierarchy is not suitable since we are not restructuring directories.\n\nPreserve hierarchy is not needed because we want better query performance, not maintaining the folder structure.\n\nSink File Type → Parquet\n\nParquet is an efficient columnar format, designed for fast analytical queries.\n\nIt supports schema inference, which is required for querying in Synapse serverless SQL pools.\n\nCSV and JSON are not optimal because they do not support columnar storage and would result in slower queries.\n\nTXT is not structured and would not allow schema inference.\n\nThus, the best selections are \"Merge files\" for Copy behavior and \"Parquet\" for Sink file type to achieve the fastest query times","comment_id":"1402234","poster":"Sathya_sree","upvote_count":"1"},{"comment_id":"1356823","timestamp":"1739617500.0","upvote_count":"1","content":"Azure Synapse serverless SQL pools perform best with fewer, larger files in Parquet format -\nso 1-Merge Files\n2- Parquet","poster":"Pey1nkh"},{"timestamp":"1739505060.0","comment_id":"1356356","upvote_count":"1","content":"Q says \"You need to move the files to a different folder\".\n\nso preserve for sure","poster":"KowshikKvt"},{"content":"1. Copy Behavior: merge files\n2. Sink file type: Paquet","comment_id":"1340674","poster":"krishna1303","upvote_count":"1","timestamp":"1736916180.0"},{"comment_id":"1303818","upvote_count":"1","poster":"Karthikyn","timestamp":"1730090100.0","content":"1. Preserve Hierarchy \n2. Parquet\nSince Merge Files behavior combines multiple files into a single output file. While this can be useful for reducing the number of files, it may introduce additional performance overhead due to the merging process. Since it ask to provide the fastest possible query time."},{"content":"This question is somewhat confusing. The detail states \"Each file contains the same data attributes and data from a subsidiary of your company\". There is a possibility that each file is for a different subsidiary and would be stored in a different hierarchical folder, so the requirement to \"preserve hierarchy\" could be the right answer here.","comment_id":"1257578","timestamp":"1722269460.0","upvote_count":"2","poster":"7082935"},{"content":"Dear Community,\n\nI would like to express my heartfelt gratitude for the thoughtful mock questions that have been shared. Your generosity in providing these valuable resources has been immensely helpful. As we engage in discussions and learn together, I am reminded of the strength and camaraderie that exists within our community.\n\nTo everyone who has contributed, whether by creating questions, participating in discussions, or simply offering encouragement, thank you. Your collective efforts make this community a vibrant and supportive place for learning and growth.\n\nLet us continue to share knowledge, support one another, and celebrate our shared passion for learning","poster":"Bakachi55","comment_id":"1173324","timestamp":"1710414840.0","upvote_count":"5"},{"comment_id":"1156324","upvote_count":"1","content":"https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory","poster":"waghsvw53","timestamp":"1708601340.0"},{"content":"1. Flatten hierarchy - Performance: Fastest; Schema inference: Straightforward;\n2. Parquet","upvote_count":"2","poster":"Azure_2023","comment_id":"1149098","timestamp":"1707820920.0"},{"content":"Merge Files: This option combines the 10 JSON files into a single Parquet file, reducing overhead and improving query performance significantly.\nParquet: This columnar format is optimized for fast queries, especially when dealing with large datasets and selective column reads. It also supports compression and schema inference.","comment_id":"1101731","upvote_count":"4","timestamp":"1703089200.0","poster":"lisa710"},{"poster":"Chemmangat","upvote_count":"2","comment_id":"1008078","timestamp":"1694748060.0","content":"My answer : Merge\nSince there is no mention of preserving the hierarchy, and the need is to make the process more efficient, merge is the way to go."},{"content":"MERGE FILES since you need to make transformation and data have the same attributes\nPARQUET because is the most effcient file format","timestamp":"1694065740.0","comment_id":"1001211","upvote_count":"2","poster":"hassexat"},{"comment_id":"993131","timestamp":"1693315260.0","poster":"kkk5566","content":"- Merge - Parquet","upvote_count":"2"},{"upvote_count":"5","content":"ChatGPT confirms it's 1. Merge, 2. Parquet","comment_id":"980790","poster":"ladistar","timestamp":"1692017460.0"},{"upvote_count":"5","poster":"rocky48","comment_id":"879914","timestamp":"1682395440.0","content":"1. Merge Files\n2. Parquet\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance"},{"comment_id":"854335","content":"Hey, the answer here should be \"Merge Files\" and \"Parquet\".\n\nThe question said nothing about hierarchies.","upvote_count":"2","poster":"Honour","timestamp":"1680090060.0"},{"comment_id":"802230","timestamp":"1675872840.0","upvote_count":"1","poster":"bubby248","content":"Merge small files will be best for fast retreiving. Parquet for sink file type"},{"comment_id":"801825","timestamp":"1675848000.0","content":"Either preserving or flattening hierarchy has little to no performance overhead, whereas merging files causes additional performance overhead. It's perserve","poster":"INDEAVR","upvote_count":"1"},{"comment_id":"746807","poster":"vigilante89","upvote_count":"5","timestamp":"1671165240.0","content":"Copy Behaviour: MERGE FILES\nBecause the small files already have same data attributes i.e. same schema. So merging all the data into one single file and converting the file to parquet makes more sense to make the query time, space and cost efficient.\n\nSink/Destination File Type: PARQUET\nThis is a no-brainer because parquet is the most efficient file format in this case in terms of time, space and cost efficiency."},{"content":"I think \"Automatically infer the schema from the underlying files\" means we should keep the same hierarchy and not merge all the data into a single file. So I would say that the first one is Preserve Hiearchy.","comment_id":"714386","comments":[{"poster":"ItsPayakan","upvote_count":"1","content":"Automatically infer the schema from underlying files is between b/w ADLS and Serverless sql pool. And not between JSON and parquet.correct?","timestamp":"1726678080.0","comment_id":"1285920"},{"timestamp":"1674615720.0","content":"If you preserve the hierarchy, you will keep them in small files. This will affect the performance negatively. That is why Merge is better","comment_id":"787233","upvote_count":"1","poster":"AhmedDaffaie"},{"upvote_count":"1","poster":"OldSchool","content":"Q:\"Files are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company.\" \nSince all have same data and attributes we can Merge them in one file and automatically infer the schema from the underlying 10 small files.\n- Merge\n- Parquet","comment_id":"720391","timestamp":"1668680700.0"}],"poster":"Selma97","timestamp":"1667982180.0","upvote_count":"2"},{"content":"1. Merge Files\n2. Parquet","poster":"Deeksha1234","comment_id":"646414","timestamp":"1660410000.0","upvote_count":"3"},{"poster":"Fiddi","content":"I would say Merge might not be possible, because we do not know if the Json contains enough information about the subsidiary, which we would need if we merge.","comment_id":"634470","timestamp":"1658390760.0","upvote_count":"2"},{"content":"The first answer should be Merge Files.","timestamp":"1656984720.0","comment_id":"627182","poster":"Ajitk27","upvote_count":"1"},{"poster":"azure900test","comment_id":"625031","timestamp":"1656565140.0","upvote_count":"2","content":"1. Merge Files\n2. JSON\nDecompression requires compute and thus costs time. Since the files are small, leaving them uncompressed is ok since it will allow faster query. Also they are JSON files so compressing them in a columnar format does not make much sense."},{"timestamp":"1652266500.0","content":"1. Merge Files\n2. Parquet","upvote_count":"2","comment_id":"600064","poster":"Dothy"},{"poster":"KashRaynardMorse","comment_id":"585283","upvote_count":"3","timestamp":"1649863680.0","comments":[{"comments":[{"timestamp":"1655278800.0","comment_id":"616604","content":"gabdu why are you trying to create a different use case? it's clearly mentioned that the data attributes are same in all the files. Answer is merge, don't confuse others looking for answers here","upvote_count":"6","poster":"Aditya0891"}],"comment_id":"595895","poster":"gabdu","timestamp":"1651470720.0","content":"it is possible that all or some schemas may be different in that case we cannot merge","upvote_count":"1"}],"content":"A requirement was \"Automatically infer the schema from the underlying files\", meaning Preserve hierarchy is needed."},{"comment_id":"578143","content":"another hot key is : You need to move the files to a different folder\n\nso answer should be preserve hierarchy.","timestamp":"1648628580.0","comments":[{"upvote_count":"1","content":"But the data attributes are same in all the files","timestamp":"1661077500.0","poster":"adizzz54","comment_id":"649711"}],"upvote_count":"4","poster":"imomins"},{"comment_id":"575464","content":"1. Preserve heirarchy - ADF is used only for processing and Synapse is the sink. Since synapse has parallel processing power, it can process the files in different folder and thus improve performance.\n2. Parquet","upvote_count":"1","comments":[{"poster":"uzairahm","timestamp":"1656130260.0","comment_id":"621945","content":"The question says: \"Provide the fastest possible query times.\" not the write times and there is no in directory there are just 10 json files so combining them into one file would be most appropriate as larger file size increase performance read in link\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#file-size","upvote_count":"1"}],"timestamp":"1648289520.0","poster":"Eyepatch993"},{"content":"\"In general, we recommend that your system have some sort of process to aggregate small files into larger ones for use by downstream applications.\"\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices\n\nTherefore I believe the answer should be 'Merge files' and 'Parquet'.","timestamp":"1642985220.0","poster":"srakrn","comment_id":"530942","upvote_count":"4"},{"timestamp":"1641712500.0","poster":"Sandip4u","comment_id":"519996","content":"Merge and parquet will be the right option , also taken reference from Udemy","upvote_count":"2"},{"comment_id":"509700","comments":[{"poster":"Boompiee","upvote_count":"2","comment_id":"598532","timestamp":"1652010000.0","content":"The overhead for merging happens once, after that it's faster every time to query the files if they are merged."},{"upvote_count":"1","timestamp":"1656130320.0","poster":"uzairahm","comment_id":"621947","content":"The question says: \"Provide the fastest possible query times.\" not the write times and there is no in directory there are just 10 json files so combining them into one file would be most appropriate as larger file size increase performance read in link\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#file-size"}],"poster":"Mahesh_mm","content":"1.As hierarchical namespace greatly improves the performance of directory management operations, which improves overall job performance, Preserver herarchy looks correct. Also there is overhead for merging files. \n2. Parquet","upvote_count":"3","timestamp":"1640535780.0"},{"content":"Merge Files and Parquet","poster":"m2shines","timestamp":"1639507080.0","comment_id":"501605","upvote_count":"1"},{"content":"shouldn't a json file be flattened first? So I think the answer is: flatten and parquet","upvote_count":"1","poster":"AM1971","timestamp":"1636549440.0","comment_id":"475418","comments":[{"timestamp":"1654510920.0","comment_id":"612294","content":"they didn't mention if the json files contains any array or something so flatten is not the correct answer I guess","poster":"Aditya0891","upvote_count":"1"}]},{"comments":[{"comments":[{"content":"No, The question says: \"Provide the fastest possible query times.\" not the write times and there is no in directory there are just 10 json files so combining them into one file would be most appropriate as larger file size increase performance read in link\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#file-size","poster":"uzairahm","timestamp":"1656130380.0","comment_id":"621948","upvote_count":"1"}],"timestamp":"1633708500.0","comment_id":"459295","content":"Is this correct?","upvote_count":"2","poster":"noobplayer"}],"upvote_count":"1","comment_id":"455880","content":"1. Preserver hierarchy\n2. Parquet","timestamp":"1633153860.0","poster":"RinkiiiiiV"},{"timestamp":"1631119860.0","poster":"Marcus1612","comment_id":"441551","content":"The files are copied/transform from one folder to another inside the same hierarchical account. The hierarchical property is defined during the account creation. The destination folder still have the hierarchical. On the other hand, as mentioned by Microsoft:Typically, analytics engines such as HDInsight and Azure Data Lake Analytics have a per-file overhead. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256MB to 100GB in size).","upvote_count":"4"},{"upvote_count":"2","timestamp":"1629825900.0","poster":"meetj","comment_id":"430931","content":"1. Merge for sure\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-file-system, clearly defined several behavior"},{"upvote_count":"5","content":"1. Merge Files: Because the question said 10 different small JSON to a different file\n2. Parquet","timestamp":"1626869880.0","poster":"elimey","comment_id":"410897"},{"content":"Box 1: Preserver herarchy\nCompared to the flat namespace on Blob storage, the hierarchical namespace greatly improves the performance of directory management operations, which\nimproves overall job performance.\nBox 2: Parquet\nAzure Data Factory parquet format is supported for Azure Data Lake Storage Gen2. Parquet supports the schema property.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction https://docs.microsoft.com/en-us/azure/data-factory/format-parquet","timestamp":"1624900140.0","comment_id":"393112","upvote_count":"2","poster":"Erte"}],"unix_timestamp":1621320900,"timestamp":"2021-05-18 08:55:00","url":"https://www.examtopics.com/discussions/microsoft/view/53010-exam-dp-203-topic-1-question-8-discussion/","exam_id":67,"answer":"","isMC":false,"answer_description":"Box 1: Preserver hierarchy -\nCompared to the flat namespace on Blob storage, the hierarchical namespace greatly improves the performance of directory management operations, which improves overall job performance.\n\nBox 2: Parquet -\nAzure Data Factory parquet format is supported for Azure Data Lake Storage Gen2.\nParquet supports the schema property.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction https://docs.microsoft.com/en-us/azure/data-factory/format-parquet","topic":"1","question_id":96,"answers_community":[]},{"id":"Ffm0Kzs90Qj6edk0H0qb","question_images":["https://img.examtopics.com/dp-203/image288.png"],"answer_ET":"","answer_description":"","question_text":"HOTSPOT\n-\n\nYou plan to use an Azure Data Lake Storage Gen2 account to implement a Data Lake development environment that meets the following requirements:\n\n• Read and write access to data must be maintained if an availability zone becomes unavailable.\n• Data that was last modified more than two years ago must be deleted automatically.\n• Costs must be minimized.\n\nWhat should you configure? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","discussion":[{"poster":"bakamon","comment_id":"905458","upvote_count":"24","content":"Statement 1: For Storage redundancy, you should select ZRS (Zone-redundant storage). This will maintain read and write access to data even if an availability zone becomes unavailable.\n\nStatement 2: For data deletion, you should select A lifecycle management policy. This will allow you to automatically delete data that was last modified more than two years ago","timestamp":"1700805780.0"},{"timestamp":"1699210380.0","poster":"henryphchan","upvote_count":"6","content":"Zone-redundant storage (ZRS) synchronously replicates your Azure managed disk across three Azure availability zones in the region you select. Each availability zone is a separate physical location with independent power, cooling, and networking","comment_id":"890175"},{"comment_id":"1199202","poster":"Alongi","upvote_count":"1","timestamp":"1729436220.0","content":"Correct guys"},{"timestamp":"1724317020.0","upvote_count":"1","content":"correct","comment_id":"1156314","poster":"Tusharsahoo"},{"poster":"AvSUN","timestamp":"1709713200.0","upvote_count":"1","content":"Correct","comment_id":"1000248"},{"upvote_count":"1","timestamp":"1709549520.0","content":"correct","comment_id":"998393","poster":"kkk5566"},{"upvote_count":"1","content":"correct","comment_id":"989798","poster":"kkk5566","timestamp":"1708854960.0"},{"poster":"Deeksha1234","timestamp":"1708042200.0","upvote_count":"2","content":"correct answer","comment_id":"982023"},{"comment_id":"902204","content":"Confusion is how the write access will be maintained in case primary zone failure?","upvote_count":"1","timestamp":"1700429640.0","poster":"Rajan191083","comments":[{"comment_id":"904122","timestamp":"1700669460.0","upvote_count":"2","poster":"Debasish93","content":"With ZRS, your data is still accessible for both read and write operations even if a zone becomes unavailable.\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy"}]},{"content":"Zone-redundant storage (ZRS) & Lifecycle Policy","poster":"rocky48","upvote_count":"4","timestamp":"1700178960.0","comment_id":"899587"},{"timestamp":"1699207020.0","content":"correct","upvote_count":"2","poster":"makkelijkzat","comment_id":"890142"},{"poster":"dksks","upvote_count":"1","comment_id":"889615","content":"correct","timestamp":"1699132320.0"}],"answer":"","isMC":false,"topic":"1","answer_images":["https://img.examtopics.com/dp-203/image289.png"],"unix_timestamp":1683227520,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/108497-exam-dp-203-topic-1-question-80-discussion/","question_id":97,"exam_id":67,"timestamp":"2023-05-04 21:12:00"},{"id":"M23nRvR8K0KUtbghK67D","answers_community":[],"answer_description":"","isMC":false,"unix_timestamp":1683227640,"question_images":["https://img.examtopics.com/dp-203/image290.png"],"question_text":"HOTSPOT\n-\n\nYou are designing an Azure Data Lake Storage Gen2 container to store data for the human resources (HR) department and the operations department at your company.\n\nYou have the following data access requirements:\n\n• After initial processing, the HR department data will be retained for seven years and rarely accessed.\n• The operations department data will be accessed frequently for the first six months, and then accessed once per month.\n\nYou need to design a data retention solution to meet the access requirements. The solution must minimize storage costs.\n\nWhat should you include in the storage policy for each department? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer":"","answer_images":["https://img.examtopics.com/dp-203/image291.png"],"question_id":98,"topic":"1","exam_id":67,"answer_ET":"","timestamp":"2023-05-04 21:14:00","url":"https://www.examtopics.com/discussions/microsoft/view/108498-exam-dp-203-topic-1-question-81-discussion/","discussion":[{"comments":[{"content":"I agree, I also felt like I was missing information. In this case however, I'd say go for 'minimizing costs'. So the lowest cost option possible.","comment_id":"965319","upvote_count":"3","poster":"semauni","timestamp":"1706430180.0"}],"poster":"[Removed]","timestamp":"1699193880.0","comment_id":"890022","content":"The answer for HR depends on the meaning of \"rarely\" and the duration of \"initial processing\". If rarely is like once a year and initial processing is complete within 24 h the answer is correct. If rarely is like on a weekly basis, archiv might be the wrong way","upvote_count":"18"},{"content":"correct","poster":"dksks","timestamp":"1699132440.0","comment_id":"889616","upvote_count":"13"},{"comment_id":"1182626","timestamp":"1727274600.0","poster":"Alongi","content":"Correct","upvote_count":"1"},{"timestamp":"1722171120.0","poster":"Charley92","content":"After initial processing, the HR department data will be retained for seven years and rarely accessed\nThat means 365*7 = 2,555\n\nThe operations department data will be accessed frequently for the first six months, and then accessed once per month.\n\naccessed frequently for the first six months - Cool 30*6 =180","upvote_count":"3","comment_id":"1134182"},{"poster":"AvSUN","timestamp":"1709713620.0","comment_id":"1000265","upvote_count":"1","content":"I had to reread the question but the answer is correct, it would have been better if they mentioned what \"rarely\" means. Issue will arise if data needs to be accessed within 180 days of moving to archive."},{"content":"correct","upvote_count":"2","timestamp":"1709549760.0","poster":"kkk5566","comment_id":"998395"},{"content":"Correct","comment_id":"907924","poster":"Ankit_Az","upvote_count":"2","timestamp":"1701090900.0"},{"poster":"Rob77","timestamp":"1700756100.0","upvote_count":"1","content":"You can't access data that was archived without rehydration. Rehydration requires either amending blob tier to hot or cold and is likely to incur a fee if stored less than 180 day or copying blob to another location... therefore \"rarely\" is unlikely a good option...","comment_id":"904981"},{"comment_id":"899588","poster":"rocky48","content":"Answer is correct","timestamp":"1700179140.0","upvote_count":"3","comments":[{"content":"After initial processing, the HR department data will be retained for seven years and rarely accessed\nThat means 365*7 = 2,555\n\nThe operations department data will be accessed frequently for the first six months, and then accessed once per month.\n\naccessed frequently for the first six months - Cool 30*6 =180","comment_id":"1148890","poster":"rocky48","timestamp":"1723517820.0","upvote_count":"1"}]},{"poster":"henryphchan","content":"the answer is correct","comment_id":"890182","timestamp":"1699210800.0","upvote_count":"4"},{"poster":"shakes103","upvote_count":"4","content":"Answer is correct","comment_id":"889910","timestamp":"1699181760.0"}]},{"id":"0pt5TsbuikyKJo9Qn3EZ","exam_id":67,"question_images":["https://img.examtopics.com/dp-203/image292.png"],"answer_ET":"","discussion":[{"content":"The answer is correct. Check \"Exercise - Design and implement a Type 1 slowly changing dimension with mapping data flows\", there is described implementation of the dataflow mentioned in this question.\n\nhttps://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/4-exercise-design-implement-type-1-dimension\n\n\nIn the exercise 'Derived column' transformation is used to add InsertedDate and ModifiedDate columns. ModifiedDate column can be used to detect whether the customer data has changed. For Upsert 'Alter row' tranformation is used. The answer is definitely correct.","timestamp":"1699642680.0","poster":"aemilka","comment_id":"894194","upvote_count":"29"},{"upvote_count":"7","timestamp":"1700808480.0","poster":"bakamon","comment_id":"905476","content":"Answer is correct..\n1) we don't need a surrogate key in SCD type 1. you can use a Derived Column transformation to compare the incoming data with the existing data in the DimCustomer table and detect changes..\n\nStatement 2: To perform an upsert to the DimCustomer table, you should use an Alter Row transformation. This transformation can be used to specify the actions to take for each row of data, such as inserting new rows or updating existing rows. The current web page context is empty."},{"comment_id":"1206437","poster":"Alongi","upvote_count":"7","timestamp":"1730723160.0","content":"I found this question on my exam 30/04/2024, and I put:\n- derived column\n- alter row\n I passed the exam with a high score, but I'm not sure if the answer is correct."},{"poster":"moneytime","content":"Aggregate and alter row.\nAggregate will give table statistics which reveals information on max.minimum,sum ,average etc.\n\nWhile alter row encompasses ,update & insert which are upset operations","comment_id":"1137413","timestamp":"1722519120.0","upvote_count":"3"},{"comment_id":"998399","timestamp":"1709550360.0","poster":"kkk5566","content":"'Derived column' &'Alter row'","upvote_count":"1"},{"content":"answer is correct","upvote_count":"1","comment_id":"982029","timestamp":"1708042680.0","poster":"Deeksha1234"},{"content":"'Derived column' & 'Alter row'","poster":"rocky48","comment_id":"899592","upvote_count":"2","timestamp":"1700179920.0"},{"upvote_count":"2","poster":"haythemsi","content":"surrogate key and assert","comment_id":"894343","timestamp":"1699654800.0"},{"timestamp":"1699449960.0","comment_id":"892072","content":"surrogate key and alter row according to chatgpt","upvote_count":"2","poster":"nicololmen"},{"content":"It should be aggregate and alter row\n\nAs we talking Type 1 slowly changing dimension, we want to replace the current row with the updated one. This can be achieved by aggregate.\n\n\"A common use of the aggregate transformation is removing or identifying duplicate entries in source data. This process is known as deduplication. Based upon a set of group by keys, use a heuristic of your choosing to determine which duplicate row to keep. Common heuristics are first(), last(), max(), and min(). Use column patterns to apply the rule to every column except for the group by columns.\"\n\nAs in https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate\n\nAlter row is correct:\nhttps://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row#merges-and-upserts-with-azure-sql-database-and-azure-synapse","poster":"[Removed]","timestamp":"1699194900.0","upvote_count":"1","comment_id":"890028"}],"timestamp":"2023-05-05 14:35:00","answer_description":"","question_text":"HOTSPOT\n-\n\nYou are developing an Azure Synapse Analytics pipeline that will include a mapping data flow named Dataflow1. Dataflow1 will read customer data from an external source and use a Type 1 slowly changing dimension (SCD) when loading the data into a table named DimCustomer in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to ensure that Dataflow1 can perform the following tasks:\n\n• Detect whether the data of a given customer has changed in the DimCustomer table.\n• Perform an upsert to the DimCustomer table.\n\nWhich type of transformation should you use for each task? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","isMC":false,"unix_timestamp":1683290100,"answer_images":["https://img.examtopics.com/dp-203/image293.png"],"topic":"1","answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/108544-exam-dp-203-topic-1-question-82-discussion/","answers_community":[],"question_id":99},{"id":"y04fpYY0iiod1keDBpJX","timestamp":"2023-05-05 11:13:00","unix_timestamp":1683277980,"answers_community":[],"answer_images":["https://img.examtopics.com/dp-203/image295.png"],"answer_description":"","question_text":"DRAG DROP\n-\n\nYou have an Azure Synapse Analytics serverless SQL pool.\n\nYou have an Azure Data Lake Storage account named adls1 that contains a public container named container1. The container1 container contains a folder named folder1.\n\nYou need to query the top 100 rows of all the CSV files in folder1.\n\nHow should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point\n\n//IMG//","exam_id":67,"answer":"","topic":"1","question_id":100,"answer_ET":"","discussion":[{"poster":"rocky48","upvote_count":"8","content":"The provided query is correct for Azure Synapse Analytics serverless SQL pool. It selects the top 100 rows from the data in CSV format located at the specified URL: https://adls1.dfs.core.windows.net/container1/folde1/*.csv. The results are returned under the alias rows. Answer is correct.","comment_id":"899597","timestamp":"1700180460.0"},{"poster":"hassexat","upvote_count":"6","content":"OPENROWSET & Bulk","timestamp":"1709803980.0","comment_id":"1001297"},{"poster":"quatrz","upvote_count":"2","timestamp":"1740415320.0","content":"\"the container 1 container contains\"","comment_id":"1361114"},{"timestamp":"1730723220.0","comment_id":"1206440","upvote_count":"3","poster":"Alongi","content":"I found this question on my exam 30/04/2024, and I put \n- openrowset\n- bulk\n I passed the exam with a high score, but I'm not sure if the answer is correct."},{"comment_id":"1182657","poster":"Alongi","upvote_count":"1","timestamp":"1727276100.0","content":"Correct"},{"upvote_count":"2","comment_id":"998401","poster":"kkk5566","timestamp":"1709550540.0","content":"openrowset..bulk"},{"timestamp":"1708042800.0","content":"ans is correct","comment_id":"982033","poster":"Deeksha1234","upvote_count":"2"},{"upvote_count":"2","poster":"Ankit_Az","comment_id":"907925","content":"Correct","timestamp":"1701091140.0"},{"poster":"henryphchan","upvote_count":"3","timestamp":"1699211280.0","content":"The answer is correct","comment_id":"890189"},{"content":"correct","timestamp":"1699195260.0","comment_id":"890031","poster":"[Removed]","upvote_count":"3"},{"timestamp":"1699182780.0","content":"Answer is correct","upvote_count":"3","comment_id":"889919","poster":"shakes103"}],"question_images":["https://img.examtopics.com/dp-203/image294.png"],"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/108532-exam-dp-203-topic-1-question-83-discussion/"}],"exam":{"name":"DP-203","id":67,"numberOfQuestions":384,"provider":"Microsoft","isMCOnly":false,"isBeta":false,"lastUpdated":"12 Apr 2025","isImplemented":true},"currentPage":20},"__N_SSP":true}