{"pageProps":{"questions":[{"id":"T7fsidBeDIQ7lgfCK9HQ","timestamp":"2019-07-31 12:41:00","question_text":"You plan to design an application that will use data from Azure Data Lake and perform sentiment analysis by using Azure Machine Learning algorithms.\nThe developers of the application use a mix of Windows- and Linux-based environments. The developers contribute to shared GitHub repositories.\nYou need all the developers to use the same tool to develop the application.\nWhat is the best tool to use? More than one answer choice may achieve the goal.","isMC":true,"topic":"1","answer_ET":"C","answer_description":"References:\nhttps://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/studio/algorithm-choice.md","answers_community":[],"choices":{"D":"Microsoft Visual Studio","C":"Azure Machine Learning Studio","A":"Microsoft Visual Studio Code","B":"Azure Notebooks"},"answer_images":[],"unix_timestamp":1564569660,"question_id":46,"question_images":[],"discussion":[{"upvote_count":"19","comment_id":"32565","timestamp":"1577270700.0","content":"Azure Machine Learning Studio is the correct answer.\nAzure Machine Learning Studio (classic) can read data directly from Azure Data Lake Store and then be used to create and deploy models. This approach uses a Hive table that points at the Azure Data Lake Store. This requires that a separate Azure HDInsight cluster be provisioned, on which the Hive table is created. \n Refer the following url: \n https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/data-lake-walkthrough","poster":"VinayC","comments":[{"content":"How can you contribute to GitHub using the Azure ML?","comment_id":"34172","upvote_count":"1","comments":[{"timestamp":"1581836340.0","content":"technically, you have a Download button for every notebook you create in ML studio. the text only mentions that the notebooks need to be shared, not through which mechanism. so although VS code could also be an option, this would require them to also install it accordingly. and because the questions states at the end that more than one choice can achieve the goal, i think it's safe to end the debate by choosing either VScode or MLS as only viable options from the 4, and i think you will be scored correct on either one","upvote_count":"3","poster":"junkz","comment_id":"51121"}],"poster":"Exposer","timestamp":"1577870820.0"}]},{"comment_id":"13778","comments":[{"poster":"awron_durat","comment_id":"254309","upvote_count":"1","content":"VS Code is also available for both Windows and Linux.","timestamp":"1609183200.0"}],"upvote_count":"13","timestamp":"1570139520.0","content":"I think that the answer is Visual Studio Code. https://code.visualstudio.com/docs","poster":"T0p1cs"},{"timestamp":"1687182720.0","upvote_count":"1","poster":"rveney","comment_id":"927568","content":"To ensure that all developers, regardless of their operating system, can collaborate on developing the application and perform sentiment analysis using Azure Machine Learning algorithms, the best tool to use would be A. Microsoft Visual Studio Code."},{"poster":"ExamGuruBhai","comment_id":"633798","content":"Visual Studio Code as it's talking about GitHub","timestamp":"1658280000.0","upvote_count":"1"},{"timestamp":"1615839240.0","content":"It is indeed a tricky question but AZ ML Studio is the correct option because it provide a common development place where you can find all the required ML development resources e.g., Notebook, Compute and Data Connectivity to different data sources.","upvote_count":"2","comment_id":"311737","poster":"[Removed]"},{"content":"I think ML Studion is Correct.","timestamp":"1613965860.0","comment_id":"296301","poster":"ayush_singh","upvote_count":"2"},{"upvote_count":"2","content":"Everyone focuses on the sharing the notebook on GitHub, missing rest of the details. The important detail here is to use existing ML Algorithms and connecting to Azure Data Lake for data, which is also available. I don't know why people said it is not available. \n\nCheck this link: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data","comment_id":"261546","poster":"aitruthseeker","timestamp":"1609996200.0"},{"comment_id":"218288","poster":"09012020","content":"Answer is D \n\nVisual Studio Code Tools for AI: This service is an extension of Visual Studio Code (VS Code) — a desktop source code editor for Windows, macOS and Linux — that helps developers create scripts and gather metrics for Azure Machine Learning experiments.\nAzure Machine Learning Studio: This is a visual, drag-and-drop tool designed to help users build and deploy predictive analysis models with no coding required.\n\nhttps://medium.com/@ahmedkhemiri24/microsoft-azure-machine-learning-d148478e867c","upvote_count":"1","timestamp":"1605245520.0","comments":[{"poster":"hved","timestamp":"1609131600.0","comment_id":"253829","content":"So based on your explanation, ans shouuld be A, correct?","upvote_count":"2"}]},{"comment_id":"184182","upvote_count":"1","timestamp":"1600750680.0","content":"currently Azure ML Studio supports import from Data Lake also using the designer https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/import-data","poster":"sayak17"},{"content":"the url in the answer is obviously broken","poster":"Nova077","upvote_count":"2","comment_id":"174529","timestamp":"1599394080.0"},{"poster":"ritam2709","comment_id":"127189","timestamp":"1593980340.0","content":"Why not Azure Notebooks?\nI think the perfect answer is VS Code. Apart from that we can use AZURE ML Studio and Azure Notebooks as well.","upvote_count":"1"},{"comment_id":"110023","upvote_count":"5","timestamp":"1592129280.0","content":"Clearly the question says \"More than one answer choice may achieve the goal\" - It would be either MLS or VS.","poster":"av6054"},{"upvote_count":"7","content":"Machine learning studio doesn't make any sense because the scenario is about sharing code and ML studio doesn't have any code or sharing with GitHub. Visual Studio is not an option because of Windows dependency. Azure Notebooks and VS Code are two suitable options. I am a little biased towards VSCode","comments":[{"content":"I think it's VS Code. You are right.","upvote_count":"1","timestamp":"1577870880.0","poster":"Exposer","comment_id":"34173"}],"poster":"Ubaid","timestamp":"1576211460.0","comment_id":"29216"},{"content":"Wouldn't Azure Notebooks make more sense as Azure Machine Learning Studio does not have an import data function for Data Lake yet...","comment_id":"5497","comments":[{"timestamp":"1567219740.0","poster":"CodeAnant","content":"I think u r correct.... as Machine Learning Studio does not have an import data function for Data Lake \nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio/what-is-ml-studio","upvote_count":"1","comment_id":"9058"}],"poster":"granota","timestamp":"1564569660.0","upvote_count":"3"}],"exam_id":39,"answer":"C","url":"https://www.examtopics.com/discussions/microsoft/view/3058-exam-ai-100-topic-1-question-8-discussion/"},{"id":"NCO6cxHsOCOPreRmAVrV","answer_images":[],"timestamp":"2019-10-03 23:51:00","answer_description":"B: To keep up with application demands in Azure Kubernetes Service (AKS), you may need to adjust the number of nodes that run your workloads. The cluster autoscaler component can watch for pods in your cluster that can't be scheduled because of resource constraints. When issues are detected, the number of nodes is increased to meet the application demand. Nodes are also regularly checked for a lack of running pods, with the number of nodes then decreased as needed. This ability to automatically scale up or down the number of nodes in your AKS cluster lets you run an efficient, cost-effective cluster.\nReference:\nhttps://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler","topic":"1","discussion":[{"comment_id":"61735","poster":"Piraat","timestamp":"1583849820.0","content":"Is this part of AI-100? I don't remember learning anything related to this...","upvote_count":"13","comments":[{"upvote_count":"1","poster":"AndyH77","content":"Simpler parts of data engineering, database, and SQL questions are included in AI-100. This type of question was also on the data engineering exam.","timestamp":"1623702960.0","comment_id":"382126"}]},{"poster":"nohaph","upvote_count":"9","content":"The question asks about nodes so the answer is Cluster Auto-scaler. If it asks about pods then the answer will be horizontal pod auto-scaler","timestamp":"1609623960.0","comment_id":"257938","comments":[{"upvote_count":"2","timestamp":"1609996500.0","poster":"aitruthseeker","content":"Simple and elegant! Yes Cluster Auto-Scaler is the answer","comment_id":"261548"}]},{"upvote_count":"1","content":"To handle the unpredictable application load in an Azure Kubernetes Service (AKS) cluster that occasionally requires more than the maximum of 32 nodes, the recommended scaling method would be B. cluster autoscaler.","timestamp":"1687182840.0","poster":"rveney","comment_id":"927569"},{"timestamp":"1673796240.0","upvote_count":"1","poster":"dev2dev","comment_id":"776734","content":"ACI: D is answer.\nhttps://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/scale-using-aks-with-aci\n \"Use the AKS virtual node to provision pods inside ACI that start in seconds. This enables AKS to run with just enough capacity for your average workload. As you run out of capacity in your AKS cluster, scale out additional pods in ACI, without any additional servers to manage\""},{"timestamp":"1673795520.0","content":"autoscaler is to scale the cluster up/down. So as per the question it can't go beyond 32 nodes and the question is what you do when we need more than 32 nodes? surely autoscaler won't do this job.","poster":"dev2dev","upvote_count":"1","comment_id":"776707"},{"poster":"heirro39","comment_id":"339688","content":"Correct answer is cluster autoscaler.\n\nRef : https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#:~:text=cluster%20autoscaler%20settings.-,Update%20an%20existing%20AKS%20cluster%20to%20enable%20the%20cluster%20autoscaler,count%20and%20%2D%2Dmax%2Dcount%20.","timestamp":"1618932840.0","upvote_count":"1"},{"content":"can the answer be A?","comment_id":"249076","upvote_count":"1","poster":"deathknight666","timestamp":"1608528600.0"},{"poster":"Akashg","timestamp":"1607434560.0","upvote_count":"4","content":"The ans is (D)Azure container Instance:\nTo rapidly scale your AKS cluster, you can integrate with Azure Container Instances (ACI).\nhttps://docs.microsoft.com/en-us/azure/aks/concepts-scale","comment_id":"238291"},{"upvote_count":"3","comment_id":"230396","timestamp":"1606665180.0","content":"My answer would be D. Azure Container Instances, since maximum of the cluster is 32 and \"occasionally and unpredictably\" we need resources above that value.\nProof: https://azure.microsoft.com/en-us/services/container-instances/#overview\nElastic bursting with AKS\nACI provides fast, isolated compute to meet traffic that comes in spikes, without the need to manage servers. For example, Azure Kubernetes Service (AKS) can use the Virtual Kubelet to provision pods inside ACI that start in seconds. This enables AKS to run with just enough capacity for your average workload. As you run out of capacity in your AKS cluster, scale out additional pods in ACI without any additional servers to manage.","poster":"userfriendly"},{"poster":"tttyyy","upvote_count":"1","comment_id":"200720","content":"In the question: \"The cluster supports a maximum of 32 nodes.\nYou discover that occasionally and unpredictably, the application requires more than 32 nodes.\"\nTo me, this is a clear indication an additional cluster is needed, therefore B.","timestamp":"1602788940.0"},{"content":"“Cluster autoscaler is typically used alongside the horizontal pod autoscaler. When combined, the horizontal pod autoscaler increases or decreases the number of pods based on application demand, and the cluster autoscaler adjusts the number of nodes as needed to run those additional pods accordingly.”","poster":"freedomeox","comment_id":"167214","upvote_count":"1","timestamp":"1598490720.0"},{"upvote_count":"2","timestamp":"1590474960.0","content":"The correct answer is B. If you check https://docs.microsoft.com/en-us/azure/aks/concepts-scale, particularly the section \"Cluster Auto-scaler\", you'll find the following excerpt:\n\"If the cluster autoscale determines that a change is required, the number of nodes in your AKS cluster is increased or decreased accordingly\"\n\nOn the reason why A. Horizontal pod auto-scaler, it becomes clear on the same web page that this scaler is limited by the number of nodes in the cluster, therefore, not acceptable","poster":"jorama","comment_id":"95824"},{"content":"I think the correct answers are \"A\" and \"B\". The horizontal pod auto-scaler increase/ decrease the number of replicas of an app that has been deployed, while the cluster auto-scaler increase/ decrease the number of nodes in the cluster. Any of these two options could fit well in the required scenario.","comment_id":"72157","poster":"Miles19","timestamp":"1586271540.0","upvote_count":"2"},{"timestamp":"1570139520.0","comment_id":"13779","upvote_count":"1","content":"Please ignore, this was for the previous question","poster":"T0p1cs"},{"content":"I think the answer is Visual Studio Code... https://code.visualstudio.com/docs","poster":"T0p1cs","comment_id":"13777","upvote_count":"1","timestamp":"1570139460.0"}],"answer_ET":"B","exam_id":39,"isMC":true,"answer":"B","question_text":"You have several AI applications that use an Azure Kubernetes Service (AKS) cluster. The cluster supports a maximum of 32 nodes.\nYou discover that occasionally and unpredictably, the application requires more than 32 nodes.\nYou need to recommend a solution to handle the unpredictable application load.\nWhich scaling method should you recommend?","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/6082-exam-ai-100-topic-1-question-9-discussion/","unix_timestamp":1570139460,"answers_community":[],"question_id":47,"choices":{"C":"manual scaling","B":"cluster autoscaler","A":"horizontal pod autoscaler","D":"Azure Container Instances"}},{"id":"KNDlES97eT8pg7QPM9k0","answer_images":["https://www.examtopics.com/assets/media/exam-media/03857/0005800001.jpg"],"isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/03857/0005700001.png"],"answer_ET":"","unix_timestamp":1569364020,"answer":"","discussion":[{"comments":[{"upvote_count":"1","comment_id":"189030","content":"https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge-csharp-udf-methods","timestamp":"1601297760.0","poster":"Ndubai"}],"poster":"Nova077","comment_id":"178812","upvote_count":"8","timestamp":"1600013700.0","content":"we are not doing Analytics on a streaming data. We will do data enrichment by using queries ( stored procedures). Hence my vote is for Azure Functions."},{"comment_id":"90418","content":"Azure Functions could be an option here, maily for two reasons:\n1. data preparation require custom code\n2. \"An Azure Function integrated with SQL Database provides multiple benefits: ... Doing the bulk of data preparation using SQL Database in-memory processing and native stored procedures was very fast\"\nhttps://azure.microsoft.com/it-it/blog/considering-azure-functions-for-a-serverless-data-streaming-scenario/","upvote_count":"5","timestamp":"1589702760.0","comments":[{"content":"Stream Analytics is designed for real-time and can run custom code (Stream Analytics Query Language, which is T-SQL like).\nhttps://docs.microsoft.com/en-us/stream-analytics-query/stream-analytics-query-language-reference","upvote_count":"5","timestamp":"1590325980.0","comment_id":"94931","poster":"carloshvp","comments":[{"content":"Therefore Stream Analytics is correct","upvote_count":"1","comments":[{"upvote_count":"4","poster":"giusecozza","comments":[{"poster":"Arinze","upvote_count":"3","timestamp":"1595895660.0","content":"Because Azure Event Hubs can be an input to Stream Analytics but not to Azure Functions, I would say Stream Analytics is the answer not Azure Functions","comment_id":"145341","comments":[{"timestamp":"1612986000.0","upvote_count":"2","comment_id":"287820","content":"Sure it can:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-reliable-event-processing\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs-trigger?tabs=csharp","poster":"Cornholioz"}]}],"timestamp":"1590828240.0","comment_id":"98728","content":"Functions are well suited for real-time processing too. Moreover, the question is asking for a component for \"data preparation\", which is a good use case for Functions. Finally, I'm not sure Stream Analytics supports calls for Stored Procedures. Some users were complaining about this on the blogs below:\nhttps://feedback.azure.com/forums/270577-stream-analytics/suggestions/38438074-support-sql-db-stored-procedures-as-asa-output\nhttps://stackoverflow.com/questions/44026676/how-can-i-set-stream-analytics-output-as-stored-procedure"}],"comment_id":"94933","poster":"carloshvp","timestamp":"1590325980.0"}]}],"poster":"giusecozza"},{"poster":"rveney","upvote_count":"1","content":"components to be used for data ingestion and data preparation are:\n\nData Ingestion: Azure Event Hubs\nData Preparation: Azure Functions","timestamp":"1687179300.0","comment_id":"927501"},{"upvote_count":"1","timestamp":"1624968420.0","poster":"123aditya","comment_id":"393755","content":"Analytics is correct as it will be used to \"process the data\"\nhttps://docs.microsoft.com/en-in/azure/stream-analytics/stream-analytics-with-azure-functions"},{"upvote_count":"1","comment_id":"387679","content":"According to this quote \"Azure Stream Analytics is an event-processing engine that can analyze high volumes of data streaming from devices and other data sources. It also supports extracting information from data streams to identify patterns and relationships. These patterns can trigger other downstream actions. In this scenario, Stream Analytics transforms the input stream from Event Hubs to identify fraudulent calls.\" IMO the second answer is Stream Analytics. https://docs.microsoft.com/bs-latn-ba/azure/architecture/example-scenario/data/fraud-detection","timestamp":"1624346520.0","poster":"damirbek369"},{"content":"This links explains it all: https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs-trigger?tabs=csharp you can use a function and the trigger can come from event hubs. Therefore, Event hubs and azure functions 100% sure","comment_id":"315844","upvote_count":"3","poster":"AlfuryDB","timestamp":"1616270100.0"},{"poster":"vilas94","upvote_count":"4","timestamp":"1603016880.0","comment_id":"201968","content":"The question says the enrichment process will require custom code, The only option for this is azure Functions.\nAzure stream Analytics can also has custom code But this option has off 09/10/2020 is in Preview \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge-csharp-udf-methods"},{"comment_id":"198893","timestamp":"1602555540.0","upvote_count":"5","content":"According to this article, the solution seems to be: Event hub+Azure Functions\nhttps://azure.microsoft.com/hu-hu/blog/considering-azure-functions-for-a-serverless-data-streaming-scenario/","poster":"CeliaZhou"},{"poster":"abbam","upvote_count":"4","comment_id":"83522","comments":[{"comment_id":"481554","content":"You are representing your thoughts, but why? Is this Roadies or Splitsvilla? This Azure -100.... Behave properly....nahale bhara khaibu","poster":"adarshsahoo","upvote_count":"1","timestamp":"1637316720.0"}],"content":"My thoughts regarding the 2nd answer: \nAgree azure Stream Analytics is good candidate for real-time analytics and complex event-processing and designed for high volumes of fast streaming data but the question here is about preparing the data and also remember the data enrichment is done at the SQL DB level and there is already an algorithm that does the classification. \nAn azure function could act as an orchestrator, not doing any heavy lifting except loading data payload to azure SQL DB, the stored procedures computes/calculates the new features.","timestamp":"1588585080.0"},{"comment_id":"12526","content":"In the first part-> The answer should be Azure IOT Hub...Why Not Event Hub ? Because the IOT Hub contains and Event Hub and hence essentially is an additional features. More important thing is that Event Hub can only receive message,where as IOT hub can send messages to individual devices.","upvote_count":"1","timestamp":"1569364020.0","comments":[{"content":"There is no requirement to write back to the devices nor anything around device management. Therefore I would go with Event Hub","comment_id":"12969","upvote_count":"26","timestamp":"1569626640.0","poster":"T0p1cs","comments":[{"upvote_count":"3","comment_id":"94929","timestamp":"1590325860.0","content":"Event Hub is correct","poster":"carloshvp"}]}],"poster":"RishiAI"}],"timestamp":"2019-09-25 00:27:00","question_text":"HOTSPOT -\nYou are designing a solution that will analyze bank transactions in real time. The transactions will be evaluated by using an algorithm and classified into one of five groups. The transaction data will be enriched with information taken from Azure SQL Database before the transactions are sent to the classification process. The enrichment process will require custom code. Data from different banks will require different stored procedures.\nYou need to develop a pipeline for the solution.\nWhich components should you use for data ingestion and data preparation? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"References:\nhttps://docs.microsoft.com/bs-latn-ba/azure/architecture/example-scenario/data/fraud-detection","url":"https://www.examtopics.com/discussions/microsoft/view/5661-exam-ai-100-topic-2-question-1-discussion/","topic":"2","question_id":48,"exam_id":39,"answers_community":[]},{"id":"Ks9HQqqcYFmnvwK6wclj","answer_description":"Step 1: From the Azure portal, configure an identity provider.\nThe Azure Bot Service and the v4 SDK include new bot authentication capabilities, providing features to make it easier to develop a bot that authenticates users to various identity providers, such as Azure AD (Azure Active Directory), GitHub, Uber, and so on.\nStep 2: From the Azure portal, create an Azure Active Directory (Azure AD) B2C service.\nAzure Active Directory B2C provides business-to-customer identity as a service. Your customers use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs.\nStep 3: From the Azure portal, create a client application\nYou can enable communication between your bot and your own client application by using the Direct Line API.\nStep 4: From the bot code, add the connection settings and OAuthPrompt\nUse an OAuth prompt to sign the user in and get a token.\nAzure AD B2C uses standards-based authentication protocols including OpenID Connect, OAuth 2.0, and SAML.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/bot-service/bot-builder-authentication?view=azure-bot-service-4.0","timestamp":"2021-06-27 17:56:00","isMC":false,"discussion":[{"content":"https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app \ncorrect sequence","upvote_count":"1","poster":"123aditya","timestamp":"1624969800.0","comment_id":"393774"},{"poster":"timosi","content":"my preferred order would be:\nStep 1: From the Azure portal, create a client application\nStep 2: From the Azure portal, create an Azure Active Directory (Azure AD) B2C service.\nStep 3: From the Azure portal, configure an identity provider.\nStep 4: From the bot code, add the connection settings and OAuthPrompt","upvote_count":"4","comment_id":"392203","timestamp":"1624809360.0"}],"topic":"2","answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03857/0006800001.jpg"],"question_id":49,"answer_ET":"","exam_id":39,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03857/0006900001.jpg"],"url":"https://www.examtopics.com/discussions/microsoft/view/56182-exam-ai-100-topic-2-question-10-discussion/","answers_community":[],"question_text":"DRAG DROP -\nYou need to create a bot to meet the following requirements:\n✑ The bot must support multiple bot channels including Direct Line.\n✑ Users must be able to sign in to the bot by using a Gmail user account and save activities and preferences.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\nSelect and Place:\n//IMG//","unix_timestamp":1624809360},{"id":"Xtxw7AlslReFmEoVYX4P","question_id":50,"answer_description":"Box 1: train -\nUtterances are input from the user that your app needs to interpret. To train LUIS to extract intents and entities from them, it's important to capture a variety of different example utterances for each intent. Active learning, or the process of continuing to train on new utterances, is essential to machine-learned intelligence that LUIS provides.\n\nBox 2: creating intents -\nEach intent needs to have example utterances, at least 15. If you have an intent that does not have any example utterances, you will not be able to train LUIS. If you have an intent with one or very few example utterances, LUIS will not accurately predict the intent.\n\nBox 3: never published -\nIn each iteration of the model, do not add a large quantity of utterances. Add utterances in quantities of 15. Train, publish, and test again.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-utteran3ce","answer":"","answers_community":[],"topic":"2","unix_timestamp":1621506540,"isMC":false,"answer_ET":"","question_text":"HOTSPOT -\nYou have an app that uses the Language Understanding (LUIS) API as shown in the following exhibit.\n//IMG//\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/03857/0007200001.jpg"],"exam_id":39,"timestamp":"2021-05-20 12:29:00","discussion":[{"comment_id":"382815","upvote_count":"1","poster":"shaimaalmeer","content":"this question was in the exam","timestamp":"1623778800.0"},{"poster":"fhqhfhqh","upvote_count":"1","comment_id":"362054","content":"This question was in the exam.","timestamp":"1621506540.0"}],"url":"https://www.examtopics.com/discussions/microsoft/view/53202-exam-ai-100-topic-2-question-11-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/03857/0007000001.png","https://www.examtopics.com/assets/media/exam-media/03857/0007100001.jpg"]}],"exam":{"isMCOnly":false,"lastUpdated":"12 Apr 2025","name":"AI-100","isImplemented":true,"isBeta":false,"provider":"Microsoft","numberOfQuestions":206,"id":39},"currentPage":10},"__N_SSP":true}