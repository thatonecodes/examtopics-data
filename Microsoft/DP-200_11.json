{"pageProps":{"questions":[{"id":"DfFQa7uDRTJK6WlqqiPy","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0010800001.png"],"answer":"","discussion":[{"comment_id":"353977","content":"The first box should be \"Binary\". It says - no transformation.","upvote_count":"6","timestamp":"1620667920.0","comments":[{"comment_id":"383522","timestamp":"1623857400.0","poster":"[Removed]","comments":[{"poster":"lgtiza","upvote_count":"3","content":"Every parquet file is also a binary file. I think the key is \"no transformations\", so why the extra work of interpreting a parquet file?! Binary and preserve hierarchy should do it imo.","timestamp":"1624320900.0","comment_id":"387490"}],"content":"Binary is only for Binary format: https://docs.microsoft.com/en-us/azure/data-factory/format-binary","upvote_count":"1"}],"poster":"SorinXp"},{"content":"First box should be \"Binary\" . I tested it with the 2 options . using paquet i got an error with the following message :\n\"Dataset Parquet1 location is a folder, the wildcard file name is required for Copy data1\"","comment_id":"458130","timestamp":"1633509480.0","poster":"medsimus","upvote_count":"3"},{"upvote_count":"4","timestamp":"1624177140.0","poster":"Hinzzz","content":"The given answer is correct Parquet and preserve hierarchy","comment_id":"386073"},{"comment_id":"384349","poster":"CarNama_IG","upvote_count":"1","content":"You can use Binary dataset in Copy activity, GetMetadata activity, or Delete activity. When using Binary dataset, ADF does not parse file content but treat it as-is. When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset..so the ans should be parquet","timestamp":"1623950340.0","comments":[{"content":"Why does it need to be parquet? Just configure the sink dataset as binary as well. This way ADF doesn't need to parse the files. You just need parquet if you want to do some transformation or when the sink dataset is an existing parquet dataset","poster":"hello_there_","upvote_count":"1","comment_id":"388769","timestamp":"1624447740.0"}]},{"poster":"maciejt","comment_id":"360808","timestamp":"1621368000.0","upvote_count":"2","content":"It should be Binary - it copies the files as they are, no need to parse the parquet format if you don't need to transform them."},{"timestamp":"1619757000.0","upvote_count":"4","comment_id":"345878","content":"Agree with the answer as both source and sink can accommodate \"parquet\" extension files using the behavior as seen below. Try working it on ADFv2\n\nFile Format: Parquet (source and sink)\nCopy behavior: Preserve Hierarchy","poster":"cadio30"},{"poster":"dangal95","upvote_count":"3","comment_id":"345491","timestamp":"1619709780.0","content":"Answer is correct. \nhttps://docs.microsoft.com/en-us/azure/data-factory/format-parquet"},{"upvote_count":"2","poster":"Dark12arrow","content":"do u have any reference ? and if u cant use parquet to load parquet files whats the point of ever choosing parquet?","comment_id":"337350","timestamp":"1618633920.0"},{"upvote_count":"3","poster":"eliabsbueno","comment_id":"336649","timestamp":"1618530180.0","content":"The first box should be \"Binary\". You can't use a parquet data source to load different parquet files."}],"answer_description":"Box 1: Parquet -\nFor Parquet datasets, the type property of the copy activity source must be set to ParquetSource..\n\nBox 2: PreserveHierarchy -\nPreserveHierarchy (default): Preserves the file hierarchy in the target folder. The relative path of the source file to the source folder is identical to the relative path of the target file to the target folder.\nIncorrect Answers:\nFlattenHierarchy: All files from the source folder are in the first level of the target folder. The target files have autogenerated names.\nMergeFiles: Merges all files from the source folder to one file. If the file name is specified, the merged file name is the specified name. Otherwise, it's an autogenerated file name.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/format-parquet https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage","question_id":51,"isMC":false,"topic":"1","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0010900001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/50232-exam-dp-200-topic-1-question-57-discussion/","question_text":"HOTSPOT -\nYou have two Azure Storage accounts named Storage1 and Storage2. Each account contains an Azure Data Lake Storage file system. The system has files that contain data stored in the Apache Parquet format.\nYou need to copy folders and files from Storage1 to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements:\n✑ No transformations must be performed.\n✑ The original folder structure must be retained.\nHow should you configure the copy activity? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","unix_timestamp":1618530180,"timestamp":"2021-04-16 01:43:00","exam_id":65,"answer_ET":""},{"id":"jJgJSXijZ193vVVcV9P1","question_id":52,"answer_description":"You can use the Delete Activity in Azure Data Factory to delete files or folders from on-premises storage stores or cloud storage stores.\nAzure Blob storage is supported.\nNote: You can also apply an Azure Blob storage lifecycle policy.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/delete-activity https://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts?tabs=azure-portal","answers_community":[],"choices":{"A":"Yes","B":"No"},"question_images":[],"discussion":[{"poster":"Nevia","timestamp":"1618383840.0","upvote_count":"4","comment_id":"335261","content":"Answer A seems to be correct https://azure.microsoft.com/it-it/blog/clean-up-files-by-built-in-delete-activity-in-azure-data-factory/"}],"answer":"A","exam_id":65,"timestamp":"2021-04-14 09:04:00","isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure subscription that contains an Azure Storage account.\nYou plan to implement changes to a data storage solution to meet regulatory and compliance standards.\nEvery day, Azure needs to identify and delete blobs that were NOT modified during the last 100 days.\nSolution: You schedule an Azure Data Factory pipeline with a delete activity.\nDoes this meet the goal?","unix_timestamp":1618383840,"topic":"1","answer_images":[],"answer_ET":"A","url":"https://www.examtopics.com/discussions/microsoft/view/50098-exam-dp-200-topic-1-question-58-discussion/"},{"id":"WMtNwXvrLOupBYSnDp6i","topic":"1","unix_timestamp":1618871340,"discussion":[{"comment_id":"357615","poster":"Amy007","content":"Yes , Yes and No.","timestamp":"1621058040.0","upvote_count":"8"},{"comment_id":"1016384","upvote_count":"1","poster":"Chemmangat","content":"From DP 203 discussions, I found out the answer is Yes for all.","timestamp":"1695612840.0"},{"comment_id":"392445","content":"What is the correct answer?? Pl advise.","upvote_count":"1","timestamp":"1624836660.0","poster":"hosan","comments":[{"poster":"captainbee","timestamp":"1625403000.0","upvote_count":"1","comment_id":"398333","content":"Doesn't matter, no-one can seem to agree on the answer on here either"}]},{"content":"What is the answer?? WHy do they not give the correct answer in the first place..what is their problem?\n]","comment_id":"390305","upvote_count":"1","timestamp":"1624614480.0","poster":"Williammm"},{"upvote_count":"3","content":"NO, YES, YES.","timestamp":"1623917220.0","comment_id":"384025","poster":"qiyan1982"},{"poster":"jasonoubre","content":"After the survey, I have no ideas. Can I know what is the answer?","timestamp":"1622560260.0","upvote_count":"1","comment_id":"371982"},{"poster":"akod","content":"This question is repetitive.","upvote_count":"2","comment_id":"339177","comments":[{"timestamp":"1618925220.0","comment_id":"339594","content":"It's different. My options are Yes, No and No (Source Whizlabs)","poster":"AngelRio","upvote_count":"2"},{"upvote_count":"7","comment_id":"354845","timestamp":"1620748080.0","content":"Its not. \nQuestion 45: The query joins ...\nQuestion 59: The query combines ...","comments":[{"upvote_count":"2","content":"The query from both items are different. Question # 45 doesn't have \"partition\" syntax that makes it incorrect. On the other hand, there are two select statements that is combined.","comment_id":"355114","timestamp":"1620779820.0","poster":"cadio30"}],"poster":"Nobody99"}],"timestamp":"1618871340.0"}],"answer_ET":"","answer_description":"Box 1: Yes -\nYou can now use a new extension of Azure Stream Analytics SQL to specify the number of partitions of a stream when reshuffling the data.\nThe outcome is a stream that has the same partition scheme. Please see below for an example:\nWITH step1 AS (SELECT * FROM [input1] PARTITION BY DeviceID INTO 10), step2 AS (SELECT * FROM [input2] PARTITION BY DeviceID INTO 10)\nSELECT * INTO [output] FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID\nNote: The new extension of Azure Stream Analytics SQL includes a keyword INTO that allows you to specify the number of partitions for a stream when performing reshuffling using a PARTITION BY statement.\n\nBox 2: Yes -\nWhen joining two streams of data explicitly repartitioned, these streams must have the same partition key and partition count.\n\nBox 3: Yes -\nStreaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. The higher the number of SUs, the more CPU and memory resources are allocated for your job.\nIn general, the best practice is to start with 6 SUs for queries that don't use PARTITION BY.\nHere there are 10 partitions, so 6x10 = 60 SUs is good.\nNote: Remember, Streaming Unit (SU) count, which is the unit of scale for Azure Stream Analytics, must be adjusted so the number of physical resources available to the job can fit the partitioned flow. In general, six SUs is a good number to assign to each partition. In case there are insufficient resources assigned to the job, the system will only apply the repartition if it benefits the job.\nReference:\nhttps://azure.microsoft.com/en-in/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption","question_text":"HOTSPOT -\nYou have the following Azure Stream Analytics query.\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0011200002.jpg"],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0011100001.png","https://www.examtopics.com/assets/media/exam-media/03872/0011200001.jpg"],"timestamp":"2021-04-20 00:29:00","exam_id":65,"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/50509-exam-dp-200-topic-1-question-59-discussion/","question_id":53,"answers_community":[],"isMC":false},{"id":"2krx8ZQmpMiaNu7NWaIz","topic":"1","question_id":54,"answer":"E","choices":{"B":"eDTUs per database only","A":"Number of transactions only","C":"Number of databases only","D":"CPU usage only","E":"eDTUs and max data size"},"timestamp":"2020-06-24 19:59:00","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/23958-exam-dp-200-topic-1-question-6-discussion/","discussion":[{"poster":"induna","timestamp":"1593021540.0","comment_id":"118665","upvote_count":"10","content":"An additional note from the reference link:\n\"There is no per-database charge for elastic pools. You are billed for each hour a pool exists at the highest eDTU or vCores, regardless of usage or whether the pool was active for less than an hour.\"\nSo no need to figure out the needs of an individual database to size the pool"},{"content":"Answer E","upvote_count":"4","poster":"JESUSBB","timestamp":"1613526660.0","comment_id":"292243"},{"upvote_count":"3","timestamp":"1607248740.0","comment_id":"236327","poster":"chaoxes","content":"Answer is correct - E.\n\nHow do I choose the correct pool size?\nThe best size for a pool depends on the aggregate resources needed for all databases in the pool. This involves determining the following:\n\n-> Maximum resources utilized by all databases in the pool (either maximum DTUs or maximum vCores depending on your choice of purchasing model).\n-> Maximum storage bytes utilized by all databases in the pool.\n\nSource: https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview"},{"poster":"syu31svc","comment_id":"225768","timestamp":"1606132440.0","upvote_count":"1","content":"Link provided supports E as the right answer"},{"timestamp":"1605588600.0","poster":"ThieryLeLuronNadieline","upvote_count":"1","comment_id":"220818","content":"indeed the correct answer is E"},{"timestamp":"1605380460.0","poster":"Egocentric","upvote_count":"1","content":"answer is E","comment_id":"219289"},{"timestamp":"1599029640.0","upvote_count":"3","poster":"Rickii","content":"Answer is E","comment_id":"171796"},{"comment_id":"165876","content":"so answer here is B i.e eDTU's per database only","timestamp":"1598348580.0","comments":[{"upvote_count":"2","timestamp":"1598962440.0","poster":"Varma_Saraswathula","content":"The best size for a pool depends on the aggregate resources needed for all databases in the pool. This involves determining the following:\n\nMaximum resources utilized by all databases in the pool (either maximum DTUs or maximum vCores depending on your choice of purchasing model).\nMaximum storage bytes utilized by all databases in the pool.","comment_id":"171341"},{"comment_id":"182437","upvote_count":"13","poster":"groy","timestamp":"1600541880.0","content":"Vijaya...please don't confuse people, answer is E!"}],"upvote_count":"1","poster":"Vijaya"}],"answer_description":"The best size for a pool depends on the aggregate resources needed for all databases in the pool. This involves determining the following:\n✑ Maximum resources utilized by all databases in the pool (either maximum DTUs or maximum vCores depending on your choice of resourcing model).\n✑ Maximum storage bytes utilized by all databases in the pool.\nNote: Elastic pools enable the developer to purchase resources for a pool shared by multiple databases to accommodate unpredictable periods of usage by individual databases. You can configure resources for the pool based either on the DTU-based purchasing model or the vCore-based purchasing model.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-pool","question_text":"A company has a SaaS solution that uses Azure SQL Database with elastic pools. The solution contains a dedicated database for each customer organization.\nCustomer organizations have peak usage at different periods during the year.\nYou need to implement the Azure SQL Database elastic pool to minimize cost.\nWhich option or options should you configure?","exam_id":65,"answer_images":[],"answer_ET":"E","isMC":true,"unix_timestamp":1593021540,"answers_community":[]},{"id":"n8dBszf74EnNxhCXbE9i","answer":"","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0011400001.jpg"],"topic":"1","question_id":55,"exam_id":65,"url":"https://www.examtopics.com/discussions/microsoft/view/49141-exam-dp-200-topic-1-question-60-discussion/","discussion":[{"poster":"Wendy_DK","content":"Step 1 Create an Azure Key vault and enable purge protection\nStep 2 Add an Azure Key Vault access policy to grant permission to the Azure Cosmos DB principal\nStep 3 Generate a new key in Azure Key Vault\nStep 4 Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed key (Enter Key URI), and enter the key URI","comment_id":"338956","comments":[{"content":"Perfect. Microsoft.DocumentDB Resouce Provider to be registerd and then all the steps mentioned here.","timestamp":"1622910000.0","upvote_count":"1","comment_id":"375316","poster":"vrmei"},{"timestamp":"1619374860.0","content":"this solution is correct. Check documentation here: \nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk","upvote_count":"2","comment_id":"342775","poster":"vaio"}],"timestamp":"1618851240.0","upvote_count":"37"},{"timestamp":"1617605640.0","upvote_count":"22","content":"Step 1: Create an Azure key vault and enable purge protection\nStep 2: Generate a new key in the Azure key vault\nStep 3: Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed Key (Enter key URI), and enter the key URI\nStep 4: Add an Azure Key Vault access policy to grant permissions to the Azure Cosmos DB principal","poster":"vaseva1","comments":[{"comments":[{"poster":"cadio30","content":"retracting my feedback here instead go for the solution below\n\nStep 1 Create an Azure Key vault and enable purge protection\nStep 2 Add an Azure Key Vault access policy to grant permission to the Azure Cosmos DB principal\nStep 3 Generate a new key in Azure Key Vault\nStep 4 Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed key (Enter Key URI), and enter the key URI","upvote_count":"4","comment_id":"351551","timestamp":"1620352740.0"}],"comment_id":"345892","upvote_count":"2","poster":"cadio30","content":"this make sense. checked the documentation in the url below\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk","timestamp":"1619758980.0"}],"comment_id":"328439"},{"content":"right sequence looks like this :\nStep 1 Create an Azure Key vault and enable purge protection\nStep 2 Generate a new key in Azure Key Vault\nStep 3 Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed key (Enter Key URI), and enter the key URI\nStep 4 Add an Azure Key Vault access policy to grant permission to the Azure Cosmos DB principal\n\nIn discussions there is confusion going on whether step 4 should come above step 3 etc..but unless we dont create a cosmos DB resource , how can we create key vault access policy and grant permission to cosmos DB principal.so step 4 should be last","poster":"nit687","upvote_count":"2","timestamp":"1624638060.0","comment_id":"390631"},{"comment_id":"358856","content":"Step 1:Create an Azure key vault and enable purge protection\nStep 2:Add an Azure Key Vault access policy to grant permissions to the Azure Cosmos DB principal\nStep 3:Generate a new key in the Azure key vault\nStep 4:Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed Key (Enter key URI), and enter the key URI","timestamp":"1621179240.0","poster":"hoangton","upvote_count":"1"},{"content":"Step 1: Create an Azure key vault and enable purge protection\nStep 2: Add an Azure Key Vault access policy to grant permissions to the Azure Cosmos DB principal (doesn't have to actually exist yet)\nStep 3: Generate a new key in the Azure key vault\nStep 4: Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed Key (Enter key URI), and enter the key URI\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk","comment_id":"350128","upvote_count":"2","timestamp":"1620204840.0","poster":"MMM777"},{"upvote_count":"1","poster":"dangal95","timestamp":"1619711580.0","comment_id":"345513","content":"These are the correct steps: \n\nStep 1 Create an Azure Key vault and enable purge protection\nStep 2 Generate a new key in Azure Key Vault\nStep 3 Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed key (Enter Key URI), and enter the key URI \nStep 4 Add an Azure Key Vault access policy to grant permission to the Azure Cosmos DB principal\n\nYou cannot add the key URI before you've even created the key so creating the Cosmos DB account AND inserting the key uri before the key even exists does not make sense. Also, you cannot add an access policy for a resource that does not exist yet so adding the access policy to the key vault before you even created the cosmos DB account does not make sense."},{"content":"The Cosmos DB account must be created as last step using previous created key. MS docs states that:\n\n\"When you create a new Azure Cosmos DB account from the Azure portal, choose Customer-managed key in the Encryption step. In the Key URI field, paste the URI/key identifier of the Azure Key Vault key that you copied from the previous step\"\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk","upvote_count":"10","poster":"alf99","comment_id":"328657","timestamp":"1617625860.0"}],"unix_timestamp":1617605640,"timestamp":"2021-04-05 08:54:00","isMC":false,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0011500001.jpg"],"question_text":"DRAG DROP -\nYou need to create an Azure Cosmos DB account that will use encryption keys managed by your organization.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\nSelect and Place:\n//IMG//","answer_ET":"","answer_description":"Step 1: Create an Azure key vault and enable purge protection\nUsing customer-managed keys with Azure Cosmos DB requires you to set two properties on the Azure Key Vault instance that you plan to use to host your encryption keys: Soft Delete and Purge Protection.\nStep 2: Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed Key (Enter key URI), and enter the key URI\nData stored in your Azure Cosmos account is automatically and seamlessly encrypted with keys managed by Microsoft (service-managed keys). Optionally, you can choose to add a second layer of encryption with keys you manage (customer-managed keys).\nStep 3: Add an Azure Key Vault access policy to grant permissions to the Azure Cosmos DB principal\nAdd an access policy to your Azure Key Vault instance\nStep 4: Generate a new key in the Azure key vault\nGenerate a key in Azure Key Vault\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk"}],"exam":{"isBeta":false,"isMCOnly":false,"numberOfQuestions":228,"lastUpdated":"12 Apr 2025","name":"DP-200","isImplemented":true,"provider":"Microsoft","id":65},"currentPage":11},"__N_SSP":true}