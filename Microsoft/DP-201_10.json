{"pageProps":{"questions":[{"id":"0kQepNocbxAWR3XKVEUO","choices":{"B":"No","A":"Yes"},"question_images":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are designing an Azure SQL Database that will use elastic pools. You plan to store data about customers in a table. Each record uses a value for\nCustomerID.\nYou need to recommend a strategy to partition data based on values in CustomerID.\nProposed Solution: Separate data into shards by using horizontal partitioning.\nDoes the solution meet the goal?","answer_images":[],"exam_id":66,"answer_ET":"A","unix_timestamp":1607434380,"answers_community":[],"question_id":46,"discussion":[{"content":"Yes is the answer","poster":"sjain91","upvote_count":"6","timestamp":"1619860980.0","comment_id":"346737"},{"comment_id":"388094","content":"No is the answer\nThis solution does not meet the requirements. You need to use sharding, which is partitioning data horizontally to distribute data across multiple databases in a scaled-out design, but CustomerID is not the best choice in this scenario. Sharding by RegionalID will make sorting by geographic location more efficient. \n\nSharding requires that the schema is the same on all of the databases involved. Sharding helps to minimize the size of individual databases, which in turn helps to improve transactional process performance. Hardware support requirements are minimized, which helps to reduce related costs. Elastic queries let you run queries across multiple shards. You can configure and manage sharding through the elastic database tools libraries or through self-sharding","upvote_count":"3","poster":"Pdpj","timestamp":"1624375800.0"},{"timestamp":"1607434380.0","poster":"syu31svc","comments":[{"upvote_count":"2","timestamp":"1618226520.0","comments":[{"comment_id":"357994","content":"I agree sharding based on region would be a better fit","comments":[{"upvote_count":"1","comment_id":"371711","poster":"cadio30","content":"this would only be correct if compound shard is created for customerid and region","timestamp":"1622537580.0"}],"upvote_count":"1","timestamp":"1621092120.0","poster":"Wisenut"},{"timestamp":"1621708320.0","content":"Well, the more the shards, the lesser is the likelihood of you facing the hot partition problem. The region will create a hot partition problem.","upvote_count":"3","poster":"suvenk","comment_id":"363884"}],"content":"I disagree. CustomerID will be unique and that means you would have as many shards as you have customers. This would be a poor design. The question is poorly worded and given the wording the answer might be correct, but it is lousy design.","comment_id":"333884","poster":"JohnCrawford"}],"comment_id":"238285","upvote_count":"4","content":"This is the correct solution"}],"timestamp":"2020-12-08 14:33:00","url":"https://www.examtopics.com/discussions/microsoft/view/39225-exam-dp-201-topic-1-question-6-discussion/","answer_description":"Horizontal Partitioning - Sharding: Data is partitioned horizontally to distribute rows across a scaled out data tier. With this approach, the schema is identical on all participating databases. This approach is also called ג€shardingג€. Sharding can be performed and managed using (1) the elastic database tools libraries or (2) self- sharding. An elastic query is used to query or compile reports across many shards.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-query-overview","topic":"1","answer":"A","isMC":true},{"id":"xkHwK1YL17jCgDBJmPnP","topic":"1","question_text":"HOTSPOT -\nYou are designing a data processing solution that will run as a Spark job on an HDInsight cluster. The solution will be used to provide near real-time information about online ordering for a retailer.\nThe solution must include a page on the company intranet that displays summary information.\nThe summary information page must meet the following requirements:\n✑ Display a summary of sales to date grouped by product categories, price range, and review scope.\n✑ Display sales summary information including total sales, sales as compared to one day ago and sales as compared to one year ago.\n✑ Reflect information for new orders as quickly as possible.\nYou need to recommend a design for the solution.\nWhat should you recommend? To answer, select the appropriate configuration in the answer area.\nHot Area:\n//IMG//","answers_community":[],"timestamp":"2020-04-02 08:11:00","discussion":[{"timestamp":"1585807860.0","poster":"kempstonjoystick","comment_id":"70355","upvote_count":"50","content":"The highighted answer and the explanation differ. Should be dataframe I believe."},{"poster":"apz333","content":"I think it should be dataframe as well. In most cases parquet and dataframe are the best choice.","upvote_count":"22","comment_id":"72896","timestamp":"1586509740.0","comments":[{"comment_id":"86007","content":"They say Dataset is good for complex ETL situations\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-perf","poster":"frakcha","timestamp":"1589016840.0","upvote_count":"1"}]},{"content":"1. Dataframe\n2. Parquet\nConfirmed","poster":"hsetin","comment_id":"438171","upvote_count":"1","timestamp":"1630625580.0"},{"content":"Anyone knows why Exam Topics have taken AWS certification questions offline? There is nothing related to AWS certifications which used to be there earlier.","comments":[{"upvote_count":"1","comment_id":"490672","content":"Hi, I found the link to the associate SA exam. \nhttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c02/view/","poster":"victor90","timestamp":"1638275820.0"}],"poster":"mchatrvd","upvote_count":"1","timestamp":"1628770560.0","comment_id":"423613"},{"upvote_count":"1","timestamp":"1628697360.0","poster":"satyamkishoresingh","comment_id":"423407","content":"The practical combination is Dataframe + Parquet . Here answer clarification is ambiguous."},{"timestamp":"1627389900.0","poster":"HichemZe","content":"1- DATFRAME\n2 - Data Format = Avro \nBecause only Avro support Streaming (Against Parquet)","comment_id":"415471","upvote_count":"1"},{"upvote_count":"5","timestamp":"1621579260.0","content":"Data abstraction = Dataframe","comment_id":"362731","poster":"ismaelrihawi"},{"comment_id":"360863","content":"Dataframe is correct , \nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/optimize-data-storage","poster":"BobFar","timestamp":"1621377000.0","upvote_count":"2"},{"content":"It's wrong selection shown in the display. It's actually \n- Data Frame [Reason for elimination Not as developer-friendly as DataSets, as there are no compile-time checks or domain object programming,don't need to use RDDs, unless you need to build a new custom RDD]\nAnyhow \"Parquet\" is selected","timestamp":"1613550720.0","poster":"Deepu1987","upvote_count":"1","comment_id":"292369"},{"comment_id":"239008","content":"https://docs.microsoft.com/en-us/azure/hdinsight/spark/optimize-data-storage:\n\"Parquet stores data in columnar format, and is highly optimized in Spark.\"\n\"DataFrames\nBest choice in most situations.\"","poster":"syu31svc","timestamp":"1607500140.0","upvote_count":"2"},{"content":"Dataset is not good for Aggregation, Should be dataframe.","timestamp":"1606032000.0","comment_id":"224821","poster":"BaisArun","upvote_count":"3"},{"timestamp":"1604838900.0","content":"Can some correct the answers??","comment_id":"215235","poster":"Nihar258255","upvote_count":"1"},{"content":"The question need quick processing but Dataset add overhead, also the query is aggregation and Dataset not good at that\n\nDataSets : Adds serialization/deserialization overhead, High GC overhead, Not good in aggregations where the performance impact can be considerable.\n\nDataFrames : Best choice in most situations, Direct memory access.","timestamp":"1593043260.0","comment_id":"118971","poster":"AhmedReda","upvote_count":"11"},{"upvote_count":"5","timestamp":"1591777080.0","comment_id":"106590","poster":"Runi","content":"Data set is Not good in aggregations where the performance impact can be considerable.So. I think dataframe should be correct one. Can anyone confirm. Please Thanks."},{"poster":"serger","upvote_count":"4","content":"dataframe for sure","timestamp":"1590560340.0","comment_id":"96601"},{"upvote_count":"7","timestamp":"1587653220.0","comment_id":"78677","content":"I think it's dataframe too.","poster":"Tombarc"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0005000001.png"],"question_id":47,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0005100001.png"],"answer":"","unix_timestamp":1585807860,"exam_id":66,"url":"https://www.examtopics.com/discussions/microsoft/view/17789-exam-dp-201-topic-1-question-7-discussion/","isMC":false,"answer_description":"Box 1: DataFrame -\n\nDataFrames -\nBest choice in most situations.\nProvides query optimization through Catalyst.\nWhole-stage code generation.\nDirect memory access.\nLow garbage collection (GC) overhead.\nNot as developer-friendly as DataSets, as there are no compile-time checks or domain object programming.\n\nBox 2: parquet -\nThe best format for performance is parquet with snappy compression, which is the default in Spark 2.x. Parquet stores data in columnar format, and is highly optimized in Spark.\nIncorrect Answers:\n\nDataSets -\nGood in complex ETL pipelines where the performance impact is acceptable.\nNot good in aggregations where the performance impact can be considerable.\n\nRDDs -\nYou do not need to use RDDs, unless you need to build a new custom RDD.\nNo query optimization through Catalyst.\nNo whole-stage code generation.\nHigh GC overhead.\nReference:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-perf","answer_ET":""},{"id":"71QO7bbyoZg89XBMhH3O","topic":"1","question_text":"You are evaluating data storage solutions to support a new application.\nYou need to recommend a data storage solution that represents data by using nodes and relationships in graph structures.\nWhich data storage solution should you recommend?","choices":{"B":"Azure Cosmos DB","A":"Blob Storage","D":"HDInsight","C":"Azure Data Lake Store"},"answers_community":[],"timestamp":"2020-10-28 00:31:00","discussion":[{"timestamp":"1621579320.0","upvote_count":"7","content":"CosmosDB with Gremlin API","comment_id":"362732","poster":"ismaelrihawi"},{"content":"the proposed solution is correct as the azure cosmos db has gremlin api that can support the graph requirement.\n\nreference: https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction","timestamp":"1621481520.0","upvote_count":"4","comment_id":"361807","poster":"cadio30"},{"content":"No other option supports Graph Structures. So it should be only Azure Cosmos DB.","timestamp":"1621151580.0","upvote_count":"1","comment_id":"358405","poster":"IAMKPR"},{"upvote_count":"1","comment_id":"353349","poster":"NamishBansal","content":"101% correct","timestamp":"1620605700.0"},{"poster":"syu31svc","timestamp":"1607167860.0","comment_id":"235645","content":"It can only be B","upvote_count":"2"},{"comment_id":"224820","timestamp":"1606031940.0","upvote_count":"2","poster":"BaisArun","content":"yes agree with Cosmos DB"},{"content":"Gremlin - API","poster":"Rajdeep_Chakraborty","timestamp":"1604639460.0","upvote_count":"3","comment_id":"213860"},{"timestamp":"1603841460.0","upvote_count":"4","comment_id":"207344","content":"I agree with the answer - Cosmos DB.","poster":"Andrexx"}],"question_images":[],"question_id":48,"unix_timestamp":1603841460,"answer_images":[],"exam_id":66,"answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/35326-exam-dp-201-topic-1-question-8-discussion/","isMC":true,"answer_description":"For large graphs with lots of entities and relationships, you can perform very complex analyses very quickly. Many graph databases provide a query language that you can use to traverse a network of relationships efficiently.\nRelevant Azure service: Cosmos DB\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview","answer_ET":"B"},{"id":"4Obb2vbQQiLf7zPgs05V","question_id":49,"answer_description":"Box 1: Hash-distributed -\n\nBox 2: ProductKey -\nProductKey is used extensively in joins.\nHash-distributed tables improve query performance on large fact tables.\n\nBox 3: Round-robin -\n\nBox 4: RegionKey -\nRound-robin tables are useful for improving loading speed.\nConsider using the round-robin distribution for your table in the following scenarios:\n✑ When getting started as a simple starting point since it is the default\n✑ If there is no obvious joining key\n✑ If there is not good candidate column for hash distributing the table\n✑ If the table does not share a common join key with other tables\n✑ If the join is less significant than other joins in the query\n✑ When the table is a temporary staging table\nNote: A distributed table appears as a single table, but the rows are actually stored across 60 distributions. The rows are distributed with a hash or round-robin algorithm.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-distribute","answer":"","isMC":false,"timestamp":"2021-03-11 19:40:00","url":"https://www.examtopics.com/discussions/microsoft/view/46530-exam-dp-201-topic-1-question-9-discussion/","topic":"1","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0005500001.png"],"question_text":"HOTSPOT -\nYou have an on-premises data warehouse that includes the following fact tables. Both tables have the following columns: DataKey, ProductKey, RegionKey.\nThere are 120 unique product keys and 65 unique region keys.\n//IMG//\n\nQueries that use the data warehouse take a long time to complete.\nYou plan to migrate the solution to use Azure Synapse Analytics. You need to ensure that the Azure-based solution optimizes query performance and minimizes processing skew.\nWhat should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"content":"Table sales:\n**Distribution type: Hash-Distributed \nFor 2 Reasons: the table is 600GB and we want to optimize queries\n**Distribution column: \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\n=> Product key is the only possible correct choice\nTable Invoices:\n**Distribution type: Hash-Distributed\nThe table size is more than 2GB and probably growing up.\nConsider using a hash-distributed table when:The table size on disk is more than 2 GB. And The table has frequent insert, update, and delete operations.\n**Distribution column: for sure it’s regionkey To minimize data movement, select a distribution column that:… same link","poster":"H_S","comment_id":"310058","timestamp":"1615676940.0","upvote_count":"31"},{"upvote_count":"9","timestamp":"1621483080.0","poster":"cadio30","content":"Distribution for both should be \"hash-distributed\" as we are talking about fact tables while round-robin is mostly use in staging tables. As a rule of the thumb when using hash-distributed it should be applied in the columns that uses JOIN, GROUP BY, DISTINCT, OVER, and HAVING and one shouldn't apply it in WHERE and DATE columns.\n\nSales: Hash-distributed, ProductKey\nInvoices: Hash-distributed, RegiongKey\n\nReference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","comment_id":"361816"},{"upvote_count":"2","poster":"Qrm_1972","timestamp":"1623352140.0","content":"The correct answer is: Sales : Hash-Distributed>>>> Product Key\n Invoices: Hash-Distributed>>>> Region Key","comment_id":"379225"},{"content":"i think Distribution should be : \"HASH-DISTRIBUTION\" as both are Fact tables and ProductKey for sales and Region Key for Invoices .","timestamp":"1621837920.0","comment_id":"365270","upvote_count":"2","poster":"SrinivasR"},{"comment_id":"361069","content":"I don't think there is a distribution column option for Round Robin. The distribution column is available only for Hash Partitioning. So it must be Hash Partitioning & Region Key for Invoice table.","upvote_count":"2","timestamp":"1621405200.0","poster":"NarenG1"},{"comment_id":"355653","upvote_count":"2","content":"As there are some different answers for table invoices.\nFor sure hash-distributed, as the table size is more than 2 GB.\nExplanation for RegionKey:\nTo minimize data movement, select a distribution column that:\nIs used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses. When two large fact tables have frequent joins, query performance improves when you distribute both tables on one of the join columns. When a table is not used in joins, consider distributing the table on a column that is frequently in the GROUP BY clause.\nIs not used in WHERE clauses. This could narrow the query to not run on all the distributions.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","poster":"DataDani","timestamp":"1620837840.0"},{"timestamp":"1615642620.0","poster":"Geo_Barros","upvote_count":"4","content":"I think that the right answer for the ditribution type at the invoice table would be hash-distributed with regionkey as the distributed key as it is used for grouping.","comment_id":"309729"},{"comments":[{"timestamp":"1618228140.0","upvote_count":"4","poster":"JohnCrawford","comment_id":"333910","content":"From the provided link we learn that generally we should not use date values as the partitioning key. As noted by H_S the Invoices table is large enough to warrant being hash distributed as well and as noted by you, Mariekumi, RegionKey would result in hot spots/skew. I think hash distributed on product key for both tables makes the most sense."}],"content":"I would say Hash distributed and Date key for both tables because date key is used extensively in queries in both tables, region key will result in skewed partitioning as 75% of data falls in one region. Also Hash is best for both because we are optimizing query performance and not loading which Round-Robin is best suited for","timestamp":"1615580640.0","upvote_count":"5","comment_id":"309143","poster":"Mariekumi"},{"timestamp":"1615488000.0","comment_id":"308215","upvote_count":"1","poster":"akram786","content":"why round robin for invoices."}],"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0005300001.png","https://www.examtopics.com/assets/media/exam-media/03774/0005400001.png"],"exam_id":66,"answer_ET":"","unix_timestamp":1615488000},{"id":"dvDfGd2kTFkwc2ZDLwRx","question_id":50,"answer_description":"Box 1: 8080 -\n1433 is the default port, but we must change it as CONT_SQL3 must not communicate over the default ports. Because port 1433 is the known standard for SQL\nServer, some organizations specify that the SQL Server port number should be changed to enhance security.\nBox 2: SQL Server Configuration Manager\nYou can configure an instance of the SQL Server Database Engine to listen on a specific fixed port by using the SQL Server Configuration Manager.\nReference:\nhttps://docs.microsoft.com/en-us/sql/database-engine/configure-windows/configure-a-server-to-listen-on-a-specific-tcp-port?view=sql-server-2017\nDesign for data security and compliance","answer":"","isMC":false,"timestamp":"2020-05-25 16:29:00","url":"https://www.examtopics.com/discussions/microsoft/view/21305-exam-dp-201-topic-10-question-1-discussion/","topic":"10","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0022700001.png"],"question_text":"HOTSPOT -\nYou need to design network access to the SQL Server data.\nWhat should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0022600005.png"],"discussion":[{"comment_id":"95457","content":"It seems the title of the boxes are swapped. Box 1 should be SQL Server Network Configuration Port and Box 2 should be Tool","timestamp":"1590416940.0","upvote_count":"21","poster":"Nits0211","comments":[{"content":"Yes you are right.","timestamp":"1591384620.0","upvote_count":"3","comment_id":"103363","poster":"thukza"}]},{"upvote_count":"1","comment_id":"380147","poster":"Mandar77","timestamp":"1623465900.0","content":"Question is ambiguous as there are 3 SQL databases having SQL data. which one are they talking about?"},{"upvote_count":"1","content":"I think is SSMS and 1433 port","poster":"AngelRio","comment_id":"364219","timestamp":"1621753440.0"},{"poster":"PasswordPassword","upvote_count":"4","content":"SQL Server listens on port 1433","timestamp":"1595872140.0","comment_id":"145190","comments":[{"timestamp":"1598285460.0","comments":[{"comment_id":"386307","content":"AZURE SQL DB not SQL VM","timestamp":"1624198020.0","poster":"chakanirban","upvote_count":"1"}],"content":"8080 because of this: \"CONT_SQL3 must not communicate over the default ports\".","upvote_count":"14","poster":"Ikrom","comment_id":"165380"},{"poster":"anamaster","upvote_count":"2","timestamp":"1619275620.0","content":"and 1433 is the default for sql server","comment_id":"342072"}]},{"poster":"pravinDataSpecialist","comment_id":"115631","timestamp":"1592749260.0","content":"yes that confused me a little .. boxes should be swapped and the question will make sense","upvote_count":"2"}],"exam_id":66,"answer_ET":"","unix_timestamp":1590416940}],"exam":{"id":66,"isImplemented":true,"isMCOnly":false,"name":"DP-201","numberOfQuestions":206,"lastUpdated":"12 Apr 2025","provider":"Microsoft","isBeta":false},"currentPage":10},"__N_SSP":true}