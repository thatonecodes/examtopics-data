{"pageProps":{"questions":[{"id":"RZauT6yJG463QRrEmJ34","choices":{"A":"Create a shortcut in the Files section.","D":"Create a shortcut in the Tables section.","E":"Use the delta format.","B":"Use the Parquet format","C":"Use the CSV format."},"timestamp":"2024-02-09 20:50:00","discussion":[{"timestamp":"1709127840.0","poster":"Felgas","upvote_count":"31","content":"Answer is DE.\nTo be able to use the SQL Endpoint you need to create the shortcut in the Tables section. The file also needs to be in the delta format to be recognised as a managed table. If you try to add a parquet file to the tables section, it will not be recognised as a table object and you won't be able to query it.","comments":[{"timestamp":"1713961980.0","content":"A parquet file shortcut is recognized as a managed table. Delta format not supported by S3. The answer cannot be E.","poster":"d47320d","comment_id":"1201359","upvote_count":"10","comments":[{"timestamp":"1742116620.0","upvote_count":"1","comment_id":"1399177","poster":"kilowd","content":"E: Only the tables in Delta format are available in The SQL analytics endpoint. Parquet, csv and other formats cannot be queried using SQL analytics endpoint."}]}],"comment_id":"1161671"},{"comment_id":"1201358","content":"B,D are the correct answers, since the Parquet file format within S3, and the pointing to it via a lakehouse Table shortcut, allows the shorcut to be recognized by Fabric as a Delta table and hence provide access to the SQL endpoint.\n\n\nExplanation:\nAs per the provided links: \"If the target of the shortcut contains data in the Delta\\Parquet format, the lakehouse automatically synchronizes the metadata and recognizes the folder as a table\".\nIt is stated that S3 contains sales data files, that is files not tables. Besides, delta tables are not supported in S3. So the file format needs to be either CSV or Parquet, leading us to choose the latter for all the reasons stated in other comments.\nSo creating a shortcut in Table section that points to S3 Parquet files, will allow Fabric to recognise it as a Delta table, which in turn enables the SQL endpoint.","timestamp":"1713961920.0","comments":[{"content":"In the question, it is not mentioned as a Parquet file. so we may not forcefully use the Parquet format.","upvote_count":"1","poster":"uvan","timestamp":"1735031160.0","comment_id":"1331059"}],"upvote_count":"22","poster":"d47320d"},{"content":"Selected Answer: DE\nSQL Endpoint you need to create the shortcut in the Tables section.","upvote_count":"1","timestamp":"1742181900.0","comment_id":"1399514","poster":"Amine_spiegel94"},{"poster":"Egocentric","content":"Selected Answer: BD\nB.use parquet format\nD.create a shortcut in the table section","timestamp":"1736872080.0","comment_id":"1340434","upvote_count":"1"},{"content":"Selected Answer: DE\nD. Create a shortcut in the Tables section.\nE. Use the delta format","poster":"NRezgui","comment_id":"1331704","timestamp":"1735158780.0","upvote_count":"1"},{"content":"Selected Answer: BD\nB OR E (Use the Parquet format or Use the delta format)\nD Create a shortcut in the Tables section","poster":"Mhunity","upvote_count":"1","timestamp":"1733294640.0","comment_id":"1321748"},{"comment_id":"1312451","poster":"Rakesh16","upvote_count":"1","content":"Selected Answer: BD\nB & D is the answer","timestamp":"1731650760.0"},{"timestamp":"1725541560.0","upvote_count":"3","comment_id":"1278944","content":"Delta uis supported:\nhttps://blog.fabric.microsoft.com/en-us/blog/public-preview-of-onelake-shortcuts-to-s3-compatible-data-sources?ft=All\n\n\"\"If your data is already in the Delta Lake format, create your shortcut in the Tables section of your lakehouse. This will allow your table shortcut to benefit from metadata synchronization across Fabric engines, letting you use this structured data where tables are used in Fabric.\"\"","poster":"moodi86"},{"timestamp":"1722307380.0","poster":"Pegooli","content":"Selected Answer: BD\nchat GBT also agree with BD","comment_id":"1257793","upvote_count":"2"},{"content":"Selected Answer: CD\nCSV is best if the data is not that large","upvote_count":"2","comment_id":"1252390","poster":"Ziggler","timestamp":"1721559600.0"},{"poster":"Ziggler","comment_id":"1252389","upvote_count":"1","content":"There is no mention of how big the data is , using CSV will be better if the data is small","timestamp":"1721559480.0"},{"comment_id":"1249344","poster":"Pegooli","content":"ChatGPT is saying amazon S3 supports both Delta and Parquet files","timestamp":"1721178660.0","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: BD\nB- Because Parquet format is supported in Amazon S3\nhttps://learn.microsoft.com/en-us/azure/data-factory/format-parquet\nD- Because you need to use the Shortcuts as managed portion of the lakehouse.","comment_id":"1245781","timestamp":"1720655160.0","poster":"6d1de25"},{"comment_id":"1232421","content":"I guess MS documentation says Delta rather than Parquet? https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint","timestamp":"1718714760.0","upvote_count":"1","poster":"OLGIS"},{"poster":"woliveiras","comment_id":"1229606","upvote_count":"5","timestamp":"1718250540.0","content":"Selected Answer: DE\nD and E. Tested. You can turn delta files available in your AWS Service and create a shortcut here."},{"comment_id":"1225887","content":"why not 'A'? The shortcut will be reading files from the S3 instead of tables.","poster":"sharnav","upvote_count":"3","timestamp":"1717729680.0"},{"content":"Selected Answer: BD\nTo effectively query sales data files stored in an Amazon S3 bucket using the SQL endpoint in your Fabric tenant's lakehouse, you should:\nB. Use the Parquet format\nD. Create a shortcut in the Tables section\n\n**Creating a shortcut in the Files section would treat the data more like raw files, which might not leverage the full potential of SQL querying capabilities. Creating the shortcut in the Tables section aligns more closely with the structured query requirements and provides a better-integrated experience.","upvote_count":"3","comment_id":"1219865","timestamp":"1716848340.0","poster":"282b85d"},{"poster":"282b85d","content":"Selected Answer: BD\nTo effectively query sales data files stored in an Amazon S3 bucket using the SQL endpoint in your Fabric tenant's lakehouse, you should:\nB. Use the Parquet format\nD. Create a shortcut in the Tables section\n**Creating a shortcut in the Files section would treat the data more like raw files, which might not leverage the full potential of SQL querying capabilities. Creating the shortcut in the Tables section aligns more closely with the structured query requirements and provides a better-integrated experience.","upvote_count":"2","timestamp":"1716848280.0","comment_id":"1219863"},{"poster":"woliveiras","timestamp":"1716176100.0","comment_id":"1214094","upvote_count":"4","content":"Selected Answer: BD\nIf you need just query, as they mentioned. Parquet (read in parallel) and shortcut in the table solve this problem. you don't need delta."},{"upvote_count":"2","comment_id":"1208575","poster":"stilferx","timestamp":"1715206740.0","content":"Selected Answer: DE\nIMHO, D & E\n\nBecause of that: Shortcuts aren't supported in other subdirectories of the Tables folder. If the target of the shortcut contains data in the Delta\\Parquet format, the lakehouse automatically synchronizes the metadata and recognizes the folder as a table.\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#lakehouse"},{"timestamp":"1714262340.0","comment_id":"1203323","content":"Selected Answer: BD\nFor querying sales data files using the SQL endpoint with files stored in an Amazon S3 storage bucket, I recommend the following two actions:\n\nFile Format: Choose the Apache Parquet file format for the sales data files. Parquet is a columnar storage file format that is optimized for complex queries and is efficient for use with SQL queries due to its data compression and encoding schemes. It supports complex data types and is ideal for large datasets1.\nShortcut Creation: Create a shortcut in the Lakehouse Explorer under the Tables section. When you create a shortcut to a Delta formatted table under Tables in Lakehouse Explorer, it will automatically register it as a table, enabling data access through Spark, SQL endpoint, and the default semantic model2.\nThese actions will help ensure efficient querying and easy accessibility of your sales data within the Fabric tenant’s lakehouse environment.","upvote_count":"2","poster":"Estratech"},{"poster":"Azure_2023","timestamp":"1714044540.0","content":"- E and D\nE. Use the delta format - Choosing Delta format is most suitable for complex environments that benefit from features like ACID transactions, schema enforcement, and historical data tracking. This choice is particularly effective in lakehouse architectures where maintaining data integrity and supporting complex queries are paramount.\nD. Create a shortcut in the Tables section - Given the structured nature of Delta format files and the need to perform SQL queries, creating a shortcut in the Tables section is most beneficial. This approach allows the SQL endpoint to efficiently query the data using table semantics, leveraging the optimizations provided by the lakehouse.","upvote_count":"1","comment_id":"1201936"},{"timestamp":"1712563200.0","content":"I just tested this. Have a S3 bucket with a CSV and a parquet file. Created a shortcut, and tried it to place it in tables. That was not successful. It asked me to move it to Files. So answer to the first part must me A. Create a shortcut in the Files section\n\nBoth csv and parquet can be loaded as table. That is part of the job. The result is that both will result in a delta table, and it is delta tables that can be viewed in SQL endpoints. So the second answer must be E. Use the delta format.","upvote_count":"2","poster":"gfors","comment_id":"1191453"},{"content":"S3 does support delta parquet: https://aws.amazon.com/blogs/big-data/introducing-native-delta-lake-table-support-with-aws-glue-crawlers/\nAnswer: DE","comment_id":"1190108","upvote_count":"2","timestamp":"1712352960.0","poster":"554b579"},{"poster":"bar_ser","content":"https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#lakehouse\n\nWithin this link, it's clear that you can link to Parquet.\n\"If the target of the shortcut contains data in the Delta\\Parquet format, the lakehouse automatically synchronizes the metadata and recognizes the folder as a table\"","upvote_count":"1","timestamp":"1711664160.0","comment_id":"1185065"},{"timestamp":"1711663440.0","poster":"bar_ser","content":"S3 does not support delta.\nhttps://productresources.collibra.com/docs/collibra/latest/Content/Catalog/S3/ref-S3-supported-file-types.htm\n\nSo E is not an option","upvote_count":"1","comments":[{"content":"based on your link why not CSV?","timestamp":"1721178600.0","poster":"Pegooli","upvote_count":"1","comment_id":"1249341"},{"poster":"a_51","upvote_count":"1","timestamp":"1711731300.0","content":"If you Load to Table the shortcut, then that creates a delta format table. I think the question is not clear exactly what is desired, but in order to show in the SQL Endpoint that is the type of step need. Many are saying not A, but if you actually try to do that setup you have to start with A as you cannot create in tables directly, after loading it move to tables. However, what is exposed in tables is what SQL endpoint knows of and where I think it could be D and E as answers. As the test has been in beta could just be a bad question.","comment_id":"1185590"}],"comment_id":"1185060"},{"timestamp":"1710084960.0","upvote_count":"2","poster":"azure_bimonster","content":"Selected Answer: DE\nD and E seem correct choices","comment_id":"1170415"},{"timestamp":"1709107140.0","upvote_count":"3","content":"Selected Answer: DE\n\"Only the tables in Delta format are available in the SQL analytics endpoint. Parquet, CSV, and other formats can not be queried using the SQL analytics endpoint. If you don't see your table, you will need convert it to Delta format.\" \n\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-overview","poster":"Blue_MSBI","comment_id":"1161379"},{"comment_id":"1161224","timestamp":"1709090760.0","content":"Selected Answer: BD\nB - Parquet is a widely accepted format for efficient querying. Delta files are usually good with Spark based querying rather than SQL endpoint based querying. While delta format is highly useful for some usecases like time traveling, incremental loads and data versioning, for simple and high efficiency querying, parquet would be the best.\n\nD - A shortcut can be created in the Tables section of the Lakehouse and not the files section.","upvote_count":"2","comments":[{"timestamp":"1709582820.0","comment_id":"1165956","upvote_count":"1","content":"Based on the below references, I go with DE too:\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-shortcuts\nDelta format is the most optimized version of the managed tables which fall under the Tables section of the Lakehouse\nAlthough shortcuts can be created at both the Table and Files level, adding directly to the Table level will enable us to utilize multiple functionalities provided by Fabric like notebooks and SQL endpoint which can be leveraged through the managed tables in Tables section.","poster":"sraakesh95","comments":[{"upvote_count":"1","poster":"a_51","timestamp":"1711026540.0","content":"I am not sure this question is great at all. If you create the shortcut in Files, then you will load it to the lakehouse and will make a table for it and that will be what is exposed for sql endpoint and will be a delta table. So with the options requires some other steps really to make happen.","comment_id":"1179256"},{"upvote_count":"1","poster":"4fd861f","content":"And as Delta is built on top of parquet it is ok to go for D","comment_id":"1179259","timestamp":"1711026660.0"}]}],"poster":"sraakesh95"},{"comment_id":"1159048","timestamp":"1708889040.0","upvote_count":"2","content":"Selected Answer: DE\nI read a requirement to query with SQL analytic endpoint. \nI'm sure, that recommended Microsoft solution then is: create a shortcut in the Tables section in Delta 'format', to benefit from transactional operations and schema evolution.\nThe File section is more suitable for unstructured data that doesn't require full SQL querying.\nParquet and csv lack the support of schema evolution.","poster":"estrelle2008"},{"comment_id":"1152927","content":"Selected Answer: BD\nI think its BD.\nDelta is not file format, lakehouse will read parquet files and create delta table.","poster":"Momoanwar","timestamp":"1708213140.0","upvote_count":"2"},{"comments":[{"comment_id":"1152772","upvote_count":"2","content":"Two things make the Parquet format a better choice: 1. the question says Query, and 2. It is a shortcut to S3 therefore you don't need the bells and whistles of the Delta format.","poster":"Bharat","timestamp":"1708196160.0"}],"comment_id":"1149581","poster":"Nicofr","content":"Selected Answer: DE\ndelta format like the default format for Fabric","upvote_count":"4","timestamp":"1707858300.0"},{"timestamp":"1707508200.0","poster":"IshtarSQL","upvote_count":"3","content":"Selected Answer: BD\nB: You should use a columnar file format such as Parquet or ORC (Optimized Row Columnar). These formats are highly optimized for analytical queries and provide efficient storage and query performance.\nD: In the Tables section of your lakehouse, you define virtual tables that represent external data sources. These virtual tables can be backed by data stored externally in formats such as Parquet or ORC in Amazon S3.","comment_id":"1145748"}],"url":"https://www.examtopics.com/discussions/microsoft/view/133467-exam-dp-600-topic-1-question-36-discussion/","question_id":111,"answers_community":["BD (54%)","DE (42%)","4%"],"isMC":true,"topic":"1","answer_images":[],"question_images":[],"answer":"BD","answer_ET":"BD","question_text":"You have a Fabric tenant that contains a lakehouse.\nYou plan to query sales data files by using the SQL endpoint. The files will be in an Amazon Simple Storage Service (Amazon S3) storage bucket.\nYou need to recommend which file format to use and where to create a shortcut.\nWhich two actions should you include in the recommendation? Each correct answer presents part of the solution.\nNOTE: Each correct answer is worth one point.","answer_description":"","unix_timestamp":1707508200,"exam_id":71},{"id":"3JepA7H6TcB2hv1GfMrM","answer_description":"","question_id":112,"answer":"A","unix_timestamp":1707509220,"question_text":"You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a subfolder named Subfolder1 that contains CSV files.\nYou need to convert the CSV files into the delta format that has V-Order optimization enabled.\nWhat should you do from Lakehouse explorer?","answer_images":[],"choices":{"C":"Create a new shortcut in the Tables section.","A":"Use the Load to Tables feature.","D":"Use the Optimize feature.","B":"Create a new shortcut in the Files section."},"exam_id":71,"topic":"1","timestamp":"2024-02-09 21:07:00","url":"https://www.examtopics.com/discussions/microsoft/view/133468-exam-dp-600-topic-1-question-37-discussion/","discussion":[{"timestamp":"1708384440.0","poster":"Sanji931","upvote_count":"11","content":"Selected Answer: A\nWith ''Load to tables'' : tables are always loaded using the Delta Lake table format with V-Order optimization enabled.\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/load-to-tables#load-to-table-capabilities-overview","comment_id":"1154345"},{"comment_id":"1312452","content":"Selected Answer: A\nUse the Load to Tables feature.","poster":"Rakesh16","upvote_count":"1","timestamp":"1731650820.0"},{"upvote_count":"1","poster":"cafb698","comment_id":"1273597","timestamp":"1724778660.0","content":"Selected Answer: A\nA, since optimize alone won't convert the CSV to Delta Format"},{"timestamp":"1724778600.0","comment_id":"1273596","content":"Selected Answer: D\nOptimize feature alone does not convert CSV files to Delta format. It is used for optimizing Delta tables after they have been created.\n\nTo fully address the requirement of converting CSV files into Delta format with V-Order optimization, the correct approach involves:\n\nLoading CSV files into a Delta table: This is typically done using features that allow for the import and conversion of data into Delta format.\nApplying V-Order optimization: Once the data is in Delta format, you use the Optimize feature to apply V-Order.\nHence, D","poster":"cafb698","upvote_count":"1"},{"timestamp":"1724137440.0","poster":"Training_ND","comment_id":"1269241","content":"Selected Answer: D\nThe Optimize feature in a Lakehouse environment is specifically designed to optimize data, including converting it into more efficient formats like Delta and applying optimizations like V-Order. This feature is what you would use to convert CSV files into the Delta format and enable V-Order optimization, which reorganizes the data to improve query performance.","upvote_count":"1"},{"timestamp":"1717336860.0","poster":"byyleoo","content":"Selected Answer: A\nA is correct","comment_id":"1223171","upvote_count":"2"},{"comment_id":"1219868","upvote_count":"2","poster":"282b85d","content":"Selected Answer: A\nThe \"Load to Tables\" feature in Lakehouse explorer allows you to import data from various file formats (such as CSV) into a table within the lakehouse. During this process, you can specify the file format for the table, and by choosing the delta format, you can enable optimizations like V-Order.","timestamp":"1716848520.0"},{"upvote_count":"2","comment_id":"1208606","content":"Selected Answer: A\nIMHO, \"A\" is right.\n\nBecause: The Lakehouse in Microsoft Fabric provides a feature to efficiently load common file types to an optimized Delta table ready for analytics. The Load to Table feature allows users to load a single file or a folder of files to a table. \n\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/load-to-tables","poster":"stilferx","timestamp":"1715215860.0"},{"poster":"prabhjot","timestamp":"1712329980.0","content":"I think Optimize ( Ans D) - https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql","upvote_count":"2","comment_id":"1189981"},{"poster":"XiltroX","upvote_count":"4","timestamp":"1709089080.0","content":"Selected Answer: A\nA is the correct answer.","comment_id":"1160814"},{"upvote_count":"3","timestamp":"1708213200.0","poster":"Momoanwar","content":"Selected Answer: A\nCorrect","comment_id":"1152928"},{"upvote_count":"2","comment_id":"1149586","content":"Selected Answer: A\n\"Load to Tables\" functionality, the Optimize check mark is set by default.","timestamp":"1707858420.0","poster":"Nicofr"},{"comment_id":"1149032","poster":"fabric1","content":"Selected Answer: A\nHi IshtarSQL, \nthe \"Optimize\" feature is only applicable on already existing tables and cannot convert CSV files as far as I know. When loading a CSV file as table using the \"Load to Tables\" functionality, the Optimize check mark is set by default. Therefore, A should be correct. \nCheers fabric1","timestamp":"1707816360.0","upvote_count":"3"},{"comment_id":"1145760","timestamp":"1707509220.0","poster":"IshtarSQL","content":"Selected Answer: D\nThe \"New Optimize\" feature in the Lakehouse Explorer to convert CSV files into Delta format with V-Order optimization enabled.\nThe \"New Optimize\" feature allows you to optimize your data in Delta Lake format, including enabling V-Order optimization. V-Order optimization improves query performance by organizing data according to the values of frequently queried columns.","upvote_count":"1"}],"answer_ET":"A","isMC":true,"answers_community":["A (91%)","9%"],"question_images":[]},{"id":"q2zWkhFB8N8JxwVgRo23","exam_id":71,"topic":"1","question_images":[],"answer_images":[],"answers_community":["D (78%)","14%","6%"],"answer_description":"","answer_ET":"D","question_id":113,"discussion":[{"poster":"SamuComqi","content":"Selected Answer: D\nD. From the Destination tabs, set Mode to Overwrite.\n\nWhen setting up the Copy Activity, you need to choose the Overwrite mode to make the partition option appear (not visibile in Append mode).","comment_id":"1153108","timestamp":"1708243800.0","upvote_count":"15"},{"timestamp":"1713770340.0","upvote_count":"7","content":"Selected Answer: D\nD. From the Destination tabs, set Mode to Overwrite.\n\nhttps://learn.microsoft.com/en-us/fabric/data-factory/tutorial-lakehouse-partition#load-data-to-lakehouse-using-partition-columns\n\nExpand Advanced, in Table action, select Overwrite, and then select Enable partition, under Partition columns, select Add column, and choose the column you want to use as the partition column. You can choose to use a single column or multiple columns as the partition column.","poster":"VAzureD","comment_id":"1200021"},{"timestamp":"1734693540.0","upvote_count":"1","comment_id":"1329417","comments":[{"timestamp":"1734934620.0","poster":"emboutchamani","content":"correction: D","upvote_count":"1","comment_id":"1330689"}],"poster":"emboutchamani","content":"Selected Answer: A\nWhat should you do first ?\nBefore setting partition you have to set mode toappend after you can select partition column"},{"content":"Selected Answer: D\nFrom the Destination tabs, set Mode to Overwrite.","comment_id":"1312453","poster":"Rakesh16","upvote_count":"1","timestamp":"1731650820.0"},{"poster":"Brainny","content":"C. The \"first\" thing to do is to Enable Partition Discovery from the Source tab, this will enable the destination tab identify the partition column, then the destination tab can be set to overwrite.\nhttps://learn.microsoft.com/en-us/fabric/data-factory/connector-lakehouse-copy-activity","comment_id":"1280773","upvote_count":"1","timestamp":"1725865500.0","comments":[{"upvote_count":"1","poster":"a2675f7","comment_id":"1302869","content":"only when the source table is partitioned first... Enable partition discovery: For files that are partitioned, specify whether to parse the partitions from the file path and add them as extra source columns, in this case source is unpartitioned","timestamp":"1729861260.0"}]},{"comment_id":"1257795","poster":"Pegooli","timestamp":"1722308040.0","content":"Selected Answer: C\nEnabling partition discovery on the Source tab allows the Copy activity to recognize the partition structure of the incoming data. This step is essential because it ensures that the data is copied into the destination table (Table1) with the correct partitions based on the specified date column.","upvote_count":"1"},{"poster":"dev2dev","content":"Selected Answer: D\nthose who still think B is the answer, emphasis on \"What should you do first?\"","comment_id":"1236089","upvote_count":"1","timestamp":"1719194100.0"},{"comment_id":"1235585","timestamp":"1719087780.0","content":"Selected Answer: D\nD. From the Destination tabs, set Mode to Overwrite.\n\nin the question, it is specified that \"we are going to specify the partition column\". Yes, we will do that, BUT before that we have to set the \"Table action\" option to \"Overwrite\".\n\nsay, if you first set the partition column & after that you select the \"overwrite\" mode, then the selected partition column will be cleared & you will have to select that again.\n\nAnd \"overwrite\" is necessary because the table is not currently partitioned. we need to re-write the existing data leveraging partition.","upvote_count":"1","poster":"Data_Works"},{"upvote_count":"3","poster":"282b85d","timestamp":"1716849180.0","comment_id":"1219872","content":"Selected Answer: C\nEnable Partition Discovery: This option ensures that the Copy activity can identify the partition column and apply it correctly to the destination table. By enabling partition discovery, you make sure that the source data's partitioning information is considered during the copy process.\nWhy Not: \nA. From the Destination tab, set Mode to Append: This setting controls how data is added to the existing table (whether new data is appended or existing data is overwritten). It does not directly address partitioning setup.\nB. From the Destination tab, select the partition column: While this is necessary to specify the partition column, it is logically the next step after enabling partition discovery. The system needs to recognize partitions before you can configure them.\nD. From the Destination tabs, set Mode to Overwrite: Similar to the Append mode, this setting determines how data is handled during the copy process but does not enable partitioning."},{"content":"Selected Answer: D\nPartition option need to switch to Overwrite mode first.","comment_id":"1214439","poster":"David_Webb","timestamp":"1716216300.0","upvote_count":"1"},{"poster":"stilferx","comment_id":"1208617","content":"Selected Answer: D\nIMHO, the answer is \"D\".\n\nBecause:\nExpand Advanced, in Table action, select Overwrite, and then select Enable partition, under Partition columns, select Add column, and choose the column you want to use as the partition column. You can choose to use a single column or multiple columns as the partition column.\n\nin https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-lakehouse-partition#load-data-to-lakehouse-using-partition-columns","timestamp":"1715216700.0","upvote_count":"4"},{"timestamp":"1713167100.0","comment_id":"1195884","poster":"emmanuelkech","content":"Selected Answer: B\nFrom the Destination tab, select the partition column. This ensures that the data is partitioned based on the specified column in the destination table","upvote_count":"3"},{"upvote_count":"6","timestamp":"1708892220.0","poster":"estrelle2008","comment_id":"1159139","content":"Selected Answer: D\nA makes no sense, it leaves data that is already in table1 unpartitioned\nB makes no sense, it is the statement that you are asked to decide for the step before that. \nC MIGHT assign date column in source data as requested partition column, but only and if source data is complete and unambiguous\nD re-creates table1, ensuring that the partitioning is applied consistently.\n\nHoping that the data that is already in table1 is also in this source data, D should be recommended answer, I guess?"},{"comment_id":"1153980","content":"Selected Answer: D\nD is correct. Partition is available when Overwrite is checked","poster":"lengzhai","upvote_count":"4","timestamp":"1708351920.0"},{"content":"Selected Answer: C\nI think its C. For partition in pipeline we have to specify column in source","poster":"Momoanwar","comment_id":"1152377","comments":[{"comment_id":"1152773","content":"The table is unpartitioned to begin with.","timestamp":"1708196280.0","poster":"Bharat","upvote_count":"1"}],"timestamp":"1708147140.0","upvote_count":"3"}],"choices":{"B":"From the Destination tab, select the partition column.","C":"From the Source tab, select Enable partition discovery.","D":"From the Destination tabs, set Mode to Overwrite.","A":"From the Destination tab, set Mode to Append."},"answer":"D","unix_timestamp":1708147140,"question_text":"You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains an unpartitioned table named Table1.\nYou plan to copy data to Table1 and partition the table based on a date column in the source data.\nYou create a Copy activity to copy the data to Table1.\nYou need to specify the partition column in the Destination settings of the Copy activity.\nWhat should you do first?","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/134050-exam-dp-600-topic-1-question-38-discussion/","timestamp":"2024-02-17 06:19:00"},{"id":"BH0vMXPRnbVVra3MoZGy","question_images":[],"topic":"1","answers_community":["B (79%)","A (21%)"],"answer_description":"","answer_ET":"B","answer":"B","answer_images":[],"unix_timestamp":1708206240,"url":"https://www.examtopics.com/discussions/microsoft/view/134088-exam-dp-600-topic-1-question-4-discussion/","question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nContoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.\n\nExisting Environment -\n\nIdentity Environment -\nContoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.\n\nData Environment -\nContoso has the following data environment:\nThe Sales division uses a Microsoft Power BI Premium capacity.\nThe semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.\nThe Research department uses an on-premises, third-party data warehousing product.\nFabric is enabled for contoso.com.\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.\nA Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.\n\nRequirements -\n\nPlanned Changes -\nContoso plans to make the following changes:\nEnable support for Fabric in the Power BI Premium capacity used by the Sales division.\nMake all the data for the Sales division and the Research division available in Fabric.\nFor the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.\nIn Productline1ws, create a lakehouse named Lakehouse1.\nIn Lakehouse1, create a shortcut to storage1 named ResearchProduct.\n\nData Analytics Requirements -\nContoso identifies the following data analytics requirements:\nAll the workspaces for the Sales division and the Research division must support all Fabric experiences.\nThe Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.\nThe Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.\nFor the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.\nFor the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.\nAll the semantic models and reports for the Research division must use version control that supports branching.\n\nData Preparation Requirements -\nContoso identifies the following data preparation requirements:\nThe Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.\nAll the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.\n\nSemantic Model Requirements -\nContoso identifies the following requirements for implementing and managing semantic models:\nThe number of rows added to the Orders table during refreshes must be minimized.\nThe semantic models in the Research division workspaces must use Direct Lake mode.\n\nGeneral Requirements -\nContoso identifies the following high-level requirements that must be considered for all solutions:\nFollow the principle of least privilege when applicable.\nMinimize implementation and maintenance effort when possible.\nWhich syntax should you use in a notebook to access the Research division data for Productline1?","choices":{"C":"external_table(‘Tables/ResearchProduct)","D":"external_table(ResearchProduct)","A":"spark.read.format(“delta”).load(“Tables/productline1/ResearchProduct”)","B":"spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct ”)"},"timestamp":"2024-02-17 22:44:00","question_id":114,"discussion":[{"upvote_count":"15","comments":[{"upvote_count":"6","content":"Yes B is correct answer as per the requirement specified in case study - For the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.","comment_id":"1187212","poster":"a998450","timestamp":"1711949880.0"}],"poster":"David_Webb","content":"Selected Answer: B\nThe correct answer is B.\nThe folder hierarchy of Tables in Lakehouse is incorrect for A.","comment_id":"1155113","timestamp":"1708476960.0"},{"upvote_count":"10","comment_id":"1205443","timestamp":"1726974840.0","content":"Selected Answer: B\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")\ndisplay(df)\nOR\ndf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000\")\ndisplay(df)\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts","poster":"wispa2001"},{"comment_id":"1411210","timestamp":"1743149220.0","content":"Selected Answer: B\nThe Correct Answer is B.\nThe hierarchy of table is correct in B option.","upvote_count":"1","poster":"Atiwari95"},{"upvote_count":"1","comment_id":"1331464","poster":"NRezgui","timestamp":"1735116960.0","content":"Selected Answer: B\nspark.sql(“SELECT * FROM Lakehouse1.ResearchProduct ”)"},{"upvote_count":"1","comment_id":"1331457","content":"Selected Answer: B\nspark.sql(“SELECT * FROM Lakehouse1.ResearchProduct ”)","timestamp":"1735116360.0","poster":"NRezgui"},{"poster":"MultiCloudIronMan","content":"Selected Answer: A\nBased on the case study details, the data for Productline1 is stored in an Azure Data Lake Storage Gen2 storage account named storage1 in the delta format. A shortcut to this storage, named ResearchProduct, is created in Lakehouse1 within the Productline1ws workspace.\n\nGiven this structure, the path \"Tables/productline1/ResearchProduct\" is justified because it references the shortcut created in Lakehouse1 that points to the data stored in storage1. This path aligns with the case study's description of the data environment and planned changes.","comment_id":"1322701","comments":[{"poster":"Chandler9714","content":"\"In Lakehouse1, create a shortcut to storage1 named ResearchProduct.\" \n\nThe shortcut is to storage 1 not productline1. If the shortcut was specific to productline1 and placed in the folder, I think this would then be correct, but thats not the case here.","upvote_count":"1","comment_id":"1365887","timestamp":"1741269420.0"}],"upvote_count":"2","timestamp":"1733482440.0"},{"poster":"Rakesh16","content":"Selected Answer: B\nB-->spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct ”)","comment_id":"1312418","upvote_count":"1","timestamp":"1731649980.0"},{"timestamp":"1729923180.0","content":"The correct answer is B)\nThough A also looks correct, but the path mentioned is incorrect. The path should be Tables/ResearchProduct. We are directly creating a shortcut with the name ResearchProduct under Tables folder in Lakehouse1. There is no mention of the productline1 folder created.","comment_id":"1303104","upvote_count":"3","poster":"jass007_k"},{"content":"Both seem to be correct option A and option B. I have tried both syntaxes with shortcut data. Also its mentioned that format of data is in delta so I will go with option A)","poster":"jass007_k","comment_id":"1303102","upvote_count":"2","timestamp":"1729922280.0"},{"content":"A is for when you want to load data.\nAnswer is B, its only when you requesting specific data from specific table","timestamp":"1729404960.0","comment_id":"1300304","upvote_count":"3","poster":"Egocentric"},{"poster":"Egocentric","upvote_count":"2","timestamp":"1724349360.0","comment_id":"1270883","content":"answer is A. in B there is no productline1"},{"poster":"nyoike","comment_id":"1262173","content":"Selected Answer: A\nWith the recent introduction of schema-enabled Lakehouses, BOTH A and B are correct. That is assuming ResearchProduct table was created in a schema-enabled Lakehouse in the productline1 schema. I have tested A in a Fabric Spark notebook that is schema-enabled and it works.","upvote_count":"6","timestamp":"1723050480.0"},{"timestamp":"1719814500.0","poster":"Miro_dd","content":"Selected Answer: B\nHierarchy for answer A is not correct","upvote_count":"1","comment_id":"1240006"},{"comment_id":"1234167","timestamp":"1718938980.0","upvote_count":"1","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts\n\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")\nOR\ndf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000\")","poster":"ziggy1117"},{"timestamp":"1716292620.0","comment_id":"1214904","content":"If answer B is the correct, why Files folder dosen`t appear between Lakehouse1 and ResearchProduct? It has to be like \"lakehouse1.Files.ResearchProduct\", hasn't it?","upvote_count":"1","poster":"ca63a55"},{"comment_id":"1207956","timestamp":"1715101140.0","comments":[{"poster":"stilferx","upvote_count":"5","comment_id":"1211072","content":"My bad, the right answer is B.\nThere is a requirement: Research Division - all tables should be managed. It means, no subfolders should be.\n\nhttps://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/5-spark-sql","timestamp":"1715633520.0"}],"poster":"stilferx","upvote_count":"1","content":"Selected Answer: A\nIMHO,\nI would go with A, because of there is a clear statement ProductLine1. \nTechnically, A and B should work, but B doesn't have \"ProductLine\", what is confusing"},{"poster":"KASH2001","upvote_count":"1","comment_id":"1207795","timestamp":"1715070900.0","content":"There is no productline1 used in Answer B. Then how it could be correct...."},{"timestamp":"1715067480.0","comment_id":"1207774","content":"Selected Answer: B\nYeah, the answer is B. Cause the folder hierarchy is kind of not true. \nIt should be something else....","upvote_count":"1","poster":"laitoanthang"},{"comment_id":"1207156","content":"The correct answer id B.\nThe productionline1 represents workspace \n so in option A \nspark.read.format(“delta”).load(“Tables/productline1/ResearchProduct”)\nthere they mentioned productionline1 which is workspace so it is wrong","upvote_count":"2","poster":"NICVU","timestamp":"1714967820.0"},{"timestamp":"1714383780.0","content":"Selected Answer: B\nB. spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct ”)","comment_id":"1203939","poster":"rmeng","upvote_count":"1"},{"content":"Selected Answer: B\nA syntax error","comment_id":"1199093","timestamp":"1713609780.0","poster":"Shri_Learning","upvote_count":"2"},{"comment_id":"1178026","content":"Should be A : This syntax uses the spark.read.format().load() method to read data from the specified location in the Delta format, which is a popular format for managing big data within data lakes or warehouses. It specifies the path where the data for Productline1's research division is stored.\n\nOption B, spark.sql(\"SELECT * FROM Lakehouse1.ResearchProduct\"), executes a SQL query to select all data from a table named ResearchProduct in a database/schema called Lakehouse1. However, it doesn't specify the path or format of the data, so it may not be appropriate for accessing specific data within a notebook, especially if it's stored in a Delta format in a specific location.","timestamp":"1710922380.0","upvote_count":"2","poster":"shem576","comments":[{"timestamp":"1729923600.0","comment_id":"1303105","poster":"jass007_k","content":"If you don't specify the format, it will be delta format for tables and parquet format for files by default.","upvote_count":"1"}]},{"upvote_count":"2","timestamp":"1709620440.0","comment_id":"1166257","poster":"earlqq","content":"Selected Answer: B\nB is the correct one"},{"comment_id":"1158922","timestamp":"1708875600.0","poster":"TashaP","upvote_count":"3","content":"Cant be A: The path specified seems to assume a direct file system access rather than accessing through a lakehouse structure or shortcut. This syntax is used for reading data from a Delta Lake storage format.\n\nThe answer IS B: assuming 'Lakehouse1.ResearchProduct' refers to a structured dataset within Lakehouse1, This syntax correctly uses Spark SQL to query data. This is consistent with how lakehouse data is accessed through SQL queries.\n\nCant be C: Incomplete/incorrect format for accessing data in a spark environment. It is not the typical syntax used for Spark DataFrame or SQL API Calls.\n\nCant be D: Similar logic to C, it is missing the context/format needed to access the data. The syntax is not a correct Spark API call."},{"timestamp":"1708396740.0","upvote_count":"4","comment_id":"1154439","content":"Selected Answer: B\nThe correct answer is B.\nA should correct as :\ndf = spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n\nThe Table folder could not handle two hierarchical","poster":"Jeff_Zhu"},{"poster":"SamuComqi","content":"B. spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct ”)\n\nThe syntax of C and D is correct for KQL databases (incorrect in this use-case). When the shortcut is created, no additional folders have been added to the Tables section, therefore answer A is incorrect. Once created, the line of answer B can be used to access data correctly.\n\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts","upvote_count":"4","comment_id":"1153083","timestamp":"1708241280.0"},{"poster":"Momoanwar","timestamp":"1708206240.0","comment_id":"1152843","content":"Selected Answer: A\nI think A. We get data from shirtcut","comments":[{"content":"The problem with answer A is that if it's a shortcut, its probably an external location. Which means its not stored in DELTA format so this statement would be wrong.","upvote_count":"1","timestamp":"1709251500.0","comment_id":"1163110","poster":"XiltroX","comments":[{"upvote_count":"1","comment_id":"1182786","timestamp":"1711401180.0","content":"https://learn.microsoft.com/es-es/fabric/onelake/onelake-shortcuts","poster":"alexares_12"}]}],"upvote_count":"2"}],"isMC":true,"exam_id":71},{"id":"oVEXjYRRky4adgWwAff7","question_images":[],"answers_community":["A (58%)","C (42%)"],"isMC":true,"choices":{"A":"a lakehouse","D":"a KQL database","B":"an Azure SQL database","C":"a warehouse"},"exam_id":71,"topic":"1","timestamp":"2024-02-09 21:22:00","answer_ET":"A","answer_description":"","answer_images":[],"unix_timestamp":1707510120,"url":"https://www.examtopics.com/discussions/microsoft/view/133469-exam-dp-600-topic-1-question-40-discussion/","question_text":"You have source data in a folder on a local computer.\nYou need to create a solution that will use Fabric to populate a data store. The solution must meet the following requirements:\nSupport the use of dataflows to load and append data to the data store.\nEnsure that Delta tables are V-Order optimized and compacted automatically.\nWhich type of data store should you use?","question_id":115,"answer":"A","discussion":[{"comment_id":"1234501","content":"I have recently taken the exam and this question was asked.Its a multiple choice question.we need to select 2 options.so as per the comments both lakehouse and warehouse are support delta tables and v-order optimization.Its A,C","timestamp":"1718981400.0","upvote_count":"15","poster":"bvmony2294","comments":[{"comment_id":"1342542","upvote_count":"3","timestamp":"1737208740.0","poster":"testtaker45","content":"Makes a ton of sense. Thanks for Sharing!"}]},{"comment_id":"1214969","upvote_count":"10","content":"Selected Answer: C\nThe answer is correct - C Warehouse\nThe key to this question is \"Ensure that Delta tables are V-Order optimized\".\nV-Order optimization isn't guaranteed in Lakehouse, and there are times when you need to run OPTIMIZE to ensure the tables are V-Order Optimized.\nThis link here shows the answer:\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data#best-practices\nThe Note says \"Regardless of how you ingest data into warehouses, the parquet files produced by the data ingestion task will be optimized using V-Order write optimization... Unlike in Fabric Data Engineering, V-Order is a global setting in Synapse Data Warehouse that cannot be disabled.\"","timestamp":"1716297660.0","poster":"bigdave987"},{"timestamp":"1742182440.0","content":"Selected Answer: A\nboth lakehouse and warehouse are support delta tables and v-order optimization.","comment_id":"1399516","poster":"Amine_spiegel94","upvote_count":"1"},{"poster":"4b35503","content":"Selected Answer: A\nAll Fabric compute engines (spark notebooks, pipelines, Dataflow Gen2) automatically create Delta tables that are vorder'd.","comment_id":"1325702","timestamp":"1734016440.0","upvote_count":"1"},{"content":"Selected Answer: C\na warehouse","comment_id":"1312455","upvote_count":"1","poster":"Rakesh16","timestamp":"1731650880.0"},{"timestamp":"1724139660.0","poster":"Training_ND","content":"Selected Answer: C\nn Microsoft Fabric, a warehouse is a specialized data store optimized for analytics and query performance. It uses V-Order write optimization, which is specifically designed to enhance read performance for parquet files across various compute engines such as Power BI, SQL, and Spark. This feature is automatically applied in Synapse Data Warehouse and cannot be disabled, ensuring that data stored in the warehouse is always optimized.","upvote_count":"1","comment_id":"1269257"},{"poster":"6d1de25","comment_id":"1245785","timestamp":"1720656480.0","content":"Selected Answer: C\nBoth A & C are correct","upvote_count":"1"},{"timestamp":"1718931300.0","upvote_count":"1","content":"C - Warehouse as for warehouse the optimized V-order is automatically enabled but for Lakehouse is setting that you need to enable or disable.","poster":"buhari","comment_id":"1234137"},{"content":"Selected Answer: A\nLakehouses can automatically handle the optimization and compaction of Delta tables, including V-Order optimization, which arranges data in an optimal order to improve query performance.\nWhy Not the Other Options?\nAzure SQL Database (Option B):Azure SQL Database is a relational database service that does not natively support Delta tables or V-Order optimization. It is more suited for traditional OLTP workloads.\nWarehouse (Option C):While warehouses are excellent for structured data and support dataflows, they may not provide the same level of native support for Delta tables and automatic V-Order optimization as a lakehouse.\nKQL Database (Option D):KQL (Kusto Query Language) databases are optimized for log and telemetry data, primarily used with Azure Data Explorer. They are not designed to support Delta tables or the specific optimizations required for large-scale transactional data processing.","upvote_count":"3","timestamp":"1716849900.0","poster":"282b85d","comment_id":"1219876"},{"poster":"stilferx","comment_id":"1208630","upvote_count":"1","timestamp":"1715218500.0","content":"Selected Answer: A\nIMHO, \"A\"\n\nbecause:\nThe Lakehouse and the Delta Lake table format are central to Microsoft Fabric, assuring that tables are optimized for analytics is a key requirement. ...\nin \nhttps://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql"},{"content":"Selected Answer: C\nlakehouse doesn't automaticly store data in delta tables while warehouse does.","comment_id":"1192882","upvote_count":"3","timestamp":"1712744220.0","poster":"PiotrO"},{"content":"The whole new platform is focused on the lakehouse and optimization for it, so answer A.","upvote_count":"2","poster":"a_51","comment_id":"1179303","timestamp":"1711029240.0"},{"comments":[{"comments":[{"content":"Read your quote one more time. It doesn't mean that the answer is a warehouse","timestamp":"1725950880.0","comment_id":"1281399","poster":"mns0173","upvote_count":"1"}],"content":"https://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data#best-practices\nThe Note says \"Regardless of how you ingest data into warehouses, the parquet files produced by the data ingestion task will be optimized using V-Order write optimization... Unlike in Fabric Data Engineering, V-Order is a global setting in Synapse Data Warehouse that cannot be disabled.\"\n\nTherefore the answer is warehouse","poster":"Gecig","comment_id":"1227899","upvote_count":"5","timestamp":"1718027220.0"}],"timestamp":"1709051640.0","content":"Selected Answer: A\nA - The only logical answer here. B is an Azure SQL database and is an Azure product but doesn't have V-Order. C is a just a generic warehouse and once again, doesn't necessarily contain any V-Order feature. D is a database that uses KQL and is irrelevant of the question.","poster":"XiltroX","comment_id":"1160818","upvote_count":"8"},{"comment_id":"1152933","upvote_count":"4","content":"Selected Answer: A\nDelta table... = Lakehouse","timestamp":"1708213560.0","poster":"Momoanwar"},{"poster":"IshtarSQL","upvote_count":"4","comment_id":"1145769","content":"Selected Answer: A\nTo meet the requirements of supporting dataflows to load and append data to the data store while ensuring that Delta tables are V-Order optimized and compacted automatically, you should use a lakehouse in Fabric as your solution.","timestamp":"1707510120.0"}]}],"exam":{"name":"DP-600","lastUpdated":"12 Apr 2025","isImplemented":true,"isBeta":false,"isMCOnly":false,"id":71,"numberOfQuestions":179,"provider":"Microsoft"},"currentPage":23},"__N_SSP":true}