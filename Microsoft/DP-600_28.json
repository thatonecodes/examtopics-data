{"pageProps":{"questions":[{"id":"z0pGdBQ5Z5LWYfN970il","exam_id":71,"url":"https://www.examtopics.com/discussions/microsoft/view/133445-exam-dp-600-topic-1-question-6-discussion/","question_images":["https://img.examtopics.com/dp-600/image4.png","https://img.examtopics.com/dp-600/image5.png","https://img.examtopics.com/dp-600/image8.png"],"question_text":"HOTSPOT -\n\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nLitware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.\n\nExisting Environment -\n\nFabric Environment -\nLitware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.\n\nAvailable Data -\nLitware has data that must be analyzed as shown in the following table.\n//IMG//\n\nThe Product data contains a single table and the following columns.\n//IMG//\n\nThe customer satisfaction data contains the following tables:\n\nSurvey -\n\nQuestion -\n\nResponse -\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.\n\nUser Problems -\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.\nProduct data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.\n\nRequirements -\n\nPlanned Changes -\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity\nThe following three workspaces will be created:\nAnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store\nDataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake\nDataSciPOC: Will contain all the notebooks and reports created by the data scientists\nThe following will be created in the AnalyticsPOC workspace:\nA data store (type to be decided)\n\nA custom semantic model -\n\nA default semantic model -\n\nInteractive reports -\nThe data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers’ discretion.\nAll the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.\n\nTechnical Requirements -\nThe data store must support the following:\nRead access by using T-SQL or Python\nSemi-structured and unstructured data\nRow-level security (RLS) for users executing T-SQL queries\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\nData will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model\nThe data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model\nThe dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\nThe product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:\nList prices that are less than or equal to 50 are in the low pricing group.\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\nList prices that are greater than 1,000 are in the high pricing group.\n\nSecurity Requirements -\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\nFabric administrators will be the workspace administrators.\nThe data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.\nThe analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.\nThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\nThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.\nThe date dimension must be available to all users of the data store.\nThe principle of least privilege must be followed.\nBoth the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:\nFabricAdmins: Fabric administrators\nAnalyticsTeam: All the members of the analytics team\nDataAnalysts: The data analysts on the analytics team\nDataScientists: The data scientists on the analytics team\nDataEngineers: The data engineers on the analytics team\nAnalyticsEngineers: The analytics engineers on the analytics team\n\nReport Requirements -\nThe data analysts must create a customer satisfaction report that meets the following requirements:\nEnables a user to select a product to filter customer survey responses to only those who have purchased that product.\nDisplays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.\nShows data as soon as the data is updated in the data store.\nEnsures that the report and the semantic model only contain data from the current and previous year.\nEnsures that the report respects any table-level security specified in the source data store.\nMinimizes the execution time of report queries.\nYou need to create a DAX measure to calculate the average overall satisfaction score.\nHow should you complete the DAX code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n//IMG//","answer":"","answer_description":"","answers_community":[],"isMC":false,"topic":"1","timestamp":"2024-02-09 13:56:00","answer_images":["https://img.examtopics.com/dp-600/image225.png"],"discussion":[{"poster":"theseon","timestamp":"1707483360.0","content":"Iwould say:\nAVERAGE('Survery'[Respone Value]) and Period\n\nI do not unterstand why we would use the CustomerKey for Average Calculation.","upvote_count":"100","comment_id":"1145472"},{"content":"Average('Survey'[Response Value]) and Period","poster":"a_51","upvote_count":"10","comments":[{"upvote_count":"5","comment_id":"1185636","content":"Response Value is the only one that even likely could be aggregated. The question text and customer key do not make sense.","timestamp":"1711739100.0","poster":"a_51"}],"comment_id":"1177559","timestamp":"1710868260.0"},{"comment_id":"1363633","timestamp":"1740845520.0","content":"Calm down guys, the correct answer is \"Average\" and \"Period\"","upvote_count":"2","poster":"rohitrc8521"},{"upvote_count":"3","poster":"Rakesh16","timestamp":"1731650040.0","comment_id":"1312420","content":"Average(‘Survey’[Response Value]) & Period is the answer"},{"poster":"jass007_k","comment_id":"1303226","content":"This will be the final answer\nRolling 12 Overall Satisfaction =\nVAR NumberOfMonths = 12\nVAR LastCurrentDate = MAX('Date'[Date])\nVAR Period = DATESINPERIOD(\n 'Date'[Date],\n LastCurrentDate,\n -NumberOfMonths,\n MONTH\n)\nVAR Result =\n CALCULATE (\n AVERAGE('Survey'[Response Value]),\n Period,\n 'Survey Question'[Question Title] = \"Overall Satisfaction\"\n )\nRETURN\n Result\nThe DATESINPERIOD function calculates a dynamic period starting from the most recent date (LastCurrentDate) and going back NumberOfMonths months. It provides a rolling 12-month window to calculate averages within this range.","upvote_count":"2","timestamp":"1729937880.0"},{"comment_id":"1192969","upvote_count":"6","poster":"VAzureD","timestamp":"1726974960.0","content":"AVERAGE('Survey'[Response Value]),\nPeriod.\n\n\nAVERAGEA('Question'[Question Text]),\nIt doesn't make much sense to average a text field when you have to extract a value from it, although AVERAGEA can do it.\n\nAVERAGEX(VALUES('Survey'[Customer Key]),\nSyntax is wrong, discarded. AVERAGEX(<table>, <expression>)\nAdditionally VALUES leaves only the distinct values\n\n\nLastCurrentDate,\nIn this case we would have only half a day. Has no sense.\n\nNumberOfMonths,\nWe would be filtering by 12. It doesn't make sense.\n\nPeriod,\nIn the variable we have the dates of the last 12 months."},{"poster":"282b85d","upvote_count":"1","comment_id":"1219624","content":"Rolling 12 Overall Satisfaction =\nVAR NumberOfMonths = 12\nVAR LastCurrentDate = MAX ( 'Date' [Date] )\nVAR Period = DATESINPERIOD ( 'Date' [Date], LastCurrentDate, -NumberOfMonths, MONTH )\nVAR Result =\n CALCULATE (\n AVERAGEX (\n VALUES('Survey'[Customer Key]),\n CALCULATE(\n AVERAGE('Survey'[Response Value])\n )\n ),\n Period,\n 'Survey Question'[Question Title] = \"Overall Satisfaction\"\n )\nRETURN\n Result\n**There should be some missing part in this screenshot, but the correct answer would be the VALUES('Survey'[Customer Key]) with the missing part CALCULATE(AVERAGE('Survey'[Response Value]))","timestamp":"1726974960.0"},{"poster":"Darshan6232","comment_id":"1227081","timestamp":"1717910880.0","upvote_count":"2","content":"It should be :\n1. Average(Survey[Response Value]) as this one is the only value that can be aggregated.\nCustomer Key Does not make any sense , as it just a key to identify a customer, this can never provide us the score.\n2. Period : Variable is defined as to select 1 year of date range. Can directly be passed in the Filter context of Calculate formula. Thanks!"},{"upvote_count":"1","comment_id":"1208485","content":"1 = AVERAGE('Survery'[Respone Value]). customer satisfaction can be determind by response value.\n2= Period. choose this just because it state overall no specific time or requires a date range.","timestamp":"1715189040.0","poster":"2dc6125"},{"content":"IMHO,\nAVERAGE('Survery'[Respone Value]) and Period\n\nWhy?\nThe first one defines actually the metric, which is avg from survey(response value), \nthe second one defines the date range, which is precalculated before as a variable Period.","comment_id":"1208037","poster":"stilferx","timestamp":"1715117040.0","upvote_count":"2"},{"timestamp":"1714388640.0","upvote_count":"1","comment_id":"1203969","poster":"rmeng","content":"AVERAGE('Survery'[Respone Value]) , Period"},{"upvote_count":"6","timestamp":"1708241940.0","comment_id":"1153086","content":"For the first part, \"AVERAGE('Survey'[Response Value])\" because the second option uses a text column as argument, and the third option is not relevant in this context (no need to perform row calculations).\nFor the second part, \"Period\": data is filtered to compute the average in the last 12 months (interval defined in the variable).","poster":"SamuComqi"},{"content":"Average and Period","comment_id":"1152846","poster":"Momoanwar","upvote_count":"5","timestamp":"1708206420.0"},{"upvote_count":"4","comment_id":"1149466","content":"Response Value and Period","poster":"Nicknamefordiscussions69","timestamp":"1707848880.0"}],"question_id":136,"answer_ET":"","unix_timestamp":1707483360},{"id":"qMDhhvKD3emSG4MUpqpB","answer_description":"","timestamp":"2024-04-21 10:07:00","answers_community":["BE (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/139283-exam-dp-600-topic-1-question-60-discussion/","question_images":["https://img.examtopics.com/dp-600/image78.png","https://img.examtopics.com/dp-600/image79.png"],"question_id":137,"answer_ET":"BE","isMC":true,"question_text":"Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\n\nOverview -\n\nLitware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.\n\n\nExisting Environment -\n\n\nFabric Environment -\n\nLitware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.\n\n\nAvailable Data -\n\nLitware has data that must be analyzed as shown in the following table.\n\n//IMG//\n\n\nThe Product data contains a single table and the following columns.\n\n//IMG//\n\n\nThe customer satisfaction data contains the following tables:\n\n• Survey\n• Question\n• Response\n\nFor each survey submitted, the following occurs:\n\n• One row is added to the Survey table.\n• One row is added to the Response table for each question in the survey.\n\nThe Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.\n\n\nUser Problems -\n\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.\n\nProduct data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.\n\n\nRequirements -\n\n\nPlanned Changes -\n\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity\n\nThe following three workspaces will be created:\n\n• AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store\n• DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake\n• DataSciPOC: Will contain all the notebooks and reports created by the data scientists\n\nThe following will be created in the AnalyticsPOC workspace:\n\n• A data store (type to be decided)\n• A custom semantic model\n• A default semantic model\n• Interactive reports\n\nThe data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers’ discretion.\n\nAll the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.\n\n\nTechnical Requirements -\n\nThe data store must support the following:\n\n• Read access by using T-SQL or Python\n• Semi-structured and unstructured data\n• Row-level security (RLS) for users executing T-SQL queries\n\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\n\nData will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model\n\nThe data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model\n\nThe dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\n\nThe product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:\n\n• List prices that are less than or equal to 50 are in the low pricing group.\n• List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\n• List prices that are greater than 1,000 are in the high pricing group.\n\n\nSecurity Requirements -\n\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.\n\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\n\n• Fabric administrators will be the workspace administrators.\n• The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.\n• The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.\n• The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\n• The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.\n• The date dimension must be available to all users of the data store.\n• The principle of least privilege must be followed.\n\nBoth the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:\n\n• FabricAdmins: Fabric administrators\n• AnalyticsTeam: All the members of the analytics team\n• DataAnalysts: The data analysts on the analytics team\n• DataScientists: The data scientists on the analytics team\n• DataEngineers: The data engineers on the analytics team\n• AnalyticsEngineers: The analytics engineers on the analytics team\n\n\nReport Requirements -\n\nThe data analysts must create a customer satisfaction report that meets the following requirements:\n\n• Enables a user to select a product to filter customer survey responses to only those who have purchased that product.\n• Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.\n• Shows data as soon as the data is updated in the data store.\n• Ensures that the report and the semantic model only contain data from the current and previous year.\n• Ensures that the report respects any table-level security specified in the source data store.\n• Minimizes the execution time of report queries.\n\nYou need to recommend a solution to prepare the tenant for the PoC.\n\nWhich two actions should you recommend performing from the Fabric Admin portal? Each correct answer presents part of the solution.\n\nNOTE: Each correct answer is worth one point.","discussion":[{"content":"Selected Answer: BE\nEnable the Users can try Microsoft Fabric paid features option for specific security groups: This will allow specific security groups (like the AnalyticsTeam, DataAnalysts, DataScientists, DataEngineers, and AnalyticsEngineers) to access and use the paid features of Microsoft Fabric necessary for the PoC. This is important to ensure that only the relevant team members can utilize these advanced features while preventing unnecessary access for other users.\nEnable the Users can create Fabric items option for specific security groups: This will allow only specific security groups to create Fabric items, ensuring that the creation of these items is controlled and managed by the appropriate team members. This helps maintain the principle of least privilege and ensures that only authorized personnel can create and manage Fabric items during the PoC.","upvote_count":"11","timestamp":"1733587500.0","comment_id":"1226177","poster":"282b85d"},{"comment_id":"1209151","content":"Selected Answer: BE\nIMHO, B & E looks good.\n\nHere, we use only for \"specific groups\", but not \"C\" which is for guests.","upvote_count":"2","timestamp":"1731210180.0","poster":"stilferx"},{"comment_id":"1200725","upvote_count":"2","poster":"VAzureD","content":"Selected Answer: BE\nI agree with the answer. The POC is only for a certain group of users.","timestamp":"1729686180.0"},{"comment_id":"1199536","timestamp":"1729498020.0","poster":"4371883","upvote_count":"2","content":"Selected Answer: BE\nThis is a POC with internal users, B & E make sense on the principle of least privilege."}],"unix_timestamp":1713686820,"topic":"1","exam_id":71,"answer":"BE","choices":{"A":"Enable the Users can try Microsoft Fabric paid features option for the entire organization.","C":"Enable the Allow Azure Active Directory guest users to access Microsoft Fabric option for specific security groups.","D":"Enable the Users can create Fabric items option and exclude specific security groups.","E":"Enable the Users can create Fabric items option for specific security groups.","B":"Enable the Users can try Microsoft Fabric paid features option for specific security groups."},"answer_images":[]},{"id":"PPX0pIl7mlIM8SpEX4cW","discussion":[{"comment_id":"1191972","poster":"dp600","content":"Direct Lake also supports row-level security and object-level security so that users only see the data they have permission to see.\nhttps://learn.microsoft.com/es-es/power-bi/enterprise/directlake-overview","comments":[{"poster":"Allapanda","upvote_count":"3","content":"It's also saying \"The analytics team has large volumes of data, some of which is semi-structured\",as far as we know Direct Query can't handle semi-structured data, but DirectLake as well.","timestamp":"1725886560.0","comment_id":"1280888"}],"timestamp":"1712638020.0","upvote_count":"14"},{"content":"SSO and Direct Lake","timestamp":"1716184740.0","comment_id":"1214141","upvote_count":"8","poster":"gtc108"},{"upvote_count":"1","content":"Single Sign-on (SSO) authentication\nDirectQuery\n\nI think this is correct 100%","comment_id":"1346102","poster":"pirate84","timestamp":"1737720840.0"},{"upvote_count":"2","content":"Where does it say the the Customer Survey Report needs RLS or am I missing something? I see that the report needs Table-level security which is in DirectQuery mode. \nIMHO, Service Principle and DirectQuery are the correct answers.","timestamp":"1724898000.0","comment_id":"1274303","poster":"cafb698"},{"upvote_count":"5","comment_id":"1230906","poster":"DarioReymago","content":"I'll select: SSO and Direct lake\nhttps://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview#single-sign-on-sso-enabled-by-default","timestamp":"1718450820.0"},{"timestamp":"1717303140.0","upvote_count":"5","content":"I am hesitant... I wouldn't say that Direct Lake mode is super obvious here. Direct Lake does support RLS/OBS and you can configure it via web interface... BUT! From what I understood, if data source (e.g. database) has RLS/OBS configured, semantic model will fall back to Direct Query and obey RLS/OBS at data source.\nAm I wrong where?","poster":"PaweuG","comment_id":"1223021"},{"comment_id":"1209160","comments":[{"comment_id":"1211581","content":"According to the link you shared, it says \"By default, Direct Lake models use single sign-on (SSO), so the effective permissions of the interactive user determine if the user is allowed or denied access to the data. If the Direct Lake model is configured to use a fixed identity, the effective permission of the fixed identity determines if users interacting with the semantic model can access the data.\"\nTherefore, why do you think that the authentication method is Service principal (fixed identity) instead of SSO (interactive user)?.","upvote_count":"8","timestamp":"1715705580.0","poster":"Fer079"}],"content":"IMHO, Service Principal & Direct Lake looks good.\n\nAnd yes, Direct Lake supports RLS, proof is here: \nDirect Lake also supports row-level security and object-level security so users only see the data they have permission to see.\nLink: https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview","poster":"stilferx","timestamp":"1715306160.0","upvote_count":"2"},{"comments":[{"upvote_count":"1","poster":"MultiCloudIronMan","content":"Service Principle and Directlake","comment_id":"1324139","timestamp":"1733761860.0"}],"upvote_count":"6","comment_id":"1191982","content":"it should be service principle and direct lake.\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/directlake-fixed-identity","poster":"pppppppppie","timestamp":"1712639160.0"},{"content":"Direct lake wont support RLS. so it would be DQ","comment_id":"1191083","poster":"neoverma","timestamp":"1712510700.0","upvote_count":"2","comments":[{"content":"Direct lake supports RLS","comment_id":"1203595","poster":"dp600","timestamp":"1714313340.0","upvote_count":"8"}]}],"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/138090-exam-dp-600-topic-1-question-61-discussion/","answer":"","unix_timestamp":1712510700,"question_images":["https://img.examtopics.com/dp-600/image78.png","https://img.examtopics.com/dp-600/image79.png","https://img.examtopics.com/dp-600/image80.png"],"answer_images":["https://img.examtopics.com/dp-600/image81.png"],"topic":"1","question_text":"HOTSPOT\n-\n\n\nCase study\n-\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\n\nOverview\n-\n\nLitware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.\n\n\nExisting Environment\n-\n\n\nFabric Environment\n-\n\nLitware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.\n\n\nAvailable Data\n-\n\nLitware has data that must be analyzed as shown in the following table.\n\n//IMG//\n\n\nThe Product data contains a single table and the following columns.\n\n//IMG//\n\n\nThe customer satisfaction data contains the following tables:\n\n• Survey\n• Question\n• Response\n\nFor each survey submitted, the following occurs:\n\n• One row is added to the Survey table.\n• One row is added to the Response table for each question in the survey.\n\nThe Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.\n\n\nUser Problems\n-\n\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.\n\nProduct data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.\n\n\nRequirements\n-\n\n\nPlanned Changes\n-\n\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity\n\nThe following three workspaces will be created:\n\n• AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store\n• DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake\n• DataSciPOC: Will contain all the notebooks and reports created by the data scientists\n\nThe following will be created in the AnalyticsPOC workspace:\n\n• A data store (type to be decided)\n• A custom semantic model\n• A default semantic model\n• Interactive reports\n\nThe data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers’ discretion.\n\nAll the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.\n\n\nTechnical Requirements\n-\n\nThe data store must support the following:\n\n• Read access by using T-SQL or Python\n• Semi-structured and unstructured data\n• Row-level security (RLS) for users executing T-SQL queries\n\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\n\nData will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model\n\nThe data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model\n\nThe dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\n\nThe product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:\n\n• List prices that are less than or equal to 50 are in the low pricing group.\n• List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\n• List prices that are greater than 1,000 are in the high pricing group.\n\n\nSecurity Requirements\n-\n\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.\n\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\n\n• Fabric administrators will be the workspace administrators.\n• The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.\n• The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.\n• The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\n• The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.\n• The date dimension must be available to all users of the data store.\n• The principle of least privilege must be followed.\n\nBoth the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:\n\n• FabricAdmins: Fabric administrators\n• AnalyticsTeam: All the members of the analytics team\n• DataAnalysts: The data analysts on the analytics team\n• DataScientists: The data scientists on the analytics team\n• DataEngineers: The data engineers on the analytics team\n• AnalyticsEngineers: The analytics engineers on the analytics team\n\n\nReport Requirements\n-\n\nThe data analysts must create a customer satisfaction report that meets the following requirements:\n\n• Enables a user to select a product to filter customer survey responses to only those who have purchased that product.\n• Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.\n• Shows data as soon as the data is updated in the data store.\n• Ensures that the report and the semantic model only contain data from the current and previous year.\n• Ensures that the report respects any table-level security specified in the source data store.\n• Minimizes the execution time of report queries.\n\n\nYou need to design a semantic model for the customer satisfaction report.\n\nWhich data source authentication method and mode should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","exam_id":71,"timestamp":"2024-04-07 19:25:00","answer_ET":"","question_id":138,"isMC":false,"answer_description":""},{"id":"d2WCI8VUZQvOUOywES3L","question_images":["https://img.examtopics.com/dp-600/image78.png","https://img.examtopics.com/dp-600/image79.png"],"answer":"AD","answer_description":"","answer_images":[],"question_id":139,"url":"https://www.examtopics.com/discussions/microsoft/view/137913-exam-dp-600-topic-1-question-62-discussion/","exam_id":71,"isMC":true,"question_text":"Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\n\nOverview -\n\nLitware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.\n\n\nExisting Environment -\n\n\nFabric Environment -\n\nLitware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.\n\n\nAvailable Data -\n\nLitware has data that must be analyzed as shown in the following table.\n\n//IMG//\n\n\nThe Product data contains a single table and the following columns.\n\n//IMG//\n\n\nThe customer satisfaction data contains the following tables:\n\n• Survey\n• Question\n• Response\n\nFor each survey submitted, the following occurs:\n\n• One row is added to the Survey table.\n• One row is added to the Response table for each question in the survey.\n\nThe Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.\n\n\nUser Problems -\n\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.\n\nProduct data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.\n\n\nRequirements -\n\n\nPlanned Changes -\n\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity\n\nThe following three workspaces will be created:\n\n• AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store\n• DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake\n• DataSciPOC: Will contain all the notebooks and reports created by the data scientists\n\nThe following will be created in the AnalyticsPOC workspace:\n\n• A data store (type to be decided)\n• A custom semantic model\n• A default semantic model\n• Interactive reports\n\nThe data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers’ discretion.\n\nAll the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.\n\n\nTechnical Requirements -\n\nThe data store must support the following:\n\n• Read access by using T-SQL or Python\n• Semi-structured and unstructured data\n• Row-level security (RLS) for users executing T-SQL queries\n\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\n\nData will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model\n\nThe data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model\n\nThe dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\n\nThe product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:\n\n• List prices that are less than or equal to 50 are in the low pricing group.\n• List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\n• List prices that are greater than 1,000 are in the high pricing group.\n\n\nSecurity Requirements -\n\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.\n\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\n\n• Fabric administrators will be the workspace administrators.\n• The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.\n• The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.\n• The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\n• The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.\n• The date dimension must be available to all users of the data store.\n• The principle of least privilege must be followed.\n\nBoth the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:\n\n• FabricAdmins: Fabric administrators\n• AnalyticsTeam: All the members of the analytics team\n• DataAnalysts: The data analysts on the analytics team\n• DataScientists: The data scientists on the analytics team\n• DataEngineers: The data engineers on the analytics team\n• AnalyticsEngineers: The analytics engineers on the analytics team\n\n\nReport Requirements -\n\nThe data analysts must create a customer satisfaction report that meets the following requirements:\n\n• Enables a user to select a product to filter customer survey responses to only those who have purchased that product.\n• Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.\n• Shows data as soon as the data is updated in the data store.\n• Ensures that the report and the semantic model only contain data from the current and previous year.\n• Ensures that the report respects any table-level security specified in the source data store.\n• Minimizes the execution time of report queries.\n\n\nYou need to implement the date dimension in the data store. The solution must meet the technical requirements.\n\nWhat are two ways to achieve the goal? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.","choices":{"C":"Populate the date dimension view by using T-SQL.","B":"Populate the date dimension table by using a Copy activity in a pipeline.","A":"Populate the date dimension table by using a dataflow.","D":"Populate the date dimension table by using a Stored procedure activity in a pipeline."},"timestamp":"2024-04-04 21:02:00","discussion":[{"upvote_count":"32","timestamp":"1712511120.0","content":"Selected Answer: AD\nas per the technical requirements - \n\"The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\"\n\n\nNo existing data source & The date dimension must always contain dates from 2010 through the end of the current year is the Key Point here\n\n\nusing the elimination method\nView wont be appropriate and COPY activity cant be used since there is no data source for DATE table. \n\nso the answer is A and D","poster":"neoverma","comment_id":"1191090"},{"comments":[{"poster":"testtaker45","comment_id":"1343632","content":"Thank you for your answer. I would say A, D. You would use T-SQL at the data frame layer in a Notebook, this wouldn't be at the Data Source Layer. That is the trick, to be affecting the Data Source.","timestamp":"1737380820.0","upvote_count":"1"},{"content":"I am not able to create a view with CTE. Could you let us know how?","poster":"os_ca","comment_id":"1225932","upvote_count":"3","timestamp":"1717742280.0"}],"comment_id":"1214321","timestamp":"1716207060.0","upvote_count":"9","content":"C,D.\nI always create dimDate using a views (T-Sql select statement with recursive CTE).\nA. is too complicated.","poster":"DirectX"},{"upvote_count":"1","poster":"bc5468521","content":"Selected Answer: CD\nTraditional way to create date dimension","timestamp":"1734027780.0","comment_id":"1325798"},{"timestamp":"1726665840.0","content":"It is specified: \"The data store must support the following: Semi-structured and unstructured data\".\nSo the data store is a lakehouse.\nStored procedures don't exist in a lakehouse.\nI think A and C","poster":"c119533","upvote_count":"3","comment_id":"1285751"},{"upvote_count":"1","content":"C, D \nit says POPULATE, not CREATE... you cannot create tables with SPs in a lakehouse, BUT you CAN populate already existing ones!","comment_id":"1255196","poster":"stv","timestamp":"1721934600.0"},{"comment_id":"1233149","timestamp":"1718828100.0","upvote_count":"1","poster":"b6daab0","content":"ChatGDP chose AB. It says \"Copy activity in tools like Azure Data Factory (ADF) allows you to copy data from various sources to your data warehouse. For a date dimension, you can generate the required dates using a source query or script and use the Copy activity to load them into your dimension table.\" and \"While using a stored procedure can also achieve this, it typically requires more complex management and might not be as straightforward or flexible as using dataflows or copy activities, which are specifically designed for ETL (Extract, Transform, Load) processes.\""},{"upvote_count":"1","comment_id":"1231535","poster":"b6daab0","content":"correction: ChatGDP says A and C","timestamp":"1718570280.0"},{"comment_id":"1231532","timestamp":"1718570220.0","content":"ChatGPT says the answer should be A and D :)","upvote_count":"1","poster":"b6daab0"},{"comments":[{"content":"I agree","comment_id":"1246217","upvote_count":"1","poster":"6d1de25","timestamp":"1720713600.0"}],"comment_id":"1230912","content":"Selected Answer: AC\nI prefered A & C. B&D need more steps to be created","timestamp":"1718451360.0","poster":"DarioReymago","upvote_count":"2"},{"upvote_count":"1","comment_id":"1229338","poster":"dev2dev","timestamp":"1718211000.0","content":"Selected Answer: CD\nC & D looks correct. A will be completed. C would need a new object to be created, rather we can simply use a one time script."},{"timestamp":"1717215060.0","comment_id":"1222497","poster":"David_Webb","upvote_count":"1","content":"Selected Answer: AD\nA and D should be the right answer to go."},{"upvote_count":"3","timestamp":"1715540100.0","comment_id":"1210343","poster":"stilferx","comments":[{"poster":"stilferx","upvote_count":"2","content":"Sorry, among ACD, I am choosing AD. That's what I meant","timestamp":"1715540160.0","comment_id":"1210344"}],"content":"Selected Answer: AD\nIMHO, AD may be good\n\nAgree, B is not good, C is just a weird way. But possible.\nSo, the question is kind of ambiguous, with no particular directions from the Microsoft site. Among ABD, I am choosing AD"}],"answers_community":["AD (90%)","5%"],"unix_timestamp":1712257320,"topic":"1","answer_ET":"AD"},{"id":"ZzJvyz7n3CaOsS8bcnnx","question_images":["https://img.examtopics.com/dp-600/image78.png","https://img.examtopics.com/dp-600/image79.png"],"exam_id":71,"question_text":"Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\n\nOverview -\n\nLitware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.\n\n\nExisting Environment -\n\n\nFabric Environment -\n\nLitware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.\n\n\nAvailable Data -\n\nLitware has data that must be analyzed as shown in the following table.\n\n//IMG//\n\n\nThe Product data contains a single table and the following columns.\n\n//IMG//\n\n\nThe customer satisfaction data contains the following tables:\n\n• Survey\n• Question\n• Response\n\nFor each survey submitted, the following occurs:\n\n• One row is added to the Survey table.\n• One row is added to the Response table for each question in the survey.\n\nThe Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.\n\n\nUser Problems -\n\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.\n\nProduct data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.\n\n\nRequirements -\n\n\nPlanned Changes -\n\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity\n\nThe following three workspaces will be created:\n\n• AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store\n• DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake\n• DataSciPOC: Will contain all the notebooks and reports created by the data scientists\n\nThe following will be created in the AnalyticsPOC workspace:\n\n• A data store (type to be decided)\n• A custom semantic model\n• A default semantic model\n• Interactive reports\n\nThe data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers’ discretion.\n\nAll the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.\n\n\nTechnical Requirements -\n\nThe data store must support the following:\n\n• Read access by using T-SQL or Python\n• Semi-structured and unstructured data\n• Row-level security (RLS) for users executing T-SQL queries\n\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\n\nData will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model\n\nThe data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model\n\nThe dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\n\nThe product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:\n\n• List prices that are less than or equal to 50 are in the low pricing group.\n• List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\n• List prices that are greater than 1,000 are in the high pricing group.\n\n\nSecurity Requirements -\n\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.\n\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\n\n• Fabric administrators will be the workspace administrators.\n• The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.\n• The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.\n• The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\n• The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.\n• The date dimension must be available to all users of the data store.\n• The principle of least privilege must be followed.\n\nBoth the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:\n\n• FabricAdmins: Fabric administrators\n• AnalyticsTeam: All the members of the analytics team\n• DataAnalysts: The data analysts on the analytics team\n• DataScientists: The data scientists on the analytics team\n• DataEngineers: The data engineers on the analytics team\n• AnalyticsEngineers: The analytics engineers on the analytics team\n\n\nReport Requirements -\n\nThe data analysts must create a customer satisfaction report that meets the following requirements:\n\n• Enables a user to select a product to filter customer survey responses to only those who have purchased that product.\n• Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.\n• Shows data as soon as the data is updated in the data store.\n• Ensures that the report and the semantic model only contain data from the current and previous year.\n• Ensures that the report respects any table-level security specified in the source data store.\n• Minimizes the execution time of report queries.\n\n\nYou need to ensure the data loading activities in the AnalyticsPOC workspace are executed in the appropriate sequence. The solution must meet the technical requirements.\n\nWhat should you do?","question_id":140,"answers_community":["D (100%)"],"answer_images":[],"answer":"D","url":"https://www.examtopics.com/discussions/microsoft/view/139284-exam-dp-600-topic-1-question-63-discussion/","answer_description":"","unix_timestamp":1713688980,"topic":"1","timestamp":"2024-04-21 10:43:00","answer_ET":"D","isMC":true,"discussion":[{"content":"Selected Answer: D\nD is correct.\n\nThe technical requirements specify that the data loading process must ensure the raw and cleansed data is updated completely before populating the dimensional model. This suggests that the data loading process involves multiple steps that need to be executed in a specific order.\n\nUsing a pipeline with dependencies between activities is the best approach to achieve this. A pipeline allows you to create a sequence of activities, with each activity dependent on the successful completion of the previous one. This ensures the data loading activities are executed in the appropriate order, meeting the technical requirements.","upvote_count":"4","timestamp":"1733380200.0","comment_id":"1224505","poster":"Evincible"},{"comment_id":"1210348","timestamp":"1731445440.0","poster":"stilferx","content":"Selected Answer: D\nIMHO, D is correct.\n\nI love Microsoft, ambiguity as is. But yes, generally, according to Microsoft BP, a pipeline in ADF with running all activities is the suggested solution. From it, it is easy to trigger different notebooks, for example","upvote_count":"2"},{"poster":"4371883","timestamp":"1729500180.0","comment_id":"1199554","upvote_count":"2","content":"Selected Answer: D\nD is correct. Pipeline can ensure the activities follow the required sequence.\nhttps://learn.microsoft.com/en-us/fabric/data-factory/activity-overview#data-transformation-activities"}],"choices":{"D":"Create a pipeline that has dependencies between activities and schedule the pipeline.","C":"Create and schedule a Spark job definition.","A":"Create a dataflow that has multiple steps and schedule the dataflow.","B":"Create and schedule a Spark notebook."}}],"exam":{"isBeta":false,"name":"DP-600","isMCOnly":false,"provider":"Microsoft","lastUpdated":"12 Apr 2025","isImplemented":true,"numberOfQuestions":179,"id":71},"currentPage":28},"__N_SSP":true}