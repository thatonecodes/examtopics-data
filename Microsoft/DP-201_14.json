{"pageProps":{"questions":[{"id":"WAeXpTB3yOYmAua5msYN","answer_description":"CONT_SQL3 requires an initial scale of 35000 IOPS.\nUltra SSD Managed Disk Offerings\n\nThe following table provides a comparison of ultra solid-state-drives (SSD) (preview), premium SSD, standard SSD, and standard hard disk drives (HDD) for managed disks to help you decide what to use.\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/disks-types","unix_timestamp":1608248460,"answers_community":[],"answer":"C","choices":{"A":"Standard SSD Managed Disk","C":"Ultra SSD Managed Disk","B":"Premium SSD Managed Disk"},"timestamp":"2020-12-18 00:41:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0002100005.jpg","https://www.examtopics.com/assets/media/exam-media/03774/0002200001.png"],"topic":"19","isMC":true,"exam_id":66,"question_id":66,"question_text":"You need to design a solution to meet the SQL Server storage requirements for CONT_SQL3.\nWhich type of disk should you recommend?","question_images":[],"discussion":[{"content":"The answer is correct, but it is interesting to note that data engineers are apparently supposed to know about hard disk selections for a VM.","timestamp":"1610654520.0","comments":[{"comment_id":"381137","upvote_count":"1","poster":"ZodiaC","content":"True it is weird...","timestamp":"1623592320.0"}],"upvote_count":"11","poster":"mohowzeh","comment_id":"267357"},{"poster":"davem0193","upvote_count":"1","comment_id":"387509","timestamp":"1624323660.0","content":"https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types"}],"url":"https://www.examtopics.com/discussions/microsoft/view/40268-exam-dp-201-topic-19-question-1-discussion/","answer_ET":"C"},{"id":"S960i2mvKKxVMWUBldy1","question_text":"You need to recommend an Azure SQL Database service tier.\nWhat should you recommend?","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/8918-exam-dp-201-topic-19-question-2-discussion/","exam_id":66,"timestamp":"2019-11-22 23:41:00","answer_description":"The data engineers must set the SQL Data Warehouse compute resources to consume 300 DWUs.\nNote: There are three architectural models that are used in Azure SQL Database:\n✑ General Purpose/Standard\n✑ Business Critical/Premium\n✑ Hyperscale\nIncorrect Answers:\nA: Business Critical service tier is designed for the applications that require low-latency responses from the underlying SSD storage (1-2 ms in average), fast recovery if the underlying infrastructure fails, or need to off-load reports, analytics, and read-only queries to the free of charge readable secondary replica of the primary database.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-service-tier-business-critical","question_images":[],"unix_timestamp":1574462460,"answer":"C","question_id":67,"topic":"19","answer_images":[],"discussion":[{"comment_id":"73813","upvote_count":"68","comments":[{"timestamp":"1594599120.0","poster":"Sedos","comment_id":"133343","content":"In the vCore-based purchasing model, you can choose between the General Purpose and Business Critical service tiers for SQL Database and SQL Managed Instance.\n\n\nIn the DTU-based purchasing model, you can choose between the basic, standard, and premium service tiers for Azure SQL Database.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/purchasing-models","upvote_count":"7"},{"content":"Zone redundancy storage is in preview for General Purpose. Answer is B.","comment_id":"270361","timestamp":"1610981340.0","poster":"ACSC","upvote_count":"1"},{"upvote_count":"4","comment_id":"365124","timestamp":"1621820640.0","content":"Now General Purpose supports Zone Redundancy. And it has cost advantages. So the answer is CORRECT","poster":"memo43"}],"content":"1. \"CONT_SQL1 and CONT_SQL2 must use the vCore model\", so the answer would be either \"Business Critical\" or \"General Purpose\"\n2. \"replicas and zone redundancy are required\", so the final answer is \"Business Critical\", because it is the only option which offers \"zone redundancy storage\".","poster":"Luke97","timestamp":"1586719200.0"},{"comments":[{"upvote_count":"7","timestamp":"1574885400.0","content":"There's also \"You must be able to independently scale compute and storage resources.\" so definitely only vCore. I vote for B as well.","poster":"MK_","comment_id":"24883"},{"comment_id":"333236","comments":[{"content":"My comment above is wrong, General Purpose limit is 12800 iops. Answer will be General purpose because also has zone redundancy since a few months .","poster":"maynard13x8","upvote_count":"3","timestamp":"1618140420.0","comment_id":"333243"}],"content":"There isn’t any option in General purpose with 8000 iops. It should be Business critical.","timestamp":"1618139880.0","upvote_count":"2","poster":"maynard13x8"}],"timestamp":"1574462460.0","upvote_count":"32","content":">>> CONT_SQL1 and CONT_SQL2 must use the vCore model \n\nVCore allow only general purpose or business critical service tiers.\n\nSince one goal is to contain costs, I'd vote for B (General Purpose)","comment_id":"23749","poster":"mauromi"},{"timestamp":"1619728140.0","content":"A. Business Critical","comment_id":"345668","upvote_count":"3","poster":"davita8"},{"upvote_count":"3","poster":"vmakhija","timestamp":"1619060160.0","content":"the diagram on this page can be useful -\nhttps://www.mssqltips.com/sqlservertip/5571/whats-in-a-dtu-choosing-the-right-resource-model-and-service-tier-for-azure-db/","comment_id":"340738"},{"content":"VCore model doesn't have Standart/Premium It has General Purpose, Bus Critical, Hyper..\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/service-tiers-vcore?tabs=azure-portal\nanswer B - General purpose","poster":"AlexD332","upvote_count":"1","timestamp":"1615652640.0","comment_id":"309844"},{"comment_id":"309842","poster":"AlexD332","timestamp":"1615652400.0","content":"The data engineers must set the SQL Data Warehouse compute resources to consume 300 DWUs.\nit's not related to SQL db","upvote_count":"1"},{"upvote_count":"3","content":"Business critical because it is vCore model and supports 8500 IOP required:\nGeneral Purpose boasts 500 IOPS per vCore to a maximum of 7000, while Business Critical is 5000 per vCore up to a maximum of 200,000 IOPS. \nhttps://www.clicdata.com/blog/microsoft-azure-sql-database-vcore-versus-dtu-which-one-to-chose/","poster":"felmasri","comments":[{"content":"General purpose supports up to 20,000 IOPS + 4 TB data.","upvote_count":"3","comment_id":"316602","comments":[{"comment_id":"333237","timestamp":"1618139940.0","content":"Where do you see that. Any link?","upvote_count":"1","poster":"maynard13x8"}],"timestamp":"1616352840.0","poster":"Jony2"}],"comment_id":"307555","timestamp":"1615428540.0"},{"content":"Premium gives up to 4 TB while Standard up to 1TB (databases given have 2TB)","upvote_count":"1","comment_id":"301401","poster":"watata","timestamp":"1614609180.0"},{"content":"\"CONT_SQL1 and CONT_SQL2 must use the vCore model\"\nso C,D and E are out\nBetween A and B, I'd pick A\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/service-tier-business-critical#when-to-choose-this-service-tier\nFrom the above link, there is a segment on updates. Since \"The storage should be configured to optimized storage for database OLTP workloads.\", Business Critical is the answer","poster":"syu31svc","upvote_count":"2","comment_id":"236257","timestamp":"1607240760.0"},{"content":"Now even General Purpose supports Zone Redundancy.","poster":"198981309810839189283928938298","comment_id":"215883","upvote_count":"5","timestamp":"1604920380.0"},{"timestamp":"1603794120.0","upvote_count":"2","comment_id":"206962","poster":"M0e","content":"vCore + zone redundant storage = business critical\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/service-tiers-vcore?tabs=azure-portal"},{"poster":"Der","comment_id":"191910","content":"I think this question is outdated.","timestamp":"1601687940.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1601282280.0","poster":"Heady978","content":"Basic, Standard & Premium are all DTU models - so no!\nBoth, general purpose & business critical are vCore models. Both support 2TB if you choose at least 12 vCores.\nBusiness critical has the option \"zone redundancy\", but the scenario asked for \"regional outage\", so \"zone redundancy\" isnt enough.\nI would take general purpose, because there is no option with a regional redundancy.","comment_id":"188910"},{"upvote_count":"2","comment_id":"92166","timestamp":"1589898660.0","content":"Due to vCore pricing model, the Business Critical type should be chosen. For number of vCores >=12 you can have storage up to 3TB.","poster":"willdy123"},{"content":"By selecting a zone redundant configuration, you can make your Premium or Business Critical databases resilient to a much larger set of failures, including catastrophic datacenter outages, without any changes to the application logic. You can also convert any existing Premium or Business Critical databases or pools to the zone redundant configuration.\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-high-availability#zone-redundant-configuration\n\nPremium supports Zone redundant configuration","poster":"azurearch","timestamp":"1589371620.0","comment_id":"88247","upvote_count":"1"},{"timestamp":"1584718980.0","content":"Answer - B","upvote_count":"3","poster":"Nehuuu","comment_id":"66295"},{"poster":"mcurko123","content":"A. Business Critical -> replicas and zone redundancy are required","comment_id":"50402","upvote_count":"25","comments":[{"upvote_count":"3","poster":"aratnat","comment_id":"253675","timestamp":"1609108380.0","content":"For General purpose zone redundancy is in preview"}],"timestamp":"1581665580.0"}],"choices":{"D":"Standard","C":"Premium","E":"Basic","B":"General Purpose","A":"Business Critical"},"answer_ET":"C","answers_community":[]},{"id":"o3IhLYtcXlslJIpXR6RO","answers_community":[],"choices":{"A":"Enable auto-shrink on the database.","C":"Enable Apache Spark RDD (RDD) caching.","E":"Configure the reading speed using Azure Data Studio.","B":"Flush the blob cache using Windows PowerShell.","D":"Enable Databricks IO (DBIO) caching."},"question_id":68,"answer_description":"Scenario: You must be able to use a file system view of data stored in a blob. You must build an architecture that will allow Contoso to use the DB FS filesystem layer over a blob store.\nDatabricks File System (DBFS) is a distributed file system installed on Azure Databricks clusters. Files in DBFS persist to Azure Blob storage, so you won't lose data even after you terminate a cluster.\nThe Databricks Delta cache, previously named Databricks IO (DBIO) caching, accelerates data reads by creating copies of remote files in nodes' local storage using a fast intermediate data format. The data is cached automatically whenever a file has to be fetched from a remote location. Successive reads of the same data are then performed locally, which results in significantly improved reading speed.\nReference:\nhttps://docs.databricks.com/delta/delta-cache.html#delta-cache\nDesign Azure data storage solutions","answer_images":[],"question_text":"You need to recommend the appropriate storage and processing solution?\nWhat should you recommend?","timestamp":"2019-11-19 18:42:00","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/8587-exam-dp-201-topic-19-question-3-discussion/","isMC":true,"discussion":[{"timestamp":"1574185320.0","comment_id":"22771","content":"Answer is D, not C","poster":"STH","upvote_count":"44"},{"timestamp":"1588682940.0","comment_id":"84125","upvote_count":"13","content":"The entire explanation supports D, answer should be D. Dittos explanation below does not eliminate D, right?","poster":"runningman"},{"poster":"davita8","timestamp":"1619728200.0","comment_id":"345671","upvote_count":"2","content":"D. Enable Databricks IO (DBIO) caching."},{"content":"Databricks IO cache, now Delta cache, is used in the context of a delta lake, which is not the case here. Apache RDD caching is to keep datasets in memory, which seems more fit to purpose? In the end I do not know for sure. What I do know, is that this business case and its questions excel in vagueness and inaccuracy of wording.","comment_id":"267386","timestamp":"1610655900.0","poster":"mohowzeh","upvote_count":"2"},{"comment_id":"244660","poster":"BungyTex","timestamp":"1608043680.0","upvote_count":"1","content":"Answer ticked is C, but then the explanation below talks about D."},{"comment_id":"240083","poster":"syu31svc","upvote_count":"2","timestamp":"1607602440.0","content":"\"You must build an architecture that will allow Contoso to use the DB FS filesystem layer over a blob store\"\nAnswer is D for sure"},{"comment_id":"41469","poster":"ditto","timestamp":"1579653600.0","upvote_count":"3","content":"I think it's c because the different file formats acceptable. \n\nThe Delta cache supports reading Parquet files in DBFS, Amazon S3, HDFS, Azure Blob storage, Azure Data Lake Storage Gen1, and Azure Data Lake Storage Gen2 (on Databricks Runtime 5.1 and above). It does not support other storage formats such as CSV, JSON, and ORC.\nhttps://docs.databricks.com/delta/optimizations/delta-cache.html#delta-and-rdd-cache-comparison"},{"timestamp":"1577900100.0","content":"Correct, Answer here should be D, not C","poster":"Shir","comment_id":"34284","upvote_count":"5"}],"topic":"19","answer_ET":"C","unix_timestamp":1574185320,"answer":"C","exam_id":66},{"id":"jWmReSZ56QsXMydMPzIW","answer":"D","question_images":[],"question_id":69,"unix_timestamp":1601623140,"url":"https://www.examtopics.com/discussions/microsoft/view/33411-exam-dp-201-topic-2-question-1-discussion/","isMC":true,"choices":{"B":"Terminate the cluster manually when processing completes.","D":"Pin the cluster.","C":"Create an Azure runbook that starts the cluster every 90 days.","A":"Clone the cluster after it is terminated."},"exam_id":66,"answer_description":"To keep an interactive cluster configuration even after it has been terminated for more than 30 days, an administrator can pin a cluster to the cluster list.\nReference:\nhttps://docs.azuredatabricks.net/clusters/clusters-manage.html#automatic-termination","answer_ET":"D","timestamp":"2020-10-02 09:19:00","topic":"2","discussion":[{"upvote_count":"12","comment_id":"236295","content":"This is same as Topic 1 Qn 12\nAnswer is D","poster":"syu31svc","timestamp":"1607245860.0"}],"answer_images":[],"question_text":"You are designing an Azure Databricks interactive cluster. The cluster will be used infrequently and will be configured for auto-termination.\nYou need to ensure that the cluster configuration is retained indefinitely after the cluster is terminated. The solution must minimize costs.\nWhat should you do?","answers_community":[]},{"id":"IZdQmJammpBW08xXSjBJ","choices":{"C":"Parquet","D":"JSON","B":"CSV","A":"Avro"},"topic":"2","timestamp":"2021-03-12 04:19:00","answers_community":[],"question_images":[],"discussion":[{"timestamp":"1615519140.0","comment_id":"308534","poster":"felmasri","upvote_count":"52","content":"I think this Answer is wrong since polybase does not support Avro.\nI will pick Parquet"},{"comment_id":"321636","content":"I understand that Databricks and Polybase will consume the data independently ... So, based on that premise the selected output format from Synapse Stream Analytics should be a format compatible with both. Since, we need the file format to be a distributed file format for speed up the queries, the only possible solutions are AVRO and Parquet. As, AVRO is no a valid solution as Polybase doesn't support this format, the only possible answer is PARQUET","poster":"jms309","timestamp":"1616825640.0","upvote_count":"15"},{"comment_id":"480725","upvote_count":"1","poster":"massnonn","timestamp":"1637241240.0","content":"for me the correct answer is parquet"},{"upvote_count":"3","poster":"dumpi","content":"Parquet is correct answer I verify","timestamp":"1623124800.0","comment_id":"377202"},{"timestamp":"1622167140.0","content":"Agreed with Parquet","poster":"KpKo","comment_id":"368426","upvote_count":"2"},{"timestamp":"1621844820.0","poster":"cadio30","comment_id":"365421","content":"Both services uses CSV and parquet as input files though parquet is the candidate for this requirement as it is the recommended file format for azure databricks and is also supported by polybase","upvote_count":"2"},{"content":"C. Parquet","comment_id":"345300","poster":"davita8","timestamp":"1619695020.0","upvote_count":"3"},{"timestamp":"1617694500.0","comment_id":"329410","upvote_count":"7","poster":"maciejt","content":"JSON and CSV don't define the types strongly and we need to preserve the data types, so those 2 are exuded.\nParquet is better optimized for read, avro is for write and requirement is to make queries fast, so parquet.\nhttps://www.datanami.com/2018/05/16/big-data-file-formats-demystified/"},{"content":"its Parquet file format","timestamp":"1616578140.0","poster":"Nik71","comment_id":"318987","upvote_count":"2"},{"content":"Polybase support requirement eliminates Avro. Not sure what the right answer is.","upvote_count":"1","timestamp":"1616538360.0","poster":"al9887655","comment_id":"318546"},{"poster":"H_S","timestamp":"1615813080.0","upvote_count":"1","comments":[{"upvote_count":"2","timestamp":"1615813200.0","comment_id":"311431","content":"https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs\nit's PARKET","poster":"H_S"}],"comment_id":"311426","content":"avro is not supported by polybase, but why not CSV"},{"content":"I think Parquet is the right Answer","timestamp":"1615630620.0","comment_id":"309615","poster":"kz_data","upvote_count":"1"}],"isMC":true,"answer":"A","exam_id":66,"question_id":70,"url":"https://www.examtopics.com/discussions/microsoft/view/46620-exam-dp-201-topic-2-question-10-discussion/","question_text":"You plan to ingest streaming social media data by using Azure Stream Analytics. The data will be stored in files in Azure Data Lake Storage, and then consumed by using Azure Databricks and PolyBase in Azure Synapse Analytics.\nYou need to recommend a Stream Analytics data output format to ensure that the queries from Databricks and PolyBase against the files encounter the fewest possible errors. The solution must ensure that the files can be queried quickly and that the data type information is retained.\nWhat should you recommend?","answer_ET":"A","answer_description":"The Avro format is great for data and message preservation.\nAvro schema with its support for evolution is essential for making the data robust for streaming architectures like Kafka, and with the metadata that schema provides, you can reason on the data. Having a schema provides robustness in providing meta-data about the data stored in Avro records which are self- documenting the data.\nReferences:\nhttp://cloudurable.com/blog/avro/index.html","answer_images":[],"unix_timestamp":1615519140}],"exam":{"isImplemented":true,"isMCOnly":false,"numberOfQuestions":206,"provider":"Microsoft","isBeta":false,"id":66,"lastUpdated":"12 Apr 2025","name":"DP-201"},"currentPage":14},"__N_SSP":true}