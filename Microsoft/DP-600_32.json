{"pageProps":{"questions":[{"id":"hy3x8TS4mQEBG6qkB2w4","answer":"","answer_images":["https://img.examtopics.com/dp-600/image104.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/137988-exam-dp-600-topic-1-question-78-discussion/","topic":"1","answer_ET":"","isMC":false,"exam_id":71,"discussion":[{"timestamp":"1713167640.0","content":"Correct. We need more questions like these. People are forgetting the importance of data modeling.\n\nStatic table:\nType 0 – Fixed Dimension. No changes allowed, dimension never changes.\n\nCommonly used SCD types:\nType 1 – No History. Update record directly, there is no record of historical values, only current state.\nType 2 – Row Versioning.\nType 3 – Previous Value column.\n\nRarely used:\nType 4 – History Table. \nType 6 – Hybrid SCD.","poster":"9878eb9","comment_id":"1195891","upvote_count":"45"},{"content":"seems correct and - Type 0 SCD, we ignore any changes and simply audit them\n\nType 1 SCD, we overwrite existing data with new values. Historical versions are not preserved.\n\nType 2 SCD involves adding new rows to the dimension table when changes occur. It allows us to track historical versions of attributes.","poster":"prabhjot","comment_id":"1190014","timestamp":"1712338380.0","upvote_count":"9"},{"comment_id":"1315829","upvote_count":"1","content":"Customers -> Type 2\nProducts -> Type 1","poster":"nappi1","timestamp":"1732194480.0"},{"poster":"Nefirs","timestamp":"1713507300.0","upvote_count":"4","content":"correct, Type 2 (new row) and Type 1 (overwrite)","comment_id":"1198361"}],"timestamp":"2024-04-05 19:33:00","question_id":156,"question_images":["https://img.examtopics.com/dp-600/image102.png","https://img.examtopics.com/dp-600/image103.png"],"unix_timestamp":1712338380,"question_text":"DRAG DROP\n-\n\nYou are implementing two dimension tables named Customers and Products in a Fabric warehouse.\n\nYou need to use slowly changing dimension (SCD) to manage the versioning of data. The solution must meet the requirements shown in the following table.\n\n//IMG//\n\n\nWhich type of SCD should you use for each table? To answer, drag the appropriate SCD types to the correct tables. Each SCD type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_description":"","answers_community":[]},{"id":"WDmYuPH6oYWnPnZcDeE2","discussion":[{"content":"Fuzzy Matching: Fuzzy matching allows you to match data even when there are minor differences, such as extra spaces, in the values. This eliminates the need to manually clean or preprocess the data before the join.","poster":"9878eb9","timestamp":"1728979080.0","upvote_count":"23","comment_id":"1195893"},{"content":"Selected Answer: B\nMerge = join\nappend = union\nso the answer is B \nwhy?\n Fuzzy Matching: Fuzzy matching allows you to match data even when there are minor differences, such as extra spaces, in the values. This eliminates the need to manually clean or preprocess the data before the join.","timestamp":"1733985840.0","comment_id":"1325438","upvote_count":"2","poster":"Mhunity"},{"content":"Selected Answer: B\nIMHO, B is good.\n\nNot append (append is like UNION).\nNot D, because spaces, and we need minimal efforts","comment_id":"1210387","poster":"stilferx","timestamp":"1731451500.0","upvote_count":"4"},{"content":"Selected Answer: B\nMerge as the requirement is for inner join. \ndoc on fuzzy join and fuzzy match:\nhttps://learn.microsoft.com/en-us/powerquery-m/table-fuzzyjoin\nhttps://learn.microsoft.com/en-us/power-query/merge-queries-fuzzy-match","upvote_count":"3","timestamp":"1729910820.0","poster":"4371883","comment_id":"1202366"},{"comment_id":"1198426","content":"Selected Answer: B\nB should be correct","poster":"Nefirs","upvote_count":"3","timestamp":"1729325340.0"}],"answer":"B","answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/138724-exam-dp-600-topic-1-question-79-discussion/","exam_id":71,"answer_images":[],"isMC":true,"answers_community":["B (100%)"],"question_id":157,"question_text":"You have a Fabric workspace named Workspace1 and an Azure SQL database.\n\nYou plan to create a dataflow that will read data from the database, and then transform the data by performing an inner join.\n\nYou need to ignore spaces in the values when performing the inner join. The solution must minimize development effort.\n\nWhat should you do?","choices":{"D":"Merge the queries by using a lookup table.","C":"Append the queries by using a lookup table.","B":"Merge the queries by using fuzzy matching.","A":"Append the queries by using fuzzy matching."},"timestamp":"2024-04-15 09:58:00","topic":"1","answer_ET":"B","unix_timestamp":1713167880},{"id":"eUODi7p8NCnRvRIJSnKx","timestamp":"2024-02-09 14:08:00","exam_id":71,"answer_images":[],"topic":"1","question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nLitware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.\n\nExisting Environment -\n\nFabric Environment -\nLitware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.\n\nAvailable Data -\nLitware has data that must be analyzed as shown in the following table.\n//IMG//\n\nThe Product data contains a single table and the following columns.\n//IMG//\n\nThe customer satisfaction data contains the following tables:\n\nSurvey -\n\nQuestion -\n\nResponse -\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.\n\nUser Problems -\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.\nProduct data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.\n\nRequirements -\n\nPlanned Changes -\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity\nThe following three workspaces will be created:\nAnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store\nDataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake\nDataSciPOC: Will contain all the notebooks and reports created by the data scientists\nThe following will be created in the AnalyticsPOC workspace:\nA data store (type to be decided)\n\nA custom semantic model -\n\nA default semantic model -\n\nInteractive reports -\nThe data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers’ discretion.\nAll the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.\n\nTechnical Requirements -\nThe data store must support the following:\nRead access by using T-SQL or Python\nSemi-structured and unstructured data\nRow-level security (RLS) for users executing T-SQL queries\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\nData will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model\nThe data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model\nThe dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\nThe product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:\nList prices that are less than or equal to 50 are in the low pricing group.\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\nList prices that are greater than 1,000 are in the high pricing group.\n\nSecurity Requirements -\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\nFabric administrators will be the workspace administrators.\nThe data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.\nThe analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.\nThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\nThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.\nThe date dimension must be available to all users of the data store.\nThe principle of least privilege must be followed.\nBoth the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:\nFabricAdmins: Fabric administrators\nAnalyticsTeam: All the members of the analytics team\nDataAnalysts: The data analysts on the analytics team\nDataScientists: The data scientists on the analytics team\nDataEngineers: The data engineers on the analytics team\nAnalyticsEngineers: The analytics engineers on the analytics team\n\nReport Requirements -\nThe data analysts must create a customer satisfaction report that meets the following requirements:\nEnables a user to select a product to filter customer survey responses to only those who have purchased that product.\nDisplays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.\nShows data as soon as the data is updated in the data store.\nEnsures that the report and the semantic model only contain data from the current and previous year.\nEnsures that the report respects any table-level security specified in the source data store.\nMinimizes the execution time of report queries.\nWhat should you recommend using to ingest the customer data into the data store in the AnalyticsPOC workspace?","choices":{"A":"a stored procedure","B":"a pipeline that contains a KQL activity","C":"a Spark notebook","D":"a dataflow"},"answers_community":["D (94%)","6%"],"discussion":[{"poster":"VAzureD","upvote_count":"9","content":"Selected Answer: D\nD. a dataflow\nEs la mejor opción.\n\"Whenever possible, the data engineers will use low-code tools for data ingestion.\"\n\"Data will be loaded without transformation in one area of the AnalyticsPOC data store\"\n\n\nA. a stored procedure\nNo tiene sentido usar un procedimiento almacenado para hacer la carga.\n\nB. a pipeline that contains a KQL activity\nKQL es para datos en tiempo real\n\nC. a Spark notebook\nPodría valer, pero el texto pone,\n\"Whenever possible, the data engineers will use low-code tools for data ingestion.\"","comments":[{"content":"i feel answer is pipeline with KQL and using kql we get real time data","timestamp":"1713762840.0","poster":"amar5555","comment_id":"1199969","upvote_count":"2","comments":[{"content":"We don't need real-time data, it is updated either hourly or daily based on the source. The instant refresh is only a reporting requirement, not an ingestion requirement. I'm *assuming* that the customer data would be updated daily.","timestamp":"1730373960.0","upvote_count":"1","poster":"semauni","comment_id":"1305388"}]}],"timestamp":"1712754240.0","comment_id":"1193007"},{"timestamp":"1708504320.0","poster":"David_Webb","content":"Selected Answer: D\nIn the Interactive reports requirement, it stated, \"Whenever possible, the data engineers will use low-code tools for data ingestion\".","upvote_count":"9","comment_id":"1155367"},{"timestamp":"1741041420.0","content":"Selected Answer: D\n\"Whenever possible, the data engineers will use low-code tools for data ingestion.\"\n\"Data will be loaded without transformation in one area of the AnalyticsPOC data store\"","comment_id":"1364622","poster":"Amine_spiegel94","upvote_count":"1"},{"comment_id":"1352232","content":"Selected Answer: D\nD (Dataflow) because low-code tools are to be used. There are no additional requirements, so no need, to do real-time processing.","poster":"7addd81","timestamp":"1738826820.0","upvote_count":"1"},{"timestamp":"1736878320.0","poster":"Vulkany","content":"Selected Answer: C\nC) Spark supports real-time or near real-time ingestion of Delta Lake-compliant Parquet data.\nIt provides full control over data processing and transformation, ensuring data availability as soon as it's ingested.\nWhile not \"low-code,\" it’s the most capable option for real-time processing and Delta Lake integration.\n\nWhy D) is not the option (in my opinion)\nDataflows rely on scheduled refresh, introducing a delay.\nThis violates the real-time data availability requirement, making it less suitable despite being low-code and user-friendly.","upvote_count":"2","comment_id":"1340479"},{"comment_id":"1331465","timestamp":"1735117380.0","content":"Selected Answer: D\na dataflow","upvote_count":"2","poster":"NRezgui"},{"timestamp":"1732905420.0","upvote_count":"1","content":"Selected Answer: D\nD, because they said to use low-code tools for data ingestion.","comment_id":"1319855","poster":"lagraoui"},{"timestamp":"1731650100.0","poster":"Rakesh16","upvote_count":"1","content":"Selected Answer: D\nDataflow is the answer","comment_id":"1312422"},{"upvote_count":"1","comment_id":"1303295","timestamp":"1729950960.0","poster":"jass007_k","content":"Its option D) Dataflow because dataflows are generally better in case of small and medium datasets and size of Product data is 200 MB"},{"content":"Strange, no one had got the correct answer yet: it is Spark Notebook.\nIf you had 50 GB, then dataflow is fine, but premium Fabric won't manage to handle 500 GB with dataflow if you need to do any updates on the data. And you should manage with less than premium/F64 capacity for these requirements. So the extra cost for Notebook programming will be saved many times over with lower capacity requirements.","comments":[{"content":"But we're not loading the survey data, the question asks about the customer data. Which is 50 MB. If we were loading the survey data, I would agree that notebooks would be best (\"when POSSIBLE\").","poster":"semauni","timestamp":"1730373840.0","comment_id":"1305384","upvote_count":"1"}],"upvote_count":"1","timestamp":"1728983880.0","comment_id":"1298160","poster":"TimoRii"},{"content":"Selected Answer: D\nIMHO,\nD (dataflow), because of this: \n\"Whenever possible, the data engineers will use low-code tools for data ingestion.\"","timestamp":"1715118000.0","upvote_count":"4","poster":"stilferx","comment_id":"1208045"},{"upvote_count":"2","poster":"rmeng","content":"Selected Answer: D\nWhenever possible, the data engineers will use low-code tools for data ingestion.","timestamp":"1714389180.0","comment_id":"1203975"},{"upvote_count":"1","content":"Selected Answer: C\n\"Shows data as soon as the data is updated in the data store\" \n\nKQL?","poster":"dp600","timestamp":"1714207200.0","comments":[{"poster":"dp600","upvote_count":"3","comment_id":"1203023","timestamp":"1714207260.0","content":"nevermind, it specified as soon as it gets to the data store, that implies as soon as it's loaded in fabric"}],"comment_id":"1203022"},{"comment_id":"1199967","upvote_count":"1","content":"i feel answer is pipeline with KQL and using kql we get real time data","poster":"amar5555","timestamp":"1713762780.0"},{"poster":"clux","comment_id":"1190452","content":"Selected Answer: D\ncorrect","timestamp":"1712415720.0","upvote_count":"1"},{"timestamp":"1710869820.0","content":"Selected Answer: D\nDataflow is the best choice.","comment_id":"1177575","upvote_count":"2","poster":"a_51"},{"poster":"rmeng","timestamp":"1709372100.0","content":"Guys, what about this requirement ? \"Shows data as soon as the data is updated in the data store.\"","upvote_count":"1","comment_id":"1164021","comments":[{"comment_id":"1169429","timestamp":"1709984340.0","content":"That's a report requirement, not a data ingestion requirement. You are talking here how to get data from a place to the analyticsPOC, not getting the data from a semantic model directly into the report.","poster":"roflcopter123","upvote_count":"5"}]},{"poster":"SamuComqi","upvote_count":"5","comments":[],"content":"Selected Answer: D\nD. a dataflow\n\nEven though the text reads \"Data will be loaded without transformation in one area of the AnalyticsPOC data store\": in general, dataflows are used when data transformations are involved after ingestion. As suggested by user BHARAT, the Copy Activity should be the optimal solution.","timestamp":"1708242540.0","comment_id":"1153092"},{"timestamp":"1708206540.0","poster":"Momoanwar","upvote_count":"1","comment_id":"1152848","content":"Selected Answer: D\nD see Bharat comment."},{"timestamp":"1707932220.0","content":"Ideally, It should be the COPY activity of the pipeline, but that is not given as a choice","upvote_count":"4","poster":"Bharat","comment_id":"1150383"},{"content":"Selected Answer: D\n\"Whenever possible, the data engineers will use low-code tools for data ingestion.\"","comment_id":"1148211","timestamp":"1707749100.0","poster":"Nicofr","upvote_count":"3"},{"comment_id":"1145481","timestamp":"1707484080.0","upvote_count":"3","poster":"theseon","comments":[{"poster":"semauni","timestamp":"1730373780.0","comment_id":"1305383","content":"It is mostly about the low code. Regardless of using dataflows, data pipelines or notebooks, you can load data, transform it and write it away, or you can just load and write away. Notebooks doesn't require you doing transformations.","upvote_count":"1"}],"content":"Selected Answer: D\nSpark Notebook is also possible but i would say D is correct: \n\"Data will be loaded without transformation in one area of the AnalyticsPOC data store\""}],"unix_timestamp":1707484080,"question_id":158,"answer_ET":"D","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/133447-exam-dp-600-topic-1-question-8-discussion/","answer":"D","question_images":["https://img.examtopics.com/dp-600/image4.png","https://img.examtopics.com/dp-600/image5.png"],"isMC":true},{"id":"puYyhMWSHyAWLbwIwnsv","discussion":[{"comments":[{"poster":"utsuha","comment_id":"1217909","content":"To clarify, AS CLONE OF does not copy the data. See this documentation: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-clone-of-transact-sql?view=fabric\n\nOption C will only copy over the metadata of table schema1.city, so I guess it is minimizing the copying of data by copying no data, lol. Option D is most correct if we want to duplicate the table itself, data and metadata. I'm sure the question is just slightly worded wrong and the intended correct answer is C, i.e. we only want to make a copy of the metadata.","comments":[{"poster":"Qordata","upvote_count":"3","timestamp":"1723017360.0","comment_id":"1261994","content":"utsuha option C is correct please read this\nUpon creation, a table clone is an independent and separate copy of the data from its source.\n\nAny changes made through DML or DDL on the source of the clone table are not reflected in the clone table.\nSimilarly, any changes made through DDL or DML on the table clone are not reflected on the source of the clone table.\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table#separate-and-independent"},{"poster":"Karthiknaidu123","upvote_count":"3","timestamp":"1724762640.0","content":"utsuha, the question clearly mentions \"The solution must minimize the copying of data.\"\nThat's why, IMO option C is correct.","comment_id":"1273428"},{"poster":"Pegooli","content":"no copy data but pointing to source data.","comment_id":"1253410","upvote_count":"1","timestamp":"1721708280.0"},{"content":"Tested option D \"clone\" is the best because clone is like a link to original source","timestamp":"1718581560.0","upvote_count":"1","poster":"DarioReymago","comment_id":"1231580","comments":[{"poster":"DarioReymago","content":"I mean tested I select option C \"CREATE TABLE schema2.city AS CLONE OF schema1.city;\"","comment_id":"1231581","upvote_count":"1","timestamp":"1718582040.0"}]}],"upvote_count":"6","timestamp":"1716597120.0"}],"upvote_count":"30","comment_id":"1191194","poster":"neoverma","timestamp":"1712522580.0","content":"The “CREATE TABLE … AS CLONE OF” statement is the most efficient way to create a copy of a table in the same Fabric tenant. This statement creates a new table in the specified schema that is an exact clone of the source table, including the table structure, data, and indexes. This approach minimizes the amount of data copied, as it simply creates a reference to the underlying data rather than performing a full table scan and data copy.\n\nOptions A and B, which use INSERT INTO or SELECT INTO, would require scanning the entire source table and copying the data row-by-row, which is less efficient than the CLONE method.\n\nOption D, CREATE TABLE … AS SELECT, would also work to create a copy of the table, but it would perform a full table scan and data copy, which is less efficient than the CLONE approach."},{"timestamp":"1712496480.0","upvote_count":"6","content":"Selected Answer: C\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table\nMicrosoft Fabric offers the capability to create near-instantaneous zero-copy clones with minimal storage costs.","comment_id":"1190972","poster":"clux"},{"timestamp":"1743400920.0","poster":"Rataxe","comment_id":"1413966","upvote_count":"1","content":"Selected Answer: D\nThe schemas are existing so the correct answer is CREATE TABLE schema2.city AS SELECT * FROM schema1.city;"},{"content":"Selected Answer: C\nshould be C, since copying of data should be minimized","comment_id":"1291152","timestamp":"1727615700.0","upvote_count":"1","poster":"paffy"},{"comments":[{"poster":"cafb698","comment_id":"1274560","upvote_count":"1","timestamp":"1724942280.0","content":"I stand corrected. Though what I mentioned above is correct, there's a trick in the question. The requirement is not to copy the data, it is to MINIMIZE the copying of data. Which will happen in the case of CLONE AS. Hence, C is the correct answer."}],"poster":"cafb698","upvote_count":"1","timestamp":"1724942040.0","content":"Selected Answer: D\nCLONE AS Creates a new table as a zero-copy clone of another table in Warehouse in Microsoft Fabric. Only the metadata of the table is copied. The underlying data of the table, stored as parquet files, is not copied. \n\nA zero-copy clone creates a replica of the table by copying the metadata, while still referencing the same data files in OneLake. The metadata is copied while the underlying data of the table stored as parquet files is not copied. The creation of a clone is similar to creating a table within a Warehouse in Microsoft Fabric.","comment_id":"1274558"},{"poster":"sandy789","comment_id":"1231170","upvote_count":"2","timestamp":"1718498100.0","content":"go with D. C is a clone of table's metadata not copy data activity"},{"timestamp":"1717569900.0","poster":"vernillen","content":"Selected Answer: C\nAS CLONE basically minimizes the copying of data. That's the biggest requirement.","comment_id":"1224567","upvote_count":"3"},{"timestamp":"1717375980.0","content":"C. CREATE TABLE schema2.city AS CLONE OF schema1.city;\n\nThis statement creates a new table named city in schema2 that has the same structure as the city table in schema1 without copying any data. It essentially creates a metadata reference to the original table, which minimizes the data copying.","comment_id":"1223341","upvote_count":"2","poster":"PiyushT"},{"poster":"282b85d","comment_id":"1220430","timestamp":"1716915180.0","upvote_count":"1","content":"Option A: INSERT INTO schema2.city SELECT * FROM schema1.city;\nThis option assumes that schema2.city already exists. It will insert data into schema2.city but will not create the table.\n\nOption B: SELECT * INTO schema2.city FROM schema1.city;\nThis option copies data from schema1.city to schema2.city but is typically used in SQL Server to create a new table and copy data. However, it doesn't handle schema changes well and might not be the best for large datasets.\n\nOption C: CREATE TABLE schema2.city AS CLONE OF schema1.city;\nThis is not a valid T-SQL syntax for creating tables.\n\nOption D: CREATE TABLE schema2.city AS SELECT * FROM schema1.city;\nThis statement creates a new table schema2.city and copies all data from schema1.city into it (CTAS). This option is efficient for creating a new table with the same schema and data as the original table."},{"upvote_count":"2","comment_id":"1215380","timestamp":"1716348840.0","poster":"4fbcd40","content":"Selected Answer: D\nD is the correct answer. \n\n Only the metadata of the table is copied. The underlying data of the table, stored as parquet files, is not copied.\n\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-clone-of-transact-sql?view=fabric"},{"timestamp":"1715546880.0","poster":"stilferx","comment_id":"1210389","content":"IMHO, C is good.\n\nBecause: Within a warehouse, a clone of a table can be created near-instantaneously using simple T-SQL. A clone of a table can be created within or across schemas in a warehouse.\n\nHere: https://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table#creation-of-a-table-clone","upvote_count":"3"},{"upvote_count":"1","poster":"dp600","comment_id":"1203461","content":"Selected Answer: C\nI would go with C, is the fastest way to replicate the schema.","timestamp":"1714292640.0"},{"content":"Selected Answer: C\nC is correct - Cloning reduces data movement","comment_id":"1198428","poster":"Nefirs","timestamp":"1713514200.0","upvote_count":"2"},{"timestamp":"1713430620.0","upvote_count":"1","content":"Selected Answer: C\nC is correct","comment_id":"1197805","poster":"Jasneet"},{"content":"Selected Answer: D\nd is the write answer","timestamp":"1712409000.0","comments":[{"poster":"neoverma","content":"why? any explanation or reference to help learn?","comment_id":"1191187","timestamp":"1712522040.0","upvote_count":"2"}],"poster":"AGTraining","comment_id":"1190411","upvote_count":"3"}],"unix_timestamp":1712409000,"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/138029-exam-dp-600-topic-1-question-80-discussion/","question_images":[],"answer":"C","answers_community":["C (67%)","D (33%)"],"timestamp":"2024-04-06 15:10:00","question_text":"You have a Fabric tenant that contains a warehouse named Warehouse1. Warehouse1 contains two schemas name schema1 and schema2 and a table named schema1.city.\n\nYou need to make a copy of schema1.city in schema2. The solution must minimize the copying of data.\n\nWhich T-SQL statement should you run?","answer_images":[],"exam_id":71,"answer_description":"","question_id":159,"choices":{"D":"CREATE TABLE schema2.city AS SELECT * FROM schema1.city;","A":"INSERT INTO schema2.city SELECT * FROM schema1.city;","B":"SELECT * INTO schema2.city FROM schema1.city;","C":"CREATE TABLE schema2.city AS CLONE OF schema1.city;"},"topic":"1","answer_ET":"C"},{"id":"RYCPNxyxCsb4cO6ke8yQ","answer_images":[],"exam_id":71,"isMC":true,"discussion":[{"upvote_count":"25","content":"Read the question carefully: \"You need to prevent new tables added to Lakehouse1 from being added automatically to the default semantic model of the lakehouse.\" The answer is in the article https://learn.microsoft.com/en-us/fabric/data-warehouse/semantic-models but \"Sync the default Power BI semantic model\". The correct answer is there A","poster":"gfors","comments":[{"poster":"ManuelG00","timestamp":"1718366880.0","comments":[{"content":"Just curious, where you can do this in the workspace settings, I cannot find it here, but I can find it under the SQL endpoint.","comment_id":"1305002","poster":"4f3a33d","timestamp":"1730294040.0","upvote_count":"1"},{"timestamp":"1719407400.0","upvote_count":"3","comment_id":"1237470","poster":"Priyanka007","content":"In your workspace you will have the Warehouse or SQL analytics endpoint and that is where you need to do the setting. So, I will go with A. If you go with C it is like saying you do the setting in that page but when you say A it points out where exactly in the page you need to do that setting."}],"content":"So, pay attention. The Answer is C\n\"Previously we auto added all tables and views in the Warehouse to the default Power BI semantic model. Based on feedback, we have modified the default behavior to not automatically add tables and views to the default Power BI semantic model. This change will ensure the background sync will not get triggered. This will also disable some actions like \"New Measure\", \"Create Report\", \"Analyze in Excel\".\n\nIf you want to change this default behavior, you can:\n\nManually enable the Sync the default Power BI semantic model setting for each Warehouse or SQL analytics endpoint in the workspace. This will restart the background sync that will incur some consumption costs.\"","upvote_count":"4","comment_id":"1230470"}],"comment_id":"1190838","timestamp":"1712475060.0"},{"content":"Selected Answer: A\nI've confirmed it on Fabric, and the correct answer is A.\nTo change this setting, you need to open the settings of either the SQL analytics endpoint object of a Lakehouse or the warehouse object of a warehouse.","comment_id":"1208758","poster":"2fe10ed","comments":[{"timestamp":"1715242800.0","content":"A small detail: Fabric has since changed this setting, and now it comes off by default.\nIf anything you would need to turn it on, but the place where you do that is still the same.","comment_id":"1208760","poster":"2fe10ed","upvote_count":"4"}],"timestamp":"1715242620.0","upvote_count":"13"},{"timestamp":"1741689960.0","content":"Selected Answer: A\ni have checked on Fabric","comment_id":"1387351","upvote_count":"1","poster":"Jane5"},{"timestamp":"1741364820.0","comment_id":"1366333","poster":"aks2304","content":"Selected Answer: B\nThe correct answer is:\n\nB. the semantic model settings\n\nExplanation:\nTo prevent new tables added to a lakehouse (e.g., Lakehouse1) from being automatically included in the default semantic model, you need to configure the semantic model settings. The semantic model is responsible for defining how data is exposed for reporting and analytics. By adjusting these settings, you can control whether new tables are automatically added to the model or require manual inclusion.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1738546200.0","poster":"goldy29","comment_id":"1350716","content":"Selected Answer: B\nyou need to modify the semantic model settings for the lake house. This allows you to control whether new tables are automatically included in the model."},{"comment_id":"1322094","poster":"MultiCloudIronMan","content":"Selected Answer: A\nBased on the information from the Microsoft Fabric documentation, it appears that configuring the SQL analytics endpoint settings is indeed the correct approach to prevent new tables added to Lakehouse1 from being automatically included in the default semantic model","upvote_count":"2","timestamp":"1733347080.0"},{"upvote_count":"5","poster":"4f3a33d","timestamp":"1729239780.0","comment_id":"1299590","content":"Selected Answer: A\nTested in Fabric; go to > SQL Endpoint > ... > Settings > Default Power BI semantic model > Sync the default Power BI semantic model > On | Off\n\nAdd objects from the lakehouse to the default Power BI semantic model. Also, update the model with any new objects added to the lakehouse. You can use this model to build reports faster with lakehouse data."},{"upvote_count":"1","poster":"Ahmadpbi","content":"A is correct .. SQL analytics endpoint settings.. I checked this option in a workspace I have. It seems off by default. It shows like below:\nSync the default Power BI semantic model\nOff\nAdd objects from the lakehouse to the default Power BI semantic model. Also, update the model with any new objects added to the lakehouse. You can use this model to build reports faster with lakehouse data.","comment_id":"1254869","timestamp":"1721897100.0"},{"comment_id":"1250733","content":"I'm going for B\nit's chat GPT answer LOL\nAccess the Lakehouse Settings:\n\nNavigate to the Lakehouse1 in your Fabric tenant.\nGo to the settings or properties of the Lakehouse1.\nLocate the Semantic Model Configuration:\n\nFind the option related to the semantic model or data modeling settings.\nDisable Automatic Addition:\n\nLook for a setting that mentions the automatic addition of new tables to the semantic model.\nDisable this setting to prevent new tables from being automatically added.","upvote_count":"2","poster":"Pegooli","timestamp":"1721344680.0"},{"content":"Selected Answer: B\nB is correct","timestamp":"1720831800.0","upvote_count":"3","poster":"6d1de25","comment_id":"1247068"},{"poster":"b6daab0","upvote_count":"1","content":"Tested in Fabric. It's A.","timestamp":"1718835300.0","comment_id":"1233263"},{"upvote_count":"7","comment_id":"1224545","poster":"Evincible","comments":[{"poster":"DarioReymago","timestamp":"1718584020.0","comment_id":"1231588","upvote_count":"1","content":"I'm not sure, you lock the semantic model but you can create new tables in Lakehouse. With option A you'll lock both"}],"content":"Selected Answer: B\nTo prevent new tables added to Lakehouse1 from being automatically added to the default semantic model of the lakehouse, you need to configure the semantic model settings.\n\nThe semantic model in a Fabric lakehouse represents the logical data model and defines how the data in the lakehouse should be interpreted and used. By default, when new tables are added to the lakehouse, they may be automatically added to the semantic model as well.\n\nTo prevent this behavior and maintain more control over the contents of the semantic model, you need to adjust the semantic model settings. This allows you to configure the lakehouse to not automatically include new tables in the semantic model, requiring you to manually add them if desired.","timestamp":"1717567620.0"},{"upvote_count":"3","content":"You should configure B. the semantic model settings to prevent new tables from being automatically added to the default semantic model of Lakehouse1","poster":"[Removed]","timestamp":"1717406700.0","comment_id":"1223505"},{"comment_id":"1223342","timestamp":"1717376100.0","poster":"PiyushT","content":"Selected Answer: B\nOpen the lakehouse in Microsoft Fabric.\nNavigate to the semantic model section.\nOpen the settings for the default semantic model.\nLook for an option that controls the automatic inclusion of new tables. This might be labeled as \"auto-update\" or something similar.\nDisable or adjust this setting to prevent automatic inclusion","upvote_count":"3"},{"poster":"282b85d","content":"Selected Answer: B\nExplanation: In Microsoft Fabric, the default behavior is to include new tables in the semantic model automatically. To control this behavior and ensure that new tables are not added automatically, you need to adjust the settings related to the semantic model.","comment_id":"1220434","upvote_count":"3","timestamp":"1716915720.0"},{"upvote_count":"3","comment_id":"1207968","timestamp":"1715104740.0","poster":"00e2928","content":"correct answer is B\nBy default, the semantic model in a lakehouse is set to automatically include new tables, but you can disable this behavior by modifying the semantic model settings. Specifically, you need to navigate to the semantic model settings for the lakehouse and find the option to disable the automatic inclusion of new tables."},{"timestamp":"1714293000.0","comments":[{"comment_id":"1228444","poster":"282b85d","content":"Sync the default Power BI semantic model : toggle on/off\nAdd objects from the warehouse to the default Power BI semantic model. Also, update the model with any new objects added to the warehouse. You can use this model to build reports faster with warehouse data. (in semantic model settings)","timestamp":"1718109180.0","upvote_count":"1"}],"content":"Selected Answer: A\n\"Manually enable the Sync the default Power BI semantic model setting for each Warehouse or SQL analytics endpoint in the workspace\"","upvote_count":"3","poster":"dp600","comment_id":"1203464"},{"timestamp":"1713508500.0","comment_id":"1198371","upvote_count":"6","content":"Selected Answer: A\nA - check here: https://learn.microsoft.com/en-us/fabric/data-warehouse/default-power-bi-semantic-model#add-or-remove-objects-to-the-default-power-bi-semantic-model","poster":"Nefirs"},{"comment_id":"1194294","upvote_count":"4","content":"it should be SQL end point setting.","poster":"pppppppppie","timestamp":"1712917560.0"},{"poster":"earlqq","comment_id":"1193439","content":"A is correct, there is \"Sync the default Power BI semantic model\" setting in sql endpoint.","timestamp":"1712811480.0","upvote_count":"4"},{"comment_id":"1190975","content":"Selected Answer: D\nI think you do that in the Warehouse setting, there is not a Warehouse option but since a warehouse is created when you create the Lakehouse, it should be option D\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/semantic-models#sync-the-default-power-bi-semantic-model","poster":"clux","upvote_count":"3","timestamp":"1712496840.0"},{"comment_id":"1189933","content":"Correct answer is C.\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/semantic-models","poster":"luiruipu","timestamp":"1712322420.0","upvote_count":"3"}],"answers_community":["A (59%)","B (35%)","6%"],"answer_description":"","choices":{"D":"the Lakehouse1 settings","C":"the workspace settings","B":"the semantic model settings","A":"the SQL analytics endpoint settings"},"url":"https://www.examtopics.com/discussions/microsoft/view/137972-exam-dp-600-topic-1-question-81-discussion/","question_images":[],"unix_timestamp":1712322420,"question_id":160,"answer_ET":"A","topic":"1","answer":"A","timestamp":"2024-04-05 15:07:00","question_text":"You have a Fabric tenant that contains a lakehouse named Lakehouse1.\n\nYou need to prevent new tables added to Lakehouse1 from being added automatically to the default semantic model of the lakehouse.\n\nWhat should you configure?"}],"exam":{"id":71,"isBeta":false,"isMCOnly":false,"isImplemented":true,"numberOfQuestions":179,"lastUpdated":"12 Apr 2025","name":"DP-600","provider":"Microsoft"},"currentPage":32},"__N_SSP":true}