{"pageProps":{"questions":[{"id":"KLS33IyXaF9ZFcdQK3zZ","answer_description":"","question_text":"HOTSPOT\n-\n\nYou have a Power BI model that contains three tables named Sales, Marketing, and Delivery. The Sales table relates to the Marketing and Delivery tables. The Marketing and Delivery tables do NOT relate to each other.\n\nYou need to create a measure to meet the following requirements:\n\n• Sum the values in a column named Sales[SalesAmount].\n• Ensure that the tables and relationships remain unchanged.\n• If a user applies a filter to a column named Marketing[Region], apply the same filter to a column named Delivery[Region].\n\nHow should you complete the DAX expression? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","question_images":["https://img.examtopics.com/dp-500/image188.png"],"answers_community":[],"answer_ET":"","question_id":81,"exam_id":70,"url":"https://www.examtopics.com/discussions/microsoft/view/126672-exam-dp-500-topic-1-question-177-discussion/","answer_images":["https://img.examtopics.com/dp-500/image189.png"],"discussion":[{"upvote_count":"6","content":"The correct answer is: TREATAS and VALUES","timestamp":"1700482860.0","comment_id":"1075386","poster":"Cata23"},{"poster":"Momoanwar","upvote_count":"1","comment_id":"1118130","content":"treatas and Values.\nChatgpt :\nTo create a DAX measure according to the requirements, you should use the following expression:\n\nCALCULATE(\n SUM(Sales[SalesAmount]),\n TREATAS(VALUES(Marketing[Region]), Delivery[Region])\n)\n\nHere's what each function does in this context:\n\n- `CALCULATE` changes the context in which the data is aggregated and is used here to sum the `SalesAmount` while applying a filter context.\n- `SUM(Sales[SalesAmount])` adds up all the values in the `SalesAmount` column from the Sales table.\n- `TREATAS` takes the values from the `Marketing[Region]` column and treats them as if they were values in the `Delivery[Region]` column, effectively transferring the filter context from the Marketing table to the Delivery table. This is necessary since there is no direct relationship between Sales and Delivery, but both are related to Marketing.","timestamp":"1704858660.0"},{"poster":"MaryemSB","upvote_count":"2","comment_id":"1097504","timestamp":"1702661400.0","content":"Treatas and Values \nhttps://learn.microsoft.com/en-us/dax/treatas-function"}],"timestamp":"2023-11-20 13:21:00","answer":"","topic":"1","unix_timestamp":1700482860,"isMC":false},{"id":"VSGDVn7O5VUEr60ec8Gl","exam_id":70,"unix_timestamp":1703086440,"answer":"","timestamp":"2023-12-20 16:34:00","topic":"1","question_images":["https://img.examtopics.com/dp-500/image190.png"],"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/129103-exam-dp-500-topic-1-question-178-discussion/","discussion":[{"content":"Tempc + convertf2c, chatgpt :\nFor the Power Query Editor function that converts Fahrenheit to Celsius, you should define the function parameter as the temperature in Fahrenheit. The function itself should then perform the conversion. Here's how you should complete the code:\n\n- For the parameter: `(TempF as number) =>`\n- For the function name in the `in` clause: `ConvertF2C`\n\nThe complete function in Power Query M language should look like this:\n\n```\n(TempF as number) => \nlet\n TempC = (TempF - 32) * 5/9\nin\n ConvertF2C\n```","upvote_count":"1","comment_id":"1118134","timestamp":"1704858900.0","poster":"Momoanwar","comments":[{"content":"I mean tempf + convertf2c sorry","comments":[{"upvote_count":"2","poster":"Legen___dary","comment_id":"1123230","timestamp":"1705312320.0","content":"ChatGPT has lead you a little astray :). The Let statement is defining a list of variables, in this case only TempC. The In statement can then reference these variables. ConvertF2C isn't defined in the Let so wouldn't be recognised by the In.\n\nThe correct answer should be TempF + TempC."}],"comment_id":"1118135","poster":"Momoanwar","timestamp":"1704858960.0","upvote_count":"1"}]},{"upvote_count":"1","comments":[{"comment_id":"1103899","poster":"MaryemSB","upvote_count":"2","content":"I correct myself: \nFirst is (TempF as number) since TempF should be passed as a parameter\nSecond, not sure but it should be TempC to retrieve the result of expression from the \"let\" statement to the \"in\" statement","timestamp":"1703314560.0"}],"comment_id":"1101689","poster":"MaryemSB","content":"Source \nTempC\nhttps://datamadness.medium.com/how-to-write-functions-in-power-query-4a29302760ce","timestamp":"1703086440.0"}],"answer_ET":"","question_id":82,"answer_images":["https://img.examtopics.com/dp-500/image191.png"],"answers_community":[],"answer_description":"","question_text":"HOTSPOT\n-\n\nYou use Power Query Editor to transform data.\n\nYou need to create a function named ConvertF2C that accepts a temperature value in Fahrenheit and returns a temperature value in Celsius.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//"},{"id":"Ze6Z1cbifQ9eHpKuZsqD","exam_id":70,"question_id":83,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/128221-exam-dp-500-topic-1-question-179-discussion/","answer":"A","choices":{"C":"Create a linked entity in DF1.","A":"Add a computed entity to DF1.","B":"Create a custom function in DS1.","D":"Add a computed column to DS1."},"timestamp":"2023-12-10 16:25:00","isMC":true,"answers_community":["A (100%)"],"unix_timestamp":1702221900,"discussion":[{"timestamp":"1702668000.0","poster":"MaryemSB","upvote_count":"3","comment_id":"1097635","content":"Selected Answer: A\n\"To perform in-storage computations, you first must create the dataflow and bring data into that Power BI dataflow storage. After you have a dataflow that contains data, you can create computed tables, which are tables that do in-storage computations.\"\nhttps://learn.microsoft.com/en-us/power-query/dataflows/computed-tables"},{"comment_id":"1092584","upvote_count":"3","timestamp":"1702221900.0","poster":"Qordata","content":"Selected Answer: A\nComputed Table/Entity would help to reduce the refresh time of dataset\nhttps://learn.microsoft.com/en-us/power-query/dataflows/computed-tables"}],"answer_ET":"A","topic":"1","question_text":"You have a Power BI dataflow named DF1 that contains the following columns:\n\n• OrderID\n• SaleDate\n• ProductID\n• SalesAmount\n• ProductCategory\n\nYou create a Power BI dataset named DS1 that uses DF1 as a data source. DS1 creates an aggregated view of the data in DF1. The view contains the following columns:\n\n• SaleMonth\n• SalesAmount\n• ProductCategory\n\nYou need to minimize how long it takes to refresh DS1.\n\nWhat should you do first?","answer_images":[],"answer_description":""},{"id":"52o3yvfcSndwkM1pDViL","exam_id":70,"unix_timestamp":1670876700,"discussion":[{"comments":[{"content":"says \"Finally\" so enabling tenant-level storage comes first","poster":"Habiba11","timestamp":"1693540380.0","upvote_count":"2","comment_id":"995636"}],"timestamp":"1691385720.0","upvote_count":"6","comment_id":"974415","poster":"ExamPage","content":"Selected Answer: C\nThe question says to configure the Workspace1....\n\nAnswer is C.\nFinally, you can connect to any ADLS Gen 2 from the Admin portal, but if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\n\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"},{"upvote_count":"1","poster":"nikolgerson","timestamp":"1714382100.0","comment_id":"1203930","content":"Selected Answer: C\npre req is to ensure there is no dataflows in the workspace"},{"poster":"reemprive","timestamp":"1705739280.0","upvote_count":"1","comment_id":"1127144","content":"It seems like everyone is missing the word \"First\". It's A."},{"poster":"Sri966","upvote_count":"2","comment_id":"1124930","timestamp":"1705493220.0","content":"Selected Answer: A\nFirst in the admin portal the setting has to be enabled"},{"timestamp":"1702717800.0","comment_id":"1098023","upvote_count":"1","content":"Selected Answer: C\ntenant-level storage is not mandatory for this case, as question is mentioning workspace connection to ADLS2.\nhowever, workspace must be empty of dataflows before connecting to the ADLS2 account \n\n\"if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\"\n\"if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\"\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration","poster":"MaryemSB"},{"upvote_count":"1","timestamp":"1694669340.0","content":"i'm a bit lost here, but my guess is also b or C since A is tenant level and you need to config a workspace","comment_id":"1007224","comments":[{"comment_id":"1007229","poster":"Deloro","upvote_count":"1","timestamp":"1694669580.0","content":"ok sorry, chatgpt; Tenant-level storage allows you to specify a default storage location for dataflows at the workspace level. > so that's what i FIRST would do"}],"poster":"Deloro"},{"poster":"fireofsea","comment_id":"997512","timestamp":"1693735380.0","content":"Selected Answer: A\nNo116 is same question，why 116 answer is A. Change the Data source settings in the dataflow queries.\nIf in No18, \"A. From the Power BI Admin portal...\" is the best answer ?","upvote_count":"2"},{"comment_id":"984166","poster":"orionduo","upvote_count":"2","timestamp":"1692331800.0","content":"Selected Answer: A\nIt looks like that A is correct."},{"timestamp":"1690901880.0","upvote_count":"3","content":"Selected Answer: C\nAnswer is C. Won't be able to create the connection while dataflows exist in the workspace -> https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration -> \"you must first ensure there are no dataflows in the workspace before connecting\"","comment_id":"969154","poster":"hoss29"},{"content":"Selected Answer: A\n100% its A","comment_id":"948244","timestamp":"1689003960.0","upvote_count":"1","poster":"Batman160591"},{"upvote_count":"1","timestamp":"1688661960.0","content":"Selected Answer: A\nAS MENTIONED EARLIER, SEE THIS https://learn.microsoft.com/en-us/azure/purview/register-scan-power-bi-tenant?tabs=Scenario1","poster":"MohsenSic","comment_id":"944854"},{"timestamp":"1688066100.0","content":"According to ChatGPT the correct answer is A: Enabling tenant-level storage allows you to configure the default storage location for dataflows in Power BI. By enabling this feature and setting up the Azure Data Lake Storage Gen2 account as the default storage location, any new dataflows created within Workspace1 will automatically store their data in the specified Azure Data Lake Storage Gen2 account.","upvote_count":"2","comment_id":"938488","poster":"PrudenceK"},{"timestamp":"1687772160.0","upvote_count":"2","comment_id":"934252","poster":"Eltooth","content":"Selected Answer: A\nMS make questions simple. Maybe this has not been transcribed fully from the exam with all the info needed. \nI would put money on MS asking for the first (logical) step to add in an ADLS, which typically requires enabling of Azure storage. Happy to be proven wrong. \n\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"},{"upvote_count":"2","comment_id":"933604","content":"The answer is C, the existing dataflows shall be deleted from a workspace before the workspace can be connected to a storage account. (check the 'Connect Power BI with your dataflow storage' video by Curbal on YouTube)","poster":"Plb2","timestamp":"1687698000.0"},{"content":"Looks like it is C, MS guide is saying that there should be no DFs if we're trying to use ADLSGen2 ACCOUNT(!), and check out this discussion as well - https://community.fabric.microsoft.com/t5/Service/Configuring-dataflow-storage-to-use-Azure-Data-Lake-Gen-2-when/m-p/2214368/highlight/true","timestamp":"1685129220.0","poster":"Cococo","comment_id":"907564","upvote_count":"2"},{"poster":"cunningjack","timestamp":"1683888180.0","comment_id":"895837","upvote_count":"3","content":"Selected Answer: C\nIf we go with the prerequisite - should we not delete the dataflows in the workspace before we connect it to ADLS? Option C ?"},{"timestamp":"1683036060.0","comment_id":"887461","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration\n\n\"This feature essentially allows you to \"bring your own storage\" to Power BI dataflows, and establish a connection at the tenant or workspace level.\"","upvote_count":"1","poster":"dev2dev"},{"upvote_count":"3","timestamp":"1682520360.0","poster":"sgodd_0298","content":"Selected Answer: B\nAnswer is B: \"Prerequisite: you can connect to any ADLS Gen 2 from the Admin portal, but if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\"\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration","comments":[{"timestamp":"1683037980.0","content":"this guy is correct, we were asked to setup the storage at the workspace level","upvote_count":"2","comment_id":"887546","poster":"fdsdfgxcvbdsfhshfg"},{"poster":"AlanMont","upvote_count":"1","timestamp":"1686511080.0","content":"I agree. We cant choose A because at the Admin Portal we have an option \"Workspace-level storage permissions\".\nhttps://learn.microsoft.com/pt-br/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration","comment_id":"920933"}],"comment_id":"881744"},{"timestamp":"1679081880.0","poster":"DarioReymago","upvote_count":"1","content":"correct is A","comment_id":"842243"},{"upvote_count":"4","timestamp":"1672857240.0","poster":"moreinva43","content":"https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration","comment_id":"765988"},{"poster":"nbagchi","upvote_count":"1","timestamp":"1670876700.0","content":"Correct","comment_id":"743304"}],"isMC":true,"question_text":"You have a Power BI workspace named Workspace1 that contains five dataflows.\nYou need to configure Workspace1 to store the dataflows in an Azure Data Lake Storage Gen2 account.\nWhat should you do first?","timestamp":"2022-12-12 21:25:00","answer":"C","answer_ET":"C","url":"https://www.examtopics.com/discussions/microsoft/view/91283-exam-dp-500-topic-1-question-18-discussion/","choices":{"C":"Delete the dataflow queries.","D":"Change the Data source settings in the dataflow queries.","B":"Disable load for all dataflow queries.","A":"From the Power BI Admin portal, enable tenant-level storage."},"answers_community":["C (50%)","A (39%)","11%"],"answer_description":"","question_images":[],"topic":"1","question_id":84,"answer_images":[]},{"id":"hdpaVJG8PELfUXf7slUY","url":"https://www.examtopics.com/discussions/microsoft/view/129104-exam-dp-500-topic-1-question-180-discussion/","answer_description":"","discussion":[{"timestamp":"1704859500.0","upvote_count":"1","content":"Chatgpt :\nTo score the data in the Azure Synapse Analytics workspace using Model1 and Transact-SQL code, you should use the following:\n\n- Function: `PREDICT`\n- Format: `ONNX`\n\nThe `PREDICT` function is used in T-SQL for scoring machine learning models, and ONNX (Open Neural Network Exchange) is a popular open format for deep learning models like DNNs.","poster":"Momoanwar","comment_id":"1118136"},{"content":"Predict \nONNX\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/queries/predict-transact-sql?view=sql-server-ver16","upvote_count":"2","timestamp":"1703087580.0","comment_id":"1101710","poster":"MaryemSB"}],"answer_images":["https://img.examtopics.com/dp-500/image193.png"],"topic":"1","isMC":false,"timestamp":"2023-12-20 16:53:00","answer":"","answer_ET":"","question_images":["https://img.examtopics.com/dp-500/image192.png"],"answers_community":[],"exam_id":70,"unix_timestamp":1703087580,"question_text":"HOTSPOT\n-\n\nYou have a deep neural network (DNN) machine learning model named Model1 and an Azure Synapse Analytics workspace named Workspace1.\n\nYou need to score the data in Workspace1 by using Model1 and Transact-SQL code.\n\nWhich Transact-SQL function should you use to score the data, and which format should you use for Model1? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","question_id":85}],"exam":{"numberOfQuestions":183,"name":"DP-500","isImplemented":true,"isMCOnly":false,"isBeta":false,"id":70,"provider":"Microsoft","lastUpdated":"12 Apr 2025"},"currentPage":17},"__N_SSP":true}