{"pageProps":{"questions":[{"id":"HnN7GPcbuP3box5PrJms","timestamp":"2021-04-11 04:34:00","discussion":[{"content":"Why not ADF? The questions said DOCDB is migrated to Cosmos DB, and never mentioned any storage or ADLSG2.","upvote_count":"16","poster":"samkslee","timestamp":"1618108440.0","comment_id":"332930"},{"timestamp":"1619233380.0","poster":"JeanBlack","upvote_count":"7","content":"Correct Answer is ADF ( A because SALESDB is a kind of COSMOS DB which is output)","comment_id":"341813"},{"poster":"Wendy_DK","timestamp":"1618764900.0","upvote_count":"4","content":"correct answer: ADF","comment_id":"338327"}],"url":"https://www.examtopics.com/discussions/microsoft/view/49857-exam-dp-200-topic-14-question-2-discussion/","choices":{"B":"Define a query that contains a JavaScript user-defined aggregates (UDA) function.","A":"Define an output to Cosmos DB.","D":"Define a transformation query.","E":"Define an output to Azure Data Lake Storage Gen2.","F":"Define a stream input.","C":"Define a reference input."},"answer_description":"✑ DOCDB stored documents that connect to the sales data in SALESDB. The documents are stored in two different JSON formats based on the sales channel.\n✑ The sales data, including the documents in JSON format, must be gathered as it arrives and analyzed online by using Azure Stream Analytics. The analytic process will perform aggregations that must be done continuously, without gaps, and without overlapping.\nAs they arrive, all the sales documents in JSON format must be transformed into one consistent format.","question_id":71,"question_images":[],"exam_id":65,"answer":"DEF","unix_timestamp":1618108440,"isMC":true,"answers_community":[],"question_text":"You need to implement event processing by using Stream Analytics to produce consistent JSON documents.\nWhich three actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer_ET":"DEF","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0014200003.png"],"topic":"14"},{"id":"Zc3YUjvuwSsYiwblCmTu","answer_ET":"C","answer_images":[],"question_images":[],"discussion":[{"timestamp":"1589836200.0","content":"Asked for Retry when needed but not automatic retry. Tumbling window trigger retry automatically after the number of minutes configured. So Scheduling Trigger might be correct answer.","upvote_count":"20","comment_id":"91637","poster":"Randy478"},{"content":"Scheduled trigger does not support retry\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison","upvote_count":"8","comment_id":"87083","timestamp":"1589196540.0","poster":"Siva_s","comments":[{"upvote_count":"2","content":"It does, when creating a schedule trigger, you specify a schedule (start date, recurrence, end date etc.)\nhttps://docs.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger","timestamp":"1594536600.0","poster":"MLCL","comment_id":"132778"}]},{"poster":"dumpsm42","upvote_count":"2","comment_id":"241434","timestamp":"1607761020.0","content":"hi to all,\nanswer A\nwhy ? because the text says \"...retry...\" so this link says it all:\nhttps://docs.microsoft.com/pt-pt/azure/data-factory/how-to-create-tumbling-window-trigger\n\numbling window triggers are a type of trigger that fires at a periodic time interval from a specified start time, while retaining state. Tumbling windows are a series of fixed-sized, non-overlapping, and contiguous time intervals. A tumbling window trigger has a one-to-one relationship with a pipeline and can only reference a singular pipeline. Tumbling window trigger is a more heavy weight alternative for schedule trigger offering a suite of features for complex scenarios(dependency on other tumbling window triggers, rerunning a failed job and set user retry for pipelines). \n\nonly with this type we can retry in case of failure, with just a scheduled trigger nope.\nthats it.\n\nregards"},{"poster":"syu31svc","comment_id":"229014","content":"When you create a schedule trigger, you specify scheduling and recurrence by using a JSON definition.\nAnswer is C","timestamp":"1606481640.0","upvote_count":"2"},{"content":"With Tumbling window trigger, you have to define the run in the intervals of Minutes & Hours and no way you can schedule it at a specific time. That's the basic requirement to run it in non business hours.","upvote_count":"1","comment_id":"211371","timestamp":"1604333100.0","poster":"anurag1p"},{"upvote_count":"3","timestamp":"1594677660.0","content":"The option is Tumbling \"schedule\" trigger and not tumbling \"window\" trigger, so schedule trigger might be best fit in this case.","comment_id":"134339","poster":"apandey"},{"poster":"RajdeepRoy","content":"Answer: C. Use a schedule trigger","timestamp":"1594038360.0","comment_id":"127805","upvote_count":"5"},{"timestamp":"1588563360.0","content":"However, All data migrations must run automatically during non-business hours is also mentioned so I got confused now and help me understand the right answer.","poster":"Dileep1","upvote_count":"1","comment_id":"83381"},{"comment_id":"83379","timestamp":"1588563240.0","poster":"Dileep1","upvote_count":"7","comments":[{"upvote_count":"6","timestamp":"1589336340.0","comment_id":"88059","poster":"drdean","content":"As \"Use a tumbling schedule trigger\" is not a real possibility, schedule trigger is the best option (unless the options have been recorded incorrectly)"},{"poster":"Luke97","timestamp":"1589830800.0","upvote_count":"3","content":"I think Tumbling Window is more useful for time-based data. The requirement said \"retry when needed\" but not \"automatically retry\".","comment_id":"91590"}],"content":"Data migrations must be reliable and retry when needed. This is mentioned in question so I think Tumbling Window trigger is the answer."}],"topic":"15","timestamp":"2020-05-04 05:34:00","answer_description":"When creating a schedule trigger, you specify a schedule (start date, recurrence, end date etc.) for the trigger, and associate with a Data Factory pipeline.\nScenario:\nAll data migration processes must use Azure Data Factory\nAll data migrations must run automatically during non-business hours\nReferences:\nhttps://docs.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger","isMC":true,"exam_id":65,"answers_community":[],"question_id":72,"answer":"C","url":"https://www.examtopics.com/discussions/microsoft/view/19549-exam-dp-200-topic-15-question-1-discussion/","question_text":"You need to ensure that phone-based poling data can be analyzed in the PollingData database.\nHow should you configure Azure Data Factory?","unix_timestamp":1588563240,"choices":{"D":"Use manual execution","B":"Use an event-based trigger","C":"Use a schedule trigger","A":"Use a tumbling schedule trigger"}},{"id":"ucWe7DPkAQnjiQGY8WUY","question_text":"HOTSPOT -\nYou need to ensure that Azure Data Factory pipelines can be deployed. How should you configure authentication and authorization for deployments? To answer, select the appropriate options in the answer choices.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer":"","unix_timestamp":1604333280,"timestamp":"2020-11-02 17:08:00","answers_community":[],"topic":"15","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0027100001.jpg"],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0027000001.jpg"],"exam_id":65,"question_id":73,"discussion":[{"poster":"anurag1p","content":"Answer mentioned is correct.","timestamp":"1604333280.0","upvote_count":"6","comment_id":"211373"}],"isMC":false,"answer_description":"The way you control access to resources using RBAC is to create role assignments. This is a key concept to understand ג€\" it's how permissions are enforced. A role assignment consists of three elements: security principal, role definition, and scope.\nScenario:\nNo credentials or secrets should be used during deployments\nPhone-based poll data must only be uploaded by authorized users from authorized devices\nContractors must not have access to any polling data other than their own\nAccess to polling data must set on a per-active directory user basis\nReferences:\nhttps://docs.microsoft.com/en-us/azure/role-based-access-control/overview","url":"https://www.examtopics.com/discussions/microsoft/view/35792-exam-dp-200-topic-15-question-2-discussion/","answer_ET":""},{"id":"c8jKDNjTpcyH4dyMGx5x","discussion":[{"poster":"SAMBIT","timestamp":"1581954300.0","content":"B & F ...check the skill set of the team","upvote_count":"43","comment_id":"51724","comments":[{"comment_id":"141820","content":"B & F: Stream Analytics query language is a subset of standard T-SQL syntax for doing Streaming computations [offered in Azure Stream Analytics].\n\nhttps://docs.microsoft.com/en-us/stream-analytics-query/stream-analytics-query-language-reference","upvote_count":"4","poster":"Treadmill","timestamp":"1595491440.0"}]},{"poster":"PHaringsNL","comments":[{"content":"it says \"Tier 9 reporting must be moved to Event Hubs, queried, and persisted in the same Azure region as the company's main office\" i think given answer is correct. but who really knows? it's not like you can argue your point during the test...","comment_id":"89072","timestamp":"1589477340.0","poster":"runningman","upvote_count":"2"}],"timestamp":"1581936540.0","upvote_count":"7","comment_id":"51611","content":"Where is it obvious this is concerning streaming data. I've read this question multiple times, but I'm not seeing anything leading to this. Exept that it states that it's internally used and internally mostly SQL developers are present."},{"comment_id":"312172","content":"processo and query, so it's T-sql + stream job \n--> B + F","poster":"rjile","upvote_count":"1","timestamp":"1615887720.0"},{"upvote_count":"3","timestamp":"1607761620.0","comment_id":"241439","content":"hi to all,\n\n\"Integration with Event Grid\nYou can create an Azure Event Grid subscription with an Event Hubs namespace as its source. The following tutorial shows you how to create an Event Grid subscription with an event hub as a source and an Azure Functions app as a sink: Process and migrate captured Event Hubs data to a Azure Synapse Analytics using Event Grid and Azure Functions.\"\n\nhttps://docs.microsoft.com/pt-pt/azure/event-hubs/event-hubs-capture-overview\n\nEF\n\nregards","poster":"dumpsm42"},{"timestamp":"1591422840.0","content":"Event Grid connects your app with other services. For example, create an application topic to send your app's event data to Event Grid and take advantage of its reliable delivery, advanced routing, and direct integration with Azure.\nhttps://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/application-integration-using-event-grid","upvote_count":"1","comment_id":"103599","poster":"chatw"},{"timestamp":"1582028820.0","content":"Stream Analytic Query is not the same with T-SQL. It is indeed a subset of T-SQL, but not the same.\nOn the other hand an EventHub provides a Kafka endpoint, so you can use Kafka statements.","upvote_count":"2","comment_id":"52058","poster":"cdume"},{"upvote_count":"5","poster":"DBQ","content":"Event Grid filter a specific event and Stream analytic does aggregation","comment_id":"25069","timestamp":"1574979660.0"},{"upvote_count":"3","content":"Event Grid and Event Hub is not the same thing","comments":[{"poster":"STH","timestamp":"1574458680.0","content":"So it's D and F","upvote_count":"11","comment_id":"23734"}],"comment_id":"23136","timestamp":"1574279040.0","poster":"Frederi"}],"answer_ET":"EF","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0027600004.png"],"answer":"EF","answers_community":[],"question_text":"You need to process and query ingested Tier 9 data.\nWhich two options should you use? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","exam_id":65,"url":"https://www.examtopics.com/discussions/microsoft/view/8733-exam-dp-200-topic-16-question-1-discussion/","timestamp":"2019-11-20 20:44:00","answer_description":"Event Hubs provides a Kafka endpoint that can be used by your existing Kafka based applications as an alternative to running your own Kafka cluster.\nYou can stream data into Kafka-enabled Event Hubs and process it with Azure Stream Analytics, in the following steps:\n✑ Create a Kafka enabled Event Hubs namespace.\n✑ Create a Kafka client that sends messages to the event hub.\n✑ Create a Stream Analytics job that copies data from the event hub into an Azure blob storage.\nScenario:\n\nTier 9 reporting must be moved to Event Hubs, queried, and persisted in the same Azure region as the company's main office\nReferences:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-kafka-stream-analytics","isMC":true,"choices":{"E":"Azure Event Grid","B":"Transact-SQL statements","F":"Azure Stream Analytics","A":"Azure Notification Hub","D":"Apache Kafka statements","C":"Azure Cache for Redis"},"topic":"16","question_id":74,"question_images":[],"unix_timestamp":1574279040},{"id":"uc2qJbTkdHIpOJRccYyO","topic":"16","answer":"A","unix_timestamp":1606484700,"answer_ET":"A","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/37912-exam-dp-200-topic-16-question-3-discussion/","timestamp":"2020-11-27 14:45:00","choices":{"A":"self-hosted integration runtime","D":"Azure integration runtime","B":"Azure-SSIS Integration Runtime","C":".NET Common Language Runtime (CLR)"},"isMC":true,"answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0027900001.png"],"question_text":"You need to set up Azure Data Factory pipelines to meet data movement requirements.\nWhich integration runtime should you use?","exam_id":65,"discussion":[{"comment_id":"229045","content":"Answer is correct","timestamp":"1606484700.0","poster":"syu31svc","upvote_count":"10"},{"content":"Please delete my answeres, cannot edit.","poster":"Hinzzz","timestamp":"1624491360.0","comment_id":"389128","upvote_count":"1"},{"upvote_count":"1","timestamp":"1624491300.0","comment_id":"389127","content":"I mean C","poster":"Hinzzz"},{"upvote_count":"1","timestamp":"1624491240.0","poster":"Hinzzz","comment_id":"389125","content":"The answer should be Azure Integration runtime. B"}],"answer_description":"The following table describes the capabilities and network support for each of the integration runtime types:\n\nScenario: The solution must support migrating databases that support external and internal application to Azure SQL Database. The migrated databases will be supported by Azure Data Factory pipelines for the continued movement, migration and updating of data both in the cloud and from local core business systems and repositories.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime","question_id":75}],"exam":{"name":"DP-200","id":65,"lastUpdated":"12 Apr 2025","isBeta":false,"numberOfQuestions":228,"isImplemented":true,"provider":"Microsoft","isMCOnly":false},"currentPage":15},"__N_SSP":true}