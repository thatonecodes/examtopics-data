{"pageProps":{"questions":[{"id":"L6B7u20vBTV0aI7uIVvL","url":"https://www.examtopics.com/discussions/microsoft/view/122643-exam-az-305-topic-2-question-35-discussion/","answer":"","question_text":"HOTSPOT\n-\n\nYour company, named Contoso, Ltd., has an Azure subscription that contains the following resources:\n\n• An Azure Synapse Analytics workspace named contosoworkspace1\n• An Azure Data Lake Storage account named contosolake1\n• An Azure SQL database named contososql1\n\nThe product data of Contoso is copied from contososql1 to contosolake1.\n\nContoso has a partner company named Fabrikam Inc. Fabrikam has an Azure subscription that contains the following resources:\n\n• A virtual machine named FabrikamVM1 that runs Microsoft SQL Server 2019\n• An Azure Storage account named fabrikamsa1\n\nContoso plans to upload the research data on FabrikamVM1 to contosolake1. During the upload, the research data must be transformed to the data formats used by Contoso.\n\nThe data in contosolake1 will be analyzed by using contosoworkspace1.\n\nYou need to recommend a solution that meets the following requirements:\n\n• Upload and transform the FabrikamVM1 research data.\n• Provide Fabrikam with restricted access to snapshots of the data in contosoworkspace1.\n\nWhat should you recommend for each requirement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_images":["https://img.examtopics.com/az-305/image223.png"],"isMC":false,"answer_ET":"","unix_timestamp":1696576920,"timestamp":"2023-10-06 09:22:00","answers_community":[],"question_images":["https://img.examtopics.com/az-305/image222.png"],"discussion":[{"poster":"mykola_yakovliev","timestamp":"1696576920.0","content":"Provided answers look correct.\n\nFor ETL operations use Azure Data Factory and Azure Synapse Pipelines are based on Azure Data Factory.\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/concepts-data-factory-differences\n\nFor restricted access use Azure Data Share:\nAzure Data Share enables organizations to securely share data with multiple customers and partners. Data providers are always in control of the data that they've shared and Azure Data Share makes it simple to manage and monitor what data was shared, when and by whom.\nIn this case snapshot-based sharing should be used.\nSource: https://learn.microsoft.com/en-us/azure/data-share/overview","comment_id":"1026332","upvote_count":"21"},{"comment_id":"1104075","timestamp":"1703342760.0","poster":"jcxxxxx2020","upvote_count":"13","content":"This question appeared on my exam today.\nAnswer is correct"},{"content":"CORRECT","timestamp":"1731426660.0","upvote_count":"2","poster":"[Removed]","comment_id":"1310699"},{"comment_id":"1281997","content":"-Azure Synapse pipelines\n-Azure Data Share","poster":"Teerawee","upvote_count":"1","timestamp":"1726037700.0"},{"comment_id":"1236310","poster":"23169fd","content":"The given answer is correct.\nAzure Synapse pipelines: They allow you to orchestrate data movement and transformation efficiently, integrating well with other Azure services.\nAzure Data Share: Provides a secure way to share data snapshots with restricted access, ensuring Fabrikam has controlled and read-only access to the necessary data.","timestamp":"1719230460.0","upvote_count":"1"},{"content":"Data Warehouse Layer: Use Azure Synapse Analytics (formerly known as Azure SQL Data Warehouse). \nManaged Serving Layer for OLAP: Use Azure Analysis Services. Azure Analysis Services is an enterprise-grade analytics engine that provides a managed OLAP database for running and serving complex analytical models.","timestamp":"1711065780.0","comment_id":"1179736","upvote_count":"1","poster":"RanOlfati"},{"content":"Got his on my exam on 06/Mar/24","timestamp":"1709802900.0","upvote_count":"10","poster":"eduardobbs","comment_id":"1167820"},{"poster":"StixxNSnares","content":"Azure synapse pipelines - Azure Synapse Pipelines is a cloud-based data integration service that allows you to create data-driven workflows for orchestrating and automating data movement and data transformation\nhttps://www.sqlshack.com/export-data-from-azure-sql-database-to-azure-data-lake-storage/\n\nAzure Data Share - Azure Data Share is a simple and safe service for sharing big data with external organizations2. It allows you to easily share data with other organizations, and it provides capabilities to ensure that only authorized users have access to the shared data.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/security/how-to-set-up-access-control","comment_id":"1068044","timestamp":"1699730160.0","upvote_count":"5"}],"exam_id":54,"question_id":106,"answer_description":"","topic":"2"},{"id":"M2ZfUYiR1ku74XkRHct2","answer_ET":"","answer_images":["https://img.examtopics.com/az-305/image225.png"],"discussion":[{"poster":"BShelat","comment_id":"1095598","timestamp":"1702481220.0","upvote_count":"41","content":"Trick to remember:\nSynapse Analytics - massive parallel processing\nAnalysis Services - OLAP"},{"comment_id":"1027285","content":"The selected answer is correct.\n\nData Warehouse: Azure Synapse Analytics (formerly SQL Data Warehouse)\nAzure Synapse Analytics is a massively parallel processing (MPP) data warehouse that can handle large amounts of data and provides a scalable solution for analytics.\n\nManaged Serving Layer: Azure Analysis Services\nAzure Analysis Services provides a fully managed platform-as-a-service (PaaS) solution for online analytical processing (OLAP) and data modeling. It is suitable for serving analytical models to thousands of end users.","timestamp":"1696678500.0","poster":"ArunS005","upvote_count":"13"},{"content":"CORRECT","comment_id":"1310700","upvote_count":"1","timestamp":"1731426900.0","poster":"[Removed]"},{"comment_id":"1304614","upvote_count":"2","content":"Why not Apache Spark pool?","timestamp":"1730222820.0","poster":"SIJUTHOMASP"},{"upvote_count":"1","poster":"23169fd","timestamp":"1719230760.0","comment_id":"1236314","content":"The given answer is correct.\nAzure Synapse Analytics dedicated SQL pool: Provides a scalable, high-performance data warehouse solution that can efficiently handle large amounts of data and supports complex queries.\nAzure Analysis Services: Offers powerful analytical processing (OLAP) capabilities, enabling fast and interactive data analysis for thousands of end users."},{"timestamp":"1696675440.0","content":"Here's how the pipeline would work:\n\nPeriodically export database updates to Azure Blob storage.\nUse Azure Data Factory to cleanse and transform the data from Blob storage.\nLoad the transformed data into your Azure Synapse Analytics data warehouse.\nUse Azure Analysis Services to create and manage OLAP models based on the data in your data warehouse.\nEnd users can connect to Azure Analysis Services to query and analyze the data.","upvote_count":"11","comment_id":"1027254","poster":"RJalal"},{"content":"correct answer","timestamp":"1696675380.0","comment_id":"1027253","upvote_count":"1","poster":"RJalal"}],"question_id":107,"exam_id":54,"answers_community":[],"question_images":["https://img.examtopics.com/az-305/image224.png"],"topic":"2","timestamp":"2023-10-07 12:43:00","question_text":"HOTSPOT\n-\n\nYou are designing a data pipeline that will integrate large amounts of data from multiple on-premises Microsoft SQL Server databases into an analytics platform in Azure. The pipeline will include the following actions:\n\n• Database updates will be exported periodically into a staging area in Azure Blob storage.\n• Data from the blob storage will be cleansed and transformed by using a highly parallelized load process.\n• The transformed data will be loaded to a data warehouse.\n• Each batch of updates will be used to refresh an online analytical processing (OLAP) model in a managed serving layer.\n• The managed serving layer will be used by thousands of end users.\n\nYou need to implement the data warehouse and serving layers.\n\nWhat should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer":"","answer_description":"","isMC":false,"unix_timestamp":1696675380,"url":"https://www.examtopics.com/discussions/microsoft/view/122759-exam-az-305-topic-2-question-36-discussion/"},{"id":"iANlEy1ojw6G0v3foGDi","question_id":108,"topic":"2","unix_timestamp":1704740400,"answer_description":"","isMC":false,"answer":"","discussion":[{"upvote_count":"10","comments":[{"upvote_count":"1","comments":[{"content":"So turns out I am mistaken. https://learn.microsoft.com/en-us/azure/azure-sql/database/read-scale-out?view=azuresql Hyperscale is the right answer. \n\"In Premium and Business Critical service tiers, only one of the read-only replicas is accessible at any given time. Hyperscale supports multiple read-only replicas.\"","upvote_count":"3","timestamp":"1734375300.0","poster":"jbnkb","comment_id":"1327580"}],"timestamp":"1734371700.0","comment_id":"1327558","poster":"jbnkb","content":"I think you misread, Business Critical has multiple read replicas, Premium tier only comes with 1 read replica and reads are not automatically load balanced. It should be Business Critical."}],"content":"As part of the requirement -> Support multiple read-only replicas.\n\nHyperscale is the right choice. Business critical tier has only 1 additional read replica. \nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql","timestamp":"1705080840.0","comment_id":"1120965","poster":"kishoredeena"},{"comment_id":"1120761","upvote_count":"5","content":"The given answers are correct. Please refer MS doc, it says that \"In Premium and Business Critical service tiers, only one of the read-only replicas is accessible at any given time. Hyperscale supports multiple read-only replicas.\" Hence for load balancing between multiple read replica, first need is that they should be available, which makes Hyperscale suitable for this. I hope this helps. https://learn.microsoft.com/en-us/azure/azure-sql/database/read-scale-out?view=azuresql","comments":[{"poster":"lukiduc9625","content":"what about \"An Azure SQL Database elastic pool\" for first box? elastic pool can have Hyperscale service tier... and read-scale-out seems to depend only on service tier... (https://learn.microsoft.com/en-us/azure/azure-sql/database/read-scale-out?view=azuresql)","upvote_count":"1","timestamp":"1709813400.0","comment_id":"1167957"}],"timestamp":"1705062480.0","poster":"MiniLa92"},{"upvote_count":"1","poster":"[Removed]","comment_id":"1310702","content":"CORRECT","timestamp":"1731427080.0"},{"poster":"84e067a","comment_id":"1288775","content":"Why would Elastic Pools or Managed Instance not be an option for Box1?","timestamp":"1727213400.0","upvote_count":"3"},{"upvote_count":"2","content":"It is hard one to figure out (Between Hyperscale and business critical) but based on this comment from the requirements\n* Automatically load balance read-only requests across all the read-only replicas\nOnly Hyperscale can do this\nNote: for thos who made comment that \"Business critical tier has only 1 additional read replica.\" is NOT true, see link below and as per comment\n\"\"each single database or elastic pool database in the Premium and Business Critical service tier is automatically provisioned with a primary read-write replica and one or more secondary read-only replicas. The secondary replicas are provisioned with the same compute size as the primary replica. **\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/read-scale-out?view=azuresql","timestamp":"1712661480.0","comment_id":"1192238","poster":"Lazylinux"},{"poster":"[Removed]","comment_id":"1116909","timestamp":"1704740400.0","upvote_count":"1","content":"First box seems correct.\n\nFor the second one, the Azure SQL Database in the Business Critical or Hyperscale service tiers automatically provisions a primary read-write replica and one or more secondary read-only replicas, so theoretically both are valid, not sure which one to choose, maybe Hyperscale is overkill.","comments":[{"content":"Thank you!","timestamp":"1705138380.0","poster":"[Removed]","comment_id":"1121456","upvote_count":"2"}]}],"question_images":["https://img.examtopics.com/az-305/image243.png"],"answers_community":[],"question_text":"HOTSPOT\n-\n\nYou have an Azure subscription.\n\nYou need to deploy a relational database. The solution must meet the following requirements:\n\n• Support multiple read-only replicas.\n• Automatically load balance read-only requests across all the read-only replicas.\n• Minimize administrative effort\n\nWhat should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_ET":"","exam_id":54,"answer_images":["https://img.examtopics.com/az-305/image244.png"],"timestamp":"2024-01-08 20:00:00","url":"https://www.examtopics.com/discussions/microsoft/view/130623-exam-az-305-topic-2-question-37-discussion/"},{"id":"rJg8zuP0CfMSndt8cPYo","discussion":[{"comments":[{"upvote_count":"1","comment_id":"1298070","timestamp":"1728977400.0","poster":"c_h_r_i_s_","content":"\"Microsoft recommends maintaining fewer than 1000 versions per blob\"\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/versioning-overview"},{"content":"https://learn.microsoft.com/en-us/azure/storage/blobs/snapshots-overview\n\nNote\nBlob versioning offers a superior way to maintain previous versions of a blob. For more information, see Blob versioning.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/versioning-overview","timestamp":"1705138980.0","comments":[{"poster":"[Removed]","comments":[{"comments":[{"poster":"prshntdxt7","timestamp":"1711037160.0","content":"Yep, seems all right.","upvote_count":"1","comment_id":"1179383"}],"content":"I will, however, stick with Blob Versioning, even Microsoft suggests this is a better way. And to minimise the costs you can simply use lifecycle policies to delete old versions.","upvote_count":"6","timestamp":"1705139220.0","poster":"[Removed]","comment_id":"1121470"}],"content":"I was debating whether to go with Blob Snapshots in the end because our requirement is that we only need one version of the file, so we can schedule a snapshot to take place let's say right before midnight, and it would work just fine. However, there's some administrative overhead to do so.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/snapshots-manage-dotnet\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/snapshots-overview\n\nTo automate the snapshot creation process, you can use Azure Logic Apps or Azure Functions.","upvote_count":"1","timestamp":"1705139160.0","comment_id":"1121468"}],"poster":"[Removed]","comment_id":"1121462","upvote_count":"2"}],"poster":"[Removed]","content":"Selected Answer: D\nI believe the given answer is correct. The key here is that File1.txt is changed every hour. Theoretically, you can do this with B as well but you need to configure a given time when the snapshot is taken. With blob versioning, you have access to 24 versions, and you can restore for example a version that took place 6 hours ago as opposed to when the blob snapshot took place (you will only have a single version, the one that the snapshot captured at the moment it was taken). For this reason I would go with D.","comment_id":"1116917","timestamp":"1704740580.0","upvote_count":"10"},{"content":"Selected Answer: D\nCouldn't be more obvious to me. Versioning. File is uploaded every hour and we need to make sure we can restore the last upload. Snapshots are daily, a restore after 11pm would mean you'd miss out on the last 23 uploads. Question is pants but the answer is still clear.","poster":"mta_outlook","comment_id":"1141462","upvote_count":"7","timestamp":"1707165780.0"},{"comment_id":"1366742","poster":"Jack2k","timestamp":"1741490040.0","upvote_count":"1","content":"Selected Answer: D\nBecause blob snapshot is for backing up, so blob versioning will be a good choice for roll back to a previous version of File1.txt"},{"content":"Selected Answer: D\nD is correct","poster":"[Removed]","comment_id":"1310707","timestamp":"1731427620.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1306598","poster":"Paputzback","content":"What happens after midnight when file1 gets deleted in order to be reset to empty in order to store transaction data for the current day only? \nThere is not indication that policies will be created to store a snaphot in cheaper tier after the last change of the day. \nIt seems like it would be a hassle to dig through the changes to get the last version of the file for a particular day.","timestamp":"1730658600.0"},{"poster":"Rybsonldz","comment_id":"1286787","timestamp":"1726831500.0","content":"I believe option D. \nAlso, spoke to chat about it in comparison of minimize storage space:\nWhy Blob Versioning Minimizes Storage:\nVersioning only stores a copy when the blob is modified, unlike snapshots which can be more space-intensive due to frequent captures.\nRetention policies can be configured with versioning to automatically delete older versions (e.g., after 30 days), helping to control storage space over time.\nConclusion:\nBlob Versioning is the better choice for minimizing storage space, as it optimizes storage by only keeping changes between versions and can automatically prune older versions according to a defined retention period. This makes it more storage-efficient compared to snapshots.\n\nSo, blob versioning remains the best solution for this scenario, as it balances the need for version recovery and storage efficiency.","upvote_count":"3"},{"timestamp":"1723118940.0","content":"Selected Answer: B\nwith blob snapshot we can fulfill the requirement of keeping the last version of the file of a particular day while minimizing storage space.","upvote_count":"1","comment_id":"1262467","poster":"Rod_DA"},{"comment_id":"1236322","poster":"23169fd","upvote_count":"2","content":"Selected Answer: D\nD. Blob Versioning\nJustification:\nBlob Versioning: Automatically keeps previous versions of an object when it is overwritten, enabling you to restore any version within the retention period.\nStorage Efficiency: Only stores the changes, minimizing the additional storage required.","timestamp":"1719231840.0"},{"comment_id":"1204159","content":"Selected Answer: D\nSolution must minimize storage space... Versioning for sure. Snapshots every hour would cause a lot of administrative load to manage storage space consumption.","timestamp":"1714413900.0","poster":"arnitjoe","upvote_count":"2"},{"content":"It says \"cumulative transaction log file named File1.txt \" means last file updated will have all the changes happened every hour. Taking snapshot of the last updated file at midnight will suffice the requirement.\nAnswer B","poster":"jayaj","comment_id":"1194876","timestamp":"1713004560.0","upvote_count":"3"},{"timestamp":"1708468860.0","comments":[{"upvote_count":"2","timestamp":"1710581820.0","content":"No. \nThe file is updated hourly, so the last updated version is also hourly. \nYou need to restore the last upload, which is hourly, not daily. \nStorage is minimized by keeping daily data available.","comment_id":"1174882","poster":"xRiot007"}],"content":"I feel B is thr right answer.\nReasons:\nWe need only the last updated version for each day, so taking snapshot of a day at midnight would be sufficient.\nSecondly, we need to minimise storage space. versioning will make 24 versions for each day. Lifecycle mgmt rules can easily delete the snapshot after 30 days.","comment_id":"1155065","upvote_count":"1","poster":"SDiwan"},{"timestamp":"1706926200.0","content":"I will go with blob-versioning for the reason that version optimizes storage by additionally storing the delta and not the entire data as in case of snapshots","poster":"TJ001","comment_id":"1138957","upvote_count":"2"},{"comment_id":"1128692","poster":"TonySuccess","timestamp":"1705931580.0","content":"Storage space: Blob versioning and daily snapshots both consume storage space, but blob versioning might consume more space if there are frequent changes to the blob. Daily snapshots only create one copy of the blob per day, while blob versioning creates a new version every time the blob is modified or deleted. Therefore, blob versioning might be more suitable for blobs that are rarely changed, while daily snapshots might be more suitable for blobs that are frequently changed.\n\nIt is a very circumstantial question, I do not feel that enough information is provided. \n\nCopilot suggests Versioning would be the best option, but it is a terrible question.","upvote_count":"2"},{"timestamp":"1704811020.0","upvote_count":"5","comment_id":"1117574","poster":"Muffay","content":"Selected Answer: B\nI will vote for B here, as the requirement is to minimize storage costs and also states it is only needed to archive the *last uploaded version* of the day. \n\nI am not aware of a native solution to schedule those snapshots, but worst case we could use scheduled Azure Functions for that."},{"upvote_count":"4","timestamp":"1704459420.0","comments":[{"comment_id":"1144535","timestamp":"1707401580.0","content":"Versioning will store 24*delta content of file which is comparable stirage to daily snapshot","poster":"mtc9","upvote_count":"3"}],"comment_id":"1114503","poster":"mns0173","content":"Versioning will store all 24 cumulative files daily. Instead we can do daily snapshots at the end of a day as it has cumulative data for the whole day. It will add administrative overhead, but reduce storage usage."}],"question_text":"You have an app named App1 that uses an Azure Blob Storage container named app1data.\n\nApp1 uploads a cumulative transaction log file named File1.txt to a block blob in app1data once every hour. File1.txt only stores transaction data from the current day.\n\nYou need to ensure that you can restore the last uploaded version of File1.txt from any day for up to 30 days after the file was overwritten. The solution must minimize storage space.\n\nWhat should you include in the solution?","choices":{"A":"container soft delete","C":"blob soft delete","D":"blob versioning","B":"blob snapshots"},"answer":"D","isMC":true,"exam_id":54,"answer_images":[],"timestamp":"2024-01-05 13:57:00","question_id":109,"answers_community":["D (79%)","B (21%)"],"answer_ET":"D","url":"https://www.examtopics.com/discussions/microsoft/view/130404-exam-az-305-topic-2-question-38-discussion/","unix_timestamp":1704459420,"answer_description":"","question_images":[],"topic":"2"},{"id":"pRKE8hkJm3VCB3BGR47r","choices":{"C":"Azure Data Share","D":"Azure Data Studio","A":"Azure Data Factory","B":"Azure Data Explorer"},"exam_id":54,"timestamp":"2024-01-04 18:17:00","topic":"2","isMC":true,"question_id":110,"answers_community":["A (100%)"],"answer_ET":"A","answer_description":"","unix_timestamp":1704388620,"answer":"A","answer_images":[],"question_images":[],"discussion":[{"comment_id":"1117576","content":"Selected Answer: A\nAzure Data Factory is correct.\n\n> Big data requires a service that can orchestrate and operationalize processes to refine these enormous stores of raw data into actionable business insights. Azure Data Factory is a managed cloud service that's built for these complex hybrid extract-transform-load (ETL), extract-load-transform (ELT), and data integration projects.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/introduction","upvote_count":"9","timestamp":"1704811260.0","poster":"Muffay"},{"content":"Selected Answer: A\nA is Correct","poster":"theptr","comment_id":"1336775","timestamp":"1736085360.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nA is correct","timestamp":"1731427740.0","poster":"[Removed]","comment_id":"1310710"},{"comment_id":"1204284","poster":"Lazylinux","timestamp":"1714442040.0","content":"Selected Answer: A\nGiven answer A is correct\nIntegrate all your data with Azure Data Factory, a fully managed, serverless data integration service. Visually integrate data sources with more than 90 built-in, maintenance-free connectors at no added cost. Easily construct ETL (extract, transform, and load) and ELT (extract, load, and transform) processes code-free in an intuitive environment or write your own code. Then deliver integrated data to Azure Synapse Analytics to unlock business insights. \nEasy rehosting of SQL Server Integration Services to build ETL and ELT pipelines code-free with built-in Git and support for continuous integration and continuous delivery (CI/CD). \nPay-as-you-go, fully managed serverless cloud service that scales on demand for a cost-effective solution. \nMore than 90 built-in connectors for ingesting all your on-premises and software as a service (SaaS) data to orchestrate and monitor at scale.","upvote_count":"1"},{"upvote_count":"2","comment_id":"1167751","poster":"Crossfader2208","content":"Selected Answer: A\nTBH for the exam like this the question seems to be a bit too simple.","timestamp":"1709797020.0"},{"comment_id":"1113924","poster":"kishoredeena","upvote_count":"4","timestamp":"1704388620.0","content":"Given answer is correct\nhttps://azure.microsoft.com/en-in/products/data-factory"}],"question_text":"You have 12 on-premises data sources that contain customer information and consist of Microsoft SQL Server, MySQL, and Oracle databases.\n\nYou have an Azure subscription.\n\nYou plan to create an Azure Data Lake Storage account that will consolidate the customer information for analysis and reporting.\n\nYou need to recommend a solution to automatically copy new information from the data sources to the Data Lake Storage account by using extract, transform and load (ETL). The solution must minimize administrative effort.\n\nWhat should you include in the recommendation?","url":"https://www.examtopics.com/discussions/microsoft/view/130346-exam-az-305-topic-2-question-39-discussion/"}],"exam":{"numberOfQuestions":286,"provider":"Microsoft","name":"AZ-305","isBeta":false,"isImplemented":true,"isMCOnly":false,"lastUpdated":"12 Apr 2025","id":54},"currentPage":22},"__N_SSP":true}