{"pageProps":{"questions":[{"id":"c2AGSLtVwHiECXFqO6PR","answers_community":[],"discussion":[{"poster":"rmk4ever","comments":[{"upvote_count":"4","comment_id":"366013","content":"This explanation is entirely correct. \n\nthe first item is referencing 'high concurrency' and one could check this while creating an interactive cluster. \n\nsecond item, a new job cluster should be created for job purposes as the existing all purpose cluster has different pricing. refer to the url provided at the bottom\n\nlastly, delta lake is configurable in the mentioned cluster version\n\nReference: https://docs.microsoft.com/en-us/azure/databricks/jobs#cluster-config-tips","timestamp":"1621907760.0","comments":[{"upvote_count":"6","poster":"cadio30","content":"btw, first item hint is when you see 'serverless' it automatically indicates the 'high concurrency' cluster mode","timestamp":"1621907820.0","comment_id":"366014"}],"poster":"cadio30"}],"upvote_count":"30","content":"1. Yes\nA cluster mode of ‘High Concurrency’ is selected, unlike all the others which are ‘Standard’. This results in a worker type of Standard_DS13_v2.\nref: https://adatis.co.uk/databricks-cluster-sizing/\n\n2. NO\nrecommended: New Job Cluster.\nWhen you run a job on a new cluster, the job is treated as a data engineering (job) workload subject to the job workload pricing. When you run a job on an existing cluster, the job is treated as a data analytics (all-purpose) workload subject to all-purpose workload pricing.\nref: https://docs.microsoft.com/en-us/azure/databricks/jobs\nScheduled batch workload- Launch new cluster via job\nref: https://docs.databricks.com/administration-guide/capacity-planning/cmbp.html#plan-capacity-and-control-cost\n\n3.YES\nDelta Lake on Databricks allows you to configure Delta Lake based on your workload patterns.\nref: https://docs.databricks.com/delta/index.html","comment_id":"179211","timestamp":"1600080180.0"},{"content":"My take on it:\nYes to multiple users - fits to support high concurrency since no scala support\nYes to efficiency - autostop and autoscale\nYes to the delta store - elastic disk (not 100% sure about that)","timestamp":"1588263300.0","upvote_count":"17","poster":"Leonido","comment_id":"81770","comments":[{"comment_id":"197772","timestamp":"1602427440.0","upvote_count":"2","content":"Auto termination is not configured for high concurrency clusters. so this cluster does not support high concurrency. So the answer should be\nNo\nYes\nNo\nrefer \nhttps://docs.databricks.com/clusters/clusters-manage.html#automatic-termination","comments":[{"timestamp":"1603534500.0","upvote_count":"5","comments":[{"poster":"awitick","timestamp":"1611043920.0","content":"exactly","comment_id":"270999","upvote_count":"1"}],"comment_id":"205035","poster":"D_Duke","content":"Auto termination is not configured for high concurrency clusters BY DEFAULT, yet you can still enable and configure it."}],"poster":"knightkkd"}]},{"comment_id":"345439","content":"it seems serverless corresponds to \"high concurrency\" as per this blogpost - https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html","poster":"karma_wins","upvote_count":"3","timestamp":"1619705280.0"},{"poster":"sdas1","upvote_count":"2","timestamp":"1610826420.0","content":"The answer is correct. I am able to create a High Concurrency cluster as per given json config.","comment_id":"269067","comments":[{"timestamp":"1610877840.0","content":"Cluster Mode - High Concurrency\nDatabricks Runtime Version\n7.4 (includes Apache Spark 3.0.1, Scala 2.12)\nNewThis Runtime version supports only Python 3.\nAutopilot Options\n\nEnable autoscaling\n\nTerminate after \n120\n minutes of inactivity\nWorker Type\nStandard_DS13_v2\n56.0 GB Memory, 8 Cores, 2 DBU\nMin Workers\n2\nMax Workers\n8\nDriver Type\nStandard_DS13_v2\n56.0 GB Memory, 8 Cores, 2 DBU","comments":[{"timestamp":"1610877840.0","upvote_count":"1","poster":"sdas1","comment_id":"269442","comments":[{"upvote_count":"2","content":"As per below link, High Concurrency clusters are configured to not terminate automatically. But while configuring High Concurrency, I am able to set the autotermination_minutes=120\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure","comment_id":"282880","poster":"sdas1","timestamp":"1612375080.0"}],"content":"{\n \"autoscale\": {\n \"min_workers\": 2,\n \"max_workers\": 8\n },\n \"cluster_name\": \"cluster2\",\n \"spark_version\": \"7.4.x-scala2.12\",\n \"spark_conf\": {\n \"spark.databricks.repl.allowedLanguages\": \"sql,python,r\",\n \"spark.databricks.cluster.profile\": \"serverless\"\n },\n \"node_type_id\": \"Standard_DS13_v2\",\n \"driver_node_type_id\": \"Standard_DS13_v2\",\n \"ssh_public_keys\": [],\n \"custom_tags\": {\n \"ResourceClass\": \"Serverless\"\n },\n \"spark_env_vars\": {\n \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\"\n },\n \"autotermination_minutes\": 120,\n \"enable_elastic_disk\": true,\n \"cluster_source\": \"UI\",\n \"init_scripts\": [],\n \"cluster_id\": \"0116-203628-tins636\"\n}"}],"comment_id":"269441","upvote_count":"2","poster":"sdas1"}]},{"comment_id":"267145","timestamp":"1610641680.0","poster":"zarga","content":"1. YES \n2. NO (use job custer to reduce cost rather than high concurency)\n3. NO (we can use Delta lake starting from spark 2.4.2 based on scala 2.12.x. In this example the cluster definition is based on scala 2.11)","upvote_count":"4"},{"comment_id":"239075","content":"allowed languages are R SQL and Python -> High concurrency cluster\nautoscaling is enabled as seen by min and max nodes -> minimise cost definitely\nno CREATE TABLE syntax -> no Delta Lake table\nYes Yes No","timestamp":"1607507640.0","poster":"syu31svc","upvote_count":"5"},{"comments":[{"content":"sorry, ignore the second point.","timestamp":"1605205140.0","poster":"lingjun","comment_id":"218081","upvote_count":"1"}],"timestamp":"1605204900.0","upvote_count":"1","poster":"lingjun","content":"1. High Concurrency \"Yes\" because of following config:\n\"spark_conf\": {\n \"spark.databricks.cluster.profile\": \"serverless\",\n \"spark.databricks.repl.allowedLanguages\": \"sql,python,r\"\n },\n2. minimise cost \"No\", because there is no auto scale config as below:\n \"autoscale\": {\n \"min_workers\": 2,\n \"max_workers\": 8\n },","comment_id":"218078"},{"comment_id":"152996","upvote_count":"1","poster":"Yaswant","content":"I think for part 2 of question \"NO\" is the right answer. Let's say we have three scheduled jobs with a difference of 180 minutes each that had to be run throughout the day. Since we have set the auto-termination to 90 minutes the cluster after executing the first schedule job remains active for 90 minutes so we'll have to pay for it. Which in turn doesn't minimize cost.","timestamp":"1596883440.0"},{"timestamp":"1595842620.0","poster":"passnow","upvote_count":"2","comment_id":"144799","content":"Data Lakes Support All Data Types\nA data lake holds big data from many sources in a raw, granular format. It can store structured, semi-structured, or unstructured data, which means data can be kept in a more flexible format so we can transform it when we’re ready to use . I stick with the default answer"},{"upvote_count":"3","poster":"shaktiprasad88","comment_id":"132779","content":"I think Answer is\nYes\nNo\nNo\n\nThe given Configuration is for Interactive Cluster -(My Sample Interactive Cluster with Delta Enabled)\n{\n \"autoscale\": {\n \"min_workers\": 2,\n \"max_workers\": 8\n },\n \"cluster_name\": \"dev_work\",\n \"spark_version\": \"6.6.x-scala2.11\",\n \"spark_conf\": {\n \"spark.databricks.delta.preview.enabled\": \"true\"\n },\n \"node_type_id\": \"Standard_DS3_v2\",\n \"driver_node_type_id\": \"Standard_DS3_v2\",\n \"ssh_public_keys\": [],\n \"custom_tags\": {},\n \"spark_env_vars\": {},\n \"autotermination_minutes\": 120,\n \"enable_elastic_disk\": true,\n \"cluster_source\": \"UI\",\n \"init_scripts\": [],\n \"cluster_id\": \"0529-111838-patch496\"\n}","timestamp":"1594536780.0","comments":[{"poster":"brcdbrcd","content":"But it says: The Databricks cluster supports the creation of a Delta Lake table.\nIt is a spark cluster and it \"supports\" if it is needed. So I would say Yes.","timestamp":"1607167200.0","upvote_count":"1","comment_id":"235628"}]},{"comment_id":"132003","timestamp":"1594461780.0","upvote_count":"4","content":"High Concurrency does not support Auto termination; Auto-scaling minimizes the cost. So, No, Yes, Yes","poster":"dip17"},{"comment_id":"131281","content":"First - True \nOptimized to run concurrent SQL, Phyton and R workloads\" Doesn't support Scala. Previously known as SERVERLESS","poster":"alexvno","timestamp":"1594370640.0","upvote_count":"1"},{"comments":[{"upvote_count":"2","content":"Standard_DS13_v2 is a High Concurrency Cluster Mode, if I select High Concurrency the Worker Type defaults to Standard_DS13_v2","poster":"essdeecee","timestamp":"1604045160.0","comment_id":"209088"}],"poster":"AhmedReda","timestamp":"1593113160.0","upvote_count":"6","content":"This link shows that standard for single user, so i think High concurrency clusters for concurrency : https://docs.microsoft.com/en-us/azure/databricks/clusters/configure\nStandard clusters\n---------------------------\nStandard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.\n1) No\n2) Yes :autoscale enabled and auto-termination was decreased from 120 default to 90\n3) Yes","comment_id":"119766"},{"upvote_count":"2","content":"Yes - Standard_DS13_V2 is cluster mode for High concurrency \nNo- It's an interactive cluster\nYes - I'm not sure, it seems like it is default setting when SQL API is chosen.","comment_id":"115514","timestamp":"1592739120.0","poster":"Abhilvs"},{"content":"In part 2 of the question, I have a confusion, in the datbricks config, the auto termination is set to 90 mins, and hence there is a provision of automatically getting the cluster down and minimizing cost. Had it been 0, it would to be auto termination disabled.\n\nAny thoughtS?","comment_id":"65235","comments":[{"timestamp":"1584610380.0","upvote_count":"3","poster":"avestabrzn","comment_id":"65937","content":"I think it talks about running a job on a job cluster instead of an interactive cluster. Not sure.."},{"content":"I think part 2 should be yes","timestamp":"1587403320.0","comment_id":"77061","comments":[{"comment_id":"96826","timestamp":"1590586620.0","content":"To minimize the cost, it shoud be set to the lower value = 10. Since it is set to 90, it means the cluster can run for nothing during the next 90 minutes after the last schedule job which is not cost-efficient so the answer \"NO\" is correct for this one.\nYES/NO/YES seams to be the correct answer.","poster":"Mathster","upvote_count":"21"}],"upvote_count":"4","poster":"Yuri1101"},{"upvote_count":"1","timestamp":"1609326600.0","comment_id":"255544","content":"High Concurrency clusters are configured to not terminate automatically. https://docs.microsoft.com/en-us/azure/databricks/clusters/configure","poster":"andreeavi","comments":[{"content":"ignore it. it's not set by default","timestamp":"1609326660.0","comment_id":"255545","upvote_count":"1","poster":"andreeavi"}]}],"timestamp":"1584463200.0","upvote_count":"2","poster":"Nehuuu"}],"question_id":71,"answer_description":"Box 1: Yes -\n\nBox 2: No -\nautotermination_minutes: Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated.\nIf specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.\n\nBox 3: Yes -\nReferences:\nhttps://docs.databricks.com/dev-tools/api/latest/clusters.html","unix_timestamp":1584463200,"topic":"2","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0015800001.png","https://www.examtopics.com/assets/media/exam-media/03774/0015900001.jpg"],"answer_ET":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0016000001.jpg"],"url":"https://www.examtopics.com/discussions/microsoft/view/16875-exam-dp-201-topic-2-question-11-discussion/","answer":"","timestamp":"2020-03-17 17:40:00","isMC":false,"question_text":"HOTSPOT -\nThe following code segment is used to create an Azure Databricks cluster.\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","exam_id":66},{"id":"XOXkxrhTme1AyGzyz9lH","unix_timestamp":1592561940,"exam_id":66,"topic":"2","answers_community":[],"question_id":72,"answer_description":"Box 1: Interactive Query -\nChoose Interactive Query cluster type to optimize for ad hoc, interactive queries.\n\nBox 2: Hadoop -\nChoose Apache Hadoop cluster type to optimize for Hive queries used as a batch process.\nNote: In Azure HDInsight, there are several cluster types and technologies that can run Apache Hive queries. When you create your HDInsight cluster, choose the appropriate cluster type to help optimize performance for your workload needs.\nFor example, choose Interactive Query cluster type to optimize for ad hoc, interactive queries. Choose Apache Hadoop cluster type to optimize for Hive queries used as a batch process. Spark and HBase cluster types can also run Hive queries.\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/azure/hdinsight/hdinsight-hadoop-optimize-hive-query?toc=%2Fko-kr%2Fazure%2Fhdinsight%2Finteractive-query%\n2FTOC.json&bc=%2Fbs-latn-ba%2Fazure%2Fbread%2Ftoc.json","discussion":[{"content":"HDInsight is not covered in the exam any more.","poster":"M0e","upvote_count":"10","timestamp":"1603456680.0","comment_id":"204671"},{"comment_id":"113853","timestamp":"1592561940.0","poster":"REZ82","upvote_count":"6","content":"Cluster types in HDInsight\nhttps://docs.microsoft.com/bs-latn-ba/azure/hdinsight/hdinsight-overview#cluster-types-in-hdinsight"},{"timestamp":"1616415480.0","content":"NOT ANY MORE IN THE DP-201","poster":"H_S","upvote_count":"1","comment_id":"317142"},{"timestamp":"1614424440.0","comment_id":"300215","poster":"Deepu1987","upvote_count":"2","content":"The given soln is correct\nInteractive qury - In-memory caching for interactive and faster Hive queries\nhadoop - A framework that uses HDFS, YARN resource management, and a simple MapReduce programming model to process and analyze batch data in parallel.\nWe're receiving qns on this topic until dp-201 is removed"},{"upvote_count":"6","content":"Answer is correct.\n\nSales: Interactive Queries\nIn-memory caching for interactive and faster Hive queries. \n\nAccounts: Hadoop\nA framework that uses HDFS, YARN resource management, and a simple MapReduce programming model to process and analyze batch data in parallel.\n\nSource: https://docs.microsoft.com/bs-latn-ba/azure/hdinsight/hdinsight-overview#cluster-types-in-hdinsight","poster":"chaoxes","comment_id":"247831","timestamp":"1608366240.0"},{"content":"Answer is correct","timestamp":"1607502180.0","upvote_count":"3","comment_id":"239022","poster":"syu31svc"}],"timestamp":"2020-06-19 12:19:00","url":"https://www.examtopics.com/discussions/microsoft/view/23499-exam-dp-201-topic-2-question-12-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0016100001.jpg","https://www.examtopics.com/assets/media/exam-media/03774/0016200001.jpg"],"answer":"","question_text":"HOTSPOT -\nA company stores large datasets in Azure, including sales transactions and customer account information.\nYou must design a solution to analyze the data. You plan to create the following HDInsight clusters:\n//IMG//\n\nYou need to ensure that the clusters support the query requirements.\nWhich cluster types should you recommend? To answer, select the appropriate configuration in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","isMC":false,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0016300001.jpg"],"answer_ET":""},{"id":"hkc3pqWh149KOjGBx8Qi","answers_community":[],"timestamp":"2020-12-15 14:34:00","topic":"2","choices":{"B":"No","A":"Yes"},"answer_images":[],"exam_id":66,"isMC":true,"question_images":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have streaming data that is received by Azure Event Hubs and stored in Azure Blob storage. The data contains social media posts that relate to a keyword of\nContoso.\nYou need to count how many times the Contoso keyword and a keyword of Litware appear in the same post every 30 seconds. The data must be available to\nMicrosoft Power BI in near real-time.\nSolution: You use Azure Data Factory and an event trigger to detect when new blobs are created. You use mapping data flows in Azure Data Factory to aggregate and filter the data, and then send the data to an Azure SQL database. You consume the data in Power BI by using DirectQuery mode.\nDoes the solution meet the goal?","question_id":73,"unix_timestamp":1608039240,"answer_description":"","discussion":[{"content":"Answer is NO. The propose solution to utilize ADF doesn't have real-time capability. Instead use of Azure Stream Analytics as it can count the data with the use of window function and output the data directly to the PowerBI dataset.","poster":"cadio30","upvote_count":"11","comment_id":"366023","timestamp":"1621908180.0"},{"upvote_count":"8","timestamp":"1615134240.0","content":"scenario: You need to count how many times .. appear in the same post every 30 seconds. \nSolution: You use Azure Data Factory and an event trigger \nAnswer: No - you don't need an event trigger - you need a schedule trigger each 30 seconds","poster":"AlexD332","comment_id":"305214"},{"poster":"sjain91","content":"Answer: No - you don't need an event trigger - you need a schedule trigger each 30 seconds","timestamp":"1619869200.0","comment_id":"346888","upvote_count":"4"},{"timestamp":"1617708060.0","comment_id":"329553","comments":[{"comment_id":"345863","timestamp":"1619755020.0","upvote_count":"1","poster":"karma_wins","content":"reply from maciejt seems only logical to me for \"No\" to this question i.e. since mapping dataflow needs few mins to spark up the cluster."}],"content":"Mapping data flow needs few minutes to start up spark cluster, it's good for batch ETL, but not suitable for real time stream processing.","upvote_count":"4","poster":"maciejt"},{"timestamp":"1610660520.0","upvote_count":"1","comment_id":"267451","content":"correction: in my previous post, forget the 30-word distance. But the \"break a post into words\" remains the reason why the proposed solution does not fully meet the requirements IMHO","poster":"mohowzeh"},{"timestamp":"1610660400.0","content":"Answer is correct in my opinion. The proposed approach is missing a step where you break out the social media post into words and investigate the 30-word \"distance\".","upvote_count":"1","comment_id":"267450","poster":"mohowzeh"},{"timestamp":"1609613760.0","upvote_count":"5","poster":"cm19","content":"ADF is not a suitable solution for realtime feeds.When Streaming analytics can do the job directly no need of triggers to identify new blobs.So the answer looks correct to me.","comment_id":"257793"},{"upvote_count":"2","content":"Why ? There should be an explanation. Solution provided seems reasonable","comment_id":"244583","timestamp":"1608039240.0","poster":"HPotter"}],"answer_ET":"B","url":"https://www.examtopics.com/discussions/microsoft/view/39912-exam-dp-201-topic-2-question-13-discussion/","answer":"B"},{"id":"QRh6M6dKypGSiCirVxwH","answer":"A","discussion":[{"upvote_count":"1","timestamp":"1641981780.0","poster":"vivekazure","comment_id":"522035","content":"Unless Power BI connects to SQL DB thru Import mode and schedule a automatic refresh, Data can never be available in near real-time."},{"timestamp":"1622251380.0","poster":"Saravjeet","upvote_count":"1","content":"What I think is the proposed solution is correct, as we can generate the report directly through power BI by connecting it to ASA but not through direct query mode, I am not able to find the source as ASA while referring to DirectQuery. Refer the link: https://docs.microsoft.com/en-us/power-bi/connect-data/power-bi-data-sources\n\nSo we have to use sql db if we use directquery mode. Thanks.","comment_id":"369099"},{"timestamp":"1621908600.0","poster":"cadio30","content":"The propose solution is feasible as the data can be stored in Azure SQL DB then use of direct mode from Power BI retrieves the latest data while 'import' connectivity mode requires schedule to refresh the dataset. And as mentioned in the requirement, it states 'near real time' unless it is explicitly label as 'real time' then Azure Stream Analytics is the most suited solution.","upvote_count":"1","comment_id":"366027"},{"comment_id":"319003","poster":"Nik71","timestamp":"1616579160.0","content":"You can add Power BI as an output within Azure Stream Analytics (ASA), and then visualize those data streams in the Power BI service in real time\n\nhttps://docs.microsoft.com/en-us/power-bi/connect-data/service-real-time-streaming#pushing-data-to-datasets","upvote_count":"3"},{"upvote_count":"3","comment_id":"318999","content":"Answer should be No why we output data from Stream to SQL db we can direct output to power BI","timestamp":"1616578980.0","poster":"Nik71","comments":[{"upvote_count":"3","content":"but they have not mentioned real-time, it is near real-time so it should be year","comment_id":"332247","timestamp":"1618022820.0","poster":"chirag1234"}]}],"question_id":74,"choices":{"B":"No","A":"Yes"},"answer_images":[],"answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/power-bi/service-real-time-streaming https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends","exam_id":66,"unix_timestamp":1616578980,"url":"https://www.examtopics.com/discussions/microsoft/view/48071-exam-dp-201-topic-2-question-14-discussion/","question_images":[],"topic":"2","answers_community":[],"isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have streaming data that is received by Azure Event Hubs and stored in Azure Blob storage. The data contains social media posts that relate to a keyword of\nContoso.\nYou need to count how many times the Contoso keyword and a keyword of Litware appear in the same post every 30 seconds. The data must be available to\nMicrosoft Power BI in near real-time.\nSolution: You create an Azure Stream Analytics job that uses an input from Event Hubs to count the posts that have the specified keywords, and then send the data to an Azure SQL database. You consume the data in Power BI by using DirectQuery mode.\nDoes the solution meet the goal?","timestamp":"2021-03-24 10:43:00","answer_ET":"A"},{"id":"qOxtyjtOykjySb9bIgyu","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have streaming data that is received by Azure Event Hubs and stored in Azure Blob storage. The data contains social media posts that relate to a keyword of\nContoso.\nYou need to count how many times the Contoso keyword and a keyword of Litware appear in the same post every 30 seconds. The data must be available to\nMicrosoft Power BI in near real-time.\nSolution: You use Azure Databricks to create a Scala notebook. You use a Structured Streaming job to connect to the event hub that counts the posts that have the specified keywords, and then writes the data to a Delta table. You consume the data in Power BI by using DirectQuery mode.\nDoes the solution meet the goal?","url":"https://www.examtopics.com/discussions/microsoft/view/35815-exam-dp-201-topic-2-question-15-discussion/","question_id":75,"discussion":[{"content":"this question is outdated. this should be perfectly possible.","poster":"Shrikant_Kulkarni","comment_id":"211500","upvote_count":"14","timestamp":"1604348580.0"},{"poster":"cadio30","comment_id":"366091","content":"The propose solution is feasible as the PowerBI can integrate to Azure Databricks. Unless the requirement changes to 'real time' then Azure Stream Analytics is suited service.\n\nReference: https://azure.microsoft.com/nl-nl/blog/structured-streaming-with-databricks-into-power-bi-cosmos-db/","upvote_count":"3","timestamp":"1621914960.0"},{"poster":"mohowzeh","content":"https://databricks.com/blog/2020/10/30/announcing-azure-databricks-power-bi-connector-public-preview.html","upvote_count":"2","timestamp":"1610660760.0","comments":[{"comment_id":"270233","comments":[{"upvote_count":"5","comment_id":"270247","timestamp":"1610973720.0","poster":"ACSC","content":"I mean, it is possible, but not near real-time. The answer is No."}],"poster":"ACSC","upvote_count":"2","content":"So, the answer is Yes, the solution meets the goal.","timestamp":"1610972460.0"}],"comment_id":"267452"}],"answer_ET":"B","answers_community":[],"choices":{"A":"Yes","B":"No"},"exam_id":66,"answer_description":"","isMC":true,"question_images":[],"answer_images":[],"unix_timestamp":1604348580,"topic":"2","answer":"B","timestamp":"2020-11-02 21:23:00"}],"exam":{"name":"DP-201","isImplemented":true,"provider":"Microsoft","id":66,"isMCOnly":false,"lastUpdated":"12 Apr 2025","isBeta":false,"numberOfQuestions":206},"currentPage":15},"__N_SSP":true}