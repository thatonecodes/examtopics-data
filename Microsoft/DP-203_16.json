{"pageProps":{"questions":[{"id":"T66b0K63KTs0INbn1ywA","exam_id":67,"answer":"","topic":"1","answers_community":[],"question_id":76,"discussion":[{"timestamp":"1667157240.0","comments":[{"poster":"tlb_20","upvote_count":"1","content":"Check the question 39, which is quite similar:\n\nSELECT SupplierKey, StockItemKey, COUNT(*)\nFROM FactPurchase\nWHERE DateKey >= 20210101 \n AND DateKey <= 20210131\nGROUP By SupplierKey, StockItemKey \n\nB. hash-distributed on PurchaseKey\nD. hash-distributed on DateKey\n\nBear in mind the type of the orderDateKey column, it's int, not date, not datetime... Doesn't it make a difference?\n\nIn this case I'd choose the ProductKey since it says queries will be done \"for specific products\", but in the previous case the WHERE clause uses the DateKey field.","timestamp":"1713270600.0","comments":[{"poster":"hypersam","upvote_count":"1","timestamp":"1736057340.0","comment_id":"1336660","content":"you don't hash dateKey because same dateKey goes to same distribution, thus only a few out of total 60 distributions are working in a query. Instead, within each distribution we can partition by dateKey, so that all distributions are used in a query and each distribution can filter data efficiently because of dateKey partition"},{"upvote_count":"1","comment_id":"1204552","poster":"Dusica","content":"Date key is same as date, it would have distributed by date and that is a no no","timestamp":"1714478940.0"}],"comment_id":"1196576"},{"comment_id":"1026734","comments":[{"timestamp":"1697026800.0","content":"If you cant read don't bother commenting","comment_id":"1040624","poster":"Euanm28","upvote_count":"3"}],"poster":"Data_Analytics","timestamp":"1696604520.0","upvote_count":"6","content":"This sentence helped me soooo much - Hash the date NEVER.. thank you"},{"upvote_count":"3","content":"Usually you don't use the hash date, because if the query is made daily, all record contains the same date, so the same node will do the job. In this example \" from the last year for a specific product\", so we have 365 separate node from date, but we have only one product!!","timestamp":"1706214420.0","comment_id":"1132034","poster":"Gikan"}],"comment_id":"707985","poster":"ted0809","upvote_count":"55","content":"you don't hash the date.. never.."},{"comment_id":"787957","poster":"Lestrang","timestamp":"1674668040.0","content":"By using the product key as the distribution key, the data for a specific product will be stored on the same node, allowing for faster aggregation of the values in SalesAmount and OrderQuantity for that product.","upvote_count":"11"},{"comment_id":"1357892","timestamp":"1739812980.0","upvote_count":"1","poster":"Pey1nkh","content":"correct answer! why Clustered Columnstore Index ? \ncause its best for large fact tables.\nReduces data storage by up to 10x.\nSpeeds up aggregation queries."},{"poster":"Dusica","comment_id":"1204554","comments":[{"poster":"17lan","comment_id":"1318461","content":"Answer to 1. Question and why other options are wrong:\n\nCorrect Answer: CLUSTERED COLUMNSTORE INDEX\nReasoning:\nA clustered columnstore index (CCI) is the default and most efficient index for large fact tables in Synapse Analytics because it compresses data significantly, reducing storage and improving query performance for analytical workloads. It is optimized for aggregations, such as summing up SalesAmount and OrderQuantity, which are typical in analytical queries.\n\nWhy Others Are Incorrect:\nCLUSTERED INDEX ([OrderDateKey]): A clustered index is better suited for transactional systems, not for analytical systems with large fact tables.\n\nHEAP: Heap tables lack any indexing, making them inefficient for queries that require aggregation or filtering on large datasets.\n\nINDEX on [ProductKey]: A non-clustered index might help in specific queries but doesn't optimize overall performance for large-scale analytics.","timestamp":"1732689420.0","upvote_count":"1"}],"upvote_count":"1","timestamp":"1714479120.0","content":"distriubtion hash(product key)\nclustered index (date key)"},{"timestamp":"1712887740.0","poster":"dgerok","comment_id":"1194087","upvote_count":"1","content":"The answer is right"},{"comment_id":"998349","upvote_count":"6","timestamp":"1693814880.0","content":"Clustered columnstore\nHash(ProductKey)","poster":"kkk5566"},{"content":"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choose-a-distribution-column-with-data-that-distributes-evenly\n\nTo balance the parallel processing, select a distribution column or set of columns that:\n\nHas many unique values. The distribution column(s) can have duplicate values. All rows with the same value are assigned to the same distribution. Since there are 60 distributions, some distributions can have > 1 unique values while others may end with zero values.\nDoes not have NULLs, or has only a few NULLs. For an extreme example, if all values in the distribution column(s) are NULL, all the rows are assigned to the same distribution. As a result, query processing is skewed to one distribution, and does not benefit from parallel processing.\nIs not a date column. All data for the same date lands in the same distribution, or will cluster records by date. If several users are all filtering on the same date (such as today's date), then only 1 of the 60 distributions do all the processing work.\n\nAns: Hash(ProductKey)","poster":"smsme323","comment_id":"679496","timestamp":"1664176680.0","upvote_count":"9"},{"timestamp":"1662434220.0","content":"must hash on OrderDateKey because that field was not a date and it was used for filter condition \"from the last year for a specific product\"","comments":[{"poster":"Lestrang","upvote_count":"14","comment_id":"672705","content":"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\nthe example in the sample-test is in this page and they used productkey for hashing, so yeah, the answer is productkey","timestamp":"1663529880.0"}],"poster":"Phund","comment_id":"660749","upvote_count":"8"},{"content":"correct","comment_id":"660682","poster":"anks84","timestamp":"1662428580.0","upvote_count":"4"}],"isMC":false,"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/80470-exam-dp-203-topic-1-question-61-discussion/","question_text":"HOTSPOT -\nYou have an Azure Synapse Analytics dedicated SQL pool.\nYou need to create a table named FactInternetSales that will be a large fact table in a dimensional model. FactInternetSales will contain 100 million rows and two columns named SalesAmount and OrderQuantity. Queries executed on FactInternetSales will aggregate the values in SalesAmount and OrderQuantity from the last year for a specific product. The solution must minimize the data size and query execution time.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2022-09-06 03:43:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0012900001.jpg","https://www.examtopics.com/assets/media/exam-media/04259/0013000001.jpg"],"unix_timestamp":1662428580,"answer_description":"Box 1: (CLUSTERED COLUMNSTORE INDEX\n\nCLUSTERED COLUMNSTORE INDEX -\nColumnstore indexes are the standard for storing and querying large data warehousing fact tables. This index uses column-based data storage and query processing to achieve gains up to 10 times the query performance in your data warehouse over traditional row-oriented storage. You can also achieve gains up to\n10 times the data compression over the uncompressed data size. Beginning with SQL Server 2016 (13.x) SP1, columnstore indexes enable operational analytics: the ability to run performant real-time analytics on a transactional workload.\nNote: Clustered columnstore index\nA clustered columnstore index is the physical storage for the entire table.\n\nTo reduce fragmentation of the column segments and improve performance, the columnstore index might store some data temporarily into a clustered index called a deltastore and a B-tree list of IDs for deleted rows. The deltastore operations are handled behind the scenes. To return the correct query results, the clustered columnstore index combines query results from both the columnstore and the deltastore.\nBox 2: HASH([ProductKey])\nA hash distributed table distributes rows based on the value in the distribution column. A hash distributed table is designed to achieve high performance for queries on large tables.\nChoose a distribution column with data that distributes evenly\nIncorrect:\n* Not HASH([OrderDateKey]). Is not a date column. All data for the same date lands in the same distribution. If several users are all filtering on the same date, then only 1 of the 60 distributions do all the processing work\n* A replicated table has a full copy of the table available on every Compute node. Queries run fast on replicated tables since joins on replicated tables don't require data movement. Replication requires extra storage, though, and isn't practical for large tables.\n* A round-robin table distributes table rows evenly across all distributions. The rows are distributed randomly. Loading data into a round-robin table is fast. Keep in mind that queries can require more data movement than the other distribution methods.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0012800001.jpg"]},{"id":"ysK4gscOMMdS5KR2r8nL","topic":"1","timestamp":"2022-09-06 03:43:00","question_id":77,"answer_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/microsoft/view/80469-exam-dp-203-topic-1-question-62-discussion/","exam_id":67,"choices":{"B":"once per year","D":"once per week","A":"once per month","C":"once per day"},"discussion":[{"content":"Remembering that we have data splitted in distribution (60 nodes) and considering that we Need a MINMIUM 1 million rows per distribution, we have:\n\nA. once per month = 30 milion / 60 = 500k record per partition\nB. once per year = 360 milion / 60 = 6 milion record per partition\nC. once per day = about 1 milion / 60 = 16k record per partition\nD. once per week =about 7.5 milion / 60 = 125k record per partition\n\ncorrect should be B","poster":"vrodriguesp","comment_id":"750755","comments":[{"content":"actually to be more accurate, I should have written record per distribution. We have 1 milion rows per distribution and 60 milion rows per partition..","comment_id":"802382","upvote_count":"2","poster":"vrodriguesp","timestamp":"1675880460.0"},{"content":"I think you left out the fact that the table already has 1 billion records. This will change your calculations.","timestamp":"1675177140.0","comment_id":"794316","poster":"yogiazaad","upvote_count":"9","comments":[{"comment_id":"796886","timestamp":"1675413420.0","upvote_count":"5","poster":"CCCool77","comments":[{"upvote_count":"1","poster":"hypersam","comment_id":"1349809","content":"CCI will be applied on rows within each new partition, so it doesn't matter how many rows in other partitions","timestamp":"1738402200.0"},{"poster":"d47320d","content":"no, we should consider everything including the table in order to come up with the most efficient solution","upvote_count":"1","timestamp":"1715640720.0","comment_id":"1211113"},{"poster":"hiyoww","content":"agree, the question ask about partition, not the table distribution","comment_id":"978187","upvote_count":"2","timestamp":"1691717940.0"}],"content":"You should consider the partition, not the table"}]},{"content":"I think you mix up the concept of distribution and partition, we always do partitions with date, distribution with Product Key. I think you over think, no need to think about the calculation","upvote_count":"1","poster":"hiyoww","comments":[{"content":"sorry may be you are right","upvote_count":"1","comments":[{"content":"I have contributor access which is purchased for $47.99 but no downloadable PDF with questions and explanations received yet.","timestamp":"1691750940.0","upvote_count":"3","poster":"Gcplearner8888","comment_id":"978533","comments":[{"content":"Contact their Customer Care for answer with your complain and they will respond to you.","poster":"Paulkuzzio","upvote_count":"1","comment_id":"1026096","timestamp":"1696553220.0"}]}],"poster":"hiyoww","timestamp":"1691718240.0","comment_id":"978190"}],"comment_id":"978186","timestamp":"1691717880.0"},{"poster":"gggmaaster","upvote_count":"3","comment_id":"1056139","timestamp":"1698491460.0","content":"Following this logic. Although A is less than 1m requirement, but it is the closest on to 1m. B met the requirement, but is is way too big hence loading to memory is slower than 500k one, already A may result in higher number of partitions, which is larger storage space."}],"timestamp":"1671531660.0","upvote_count":"99"},{"comment_id":"660681","timestamp":"1662428580.0","poster":"anks84","upvote_count":"11","content":"Correct, \nConsidering the high volume of data, for faster queries its recommended to create fewer partitions.\n\" If a table contains fewer than the recommended minimum number of rows per partition(i.e. 60 million rows per month for 60 distributed partitions, consider using fewer partitions in order to increase the number of rows per partition.\""},{"comment_id":"1359403","poster":"ShdwZephyr","upvote_count":"1","content":"Selected Answer: A\nI would say its option A.\n- Efficiently load new data (in this case, 30 million new rows each month).\n- Improve query performance for time-based queries.","timestamp":"1740075060.0"},{"comment_id":"1350427","poster":"prem__raj","timestamp":"1738500660.0","upvote_count":"1","content":"Selected Answer: A\nI think it should be by month. There are 12 months in a year. The table already has 1 billion rows, we have 60 distribution nodes. So 1 billion/60 = 16.6 Million approx per node. We get 30 million data per month, we distribute it to 60 nodes, that is 30 Million/60 = 50,000 rows per node. Now we have 16.6 million + 50,000 rows per node that’s 16,650,00 rows. Now if we partition by month that’s 16,650,00/12 we’ll have close to 138750 which is 1.3 million. And the optimal size per partition is close to 1 million. Therefore A"},{"content":"Selected Answer: A\nCreating a partition once per month strikes the right balance between performance, manageability, and data loading efficiency","comment_id":"1322049","upvote_count":"1","poster":"abhi_11kr1","timestamp":"1733338500.0"},{"content":"Selected Answer: A\nI think the loading performance over here is the determining factor. With a partition by month the date can be quickly swapped into the table with a partition swap. \n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition","timestamp":"1731234600.0","upvote_count":"1","poster":"mavdplas","comment_id":"1309398"},{"upvote_count":"1","content":"The table currently has 1 billion rows. With 60 partitions, this means there are approximately 16,666,666 rows per partition.\n\nEach month, an additional 30 million rows will be added. After the first month, the table will have 1.03 billion rows, resulting in approximately 17,166,666 rows per partition.\n\nI recommend going with Option A: partitioning once per month.","poster":"renan_ineu","comments":[{"comments":[{"poster":"prem__raj","upvote_count":"1","timestamp":"1738500300.0","content":"Data will be partitioned only after it's distributed. The 30 million would be distributed among the 60 nodes and then partitioned in each node inside.","comment_id":"1350422"}],"comment_id":"1348706","timestamp":"1738175760.0","poster":"hypersam","upvote_count":"1","content":"wrong. New data will be saved in a new partition, not distributed across all previous partitions. so it doesn't matter how much data in previous partitions"}],"timestamp":"1724525220.0","comment_id":"1271801"},{"poster":"d47320d","upvote_count":"1","comment_id":"1211112","timestamp":"1715640600.0","content":"We have to consider all information given to us about Table1, including existing data, in order to optimize query performance and data loading for the specific table.\nSo, a year after Table1 will contain 1360M i.e. around 22.6M per distribution having specified one partition per year.\nInstead, if we specify one partition by month, then we will have around 1.8M per distribution and partition.\nTypical analytical queries involve filter predicates for a specific month.\nIsn't it more efficient to process 1.8M instead of 22.6M ? (optimize query performance).\nLoading data to a new empty or smaller partition isn't it faster than loading it to one with pre-existing data or larger partition (optimize data loading)?\nThe above leads us to \"A. once per month\"."},{"comments":[{"timestamp":"1715640840.0","poster":"d47320d","comment_id":"1211115","content":"you are totally ignoring existing table data","upvote_count":"1"}],"upvote_count":"1","poster":"Alongi","content":"Selected Answer: B\nIt's B for that reason:\n12month * 30 mln / 60 nodes = 6 mln record per partition","timestamp":"1713211440.0","comment_id":"1196194"},{"content":"Selected Answer: B\nB) once per year, because your partition should contain more than 60 mln rows (1 mln for each of 60 nodes)","upvote_count":"2","poster":"dgerok","timestamp":"1713162060.0","comment_id":"1195847"},{"content":"Selected Answer: B\nB. once per year = 360 milion / 60 = 6 milion record per partition","upvote_count":"2","timestamp":"1712887860.0","comment_id":"1194088","poster":"dgerok"},{"poster":"[Removed]","upvote_count":"2","timestamp":"1710743520.0","content":"Selected Answer: A\nIt is about partition, each distribution has several partitions.","comment_id":"1176300"},{"content":"Selected Answer: A\nA partition is divided into distribution units.\n\nA) eleven per month => 30 million / 60 = 500k rows per partition\nB) eleven per year => 360 million / 60 = 6 million rows per partition\nC) eleven per day => 1 million / 60 = 15k rows per partition\nD) eleven per week => 7.5~ million/60 = 125k rows per partition\n\nBut since I already have 1 billion rows distributed, each distribution node would have 16.7 million rows, fulfilling more than the minimum, since I am interested in having a certain reasonable number of partitions to increase the speed of the queries, I would choose once per month, because once per year would create very few partitions.\n\nTherefore the answer is A)","upvote_count":"4","timestamp":"1709417880.0","comment_id":"1164387","poster":"mav2000","comments":[{"content":"you'll have new partitions whenever you update the table's partitions. if you create a new partition every month, this new partition will have only 500k rows per distribution. I agree that the previous partitions have more than enough records, but the problem is with the new partition.","comment_id":"1262802","poster":"SajadAhm","upvote_count":"2","timestamp":"1723187400.0"}]},{"comment_id":"1145101","content":"Selected Answer: A\nGiven that you’ll be adding 30 million rows per month, consider partitioning once per month based on the Sales Date column.\nThis approach balances data maintenance and query performance while avoiding excessive partitioning.\nTherefore, the correct answer is A. once per month","poster":"rocky48","timestamp":"1707435180.0","upvote_count":"2"},{"content":"To optimize query performance and data loading for your Azure Synapse Analytics dedicated SQL pool table, you should create a partition based on the Sales Date column. \nPartitioning Frequency:\nThe decision on how often to create a partition depends on the data volume and your specific use case. Minimum Rows per Partition: For optimal compression and performance of clustered columnstore tables, it’s recommended to have a minimum of 1 million rows per distribution and partition.\nAutomatic Distribution: Dedicated SQL pools already divide each table into 60 distributed databases.\nRecommendation:\nGiven that you’ll be adding 30 million rows per month, consider partitioning once per month based on the Sales Date column.\nThis approach balances data maintenance and query performance while avoiding excessive partitioning.\nTherefore, the correct answer is A. once per month","poster":"rocky48","timestamp":"1707435060.0","comment_id":"1145099","upvote_count":"1"},{"comment_id":"1136779","upvote_count":"1","poster":"moneytime","timestamp":"1706707620.0","comments":[{"content":"Your calculation logic is incorrect.\nA year after Table1 will contain 1360M i.e. around 22.6M per distribution having specified one partition per year. Instead, if we specify one partition by week, then we will have around 0.4M per distribution and partition, which is below than 1M Microsoft recommendation.\nThis makes your choice wrong.","upvote_count":"2","comment_id":"1211118","poster":"d47320d","timestamp":"1715641200.0"}],"content":"I chose D\nReason:\nFor optimal performance and compression the followings are true;\nThe minimum record to be load is 60M permonth.\nThis implies that for ;\n*Per week = 60/4= 15M records\n*Pe rday = 15/7 =2.1M records\n*Per year= 12 *60M = 740M\nand so on for hours ,minutes ,sec. in the right proportions.\nConsidering ,the case at hand.\nAdding 30M rows monthly will have negative impact on the performance and compression of the table .The reason is that the quantity of records to load is less than the required minimum value of 60M for monthly partition.\n\nIn other to correct this ,30M minimum rows show be added every 2 weeks(not permonth) or 10M minimum rows per week.\nSo ,the most reasonable advice is to load the table perweek for optimal performance and compression."},{"upvote_count":"1","poster":"Charley92","content":"To partition Table1 based on the Sales Date column, you can create a partition for each month. Since thirty million rows will be added to Table1 each month, creating a partition once per month would be the most optimal solution for query performance and data loading 1. This way, the data can be easily queried and loaded, and the partitioning will not be too granular or too coarse 1.\n\nTherefore, the answer is A. once per month.","timestamp":"1706326740.0","comment_id":"1133030"},{"comment_id":"1126803","upvote_count":"1","timestamp":"1705680960.0","poster":"be8a152","content":"The table already has 1 billion records (ie 100 Million) + an additional 30 Million per month. So going by the theory that a minimum of 1 million rows should be accommodated by each of the 60 distributions created in the background each distribution will end up with 130 Million / 60 = 2.6 millions. So A. Once per month is the right answer"},{"content":"Selected Answer: A\nDid we ever come to a consensus on this? My thought would be once/month. You want to keep your records under 1 million, so the 60 million doesn't even come into it.","timestamp":"1704415380.0","comment_id":"1114147","poster":"sdg2844","upvote_count":"1"},{"upvote_count":"1","timestamp":"1702293120.0","comment_id":"1093390","content":"Once per month is correct","poster":"d046bc0"},{"content":"Chatgpt say A\nConsidering the frequency of data loads and the typical access patterns, the best practice according to the Azure Synapse Analytics documentation would be:\n\nA. **once per month**\n\nThis aligns with the monthly data load pattern and would facilitate efficient data management and query optimization.","comments":[{"upvote_count":"1","comment_id":"1278272","content":"man, don't trust fully in chat gpt, always rely on microsoft docs. for me, the bot said the best option would be D.","timestamp":"1725455280.0","poster":"renan_ineu"}],"comment_id":"1089732","timestamp":"1701893280.0","poster":"Momoanwar","upvote_count":"3"},{"content":"Selected Answer: B\nIf we partition per month, then each partition will have 30 million records. If each partition has 60 distributions, than this means each distribution has 30,000,000/60=500,000 rows.\nNow the key is in the Microsoft documentation sentence: \" For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed.\" A MINIMUM OF 1 MILLION. If we partition by month, it will be half of the minimum. Therefore it cannot be a month. Ideally it would be 2 months, so that each partition had 1 million rows. However, that is not an option. \nTherefore correct option= PER YEAR","timestamp":"1696770900.0","comments":[{"timestamp":"1715641620.0","content":"again same error, existing table data gets ignored.","upvote_count":"1","comment_id":"1211122","poster":"d47320d"}],"comment_id":"1027986","upvote_count":"4","poster":"ellala"},{"comment_id":"1002312","upvote_count":"1","poster":"[Removed]","timestamp":"1694165340.0","content":"Selected Answer: B\n60 mill"},{"poster":"[Removed]","content":"Selected Answer: A\nEach partition 60million","timestamp":"1694165280.0","comment_id":"1002311","upvote_count":"2"},{"comments":[{"upvote_count":"1","comment_id":"1000199","content":"Can you delete this comment? I was wrong re read the docs","timestamp":"1693978800.0","poster":"AvSUN"}],"upvote_count":"1","content":"Once a month is correct, partition and distributions are not the same thing","poster":"AvSUN","comment_id":"1000195","timestamp":"1693978620.0"},{"content":"Selected Answer: A\nA is correct","timestamp":"1693814940.0","upvote_count":"2","poster":"kkk5566","comment_id":"998351"},{"content":"Can anyone help solving this question with proper reason","comment_id":"974492","timestamp":"1691394120.0","poster":"akhil5432","upvote_count":"1"},{"content":"Selected Answer: B\nCorrect, only need to consider the newly added data here.","timestamp":"1690216800.0","upvote_count":"1","poster":"Andrew_Chen","comment_id":"961823"},{"upvote_count":"1","timestamp":"1690185660.0","content":"Selected Answer: B\nTrick question, should consider that more than 1 million is better","comment_id":"961308","poster":"Andrew_Chen"},{"comment_id":"936209","poster":"auwia","timestamp":"1687932060.0","upvote_count":"2","content":"Selected Answer: B\nProvided answer is correct."},{"timestamp":"1687897440.0","upvote_count":"1","comment_id":"935820","poster":"auwia","content":"Selected Answer: A\nThe solution must optimize query performance and data loading. => Will receive about 30M records each month, means l could means single file arriving 1 time per month. Partitioning by month we have good performance on data loading and good performance in general, because probably somebody will check monthly data."},{"content":"per year will decrease the query performance as it will create larger partition size. per month will improve query performance , faster data loading and easier data management. So the answer is \nPer month\nAs the question itself says that 30 million rows were added each month","comments":[{"comment_id":"906182","poster":"ajhak","timestamp":"1684968720.0","content":"Sir, you are incorrect.","upvote_count":"2"}],"timestamp":"1684867860.0","poster":"janaki","upvote_count":"1","comment_id":"905153"},{"poster":"ajhak","comment_id":"901475","timestamp":"1684438920.0","content":"Selected Answer: B\nWhen creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, A MINIMUM OF 1 MILLION ROWS PER DISTRIBUTION AND PARTITION IS NEEDED. Before partitions are created, dedicated SQL pool already divides each table into 60 distributions.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition","upvote_count":"2"},{"upvote_count":"1","timestamp":"1683861720.0","content":"Selected Answer: B\nCorrect Answer is B\nConsidering the high volume of data, for faster queries its recommended to create fewer partitions.","poster":"rocky48","comment_id":"895614"},{"upvote_count":"1","comment_id":"849252","timestamp":"1679657220.0","poster":"Hisayuki","content":"Selected Answer: A\n- 30 million rows arriving every month\n- A partition can contain of 60 million rows (60 distributions * 1 million / distribution)\n- The partition you prepared will be fulled by about two months.\n- So you should prepare a partition every two month at least. Instead, you have to choose \"A. once per month\". \n- After that, the oldest partition in \"1 Billion rows table\" will be switched to temporary partition in table which will be dropped soon for deletion.","comments":[{"poster":"Bear_Polar","comment_id":"867782","upvote_count":"2","timestamp":"1681261620.0","content":"you should prepare a partition every two month at least. Instead, you have to choose \"A. once per month\"?"}]},{"comment_id":"831551","poster":"esaade","timestamp":"1678163400.0","upvote_count":"1","content":"To optimize query performance and data loading, you should create partitions based on the Sales Date column. Since thirty million rows will be added to Table1 each month, it is recommended to create a partition once per month.\n\nCreating partitions based on Sales Date will improve query performance by reducing the amount of data scanned by the queries. When a query is executed, only the relevant partitions are scanned, which reduces the query execution time. Additionally, partitioning can improve data loading performance because it allows for parallel loading of data into the appropriate partitions.\n\nCreating partitions more frequently than once per month may not be necessary and could increase overhead. Creating partitions less frequently than once per month could result in larger partitions, which could impact query performance and data loading. Therefore, the correct answer is A. once per month."},{"timestamp":"1675608240.0","content":"Selected Answer: B\nAnswer B is correct","poster":"sb0517","upvote_count":"2","comment_id":"798904"},{"comment_id":"796778","poster":"[Removed]","upvote_count":"2","content":"Selected Answer: A\nYou should think about when you look up the range you want.\nThe annual unit is too large. So annual partitions are less useful.\nSince large amounts of data are added on a monthly basis, a monthly partition is in fact the most effective.","timestamp":"1675402800.0"},{"comment_id":"789424","timestamp":"1674808080.0","upvote_count":"2","content":"Selected Answer: B\nCorrect answer is B","poster":"astone42"},{"poster":"shakes103","content":"Selected Answer: A\nOnce per month","timestamp":"1674258300.0","upvote_count":"1","comment_id":"782842"},{"comment_id":"776068","poster":"nicky87654","content":"Selected Answer: A\nA. once per month","upvote_count":"1","timestamp":"1673743320.0"},{"timestamp":"1673360640.0","content":"Selected Answer: A\nPartitioning a table can improve query performance by allowing the query optimizer to limit the amount of data it needs to scan. When partitioning a table based on a column like Sales Date, it's important to consider the data loading pattern and the types of queries that will be run against the table.\n\nIn this case, since 30 million rows are added to the table each month, it would likely be beneficial to partition the table on a monthly basis. This would ensure that new data is being added to the appropriate partition and that queries that filter on a specific month can be optimized.\n\nOption A - Once per month would be the optimal option here, which will optimize query performance and data loading.","poster":"Sima_al","upvote_count":"3","comment_id":"771507"},{"comment_id":"767644","upvote_count":"1","comments":[{"upvote_count":"3","content":"It is\nOptimal number of rows is 1 million.\nso records/partitions*60 would leave us with 500,00 per partitions, which is less than minimum value of 1 million, so only a year would meet the criteria here.","timestamp":"1674668640.0","poster":"Lestrang","comment_id":"787962"}],"timestamp":"1673008020.0","poster":"psicktrick","content":"why is the solution not considering the 60 distributions that are already created due to hashing?"},{"comment_id":"743359","upvote_count":"2","content":"Selected Answer: C\n30million/30days=1million, why not C???","poster":"youngbug","timestamp":"1670881920.0"}],"question_text":"You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1. Table1 contains the following:\n✑ One billion rows\n✑ A clustered columnstore index\n✑ A hash-distributed column named Product Key\n✑ A column named Sales Date that is of the date data type and cannot be null\nThirty million rows will be added to Table1 each month.\nYou need to partition Table1 based on the Sales Date column. The solution must optimize query performance and data loading.\nHow often should you create a partition?","answer_description":"","answer_ET":"A","unix_timestamp":1662428580,"question_images":[],"answers_community":["A (53%)","B (43%)","4%"],"isMC":true},{"id":"xIWLKvnCvw4izZhxZ7yN","question_text":"You have an Azure Databricks workspace that contains a Delta Lake dimension table named Table1.\nTable1 is a Type 2 slowly changing dimension (SCD) table.\nYou need to apply updates from a source table to Table1.\nWhich Apache Spark SQL operation should you use?","choices":{"C":"ALTER","A":"CREATE","D":"MERGE","B":"UPDATE"},"question_images":[],"answers_community":["D (100%)"],"topic":"1","question_id":78,"timestamp":"2022-09-06 03:40:00","discussion":[{"poster":"labriji","upvote_count":"13","content":"Selected Answer: D\nWhen applying updates to a Type 2 slowly changing dimension (SCD) table in Azure Databricks, the best option is to use the MERGE operation in Apache Spark SQL. This operation allows you to combine the data from the source table with the data in the destination table, and then update or insert the appropriate r\necords. The MERGE operation provides a powerful and flexible way to handle updates for SCD tables, as it can handle both updates and inserts in a single operation. Additionally, this operation can be performed on Delta Lake tables, which can easily handle the ACID transactions needed for handling SCD updates.","comment_id":"772559","timestamp":"1673443440.0"},{"poster":"Naman1605","upvote_count":"9","comment_id":"670966","content":"Selected Answer: D\ncorrect D","timestamp":"1663346400.0"},{"upvote_count":"1","timestamp":"1732854420.0","poster":"EmnCours","content":"Selected Answer: D\nCorrect Answer: D","comment_id":"1319535"},{"timestamp":"1712887980.0","content":"Type 2 SCD means you have a new row to insert for the changes. The only MERGE supports this operation among the possible answers","upvote_count":"1","comment_id":"1194090","poster":"dgerok"},{"upvote_count":"3","timestamp":"1707435360.0","poster":"rocky48","content":"Selected Answer: D\nMERGE Operation:\nThe MERGE operation in Apache Spark SQL (and specifically in Delta Lake) combines the capabilities of both INSERT and UPDATE. It allows you to upsert data (insert new records or update existing records) based on a specified condition. When you have a slowly changing dimension (SCD) table like Table1, where historical data needs to be maintained. \nTherefore, the correct answer is D. MERGE","comment_id":"1145103"},{"comment_id":"998352","timestamp":"1693815000.0","poster":"kkk5566","upvote_count":"4","content":"Selected Answer: D\nTo update or upsert records in a delta lake in Databricks use the \"Merge\" command."},{"upvote_count":"4","poster":"ajhak","comment_id":"906184","content":"Selected Answer: D\nTo update or upsert records in a delta lake in Databricks use the \"Merge\" command.\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/merge","timestamp":"1684968780.0"},{"comment_id":"660677","content":"Correct","poster":"anks84","timestamp":"1662428400.0","upvote_count":"3"}],"answer_images":[],"answer_description":"","exam_id":67,"isMC":true,"unix_timestamp":1662428400,"answer_ET":"D","url":"https://www.examtopics.com/discussions/microsoft/view/80467-exam-dp-203-topic-1-question-63-discussion/","answer":"D"},{"id":"cN9tQJqoeF99tCLdbcJN","answer_images":[],"discussion":[{"comment_id":"901476","content":"Selected Answer: D\nParquet has columnar format, best for reading a few columns. Also when in doubt, just select Parquet. More often than not, that is gonna be the correct answer if you don't know it.","upvote_count":"13","timestamp":"1684439820.0","poster":"ajhak"},{"poster":"anks84","upvote_count":"7","timestamp":"1662428640.0","content":"Correct as the requirement is Column based.","comment_id":"660683"},{"content":"Selected Answer: D\nCorrect Answer: D","comment_id":"1319536","upvote_count":"1","poster":"EmnCours","timestamp":"1732854540.0"},{"timestamp":"1731629520.0","comment_id":"1312351","content":"The answer is \"D\"","poster":"Piantoni","upvote_count":"1"},{"poster":"kkk5566","timestamp":"1693815000.0","upvote_count":"2","content":"Selected Answer: D\nD is correct","comment_id":"998353"},{"poster":"Reloadedvn","upvote_count":"2","content":"Selected Answer: D\nParquet format satisfies all reqs","timestamp":"1684032780.0","comment_id":"897230"},{"poster":"GodfreyMbizo","upvote_count":"3","content":"Parquet","comment_id":"793129","timestamp":"1675100760.0"},{"poster":"youngbug","timestamp":"1670882580.0","content":"Selected Answer: D\nAvro is totally a serialization format. It combines of JSON and Raw binary files. Why is the explanation like this? It's misleading.","comment_id":"743368","upvote_count":"4"}],"unix_timestamp":1662428640,"exam_id":67,"answer_ET":"D","question_id":79,"isMC":true,"question_images":[],"timestamp":"2022-09-06 03:44:00","question_text":"You are designing an Azure Data Lake Storage solution that will transform raw JSON files for use in an analytical workload.\nYou need to recommend a format for the transformed files. The solution must meet the following requirements:\n✑ Contain information about the data types of each column in the files.\n✑ Support querying a subset of columns in the files.\n✑ Support read-heavy analytical workloads.\n✑ Minimize the file size.\nWhat should you recommend?","choices":{"D":"Apache Parquet","B":"CSV","C":"Apache Avro","A":"JSON"},"url":"https://www.examtopics.com/discussions/microsoft/view/80471-exam-dp-203-topic-1-question-64-discussion/","answer_description":"","topic":"1","answers_community":["D (100%)"],"answer":"D"},{"id":"sI5LAMXbfk3jT1OXDpYh","topic":"1","answer_description":"","isMC":true,"answer":"A","url":"https://www.examtopics.com/discussions/microsoft/view/79463-exam-dp-203-topic-1-question-65-discussion/","discussion":[{"comments":[{"content":"More than 1 solution might be right. The question here is: if row size is reduced to 1MB, will loading go faster? The answer then is yes: whether compression is better or not, is not relevant.","poster":"semauni","timestamp":"1690457340.0","comment_id":"964651","upvote_count":"5"},{"upvote_count":"4","comment_id":"659252","timestamp":"1662293640.0","content":"Exactly\ncompress because a lot of row have more than 1MB length","poster":"dom271219"},{"upvote_count":"5","timestamp":"1683544560.0","comment_id":"892058","poster":"kim32","content":"The question before was more than that 1 MB but here is less than 1 MB. since, it is less, then answer is Yes."}],"comment_id":"657549","upvote_count":"34","poster":"Tj87","timestamp":"1662132240.0","content":"I think we had this question in the previous pages and the correct answer was set as \" compress the files\""},{"poster":"Phund","timestamp":"1662436380.0","content":"Selected Answer: A\n\"ensure that each row is less than 1 MB\" and the condition for polybase is <1M, whatever method you used","upvote_count":"16","comment_id":"660773"},{"comment_id":"1400899","content":"Selected Answer: B\ncorrect answer is B, it's the same question 24 topic 1","poster":"ecalvo","upvote_count":"1","timestamp":"1742441340.0"},{"content":"Selected Answer: B\nAnswer: B. No\nWhile ensuring that each row is less than 1 MB may help in some cases, it does not fully address the performance challenges of copying data efficiently into Azure Synapse Analytics.","timestamp":"1742401380.0","poster":"imatheushenrique","comment_id":"1400585","upvote_count":"1"},{"upvote_count":"1","timestamp":"1735407000.0","poster":"moize","content":"Selected Answer: B\nRéduire la taille des lignes à moins de 1 Mo ne garantit pas nécessairement une copie plus rapide des données vers Azure Synapse Analytics. Il serait plus efficace d'utiliser des techniques d'optimisation de transfert de données, comme la compression des fichiers ou l'utilisation de formats de fichiers optimisés pour les performances, tels que Parquet ou ORC.","comment_id":"1333065"},{"timestamp":"1712888760.0","comment_id":"1194094","upvote_count":"4","content":"Selected Answer: B\nIf you modify the files to ensure that each row is less than 1 MB, you might end up truncating or losing data from those rows.\nTo achieve faster data copying, consider alternative approaches such as:\nCompression: Compress the files before transferring them to Azure Synapse Analytics. This can reduce the overall size of the data and improve transfer speed.\nParallelization: Split the data into smaller chunks and copy them in parallel to take advantage of multiple resources.\nOptimized Data Types: Ensure that numerical values are stored using appropriate data types (e.g., integers, floats) to minimize storage space.\nBatch Processing: Process the data in batches rather than row by row to optimize data transfer.\nThe question should be read carefully and attentively.\nYou need to prepare the files to ensure that the data copies QUICKLY.\nNobody asks if this improve...\nI agree with rocky48. You need do more to copy the data QUICKLY. \nSo, the answer is B (NO)","comments":[{"content":"I am with this answer. It should never be a good practise modifying the data at loading time.","timestamp":"1719495960.0","comment_id":"1238196","poster":"stornati","upvote_count":"2"}],"poster":"dgerok"},{"comment_id":"1151784","timestamp":"1708066440.0","upvote_count":"1","poster":"ItsAB","content":"Selected Answer: A\nGiven the large size of the table, I will utilize PolyBase for data transfer. Additionally, considering PolyBase's constraint that it cannot load rows exceeding 1MB in size, I will compress rows to ensure compliance with this requirement, thereby making PolyBase the optimal choice for data transfer. => Option A: yes"},{"poster":"stickslinger","upvote_count":"1","content":"Shouldn't this be \"B: No\". Where does the question ask about Polybase? Modifying the rows of data could affect the integrity of the data","timestamp":"1707476880.0","comment_id":"1145396"},{"comment_id":"1145106","poster":"rocky48","upvote_count":"3","content":"Selected Answer: B\nNo, this solution does not meet the goal. While modifying the files to ensure that each row is less than 1 MB might help with individual row sizes, it won’t necessarily improve the overall data transfer speed. The total size of the files in the storage account is still 100 GB, and copying large volumes of data can be time-consuming regardless of individual row sizes. To optimize data transfer speed, consider other strategies such as parallelizing the data transfer, optimizing network bandwidth, or using appropriate data loading techniques in Azure Synapse Analytics.","timestamp":"1707435720.0"},{"content":"No, this solution does not meet the goal. While modifying the files to ensure that each row is less than 1 MB might help with individual row sizes, it won’t necessarily improve the overall data transfer speed. The total size of the files in the storage account is still 100 GB, and copying large volumes of data can be time-consuming regardless of individual row sizes. To optimize data transfer speed, consider other strategies such as parallelizing the data transfer, optimizing network bandwidth, or using appropriate data loading techniques in Azure Synapse Analytics.","poster":"rocky48","timestamp":"1707435660.0","upvote_count":"2","comment_id":"1145105"},{"poster":"Charley92","upvote_count":"3","comment_id":"1133834","timestamp":"1706417820.0","content":"Selected Answer: B\nNo, this solution does not meet the goal. The files contain rows of text and numerical values, and 75% of the rows contain description data that has an average length of 1.1 MB. If you modify the files to ensure that each row is less than 1 MB, you may end up splitting the description data into multiple rows, which could affect the integrity of the data"},{"content":"Selected Answer: A\n**Variations**\nSolution: You convert the files to compressed delimited text files.\nDoes this meet the goal? **YES**\nSolution: You copy the files to a table that has a columnstore index.\nDoes this meet the goal? **NO**\nSolution: You modify the files to ensure that each row is more than 1 MB.\nDoes this meet the goal? **NO**\nSolution: You modify the files to ensure that each row is less than 1 MB.\nDoes this meet the goal? **YES**","upvote_count":"14","timestamp":"1704415080.0","poster":"ExamDestroyer69","comment_id":"1114144"},{"timestamp":"1700457660.0","upvote_count":"1","poster":"hcq31818","comment_id":"1075163","content":"Selected Answer: A\nPolyBase enables Azure Synapse Analytics to import and export data from Azure Data Lake Store, and from Azure Blob Storage. And it supports row sizes up to 1MB.\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver16#:~:text=Azure%20integration,and%20from%20Azure%20Blob%20Storage.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver16"},{"content":"Selected Answer: A\nA is correct","upvote_count":"1","timestamp":"1693815360.0","poster":"kkk5566","comment_id":"998362"},{"poster":"auwia","content":"Selected Answer: A\nYes, with less of 1Mb file we increase performance.","upvote_count":"2","comment_id":"936211","timestamp":"1687932180.0"},{"upvote_count":"2","poster":"e5019c6","comment_id":"934016","timestamp":"1687751580.0","content":"i thought that polybase just query the tables and dont do any process of ETL or ELT."},{"timestamp":"1683862020.0","upvote_count":"4","poster":"rocky48","comment_id":"895617","content":"Selected Answer: A\n\"You modify the files to ensure that each row is more than 1 MB\" and the answer was \"No\". This particular question asks if \"You modify the files to ensure that each row is less than 1 MB\", and the answer given is \"Yes\"."},{"comment_id":"831559","content":"No, modifying the files to ensure that each row is less than 1 MB does not necessarily meet the goal of ensuring that the data copies quickly.\n\nWhile it is true that large row sizes can impact data copy performance, simply reducing the row size to less than 1 MB may not be enough to optimize the data copy process. The performance of the data copy process can also be affected by factors such as network bandwidth, database design, and the method used to copy the data.\n\nTo ensure that the data copies quickly, you could consider other techniques such as compressing the data, using parallel data copy processes, and optimizing the database schema for efficient data loading.\n\nTherefore, the correct answer is B. No.","upvote_count":"2","poster":"esaade","timestamp":"1678163880.0"},{"comment_id":"793643","content":"B. No\n\nModifying the files to ensure that each row is less than 1 MB may not be enough to ensure that the data copies quickly to Azure Synapse Analytics. Other factors such as network bandwidth, data compression, and parallel processing of data can also impact the speed of data transfer. To optimize data transfer, it may be necessary to implement data compression techniques, increase network bandwidth, or parallelize the data transfer process.","upvote_count":"2","poster":"akk_1289","timestamp":"1675139400.0"},{"timestamp":"1674824760.0","content":"Selected Answer: B\nQuestion is very not clear.","upvote_count":"2","comment_id":"789612","poster":"panda_azzurro"},{"poster":"astone42","timestamp":"1674808260.0","content":"Selected Answer: A\nIt's A. The polybase has as condition <1M. I can't understand why people are confused.\nIt's extremely straightforward.","comment_id":"789429","upvote_count":"2"},{"timestamp":"1673388960.0","comment_id":"771849","content":"Selected Answer: B\nAnswer should be No","poster":"sreekan2","upvote_count":"1"},{"timestamp":"1673361540.0","poster":"Sima_al","comment_id":"771525","upvote_count":"2","content":"Selected Answer: B\nEnsuring that each row is less than 1 MB will not necessarily ensure that the data copies quickly. While it is true that reducing the size of each row can help to improve the speed at which data is copied, there are other factors that can affect the performance of the data transfer. Some other things to consider would be the number of concurrent connections allowed, the network bandwidth and latency, and the file format used."},{"timestamp":"1671615720.0","comment_id":"752087","upvote_count":"3","content":"https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/data-factory/v1/data-factory-azure-sql-data-warehouse-connector.md\nIf you have source data with rows of size greater than 1 MB, you may want to split the source tables vertically into several small ones where the largest row size of each of them does not exceed the limit. The smaller tables can then be loaded using PolyBase and merged together in Azure Synapse Analytics.\nAnswer = Y (Modify the file to ensure that each row is less than 1 MB)","poster":"VivekMadas"},{"comment_id":"739962","upvote_count":"1","poster":"temacc","timestamp":"1670578740.0","content":"1MB restriction is suitable for Copy command?\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#azure-sql-data-warehouse-as-sink"},{"timestamp":"1670496540.0","content":"Selected Answer: B\nshould be B","comment_id":"738911","poster":"Jay_98_11","upvote_count":"2"},{"poster":"urielramoss","comment_id":"725716","upvote_count":"2","content":"Selected Answer: B\nIts B!!!!!","timestamp":"1669283760.0"},{"comment_id":"724635","upvote_count":"3","comments":[{"content":"Here the answer is Right and it is A.\nPolyBase can't load rows that have more than 1,000,000(1MB) bytes of data. When you put data into the text files in Azure Blob storage or Azure Data Lake Store, they must have fewer than 1,000,000 bytes of data.","poster":"AJ077","comment_id":"727501","upvote_count":"2","timestamp":"1669464780.0"}],"timestamp":"1669147920.0","poster":"SomethingRight100","content":"I found question 24 (topic 1) in previous page and the question was \"You modify the files to ensure that each row is more than 1 MB\" and the answer was \"No\". This particular question asks if \"You modify the files to ensure that each row is less than 1 MB\", and the answer given is \"Yes\". One way you modify the files to make it smaller is to \"convert the files to compressed delimited text files\""},{"upvote_count":"1","comment_id":"723832","timestamp":"1669056300.0","poster":"Iamthealpha","content":"The answer should be B using compress delimited text"},{"upvote_count":"3","timestamp":"1667368020.0","poster":"Einsatz","content":"Had same question on previous page and correct answer was marked as Compress Files. Here it is marked different. Different answer on different pages for same question.","comment_id":"709634"},{"upvote_count":"2","timestamp":"1662428820.0","content":"@Tj87. : thats correct !\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data","poster":"anks84","comment_id":"660690"}],"answers_community":["A (65%)","B (35%)"],"timestamp":"2022-09-02 17:24:00","question_id":80,"answer_ET":"A","question_images":[],"answer_images":[],"choices":{"A":"Yes","B":"No"},"exam_id":67,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\nYou need to prepare the files to ensure that the data copies quickly.\nSolution: You modify the files to ensure that each row is less than 1 MB.\nDoes this meet the goal?","unix_timestamp":1662132240}],"exam":{"lastUpdated":"12 Apr 2025","isImplemented":true,"provider":"Microsoft","name":"DP-203","id":67,"isBeta":false,"isMCOnly":false,"numberOfQuestions":384},"currentPage":16},"__N_SSP":true}