{"pageProps":{"questions":[{"id":"eBmPADdtpwoxecU0IAD6","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0020400001.jpg"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0020500001.jpg"],"question_id":111,"timestamp":"2021-05-23 17:24:00","answers_community":[],"question_text":"HOTSPOT -\nYou have an Azure event hub named retailhub that has 16 partitions. Transactions are posted to retailhub. Each transaction includes the transaction ID, the individual line items, and the payment details. The transaction ID is used as the partition key.\nYou are designing an Azure Stream Analytics job to identify potentially fraudulent transactions at a retail store. The job will use retailhub as the input. The job will output the transaction ID, the individual line items, the payment details, a fraud score, and a fraud indicator.\nYou plan to send the output to an Azure event hub named fraudhub.\nYou need to ensure that the fraud detection solution is highly scalable and processes transactions as quickly as possible.\nHow should you structure the output of the Stream Analytics job? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"upvote_count":"9","poster":"IAMKPR","content":"Given answer and explanation are correct.","timestamp":"1621783440.0","comment_id":"364689"}],"answer_description":"Box 1: 16 -\nFor Event Hubs you need to set the partition key explicitly.\nAn embarrassingly parallel job is the most scalable scenario in Azure Stream Analytics. It connects one partition of the input to one instance of the query to one partition of the output.\n\nBox 2: Transaction ID -\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions","isMC":false,"topic":"2","exam_id":66,"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/53426-exam-dp-201-topic-2-question-48-discussion/","unix_timestamp":1621783440,"answer_ET":""},{"id":"6fNTIBIaEsD0W9z36AhU","answer":"","isMC":false,"unix_timestamp":1617715200,"url":"https://www.examtopics.com/discussions/microsoft/view/49409-exam-dp-201-topic-2-question-49-discussion/","answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0020600001.png"],"question_text":"DRAG DROP -\nYou have a CSV file in Azure Blob storage. The file does NOT have a header row.\nYou need to use Azure Data Factory to copy the file to an Azure SQL database. The solution must minimize how long it takes to copy the file.\nHow should you configure the copy process? To answer, drag the appropriate components to the correct locations. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","question_id":112,"topic":"2","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0020700001.png"],"exam_id":66,"discussion":[{"upvote_count":"62","comment_id":"329652","timestamp":"1617715200.0","poster":"suman13","content":"2n box: copy activity\n3rd box: azure sqldb dataset"},{"poster":"AngelRio","upvote_count":"20","content":"Input: A delimited text dataset...\nOutput: Azure SQL DB dataset...\nPipeline: Copy Activity ....","comment_id":"372118","timestamp":"1622571840.0"},{"content":"I have performed this activity during my course. I am 200% sure that second one is copy activity an third one is sql db with fixed schema","timestamp":"1622645220.0","upvote_count":"9","poster":"BitchNigga","comment_id":"372796"},{"upvote_count":"1","comment_id":"366852","poster":"cadio30","timestamp":"1622008560.0","content":"First layer corresponds to linked services while the second layer is for the dataset and lastly the pipeline level. Therefore second layer is CSV and Azure SQL Database then copy activity."},{"timestamp":"1617972780.0","upvote_count":"7","content":"This is completely wrong. Both middle boxes are an abstraction layer, so if left box is a dataset of input, then right box is a dataset for output. There is no requirement to transform the data, only copy, so pipeline consists only of copy activity. If we were using data flow to copy, then we would not need copy activity, because data flow could copy directly to sql database, but requirement is performance and data flow need to start up the cluster that it is run at, while copy activity works instantly.","poster":"maciejt","comment_id":"331948"}],"answers_community":[],"timestamp":"2021-04-06 15:20:00","answer_description":"Input: A delimited text dataset that has a comma a column delimiter columnDelimiter: The character(s) used to separate columns in a file.\nThe default value is comma ,. When the column delimiter is defined as empty string, which means no delimiter, the whole line is taken as a single column.\nPipeline: A data flow activity that has a general purpose compute type\nWhen you're transforming data in mapping data flows, you can read and write files from Azure Blob storage.\nOutput: A copy activity that has an explicit schema mapping\nUse Copy Activity in Azure Data Factory to copy data from and to Azure SQL Database, and use Data Flow to transform data in Azure SQL Database.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/format-delimited-text https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database"},{"id":"NW6Ja9ec6W3p9CVsTW8B","exam_id":66,"question_images":[],"answer":"D","isMC":true,"answer_ET":"D","answer_images":[],"discussion":[{"upvote_count":"14","poster":"bdloko","content":"Not C: Solution must minimize development effort.\nD: Alerts in Azure Monitor can identify important information in your Log Analytics repository.","timestamp":"1617441660.0","comment_id":"327257"},{"comment_id":"365404","content":"Appropriate answer is azure log analytics as the it can be configured easily by sending the logs into the component","timestamp":"1621843200.0","poster":"cadio30","upvote_count":"2"},{"upvote_count":"3","timestamp":"1617694860.0","comments":[{"content":"Sorry. D is correct. Bdloko is right.","timestamp":"1617696060.0","comment_id":"329428","upvote_count":"3","poster":"maynard13x8"}],"poster":"maynard13x8","comment_id":"329417","content":"Destination should be Azure storage because itâ€™s the place where the logs are saved when you enable auditing."}],"question_text":"You are designing an audit strategy for an Azure SQL Database environment.\nYou need to recommend a solution to provide real-time notifications for potential security breaches. The solution must minimize development effort.\nWhich destination should you include in the recommendation?","answers_community":[],"answer_description":"Auditing for Azure SQL Database and SQL Data Warehouse tracks database events and writes them to an audit log in your Azure storage account, Log Analytics workspace or Event Hubs.\nAlerts in Azure Monitor can identify important information in your Log Analytics repository. They are created by alert rules that automatically run log searches at regular intervals, and if results of the log search match particular criteria, then an alert record is created and it can be configured to perform an automated response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-auditing https://docs.microsoft.com/en-us/azure/azure-monitor/learn/tutorial-response","timestamp":"2021-04-03 11:21:00","unix_timestamp":1617441660,"choices":{"A":"Azure Blob storage","D":"Azure Log Analytics","C":"Azure Event Hubs","B":"Azure Synapse Analytics"},"url":"https://www.examtopics.com/discussions/microsoft/view/48910-exam-dp-201-topic-2-question-5-discussion/","question_id":113,"topic":"2"},{"id":"f20cbCpmyLmEkrICXuqx","exam_id":66,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0020800001.png","https://www.examtopics.com/assets/media/exam-media/03774/0020900001.jpg"],"answer_ET":"","answer_description":"Layer 2: Azure Data Lake Storage Gen2\nLayer 3: Azure Synapse Analytics\nAzure Synapse Analytics can be used for batch processing.\nNote: Layer 1 = speed layer, layer 2 = batch layer, layer 3 = serving layer\nNote 2: Lambda architectures use batch-processing, stream-processing, and a serving layer to minimize the latency involved in querying big data.\n\nReference:\nhttps://azure.microsoft.com/en-us/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/ https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing","answers_community":[],"answer":"","discussion":[{"upvote_count":"8","comment_id":"318994","content":"Correct answer","comments":[{"timestamp":"1622008620.0","upvote_count":"2","poster":"cadio30","content":"Entirely correct","comment_id":"366855"}],"poster":"Anagarika","timestamp":"1616578800.0"},{"content":"Layer 2 : Azure data lake GEN2\nLayer 3: ASA","timestamp":"1623372360.0","comment_id":"379360","comments":[{"content":"Sorry ASA : Azure Synapse Analytics","poster":"Srinivasnaveen","comment_id":"379361","upvote_count":"1","timestamp":"1623372480.0"}],"poster":"Srinivasnaveen","upvote_count":"2"}],"topic":"2","timestamp":"2021-03-24 10:40:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0020900002.jpg","https://www.examtopics.com/assets/media/exam-media/03774/0021000001.jpg"],"question_id":114,"unix_timestamp":1616578800,"url":"https://www.examtopics.com/discussions/microsoft/view/48070-exam-dp-201-topic-2-question-50-discussion/","question_text":"DRAG DROP -\nYou are planning a design pattern based on the Lambda architecture as shown in the exhibit.\n//IMG//\n\nWhich Azure services should you use for the cold path? To answer, drag the appropriate services to the correct layers. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","isMC":false},{"id":"Ih7gS91T4lEEj3oy59yz","answer_images":[],"choices":{"A":"an Azure Cosmos DB stored procedure executed by an Azure logic app","B":"an Azure Cosmos DB REST API Delete Document operation called by an Azure function","C":"Time To Live (TTL) setting in Azure Cosmos DB","D":"an Azure Cosmos DB change feed queried by an Azure function"},"answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/time-to-live","question_images":[],"question_id":115,"question_text":"You need to recommend an Azure Cosmos DB solution that meets the following requirements:\nâœ‘ All data that was NOT modified during the last 30 days must be purged automatically.\nâœ‘ The solution must NOT affect ongoing user requests.\nWhat should you recommend using to purge the data?","answer":"C","unix_timestamp":1621755780,"timestamp":"2021-05-23 09:43:00","discussion":[{"comment_id":"364257","poster":"memo43","content":"answer is CORRECT","timestamp":"1621755780.0","upvote_count":"7"}],"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/53387-exam-dp-201-topic-2-question-51-discussion/","topic":"2","exam_id":66,"isMC":true,"answer_ET":"C"}],"exam":{"name":"DP-201","id":66,"numberOfQuestions":206,"lastUpdated":"12 Apr 2025","isMCOnly":false,"isBeta":false,"provider":"Microsoft","isImplemented":true},"currentPage":23},"__N_SSP":true}