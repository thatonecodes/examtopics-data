{"pageProps":{"questions":[{"id":"mLl4dfnitVfYfqfEJ2CA","answer_description":"","discussion":[{"poster":"Nefirs","comments":[{"timestamp":"1729797840.0","content":"Data flow being \"more like a whole\" doesn't related to Power Query m. Instead, the Copy Data activity, when added within the pipeline, will allow access to Power query m.","poster":"d47320d","comment_id":"1201600","upvote_count":"1"}],"timestamp":"1729317180.0","comment_id":"1198345","content":"Selected Answer: A\nI think Dataflow since dataflow is like Power Query. But question is a bit unclear. Because data flow in itself is more like a whole, well, dataflow but not a singular activity like e.g. Copy Data.","upvote_count":"12"},{"upvote_count":"1","timestamp":"1739101380.0","content":"Selected Answer: A\nDataflow is Power Query. As we need to support M formulas from Power Query, dataflow is the answer","poster":"MYPE","comment_id":"1353880"},{"content":"Selected Answer: A\nYou can add dataflow to pipeline as activity: https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-dataflows-gen2-pipeline-activity","comment_id":"1227949","poster":"werka","upvote_count":"2","timestamp":"1733851320.0"},{"timestamp":"1733387520.0","poster":"vernillen","content":"Selected Answer: A\nAlthough you're just copying data, which would result in a 'Copy Data' activity... you would preferably go with a Dataflow in Fabric. Especially because the big take away is that the activity must support Power Query M","comment_id":"1224560","upvote_count":"2"},{"upvote_count":"1","comment_id":"1220422","poster":"282b85d","content":"Power Query M Support: Dataflows in Azure Data Factory and Synapse Analytics support the Power Query M formula language, enabling you to perform complex transformations and data manipulations as part of the data ingestion process.\nTransformations: Dataflows allow for a wide range of data transformation capabilities which are especially useful when working with CSV files to cleanse, aggregate, or reshape data before loading it into the destination.","timestamp":"1732818720.0"},{"content":"A. Dataflow use powerquery. i don't think you have powerquery with copy data","upvote_count":"2","poster":"2dc6125","timestamp":"1731387240.0","comment_id":"1210094"},{"poster":"Estratech","content":"The answer is Correct! To copy data from CSV files to Lakehouse1 in Workspace1, you should add a copy activity to Pipeline1. https://learn.microsoft.com/en-us/fabric/data-factory/connector-lakehouse-copy-activity","upvote_count":"3","comment_id":"1197512","timestamp":"1729200180.0"},{"content":"Selected Answer: D\nYou can specify transformations using Power Query M expressions during the data copy process. This ensures that your data is transformed according to your requirements before being loaded into Lakehouse1","comment_id":"1196423","timestamp":"1729062120.0","upvote_count":"1","poster":"emmanuelkech","comments":[{"comment_id":"1198346","content":"I don't think you can use Power Query M in a copy data process to transform data. Copy activity simply copies data from A to B.","upvote_count":"1","poster":"Nefirs","timestamp":"1729317240.0"}]}],"choices":{"A":"Dataflow","D":"Copy data","C":"Script","B":"Notebook"},"answer_ET":"A","answer":"A","question_images":[],"timestamp":"2024-04-16 09:02:00","answers_community":["A (94%)","6%"],"question_text":"You have a Fabric workspace named Workspace1 that contains a lakehouse named Lakehouse1.\n\nIn Workspace1, you create a data pipeline named Pipeline1.\n\nYou have CSV files stored in an Azure Storage account.\n\nYou need to add an activity to Pipeline1 that will copy data from the CSV files to Lakehouse1. The activity must support Power Query M formula language expressions.\n\nWhich type of activity should you add?","unix_timestamp":1713250920,"url":"https://www.examtopics.com/discussions/microsoft/view/138811-exam-dp-600-topic-1-question-73-discussion/","isMC":true,"answer_images":[],"exam_id":71,"topic":"1","question_id":151},{"id":"q2w11CJHxqj70Qb5g0zP","question_id":152,"answer_images":["https://img.examtopics.com/dp-600/image235.png"],"unix_timestamp":1712301720,"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/137950-exam-dp-600-topic-1-question-74-discussion/","exam_id":71,"isMC":false,"answer":"","answer_description":"","question_text":"HOTSPOT -\n\nYou have a Fabric tenant that contains lakehouse named Lakehouse1. Lakehouse1 contains a Delta table with eight columns.\n\nYou receive new data that contains the same eight columns and two additional columns.\n\nYou create a Spark DataFrame and assign the DataFrame to a variable named df. The DataFrame contains the new data.\n\nYou need to add the new data to the Delta table to meet the following requirements:\n\n• Keep all the existing rows.\n• Ensure that all the new data is added to the table.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","discussion":[{"poster":"SJSull","timestamp":"1728112920.0","content":"append, (\"mergeschema\", \"true\") - schema evolution, same columns and 2 added new columns","comment_id":"1189703","upvote_count":"49"},{"upvote_count":"20","timestamp":"1728978120.0","comment_id":"1195883","content":"- append\n- (\"mergeschema\", \"true\")\n\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/update-schema\n\nAdd columns with automatic schema update\nColumns that are present in the DataFrame but missing from the table are automatically added as part of a write transaction when:\n\nwrite or writeStream have .option(\"mergeSchema\", \"true\")\nspark.databricks.delta.schema.autoMerge.enabled is true","poster":"9878eb9"},{"comment_id":"1224562","poster":"vernillen","upvote_count":"5","timestamp":"1733387820.0","content":"1. Append\n--> Because of \"Keep all the existing rows\"\n\n2. mergeSchema, true\n--> Because there will be 2 additional columns, so it's they are not deleted from the new version. If they would have been deleted from the new version, you would have to use \"overwriteSchema, true\". Since that option replaces the existing schema with the schema of the new dataframe. mergeSchema, on the contrary, allows the addition of new columns rather than overwriting the schema."},{"content":"append\nmergeschema=\"true\"\n\nappend as all existing rows must be kept.\naccording to this: https://learn.microsoft.com/en-us/azure/databricks/delta/update-schema\noverwriteschema is for \"change a column’s type or name or drop a column\"\nmergeschema is for \"Columns that are present in the DataFrame but missing from the table are automatically added\"","upvote_count":"7","timestamp":"1729909800.0","comment_id":"1202364","poster":"4371883"}],"topic":"1","answers_community":[],"timestamp":"2024-04-05 09:22:00","question_images":["https://img.examtopics.com/dp-600/image96.png"]},{"id":"swz87CkTIpPiQlWSPQBa","question_text":"HOTSPOT\n-\n\nYou have a Fabric warehouse that contains a table named Sales.Orders. Sales.Orders contains the following columns.\n\n//IMG//\n\n\nYou need to write a T-SQL query that will return the following columns.\n\n//IMG//\n\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","exam_id":71,"unix_timestamp":1713506700,"question_id":153,"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/139147-exam-dp-600-topic-1-question-75-discussion/","answer_images":["https://img.examtopics.com/dp-600/image101.png"],"answers_community":[],"answer_ET":"","answer":"","question_images":["https://img.examtopics.com/dp-600/image98.png","https://img.examtopics.com/dp-600/image99.png","https://img.examtopics.com/dp-600/image100.png"],"discussion":[{"timestamp":"1713506700.0","upvote_count":"23","poster":"Nefirs","content":"IMHO answer is correct:\nhttps://learn.microsoft.com/de-de/sql/t-sql/language-elements/coalesce-transact-sql?view=sql-server-ver16\nhttps://learn.microsoft.com/de-de/sql/t-sql/functions/logical-functions-least-transact-sql?view=sql-server-ver16","comment_id":"1198353"},{"timestamp":"1738545240.0","comment_id":"1350711","poster":"goldy29","content":"COALESCE(Weight, Quantity, 1) will return the first non-null value among Weight, Quantity, or 1, it is the most appropriate choice.\nLEAST(ListPrice, SalePrice) finds the smaller value between ListPrice and SalePrice, it is the best choice for calculating OrderPrice.","upvote_count":"3"},{"upvote_count":"1","timestamp":"1716227640.0","comment_id":"1214534","poster":"72bd3bc","comments":[{"content":"Its simple, COALESCE returns first non-null value from the parameters passed, see there are three parameters used in the specific order which is asked in the question. MIN is grouping function which is not in the query so LEAST should be used","timestamp":"1718382420.0","comment_id":"1230562","poster":"dev2dev","upvote_count":"3"}],"content":"strange question. In the second box coalesce is not correct because it doesn't choose the smallest. Least also has problems - it doesn't ignore null:\nIf one or more arguments aren't NULL, then NULL arguments are ignored during comparison. If all arguments are NULL, then LEAST returns NULL.\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/logical-functions-least-transact-sql?view=sql-server-ver16\n\nMIN compares all the rows in one column and choose the smallest, so it is also the wrong choice. \n\nI feel confused."},{"timestamp":"1716177060.0","comment_id":"1214097","upvote_count":"3","content":"IMHO answer is correct: COALESCE, LEAST","poster":"serch_engine"},{"poster":"stilferx","content":"IMHO, Coalesce -> Least is winner.\n\nLeast because of that:\nIf one or more arguments aren't NULL, then NULL arguments are ignored during comparison. If all arguments are NULL, then LEAST returns NULL.\n\nLink: https://learn.microsoft.com/en-us/sql/t-sql/functions/logical-functions-least-transact-sql?view=sql-server-ver16","timestamp":"1715545800.0","upvote_count":"2","comment_id":"1210383"}],"topic":"1","answer_description":"","timestamp":"2024-04-19 08:05:00"},{"id":"8hLHo3uBFtFxpO45HqkR","unix_timestamp":1713514020,"choices":{"D":"right anti","B":"full outer","F":"left anti","E":"right outer","A":"inner","C":"left outer"},"question_text":"You have a Fabric tenant that contains a lakehouse.\n\nYou plan to use a visual query to merge two tables.\n\nYou need to ensure that the query returns all the rows in both tables.\n\nWhich type of join should you use?","answer_description":"","answer_ET":"B","discussion":[{"poster":"nappi1","comment_id":"1315785","content":"Selected Answer: B\nsince there is no other information on the tables, the answer is the academical one","timestamp":"1732188840.0","upvote_count":"1"},{"comment_id":"1224564","timestamp":"1717569600.0","poster":"vernillen","upvote_count":"1","content":"Selected Answer: B\nJust making it more obvious, but yea... all other options wouldn't return all the rows in both tables but exclude some."},{"content":"Selected Answer: B\nIMHO, 1st grage of SQL school - FULL OUTER JOIN.\n\nBecause - ALL rows, not from left, right, ... table(s)","poster":"stilferx","timestamp":"1715545860.0","comment_id":"1210384","upvote_count":"3"},{"upvote_count":"3","content":"B. Left or right will only return one table all rows and other table if exist in the other table. inner will only return if data match in both tables so Full (outer) join will return all data from both tables","timestamp":"1715482860.0","poster":"2dc6125","comment_id":"1210096"},{"content":"Selected Answer: B\nFULL (OUTER) JOIN\n\nhttps://www.w3schools.com/sql/sql_join.asp","comment_id":"1200973","poster":"VAzureD","upvote_count":"4","timestamp":"1713904260.0"},{"content":"Selected Answer: B\nfull outer keeps rows from both sides","comment_id":"1198423","upvote_count":"2","timestamp":"1713514020.0","poster":"Nefirs"}],"topic":"1","answer":"B","isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/139156-exam-dp-600-topic-1-question-76-discussion/","timestamp":"2024-04-19 10:07:00","answers_community":["B (100%)"],"exam_id":71,"question_id":154,"answer_images":[]},{"id":"9wR6VtpBvr0FZpQZ99BY","answer_ET":"D","unix_timestamp":1712338080,"url":"https://www.examtopics.com/discussions/microsoft/view/137987-exam-dp-600-topic-1-question-77-discussion/","question_id":155,"answer_description":"","choices":{"B":"Run the OPTIMIZE command and specify the Z-order parameter.","D":"Run the VACUUM command.","C":"Run the OPTIMIZE command and specify the V-order parameter.","A":"From OneLake file explorer, delete the files."},"question_images":[],"answer":"D","answer_images":[],"topic":"1","exam_id":71,"answers_community":["D (100%)"],"discussion":[{"poster":"Nefirs","content":"Selected Answer: D\nD: https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance","comment_id":"1198359","upvote_count":"10","timestamp":"1729318380.0"},{"upvote_count":"3","poster":"vernillen","comment_id":"1224565","timestamp":"1733388060.0","content":"Selected Answer: D\nJust think of the aspect of a real VACUUM: Cleanup of dust just lying around"},{"content":"Selected Answer: D\nIMHO, D) Vacuum is good","timestamp":"1731450780.0","upvote_count":"2","comment_id":"1210385","poster":"stilferx"},{"poster":"prabhjot","timestamp":"1728149280.0","comment_id":"1190011","upvote_count":"4","content":"Yes D - To remove files from a Delta table that are no longer referenced by the current version of the table, you should run the VACUUM command."}],"isMC":true,"timestamp":"2024-04-05 19:28:00","question_text":"You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table that has one million Parquet files.\n\nYou need to remove files that were NOT referenced by the table during the past 30 days. The solution must ensure that the transaction log remains consistent, and the ACID properties of the table are maintained.\n\nWhat should you do?"}],"exam":{"provider":"Microsoft","lastUpdated":"12 Apr 2025","numberOfQuestions":179,"isImplemented":true,"isBeta":false,"isMCOnly":false,"name":"DP-600","id":71},"currentPage":31},"__N_SSP":true}