{"pageProps":{"questions":[{"id":"FCngrpp9VIFjxJwSiKsK","unix_timestamp":1673727000,"answer_description":"","question_id":336,"answer":"D","answer_ET":"D","timestamp":"2023-01-14 21:10:00","answers_community":["D (100%)"],"exam_id":67,"answer_images":[],"isMC":true,"question_images":[],"choices":{"C":"Azure Data Lake Storage","D":"Azure Databricks","A":"Azure HDInsight","B":"Azure Data Factory"},"topic":"4","question_text":"A company plans to use Apache Spark analytics to analyze intrusion detection data.\n\nYou need to recommend a solution to analyze network and system activity data for malicious activities and policy violations. The solution must minimize administrative efforts.\n\nWhat should you recommend?","url":"https://www.examtopics.com/discussions/microsoft/view/95331-exam-dp-203-topic-4-question-40-discussion/","discussion":[{"upvote_count":"6","poster":"Mouli10","timestamp":"1689428340.0","content":"Selected Answer: D\nAzure databricks","comment_id":"776752"},{"upvote_count":"1","timestamp":"1728100320.0","comment_id":"1189644","poster":"AccountHatz","content":"Selected Answer: D\nI think it is D) Azure Databricks\nhttps://learn.microsoft.com/en-us/azure/databricks/security/privacy/enhanced-security-monitoring"},{"poster":"kkk5566","comment_id":"995670","upvote_count":"1","content":"Selected Answer: D\ncorrect","timestamp":"1709274660.0"},{"poster":"vctrhugo","timestamp":"1703202540.0","content":"Selected Answer: D\nBy leveraging Azure Databricks, you can easily perform advanced analytics on the intrusion detection data using Spark's powerful distributed processing capabilities. Databricks provides an interactive and collaborative environment where you can write Spark code, explore and visualize data, and build machine learning models. It also integrates with popular data sources, including Azure Data Lake Storage, for efficient data ingestion and processing.","comment_id":"929972","upvote_count":"3"},{"upvote_count":"3","poster":"Stefan94","content":"Correct","timestamp":"1689358200.0","comment_id":"775889"}]},{"id":"ecgVIjdHu2jvRBoniznI","answer_images":["https://img.examtopics.com/dp-203/image286.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/105226-exam-dp-203-topic-4-question-41-discussion/","unix_timestamp":1680648660,"question_images":["https://img.examtopics.com/dp-203/image285.png"],"exam_id":67,"question_id":337,"discussion":[{"comment_id":"935187","content":"The sys.dm_pdw_lock_waits view is specific to SQL Server and is used to monitor lock waits and lock resources in regular SQL Server environments, not in Azure Synapse Analytics dedicated SQL pools.\n\nMy answers are:\n1. sys.dm_pdw_exec_requests\n2. sys.dm_pdw_waits\nThere is a similar question in the microsoft official practice assessment and the explaination is the following:\nThe sys.dm_pdw_waits view holds information about all wait stats encountered during the execution of a request or query, including locks and waits on a transmission queue","upvote_count":"23","timestamp":"1687858140.0","poster":"auwia"},{"poster":"bp_a_user","comment_id":"884153","timestamp":"1682755920.0","content":"Its dm_pwd_waits:\nQueries in the Suspended state can be queued due to a large number of active running queries. These queries also appear in the sys.dm_pdw_waits waits query with a type of UserConcurrencyResourceTyp\nfrom the official learning path: https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/6-use-dynamic-management-views-to-identify-troubleshoot-query-performance","upvote_count":"8"},{"comment_id":"1246037","timestamp":"1720689900.0","upvote_count":"2","poster":"fahfouhi94","content":"explained here : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor\n1. sys.dm_pdw_exec_requests\n2. sys.dm_pdw_waits"},{"poster":"MBRSDG","comment_id":"1187912","timestamp":"1712043120.0","content":"1. sys.dm_pdw_sql_requests\nIt is more precise about the request made in the question. Please notice that exec_requests contains also all the informations from sql_requests, which is a subset of exec_requests. But we're only interested in SQL queries performances, so even sql_requests is OK. \n\n2. sys.dm_pdw_lock_waits\nwe're specifically referring to time spent in waiting a resource lock, so this is the correct table. It is supported by dedicated SQL pool, but not by serverless.","upvote_count":"1"},{"comment_id":"1123416","upvote_count":"2","content":"1. sys.dm_pdw_exec_requests\n\nThis DMV provides information about all active queries in the database, including their query ID, status, execution time, and resource utilization. You can use this DMV to identify long-running queries by filtering for queries that have been running for a long period of time.\n\n2. sys.dm_pdw_waits\n\nThis DMV provides information about the wait stats encountered by active queries. You can use this DMV to identify queries that are waiting for resources, such as CPU, memory, or I/O.","timestamp":"1705326540.0","poster":"Azure_2023"},{"poster":"matiandal","upvote_count":"1","comment_id":"1065748","timestamp":"1699459320.0","content":"provided answers are correct"},{"timestamp":"1693542780.0","content":"1. sys.dm_pdw_exec_requests\n2. sys.dm_pdw_waits","comment_id":"995674","poster":"kkk5566","upvote_count":"7"},{"content":"\"Queries in the Suspended state can be queued due to a large number of active running queries. These queries also appear in the sys.dm_pdw_waits waits query with a type of UserConcurrencyResourceType.\"","upvote_count":"2","comment_id":"929974","timestamp":"1687384260.0","poster":"vctrhugo"},{"comments":[{"poster":"AHUI","upvote_count":"21","timestamp":"1680649020.0","content":"box 1: is correct\nbox 2: sys.dm_pdw_waits\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-waits-transact-sql?view=aps-pdw-2016-au7","comment_id":"861570"}],"timestamp":"1680648660.0","comment_id":"861566","upvote_count":"3","content":"correct\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/monitoring-with-dmvs?view=azuresql","poster":"AHUI"}],"answers_community":[],"timestamp":"2023-04-05 00:51:00","question_text":"HOTSPOT\n-\n\nYou have an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to monitor the database for long-running queries and identify which queries are waiting on resources.\n\nWhich dynamic management view should you use for each requirement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct answer is worth one point.\n\n//IMG//","answer":"","isMC":false,"answer_ET":"","answer_description":"","topic":"4"},{"id":"xBEQa5oEyYRVIMHzxh3C","unix_timestamp":1684725480,"answer_ET":"A","answer":"A","choices":{"B":"Scale up the data flow runtime of the Azure integration runtime and scale out the self-hosted integration runtime.","C":"Scale up the data flow runtime of the Azure integration runtime.","A":"Scale out the self-hosted integration runtime."},"exam_id":67,"answer_images":[],"question_text":"You have an Azure Data Factory pipeline named pipeline1 that includes a Copy activity named Copy1. Copy1 has the following configurations:\n\n• The source of Copy1 is a table in an on-premises Microsoft SQL Server instance that is accessed by using a linked service connected via a self-hosted integration runtime.\n• The sink of Copy1 uses a table in an Azure SQL database that is accessed by using a linked service connected via an Azure integration runtime.\n\nYou need to maximize the amount of compute resources available to Copy1. The solution must minimize administrative effort.\n\nWhat should you do?","topic":"4","answer_description":"","question_images":[],"question_id":338,"timestamp":"2023-05-22 05:18:00","discussion":[{"poster":"BillMyl","timestamp":"1685073840.0","upvote_count":"18","content":"I would answer A.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime\n\nCopying between a cloud data source and a data source in a private network: if either the source or sink linked service points to a self-hosted IR, the copy activity is executed on the self-hosted IR.","comment_id":"907026"},{"content":"Why not B? \n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime\n\nAzure integration runtime provides the native compute to move data between cloud data stores in a secure, reliable, and high-performance manner. You can set how many data integration units to use on the copy activity, and the compute size of the Azure IR is elastically scaled up accordingly without requiring you to explicitly adjust the size of the Azure Integration Runtime.\n\nFor high availability and scalability, you can scale out the self-hosted IR by associating the logical instance with multiple on-premises machines in active-active mode.","comments":[{"upvote_count":"1","comment_id":"1344781","timestamp":"1737553620.0","poster":"sakis213","content":"We dont have mapping data flows here"}],"comment_id":"903682","poster":"azure_user11","upvote_count":"13","timestamp":"1684725480.0"},{"poster":"606a82e","upvote_count":"1","comment_id":"1261192","timestamp":"1722889020.0","content":"Selected Answer: C\nCorrect"},{"poster":"e56bb91","upvote_count":"3","timestamp":"1720176180.0","comment_id":"1242705","content":"To maximize the amount of compute resources available to the Copy activity (Copy1) in your Azure Data Factory pipeline while minimizing administrative effort, the most appropriate action would be:\n\nA. Scale out the self-hosted integration runtime.\n\nExplanation:\nSelf-Hosted Integration Runtime (SHIR): Since the source of Copy1 is an on-premises SQL Server, the data needs to be transferred using the self-hosted integration runtime. Scaling out the self-hosted integration runtime means adding more nodes to the integration runtime cluster, which increases the number of parallel connections and throughput for data transfer from the on-premises SQL Server to Azure.\n\nAzure Integration Runtime: The sink is an Azure SQL Database accessed via an Azure integration runtime. Azure integration runtime is managed by Azure, and it automatically scales based on the load. Therefore, you generally do not need to manually scale up the Azure integration runtime."},{"timestamp":"1719122700.0","upvote_count":"1","poster":"SarathChandra","comment_id":"1235705","content":"Selected Answer: A\nA is correct"},{"poster":"Bakhtiyor","comment_id":"1203230","content":"Selected Answer: B\nI chose B option","timestamp":"1714242240.0","upvote_count":"1"},{"upvote_count":"1","comments":[{"timestamp":"1714131300.0","poster":"Dusica","content":"besides it is a copy activity, there is now Data Flow","upvote_count":"2","comment_id":"1202561"}],"content":"A is the answer; as BillMyI quoted microsoft - copy is happening on self-hosted IR","timestamp":"1714131240.0","poster":"Dusica","comment_id":"1202559"},{"upvote_count":"1","content":"Selected Answer: C\nc is correct","poster":"f214eb2","timestamp":"1713410880.0","comment_id":"1197665"},{"upvote_count":"2","timestamp":"1711890360.0","poster":"MBRSDG","content":"Selected Answer: C\nThe question explicitly requires to minimize the administrative cost. Every time there's such a request, the solution must be as much automated as possible. Self-Hosted has to be managed by some IT department, so it implies effort ad then administrative costs. Instead, simply scaling up the data flow runtime does not require infrastructural costs, since Azure provides such a machine type in the cloud, so we don't have to handle our machine in the company. \n\nJust a detail: dataflows, even with a giant cluster, have very poor performances in practical situations. Just scaling up the IR could be not enough...","comment_id":"1186805"},{"content":"Selected Answer: C\nes, the answer is still the same. Scaling up the data flow runtime of the Azure integration runtime is still the best option to maximize the amount of compute resources available to Copy1.\n\nThe reasoning is that the copy activity is configured to use the self-hosted integration runtime because the source linked service is connected to a self-hosted integration runtime. This means that the copy activity will be executed on the self-hosted integration runtime, and scaling up the Azure integration runtime will have no effect on the copy activity's performance.\n\nScaling up the Azure integration runtime would only help if the source and sink linked services were both connected to the Azure integration runtime. In that case, scaling up the Azure integration runtime would provide more processing power for the copy activity.","comment_id":"1123425","upvote_count":"3","poster":"Azure_2023","timestamp":"1705327380.0"},{"timestamp":"1704800760.0","poster":"jongert","comment_id":"1117445","content":"Integration runtime is hosted on the location of the sink for copy activity if I am not mistaken.","upvote_count":"1"},{"poster":"dakku987","timestamp":"1704217500.0","upvote_count":"2","comment_id":"1112110","content":"Selected Answer: C\nchat gpt \nC. Scale up the data flow runtime of the Azure integration runtime.\n\nExplanation:\n\nIn Azure Data Factory, when you're copying data between different data stores, the compute resources used by the Copy activity are mainly determined by the data flow involved in the copying process. Azure Data Factory provides two types of integration runtimes:"},{"content":"Selected Answer: A\nChatgpt:\nThe self-hosted integration runtime can be scaled out by adding additional nodes, which allows it to process more activities simultaneously. This is a way to increase compute resources without a significant administrative overhead since it involves configuration changes rather than physical infrastructure changes.\n\nOptions B and C involve scaling up the data flow runtime, which is not applicable in this context since the Copy activity does not use data flow runtime; it uses the integration runtime for data movement. Therefore, the correct answer to maximize compute resources for Copy1 with minimal administrative effort is:\n\nA. Scale out the self-hosted integration runtime.","upvote_count":"2","timestamp":"1703525820.0","poster":"Momoanwar","comment_id":"1105435"},{"comment_id":"1077957","poster":"SATHTECH","upvote_count":"2","timestamp":"1700702220.0","content":"For maximizing the amount of compute resources available to the Copy activity in Azure Data Factory, you should consider scaling up the data flow runtime of the Azure integration runtime.\n\nOption C. Scale up the data flow runtime of the Azure integration runtime."},{"timestamp":"1693544580.0","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#self-hosted-ir-compute-resource-and-scaling\nA should be corrrect.","comment_id":"995698","poster":"kkk5566","comments":[{"content":"correct it , C ,Option A, “Scale out the self-hosted integration runtime,” is not the best solution to maximize the amount of compute resources available to Copy1 because it would not minimize administrative effort. Scaling out the self-hosted integration runtime would involve adding more nodes to the runtime pool, which would require allocating new virtual machines and registering new nodes on the integration runtime. This process can be time-consuming and would require additional administrative effort1.","poster":"kkk5566","timestamp":"1693544940.0","comment_id":"995703","upvote_count":"4"}],"upvote_count":"2"},{"comment_id":"982154","upvote_count":"3","timestamp":"1692157140.0","content":"Selected Answer: A\nif either the source or sink linked service points to a self-hosted IR, the copy activity is executed on the self-hosted IR.","poster":"[Removed]"},{"comment_id":"946507","upvote_count":"3","poster":"alegiordx","timestamp":"1688821980.0","content":"Selected Answer: A\nMy answer is A due to the precedence criteria among Integration runtimes selection when source and sink linked services are linked to different IRs, as described here \nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#determining-which-ir-to-use"},{"comment_id":"945883","timestamp":"1688749380.0","content":"Selected Answer: C\nThe important point is - \"The solution must minimize administrative effort.\"\n\nAzure integration runtime - Cloud-based service - Not as scalable as a SHIR\nSelf-hosted integration runtime - More scalable - Requires more administrative effort\n\nScaling up the Azure IR will give us more compute resources without increasing the administrative effort.","upvote_count":"2","poster":"tsmk"},{"content":"Selected Answer: A\nAccording to the MS document, A seems to be the correct answer.","comment_id":"941641","timestamp":"1688374500.0","poster":"andjurovicela","upvote_count":"3"},{"comment_id":"917240","comments":[{"upvote_count":"2","timestamp":"1687387260.0","content":"scaling out the self hosted will not increase the amount of compute resources becoz its already running on the physical machine (on-premises) . However, the Azure integration runtime is a managed service, so scaling up its data flow runtime will increase the amount of compute resources available to Copy1.","comment_id":"929982","poster":"JG1984"}],"upvote_count":"1","timestamp":"1686142680.0","content":"Selected Answer: B\nI would answer B","poster":"Azure_2023"}],"answers_community":["A (52%)","C (41%)","7%"],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/109869-exam-dp-203-topic-4-question-42-discussion/"},{"id":"eWrwP4Kc0iIP9ItZUeGG","topic":"4","question_images":[],"answer_description":"","timestamp":"2023-05-09 09:30:00","answer":"BD","answer_images":[],"question_id":339,"url":"https://www.examtopics.com/discussions/microsoft/view/108789-exam-dp-203-topic-4-question-43-discussion/","unix_timestamp":1683617400,"discussion":[{"timestamp":"1683617400.0","poster":"[Removed]","comment_id":"892892","upvote_count":"9","content":"Selected Answer: BD\nSeems correct:\nhttps://learn.microsoft.com/en-us/azure/databricks/optimizations/dynamic-file-pruning\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/data-skipping"},{"comment_id":"938574","upvote_count":"8","content":"Selected Answer: BD\nDynamic file pruning, can significantly improve the performance of many queries on Delta Lake tables. Dynamic file pruning is especially efficient for non-partitioned tables, or for joins on non-partitioned columns. The performance impact of dynamic file pruning is often correlated to the clustering of data so consider using Z-Ordering to maximize the benefit.","poster":"vctrhugo","timestamp":"1688073000.0"},{"content":"Selected Answer: BC\nBy sorting the data files based on one or more columns, Z-Ordering can significantly improve the performance of queries that filter on those columns.\nCaching in Apache Spark allows frequently accessed data to be stored in memory, reducing the time it takes to read the data for subsequent operations. This can be particularly useful for speeding up joins and repeated queries against non-partitioned tables, as the data is readily available without having to be read from disk repeatedly.","poster":"Aurangzaib","comment_id":"1267600","timestamp":"1723887540.0","upvote_count":"1"},{"poster":"MBRSDG","upvote_count":"1","comment_id":"1186797","timestamp":"1711889880.0","content":"https://www.databricks.com/blog/2020/04/30/faster-sql-queries-on-delta-lake-with-dynamic-file-pruning.html"},{"comment_id":"995692","content":"Selected Answer: BD\ncorrect","timestamp":"1693543980.0","poster":"kkk5566","upvote_count":"2"}],"isMC":true,"answer_ET":"BD","question_text":"You are designing a solution that will use tables in Delta Lake on Azure Databricks.\n\nYou need to minimize how long it takes to perform the following:\n\n• Queries against non-partitioned tables\n• Joins on non-partitioned columns\n\nWhich two options should you include in the solution? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.","answers_community":["BD (95%)","5%"],"exam_id":67,"choices":{"B":"Z-Ordering","D":"dynamic file pruning (DFP)","A":"the clone command","C":"Apache Spark caching"}},{"id":"xYWaGOXJo5a1lNoDsum2","answer":"C","timestamp":"2023-05-10 16:29:00","unix_timestamp":1683728940,"topic":"4","url":"https://www.examtopics.com/discussions/microsoft/view/108874-exam-dp-203-topic-4-question-44-discussion/","question_images":[],"question_text":"You have an Azure Data Lake Storage Gen2 account named account1 that contains a container named container1.\n\nYou plan to create lifecycle management policy rules for container1.\n\nYou need to ensure that you can create rules that will move blobs between access tiers based on when each blob was accessed last.\n\nWhat should you do first?","answer_description":"","isMC":true,"discussion":[{"poster":"cloud_lady","timestamp":"1699633740.0","comment_id":"894082","upvote_count":"9","content":"Selected Answer: C\nAnswer is correct.\nCustomers stores huge amount of data in Azure blob storage. Sometimes this data is accessed frequently and other times infrequently. Last access time tracking integrates with the lifecycle of Azure blob storage to allow automatic tiering and deletion of data based on when individual blobs are accessed last.","comments":[{"poster":"MBRSDG","upvote_count":"1","timestamp":"1727699820.0","comment_id":"1186777","content":"in addition --> https://azure.microsoft.com/en-us/updates/azure-blob-storage-last-access-time-tracking-now-generally-available/"}]},{"poster":"kkk5566","content":"Selected Answer: C\ncorrect","upvote_count":"1","timestamp":"1709277060.0","comment_id":"995705"}],"choices":{"B":"Create an Azure application","D":"Enable the hierarchical namespace","C":"Enable access time tracking","A":"Configure object replication"},"answer_images":[],"question_id":340,"answer_ET":"C","answers_community":["C (100%)"],"exam_id":67}],"exam":{"provider":"Microsoft","lastUpdated":"12 Apr 2025","name":"DP-203","id":67,"numberOfQuestions":384,"isMCOnly":false,"isImplemented":true,"isBeta":false},"currentPage":68},"__N_SSP":true}