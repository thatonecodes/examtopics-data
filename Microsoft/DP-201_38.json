{"pageProps":{"questions":[{"id":"AJgWgaHcbTDT8Dq3W80t","discussion":[{"poster":"m83x","timestamp":"1597192140.0","comments":[{"content":"I don't think so. That would be too easy.","comment_id":"183429","upvote_count":"1","timestamp":"1600662000.0","poster":"AJMorgan591"},{"comment_id":"251025","comments":[{"upvote_count":"2","content":"yes for that make changes to storage account(not to the app) cos that is the question about","comment_id":"392851","timestamp":"1624878720.0","poster":"tes"}],"content":"BCD should be the correct answer, no disagreement with other comments on B and C. I would go with D because during failover some data transactions can potentially be lost hence the best practice is to have a re-try logic. This is one of the requirements as well \"Minimize potential data loss\". E to me doesn't make sense since it has to do more with data lifecycle management than minimizing potential data loss.","timestamp":"1608735420.0","poster":"saponazureguy","upvote_count":"5"}],"upvote_count":"27","content":"The question is \"You need to recommend *changes to the storage account* to meet the following requirements:\", not changes to the app.. so it is BCE","comment_id":"155962"},{"poster":"M0e","comment_id":"207141","comments":[{"timestamp":"1608735300.0","upvote_count":"2","content":"I agree, BCD should be the correct answer, no disagreement with other comments on B and C. I would go with D because as MOe pointed out, during failover some requests can potentially be lost hence the best practice is to have a re-try logic. This is one of the requirements as well \"Minimize potential data loss\". E to me doesnt make sense since it has to do more with data lifecycle management than minimizing potential data loss.","comments":[],"comment_id":"251024","poster":"saponazureguy"}],"timestamp":"1603813920.0","upvote_count":"17","content":"Am I the only one who says the correct answer is B, C, D?"},{"comment_id":"427180","poster":"lgtiza","upvote_count":"1","timestamp":"1629342600.0","content":"BCE is correct and not BCD. At first I also thought option E didn't make any sense, but it's not the \"Lifecycle Management\" feature (intended to move to cool or archive and then delete), this Time-Retention Policy is an access policy set up at container level to do exactly the opposite, to avoid deletes by setting up a time-retention period (and you can even lock those policies to comply with legal regulations). Definitely BCE."},{"timestamp":"1615849800.0","poster":"H_S","comment_id":"311834","content":"Answer: BCE correct agree","upvote_count":"2"},{"poster":"syu31svc","timestamp":"1607440560.0","upvote_count":"8","content":"Answer is correct\nRA-GRS for high availability\nsoft delete and time retention for data loss protection","comment_id":"238420"},{"upvote_count":"7","comment_id":"236388","timestamp":"1607254920.0","content":"Time-based retention policy support: Users can set policies to store data for a specified interval. When a time-based retention policy is set, blobs can be created and read, but not modified or deleted. After the retention period has expired, blobs can be deleted but not overwritten.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-immutable-storage\n\nB,C,E","poster":"brcdbrcd"},{"comment_id":"213589","poster":"rocksonroll","content":"I think the answers are right because of \"Minimize potential data loss.\"","timestamp":"1604597040.0","upvote_count":"2"},{"timestamp":"1600661880.0","comments":[{"timestamp":"1603813860.0","poster":"M0e","upvote_count":"9","comment_id":"207140","content":"Soft delete is aimed at \"minimizing potential data loss\". Last Sync Time query is not required since sending a request to an unavailable zone or region would fail anyway. The retry mechanism is to hold the application write requests until Azure re-directs them to the paired region when it becomes writable; as the result of automatic fail-over. Re-direction of the read requests happens instantaneously in case of an outage --- My answers would be B, C, D.","comments":[{"timestamp":"1608735240.0","poster":"saponazureguy","content":"I agree, BCD should be the correct answer, no disagreement with other comments on B and C. I would go with D because as MOe pointed out, during failover some requests can potentially be lost hence the best practice is to have a re-try logic. This is one of the requirements as well \"Minimize potential data loss\". E to me doesnt make sense since it has to do more with data lifecycle management than minimizing potential data loss.","upvote_count":"1","comment_id":"251023"}]},{"comment_id":"224352","poster":"anurag1p","content":"Agree with you, the context of the question is based on reading and writing to the Storage account through app. There is no mention of deletion. Considering that, A,C,D looks logical.","timestamp":"1605965100.0","upvote_count":"2"}],"comment_id":"183428","upvote_count":"2","poster":"AJMorgan591","content":"This question could be interpreted another way: \"Minimize potential data loss if the primary region has an outage\". The question already mentions high availability, and there's no mention of file deletion, so I interpret this question as being focused entirely on high availability.\n\nIn which case the answer would be: A, C, D.\n\nEnable RA-GRS.\nIf the primary goes down, check the LastSyncTime of the storage account, and if data was written to the primary after the LastSyncTime, it will have been lost. Therefore, add retry logic to the app for storage account interactions to ensure such data is eventually written successfully.\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/last-sync-time-get\n\nI know the question says \"changes to the storage account\", and there are indeed three answers that involve changes to the storage account, but that's effectively giving the solution away, and common sense suggests Microsoft aren't going to make it that easy for you :)"},{"comment_id":"149786","timestamp":"1596461640.0","poster":"GeoffWright","content":"ABC - https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy#read-access-to-data-in-the-secondary-region","upvote_count":"3"}],"unix_timestamp":1596461640,"timestamp":"2020-08-03 15:34:00","question_id":186,"answer":"BCE","choices":{"B":"From the storage account, enable soft deletes.","C":"From the storage account, enable read-access geo-redundancy storage (RA-GRS).","E":"From the storage account, enable a time-based retention policy.","D":"From the app, add retry logic to the storage account interactions.","A":"From the app, query the LastSyncTime of the storage account."},"answer_description":"Soft delete protects blob data from being accidentally or erroneously modified or deleted. When soft delete is enabled for a storage account, blobs, blob versions\n(preview), and snapshots in that storage account may be recovered after they are deleted, within a retention period that you specify.\nGeo-redundant storage (with GRS or GZRS) replicates your data to another physical location in the secondary region to protect against regional outages.\nHowever, that data is available to be read only if the customer or Microsoft initiates a failover from the primary to secondary region. When you enable read access to the secondary region, your data is available to be read if the primary region becomes unavailable. For read access to the secondary region, enable read-access geo-redundant storage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS).\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/soft-delete-overview https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy#read-access-to-data-in-the-secondary-region","question_images":[],"answer_ET":"BCE","topic":"4","exam_id":66,"question_text":"You have a line-of-business (LOB) app that reads files from and writes files to Azure Blob storage in an Azure Storage account.\nYou need to recommend changes to the storage account to meet the following requirements:\nProvide the highest possible availability.\nMinimize potential data loss.\nWhich three changes should you recommend? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answers_community":[],"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/27226-exam-dp-201-topic-4-question-6-discussion/","isMC":true},{"id":"CXVOmq7Vj5HW1KQYgkqj","question_id":187,"url":"https://www.examtopics.com/discussions/microsoft/view/24611-exam-dp-201-topic-4-question-7-discussion/","answer_description":"Azure's cool storage tier, also known as Azure cool Blob storage, is for infrequently-accessed data that needs to be stored for a minimum of 30 days. Typical use cases include backing up data before tiering to archival systems, legal data, media files, system audit information, datasets used for big data analysis and more.\nThe storage cost for this Azure cold storage tier is lower than that of hot storage tier. Since it is expected that the data stored in this tier will be accessed less frequently, the data access charges are high when compared to hot tier. There are no additional changes required in your applications as these tiers can be accessed using APIs in the same manner that you access Azure storage.\nReference:\nhttps://cloud.netapp.com/blog/low-cost-storage-options-on-azure","timestamp":"2020-07-02 07:18:00","unix_timestamp":1593667080,"question_text":"A company is evaluating data storage solutions.\nYou need to recommend a data storage solution that meets the following requirements:\n✑ Minimize costs for storing blob objects.\n✑ Optimize access for data that is infrequently accessed.\n✑ Data must be stored for at least 30 days.\n✑ Data availability must be at least 99 percent.\nWhat should you recommend?","choices":{"D":"Archive","B":"Cold","A":"Premium","C":"Hot"},"isMC":true,"discussion":[{"comments":[{"poster":"Brickrocks","timestamp":"1607397960.0","content":"that's cool","upvote_count":"11","comment_id":"237834"}],"upvote_count":"27","content":"It should be 'Cool', not 'Cold' ;)\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers","timestamp":"1593667080.0","poster":"Wirehinge","comment_id":"124742"},{"timestamp":"1601386860.0","content":"you're totally right: cool","upvote_count":"5","comment_id":"189703","poster":"EnricVives"},{"comment_id":"392854","poster":"tes","upvote_count":"1","timestamp":"1624878840.0","content":"If you cool it for 30 days then it becomes cold.. chill"}],"answer":"B","answer_ET":"B","answer_images":[],"topic":"4","question_images":[],"answers_community":[],"exam_id":66},{"id":"Z50IgpD4pjeiOimmys89","unix_timestamp":1581075840,"question_id":188,"timestamp":"2020-02-07 12:44:00","exam_id":66,"answer_images":[],"choices":{"D":"Database sharding","C":"Elastic pools","B":"Managed instances","A":"Read scale-out"},"question_images":[],"isMC":true,"answers_community":[],"topic":"4","url":"https://www.examtopics.com/discussions/microsoft/view/13553-exam-dp-201-topic-4-question-8-discussion/","answer_ET":"C","answer":"C","question_text":"A company has many applications. Each application is supported by separate on-premises databases.\nYou must migrate the databases to Azure SQL Database. You have the following requirements:\n✑ Organize databases into groups based on database usage.\n✑ Define the maximum resource limit available for each group of databases.\nYou need to recommend technologies to scale the databases to support expected increases in demand.\nWhat should you recommend?","answer_description":"SQL Database elastic pools are a simple, cost-effective solution for managing and scaling multiple databases that have varying and unpredictable usage demands. The databases in an elastic pool are on a single Azure SQL Database server and share a set number of resources at a set price.\nYou can configure resources for the pool based either on the DTU-based purchasing model or the vCore-based purchasing model.\nIncorrect Answers:\nD: Database sharding is a type of horizontal partitioning that splits large databases into smaller components, which are faster and easier to manage.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-pool","discussion":[{"content":"Answer is correct (C) Elastic Pool\n\nSearch for \"maximum resource limit\" in the below link\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/scale-resources","timestamp":"1593167640.0","comment_id":"120429","poster":"AhmedReda","upvote_count":"24"},{"poster":"syu31svc","comment_id":"238390","timestamp":"1607439420.0","content":"100% C for correct","upvote_count":"6"},{"upvote_count":"2","comment_id":"88197","timestamp":"1589365440.0","comments":[{"upvote_count":"5","comment_id":"110898","timestamp":"1592233320.0","content":"Do not confuse","poster":"BHAWS"},{"poster":"mmat","upvote_count":"6","content":"How do you create database groups and allocate resources to those groups on managed instance? Elastic pools seems as an obvious answer considering the emphasis on \"groups\".","comment_id":"121374","timestamp":"1593278340.0"}],"poster":"201p","content":"Why not B? Since its a migration case. Managed instance would be more suited."},{"poster":"epgd","timestamp":"1581075840.0","comments":[{"upvote_count":"10","content":"because of \"Define the maximum resource limit available\"","poster":"Loai","timestamp":"1587623400.0","comment_id":"78217"},{"upvote_count":"1","content":"Usually total demand of elastic pool is lower than sum of all demands of individual databases","timestamp":"1588368240.0","poster":"Leonido","comment_id":"82347"}],"content":"Why is Elastic Pool when the increases are expected...?¿","upvote_count":"1","comment_id":"47706"}]},{"id":"c9NTLmIdS3K9rOYvQTW3","unix_timestamp":1607234820,"question_id":189,"timestamp":"2020-12-06 07:07:00","exam_id":66,"answer_images":[],"choices":{"B":"Dump and restore","D":"MySQL Workbench","C":"Import and export","A":"Azure Database Migration Service"},"question_images":[],"topic":"4","answers_community":[],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/38962-exam-dp-201-topic-4-question-9-discussion/","answer_ET":"A","answer":"A","question_text":"You have an on-premises MySQL database that is 800 GB in size.\nYou need to migrate a MySQL database to Azure Database for MySQL. You must minimize service interruption to live sites or applications that use the database.\nWhat should you recommend?","answer_description":"You can perform MySQL migrations to Azure Database for MySQL with minimal downtime by using the newly introduced continuous sync capability for the Azure\nDatabase Migration Service (DMS). This functionality limits the amount of downtime that is incurred by the application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/mysql/howto-migrate-online","discussion":[{"poster":"Wendy_DK","content":"Answer is correct.\nAzure DMS performs an initial load of your on-premises to Azure Database for MySQL, and then continuously syncs any new transactions to Azure while the application remains running. After the data catches up on the target Azure side, you stop the application for a brief moment (minimum downtime), wait for the last batch of data (from the time you stop the application until the application is effectively unavailable to take any new traffic) to catch up in the target, and then update your connection string to point to Azure. When you are finished, your application will be live on Azure!","comment_id":"362520","upvote_count":"5","timestamp":"1621547160.0"},{"poster":"syu31svc","comment_id":"236202","content":"Other options are obviously wrong","upvote_count":"3","comments":[{"comment_id":"364831","upvote_count":"2","content":"not obviously wrong! You can migrate db with dump/restore. But this is not cover the minimum downtime... so ANSWER IS CORRECT","poster":"memo43","timestamp":"1621790280.0"}],"timestamp":"1607234820.0"}]},{"id":"hTs2zadZPQC883e7JFk2","question_id":190,"discussion":[{"content":"I agree with C","poster":"elimey","comment_id":"419533","timestamp":"1628056920.0","upvote_count":"1"},{"poster":"Dhaval_Azure","content":"shard key should be static \"The shard key should be static. It shouldn't be based on data that might change.\" \nhttps://docs.microsoft.com/en-us/azure/architecture/patterns/sharding","upvote_count":"1","timestamp":"1624989000.0","comment_id":"394118"},{"comment_id":"383358","comments":[{"comment_id":"394121","content":"I agree as \"The shard key should be static. It shouldn't be based on data that might change.\" \n\nhttps://docs.microsoft.com/en-us/azure/architecture/patterns/sharding","upvote_count":"1","timestamp":"1624989180.0","poster":"Dhaval_Azure"}],"upvote_count":"4","timestamp":"1623845340.0","content":"I believe that C is the best option:\nA) A binary representation of the license plate could cause problems due to the non-numeric characters in the plate number.\nB) Speeds will more often than not be similar leading to hotspots\nD) Rush hour traffic will cause hotspots as well if you use timespans\n\nYes, some locations may have significantly more traffic than others but I think the likelihood of the differences being so great as to cause hotspots is minimal because real-world logic tells me that the planners are not going to put sensors in low-traffic areas.","poster":"BigMF"},{"poster":"ZodiaC","timestamp":"1623571260.0","upvote_count":"1","comment_id":"380938","content":"Is it really D?"},{"comments":[{"poster":"BobFar","content":"date and time columns are not good options for sharding","comment_id":"376258","timestamp":"1623001740.0","upvote_count":"1"}],"comment_id":"374868","content":"I think the D is correct answer.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-scale-shard-map-management\nThe binary representation of plate number is not supported among available data types for sharding keys. Time range mapping is more appropriate solution.","upvote_count":"2","timestamp":"1622879160.0","poster":"dbdev"},{"timestamp":"1622864460.0","poster":"Plhiwndkro","comment_id":"374708","upvote_count":"1","content":"I would go for D: https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-scale-shard-map-management"},{"upvote_count":"4","comments":[{"comment_id":"379746","poster":"MMM777","content":"C: Yes a list based mapping where sensors in similar locations are sharded together makes sense. You want to see the sensors over time in that area. Lat/Long would be mapped to shards. Time-based will cause hot-spots.","timestamp":"1623409620.0","upvote_count":"2"}],"timestamp":"1622283900.0","content":"I would go with C. Thoughts?","poster":"Ous01","comment_id":"369359"},{"content":"Wouldn't D be a better choice here?","upvote_count":"1","poster":"skbi570","comment_id":"363163","timestamp":"1621613940.0"}],"answer_description":"Data used for Planning Assistance must be stored in a sharded Azure SQL Database.\nA shard typically contains items that fall within a specified range determined by one or more attributes of the data. These attributes form the shard key (sometimes referred to as the partition key). The shard key should be static. It shouldn't be based on data that might change.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/patterns/sharding","choices":{"D":"a range mapping shard map on the time column","A":"a list mapping shard map on the binary representation of the License Plate column","C":"a list mapping shard map on the location column","B":"a range mapping shard map on the binary representation of the speed column"},"answers_community":[],"timestamp":"2021-05-21 18:19:00","answer":"A","answer_ET":"A","question_text":"You need to design a sharding strategy for the Planning Assistance database.\nWhat should you recommend?","url":"https://www.examtopics.com/discussions/microsoft/view/53291-exam-dp-201-topic-5-question-2-discussion/","exam_id":66,"unix_timestamp":1621613940,"topic":"5","answer_images":[],"isMC":true,"question_images":[]}],"exam":{"isBeta":false,"provider":"Microsoft","isImplemented":true,"name":"DP-201","lastUpdated":"12 Apr 2025","isMCOnly":false,"id":66,"numberOfQuestions":206},"currentPage":38},"__N_SSP":true}