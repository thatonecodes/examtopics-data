{"pageProps":{"questions":[{"id":"2X7OJ2AfXrEKAsKQG8hQ","answers_community":["E (75%)","D (25%)"],"discussion":[{"timestamp":"1733671140.0","content":"Selected Answer: E\n- Existing tables aren't affected. New tables are made available in OneLake.\n\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-house-onelake-availability#how-it-works","upvote_count":"15","comments":[{"poster":"toniteo","content":"agreed with answer E after reading and evaluating the article carefully.","comment_id":"1360009","timestamp":"1740194280.0","upvote_count":"1"}],"poster":"Tuki93","comment_id":"1323596"},{"timestamp":"1733858700.0","poster":"IshtarSQL","upvote_count":"7","content":"Selected Answer: E\nWhen you enable OneLake availability for an eventhouse in Microsoft Fabric, only new data ingested into the eventhouse will be synced and made available in OneLake.","comment_id":"1324723"},{"upvote_count":"1","comment_id":"1411101","content":"Selected Answer: E\nWe can enable it on both database and table levels. I think Microsoft just want to ask for default settings with them. on database level, default option is \"Apply to existing tables\" which will map all tables to Onelake but don't include any old data. same issue when we try it on table level though UI.","poster":"zxc01","timestamp":"1743121920.0"},{"content":"Selected Answer: D\nThe correct answer is:\n\nD. both new data and existing data in the eventhouse\n\nWhen you enable OneLake availability for an eventhouse, you have the option to apply this setting to existing tables by selecting the \"Apply to existing tables\" option. This means that both new data and existing data will be available in OneLake.\n\nAs stated in the article: \"When turning on the feature, you can also choose to apply this option to existing tables by selecting the Apply to existing tables option, to include historic backfill.\"\n\nThis explains why both new and existing data will be copied to OneLake.\n\nCheck this link:\n\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-house-onelake-availability#how-it-works","poster":"CMDev","upvote_count":"2","comments":[{"upvote_count":"1","content":"When you enable OneLake availability for an eventhouse, you have the option to apply this setting to existing tables, which includes the data already present in those tables. This means that both new data (data added after enabling OneLake availability) and existing data (data that was already in the eventhouse at the time of enabling OneLake availability) will be available in OneLake.\n\nAs the article states: \"When turning on the feature, you can also choose to apply this option to existing tables by selecting the Apply to existing tables option, to include historic backfill.\"\n\nThis indicates that enabling OneLake availability can include the historical data, meaning the existing data, in addition to any new data added afterwards.","timestamp":"1743001860.0","comment_id":"1410448","poster":"CMDev"}],"timestamp":"1743001680.0","comment_id":"1410447"},{"poster":"Kiket2ride","upvote_count":"1","timestamp":"1742739240.0","content":"Selected Answer: D\nI think it used to be that only new data was available but documentation now says otherwise: \nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-house-onelake-availability#how-it-works\nWhen you turn availability back on, all data is made available in OneLake, including historic backfill.\nI vote for D then","comment_id":"1402314"},{"content":"Selected Answer: E\nIf we strictly follow the documentation's explanation that only new tables are made available in OneLake, then your choice E. only new data added to the eventhouse is the most precise answer.\n\n✅ Final Answer: E. only new data added to the eventhouse","upvote_count":"2","timestamp":"1740679080.0","comment_id":"1362672","poster":"DarkDerf"},{"content":"Selected Answer: D\nThe correct answer is:\n\nD. both new data and existing data in the eventhouse\n\nExplanation:\nWhen you enable OneLake availability for an eventhouse in Microsoft Fabric, all existing data within the eventhouse, along with any new data added afterward, is copied to OneLake. This ensures that users can access both past and future data through OneLake file explorer.\n\nThis means that enabling OneLake availability makes the entire dataset in the eventhouse visible in OneLake, not just new data moving forward.","poster":"DarkDerf","timestamp":"1739839380.0","upvote_count":"1","comment_id":"1358052"},{"content":"Selected Answer: E\nOnly new data will be loaded to OneLake","upvote_count":"3","timestamp":"1739744340.0","poster":"henryphchan","comment_id":"1357416"},{"comment_id":"1356794","content":"Selected Answer: D\nAs per the Microsoft document you can enable onelake along with an option of existing tables as well. Here is the reference document. https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-house-onelake-availability","upvote_count":"1","timestamp":"1739613240.0","poster":"Anushareddysri","comments":[{"poster":"henryphchan","upvote_count":"1","content":"It applies to existing tables but not exist data, so I vote for E","comment_id":"1357415","timestamp":"1739744280.0"}]},{"timestamp":"1738736160.0","poster":"h2o_molecule","content":"Selected Answer: D\nYou can turn on OneLake availability at the database or table level. When enabled at the database level, all new tables and their data are made available in OneLake. When turning on the feature, you can also choose to apply this option to existing tables by selecting the Apply to existing tables option.","upvote_count":"2","comment_id":"1351730"},{"upvote_count":"2","poster":"e0f0ce6","content":"Selected Answer: E\nI think E is correct:\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-house-onelake-availability","timestamp":"1738490160.0","comment_id":"1350313"},{"upvote_count":"3","poster":"QAZdbarhate12345678","comment_id":"1326759","timestamp":"1734252720.0","content":"Selected Answer: D\nThe correct answer is:\nD. both new data and existing data in the eventhouse\nWhen you enable OneLake availability for the eventhouse, both the existing data and any new data added to the eventhouse will be copied to OneLake. This allows users to access the eventhouse data from OneLake file explorer."}],"answer_description":"","question_id":6,"question_images":[],"timestamp":"2024-12-08 16:19:00","answer_images":[],"answer_ET":"E","url":"https://www.examtopics.com/discussions/microsoft/view/152690-exam-dp-700-topic-1-question-14-discussion/","isMC":true,"unix_timestamp":1733671140,"exam_id":72,"answer":"E","question_text":"You have a Fabric workspace that contains a Real-Time Intelligence solution and an eventhouse.\nUsers report that from OneLake file explorer, they cannot see the data from the eventhouse.\nYou enable OneLake availability for the eventhouse.\nWhat will be copied to OneLake?","topic":"1","choices":{"B":"only the existing data in the eventhouse","D":"both new data and existing data in the eventhouse","E":"only new data added to the eventhouse","C":"no data","A":"only data added to new databases that are added to the eventhouse"}},{"id":"rANnJA46udAgXVBtFitB","url":"https://www.examtopics.com/discussions/microsoft/view/152676-exam-dp-700-topic-1-question-15-discussion/","timestamp":"2024-12-08 12:42:00","answer_description":"","exam_id":72,"question_images":[],"question_text":"You have a Fabric workspace named Workspace1.\nYou plan to integrate Workspace1 with Azure DevOps.\nYou will use a Fabric deployment pipeline named deployPipeline1 to deploy items from Workspace1 to higher environment workspaces as part of a medallion architecture. You will run deployPipeline1 by using an API call from an Azure DevOps pipeline.\nYou need to configure API authentication between Azure DevOps and Fabric.\nWhich type of authentication should you use?","discussion":[{"timestamp":"1740647400.0","upvote_count":"3","content":"Selected Answer: B\nMicrosoft Entra supported identities\nThis API supports the Microsoft identities listed in this section.\n\nIdentity Support\nUser Yes\nService principal and Managed identities Only Power BI Items are supported . ref https://learn.microsoft.com/en-us/rest/api/fabric/core/deployment-pipelines/deploy-stage-content?tabs=HTTP","poster":"hebertorosillo","comment_id":"1362465"},{"timestamp":"1739744340.0","comments":[{"upvote_count":"1","content":"Service principal is only supported for Power BI Items deployment. In this question no details is given so assume warehouses and laekhouses are there as well. So the right qnswer is User","poster":"shmmini","comment_id":"1411406","timestamp":"1743181560.0"}],"poster":"henryphchan","comment_id":"1357418","upvote_count":"3","content":"Selected Answer: A\nA service principal is an identity created for use with applications, hosted services, and automated tools to access Azure resources. It provides a secure, non-interactive way for Azure DevOps pipelines to authenticate and interact with Fabric APIs without needing user credentials."},{"upvote_count":"1","comment_id":"1352480","content":"Selected Answer: A\nto interact with azure services and devops to implement automation, service principal is most commonly used","poster":"prabhjot","timestamp":"1738856220.0"},{"poster":"h2o_molecule","content":"Selected Answer: A\nHere's a brief overview of the options:\n\nService principal: Provides a secure and scalable method for authenticating and authorizing access between Azure DevOps and other services. Ideal for automation and integration scenarios.\n\nMicrosoft Entra username and password: Less secure and not recommended for automated processes.\n\nManaged private endpoint: Used for secure, private connectivity between services, but not specifically for authentication in this context.\n\nWorkspace identity: Not typically used for cross-service API authentication in Azure DevOps integration scenarios.","comment_id":"1351731","upvote_count":"1","timestamp":"1738736280.0"},{"poster":"robertlavigne","timestamp":"1738675620.0","comment_id":"1351373","upvote_count":"3","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/rest/api/fabric/core/deployment-pipelines/deploy-stage-content?tabs=HTTP\nThe above documentation shows that the deploy pipeline API can only use a service principal to deploy PowerBI content. Since we are deploying content related to a medallion architecture we will need to use user based authentication. \nWorkspace ID is for fabric to reach out to other things which isn't what we want. And a managed private-endpoint is not relevant."},{"comment_id":"1344742","content":"Selected Answer: A\nTo configure API authentication between Azure DevOps and Fabric, a service principal is the recommended approach. \n\nWorkspace identity: Workspace identities are a feature within Fabric itself. They are used to control access to resources within the Fabric workspace. While relevant for managing access within Fabric, they don't directly facilitate authentication with external services like Azure DevOps.","poster":"viskas","upvote_count":"3","timestamp":"1737549600.0"},{"poster":"Meir","timestamp":"1735053180.0","content":"Selected Answer: A\nSee https://learn.microsoft.com/en-us/answers/questions/2074603/run-pipeline-via-azure-function-by-rest-api","upvote_count":"4","comment_id":"1331162","comments":[{"poster":"GHill1982","content":"Agree. Workspace identity is primarily designed for connecting Fabric items to storage accounts and is not the most suitable option for API authentication between Azure DevOps and Fabric.","comment_id":"1333713","upvote_count":"2","timestamp":"1735504500.0"}]},{"poster":"mixonfreddy","upvote_count":"2","content":"Selected Answer: D\nUse of service principal is discouraged","comment_id":"1327498","timestamp":"1734364860.0"}],"isMC":true,"choices":{"B":"Microsoft Entra username and password","D":"workspace identity","A":"service principal","C":"managed private endpoint"},"answer_images":[],"unix_timestamp":1733658120,"answers_community":["A (60%)","B (30%)","10%"],"topic":"1","question_id":7,"answer_ET":"A","answer":"A"},{"id":"RTEN4oOiNb7aKcXHi2St","isMC":true,"exam_id":72,"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/152675-exam-dp-700-topic-1-question-16-discussion/","answer":"C","question_images":["https://img.examtopics.com/dp-700/image7.png","https://img.examtopics.com/dp-700/image8.png"],"question_text":"You have a Google Cloud Storage (GCS) container named storage1 that contains the files shown in the following table.\n//IMG//\n\nYou have a Fabric workspace named Workspace1 that has the cache for shortcuts enabled. Workspace1 contains a lakehouse named Lakehouse1. Lakehouse1 has the shortcuts shown in the following table.\n//IMG//\n\nYou need to read data from all the shortcuts.\nWhich shortcuts will retrieve data from the cache?","answer_images":[],"answer_ET":"C","choices":{"D":"Products, Stores, and Trips","C":"Stores and Products only","F":"Products and Trips only","E":"Trips only","B":"Products only","A":"Stores only"},"discussion":[{"content":"Selected Answer: C\nShortcut caching can be used to reduce egress costs associated with cross-cloud data access. As files are read through an external shortcut, the files are stored in a cache for the Fabric workspace. Subsequent read requests are served from cache rather than the remote storage provider. Cached files have a retention period of 24 hours.\n\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#caching","comment_id":"1323601","poster":"Tuki93","upvote_count":"14","timestamp":"1733671740.0"},{"upvote_count":"2","content":"Selected Answer: C\nBy default is C but now D can also be true. See here:\nThe retention period for cached files can be set from 1-28 days\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#caching","comments":[{"content":"I am dealing with same issue. Could be C or D depending on your configuration.","comment_id":"1410102","timestamp":"1742919600.0","poster":"Adriel_1996","upvote_count":"2"}],"comment_id":"1402315","poster":"Kiket2ride","timestamp":"1742739360.0"},{"poster":"henryphchan","timestamp":"1739744460.0","comment_id":"1357419","upvote_count":"2","content":"Selected Answer: C\nData within 1GB and accessed less than 24 hours will be cached"},{"poster":"Bharat","comment_id":"1326643","upvote_count":"2","comments":[{"comment_id":"1327089","timestamp":"1734302700.0","upvote_count":"2","content":"Sorry, I didn't pay attention to the retention period. Because it exceeds 24 hours, it will not be read from the cache. Therefore, the answer is correct.","poster":"Bharat"}],"timestamp":"1734224160.0","content":"Selected Answer: D\nI think that the correct answer is D instead of C because the caching is enabled for ALL types of files in a Lakehouse whether they are Parquet, JSON, or CSV."}],"unix_timestamp":1733658060,"answers_community":["C (90%)","10%"],"answer_description":"","question_id":8,"timestamp":"2024-12-08 12:41:00"},{"id":"LJBRNeiXLwyqwoxyl8Fg","answer_ET":"B","topic":"1","timestamp":"2024-12-29 15:43:00","answer_description":"","answer":"B","answer_images":[],"answers_community":["B (100%)"],"unix_timestamp":1735483380,"choices":{"D":"a data management gateway","B":"a managed private endpoint","A":"an on-premises data gateway","C":"an integration runtime"},"url":"https://www.examtopics.com/discussions/microsoft/view/153605-exam-dp-700-topic-1-question-17-discussion/","discussion":[{"comment_id":"1346106","content":"Selected Answer: B\nProvided answer is correct","upvote_count":"3","poster":"Tuki93","timestamp":"1737721260.0"},{"timestamp":"1735575120.0","poster":"GHill1982","upvote_count":"3","comments":[{"timestamp":"1737180840.0","comment_id":"1342432","poster":"realexamguru","upvote_count":"1","content":"correct\nhttps://learn.microsoft.com/en-us/fabric/security/security-managed-private-endpoints-overview#supported-item-types"}],"content":"Selected Answer: B\nTo ensure that Job1 can access the data in Source1, you need to create a managed private endpoint. This will allow the Spark job to securely connect to the Azure SQL database without requiring public internet access.","comment_id":"1334240"}],"exam_id":72,"question_id":9,"isMC":true,"question_images":[],"question_text":"You have a Fabric workspace named Workspace1 that contains an Apache Spark job definition named Job1.\nYou have an Azure SQL database named Source1 that has public internet access disabled.\nYou need to ensure that Job1 can access the data in Source1.\nWhat should you create?"},{"id":"F0vzQyl7SMXnvQE2MTjA","answer_images":[],"topic":"1","unix_timestamp":1734038400,"answer_description":"","answers_community":["C (46%)","B (41%)","11%"],"question_images":["https://img.examtopics.com/dp-700/image9.png"],"discussion":[{"timestamp":"1737721680.0","content":"Selected Answer: C\nShortcut caching is currently only supported for GCS, S3 and S3 compatible shortcuts.\n\n If a file hasn’t been accessed for more than 24 hrs it's purged from the cache. Individual files greater than 1 GB in size aren't cached\n\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#caching","upvote_count":"6","comment_id":"1346112","poster":"Tuki93"},{"timestamp":"1739744820.0","content":"Selected Answer: C\nThe answer is C. \nPlease refer to MS docs (https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts)\n- Shortcut caching can be used to reduce egress costs associated with cross-cloud data access. \n- Shortcut caching is currently only supported for GCS, S3 and S3 compatible shortcuts.","comment_id":"1357428","poster":"henryphchan","upvote_count":"5","comments":[{"content":"- Cached files have a retention period of 24 hours.\n- Individual files greater than 1 GB in size aren't cached.","poster":"henryphchan","upvote_count":"2","comment_id":"1357430","timestamp":"1739744880.0"}]},{"timestamp":"1743488400.0","upvote_count":"2","content":"Selected Answer: B\nProducts and store because they have less than a GB","comment_id":"1416571","poster":"Tamele001"},{"timestamp":"1742920080.0","content":"Selected Answer: C\nProductFile is part of ADLS GEN2 which according documentation \"Shortcut caching is currently only supported for GCS, S3 and S3 compatible shortcuts.\" hence is not caching. \nTripsFile exccede 1 GB size\nOnly StoreFile support caching and has the right size\nBased on: https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts \nCaching and S3 shortcuts","upvote_count":"4","comment_id":"1410106","poster":"Adriel_1996"},{"content":"Selected Answer: B\ns3 can be saved https://learn.microsoft.com/en-us/fabric/onelake/create-s3-shortcut \nand items bigger than 1GB can not cached. \"If a file hasn’t been accessed for more than 24 hrs it's purged from the cache. Individual files greater than 1 GB in size aren't cached.\" https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts","timestamp":"1741686540.0","upvote_count":"1","comment_id":"1387338","poster":"cuomuqi"},{"upvote_count":"2","poster":"vish9","comment_id":"1366277","timestamp":"1741354320.0","content":"Selected Answer: B\nIn Microsoft Fabric, when the cache for shortcuts is enabled, the caching mechanism applies based on the file size and source location.\n\nProductFile (50MB) from storage1 (Azure Data Lake Storage Gen2)\nCached ✅ (because it's from Azure Data Lake and is small)\nTripsFile (2GB) from storage2 (Amazon S3)\nNot cached ❌ (because it's larger than 1GB)\nStoreFile (25MB) from storage2 (Amazon S3)\nCached ✅ (because it's smaller than 1GB)\nCache Behavior:\nFiles ≤ 1GB from Azure Data Lake Storage (ADLS) or Amazon S3 are cached.\nFiles > 1GB are not cached.\nAnswer:\nThe data retrieved from the cache will be Products (ProductFile) and Stores (StoreFile)."},{"comment_id":"1365162","content":"Selected Answer: B\nS3 and ADLS G2 are both supported.\nOnly limitation is the size, meaning Trips is too big to be cached.","timestamp":"1741134000.0","poster":"JensQ","upvote_count":"2"},{"upvote_count":"2","timestamp":"1740745620.0","comment_id":"1362996","poster":"DarkDerf","content":"Selected Answer: D\nSince caching works only for Azure Data Lake Storage Gen2, the shortcuts pointing to Amazon S3 (storage2) will not be retrieved from the cache.\n\nEvaluating the shortcuts:\nProducts (shortcut to ProductFile - ADLS Gen2) → ✅ Cached\nStores (shortcut to StoreFile - Amazon S3) → ❌ Not Cached\nTrips (shortcut to TripsFile - Amazon S3) → ❌ Not Cached\nAnswer:\nThe cached data will be retrieved only for Products.\n\nThus, the correct answer is:\n\nD. Products only"},{"poster":"ChenFu","upvote_count":"1","comments":[{"timestamp":"1742920140.0","comment_id":"1410107","content":"Shortcut source – Files in Azure Data Lake Storage Gen2 where is the documentation that suports ?","poster":"Adriel_1996","upvote_count":"1"}],"content":"Selected Answer: E\nIn Microsoft Fabric, when cache for shortcuts is enabled, the data from the shortcuts is automatically cached in OneLake for faster access. The key factors that determine cache availability are:\n\nShortcut source – Files in Azure Data Lake Storage Gen2 and Amazon S3 are both supported for caching.\nCache enabled – If caching is enabled for the workspace, all accessible shortcuts are cached.\nData access – Cached data is updated when the source changes or when the cache expires (typically after 30 days of inactivity).","comment_id":"1359675","timestamp":"1740126060.0"},{"timestamp":"1738856640.0","comment_id":"1352483","poster":"prabhjot","content":"Selected Answer: B\nShortcuts are accessible from both ADLS and Amazon S3 as of the time of writing this answer. Trips is too large to be cached","upvote_count":"4"},{"poster":"Diana11","upvote_count":"1","timestamp":"1738737540.0","comments":[{"content":"..and Store, because in Lakehouse1, we have -> A shortcut to StoreFile aliased as Stores\nCorrect answer is B. Products and Store only. Trips is too large to be cached (2 GB).","comment_id":"1351807","poster":"Diana11","upvote_count":"1","timestamp":"1738751940.0"}],"content":"Selected Answer: D\nIn Microsoft Fabric, when the cache for shortcuts is enabled, data accessed via shortcuts can be cached in OneLake, reducing latency and improving performance. However, not all shortcuts are automatically cached - only those pointing to Azure Data Lake Storage Gen2 (ADLS Gen2)\nThe only shortcut whose data will be retrieved from the cache is Products","comment_id":"1351736"},{"content":"Selected Answer: D\nOnly the Azure-based shortcut (Products) retrieves data from the cache.","timestamp":"1738583100.0","comment_id":"1350843","upvote_count":"1","poster":"Coder99"},{"poster":"amli123","content":"Selected Answer: C\nFiles > 1GB not cached\nShorcut caching only supported for S3","upvote_count":"2","comment_id":"1342796","timestamp":"1737242640.0"},{"upvote_count":"4","poster":"01ceacf","timestamp":"1734038400.0","comment_id":"1325871","content":"Selected Answer: B\nFile greater than 1GB in size are not cached.\n\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts"}],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/152939-exam-dp-700-topic-1-question-18-discussion/","question_id":10,"exam_id":72,"answer_ET":"C","answer":"C","choices":{"C":"Stores only","E":"Products, Stores, and Trips","B":"Products and Store only","D":"Products only","A":"Trips and Stores only"},"question_text":"You have an Azure Data Lake Storage Gen2 account named storage1 and an Amazon S3 bucket named storage2.\nYou have the Delta Parquet files shown in the following table.\n//IMG//\n\nYou have a Fabric workspace named Workspace1 that has the cache for shortcuts enabled. Workspace1 contains a lakehouse named Lakehouse1. Lakehouse1 has the following shortcuts:\nA shortcut to ProductFile aliased as Products\nA shortcut to StoreFile aliased as Stores\nA shortcut to TripsFile aliased as Trips\nThe data from which shortcuts will be retrieved from the cache?","timestamp":"2024-12-12 22:20:00"}],"exam":{"isBeta":false,"lastUpdated":"12 Apr 2025","provider":"Microsoft","isMCOnly":false,"id":72,"isImplemented":true,"name":"DP-700","numberOfQuestions":97},"currentPage":2},"__N_SSP":true}