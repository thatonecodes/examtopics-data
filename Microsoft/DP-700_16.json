{"pageProps":{"questions":[{"id":"FdblhXDicrkJbpCw5yEL","choices":{"A":"a data pipeline","B":"an Apache Spark job definition","C":"a streaming dataflow","D":"a notebook"},"answer_ET":"A","discussion":[{"poster":"5e89616","upvote_count":"1","timestamp":"1744198320.0","comment_id":"1559227","content":"Selected Answer: A\nA. Data Pipeline - correct\nB. Apache Spark job definition Designed for data processing and transformation, not optimized for moving data via gateway.\nC. Streaming dataflow Used for real-time streaming data, not batch data movement from SQL Server.\nD. Notebook Can connect using JDBC/ODBC, but it doesn't support the gateway natively. Also requires more code and less suitable for production data movement."}],"unix_timestamp":1744198320,"answer_description":"","answer_images":[],"exam_id":72,"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/302468-exam-dp-700-topic-2-question-33-discussion/","answer":"A","timestamp":"2025-04-09 13:32:00","question_id":76,"topic":"2","question_text":"You have a Fabric workspace that contains a warehouse named Warehouse1.\n\nYou have an on-premises Microsoft SQL Server database named Database1 that is accessed by using an on-premises data gateway.\n\nYou need to copy data from Database1 to Warehouse1.\n\nWhich item should you use?","answers_community":["A (100%)"]},{"id":"XItAWoeFLnQNZS2PTWOg","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/302496-exam-dp-700-topic-2-question-34-discussion/","answer_images":[],"question_id":77,"answers_community":["A (100%)"],"discussion":[{"poster":"zxc01","content":"Selected Answer: A\nwhy don't use hash function on source side? the question lost many details. I just can guess they try to use incremental loading method. Normally we need keep Hask key values in target table column and use hash function to get changed imported rows from source side if their hash values cannot match with target hash value column.","timestamp":"1744244460.0","upvote_count":"1","comment_id":"1559422"}],"question_images":[],"exam_id":72,"timestamp":"2025-04-10 02:21:00","unix_timestamp":1744244460,"choices":{"B":"a direct attributes comparison across the attributes in the DimCustomer table.","A":"a hash function to compare the attributes in the source table.","C":"a direct attributes comparison for the attributes in the source table.","D":"a hash function to compare the attributes in the DimCustomer table."},"question_text":"You have a Fabric warehouse named DW1 that contains a Type 2 slowly changing dimension (SCD) dimension table named DimCustomer. DimCustomer contains 100 columns and 20 million rows. The columns are of various data types, including int, varchar, date, and varbinary.\n\nYou need to identify incoming changes to the table and update the records when there is a change. The solution must minimize resource consumption.\n\nWhat should you use to identify changes to attributes?","answer":"A","topic":"2","answer_description":"","answer_ET":"D"},{"id":"GVEnLx2zmDzXOuad5zPx","discussion":[{"comment_id":"1559444","timestamp":"1744249740.0","content":"I think both Apache Spark Structured Streaming and data pipeline(if we think it can play role of Fabric streaming solution) can match it. Howerver, question said \"The solution must minimize development effort\", data pipeline is better? \nSource2 didn't give enough details, if this structured data is database and configured with CDC, then eventstream is option. Source 3 should be data pipeline.","upvote_count":"1","poster":"zxc01"}],"url":"https://www.examtopics.com/discussions/microsoft/view/302502-exam-dp-700-topic-2-question-39-discussion/","unix_timestamp":1744249740,"exam_id":72,"timestamp":"2025-04-10 03:49:00","answer":"","isMC":false,"question_text":"HOTSPOT\n-\n\nYou need to recommend a Fabric streaming solution that will use the sources shown in the following table.\n\n//IMG//\n\n\nThe solution must minimize development effort.\n\nWhat should you include in the recommendation for each source? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","topic":"2","question_id":78,"answers_community":[],"answer_description":"","question_images":["https://img.examtopics.com/dp-700/image102.png","https://img.examtopics.com/dp-700/image103.png"],"answer_ET":"","answer_images":["https://img.examtopics.com/dp-700/image104.png"]},{"id":"BY0aCJRkEpLcjICS9aNx","answer_ET":"D","answers_community":["D (82%)","B (18%)"],"choices":{"D":"Create a shortcut and ensure that caching is enabled for the workspace.","B":"Create a shortcut and ensure that caching is disabled for the workspace.","C":"Create a workspace identity and use the identity in a data pipeline.","A":"Create a workspace identity and enable high concurrency for the notebooks."},"answer_description":"","question_images":[],"question_text":"Case Study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview. Company Overview -\nContoso, Ltd. is an online retail company that wants to modernize its analytics platform by moving to Fabric. The company plans to begin using Fabric for marketing analytics.\n\nOverview. IT Structure -\nThe company’s IT department has a team of data analysts and a team of data engineers that use analytics systems.\nThe data engineers perform the ingestion, transformation, and loading of data. They prefer to use Python or SQL to transform the data.\nThe data analysts query data and create semantic models and reports. They are qualified to write queries in Power Query and T-SQL.\n\nExisting Environment. Fabric -\nContoso has an F64 capacity named Cap1. All Fabric users are allowed to create items.\nContoso has two workspaces named WorkspaceA and WorkspaceB that currently use Pro license mode.\nExisting Environment. Source Systems\nContoso has a point of sale (POS) system named POS1 that uses an instance of SQL Server on Azure Virtual Machines in the same Microsoft Entra tenant as Fabric. The host virtual machine is on a private virtual network that has public access blocked. POS1 contains all the sales transactions that were processed on the company’s website.\nThe company has a software as a service (SaaS) online marketing app named MAR1. MAR1 has seven entities. The entities contain data that relates to email open rates and interaction rates, as well as website interactions. The data can be exported from MAR1 by calling REST APIs. Each entity has a different endpoint.\nContoso has been using MAR1 for one year. Data from prior years is stored in Parquet files in an Amazon Simple Storage Service (Amazon S3) bucket. There are 12 files that range in size from 300 MB to 900 MB and relate to email interactions.\nExisting Environment. Product Data\nPOS1 contains a product list and related data. The data comes from the following three tables:\n\nProducts -\n\nProductCategories -\n\nProductSubcategories -\nIn the data, products are related to product subcategories, and subcategories are related to product categories.\n\nExisting Environment. Azure -\nContoso has a Microsoft Entra tenant that has the following mail-enabled security groups:\nDataAnalysts: Contains the data analysts\nDataEngineers: Contains the data engineers\nContoso has an Azure subscription.\nThe company has an existing Azure DevOps organization and creates a new project for repositories that relate to Fabric.\nExisting Environment. User Problems\nThe VP of marketing at Contoso requires analysis on the effectiveness of different types of email content. It typically takes a week to manually compile and analyze the data. Contoso wants to reduce the time to less than one day by using Fabric.\nThe data engineering team has successfully exported data from MAR1. The team experiences transient connectivity errors, which causes the data exports to fail.\n\nRequirements. Planned Changes -\nContoso plans to create the following two lakehouses:\nLakehouse1: Will store both raw and cleansed data from the sources\nLakehouse2: Will serve data in a dimensional model to users for analytical queries\nAdditional items will be added to facilitate data ingestion and transformation.\nContoso plans to use Azure Repos for source control in Fabric.\nRequirements. Technical Requirements\nThe new lakehouses must follow a medallion architecture by using the following three layers: bronze, silver, and gold. There will be extensive data cleansing required to populate the MAR1 data in the silver layer, including deduplication, the handling of missing values, and the standardizing of capitalization.\nEach layer must be fully populated before moving on to the next layer. If any step in populating the lakehouses fails, an email must be sent to the data engineers.\nData imports must run simultaneously, when possible.\nThe use of email data from the Amazon S3 bucket must meet the following requirements:\nMinimize egress costs associated with cross-cloud data access.\nPrevent saving a copy of the raw data in the lakehouses.\nItems that relate to data ingestion must meet the following requirements:\nThe items must be source controlled alongside other workspace items.\nIngested data must land in the bronze layer of Lakehouse1 in the Delta format.\nNo changes other than changes to the file formats must be implemented before the data lands in the bronze layer.\nDevelopment effort must be minimized and a built-in connection must be used to import the source data.\nIn the event of a connectivity error, the ingestion processes must attempt the connection again.\nLakehouses, data pipelines, and notebooks must be stored in WorkspaceA. Semantic models, reports, and dataflows must be stored in WorkspaceB.\nOnce a week, old files that are no longer referenced by a Delta table log must be removed.\nRequirements. Data Transformation\nIn the POS1 product data, ProductID values are unique. The product dimension in the gold layer must include only active products from product list. Active products are identified by an IsActive value of 1.\nSome product categories and subcategories are NOT assigned to any product. They are NOT analytically relevant and must be omitted from the product dimension in the gold layer.\n\nRequirements. Data Security -\nSecurity in Fabric must meet the following requirements:\nThe data engineers must have read and write access to all the lakehouses, including the underlying files.\nThe data analysts must only have read access to the Delta tables in the gold layer.\nThe data analysts must NOT have access to the data in the bronze and silver layers.\nThe data engineers must be able to commit changes to source control in WorkspaceA.\nYou need to ensure that usage of the data in the Amazon S3 bucket meets the technical requirements.\nWhat should you do?","discussion":[{"upvote_count":"12","poster":"GHill1982","comment_id":"1334722","content":"Selected Answer: D\nEnabling caching for the workspace will help minimize egress costs by reducing the amount of data that needs to be transferred across clouds. Creating a shortcut ensures that the raw data is not duplicated in the lakehouse.","timestamp":"1735638120.0"},{"timestamp":"1744178640.0","upvote_count":"1","comment_id":"1559152","poster":"5e89616","content":"Selected Answer: D\nThe use of email data from the Amazon S3 bucket must meet the following requirements:\n- Minimize egress costs associated with cross-cloud data access -> B or D\n- Prevent saving a copy of the raw data in the lakehouses -> B (no caching)"},{"poster":"Kiket2ride","timestamp":"1743148980.0","upvote_count":"1","content":"Selected Answer: D\nCorrect answer is D because you need to minimize egress cost and at the same time you don't store data in raw because cache on means the data stays there for no more than 28 days","comment_id":"1411207"},{"content":"Selected Answer: B\nCaching should be disabled.","upvote_count":"1","comment_id":"1401935","poster":"vish9","timestamp":"1742655180.0"},{"comment_id":"1364512","content":"Selected Answer: B\nNOT \"D\": Create a shortcut and ensure that caching is enabled for the workspace.\nEnabling caching would store temporary copies of the data in Fabric, which contradicts the requirement to prevent storing raw data in the lakehouses.\n\nFinal Answer:\n✅ B. Create a shortcut and ensure that caching is disabled for the workspace.","timestamp":"1741023840.0","poster":"DarkDerf","upvote_count":"2"},{"comments":[{"poster":"DirectX","comment_id":"1364442","content":"If you disable cache-ing, data will flow from S3 to Azure all the time.\nIf you enable cache-ing, it will refer to the cached data first, thus resulting in lower costs for Amazon S3","upvote_count":"1","timestamp":"1741012560.0"}],"upvote_count":"1","content":"Selected Answer: B\nThe correct answer is B. Create a shortcut and ensure that caching is disabled for the workspace\nCreate a shortcut: A shortcut allows you to reference data in the Amazon S3 bucket without copying the data into the lakehouse. This meets the requirement to prevent saving a copy of the raw data in the lakehouses.\n\nEnsure that caching is disabled for the workspace: Disabling caching ensures that the data is accessed directly from the Amazon S3 bucket, minimizing egress costs associated with cross-cloud data access.\n\nAnswer isn't D because:\nEnabling caching would result in data being copied to the lakehouse, which violates the requirement to prevent saving a copy of the raw data.","timestamp":"1739641200.0","poster":"Sunnyb","comment_id":"1356960"},{"timestamp":"1738911180.0","upvote_count":"4","content":"Selected Answer: D\nshortcut creation and caching is available for Amazon S3 bucket and ADLS; reduces egress costs and also makes sure no data duplication done!","comment_id":"1352868","poster":"prabhjot"}],"timestamp":"2024-12-31 10:42:00","exam_id":72,"unix_timestamp":1735638120,"answer":"D","question_id":79,"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/153711-exam-dp-700-topic-2-question-4-discussion/","topic":"2","answer_images":[]},{"id":"uaSjWLRvynY064B7wO8d","isMC":false,"answer_ET":"","answers_community":[],"question_id":80,"url":"https://www.examtopics.com/discussions/microsoft/view/154682-exam-dp-700-topic-2-question-5-discussion/","question_text":"HOTSPOT -\n\nCase Study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview. Company Overview -\nContoso, Ltd. is an online retail company that wants to modernize its analytics platform by moving to Fabric. The company plans to begin using Fabric for marketing analytics.\n\nOverview. IT Structure -\nThe company’s IT department has a team of data analysts and a team of data engineers that use analytics systems.\nThe data engineers perform the ingestion, transformation, and loading of data. They prefer to use Python or SQL to transform the data.\nThe data analysts query data and create semantic models and reports. They are qualified to write queries in Power Query and T-SQL.\n\nExisting Environment. Fabric -\nContoso has an F64 capacity named Cap1. All Fabric users are allowed to create items.\nContoso has two workspaces named WorkspaceA and WorkspaceB that currently use Pro license mode.\nExisting Environment. Source Systems\nContoso has a point of sale (POS) system named POS1 that uses an instance of SQL Server on Azure Virtual Machines in the same Microsoft Entra tenant as Fabric. The host virtual machine is on a private virtual network that has public access blocked. POS1 contains all the sales transactions that were processed on the company’s website.\nThe company has a software as a service (SaaS) online marketing app named MAR1. MAR1 has seven entities. The entities contain data that relates to email open rates and interaction rates, as well as website interactions. The data can be exported from MAR1 by calling REST APIs. Each entity has a different endpoint.\nContoso has been using MAR1 for one year. Data from prior years is stored in Parquet files in an Amazon Simple Storage Service (Amazon S3) bucket. There are 12 files that range in size from 300 MB to 900 MB and relate to email interactions.\nExisting Environment. Product Data\nPOS1 contains a product list and related data. The data comes from the following three tables:\n\nProducts -\n\nProductCategories -\n\nProductSubcategories -\nIn the data, products are related to product subcategories, and subcategories are related to product categories.\n\nExisting Environment. Azure -\nContoso has a Microsoft Entra tenant that has the following mail-enabled security groups:\nDataAnalysts: Contains the data analysts\nDataEngineers: Contains the data engineers\nContoso has an Azure subscription.\nThe company has an existing Azure DevOps organization and creates a new project for repositories that relate to Fabric.\nExisting Environment. User Problems\nThe VP of marketing at Contoso requires analysis on the effectiveness of different types of email content. It typically takes a week to manually compile and analyze the data. Contoso wants to reduce the time to less than one day by using Fabric.\nThe data engineering team has successfully exported data from MAR1. The team experiences transient connectivity errors, which causes the data exports to fail.\n\nRequirements. Planned Changes -\nContoso plans to create the following two lakehouses:\nLakehouse1: Will store both raw and cleansed data from the sources\nLakehouse2: Will serve data in a dimensional model to users for analytical queries\nAdditional items will be added to facilitate data ingestion and transformation.\nContoso plans to use Azure Repos for source control in Fabric.\nRequirements. Technical Requirements\nThe new lakehouses must follow a medallion architecture by using the following three layers: bronze, silver, and gold. There will be extensive data cleansing required to populate the MAR1 data in the silver layer, including deduplication, the handling of missing values, and the standardizing of capitalization.\nEach layer must be fully populated before moving on to the next layer. If any step in populating the lakehouses fails, an email must be sent to the data engineers.\nData imports must run simultaneously, when possible.\nThe use of email data from the Amazon S3 bucket must meet the following requirements:\nMinimize egress costs associated with cross-cloud data access.\nPrevent saving a copy of the raw data in the lakehouses.\nItems that relate to data ingestion must meet the following requirements:\nThe items must be source controlled alongside other workspace items.\nIngested data must land in the bronze layer of Lakehouse1 in the Delta format.\nNo changes other than changes to the file formats must be implemented before the data lands in the bronze layer.\nDevelopment effort must be minimized and a built-in connection must be used to import the source data.\nIn the event of a connectivity error, the ingestion processes must attempt the connection again.\nLakehouses, data pipelines, and notebooks must be stored in WorkspaceA. Semantic models, reports, and dataflows must be stored in WorkspaceB.\nOnce a week, old files that are no longer referenced by a Delta table log must be removed.\nRequirements. Data Transformation\nIn the POS1 product data, ProductID values are unique. The product dimension in the gold layer must include only active products from product list. Active products are identified by an IsActive value of 1.\nSome product categories and subcategories are NOT assigned to any product. They are NOT analytically relevant and must be omitted from the product dimension in the gold layer.\n\nRequirements. Data Security -\nSecurity in Fabric must meet the following requirements:\nThe data engineers must have read and write access to all the lakehouses, including the underlying files.\nThe data analysts must only have read access to the Delta tables in the gold layer.\nThe data analysts must NOT have access to the data in the bronze and silver layers.\nThe data engineers must be able to commit changes to source control in WorkspaceA.\nYou need to create the product dimension.\nHow should you complete the Apache Spark SQL code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n//IMG//","answer_description":"","timestamp":"2025-01-16 14:12:00","discussion":[{"poster":"MDWPartners","upvote_count":"9","timestamp":"1737033120.0","comment_id":"1341700","content":"LEFT OUTER JOIN\n\nINNER JOIN\n\nIsActive = 1\n\nthe first join should be left because we want to keep all the products. Second should be inner to omit unused categories\n\nWe need to keep active products\n\nIn the POS1 product data, ProductID values are unique. The product dimension in the gold layer must include only active products from product list. Active products are identified by an IsActive value of 1.\nSome product categories and subcategories are NOT assigned to any product. They are NOT analytically relevant and must be omitted from the product dimension in the gold layer."},{"comment_id":"1401371","content":"INNER INNER IsActive = 1\n\nThe product dimension in the gold layer must include only active products from product list. Active products are identified by an IsActive value of 1.\nSome product categories and subcategories are NOT assigned to any product. They are NOT analytically relevant and must be omitted from the product dimension in the gold layer.\n\nLEFT INNER at the end is the same result but why LEFT first if the second join is INNER?","poster":"vigaro","timestamp":"1742511960.0","upvote_count":"2"},{"poster":"DirectX","upvote_count":"1","comment_id":"1361548","timestamp":"1740509820.0","content":"realexamguru's comment is correct. According to the info, there is no difference between left join and inner join in this case. \nThere would be difference when there are Products with empty values for CategoryId and SubcategoryId fields."},{"poster":"giraf","content":"left outer/left outer/isactive=1\nFirst left outer: This way you keep all your products. It doesn't say you want to filter out products without an subcategory.\nSecond left outer: If you use inner join, subcategories without a category will be filtered out. That's not the question. You want to keep subcategories even without a category. So, left outer.\nIsActive = 1: No discussion I think.","comment_id":"1357369","upvote_count":"1","timestamp":"1739736660.0"},{"timestamp":"1739126700.0","poster":"realexamguru","content":"Inner Join / Inner Join / IsActive = 1 if all products have subcategories\nLeft Outer Join / Left Outer Join / IsActive = 1 if some products are missing subcategory information\nBased on the the details that we have there should be no difference between a left join and an inner join. We know there are some extra categories and subcategories but we don't know if there is any product without a subcategory. Probably we are missing another piece of information. I prefer inner join since it performs better.\nFull Join and anti join are just obviously incorrect. And there's no such thing as an outer join.","upvote_count":"3","comment_id":"1354079"},{"poster":"4371883","timestamp":"1737780840.0","content":"left outer; left outer; IsActive = 1\nThe above can achieve the requirements below as well:\n- In the POS1 product data, ProductID values are unique. The product dimension in the gold layer must include only active products from product list. Active products are identified by an IsActive value of 1.\n- Some product categories and subcategories are NOT assigned to any product. They are NOT analytically relevant and must be omitted from the product dimension in the gold layer.","comment_id":"1346328","upvote_count":"4"}],"answer_images":["https://img.examtopics.com/dp-700/image33.png"],"exam_id":72,"topic":"2","answer":"","unix_timestamp":1737033120,"question_images":["https://img.examtopics.com/dp-700/image32.png"]}],"exam":{"isImplemented":true,"id":72,"isBeta":false,"numberOfQuestions":97,"provider":"Microsoft","name":"DP-700","isMCOnly":false,"lastUpdated":"12 Apr 2025"},"currentPage":16},"__N_SSP":true}