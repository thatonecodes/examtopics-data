{"pageProps":{"questions":[{"id":"qssCQeASbiyBVBaUgn9D","discussion":[{"timestamp":"1637591400.0","upvote_count":"47","comment_id":"484290","content":"Selected Answer: B\nCorrect","poster":"FredNo","comments":[{"comment_id":"635968","timestamp":"1658657820.0","poster":"Deepshikha1228","upvote_count":"2","content":"B is correct"}]},{"poster":"GameLift","timestamp":"1631273700.0","upvote_count":"31","comments":[{"content":"Yes, your logic is correct!","comment_id":"444258","poster":"Podavenna","timestamp":"1631582100.0","upvote_count":"9"},{"comment_id":"1140973","poster":"saqib839","content":"Plus its better to use hash distribution on column where group by or joins are used","timestamp":"1707129360.0","upvote_count":"1"}],"comment_id":"442447","content":"Is it hash-distributed on PurchaseKey and not on IsOrderFinalized because 'IsOrderFinalized' yields less distributions(rows either contain yes,no values) compared to PurchaseKey?"},{"poster":"adaod14","upvote_count":"4","timestamp":"1741094280.0","comment_id":"1364914","content":"Selected Answer: B\nCorrect Answer:\nB. Hash-Distributed on PurchaseKey\n\nüí° Explanation:\nAzure Synapse Analytics supports three table distribution methods: Replicated, Hash-Distributed, and Round-Robin. The best choice depends on the workload and query patterns.\n\n1Ô∏è‚É£ Why Use Hash Distribution?\nThe FactPurchase table will have 1 million rows added daily and contain three years of data ‚Üí This results in over 1 billion rows, making hash distribution the best option for efficient querying.\nQueries GROUP BY SupplierKey, StockItemKey, and IsOrderFinalized ‚Üí A hash-distributed table will ensure these columns are evenly distributed across compute nodes, reducing data movement and improving query performance.\n2Ô∏è‚É£ Why Hash on PurchaseKey?\nPurchaseKey is the primary key (unique for each purchase).\nIdeal for distributing data evenly across compute nodes, avoiding data skew.\nImproves aggregation queries where GROUP BY is used on multiple dimensions."},{"poster":"RAG11","upvote_count":"1","content":"Selected Answer: D\nD, Since the query doesn't reference PurchaseKey for filtering or grouping, hash distribution on PurchaseKey doesn't help with optimizing the query and would not be the best choice.","timestamp":"1740008700.0","comment_id":"1359006"},{"poster":"thanglai","content":"Hash-Distributed: For large fact tables where joins or aggregations are common","timestamp":"1736132460.0","upvote_count":"2","comment_id":"1336942"},{"content":"Selected Answer: B\nCorrect Answer: B","comment_id":"1318451","timestamp":"1732686780.0","poster":"EmnCours","upvote_count":"1"},{"upvote_count":"3","timestamp":"1720255980.0","content":"Selected Answer: C\nOptimal Distribution\nGiven that the query performs a GROUP BY on SupplierKey, StockItemKey, and IsOrderFinalized, the most balanced approach is to use Round-robin distribution. While it does not ensure that rows with the same key are stored together, it avoids data skew and ensures even distribution, which helps in achieving better performance for aggregate queries.","poster":"Okkier","comments":[{"comment_id":"1342297","content":"I hear where you are coming from. The fact that the PurchaseKey is not used in the query also made me think this for a sec. So even when using the hash the situation becomes like a round-robin distribution in terms of data access in this specific query. But generally hash will be more efficient because of the predictable hash distribution structure, which is better than a completely random approach. Seeing as we are using the purchaskey which i assume is unique, we don't need to worry about skew at all. So I Still think B is the correct choice here.","poster":"JustImperius","timestamp":"1737140700.0","upvote_count":"1"},{"timestamp":"1735880340.0","upvote_count":"1","poster":"hypersam","content":"hash distribution on high cardinatlity column can also avoid data skew","comment_id":"1335891"}],"comment_id":"1243281"},{"comment_id":"1189129","timestamp":"1712212440.0","upvote_count":"2","content":"Selected Answer: B\nalmost exactly what's shown in a example of the official docs --> https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choose-a-distribution-column","poster":"MBRSDG"},{"timestamp":"1704237240.0","upvote_count":"1","poster":"sdg2844","content":"Selected Answer: B\nCorrect. Column with many unique values. Also, it's USUALLY not a column that is used in whereclauses or groupings or such, which this isn't.","comment_id":"1112348"},{"content":"Selected Answer: B\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choosing-a-distribution-column","timestamp":"1696655400.0","poster":"pperf","comment_id":"1027100","upvote_count":"1"},{"timestamp":"1694691180.0","content":"why the answer says it cannot be a date column?","upvote_count":"1","comment_id":"1007531","poster":"jiajiani"},{"timestamp":"1694493480.0","comment_id":"1005362","upvote_count":"2","comments":[{"upvote_count":"3","poster":"jiajiani","comment_id":"1008430","timestamp":"1694779080.0","content":"why we cannot use data column?"}],"content":"Selected Answer: B\nHash-distributed tables improve query performance on large fact tables. The PurchaseKey has many unique values, does not have NULLs and is not a date column.","poster":"74gjd_37"},{"upvote_count":"1","comment_id":"993699","timestamp":"1693368960.0","poster":"kkk5566","content":"Selected Answer: B\nB is correct"},{"timestamp":"1691271120.0","comment_id":"973389","comments":[{"upvote_count":"3","poster":"SolutionA","timestamp":"1691271840.0","content":"on second thought if purchasekey is not unique what is the constraint and how its created , as the question didn't mention more details , i would go with round robin not the has distributed","comment_id":"973394"}],"content":"in this case the sql where condition is on datekey so hash-distributed on PurchaseKey or Round robin distributed table the sql cost will be the same as it will be full table scan","upvote_count":"1","poster":"SolutionA"},{"upvote_count":"1","timestamp":"1683802320.0","poster":"mamahani","content":"Selected Answer: B\nB is correct","comment_id":"894955"},{"content":"Selected Answer: B\nB. Hash the purchasekey to evenly distribute the data into 60 distributions.","upvote_count":"1","poster":"henryphchan","timestamp":"1683663060.0","comment_id":"893411"},{"comment_id":"795415","upvote_count":"3","timestamp":"1675277040.0","content":"Selected Answer: B\nB is the Correct Answer","poster":"SHENOOOO"},{"poster":"astone42","timestamp":"1674645780.0","comment_id":"787548","content":"Selected Answer: B\nB is correct.","upvote_count":"2"},{"upvote_count":"5","content":"Ideally there should be an option to create partition DateKey. When we use the partition key column in the where condition , the unwanted partition's data will be eliminated automatically. that's the beauty of the partition and how it works in conjunction with the query. However, would like to know from the experts in the forum.","poster":"DindaS","timestamp":"1674337140.0","comment_id":"783744"},{"content":"Selected Answer: B\nwhat about B plus (imaginary) partitioning on date ? Or is error in question because Purchase Key by itself would not be very helpful","timestamp":"1673761680.0","poster":"Dusica","upvote_count":"1","comment_id":"776188"},{"upvote_count":"3","poster":"vigilante89","content":"B is correct!!!\n\nBecause \"hash-distributed on IsOrderFinalized\" as a distribution column would only use 2 out of 60 distributions (for Yes, No) which is a waste of compute and time resources. \n\nSo \"hash-distributed on PurchaseKey\" with multiple unique values will utilize all 60 distributions and make the query process much faster and utilize all the compute efficiently.","comments":[{"poster":"thapasuman","content":"but purchasekey is unique and each entries will create unique purchasekey. So, how it is helpful in distribution using hashing on all unique entries purchasekey. Although it is fact-table, it is cost effective to use Roundrobin.","timestamp":"1708209840.0","comment_id":"1152897","upvote_count":"2"}],"timestamp":"1670156160.0","comment_id":"735067"},{"poster":"Rrk07","timestamp":"1669481940.0","comment_id":"727699","content":"Correct answer","upvote_count":"1"},{"comment_id":"719166","upvote_count":"1","poster":"temacc","timestamp":"1668551460.0","content":"Selected Answer: B\nB if PurchaseKey have high cardinality, in other case C."},{"content":"Selected Answer: D\nselect a distribution column or set of columns that is used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses. When a table is not used in joins, consider distributing the table on a column or column set that is frequently in the GROUP BY clause.","comment_id":"690101","upvote_count":"5","timestamp":"1665314280.0","poster":"greenlever"},{"poster":"Deeksha1234","content":"Selected Answer: B\ncorrect","upvote_count":"1","comment_id":"646424","timestamp":"1660411500.0"},{"timestamp":"1656421620.0","poster":"ROLLINGROCKS","content":"The problems I see with B is:\n- PurchaseKey is not in the query\n- PurchaseKey has too many values for it to be distributed in an effective way","upvote_count":"7","comment_id":"623978","comments":[{"timestamp":"1673416560.0","poster":"Dusica","content":"agree with both arguments","comment_id":"772090","upvote_count":"1"}]},{"upvote_count":"9","comment_id":"621782","content":"Answer is incorrect, obviously. PurchaseKey does'n help at all. I work with Synapse in production 2+ years. It is not in grouping! Two option: 1) this is mistake and there should be some column from group by section (but not IsOrderFinalized - likely bit datatype - low selectivity - not distributed well) 2)if there is no mistake - round_robin is valid choise - 1M rows it is nothing for Synapse, rr is appropriate.\nBut i think it is mistake","timestamp":"1656089640.0","comments":[{"upvote_count":"1","comment_id":"772091","content":"Agree - I had the same reasoning","timestamp":"1673416620.0","poster":"Dusica"},{"poster":"ChiragShah4885","upvote_count":"1","comment_id":"635886","content":"1 M Rows daily and table has 3 years old data so total no of data in table will be too high","timestamp":"1658640840.0"}],"poster":"vlad888"},{"content":"Selected Answer: B\nI agree with B.","comment_id":"619818","timestamp":"1655816700.0","poster":"georgiakon","upvote_count":"1"},{"timestamp":"1652271000.0","upvote_count":"1","poster":"Dothy","comment_id":"600108","content":"B Correct"},{"upvote_count":"1","content":"B Correct","poster":"SandipSingha","comment_id":"598845","timestamp":"1652073540.0"},{"content":"Selected Answer: B\nB is correct","timestamp":"1649173680.0","comment_id":"581336","poster":"sarapaisley","upvote_count":"2"},{"comment_id":"565242","upvote_count":"1","content":"Selected Answer: B\nCORRECT","timestamp":"1646977740.0","poster":"Anshul2910"},{"poster":"Istiaque","comment_id":"545025","content":"Selected Answer: B\nA round-robin distributed table distributes table rows evenly across all distributions. The assignment of rows to distributions is random. Unlike hash-distributed tables, rows with equal values are not guaranteed to be assigned to the same distribution.\n\nAs a result, the system sometimes needs to invoke a data movement operation to better organize your data before it can resolve a query. This extra step can slow down your queries.","timestamp":"1644552000.0","upvote_count":"2"},{"content":"Selected Answer: C\nThe options do not have correct key selected for hash distribution and query performance will improve only if correct distribution column is selected. Also question says 1 million rows but how much those rows convert into actual GB of data is a question the data types are majorly int which arn't bulky. hence I will go for round robin instead of hash distribution.","comments":[{"timestamp":"1646971440.0","upvote_count":"4","poster":"vineet1234","comment_id":"565176","content":"Incorrect.. 1 million rows added per day. And the table has 3 years of data. So it's a large fact table. So Hash distributed. On purchase key (not on IsOrderFinalized, as it's very low cardinality)"}],"timestamp":"1643266440.0","upvote_count":"2","poster":"PallaviPatel","comment_id":"533562"},{"content":"Selected Answer: D\nHash field should be used in join, group by, having. SupplierKey, StockItemKey, IsOrderFinalized are group by fields. PurchaseKey doesn‚Äôt exist in the query, why select PurchaseKey as hash key?\n\nI select D. IsOrderFinalized may only provide 2 partitions, not as good as suppliekey and stockitemkey, but at least it is a group by column.","upvote_count":"3","timestamp":"1640984760.0","poster":"Canary_2021","comment_id":"514223","comments":[{"content":"To balance the parallel processing, select a distribution column that:\n* Has many unique values.\n* Does not have NULLs, or has only a few NULLs.\n* Is not a date column.\n\nBased on these descriptions, maybe B is the right answer. Just purchasekey is not a part of the query, is it still improve performance of this specific query?","poster":"Canary_2021","timestamp":"1640985540.0","comment_id":"514229","upvote_count":"4"}]},{"content":"B is correct","comment_id":"510315","upvote_count":"2","timestamp":"1640610900.0","poster":"Mahesh_mm"},{"poster":"kahei","timestamp":"1639225860.0","content":"Selected Answer: B","upvote_count":"1","comment_id":"499392"},{"poster":"alexleonvalencia","upvote_count":"2","timestamp":"1639194900.0","content":"Selected Answer: B\nB es la respuesta correcta.","comment_id":"499114"},{"poster":"stuard","timestamp":"1634151900.0","upvote_count":"7","content":"hash-distributed on PurchaseKey and round-robin are going to provide the same result (in a case PurchaseKey has even distribution) for the query as this specific query does not use PurchaseKey. However, round-robin is going to provide a slightly faster loading time.","comment_id":"461690"},{"upvote_count":"1","content":"Yes Agree..","timestamp":"1633240500.0","comment_id":"456430","poster":"RinkiiiiiV"},{"upvote_count":"4","content":"Correct","timestamp":"1631558700.0","comment_id":"444154","poster":"Gilvan"}],"isMC":true,"question_id":31,"topic":"1","unix_timestamp":1631273700,"choices":{"B":"hash-distributed on PurchaseKey","A":"replicated","D":"hash-distributed on IsOrderFinalized","C":"round-robin"},"question_text":"You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns.\n//IMG//\n\nFactPurchase will have 1 million rows of data added daily and will contain three years of data.\nTransact-SQL queries similar to the following query will be executed daily.\n\nSELECT -\nSupplierKey, StockItemKey, IsOrderFinalized, COUNT(*)\n\nFROM FactPurchase -\n\nWHERE DateKey >= 20210101 -\n\nAND DateKey <= 20210131 -\nGROUP By SupplierKey, StockItemKey, IsOrderFinalized\nWhich table distribution will minimize query times?","answer_ET":"B","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0005200001.png"],"timestamp":"2021-09-10 13:35:00","answer":"B","answers_community":["B (85%)","Other"],"exam_id":67,"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/61794-exam-dp-203-topic-1-question-20-discussion/","answer_images":[]},{"id":"XJAlUFfAORWoYCk3IVZc","answer_description":"Box 1: DimEvent -\n\nBox 2: DimChannel -\n\nBox 3: FactEvents -\nFact tables store observations or events, and can be sales orders, stock balances, exchange rates, temperatures, etc\nReference:\nhttps://docs.microsoft.com/en-us/power-bi/guidance/star-schema","question_id":32,"exam_id":67,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0005400001.png","https://www.examtopics.com/assets/media/exam-media/04259/0005500001.png"],"answer":"","answer_ET":"","topic":"1","question_text":"HOTSPOT -\nFrom a website analytics system, you receive data extracts about user interactions such as downloads, link clicks, form submissions, and video plays.\nThe data contains the following columns.\n//IMG//\n\nYou need to design a star schema to support analytical queries of the data. The star schema will contain four tables including a date dimension.\nTo which table should you add each column? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"poster":"gssd4scoder","comment_id":"467233","content":"It seems to be correct","upvote_count":"73","timestamp":"1650858960.0"},{"poster":"DingDongSingSong","comments":[{"comment_id":"622107","poster":"sdegcp","timestamp":"1671973740.0","content":"Question says,including a dimTime table so we need to design only 3 tables.","upvote_count":"4"},{"poster":"allagowf","timestamp":"1680077700.0","upvote_count":"1","content":"the question is clear try to read it again, there is no need to design DIM_DATE","comment_id":"682469"}],"content":"What is this question? It is poorly written. I couldn't even understand what's being asked here. It talks about 4 tables, yet the answer shows 3. Then, the columns mentioned in the question don't match the column/attributes shown in the 3 tables noted in the answer.","timestamp":"1663763880.0","upvote_count":"27","comment_id":"572327"},{"comment_id":"1183256","poster":"dgerok","content":"Correct answer","timestamp":"1727343600.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1177539","content":"Correct answer","poster":"Alongi","timestamp":"1726756380.0"},{"poster":"hassexat","comment_id":"1001226","upvote_count":"1","content":"Correct provided answer","timestamp":"1709799420.0"},{"content":"EventCategory -> dimEvent\nchannelGrouping -> dimChannel\nTotalEvents -> factEven","poster":"kkk5566","timestamp":"1709474460.0","comment_id":"997581","upvote_count":"1"},{"comment_id":"894957","timestamp":"1699707180.0","poster":"mamahani","upvote_count":"1","content":"DimEvent / DimChannel / FactEvents"},{"comment_id":"795418","content":"Category or Group will go with the table DIM, Total Events will go with the Fact","timestamp":"1690908480.0","upvote_count":"2","poster":"SHENOOOO"},{"poster":"Deeksha1234","comment_id":"646425","content":"correct","upvote_count":"1","timestamp":"1676316480.0"},{"content":"EventCategory -> dimEvent\nchannelGrouping -> dimChannel\nTotalEvents -> factEven","upvote_count":"4","poster":"Rrk07","comment_id":"607900","timestamp":"1669530600.0"},{"poster":"Dothy","comment_id":"600112","upvote_count":"1","content":"EventCategory -> dimEvent\nchannelGrouping -> dimChannel\nTotalEvents -> factEven","timestamp":"1668176040.0"},{"content":"EventCategory ==> dimEvent\nchannelGrouping ==> dimChannel\nTotalEvents ==> factEvent\n\nExplanation:\nA bit of knowledge of Google Analytics Universal helps to understand this question. eventCategory, eventAction and eventLabel all contain information about the event/action done on the website, and can be logically be grouped together. ChannelGrouping is about how the user came on the website (through Google, and advertisement, an email link, etc.) and is not related to events at all. It therefore would make sense to put it in a second dim table.","comment_id":"598452","poster":"JJdeWit","upvote_count":"4","timestamp":"1667894640.0"},{"poster":"Mahesh_mm","content":"Answer is correct","timestamp":"1656328620.0","upvote_count":"4","comment_id":"510316"},{"comments":[{"poster":"manquak","content":"It is supposed to contain 4 tables. Date, Event, Fact so the logical conclusion would be to include the channel dimension. If it were up to me though I'd use the channel as a degenerate dimension and store it in fact table if it's the only information that we have provided.","comment_id":"436891","upvote_count":"3","timestamp":"1646123580.0"},{"upvote_count":"1","poster":"Seansmyrke","comment_id":"558301","content":"I mean if you think about it, ChannelName (facebook,google,youtube), ChannelType (paid media, free posts, ads), ChannleDelivery (chrome, etc etc). Just thinking out loud","timestamp":"1661709600.0"}],"upvote_count":"4","timestamp":"1646059260.0","poster":"laszek","comment_id":"434589","content":"I would add ChannelGrouping to DimEvents table. What would DimChannel table contain? only one column? No sense to me"}],"answers_community":[],"timestamp":"2021-08-29 14:41:00","unix_timestamp":1630240860,"url":"https://www.examtopics.com/discussions/microsoft/view/60955-exam-dp-203-topic-1-question-21-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0005600001.png"],"isMC":false},{"id":"BOc7mPZ4XayFHqs5ecGj","answers_community":["A (78%)","B (22%)"],"isMC":true,"answer_description":"","choices":{"A":"Yes","B":"No"},"url":"https://www.examtopics.com/discussions/microsoft/view/62349-exam-dp-203-topic-1-question-22-discussion/","question_images":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\nYou need to prepare the files to ensure that the data copies quickly.\nSolution: You convert the files to compressed delimited text files.\nDoes this meet the goal?","exam_id":67,"discussion":[{"comments":[{"timestamp":"1642117800.0","upvote_count":"5","comment_id":"523198","poster":"ANath","content":"The answer should be A.\nhttps://azure.microsoft.com/en-gb/blog/increasing-polybase-row-width-limitation-in-azure-sql-data-warehouse/"},{"comment_id":"458219","comments":[{"content":"i think when compression is in question we should go for parquet/avro bcz only they give compression","poster":"dakku987","comment_id":"1101347","upvote_count":"1","timestamp":"1703057760.0"}],"content":"After reading the other questions oh this topic I go with A because the relevant part seems to be the compression.","upvote_count":"4","poster":"Thij","timestamp":"1633520160.0"}],"comment_id":"457699","timestamp":"1633437300.0","upvote_count":"18","poster":"Fahd92","content":"They said you need to prepare the files to copy, maybe the mean we should make them less than 1MB ? so it will be A else would be B !!!!"},{"content":"Selected Answer: B\nNO, what would accelarate the copying is using polybase and hence multiple splitted files.","upvote_count":"1","poster":"Amino23","timestamp":"1743400080.0","comment_id":"1413959"},{"poster":"thespy","comment_id":"1359116","upvote_count":"1","timestamp":"1740033840.0","content":"Selected Answer: B\nParquet for column level compression and better compression"},{"upvote_count":"1","poster":"Pey1nkh","timestamp":"1739709600.0","comment_id":"1357237","content":"Selected Answer: B\nWhile compressing delimited text files reduces file size, it does not optimize for fast loading into Azure Synapse Analytics.Use Parquet format !"},{"timestamp":"1735462380.0","upvote_count":"1","content":"Selected Answer: A\nYes, converting the files to compressed delimited text files is a good solution to ensure the data copies quickly to Azure Synapse Analytics.","poster":"iam_momo88","comment_id":"1333414"},{"content":"Selected Answer: A\nCorrect Answer: A","comment_id":"1318452","poster":"EmnCours","upvote_count":"1","timestamp":"1732687080.0"},{"upvote_count":"1","poster":"akhil5432","comment_id":"972243","content":"Selected Answer: A\n\"a\" is correct option","timestamp":"1726815120.0"},{"content":"Selected Answer: A\nAnswer is yes","upvote_count":"1","comment_id":"997582","timestamp":"1726815120.0","poster":"kkk5566"},{"timestamp":"1726815120.0","upvote_count":"2","comment_id":"1053019","content":"The answer is A\nCompression doesn't not only help to reduce the size or space occupied by a file in a storage but also increases the speed of file movement during transfer","poster":"moneytime"},{"poster":"SHENOOOO","comment_id":"795421","timestamp":"1675277340.0","content":"Selected Answer: A\nA will do the job","upvote_count":"3"},{"timestamp":"1669482120.0","upvote_count":"1","content":"Delimited text file is true,","poster":"Rrk07","comment_id":"727700"},{"comment_id":"688984","timestamp":"1665195240.0","upvote_count":"4","content":"Selected Answer: A\nFor the fastest load, use compressed delimited text files\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices","poster":"greenlever"},{"upvote_count":"1","content":"Selected Answer: A\nyes, answer is A","poster":"Deeksha1234","comment_id":"646721","timestamp":"1660477260.0"},{"comments":[{"poster":"dgerok","timestamp":"1711453740.0","comment_id":"1183267","upvote_count":"1","content":"PolyBase can load more than 1 mln rows since SQL Server 2019\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver16"}],"poster":"Janisys","timestamp":"1658823540.0","comment_id":"637281","upvote_count":"3","content":"Selected Answer: A\nPolyBase can't load rows that have more than 1,000,000 bytes of data. When you put data into the text files in Azure Blob storage or Azure Data Lake Store, they must have fewer than 1,000,000 bytes of data. This byte limitation is true regardless of the table schema.\nAll file formats have different performance characteristics. For the fastest load, use compressed delimited text files. Split large compressed files into smaller compressed files."},{"poster":"Deepshikha1228","comment_id":"635980","timestamp":"1658659200.0","content":"A is correct ,with copy command\nPolyBase COPY\nNeeds CONTROL permission Relaxed permission\nHas row width limits No row width limit\nNo delimiters within text Supports delimiters in text\nFixed line delimiter Supports custom column and row delimiters\nComplex to set up in code Reduces amount of code","upvote_count":"2"},{"timestamp":"1654663740.0","upvote_count":"4","comment_id":"613068","poster":"objecto","content":"Selected Answer: A\nIt's just a copy to storage so zipping it will work fine."},{"content":"It says about files compression. which will reduce the file size. so Answer is correct","timestamp":"1653626340.0","upvote_count":"1","poster":"Rrk07","comment_id":"607908"},{"timestamp":"1650980640.0","comment_id":"592475","upvote_count":"2","poster":"Muishkin","content":"A text file seems to be too simple an answer however true as per the microsoft link.I was thinking of parquet/avro files"},{"comments":[{"upvote_count":"3","content":"I initially thought so too, however isn't this limit only relevant to PolyBase copy? It is not mentioned which method is used to transfer the data so you could fit more than 1mb into a column in the table if you want to, you just have to use something else e.g. COPY command.","poster":"kamil_k","timestamp":"1647018900.0","comment_id":"565677"}],"upvote_count":"3","comment_id":"564181","content":"Selected Answer: B\nFrom the question: \"75% of the rows contain description data that has an average length of 1.1 MB\". You can't \nFrom the documentation: \"When you put data into the text files in Azure Blob storage or Azure Data Lake Store, they must have fewer than 1,000,000 bytes of data.\"\nSo 75% of rows aren't good for a delimited text files... why you said answer is yes?","poster":"Massy","timestamp":"1646845500.0"},{"upvote_count":"2","timestamp":"1643273220.0","comment_id":"533612","content":"Selected Answer: A\ncorrect answer.","poster":"PallaviPatel"},{"upvote_count":"1","timestamp":"1640611440.0","comment_id":"510321","content":"A is correct","poster":"Mahesh_mm"},{"timestamp":"1639211220.0","upvote_count":"2","poster":"alexleonvalencia","content":"Selected Answer: A\nCorrecto","comment_id":"499202"},{"poster":"rashjan","content":"Selected Answer: A\ncorrect because compression","comment_id":"496072","upvote_count":"1","timestamp":"1638885960.0"},{"timestamp":"1634667420.0","upvote_count":"1","poster":"Odoxtoom","content":"Consider this sets one question:\nWhat should you do to improve loading times?\nWhat | Yes | No |\ncompressed | O | O |\ncolumnstore | O | O |\n> 1MB | O | O |\n\nSo now answers should be clear","comment_id":"464783"},{"content":"As per Microsoft \n\nRow size and data type limits\nPolyBase loads are limited to rows smaller than 1 MB. It cannot be used to load to VARCHR(MAX), NVARCHAR(MAX), or VARBINARY(MAX). For more information, see Azure Synapse Analytics service capacity limits.\n\nWhen your source data has rows greater than 1 MB, you might want to vertically split the source tables into several small ones. Make sure that the largest size of each row doesn't exceed the limit. The smaller tables can then be loaded by using PolyBase and merged together in Azure Synapse Analytics.","comment_id":"462971","poster":"HaliBrickclay","upvote_count":"2","timestamp":"1634369700.0"},{"content":"The answer should be 'yes\" \nAll file formats have different performance characteristics. For the fastest load, use compressed delimited text files. The difference between UTF-8 and UTF-16 performance is minimal.","comment_id":"460353","poster":"jamesraju","timestamp":"1633924500.0","upvote_count":"1"},{"comment_id":"456446","content":"correct Answer is B","upvote_count":"1","poster":"RinkiiiiiV","timestamp":"1633243380.0"},{"poster":"gk765","timestamp":"1632392220.0","comment_id":"450136","upvote_count":"3","content":"Correct Answer is B. There is limit of 1MB when it comes to the row length. Hence you have to modify the files to ensure the row size is less than 1MB"},{"timestamp":"1632025740.0","upvote_count":"1","content":"Answer is correct","comment_id":"447394","poster":"kolakone"}],"timestamp":"2021-09-19 06:29:00","topic":"1","answer_images":[],"answer_ET":"A","question_id":33,"unix_timestamp":1632025740,"answer":"A"},{"id":"OTQFPMoYcLrRjmfrFHgw","answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/61378-exam-dp-203-topic-1-question-23-discussion/","answer_ET":"B","answer_images":[],"answer_description":"","discussion":[{"timestamp":"1644673020.0","comment_id":"545899","content":"From the documentation, loads to heap table are faster than indexed tables. So, better to use heap table than columnstore index table in this case.\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index#heap-tables","upvote_count":"12","poster":"bhanuprasad9331"},{"poster":"Odoxtoom","upvote_count":"9","timestamp":"1634667420.0","comment_id":"464782","content":"Consider this sets one question:\nWhat should you do to improve loading times?\nWhat | Yes | No |\ncompressed | O | O |\ncolumnstore | O | O |\n> 1MB | O | O |\n\nSo now answers should be clear","comments":[{"upvote_count":"6","comment_id":"669818","timestamp":"1663238640.0","content":"I Think what he tried to show was:\nSet Answer Matrix\n\nWhat should you do to improve loading times?\nWhat | Yes | No |\ncompressed | X | O |\ncolumnstore | O | X |\n> 1MB | O | X |\n\nSo all three variations of this question and x is marking the correct answer.","poster":"tbhtp"},{"timestamp":"1634909460.0","upvote_count":"11","content":"Can You explain this in more details?","poster":"Julius7000","comment_id":"466161"},{"timestamp":"1638904380.0","content":"I really didn't understand this , can you explain?","poster":"helly13","comment_id":"496275","upvote_count":"7","comments":[{"timestamp":"1660028160.0","content":"it's a virtualized chart that the guy want to simplified the question set.","poster":"aurorafang","comment_id":"644383","comments":[{"comment_id":"664353","upvote_count":"6","timestamp":"1662710520.0","content":"but it becomes more complicated with this chart haha","poster":"ML_Novice"}],"upvote_count":"2"}]}]},{"comment_id":"1357238","timestamp":"1739709960.0","content":"Selected Answer: A\n- A columnstore index stores columns separately, allowing queries to only read relevant columns, making queries much faster!\n- Columnstore indexes automatically compress data, reducing storage size and I/O.\n- This is especially useful for large text data like descriptions and numerical values","poster":"Pey1nkh","upvote_count":"1"},{"upvote_count":"1","comment_id":"1319776","poster":"moize","timestamp":"1732894020.0","content":"Selected Answer: A\nMoi je vote A). Selon Copilot : Oui, copier les fichiers dans une table avec un index columnstore dans Azure Synapse Analytics peut r√©pondre √† l'objectif de copier les donn√©es rapidement. Les index columnstore sont con√ßus pour optimiser les performances des requ√™tes et la compression des donn√©es, ce qui est particuli√®rement utile pour les grandes quantit√©s de donn√©es."},{"content":"Selected Answer: B\nCorrect Answer: B","poster":"EmnCours","upvote_count":"1","timestamp":"1732687200.0","comment_id":"1318453"},{"poster":"kkk5566","upvote_count":"1","content":"Selected Answer: B\nAnswer is no ,u use HEAP idx","timestamp":"1693742940.0","comment_id":"997588"},{"timestamp":"1674504120.0","content":"For fast loading to a table, using a staging table which is a heap table.","upvote_count":"1","poster":"youngbug","comment_id":"785785"},{"upvote_count":"1","comment_id":"783749","timestamp":"1674337560.0","poster":"DindaS","content":"its always recommended to load the data into a staging where the table should be a heap table and data will be loaded using ROUND_ROBIN mechanism"},{"timestamp":"1660477320.0","upvote_count":"1","comment_id":"646723","content":"B is right","poster":"Deeksha1234"},{"upvote_count":"5","comment_id":"637282","content":"Correct Answer: B\nTo achieve the fastest loading speed for moving data into a data warehouse table, load data into a staging table. Define the staging table as a heap and use round-robin for the distribution option","poster":"Janisys","timestamp":"1658824080.0"},{"upvote_count":"1","poster":"Deepshikha1228","comment_id":"636006","timestamp":"1658661480.0","content":"B is right"},{"poster":"Amsterliese","upvote_count":"4","timestamp":"1649396340.0","comment_id":"582692","comments":[{"content":"Yes load to a table without indexes for faster load right?","comment_id":"592477","upvote_count":"1","timestamp":"1650980820.0","poster":"Muishkin"}],"content":"Columnstore index would be used for faster reading, but the question is only about faster loading. So for faster loading you want the least possible overhead. So the answer should be no. Am I right?"},{"content":"Selected Answer: B\nB is correct","timestamp":"1646040240.0","poster":"lionurag","comment_id":"557963","upvote_count":"3"},{"comment_id":"533614","upvote_count":"1","content":"Selected Answer: B\nB is correct.","timestamp":"1643273400.0","poster":"PallaviPatel"},{"comment_id":"523988","content":"NO is the answer.","upvote_count":"1","poster":"DE_Sanjay","timestamp":"1642230840.0"},{"comment_id":"510324","content":"B is correct","timestamp":"1640611560.0","upvote_count":"1","poster":"Mahesh_mm"},{"comment_id":"496074","timestamp":"1638886080.0","content":"Selected Answer: B\nCorrect Answer: No.","poster":"rashjan","upvote_count":"2"},{"timestamp":"1633514940.0","comment_id":"458168","upvote_count":"3","content":"No, The index will expand the time of insertion","poster":"sachabess79"},{"poster":"michalS","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/guidance-for-loading-data. \"For the fastest load, use compressed delimited text files.\"","upvote_count":"1","comment_id":"437893","comments":[{"content":"But the row size also need to be < 1 MB\nSo, files need to be modified to make all rows < 1 MB\nAnswer: NO","comment_id":"442916","poster":"umeshkd05","timestamp":"1631353860.0","upvote_count":"4","comments":[{"content":"In other words, i think that 100GB is much to much for the columnstore index memorywise. The documentation in unclear with the context of this particular question, but i think the ansewer is NO, as ithe given answer is the wrong idea anyways.","upvote_count":"1","comment_id":"466168","timestamp":"1634909940.0","poster":"Julius7000"},{"comment_id":"450138","poster":"gk765","timestamp":"1632392340.0","upvote_count":"2","content":"Correct answer should be NO"},{"upvote_count":"1","comment_id":"466166","content":"Not Row size, row NUMBER have to be at maximum of 1,048,576 rows. \n\"When there is memory pressure, the columnstore index might not be able to achieve maximum compression rates. This effects query performance.\"","poster":"Julius7000","timestamp":"1634909760.0"}]}],"timestamp":"1630587480.0"}],"question_id":34,"choices":{"B":"No","A":"Yes"},"topic":"1","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\nYou need to prepare the files to ensure that the data copies quickly.\nSolution: You copy the files to a table that has a columnstore index.\nDoes this meet the goal?","unix_timestamp":1630587480,"isMC":true,"timestamp":"2021-09-02 14:58:00","answers_community":["B (80%)","A (20%)"],"exam_id":67,"question_images":[]},{"id":"SmpOy1l4rM2Gf0sLmntj","unix_timestamp":1631627640,"answers_community":["B (100%)"],"answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/62031-exam-dp-203-topic-1-question-24-discussion/","answer_description":"","exam_id":67,"answer_images":[],"timestamp":"2021-09-14 15:54:00","isMC":true,"answer_ET":"B","question_id":35,"question_images":[],"discussion":[{"comment_id":"444596","poster":"Gilvan","timestamp":"1631627640.0","content":"No, rows need to have less than 1 MB. A batch size between 100 K to 1M rows is the recommended baseline for determining optimal batch size capacity.","upvote_count":"13"},{"comment_id":"514917","content":"PolyBase can't load rows that have more than 1,000,000 bytes of data. When you put data into the text files in Azure Blob storage or Azure Data Lake Store, they must have fewer than 1,000,000 bytes of data. This byte limitation is true regardless of the table schema.","comments":[{"comments":[{"comment_id":"1136051","content":"Azure synapse analytics use polybase by default.","timestamp":"1706640000.0","upvote_count":"1","poster":"saqib839"}],"poster":"kamil_k","comment_id":"565678","timestamp":"1647019200.0","content":"is it stated anywhere that we have to use PolyBase? What about COPY command?","upvote_count":"2"},{"poster":"amarG1996","upvote_count":"2","comment_id":"514918","timestamp":"1641117540.0","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices#prepare-data-in-azure-storage"}],"poster":"amarG1996","upvote_count":"8","timestamp":"1641117540.0"},{"upvote_count":"2","poster":"moize","comment_id":"1319777","content":"Selected Answer: B\nSelon Copilot : Non, modifier les fichiers pour garantir que chaque ligne fait plus de 1 Mo n'est pas n√©cessairement la meilleure solution pour garantir une copie rapide des donn√©es vers Azure Synapse Analytics. En fait, cela pourrait m√™me compliquer le processus de copie et de traitement des donn√©es.","timestamp":"1732894200.0"},{"content":"Selected Answer: B\nCorrect Answer: B","upvote_count":"1","poster":"EmnCours","comment_id":"1318454","timestamp":"1732687260.0"},{"comment_id":"997590","upvote_count":"1","content":"Selected Answer: B\nAnswer is no","timestamp":"1693743000.0","poster":"kkk5566"},{"timestamp":"1670157720.0","poster":"vigilante89","comment_id":"735081","upvote_count":"2","content":"Selected Answer: B\nB is correct!!!"},{"poster":"Deeksha1234","content":"B is correct, agree with explanation by Amar","upvote_count":"1","comment_id":"646724","timestamp":"1660477500.0"},{"timestamp":"1643273460.0","upvote_count":"4","poster":"PallaviPatel","content":"Selected Answer: B\nB is correct.","comment_id":"533616"},{"timestamp":"1640611980.0","upvote_count":"1","poster":"Mahesh_mm","content":"Answer is No","comment_id":"510328"},{"comment_id":"496075","timestamp":"1638886140.0","poster":"rashjan","content":"Selected Answer: B\nCorrect Answer: No.","upvote_count":"2"},{"poster":"Odoxtoom","upvote_count":"1","content":"Consider this sets one question:\nWhat should you do to improve loading times?\nWhat | Yes | No |\ncompressed | O | O |\ncolumnstore | O | O |\n> 1MB | O | O |\n\nSo now answers should be clear","comments":[{"timestamp":"1637744220.0","comment_id":"485792","comments":[{"poster":"Bishtu","content":"Yes \nNo\nNo","upvote_count":"2","comment_id":"509027","timestamp":"1640423100.0"}],"upvote_count":"5","poster":"Aslam208","content":"@Odoxtoom, can you please explain your answer and specify based on this matrix which option is correct."}],"comment_id":"464781","timestamp":"1634667360.0"}],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\nYou need to prepare the files to ensure that the data copies quickly.\nSolution: You modify the files to ensure that each row is more than 1 MB.\nDoes this meet the goal?","topic":"1","choices":{"A":"Yes","B":"No"}}],"exam":{"isMCOnly":false,"isBeta":false,"provider":"Microsoft","id":67,"numberOfQuestions":384,"name":"DP-203","lastUpdated":"12 Apr 2025","isImplemented":true},"currentPage":7},"__N_SSP":true}