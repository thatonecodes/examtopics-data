{"pageProps":{"questions":[{"id":"JBJ9CIwB4un82mUR3HXD","question_text":"HOTSPOT -\nYou are designing an Azure Data Factory solution that will download up to 5 TB of data from several REST APIs.\nThe solution must meet the following staging requirements:\n✑ Ensure that the data can be landed quickly and in parallel to a staging area.\n✑ Minimize the need to return to the API sources to retrieve the data again should a later activity in the pipeline fail.\nThe solution must meet the following analysis requirements:\n✑ Ensure that the data can be loaded in parallel.\n✑ Ensure that users and applications can query the data without requiring an additional compute engine.\nWhat should you include in the solution to meet the requirements? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"content":"Look at this: https://docs.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features\n\nWhen you activate the staging feature, first the data is copied from the source data store to the staging storage (bring your own Azure Blob or Azure Data Lake Storage Gen2). Next, the data is copied from the staging to the sink data store. The copy activity automatically manages the two-stage flow for you, and also cleans up temporary data from the staging storage after the data movement is complete.","upvote_count":"2","poster":"Marcus1612","timestamp":"1633017540.0","comment_id":"455013"},{"comment_id":"338258","poster":"anamaster","upvote_count":"2","content":"correct, but the explanation for synapse is that ASA allows querying","timestamp":"1618756860.0"}],"answer_ET":"","answers_community":[],"unix_timestamp":1618756860,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0019200001.jpg"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0019300001.jpg"],"isMC":false,"topic":"2","answer_description":"Box 1: Azure Blob storage -\nWhen you activate the staging feature, first the data is copied from the source data store to the staging storage (bring your own Azure Blob or Azure Data Lake\nStorage Gen2).\n\nBox 2: Azure Synapse Analytics -\nThe Azure Synapse Analytics connector in copy activity provides built-in data partitioning to copy data in parallel.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse","answer":"","timestamp":"2021-04-18 16:41:00","exam_id":66,"url":"https://www.examtopics.com/discussions/microsoft/view/50404-exam-dp-201-topic-2-question-39-discussion/","question_id":101},{"id":"9S0p8NKPn5wFbK9ZX9X8","answer":"A","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0015000007.jpg"],"question_text":"You are planning a design pattern based on the Lambda architecture as shown in the exhibit.\n//IMG//\n\nWhich Azure service should you use for the hot path?","topic":"2","answer_ET":"A","isMC":true,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0014900001.png"],"exam_id":66,"url":"https://www.examtopics.com/discussions/microsoft/view/48909-exam-dp-201-topic-2-question-4-discussion/","timestamp":"2021-04-03 11:10:00","answers_community":[],"choices":{"C":"Azure Data Factory","B":"Azure Data Lake Storage Gen2","D":"Azure Synapse Analytics","A":"Azure Databricks"},"discussion":[{"timestamp":"1621766700.0","comments":[{"poster":"111222333","content":"Sorry, I see now that Azure Stream Analytics is not a proposed solution, so Databricks is the best option among others.","upvote_count":"4","timestamp":"1621766820.0","comment_id":"364404"}],"comment_id":"364402","content":"Why not Azure Stream Analytics? A speed layer (hot path) analyzes data in real time. Databricks is for batch processing.\n\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/#lambda-architecture","upvote_count":"3","poster":"111222333"},{"comment_id":"327249","poster":"bdloko","content":"Streaming vs Batching...","upvote_count":"1","timestamp":"1617441000.0"}],"answer_description":"In Azure, all of the following data stores will meet the core requirements supporting real-time processing:\n✑ Apache Spark in Azure Databricks\n✑ Azure Stream Analytics\n✑ HDInsight with Spark Streaming\n✑ HDInsight with Storm\n✑ Azure Functions\n✑ Azure App Service WebJobs\nNote: Lambda architectures use batch-processing, stream-processing, and a serving layer to minimize the latency involved in querying big data.\n\nReference:\nhttps://azure.microsoft.com/en-us/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/ https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing","question_id":102,"unix_timestamp":1617441000},{"id":"jcwgiWQe94p3UJaBKOdV","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/50405-exam-dp-201-topic-2-question-40-discussion/","answers_community":[],"answer_images":[],"unix_timestamp":1618756920,"timestamp":"2021-04-18 16:42:00","exam_id":66,"question_id":103,"answer_ET":"C","answer":"C","topic":"2","question_text":"A company purchases IoT devices to monitor manufacturing machinery. The company uses an IoT appliance to communicate with the IoT devices.\nThe company must be able to monitor the devices in real-time.\nYou need to design the solution.\nWhat should you recommend?","question_images":[],"discussion":[{"upvote_count":"6","comment_id":"385319","poster":"AliceIS","content":"appears fourth time","timestamp":"1624088460.0"},{"timestamp":"1624364520.0","content":"to much duplicated","poster":"ZodiaC","comment_id":"387945","upvote_count":"2"},{"poster":"NamishBansal","upvote_count":"1","timestamp":"1620759060.0","content":"a and d are same answer","comment_id":"354961"},{"comment_id":"338259","timestamp":"1618756920.0","content":"c but edge job","upvote_count":"3","poster":"anamaster"}],"answer_description":"The Stream Analytics query language allows to perform CEP (Complex Event Processing) by offering a wide array of functions for analyzing streaming data. This query language supports simple data manipulation, aggregation and analytics functions, geospatial functions, pattern matching and anomaly detection. You can edit queries in the portal or using our development tools, and test them using sample data that is extracted from a live stream.\nNote: Stream Analytics is a cost-effective event processing engine that helps uncover real-time insights from devices, sensors, infrastructure, applications and data quickly and easily.\nMonitor and manage Stream Analytics resources with Azure PowerShell cmdlets and powershell scripting that execute basic Stream Analytics tasks.\nReference:\nhttps://cloudblogs.microsoft.com/sqlserver/2014/10/29/microsoft-adds-iot-streaming-analytics-data-production-and-workflow-services-to-azure/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction","choices":{"D":"Azure Data Factory instance using Azure Portal","A":"Azure Data Factory instance using Azure Portal","C":"Azure Stream Analytics cloud job using Azure Portal","B":"Azure Analysis Services using Microsoft Visual Studio"}},{"id":"HJSgCgEV9Ws5B4mbMWZi","answer_ET":"C","topic":"2","timestamp":"2021-03-13 17:56:00","exam_id":66,"answer_description":"Create a function, with the help of a blob trigger template, which is triggered when files are uploaded to or updated in Azure Blob storage.\nYou use a consumption plan, which is a hosting plan that defines how resources are allocated to your function app. In the default Consumption Plan, resources are added dynamically as required by your functions. In this serverless hosting, you only pay for the time your functions run. When you run in an App Service plan, you must manage the scaling of your function app.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function","unix_timestamp":1615654560,"url":"https://www.examtopics.com/discussions/microsoft/view/46914-exam-dp-201-topic-2-question-41-discussion/","answers_community":["B (100%)"],"answer_images":[],"question_id":104,"discussion":[{"upvote_count":"39","comment_id":"309861","poster":"sdas1","timestamp":"1615654560.0","content":"The solution is B - Deploy the Azure Function in a Consumption plan and use an Event Grid trigger. 1M blobs will cripple the ability of blob trigger to provide the events.\nThe EventGrid trigger is instantaneous, so it depends on your needs."},{"upvote_count":"8","comment_id":"356580","content":"It repeated question","timestamp":"1620928380.0","poster":"Wendy_DK"},{"poster":"dakku987","upvote_count":"1","content":"Selected Answer: B\nB. Deploy the Azure Function in a Consumption plan and use an Event Grid trigger.\n\nConsumption Plan: Azure Functions in a Consumption Plan automatically scales based on the number of incoming events. It is a serverless option where you only pay for the actual execution of functions.\n\nEvent Grid Trigger: Using an Event Grid trigger allows you to respond to events in Azure Blob Storage, such as new blobs being created. This is a more event-driven and scalable approach compared to polling for changes. \n\nchat gpt","timestamp":"1703610660.0","comment_id":"1106242"},{"timestamp":"1634898120.0","content":"repeated . Answer B","poster":"satyamkishoresingh","comment_id":"466087","upvote_count":"2"},{"upvote_count":"4","content":"C. Deploy the Azure Function in a Consumption plan and use a Blob trigger.","poster":"davita8","timestamp":"1619706000.0","comment_id":"345448"},{"poster":"davita8","timestamp":"1619705940.0","upvote_count":"2","comment_id":"345447","content":"B. Deploy the Azure Function in a Consumption plan and use an Event Grid trigger."},{"poster":"karma_wins","timestamp":"1619587140.0","content":"\"B\" is correct because https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp#event-grid-trigger","comments":[{"upvote_count":"1","content":"Agree with the propose solution as the url states the scenario to utilize event trigger","timestamp":"1621940520.0","comment_id":"366350","poster":"cadio30"}],"upvote_count":"4","comment_id":"344408"},{"timestamp":"1618400760.0","comment_id":"335423","upvote_count":"2","content":"The solution is D - Deploy the Azure Function in an App Service plan and use an Event Grid trigger.\nApp Service plan - ... If you need low latency in your blob triggered functions, consider running your function app in an App Service plan. SOURCE -> https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function\n Event Grid trigger - ... The Event Grid trigger also has built-in support for blob events. Use Event Grid instead of the Blob storage trigger for the following scenarios:\n...\nHigh-scale: High scale can be loosely defined as containers that have more than 100,000 blobs in them or storage accounts that have more than 100 blob updates per second.\nSOURCE - > https://docs.microsoft.com/en-gb/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp","poster":"v_gul","comments":[{"poster":"Apox","comment_id":"343916","upvote_count":"2","content":"Since minimizing costs is a requirement, and low latency is not, the correct answer should be a \"consumption plan\". Otherwise I agree with Event Grid Trigger, as this should be used if there are more than 100 000 blobs.","timestamp":"1619521740.0"}]},{"poster":"H_S","timestamp":"1615804860.0","upvote_count":"2","content":"When your function app runs in the default Consumption plan, there may be a delay of up to several minutes between the blob being added or updated and the function being triggered. If you need low latency in your blob triggered functions, consider running your function app in an App Service plan.","comment_id":"311324","comments":[{"comment_id":"319391","content":"\"You can also use an Event Grid trigger with your Blob storage account.\"\nhttps://docs.microsoft.com/en-gb/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp","timestamp":"1616607360.0","upvote_count":"1","poster":"szpinat"}]}],"isMC":true,"question_images":[],"choices":{"B":"Deploy the Azure Function in a Consumption plan and use an Event Grid trigger.","C":"Deploy the Azure Function in a Consumption plan and use a Blob trigger.","A":"Deploy the Azure Function in an App Service plan and use a Blob trigger.","D":"Deploy the Azure Function in an App Service plan and use an Event Grid trigger."},"question_text":"You are designing a real-time stream solution based on Azure Functions. The solution will process data uploaded to Azure Blob Storage.\nThe solution requirements are as follows:\n✑ Support up to 1 million blobs.\n✑ Scaling must occur automatically.\n✑ Costs must be minimized.\nWhat should you recommend?","answer":"B"},{"id":"TfFNRtwsJ1OsCAro0gfc","timestamp":"2020-05-28 15:43:00","exam_id":66,"answers_community":["D (100%)"],"discussion":[{"poster":"extraego","comment_id":"157808","upvote_count":"29","content":"The answer E is correct. It's not asking how to migrate but what tool to use for setting up a transactional replication. \"Replication can be configured by using SQL Server Management Studio\". https://docs.microsoft.com/en-us/azure/azure-sql/database/replication-to-sql-database","comments":[{"comment_id":"366821","poster":"cadio30","upvote_count":"1","content":"This is the correct answer as it needs configuration on the SSMS side to perform the replication\n\nReference: https://www.sqlshack.com/sql-server-database-migration-to-azure-sql-database-using-sql-server-transactional-replication/","timestamp":"1622002080.0"}],"timestamp":"1597379520.0"},{"content":"I think he means the updated versions of the services and tools not the data","upvote_count":"5","poster":"AhmedReda","comment_id":"119584","timestamp":"1593100320.0","comments":[{"timestamp":"1593796560.0","comment_id":"125723","poster":"MLCL","content":"Exactly","upvote_count":"2"}]},{"upvote_count":"1","poster":"dakku987","content":"Selected Answer: D\nD. SQL Server Agent for SQL Server 2017 or later\n\nExplanation:\n\nSQL Server Agent: SQL Server Agent is a component of SQL Server that enables the scheduling and automation of administrative tasks, including replication. You can use SQL Server Agent to set up and manage replication subscriptions.\nAzure SQL Database supports various types of replication, such as transactional replication or snapshot replication, depending on your synchronization requirements. SQL Server Agent is commonly used for managing replication in SQL Server environments.","timestamp":"1703610900.0","comment_id":"1106247"},{"timestamp":"1607438100.0","content":"E is the best answer though I would say SQL Data Sync is the right solution for this question","comment_id":"238353","poster":"syu31svc","upvote_count":"3"},{"upvote_count":"3","comment_id":"163859","content":"The given answer \"E: SQL Server Management Studio 17.9.1 or later\" is correct based on the remarks from: https://docs.microsoft.com/en-us/azure/azure-sql/database/replication-to-sql-database.\n\n 1. Replication can be configured by using SQL Server Management Studio or by executing Transact-SQL statements on the publisher. You cannot configure replication by using the Azure portal.\n2. Replication can only use SQL Server authentication logins to connect to Azure SQL Database.","poster":"Ikrom","timestamp":"1598124420.0"},{"timestamp":"1592393940.0","content":"I Believe the answer should be SQL Data Sync which is not there in the options","poster":"pravinDataSpecialist","comment_id":"112376","upvote_count":"3"},{"upvote_count":"3","content":"I believe the answer should be B (SQL Data sync tool)\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database","timestamp":"1590673380.0","comments":[{"upvote_count":"3","poster":"drdean","timestamp":"1591832340.0","content":"That's now what B says but were it an option I agree SQL Data Sync would be correct","comment_id":"107280"},{"timestamp":"1597378800.0","content":"SQL Server Data Tools (SSDT) allows Visual Studio to create a SQL Server database project. It is nothing to do with migration/replication. It's a development tool.","comment_id":"157803","poster":"extraego","upvote_count":"3"}],"poster":"Abhitm","comment_id":"97593"}],"answer_description":"To set up the database as a subscriber we need to configure database replication. You can use SQL Server Management Studio to configure replication. Use the latest versions of SQL Server Management Studio in order to be able to use all the features of Azure SQL Database.\nReference:\nhttps://www.sqlshack.com/sql-server-database-migration-to-azure-sql-database-using-sql-server-transactional-replication/","answer_ET":"E","isMC":true,"question_id":105,"url":"https://www.examtopics.com/discussions/microsoft/view/21521-exam-dp-201-topic-2-question-42-discussion/","topic":"2","question_text":"You plan to migrate data to Azure SQL Database.\nThe database must remain synchronized with updates to Microsoft Azure and SQL Server.\nYou need to set up the database as a subscriber.\nWhat should you recommend?","unix_timestamp":1590673380,"answer_images":[],"answer":"D","choices":{"E":"SQL Server Management Studio 17.9.1 or later","C":"Data Migration Assistant","B":"SQL Server Data Tools","A":"Azure Data Factory","D":"SQL Server Agent for SQL Server 2017 or later"},"question_images":[]}],"exam":{"numberOfQuestions":206,"id":66,"name":"DP-201","lastUpdated":"12 Apr 2025","isBeta":false,"provider":"Microsoft","isMCOnly":false,"isImplemented":true},"currentPage":21},"__N_SSP":true}