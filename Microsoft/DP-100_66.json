{"pageProps":{"questions":[{"id":"2BjCWtkLIwukv23QLFY4","question_text":"HOTSPOT -\nYou are tuning a hyperparameter for an algorithm. The following table shows a data set with different hyperparameter, training error, and validation errors.\n//IMG//\n\nUse the drop-down menus to select the answer choice that answers each question based on the information presented in the graphic.\nHot Area:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/20676-exam-dp-100-topic-3-question-63-discussion/","question_id":326,"timestamp":"2020-05-16 04:52:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0024900001.png","https://www.examtopics.com/assets/media/exam-media/04274/0025000001.png"],"isMC":false,"answers_community":[],"discussion":[{"content":"Answers looks correct to me\nDifference between Erros:\n105-095: 10\n200-085: 115\n250-100: 150\n105-100: 5 ---> This is the best H value. Agree with #4 for Q1\n400-050: 350 -> Highest Diff. So Poor for Q2","upvote_count":"41","comments":[{"poster":"snegnik","comment_id":"909206","content":"It depends on the main measure we use. If it is bias, we should find low numbers, and if it is variance, we should find a small difference. The best way to find a good trade-off between bias and variance is to have low error numbers and a small difference between test and validation errors.","timestamp":"1701254100.0","upvote_count":"1"},{"comment_id":"807305","content":"Doing the difference here makes no sense since in the #1H and #4H both has 105 so we will look to the smallest Validation error between #1H and #4H, so 100 > 95 then we will take #1H 1 as the H value to choose.","upvote_count":"1","timestamp":"1691917380.0","poster":"Yoshizn"},{"content":"This depends on the trade-off curve validation error and training error are making. If they intersect then we cannot use this logic","comment_id":"299604","poster":"akgarg00","upvote_count":"1","timestamp":"1629954360.0"},{"upvote_count":"15","timestamp":"1628251620.0","poster":"HkIsCrazY","comment_id":"284853","content":"Why would you take the difference? it makes no sense! Best H value should be option A. 105 and 95 \n\nreason - validation error in option A is 95 whereas for option D it is 100.Training error is same in both case."}],"comment_id":"117867","timestamp":"1608766860.0","poster":"pepmir"},{"content":"Why not 1 with lowest value in both training and validation?","timestamp":"1605502320.0","upvote_count":"22","comment_id":"89728","comments":[{"timestamp":"1620788940.0","content":"exactly normally training isn't greater than 70% data. if we have 50%-50% split of test and training then it's fine to have a closest match.","poster":"swatidorge","comment_id":"217690","upvote_count":"2"},{"upvote_count":"2","content":"Yes, why not 1","comment_id":"190103","timestamp":"1617084060.0","poster":"nato16"}],"poster":"Yilu"},{"timestamp":"1732045680.0","content":"used ChatGpt 4 and got this explanation which I agreed:\nThe best hyperparameters to select would be the ones that have the lowest validation error, as this indicates how well the model is likely to perform on unseen data. In this case, that would be the one with a validation error of 50.\n\nThe poorest training result would be the one with the highest training error. In this case, that would be the one with a training error of 400.\nIn general, the goal of hyperparameter tuning is to minimize the validation error, which indicates how well the model is likely to perform on unseen data. The model with the lowest variance isn't necessarily the best model. A model with high bias can have low variance, but still be inaccurate. Similarly, a model with low bias can have high variance, but still be accurate. This is known as the bias-variance tradeoff.","upvote_count":"1","comment_id":"1213907","poster":"sl_mslconsulting"},{"comment_id":"957021","content":"Based on these values, the optimal hyperparameter setting seems to be H1. It has the lowest total error when you consider both the training and validation error, which suggests it may be the best compromise between underfitting and overfitting.\n\nThe hyperparameter setting that displays the poorest training result would be H5, as it has the highest training error (TE=400), suggesting it might be underfitting to the training data.","poster":"phdykd","upvote_count":"2","timestamp":"1705717860.0"},{"upvote_count":"2","poster":"Gferreira","comment_id":"772975","timestamp":"1689111540.0","content":"chatGcP said : \nThe best results are those that have a low training error and a low validation error. In the first case, the training error is 105 and the validation error is 95, while in the second case the training error is 105 and the validation error is 100. Therefore, the first case is better, as the validation error is lower. This indicates that the model is generalizing well and is not \"memorizing\" the training data."},{"timestamp":"1681248240.0","content":"The answer should be 1 and 5. When training/testing a model, the problem of overfitting and underfitting need to be considered. In the case of the best H value. H = 1 clearly produced the best model with minimum validation error on the test dataset (which is the dataset we care about).","upvote_count":"7","poster":"Mckay_","comment_id":"692421"},{"upvote_count":"1","poster":"ning","comment_id":"610122","timestamp":"1669900380.0","content":"Poorest training result --> 5\nBest H Parameter, this question does not have enough information, we do not know the sample size for training and test data, If there are both in millions, then no one cares about 100 errors vs 500 errors, if they are only in thousands, then I will only consider 1 and 4, in this case I guess 4 is given slight better results in testing, so I will go 4"},{"upvote_count":"1","timestamp":"1665001740.0","comment_id":"581453","content":"The question is on stack exchange\n\nhttps://stats.stackexchange.com/questions/570322/how-to-choose-a-models-hyperparameters-in-terms-of-the-variance/570485#570485","poster":"David_Tadeu"},{"comment_id":"565832","poster":"synapse","timestamp":"1662930000.0","content":"The answer is 1 and 5... Why would you choose an option with the two closest error? Would you choose 300 and 299 as the best ?","upvote_count":"3"},{"timestamp":"1657848720.0","upvote_count":"1","poster":"TheCyanideLancer","content":"Agree with pepmir. 4 has least difference between validation and training result, and box 2 is about \"poorest training result\" which is by data given, 5","comment_id":"523910"},{"poster":"dija123","comment_id":"501077","timestamp":"1655176140.0","upvote_count":"2","content":"Underfitting – Validation and training error high\nOverfitting – Validation error is high, training error low\nGood fit – Validation error low, slightly higher than the training error\nUnknown fit - Validation error low, training error 'high'"},{"timestamp":"1639599420.0","upvote_count":"2","content":"We have to see which model generalizes well on test data..clearly in option 1 difference of train and test is 10..while in option 4 difference is only 5. So 4th one may generalize well .When we do train and test split , our target is to have as close train and test error along with minimum error","poster":"nit687","comment_id":"382841"},{"poster":"kty","content":"the answer is 1 and 5 \nfor those who calculate the difference between losses, \nif we have 500 and 498 we would then chose this option?","timestamp":"1631987340.0","upvote_count":"17","comment_id":"314359"},{"poster":"adbush","comment_id":"290175","timestamp":"1628929200.0","upvote_count":"3","content":"the best model is not 4, it is 1\nlooking at the difference between training and validation errors is not helpful - by this logic a model with TE 105 VE 110 would also be better than model 1. This is clearly not the case."},{"poster":"fredgu","comment_id":"234772","content":"Pepmir's explanation is correct.","timestamp":"1622789700.0","upvote_count":"1"},{"comment_id":"217684","upvote_count":"1","content":"Why not opt 2","timestamp":"1620787680.0","poster":"Pucha"},{"upvote_count":"6","comment_id":"136352","comments":[{"poster":"Paa_Kwesi","comment_id":"233881","content":"So rather this is a case of underfitting","timestamp":"1622711040.0","upvote_count":"2"}],"poster":"CleMue","timestamp":"1610797920.0","content":"This question is a weird one. The training error here is much higher than the validation error. Usually it's the other way around. Depending on the degree of overfitting, the VE can be a lot higher than the TE, but almost never smaller than the TE.\n\nStill the general rule for such a question is: \n1) Go for the H with the smallest VE\n2) The H with the highest VE is the worst. Unfortunately here H=3 and H=4 are equally bad, so doesn't make sense to choose only one of them\nw"},{"upvote_count":"3","content":"Poorest Training Result: Means highest VE that is 100.\nMore the error's its poor result right?","timestamp":"1608766620.0","poster":"pepmir","comment_id":"117863"},{"upvote_count":"1","comment_id":"117861","timestamp":"1608766440.0","content":"105 TE and 100 VE: is a classic case of OverFit.","poster":"pepmir"}],"exam_id":64,"answer_description":"Box 1: 4 -\nChoose the one which has lower training and validation error and also the closest match.\nMinimize variance (difference between validation error and train error).\n\nBox 2: 5 -\nMinimize variance (difference between validation error and train error).\nReference:\nhttps://medium.com/comet-ml/organizing-machine-learning-projects-project-management-guidelines-2d2b85651bbd","unix_timestamp":1589597520,"answer_ET":"","answer":"","topic":"3","answer_images":["https://www.examtopics.com/assets/media/exam-media/04274/0025100001.png"]},{"id":"CuSltgK9xXUtOTxygX2n","answer_description":"The two steps are present: process_step and train_step\nThe training data input is not setup correctly.\nNote:\nData used in pipeline can be produced by one step and consumed in another step by providing a PipelineData object as an output of one step and an input of one or more subsequent steps.\nPipelineData objects are also used when constructing Pipelines to describe step dependencies. To specify that a step requires the output of another step as input, use a PipelineData object in the constructor of both steps.\nFor example, the pipeline train step depends on the process_step_output output of the pipeline process step: from azureml.pipeline.core import Pipeline, PipelineData from azureml.pipeline.steps import PythonScriptStep datastore = ws.get_default_datastore() process_step_output = PipelineData(\"processed_data\", datastore=datastore) process_step = PythonScriptStep(script_name=\"process.py\", arguments=[\"--data_for_train\", process_step_output], outputs=[process_step_output], compute_target=aml_compute, source_directory=process_directory) train_step = PythonScriptStep(script_name=\"train.py\", arguments=[\"--data_for_train\", process_step_output], inputs=[process_step_output], compute_target=aml_compute, source_directory=train_directory) pipeline = Pipeline(workspace=ws, steps=[process_step, train_step])\nReference:\nhttps://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py","answer_ET":"B","answer_images":[],"topic":"3","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou create a model to forecast weather conditions based on historical data.\nYou need to create a pipeline that runs a processing script to load data from a datastore and pass the processed data to a machine learning model training script.\nSolution: Run the following code:\n//IMG//\n\nDoes the solution meet the goal?","exam_id":64,"choices":{"A":"Yes","B":"No"},"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/47680-exam-dp-100-topic-3-question-64-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0025200001.png"],"discussion":[{"comment_id":"314374","comments":[{"comment_id":"479976","poster":"ML_Novice","upvote_count":"2","content":"how the first preprocess step doesn't require any input entry?\nthanks for your response","timestamp":"1684314900.0"},{"timestamp":"1732889340.0","poster":"snegnik","comment_id":"909374","upvote_count":"1","content":"Where is input for process_step?"}],"upvote_count":"14","timestamp":"1663524180.0","poster":"kty","content":"datastore = ws.get_default_datastore()\n process_step_output = PipelineData(\"processed_data\", datastore=datastore)\n process_step = PythonScriptStep(script_name=\"process.py\",\n arguments=[\"--data_for_train\", process_step_output],\n outputs=[process_step_output],\n compute_target=aml_compute,\n source_directory=process_directory)\n train_step = PythonScriptStep(script_name=\"train.py\",\n arguments=[\"--data_for_train\", process_step_output],\n inputs=[process_step_output],\n compute_target=aml_compute,\n source_directory=train_directory)\n\n pipeline = Pipeline(workspace=ws, steps=[process_step, train_step])"},{"content":"only line#2 should be fixed: data_output = PipelineData(\"processed_data\", datastore=datastore)","poster":"dev2dev","comment_id":"321587","timestamp":"1664242620.0","upvote_count":"8"},{"upvote_count":"1","timestamp":"1732890000.0","poster":"snegnik","content":"I think this should be better\ndatastore = ws.get_default_datastore()\ninput_data = PipelineData(\"input_data\", datastore=datastore)\nprocess_step_output = PipelineData(\"processed_data\", datastore=datastore)\nmodel_output = PipelineData(\"trained_model\", datastore=datastore)\n\nprocess_step = PythonScriptStep(script_name=\"process.py\",\narguments=[\"--data_for_train\", input_data],\ninputs=[input_data],\noutputs=[process_step_output],\ncompute_target=aml_compute,\nsource_directory=process_directory)\n\ntrain_step = PythonScriptStep(script_name=\"train.py\",\narguments=[\"--processed_data\", process_step_output, \"--output_model\", model_output],\ninputs=[process_step_output],\noutputs=[model_output],\ncompute_target=aml_compute,\nsource_directory=train_directory)\n\npipeline = Pipeline(workspace=ws, steps=[process_step, train_step])","comment_id":"909385"},{"poster":"skrjha20","content":"Code should be as below.\ndatastore = ws.get_default_datastore()\n process_step_output = PipelineData(\"processed_data\", datastore=datastore)\n process_step = PythonScriptStep(script_name=\"process.py\",\n arguments=[\"--data_for_train\", process_step_output],\n outputs=[process_step_output],\n compute_target=aml_compute,\n source_directory=process_directory)\n train_step = PythonScriptStep(script_name=\"train.py\",\n arguments=[\"--data_for_train\", process_step_output],\n inputs=[process_step_output],\n compute_target=aml_compute,\n source_directory=train_directory)\n\n pipeline = Pipeline(workspace=ws, steps=[process_step, train_step])\n\nHowever in 2 nd line tried to use read_csv() which is wrong","timestamp":"1680160800.0","comment_id":"454772","upvote_count":"2"},{"timestamp":"1671979020.0","upvote_count":"2","content":"https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py\n\nThis one is incorrect since process_step_output is set up incorrectly\n\nin order to set up whole pipeline you should connect all components ( compute node, datastore) set within workspace because when code is running. The resources will generate within your Azure Machine Learning resource which mean reading data locally will not to be able to read when compute note is spawn.","comment_id":"390476","poster":"SnowCheetah"}],"question_id":327,"isMC":true,"timestamp":"2021-03-18 21:03:00","answer":"B","unix_timestamp":1616097780},{"id":"KHcHuk2dLT0LY6pKQyd8","answer":"B","unix_timestamp":1644415380,"discussion":[{"poster":"Peeking","content":"Selected Answer: B\nThere is no training step where the PipelineData (output of process_step) will be used as input.","comment_id":"837557","timestamp":"1726187580.0","upvote_count":"1"},{"timestamp":"1701437100.0","comment_id":"610129","poster":"ning","content":"Missing training step","upvote_count":"2"},{"upvote_count":"1","content":"prepped_data = OutputFileDatasetConfig(\"prepped_data\")\nthen add prepped_data to the arguments parameter","timestamp":"1694079720.0","poster":"TheYazan","comment_id":"562577"}],"isMC":true,"answers_community":["B (100%)"],"topic":"3","choices":{"A":"Yes","B":"No"},"question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0025400001.png"],"exam_id":64,"answer_description":"train_step is missing.\nReference:\nhttps://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py","question_id":328,"answer_ET":"B","url":"https://www.examtopics.com/discussions/microsoft/view/71231-exam-dp-100-topic-3-question-65-discussion/","timestamp":"2022-02-09 15:03:00","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou create a model to forecast weather conditions based on historical data.\nYou need to create a pipeline that runs a processing script to load data from a datastore and pass the processed data to a machine learning model training script.\nSolution: Run the following code:\n//IMG//\n\nDoes the solution meet the goal?","answer_images":[]},{"id":"mBIk6yRiuTsliCKoq8xd","discussion":[{"timestamp":"1648346520.0","content":"in train step the arguments=[\"--data_for_train\", data_input] should be arguments=[\"--data_for_train\", data_output]","comment_id":"452128","poster":"lander_c","upvote_count":"7"},{"upvote_count":"5","timestamp":"1641699060.0","content":"In train step, \"arguments=[\"--data_for_train\", data_input]\" is not correct.","comment_id":"402340","poster":"YipingRuan"},{"timestamp":"1723590360.0","content":"Selected Answer: B\nI think in both the steps, it should be arguments=[\"--data_for_train\", data_output]","comment_id":"1149722","comments":[{"content":"training step","upvote_count":"1","comment_id":"1149723","poster":"deyoz","comments":[{"upvote_count":"1","timestamp":"1723590480.0","poster":"deyoz","content":"i mean not just in training step.","comment_id":"1149724"}],"timestamp":"1723590420.0"}],"upvote_count":"1","poster":"deyoz"},{"content":"Missing data input for training step","upvote_count":"1","poster":"ning","comment_id":"610128","timestamp":"1669901040.0"},{"upvote_count":"1","timestamp":"1650615840.0","comment_id":"466049","content":"in second line it's a 'rawdatastore` which is not created before","poster":"jkuz","comments":[{"timestamp":"1683503400.0","comment_id":"713405","poster":"silva_831","content":"agree, since rawdatastore is not created in context.","upvote_count":"1"}]},{"comment_id":"422495","upvote_count":"1","poster":"VJPrakash","content":"I think the answer should be 'YES'.\nThe value is passed from process to train. How its being used depends on the script","timestamp":"1644479580.0"},{"content":"the \"date_input' line doesn't seem to be needed here","poster":"BigSoda","comment_id":"375794","upvote_count":"3","timestamp":"1638786000.0"}],"answer_description":"Note: Data used in pipeline can be produced by one step and consumed in another step by providing a PipelineData object as an output of one step and an input of one or more subsequent steps.\nCompare with this example, the pipeline train step depends on the process_step_output output of the pipeline process step: from azureml.pipeline.core import Pipeline, PipelineData from azureml.pipeline.steps import PythonScriptStep datastore = ws.get_default_datastore() process_step_output = PipelineData(\"processed_data\", datastore=datastore) process_step = PythonScriptStep(script_name=\"process.py\", arguments=[\"--data_for_train\", process_step_output], outputs=[process_step_output], compute_target=aml_compute, source_directory=process_directory) train_step = PythonScriptStep(script_name=\"train.py\", arguments=[\"--data_for_train\", process_step_output], inputs=[process_step_output], compute_target=aml_compute, source_directory=train_directory) pipeline = Pipeline(workspace=ws, steps=[process_step, train_step])\nReference:\nhttps://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py","unix_timestamp":1622967600,"exam_id":64,"url":"https://www.examtopics.com/discussions/microsoft/view/54696-exam-dp-100-topic-3-question-66-discussion/","timestamp":"2021-06-06 10:20:00","topic":"3","answer_images":[],"answer":"B","choices":{"A":"Yes","B":"No"},"isMC":true,"answers_community":["B (100%)"],"question_id":329,"answer_ET":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou create a model to forecast weather conditions based on historical data.\nYou need to create a pipeline that runs a processing script to load data from a datastore and pass the processed data to a machine learning model training script.\nSolution: Run the following code:\n//IMG//\n\nDoes the solution meet the goal?","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0025500001.png"]},{"id":"0TXqXBHQGSV2rPE3jAaP","answer_description":"","isMC":true,"discussion":[{"timestamp":"1724260740.0","content":"Selected Answer: B\nThe Environment class was not imported and there was no .from_conda_specification or python dependencies. The answer is 'No'","comment_id":"817049","upvote_count":"3","poster":"Peeking"},{"upvote_count":"1","timestamp":"1723833420.0","content":"No. from azureml.core import Environment\nfrom azureml.train.sklearn import SKLearn\n\n# Create a Python environment for the experiment\nenv = Environment.from_conda_specification(name='sklearn-env', file_path='./scripts/myenv.yml')\n\n# Specify the required packages\nenv.python.conda_dependencies.add_pip_package(\"scikit-learn\")\n\n# Create an estimator for the experiment\nestimator = SKLearn(source_directory='./scripts',\n compute_target=aml_compute,\n entry_script='train.py',\n environment_definition=env)","poster":"phdykd","comment_id":"811082"},{"comment_id":"739463","content":"Selected Answer: B\nNowhere in the code does this solution include the required packages for model training","upvote_count":"2","timestamp":"1717872900.0","poster":"michaelmorar"},{"poster":"jkuz","comments":[{"content":"src = ScriptRunConfig(source_directory=project_folder,\n script='train.py',\n compute_target=my_compute_target,\n environment=myenv)\n\nIndeed the environment seems to be missing:\nhttps://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-set-up-training-targets?view=azure-ml-py","timestamp":"1718430600.0","upvote_count":"2","poster":"Arend78","comment_id":"745829"}],"timestamp":"1682155560.0","comment_id":"466067","upvote_count":"4","content":"Deprecated. \nhttps://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py\n\nIf ignore deprecated, it seems to work (see the link)."},{"upvote_count":"4","poster":"medsimus","content":"outdated\nhttps://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.sklearn.sklearn?view=azure-ml-py","comment_id":"326597","timestamp":"1664713920.0"}],"answer":"B","answers_community":["B (100%)"],"exam_id":64,"answer_ET":"B","timestamp":"2021-04-02 14:32:00","answer_images":[],"question_id":330,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have a Python script named train.py in a local folder named scripts. The script trains a regression model by using scikit-learn. The script includes code to load a training data file which is also located in the scripts folder.\nYou must run the script as an Azure ML experiment on a compute cluster named aml-compute.\nYou need to configure the run to ensure that the environment includes the required packages for model training. You have instantiated a variable named aml- compute that references the target compute cluster.\nSolution: Run the following code:\n//IMG//\n\nDoes the solution meet the goal?","choices":{"A":"Yes","B":"No"},"url":"https://www.examtopics.com/discussions/microsoft/view/48804-exam-dp-100-topic-3-question-67-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0025600001.png"],"topic":"3","unix_timestamp":1617366720}],"exam":{"lastUpdated":"12 Apr 2025","name":"DP-100","provider":"Microsoft","numberOfQuestions":512,"isBeta":false,"isMCOnly":false,"id":64,"isImplemented":true},"currentPage":66},"__N_SSP":true}