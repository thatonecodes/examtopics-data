{"pageProps":{"questions":[{"id":"3CnbUaAmQhBxGWAesO5z","url":"https://www.examtopics.com/discussions/microsoft/view/61731-exam-az-304-topic-3-question-43-discussion/","question_images":[],"timestamp":"2021-09-09 02:42:00","question_text":"You are designing an Azure Cosmos DB solution that will host multiple writable replicas in multiple Azure regions.\nYou need to recommend the strongest database consistency level for the design. The solution must meet the following requirements:\nâœ‘ Provide a latency-based Service Level Agreement (SLA) for writes.\nâœ‘ Support multiple regions.\nWhich consistency level should you recommend?","exam_id":53,"question_id":131,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04027/0018800001.jpg"],"answer":"A","isMC":true,"topic":"3","unix_timestamp":1631148120,"choices":{"C":"session","D":"consistent prefix","A":"bounded staleness","B":"strong"},"answer_description":"Each level provides availability and performance tradeoffs. The following image shows the different consistency levels as a spectrum.\n\nNote: The service offers comprehensive 99.99% SLAs which covers the guarantees for throughput, consistency, availability and latency for the Azure Cosmos DB\nDatabase Accounts scoped to a single Azure region configured with any of the five Consistency Levels or Database Accounts spanning multiple Azure regions, configured with any of the four relaxed Consistency Levels.\nIncorrect Answers:\nB: Strong consistency for accounts with regions spanning more than 5000 miles (8000 kilometers) is blocked by default due to high write latency. To enable this capability please contact support.\nReference:\nhttps://azure.microsoft.com/en-us/support/legal/sla/cosmos-db/v1_3/ https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels#consistency-levels-and-latency","answer_ET":"A","discussion":[{"content":"Bounded staleness\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\nBounded staleness is frequently chosen by globally distributed applications that expect low write latencies but require total global order guarantee. Bounded staleness is great for applications featuring group collaboration and sharing, stock ticker, publish-subscribe/queueing etc.","upvote_count":"18","comment_id":"441698","timestamp":"1631148120.0","poster":"booboo2k"},{"upvote_count":"5","comment_id":"441728","content":"B: Strong ? \nhttps://docs.microsoft.com/ja-jp/azure/cosmos-db/consistency-levels","comments":[{"poster":"examineezer","content":"Not if the regions span more than 5000 miles","upvote_count":"1","timestamp":"1636495740.0","comment_id":"475092"}],"timestamp":"1631155260.0","poster":"exampass999"},{"timestamp":"1647359520.0","upvote_count":"1","poster":"Dawn7","content":"Selected Answer: A\nSeems correct","comment_id":"568498"},{"comments":[{"poster":"examineezer","comment_id":"514580","timestamp":"1641052740.0","content":"See also here for Bounded Staleness write latency SLA \n\nhttps://azure.microsoft.com/en-gb/support/legal/sla/cosmos-db/v1_3/","upvote_count":"2"}],"content":"\"Provide a latency-based Service Level Agreement (SLA) for writes.\" is actually the key here I think.\n\nFor Azure Cosmos accounts configured with strong consistency with more than one region, the write latency is equal to two times round-trip time (RTT) between any of the two farthest regions, plus 10 milliseconds at the 99th percentile. High network RTT between the regions will translate to higher latency for Cosmos DB requests since strong consistency completes an operation only after ensuring that it has been committed to all regions within an account.\n\nThe exact RTT latency is a function of speed-of-light distance and the Azure networking topology. Azure networking doesn't provide any latency SLAs for the RTT between any two Azure regions, however it does publish Azure network round-trip latency statistics.\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels","comment_id":"514579","poster":"examineezer","upvote_count":"1","timestamp":"1641052560.0"},{"content":"Strong consistency for accounts with regions spanning more than 5000 miles (8000 kilometers) is blocked by default due to high write latency. To enable this capability please contact support.\n\nIn session consistency, within a single client session reads are guaranteed to honor the consistent-prefix, monotonic reads, monotonic writes, read-your-writes, and write-follows-reads guarantees.\n\nIn consistent prefix option, updates that are returned contain some prefix of all the updates, with no gaps. Consistent prefix consistency level guarantees that reads never see out-of-order writes.\n\nAnswer is A","timestamp":"1633350480.0","comment_id":"457118","upvote_count":"3","poster":"syu31svc"},{"comment_id":"452464","upvote_count":"2","poster":"icklebear","content":"A bounded stateless due to - https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels#write-latency-and-strong-consistency this paragrpah \"For Azure Cosmos accounts configured with strong consistency with more than one region, the write latency is equal to two times round-trip time (RTT) between any of the two farthest regions, plus 10 milliseconds at the 99th percentile. High network RTT between the regions will translate to higher latency for Cosmos DB requests since strong consistency completes an operation only after ensuring that it has been committed to all regions within an account.\nThe exact RTT latency is a function of speed-of-light distance and the Azure networking topology. Azure networking doesn't provide any latency SLAs for the RTT between any two Azure regions, however it does publish Azure network round-trip latency statistics.\"","timestamp":"1632749640.0"}],"answers_community":["A (100%)"]},{"id":"AK3IX6zvhbO2UpDUgdSR","choices":{"D":"Replace Azure Databricks with Azure Machine Learning.","C":"Replace Azure Data Factory with CRON jobs that use AzCopy.","A":"Replace Azure Synapse Analytics and Azure Analysis Services with SQL Server on an Azure virtual machine.","B":"Replace Azure Synapse Analytics with Azure SQL Database Hyperscale."},"topic":"3","timestamp":"2020-12-03 14:31:00","question_id":132,"unix_timestamp":1607002260,"question_images":["https://www.examtopics.com/assets/media/exam-media/04027/0014300001.jpg","https://www.examtopics.com/assets/media/exam-media/04027/0014300002.png"],"answer":"C","exam_id":53,"answer_images":[],"question_text":"You are reviewing an Azure architecture as shown in the Architecture exhibit. (Click the Architecture tab.)\n//IMG//\n\nThe estimated monthly costs for the architecture are shown in the Costs exhibit. (Click the Costs tab.)\n//IMG//\n\nThe log files are generated by user activity to Apache web servers. The log files are in a consistent format. Approximately 1 GB of logs are generated per day.\nMicrosoft Power BI is used to display weekly reports of the user activity.\nYou need to recommend a solution to minimize costs while maintaining the functionality of the architecture.\nWhat should you recommend?","answer_description":"","answers_community":["C (100%)"],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/38657-exam-az-304-topic-3-question-5-discussion/","answer_ET":"C","discussion":[{"comment_id":"241593","timestamp":"1607777400.0","content":"The question states that \"The log files are in a consistent format.\" So, no ETL is required. The files can be copied directly using the AzCopy command.","upvote_count":"71","poster":"gcpjay"},{"timestamp":"1613615100.0","upvote_count":"22","comments":[{"timestamp":"1634247120.0","comment_id":"462253","poster":"tteesstt","content":"I made a mistake once and kept ADF running for 2 days. The pipeline was pretty basic - to copy logs to SQL Database. The bill for those 2 days was over 100$.","upvote_count":"2","comments":[{"content":"I left Dedicated SQL Pool running for 2 days and it costed 500$ ðŸ¥²","upvote_count":"1","timestamp":"1694325120.0","poster":"ShivaUdari","comment_id":"1003721"}]}],"comment_id":"293017","poster":"JDA","content":"The answer is C. In the real world, I'd be asking some hard questions about why ADF was costing so much with that traffic volume ..."},{"poster":"GaneshPP","upvote_count":"1","comment_id":"690741","content":"Even though C is the right answer, its like recommending - don't use cloud.","timestamp":"1665381360.0"},{"timestamp":"1661015100.0","upvote_count":"1","content":"\"Replace Azure Data Factory with CRON jobs that use AzCopy.\" but where are the Cron jobs using AzCopy running?","comments":[{"comment_id":"1003722","upvote_count":"1","content":"Majority Apache Servers on Linux, so CRON can be used.","timestamp":"1694325300.0","poster":"ShivaUdari"}],"comment_id":"649505","poster":"pingpongset"},{"poster":"AubinBakana","timestamp":"1660206960.0","upvote_count":"2","content":"Selected Answer: C\nThe fact that they are using ADF & Synapse Analysis is a huge alert. They both pretty much do the same in most case. A good place to start to look. And if you are just copying log files to ADL, of course Azure Data Factory with CRON jobs has to be the answer","comment_id":"645363"},{"upvote_count":"1","comment_id":"568136","content":"Selected Answer: C\nAnswer is c","timestamp":"1647322200.0","poster":"Dawn7"},{"content":"Selected Answer: C\nAZCopy","upvote_count":"1","comment_id":"564564","timestamp":"1646890800.0","poster":"plmmsg"},{"comment_id":"511312","poster":"ShivaNatarajan","timestamp":"1640708100.0","content":"If not ADF which service is going to be used to orchestrate this data pipeline ?","upvote_count":"1"},{"upvote_count":"3","timestamp":"1640255700.0","content":"Appere on exam 23-dec-2021","comment_id":"507731","poster":"Dpejic"},{"poster":"syu31svc","timestamp":"1633302840.0","comment_id":"456785","content":"I'd take C for 2 reasons\n\n1) Data Factory is the most expensive service used so it should be replaced\n2) Log files in consistent format so just use AzCopy will do","upvote_count":"4"},{"comment_id":"455372","upvote_count":"3","timestamp":"1633072380.0","content":"Was in exam today 1-10-2021. I passed with score 896. I chose C","poster":"dkltruong88"},{"content":"Correct. Also appears in the practice questions for AZ304","upvote_count":"2","comment_id":"441925","poster":"Tachinsky","timestamp":"1631185800.0"},{"upvote_count":"1","timestamp":"1630299720.0","poster":"Gautam1985","comment_id":"435086","content":"Correct"},{"poster":"StarkStrange","upvote_count":"1","timestamp":"1629794040.0","comment_id":"430614","content":"Have used a similar arch in one of our use case. ADF doesn't cost this much, anyways, with this bill probably the given ans is correct."},{"comment_id":"427143","content":"Since, ADF is being used for trivial task of just copying the log files already in consistent format. Hence, it need to go to save the cost.","poster":"az_architect","upvote_count":"3","timestamp":"1629336840.0"},{"upvote_count":"4","timestamp":"1625195700.0","comment_id":"396499","content":"if we notice in the diagram Azure Data Factory is where the most money is spent, and the solution is to minimize costs hence we need to look for a solution that deals with data factory and makes sense so I would say C is the right answer.","poster":"AravindITGuy"},{"timestamp":"1614303960.0","content":"https://medium.com/@nimishrao/moving-a-file-in-seconds-to-your-azure-data-lake-generation-2-using-azcopy-6dde114258","comment_id":"299484","upvote_count":"2","poster":"atwind"},{"poster":"FK2974","upvote_count":"8","comment_id":"280765","timestamp":"1612120500.0","content":"I've ask this question with MS trainer and the correct answer is C!!"},{"comment_id":"276031","content":"C. Replace Azure Data Factory with CRON jobs that use AzCopy.","poster":"glam","upvote_count":"5","timestamp":"1611585840.0"},{"poster":"milind8451","upvote_count":"3","comment_id":"274578","timestamp":"1611405660.0","content":"Right answer. If log files are consistent then AzCopy is enough to copy data and it will save cost of ADF."},{"timestamp":"1607002260.0","comments":[{"content":"i agree another one of those weird questions check the budgets example here talks about 1 TB per day for the entire month and the costs are so cheap.\nhttps://docs.microsoft.com/en-us/azure/data-factory/plan-manage-costs","timestamp":"1607349840.0","upvote_count":"2","poster":"uzairahm007","comment_id":"237383"}],"content":"Not sure how ADF could be almost 5000 per month if it get only 1gb per day, almost imposible even if it host VM with self hosted IR it will be cheaper, I maybe better replace Synapse and AAS, but yes if in some case it will take 5000 per month then better replace ADF","comment_id":"234011","poster":"DIMONANDREY","upvote_count":"3"}]},{"id":"KDQvtemwRi7P8vukog4y","unix_timestamp":1607873640,"timestamp":"2020-12-13 16:34:00","question_id":133,"answer_images":[],"answers_community":["D (100%)"],"discussion":[{"comments":[{"comments":[{"timestamp":"1627466460.0","content":"wrong: \"SQL Managed Instance has private IP addresses in its own virtual network\", ref\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/connect-application-instance#connect-inside-a-different-vnet","comment_id":"416132","upvote_count":"9","poster":"tita_tovenaar"}],"content":"No, it should be E. SQLMI doesn't support private IP address","upvote_count":"1","timestamp":"1620484560.0","comment_id":"352476","poster":"QiangQiang"},{"upvote_count":"9","comment_id":"449765","timestamp":"1632347760.0","poster":"sjai","content":"At a high level, SQL Managed Instance is a set of service components. These components are hosted on a dedicated set of isolated virtual machines that run inside the customer's virtual network subnet. These machines form a virtual cluster.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/connectivity-architecture-overview"}],"upvote_count":"46","timestamp":"1607873640.0","content":"Correct answer","comment_id":"242691","poster":"H"},{"timestamp":"1623635520.0","comment_id":"381462","poster":"erickim007","content":"the given answer is correct. Cannot have Azure SQL because it does not support VNET integration. SQL MI supports it.","comments":[{"comment_id":"538954","content":"Azure SQL Database can leverage private endpoint to support VNET integration\nhttps://docs.microsoft.com/en-us/azure/virtual-network/vnet-integration-for-azure-services#private-link-and-private-endpoints","poster":"bruncili","upvote_count":"1","timestamp":"1643820240.0"}],"upvote_count":"14"},{"comment_id":"550211","upvote_count":"1","poster":"jr_luciano","content":"Selected Answer: D\nCorrect answer","timestamp":"1645187580.0"},{"content":"The correct answer is D as one of the features related to Azure SQL Managed Instance is native Vnet integration - a native virtual network (VNet) implementation that addresses common security concerns\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview","upvote_count":"1","comment_id":"541721","poster":"Akakentavr","timestamp":"1644152760.0"},{"content":"E - Azure SQL Database with elastic pools would be correct as well since it can have a private endpoint in the VNET","poster":"c_groleau","comment_id":"534969","upvote_count":"1","comments":[{"upvote_count":"1","poster":"TheBank","content":"But its not native VNET Integration as with Azure SQL Managed Instance","timestamp":"1652887080.0","comment_id":"603354"}],"timestamp":"1643404440.0"},{"upvote_count":"2","timestamp":"1640175540.0","poster":"Eitant","comment_id":"507061","content":"Selected Answer: D\nCorrect answer"},{"poster":"syu31svc","comment_id":"454122","timestamp":"1632918300.0","content":"\"Support automatic patching and version updates\"\n\nAnswer is D for sure","upvote_count":"5"},{"timestamp":"1632001140.0","poster":"GuxMAN","content":"I think it's a trick question. I have been studying for the exam and I don't remember reading from Azure SQL Database MI, but I have read from Azure SQL MI, and only the above three options (A, B and C) indicate SQL Server, and the question is about the options for migrate SQL Server instances. The most reasonable option (following the observation if it is DB or Server) is C (IaaS), but this does not support automatic updates and patches. I'll see if this question will be in my close exam.","upvote_count":"1","comment_id":"447288"},{"comments":[{"poster":"examineezer","comment_id":"511294","comments":[{"comment_id":"526088","content":"Azure supports automated backup and automated patching for an Azure VM with SQL Server installed. It must be either SQL Server 2014 or 2016+, and with the SQL IaaS Agent extension in full management mode. A pair of VMs with AGs can support the multiple SQL instances required. Not strictly mentioned in the question, but reasonable implication is existing on-premises SQL licensing. With Software Assurance in a migration scenario both on-premises and cloud VMs are covered for 180 days. An Azure VM as the SQL AG failover server would not require a license if used purely for business continuity. Using SQL on an Azure VM with multiple instances could be the best solution.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/automated-backup\nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/automated-patching\nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/sql-server-iaas-agent-extension-automate-management\nI think my answer will be C.","upvote_count":"2","poster":"yyuryyucicuryyforme","timestamp":"1642451340.0"}],"upvote_count":"1","content":"This doesnt support automatic patching and version updates to SQL Server though does it?","timestamp":"1640707320.0"}],"content":"C. SQL Server Infrastructure-as-a-Service (IaaS) virtual machine (VM)\n\nRequirement - Provide a native VNET with private IP addressing.\nThis cannot be achieved on Azure SQL. With VNET integration you can restrict only the services within that VNET can reach DB, but still they have to reach the public endpoint of the DB (xx.database.windows.net) and not to a private IP.","comment_id":"437820","upvote_count":"1","poster":"SRJ11","timestamp":"1630581540.0"},{"upvote_count":"2","comment_id":"425028","content":"D is the correct answer.\n\nEverything is in this section https://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview#key-features-and-capabilities","poster":"teehex","timestamp":"1629001020.0"},{"content":"Answer is D , refer the link and check in that 'Key features and capabilities\" , all the requirements are meet. \nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview","poster":"Shashprasad","timestamp":"1626478320.0","upvote_count":"2","comment_id":"408138"},{"poster":"Oracleist","comments":[{"content":"1. Single-tenant with dedicated underlying infrastructure (compute, storage). 2. Automatic patch","comment_id":"503099","poster":"parkranger","upvote_count":"1","timestamp":"1639676760.0"},{"upvote_count":"2","poster":"reubems","comment_id":"365465","content":"Exactly, on that link you mention it appears as capabilities this:\nIsolated environment (VNet integration, single tenant service, dedicated compute and storage)\n\nSo the right answer is Managed Instance.","timestamp":"1621848420.0"}],"content":"Managed Instance(Be in a single-tenant environment with dedicated underlying infrastructure (compute, storage))\n\nread \n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview","timestamp":"1620914580.0","comment_id":"356453","upvote_count":"4"},{"content":"it should be E.","poster":"QiangQiang","upvote_count":"1","comment_id":"352475","timestamp":"1620484500.0"},{"content":"You guys seem to have missed the most important part of the question \" Be in a single-tenant environment with dedicated underlying infrastructure (compute, storage)\" Managed instances runs on shared infrastructure. Only IAAS can allow you to choose reserved instances","comment_id":"334316","poster":"nooranikhan","upvote_count":"4","timestamp":"1618269060.0","comments":[{"content":"read better\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview\n\nKey features and capabilities","poster":"Oracleist","timestamp":"1618815780.0","comment_id":"338664","upvote_count":"7"},{"upvote_count":"2","timestamp":"1622032200.0","comment_id":"367124","poster":"4tune","content":"reserved instances doesn't equate to dedicated or isolated"}]},{"comments":[{"comment_id":"321173","content":"because of \"native Vnet with private addressing\".\nSQL Managed Instance can be injected in customer's VNet. \nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/features-comparison","upvote_count":"2","timestamp":"1616766480.0","poster":"yaiba"}],"comment_id":"300502","upvote_count":"3","content":"why isn't \"E. Azure SQL Database with elastic pools\" a valid option then?","poster":"dcprice","timestamp":"1614467640.0"},{"poster":"glam","timestamp":"1611587100.0","content":"D. Azure SQL Database Managed Instance","comment_id":"276041","upvote_count":"3"},{"comment_id":"274579","timestamp":"1611405780.0","poster":"milind8451","content":"Right ans.","upvote_count":"3"},{"timestamp":"1609018380.0","poster":"kopper2019","comment_id":"252922","upvote_count":"3","content":"correct"}],"exam_id":53,"answer_ET":"D","choices":{"C":"SQL Server Infrastructure-as-a-Service (IaaS) virtual machine (VM)","D":"Azure SQL Database Managed Instance","E":"Azure SQL Database with elastic pools","A":"SQL Server in a Docker container running on Azure Container Instances (ACI)","B":"SQL Server in Docker containers running on Azure Kubernetes Service (AKS)"},"question_images":[],"answer":"D","question_text":"You deploy Azure App Service Web Apps that connect to on-premises Microsoft SQL Server instances by using Azure ExpressRoute. You plan to migrate the\nSQL Server instances to Azure.\nMigration of the SQL Server instances to Azure must:\nâœ‘ Support automatic patching and version updates to SQL Server.\nâœ‘ Provide automatic backup services.\nâœ‘ Allow for high-availability of the instances.\nâœ‘ Provide a native VNET with private IP addressing.\nâœ‘ Encrypt all data in transit.\nâœ‘ Be in a single-tenant environment with dedicated underlying infrastructure (compute, storage).\nYou need to migrate the SQL Server instances to Azure.\nWhich Azure service should you use?","topic":"3","url":"https://www.examtopics.com/discussions/microsoft/view/39772-exam-az-304-topic-3-question-6-discussion/","isMC":true,"answer_description":""},{"id":"y8bC5CnOIjglvsFya79D","answer_ET":"A","isMC":true,"exam_id":53,"url":"https://www.examtopics.com/discussions/microsoft/view/39154-exam-az-304-topic-3-question-7-discussion/","answer_images":[],"answer_description":"","question_id":134,"answer":"A","timestamp":"2020-12-07 22:24:00","choices":{"C":"Hot","A":"Cool","B":"Archive"},"discussion":[{"poster":"nasascientist","content":"Cool tier is correct answer.. hot tier will be costlier and accessing data from archive tier takes time.","timestamp":"1607376240.0","upvote_count":"42","comment_id":"237651"},{"timestamp":"1613505540.0","comments":[{"upvote_count":"10","timestamp":"1614541140.0","comment_id":"300950","content":"I'll see you when you get there...","poster":"Sam12"},{"poster":"us3r","comment_id":"545750","upvote_count":"6","timestamp":"1644661560.0","content":"As I walk through the valley of the shadow of death\nI take a look at my life, and realize there's nothin' left"}],"content":"Coolio","comment_id":"292064","poster":"crraburn","upvote_count":"21"},{"timestamp":"1648573920.0","content":"Selected Answer: A\narchive is not immediately accessible, so it's cool","poster":"examrobot","upvote_count":"1","comment_id":"577752"},{"upvote_count":"1","poster":"Netspud","content":"Selected Answer: A\nA is correct","timestamp":"1648521180.0","comment_id":"577194"},{"content":"Both Cool and Hot are online so for sure Cool.","comment_id":"575295","timestamp":"1648248540.0","poster":"azahran","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: A\nSorry, I will go with A. Cool.","poster":"Dawn7","comment_id":"568139","timestamp":"1647322320.0"},{"comments":[],"timestamp":"1647322260.0","content":"Selected Answer: B\nI will go with B","comment_id":"568138","poster":"Dawn7","upvote_count":"1"},{"poster":"Dawn7","comment_id":"547815","content":"Selected Answer: A\nA is correct.","timestamp":"1644935760.0","upvote_count":"1"},{"content":"Cool ----> correct answerr","comment_id":"512956","timestamp":"1640831760.0","upvote_count":"1","poster":"Carroyo826"},{"content":"\"always available for immediate access\" yet \"minimize storage costs\"\n\nAnswer is A","upvote_count":"4","poster":"syu31svc","comment_id":"457666","timestamp":"1633434360.0"},{"upvote_count":"3","comment_id":"448130","timestamp":"1632133260.0","poster":"nkv","content":"came in exam on 20-sep-21, I passed, i choose given answer"},{"timestamp":"1631034120.0","comments":[{"poster":"Spooky7","comment_id":"451032","upvote_count":"1","content":"In cool tier you also have immediate access, just little bit slower than in hot tier. In archive tier you don't have an access and you need request it first.","timestamp":"1632506400.0"},{"poster":"Sathya22","content":"Read the question carefully . It was mentioned that files will be accessed RARELY . So COOL is the correct answer","upvote_count":"1","comment_id":"456527","timestamp":"1633257600.0"}],"content":"Hot because the data in Blob storage is always available for immediate access","poster":"asahel","comment_id":"441015","upvote_count":"1"},{"comment_id":"415630","timestamp":"1627406640.0","content":"was in exam today, answer is correct","upvote_count":"3","poster":"BlackZeros"},{"poster":"SnakePlissken","upvote_count":"4","timestamp":"1622975340.0","comment_id":"375909","content":"Wow, a fundamentals question!"},{"upvote_count":"3","poster":"glam","content":"A. Cool","timestamp":"1611587220.0","comment_id":"276043"}],"answers_community":["A (83%)","B (17%)"],"question_text":"You plan to store data in Azure Blob storage for many years. The stored data will be accessed rarely.\nYou need to ensure that the data in Blob storage is always available for immediate access. The solution must minimize storage costs.\nWhich storage tier should you use?","question_images":[],"unix_timestamp":1607376240,"topic":"3"},{"id":"XN332w2xZyaVblrbbKIX","question_text":"DRAG DROP -\nYou are designing a virtual machine that will run Microsoft SQL Server and will contain two data disks. The first data disk will store log files, and the second data disk will store data. Both disks are P40 managed disks.\nYou need to recommend a caching policy for each disk. The policy must provide the best overall performance for the virtual machine while preserving integrity of the SQL data and logs.\nWhich caching policy should you recommend for each disk? To answer, drag the appropriate policies to the correct disks. Each policy may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nSelect and Place:\n//IMG//","timestamp":"2020-12-01 04:26:00","question_id":135,"isMC":false,"exam_id":53,"answers_community":[],"answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/sql/virtual-machines-windows-sql-performance","url":"https://www.examtopics.com/discussions/microsoft/view/38186-exam-az-304-topic-3-question-8-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04027/0014600001.png"],"answer":"","unix_timestamp":1606793160,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04027/0014700001.png"],"topic":"3","answer_ET":"","discussion":[{"comment_id":"231542","timestamp":"1606793160.0","upvote_count":"97","content":"The answers provided are correct, here is the explanation and supporting websites:\nLog: Noneâ€”Log files have primarily write-heavy operations. Therefore, they do not benefit from the ReadOnly cache.\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/premium-storage-performance#disk-caching\n\nData: readonlyâ€”If you have separate storage pools for the log and data files, enable read caching only on the storage pool for the data files.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices","poster":"M4gnet1k","comments":[{"comment_id":"454029","content":"Good find there!","upvote_count":"4","timestamp":"1632910860.0","poster":"syu31svc"},{"poster":"AubinBakana","comment_id":"645373","timestamp":"1660208340.0","upvote_count":"1","content":"Thank you"}]},{"timestamp":"1611588000.0","poster":"glam","comment_id":"276047","content":"Log: Noneâ€”\nData: readonlyâ€”","upvote_count":"7"},{"content":"Set host caching to read-only for data file disks.\nSet host caching to none for log file disks.","poster":"ranjitklive","comment_id":"574073","upvote_count":"2","timestamp":"1648097460.0"},{"timestamp":"1640193960.0","comment_id":"507266","poster":"Droz36","upvote_count":"1","content":"https://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices-storage\n\nCorrect"},{"timestamp":"1612807380.0","upvote_count":"4","comment_id":"286361","poster":"azurecert2021","content":"correct."},{"comment_id":"252964","poster":"kopper2019","timestamp":"1609024380.0","content":"correct\n- Use premium SSDs for the best price/performance advantages. Configure Read only cache for data files and no cache for the log file.","upvote_count":"4"},{"comment_id":"236969","upvote_count":"3","timestamp":"1607312580.0","content":"Following are the recommended disk cache settings for data disks,\n\nTABLE 9\nDisk caching setting recommendation on when to use this setting\nNone Configure host-cache as None for write-only and write-heavy disks.\nReadOnly Configure host-cache as ReadOnly for read-only and read-write disks.\nReadWrite Configure host-cache as ReadWrite only if your application properly handles writing cached data to persistent disks when needed.\n\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/premium-storage-performance#disk-caching","poster":"JustDiscussing"},{"upvote_count":"3","poster":"xAlx","timestamp":"1607092860.0","comment_id":"234975","content":"Correct\nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices#disks-guidance\n\nIf you are using separate disks for data and log files, enable read caching on the data disks hosting your data files and TempDB data files. This can result in a significant performance benefit. Do not enable caching on the disk holding the log file as this causes a minor decrease in performance.","comments":[{"upvote_count":"2","comment_id":"526093","timestamp":"1642451760.0","poster":"yyuryyucicuryyforme","content":"Indeed \nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices-storage#data-file-caching-policies\nata disk Enable Read-only caching for the disks hosting SQL Server data files.\nReads from cache will be faster than the uncached reads from the data disk.\nUncached IOPS and throughput plus Cached IOPS and throughput will yield the total possible performance available from the virtual machine within the VMs limits, but actual performance will vary based on the workload's ability to use the cache (cache hit ratio).\nTransaction log disk Set the caching policy to None for disks hosting the transaction log. There is no performance benefit to enabling caching for the Transaction log disk, and in fact having either Read-only or Read/Write caching enabled on the log drive can degrade performance of the writes against the drive and decrease the amount of cache available for reads on the data drive."}]}]}],"exam":{"lastUpdated":"12 Apr 2025","numberOfQuestions":237,"isImplemented":true,"provider":"Microsoft","name":"AZ-304","isBeta":false,"id":53,"isMCOnly":false},"currentPage":27},"__N_SSP":true}