{"pageProps":{"questions":[{"id":"1iYgGDrxlhLD8vSPQOd2","topic":"2","answer_ET":"B","url":"https://www.examtopics.com/discussions/microsoft/view/99422-exam-az-305-topic-2-question-26-discussion/","answers_community":["B (91%)","9%"],"question_text":"You plan to migrate on-premises MySQL databases to Azure Database for MySQL Flexible Server.\n\nYou need to recommend a solution for the Azure Database for MySQL Flexible Server configuration. The solution must meet the following requirements:\n\n• The databases must be accessible if a datacenter fails.\n• Costs must be minimized.\n\nWhich compute tier should you recommend?","choices":{"B":"General Purpose","C":"Memory Optimized","A":"Burstable"},"timestamp":"2023-02-16 15:37:00","answer_description":"","answer_images":[],"answer":"B","exam_id":54,"discussion":[{"content":"Selected Answer: B\nB is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-high-availability#limitations\nHere are some considerations to keep in mind when you use high availability:\n- High availability isn't supported in the burstable compute tier.","comments":[{"content":"But the question did not ask for high availability","poster":"study_for_azure","timestamp":"1677558360.0","comment_id":"824382","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"824385","timestamp":"1677558540.0","poster":"study_for_azure","content":"sry, my bad, it is Zone Redundant High Availability. It should be B"}]},{"timestamp":"1677596280.0","poster":"zellck","comment_id":"824944","upvote_count":"12","content":"Got this in Feb 2023 exam."}],"upvote_count":"27","timestamp":"1677328920.0","comment_id":"821479","poster":"zellck"},{"content":"B. General Purpose\n\nThe General Purpose compute tier provides a balance between performance and cost. It is suitable for most common workloads and offers a good combination of CPU and memory resources. It provides high availability and fault tolerance by utilizing Azure's infrastructure across multiple datacenters. This ensures that the databases remain accessible even if a datacenter fails.\n\nThe Burstable compute tier (option A) is designed for workloads with variable or unpredictable usage patterns. It provides burstable CPU performance but may not be the optimal choice for ensuring availability during a datacenter failure.\n\nThe Memory Optimized compute tier (option C) is designed for memory-intensive workloads that require high memory capacity. While it provides excellent performance for memory-bound workloads, it may not be necessary for minimizing costs or meeting the specified requirements.","poster":"Tr619899","comment_id":"898446","timestamp":"1684164300.0","upvote_count":"13"},{"upvote_count":"2","poster":"[Removed]","timestamp":"1731424920.0","comment_id":"1310684","content":"Selected Answer: B\nB is correct"},{"content":"it's B\nZone redundancy:\n\nThe zone-redundancy option is only available in regions that support availability zones.\n\nZone-redundancy is not supported for:\n\nAzure Database for PostgreSQL – Single Server SKU.\nBurstable compute tier.\nRegions with single-zone availability.\nwith burstable we don'T have availability options to choose (tested) thats mean another database replica in another zone wich we will swith after datacenter outage, the only thing that our data is stored in zone redundant so we won't loose it if we have datacenter outage","upvote_count":"1","timestamp":"1724435820.0","comment_id":"1271403","poster":"KarimaMaf"},{"upvote_count":"1","comment_id":"1206386","timestamp":"1714809960.0","poster":"Chenn","content":"Based on the requirements, the Burstable compute tier would be the most suitable choice for the Azure Database for MySQL Flexible Server configuration.\nThe Burstable tier is ideal for workloads that don’t need continuous full compute capacity. It provides better cost optimization controls with the ability to stop/start the server. Moreover, the flexible server architecture allows users to opt for high availability within a single availability zone and across multiple availability zones, ensuring that the databases remain accessible even if a datacenter fails."},{"timestamp":"1714384620.0","poster":"Lazylinux","upvote_count":"1","content":"Selected Answer: B\nI would go for B\nFlexible Server is a fully managed production-ready database service designed for more granular control and flexibility over database management functions and configuration settings. The flexible server architecture allows users to opt for high availability within a single availability zone and across multiple availability zones. Flexible servers provide better cost optimization controls with the ability to stop/start the server and burstable compute tier, ideal for workloads that don't need full compute capacity continuously. Flexible Server also supports reserved instances allowing you to save up to 63% cost, which is ideal for production workloads with predictable compute capacity requirements.","comment_id":"1203946"},{"content":"I think answer is correct. https://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-service-tiers-storage","poster":"AM77","timestamp":"1712196180.0","upvote_count":"1","comment_id":"1189027"},{"upvote_count":"1","comment_id":"1188518","timestamp":"1712130600.0","poster":"Azure2020","content":"The databases must be accessible if a datacenter fails!\nAzure availability zones are at least three physically separate groups of datacenters within each Azure region.\nIf one datacenter fails then the data will be available in other datacenter in same zone.\n\nSo the answer is correct. \n\nhttps://learn.microsoft.com/en-us/azure/reliability/reliability-postgresql-flexible-server"},{"timestamp":"1709954880.0","upvote_count":"1","comment_id":"1169243","content":"Selected Answer: B\nTested in Lab, B - General Purpose is correct. \n\nBurstable doesn't support high availability. Only General Purpose or Business Critical","poster":"chair123"},{"comment_id":"1091742","poster":"ziggy1117","timestamp":"1702118460.0","upvote_count":"2","comments":[{"comment_id":"1107255","upvote_count":"2","timestamp":"1703718540.0","content":"High availability isn't supported in the burstable compute tier.\n\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-high-availability","poster":"JAUMPE","comments":[{"timestamp":"1705672620.0","poster":"TonySuccess","comment_id":"1126714","upvote_count":"1","content":"Therefore, while the Burstable compute tier offers significant cost and flexibility advantages for certain types of workloads, it is not recommended for production workloads that require consistent CPU performance. Note that the Burstable tier doesn't support functionality of creating Read Replicas and High availability feature. For such workloads and features, other compute tiers, such as the General Purpose or Business Critical are more appropriate.\n\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-service-tiers-storage#performance-limitations-of-burstable-series-instances"}]}],"content":"Selected Answer: A\nA. Burstable. It provides the lowest cost. Both Burstable and General Purpose provide Zone Redundancy\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-service-tiers-storage\nhttps://azure.microsoft.com/en-us/pricing/details/mysql/"},{"content":"Got this on Sept. 29, 2023","timestamp":"1696032660.0","poster":"Exams_Prep_2021","upvote_count":"2","comment_id":"1021125"},{"upvote_count":"3","timestamp":"1695619440.0","poster":"Forex19","comments":[{"poster":"GotDamnImIn","comment_id":"1017089","content":"How was the exam? I'm writing tomorrow on the 26th","timestamp":"1695664200.0","upvote_count":"2"}],"comment_id":"1016469","content":"I had question at 24th Sep 2023"},{"timestamp":"1695230160.0","poster":"marcellov","upvote_count":"3","comment_id":"1012520","content":"Selected Answer: B\nWhen trying to enable high availability with the Burstable tier, I see this message:\n\"High availability is not supported with the compute tier choice. If you would like to configure high availability, the compute tier will be upgraded to the General Purpose compute tier or you may choose a different compute tier by clicking 'Configure Server' below.\"\nSo, General Purpose is the answer to minimize costs."},{"comment_id":"1010798","timestamp":"1695060240.0","upvote_count":"1","content":"Selected Answer: B\nGeneral purpose provides Zone redundanty","poster":"Elecktrus"},{"upvote_count":"1","timestamp":"1689225060.0","poster":"suneitpatel","content":"Selected Answer: B\nZone redundancy requirement which General purpose provides.","comment_id":"950349"},{"timestamp":"1686445320.0","content":"Burstable is not zone redundant - so B is correct","upvote_count":"2","poster":"sk2022","comment_id":"920344"},{"comment_id":"880742","poster":"yonie","timestamp":"1682447340.0","content":"Selected Answer: B\nHigh availability isn't supported in the burstable compute tier.\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-high-availability#limitations","upvote_count":"1"},{"comment_id":"862587","poster":"MeerKatZA","upvote_count":"1","timestamp":"1680738780.0","content":"Selected Answer: B\nHas to be B, if the requirement states \"The databases must be accessible if a datacenter fails.\".\n\nChecked in portal, could not set up high availability unless I switched from burstable to Gen-Purpose."},{"poster":"EXzw","upvote_count":"1","content":"Selected Answer: B\nOnly general-purpose tier supports Zone Redundant. tested in portal.","comment_id":"851663","timestamp":"1679889300.0"},{"poster":"NotMeAnyWay","upvote_count":"3","content":"Selected Answer: B\nB. General Purpose\n\nThe General Purpose compute tier provides a balance between performance and cost. While it may not be as cost-effective as the Burstable tier, it supports high availability, which is essential for meeting the requirement of database accessibility if a datacenter fails. In addition, the General Purpose tier will generally have lower costs compared to the Memory Optimized tier. To ensure high availability, you will need to configure zone-redundant backups and enable the geo-redundant backup option.","timestamp":"1679586240.0","comment_id":"848394"},{"comment_id":"825213","timestamp":"1677613200.0","poster":"VBK8579","upvote_count":"3","content":"Selected Answer: B\nB. General Purpose is recommended as it balances performance and cost, and provides options for automatic failover to ensure high availability in case of datacenter failure."},{"comment_id":"819684","content":"Selected Answer: B\nHigh availability is not supported with the compute tier choice (Burstable). If you would like to configure high availability, the compute tier will be upgraded to the General Purpose compute tier or higher","timestamp":"1677182640.0","poster":"abxc","upvote_count":"4"},{"poster":"4PHL","comment_id":"811096","upvote_count":"2","content":"A correct.\nBurstable compute: Optimize your compute costs with low-cost burstable compute SKUs that let you pay for performance only when you need it. Scale compute or storage on demand dynamically as your workload needs change. \n\nhttps://techcommunity.microsoft.com/t5/azure-database-for-mysql-blog/announcing-flexible-server-for-azure-database-for-mysql/ba-p/1686617#:~:text=Flexible%20Server%20is%20a%20new%20deployment%20option%20for,new%20ways%20to%20optimize%20cost%20with%20stop%2Fstart%20capabilities.","timestamp":"1676581200.0"},{"comment_id":"810760","timestamp":"1676558220.0","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/overview","poster":"Coolfrenesie","upvote_count":"3"}],"question_id":96,"isMC":true,"question_images":[],"unix_timestamp":1676558220},{"id":"IkUKoTIHBwrYMDZYmIoL","answer_ET":"B","discussion":[{"upvote_count":"25","poster":"jozir8","timestamp":"1681887900.0","comment_id":"874330","content":"Correct answer: B\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/choose-api\n\nStore data relationally:\n- NoSQL stores data in document format\n- MongoDB stores data in a document structure (BSON format)\n\nSupport SQL Queries:\n- Apache Cassandra uses Cassandra Query Language (CQL)\n\nIf you’re looking for a managed open source relational database with high performance and geo-replication, Azure Cosmos DB for PostgreSQL is the recommended choice."},{"poster":"NotMeAnyWay","upvote_count":"10","comment_id":"950786","timestamp":"1689261900.0","content":"Selected Answer: B\nThe correct answer is B. PostgreSQL.\n\nAzure Cosmos DB's API for PostgreSQL provides full support for SQL queries, geo-replication, and allows you to store and access data relationally. It offers automatic and instant scalability, global distribution, and effortless replication of data across Azure regions, fulfilling all of your mentioned requirements.\n\nA. Apache Cassandra is a NoSQL database that does not natively support SQL queries. While it does offer some SQL-like capabilities, it is not a fully relational database.\n\nC. MongoDB is a NoSQL database and does not support the relational data model, although it does provide SQL-like query language.\n\nD. NoSQL is a type of database design that can store and retrieve data, but it isn't a specific API. Also, not all NoSQL databases support SQL queries and relational data storage."},{"timestamp":"1732697820.0","upvote_count":"1","poster":"Thanveer","content":"Selected Answer: B\nB. PostgreSQL","comment_id":"1318524"},{"upvote_count":"1","poster":"[Removed]","content":"Selected Answer: B\nB is correct","comment_id":"1310685","timestamp":"1731425100.0"},{"content":"These are databases not APIs so I don't quite understand this question.","poster":"Sephethus","timestamp":"1725983940.0","comment_id":"1281659","upvote_count":"1"},{"comment_id":"1076895","upvote_count":"3","content":"Selected Answer: B\nIf you’re looking for a managed open source relational database with high performance and geo-replication, Azure Cosmos DB for PostgreSQL is the recommended choice. To learn more, see the Azure Cosmos DB for PostgreSQL introduction.","timestamp":"1700624220.0","poster":"fire009"},{"timestamp":"1684164360.0","upvote_count":"4","content":"B. PostgreSQL\n\nAzure Cosmos DB provides support for multiple APIs, each tailored to different data models and query languages. The PostgreSQL API is well-suited for applications that require relational data storage and the ability to execute SQL queries. It offers compatibility with the PostgreSQL wire protocol and supports standard SQL syntax, allowing you to leverage your existing SQL skills and tools.\n\nAdditionally, the PostgreSQL API in Azure Cosmos DB provides built-in support for geo-replication, allowing you to replicate your data across multiple regions for high availability and disaster recovery purposes. This ensures that your data is accessible and resilient even in the event of a regional outage or failure.\n\nTherefore, the recommended API for Azure Cosmos DB in this scenario is the PostgreSQL API.","poster":"Tr619899","comment_id":"898447"},{"poster":"yonie","comment_id":"880746","upvote_count":"1","timestamp":"1682447460.0","content":"Selected Answer: B\nIf you’re looking for a managed open source relational database with high performance and geo-replication, Azure Cosmos DB for PostgreSQL is the recommended choice. To learn more, see the\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/choose-api#api-for-postgresql"},{"timestamp":"1681904100.0","upvote_count":"1","poster":"Bigbluee","content":"Selected Answer: B\nB: PostgreSQL\n\nBut finding proper info in one place.....\nI am not DB guy at all.\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/choose-api#api-for-postgresql","comment_id":"874554"},{"timestamp":"1681895460.0","upvote_count":"3","content":"API for NoSQL is native to Azure Cosmos DB.","comment_id":"874439","poster":"stdevops"}],"answer_description":"","question_id":97,"unix_timestamp":1681887900,"timestamp":"2023-04-19 09:05:00","question_text":"You are designing an app that will use Azure Cosmos DB to collate sales from multiple countries.\n\nYou need to recommend an API for the app. The solution must meet the following requirements:\n\n• Support SQL queries.\n• Support geo-replication.\n• Store and access data relationally.\n\nWhich API should you recommend?","exam_id":54,"topic":"2","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/106681-exam-az-305-topic-2-question-27-discussion/","choices":{"C":"MongoDB","B":"PostgreSQL","D":"NoSQL","A":"Apache Cassandra"},"answers_community":["B (100%)"],"question_images":[],"answer":"B"},{"id":"Mln1loCKu5GFUT1HOsYm","answer_ET":"","question_images":["https://img.examtopics.com/az-305/image189.png"],"answer_description":"","timestamp":"2023-04-19 11:21:00","exam_id":54,"unix_timestamp":1681896060,"question_text":"HOTSPOT\n-\n\nYou have an app that generates 50,000 events daily.\n\nYou plan to stream the events to an Azure event hub and use Event Hubs Capture to implement cold path processing of the events. The output of Event Hubs Capture will be consumed by a reporting system.\n\nYou need to identify which type of Azure storage must be provisioned to support Event Hubs Capture, and which inbound data format the reporting system must support.\n\nWhat should you identify? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_images":["https://img.examtopics.com/az-305/image190.png"],"answers_community":[],"discussion":[{"timestamp":"1685373840.0","upvote_count":"165","comments":[{"comment_id":"1043664","comments":[{"upvote_count":"6","content":"Bill Gates","poster":"Lazylinux","comments":[{"content":"I'd be willing to bet a lot of money that bill gates doesn't know shit about Avro LMFAO","timestamp":"1732330560.0","comment_id":"1316549","poster":"Pringlesucka","upvote_count":"2"}],"comment_id":"1204231","timestamp":"1714430940.0"}],"timestamp":"1697308680.0","upvote_count":"9","poster":"U4ea","content":"Seriously, who knows this by heart?"},{"timestamp":"1725394260.0","content":"You're not the only one.","comment_id":"1277805","poster":"josola","upvote_count":"3"},{"upvote_count":"9","poster":"maltlk","timestamp":"1705608840.0","content":"I guess we are on the same boat","comment_id":"1126154"},{"upvote_count":"18","comment_id":"1006892","content":"You are not alone","poster":"Ivantor","timestamp":"1694628360.0"},{"content":"You're not alone, some question makes me feel like I don't know anything.","upvote_count":"8","poster":"Tplenty","comment_id":"1070989","timestamp":"1700011020.0"}],"content":"Man sometimes I think I know what I'm talking about with Azure, and then I see a question like this and I question my sanity.","poster":"jspisak","comment_id":"909512"},{"timestamp":"1689262200.0","upvote_count":"40","poster":"NotMeAnyWay","comment_id":"950793","content":"1. Storage Type: Azure Data Lake Storage Gen2\n\nAzure Event Hubs Capture allows captured data to be written either to Azure Blob Storage or Azure Data Lake Storage Gen2. Given the nature of the data and its use in reporting and analysis, Azure Data Lake Storage Gen2 is the more appropriate choice because it is designed for big data analytics.\n\n2. Data format: Avro\n\nEvent Hubs Capture uses Avro format for the data it captures. Avro is a row-oriented format that is suitable for various data types, it's compact, fast, binary, and enables efficient and fast serialization of data. This makes it a good choice for Event Hubs Capture."},{"comment_id":"1317342","timestamp":"1732515000.0","poster":"Thanveer","upvote_count":"2","content":"* Event Hubs doesn't support capturing events in a premium storage account.\n* Event Hubs Capture supports any non-premium Azure storage account with support for block blobs.\n\nAnswer : \n1.Storage Type: Azure Data Lake Storage Gen2\n2. Data format: Avro"},{"content":"CORRECT","upvote_count":"1","timestamp":"1731425340.0","comment_id":"1310687","poster":"[Removed]"},{"timestamp":"1723199520.0","upvote_count":"4","comment_id":"1262865","content":"This question appeared in the exam, August 2024, I gave this same answer provided here. I scored 870","poster":"Len83"},{"poster":"23169fd","comment_id":"1236293","comments":[{"poster":"23169fd","content":"why not other options ?\nStorage Type:\nPremium block blobs: Designed for high-performance workloads but not optimized for big data analytics and hierarchical storage.\nPremium file shares: Suitable for high-performance file sharing but lacks the scalability and analytics features of ADLS Gen2.\nData Format:\nApache Parquet: Columnar storage format optimized for read-heavy operations, but not natively supported by Event Hubs Capture.\nJSON: Readable and widely used, but less efficient in terms of storage and performance compared to Avro for streaming data.","comment_id":"1236294","timestamp":"1719229140.0","upvote_count":"1"}],"content":"The given answer is correct.\nAzure Data Lake Storage Gen2 is designed for big data analytics and is highly scalable, making it suitable for storing large volumes of event data.\nAvro is a compact, fast binary format supported by Event Hubs Capture, optimized for efficiency and performance in data streaming scenarios.","upvote_count":"1","timestamp":"1719229080.0"},{"comment_id":"1214820","content":"Response from Gemini AI:\n\nStorage Type: Event Hubs Capture allows captured data to be written to either Azure Blob Storage or Azure Data Lake Storage Gen2. However, for cold path processing scenarios, which involve analyzing historical data, Azure Data Lake Storage Gen2 is the more suitable choice. It's designed for big data analytics workloads and offers better performance and scalability for working with large datasets captured from event hubs.\n\nInbound Data Format: Event Hubs Capture uses Avro format for the captured data. Avro is a widely used open-source data format specifically designed for data exchange. It's a row-oriented, binary format that provides rich data structures with inline schema definition. This makes it efficient for storage and easy for various analytics tools and reporting systems to understand and process the captured event data.","upvote_count":"1","poster":"AAsif098","timestamp":"1716281040.0"},{"comment_id":"1169247","upvote_count":"1","poster":"chair123","comments":[{"timestamp":"1709956140.0","content":"Box 2 - Avro\n\nExplanation:\n\nSupported Formats: While Event Hubs itself can handle various data formats including JSON, Avro, and Apache Parquet, Event Hubs Capture specifically writes data in Apache Avro format. This format is well-suited for cold path processing due to its:\n - Compact nature\n - Speed\n - Ability to represent complex data structures\n - Inline schema definition for easier data understanding\n\nWhy not JSON or Parquet?\nJSON: While JSON is a common data interchange format, it can be less efficient for cold path processing due to its larger size compared to Avro.\nParquet: Although Azure Stream Analytics can be used to capture Event Hubs data in Parquet format, Event Hubs Capture itself doesn't directly support Parquet.","upvote_count":"1","comment_id":"1169249","poster":"chair123"}],"timestamp":"1709956020.0","content":"Based on Gemini AI:\nBox 2-"},{"timestamp":"1709497620.0","upvote_count":"1","poster":"4fd861f","content":"For streaming Avro is made for it compared to Parquet as it row oriented format so if you have batch in the question => Parquet, Streaming => Avro","comment_id":"1165062"},{"comment_id":"1081261","poster":"fodocel235","upvote_count":"1","timestamp":"1701068100.0","content":"Correct given answers.\n\n\"Azure Data Lake Storage Gen 2\" or \"Azure Storage Account\" can be used as a Storage Account via the Portal. Just to be sure, I created a Premium Storage Account (blob) and this is - NOT - a valid option to store the Captured files.\n\nBy default Avro is selected via the Portal. Also Parquet and Delta Lake (preview) are supported via the Portal."},{"timestamp":"1701026160.0","upvote_count":"2","poster":"J404","comment_id":"1080981","content":"Correct Answer:\n- Azure Data Lake Storage Gen2\n- Apache Parquet\n\nI am thinkin rather best-practice driven rather than looking into docs. If I'd set up an analytics service in Azure, I'd prefer Databricks. In Databricks I am always working with Parquet files rather than Avro.\n\nAvro is often used in case of streaming. Single messages can be compressed and a schema is still enforced. But the question is only about analytics.\n\nWhile as I am preferring Avro in context of streaming, I am preferring Parquet for data analysis."},{"comment_id":"1055020","upvote_count":"2","timestamp":"1698368280.0","content":"avro or apache parquet both are correct answer. however apache parquet is columnar storage format that provides efficient commpression and query performance.","poster":"randy0077"},{"content":"The answer is correct.\n\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#how-event-hubs-capture-works\n\nThe capture can be in Parquet format however if you use no code editor which is outside the scope of the question.","timestamp":"1695995640.0","poster":"ntma3b","comment_id":"1020842","upvote_count":"3"},{"upvote_count":"2","poster":"Mladen_66","content":"Capture data to ADLS Gen2 in Parquet format: https://learn.microsoft.com/en-us/azure/stream-analytics/event-hubs-parquet-capture-tutorial","comment_id":"1002107","comments":[{"content":"Capture data to ADLS Gen2 in Parquet format tile.","poster":"husam421","upvote_count":"1","timestamp":"1695741360.0","comment_id":"1017910"}],"timestamp":"1694147700.0"},{"comment_id":"941610","content":"Azure Data Lake Storage Gen2 is not a premium storage account. It is a storage account type that provides a unified storage solution for both structured and unstructured data. As Premium Storage options are not supported by Event Hubs Capture","poster":"Ashfarqk","upvote_count":"1","timestamp":"1688371200.0"},{"upvote_count":"4","poster":"Tr619899","comment_id":"911475","content":"To support Event Hubs Capture, the appropriate Azure storage type is Azure Data Lake Storage Gen2. Event Hubs Capture is specifically designed to write captured events directly to Azure Data Lake Storage Gen2, providing a durable and scalable storage solution.\n\nRegarding the inbound data format that the reporting system must support, the data format supported by Event Hubs Capture is Apache Avro. Event Hubs Capture writes the captured events in Avro format by default. Therefore, the reporting system should be able to consume and process data in the Apache Avro format.\n\nSo the correct selections would be:\n\nStorage Type: Azure Data Lake Storage Gen2\nData Format: Apache Avro","timestamp":"1685555340.0"},{"content":"Answer is not correct I side and agree with the explanation by Sanaie. \nAzure Data Lake Storage Gen2, as premium storage options are not supported by Event Hubs Capture.\n\nApache Parquet is better suited for data analytics compared to Avro and JSON. \nAvro and Parquet are the only supported formats I have seen in the documentation.\nAs we have an analytics case here I would suggest Parquet. \nAvro, however, is the default option and doesn't need any specific configurations.","timestamp":"1685349540.0","poster":"sw1000","comment_id":"909211","upvote_count":"1"},{"content":"Event hub writes only in Avro format\n\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview","comment_id":"879269","comments":[{"poster":"[Removed]","upvote_count":"2","content":"That's not true, it can also write in Parquet if you use the no code editor.\n\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#how-event-hubs-capture-works","comment_id":"896049","timestamp":"1683907680.0"}],"timestamp":"1682336280.0","upvote_count":"2","poster":"C_M_M"},{"content":"Correct answers.\n\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#how-event-hubs-capture-works\n\nAlso:\n\nThe destination storage (Azure Storage or Azure Data Lake Storage) account must be in the same subscription as the event hub.\nEvent Hubs doesn't support capturing events in a premium storage account.","timestamp":"1681904640.0","comment_id":"874566","upvote_count":"6","poster":"Bigbluee"},{"timestamp":"1681896060.0","comment_id":"874446","poster":"Sanaie","upvote_count":"2","content":"The storage type that must be provisioned to support Event Hubs Capture is Azure Data Lake Storage Gen2.\n\nEvent Hubs Capture stores the data it captures in Azure Blob storage or Azure Data Lake Storage Gen2. While Premium Block Blobs and Premium file shares are both Azure Blob storage options, they are not specifically required for this scenario. Therefore, Azure Data Lake Storage Gen2 is the best choice because it is optimized for big data analytics workloads, supports high-volume, low-latency workloads, and has built-in security and compliance features.\n\nThe inbound data format that the reporting system must support is Apache parquet.\n\nEvent Hubs Capture can store captured data in either Avro or JSON format, but Apache parquet is the most efficient format for big data analytics. Parquet is columnar, which means that it is optimized for reading only the columns that are needed, rather than reading entire rows of data. This makes it faster and more efficient for processing large amounts of data. Therefore, the reporting system should support Apache parquet as the inbound data format."}],"answer":"","topic":"2","question_id":98,"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/106695-exam-az-305-topic-2-question-28-discussion/"},{"id":"9KzYUP2IPphPRMJIxhu5","url":"https://www.examtopics.com/discussions/microsoft/view/115064-exam-az-305-topic-2-question-29-discussion/","answer":"C","unix_timestamp":1689262500,"answers_community":["C (100%)"],"question_images":["https://img.examtopics.com/az-305/image195.png"],"answer_ET":"C","question_text":"You have the resources shown in the following table.\n\n//IMG//\n\n\nCDB1 hosts a container that stores continuously updated operational data.\n\nYou are designing a solution that will use AS1 to analyze the operational data daily.\n\nYou need to recommend a solution to analyze the data without affecting the performance of the operational data store.\n\nWhat should you include in the recommendation?","answer_description":"","answer_images":[],"discussion":[{"upvote_count":"21","timestamp":"1689262500.0","content":"Selected Answer: C\nThe correct answer is C. Azure Synapse Link for Azure Cosmos DB.\n\nAzure Synapse Link for Azure Cosmos DB creates a tight integration between Azure Cosmos DB and Azure Synapse Analytics, allowing you to run near real-time analytics over operational data in Azure Cosmos DB. It creates a \"no-ETL\" (Extract, Transform, Load) environment that allows you to analyze data directly without affecting the performance of the transactional workload, which is exactly what is required in this scenario.\n\nA. Azure Data Factory with Azure Cosmos DB and Azure Synapse Analytics connectors would require ETL operations which might impact the performance of the operational data store.\n\nB. Azure Synapse Analytics with PolyBase data loading is more appropriate for loading data from external data sources such as Azure Blob Storage or Azure Data Lake Storage. \n\nD. Azure Cosmos DB change feed doesn't directly address the need for analytics without affecting the performance of the operational data store.","comment_id":"950799","poster":"NotMeAnyWay"},{"content":"Selected Answer: C\nAzure Synapse Link for Azure Cosmos DB.\n\nAzure Synapse Link for Azure Cosmos DB is a cloud-native hybrid transactional and analytical processing (HTAP) capability that enables near real time analytics over operational data in Azure Cosmos DB. Azure Synapse Link creates a tight seamless integration between Azure Cosmos DB and Azure Synapse Analytics. It enables customers to run near real-time analytics over their operational data with full performance isolation from their transactional workloads and without an ETL pipeline\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/synapse-link","poster":"Elecktrus","comment_id":"1011280","upvote_count":"6","timestamp":"1695125940.0"},{"timestamp":"1731425400.0","comment_id":"1310688","poster":"[Removed]","content":"Selected Answer: C\nC is correct","upvote_count":"1"},{"upvote_count":"1","content":"C is correct\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/synapse-link","comment_id":"1298035","timestamp":"1728975360.0","poster":"c_h_r_i_s_"},{"timestamp":"1723199520.0","comment_id":"1262866","upvote_count":"1","content":"This question appeared in the exam, August 2024, I gave this same answer as listed here. I scored 870","poster":"Len83"},{"poster":"23169fd","timestamp":"1719229260.0","comments":[{"timestamp":"1719229320.0","poster":"23169fd","content":"A. Azure Data Factory with Azure Cosmos DB and Azure Synapse Analytics connectors\nReason: While Azure Data Factory can move data from Cosmos DB to Synapse Analytics, it typically involves batch processing, which may not be real-time and could affect operational data performance during the ETL process.\nB. Azure Synapse Analytics with PolyBase data loading\nReason: PolyBase is excellent for loading large volumes of data but is designed for structured data and batch loading, not continuous real-time analysis, and may impact the performance during large data transfers.\nD. Azure Cosmos DB change feed\nReason: The change feed provides a continuous log of changes but requires additional processing to integrate with Synapse Analytics, which can complicate the solution and impact performance.","upvote_count":"1","comment_id":"1236296"}],"comment_id":"1236295","content":"Selected Answer: C\nAzure Synapse Link enables real-time analytics on operational data stored in Azure Cosmos DB without impacting the performance of the operational database.\nIt allows seamless integration between Azure Cosmos DB and Azure Synapse Analytics, providing a hybrid transactional and analytical processing (HTAP) capability.","upvote_count":"1"},{"timestamp":"1706333400.0","content":"the same as topic 1st question 26.","comment_id":"1133078","poster":"kingfighers","upvote_count":"3"},{"timestamp":"1699812960.0","upvote_count":"2","content":"Selected Answer: C\nsame question as before i forgot the number","comment_id":"1068757","poster":"thamaster"},{"upvote_count":"4","poster":"AdventureChick","timestamp":"1695672780.0","content":"Selected Answer: C\nI agree with others & their logic. Just adding my vote.","comment_id":"1017184"}],"isMC":true,"timestamp":"2023-07-13 17:35:00","topic":"2","choices":{"A":"Azure Data Factory with Azure Cosmos DB and Azure Synapse Analytics connectors","C":"Azure Synapse Link for Azure Cosmos DB","D":"Azure Cosmos DB change feed","B":"Azure Synapse Analytics with PolyBase data loading"},"question_id":99,"exam_id":54},{"id":"1l2IuHyoyvheR6zrKFpo","answer_images":[],"timestamp":"2021-12-13 18:12:00","discussion":[{"content":"Selected Answer: D\nCorrect answer - D","poster":"Eltooth","upvote_count":"25","comment_id":"503230","timestamp":"1639693620.0"},{"comments":[{"comment_id":"990514","timestamp":"1693023840.0","upvote_count":"2","content":"Correct. Below url will provide better understanding on those services - https://learn.microsoft.com/en-us/training/modules/design-application-architecture/3-design-messaging-solution","poster":"ksksilva2022"}],"upvote_count":"21","poster":"MicroNoob","content":"Selected Answer: D\nNo doubt, the Service Bus Topic is exactly what you would need if multiple applications want to send messages to consumers.","comment_id":"528882","timestamp":"1642728480.0"},{"timestamp":"1743416940.0","content":"Selected Answer: D\nService Bus topics and subscriptions support a publish/subscribe messaging communication model.\n\nIn contrast with Service Bus queues, in which each message is processed by a single consumer, topics and subscriptions provide a one-to-many form of communication, using a publish/subscribe pattern.\n\nhttps://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-topics-subscriptions-portal","upvote_count":"2","comment_id":"1414679","poster":"serbanvadi"},{"poster":"serbanvadi","content":"Selected Answer: D\nIn contrast with Service Bus queues, in which each message is processed by a single consumer, topics and subscriptions provide a one-to-many form of communication, using a publish/subscribe pattern. It's possible to register multiple subscriptions to a topic. When a message is sent to a topic, it's then made available to each subscription to handle/process independently. \n\nhttps://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-topics-subscriptions-portal","timestamp":"1743182580.0","upvote_count":"1","comment_id":"1411410"},{"comment_id":"1401622","timestamp":"1742571000.0","upvote_count":"1","poster":"Muler5692","content":"Selected Answer: D\nSo i too bought the Contributor access! Wish me luck"},{"timestamp":"1732695240.0","comment_id":"1318491","poster":"Thanveer","upvote_count":"1","content":"Selected Answer: D\nCorrect answer - D"},{"timestamp":"1731335580.0","upvote_count":"1","content":"Selected Answer: D\nD is correct","poster":"[Removed]","comment_id":"1310146"},{"poster":"nav109","comment_id":"1075303","upvote_count":"4","timestamp":"1700474520.0","content":"Got this on Nov. 17, 2023"},{"poster":"stonwall12","upvote_count":"5","timestamp":"1694469480.0","content":"Correct Answer - D: Azure Service Bus Topic\n- The key detail in the requirement is that in the future, multiple applications will process the shipping requests. This implies a need for a publish-subscribe model where multiple subscribers (applications) can independently process messages (transactions).\n\n- Azure Service Bus Topic: Topics in Azure Service Bus support the publish-subscribe pattern. Multiple subscribers can independently retrieve filtered or unfiltered messages from the topic. This is the best fit for the described requirement.\n\nhttps://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions","comment_id":"1005186"},{"content":"Selected Answer: D\nD. one Azure Service Bus topic\n\nIn this scenario, you should recommend using an Azure Service Bus topic. Topics provide a publish-subscribe messaging pattern that allows multiple subscribers to independently retrieve messages based on their specific needs. As you add more applications to process shipping requests, each application can subscribe to the topic and filter the messages based on the transaction details.","upvote_count":"5","poster":"NotMeAnyWay","comment_id":"846145","timestamp":"1679415060.0"},{"upvote_count":"3","poster":"zellck","timestamp":"1677082380.0","comment_id":"818028","content":"Selected Answer: D\nD is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions#topics-and-subscriptions\nA queue allows processing of a message by a single consumer. In contrast to queues, topics and subscriptions provide a one-to-many form of communication in a publish and subscribe pattern. It's useful for scaling to large numbers of recipients. Each published message is made available to each subscription registered with the topic. Publisher sends a message to a topic and one or more subscribers receive a copy of the message."},{"upvote_count":"1","comment_id":"790394","poster":"OPT_001122","timestamp":"1674893640.0","content":"Selected Answer: D\nD. one Azure Service Bus topic\nCorrect answer\n\nTopic is for one to many"},{"upvote_count":"2","poster":"Snownoodles","timestamp":"1666491900.0","comment_id":"701864","content":"Selected Answer: D\nservice bus topic - 1:N"},{"comment_id":"657291","poster":"Gowind","timestamp":"1662114300.0","comments":[{"content":"Sorry answer is D not because of having multiple consumers but because of the need of filtering based on the transaction details. Publisher sends a message to a topic and one or more subscribers receive a copy of the message, depending on filter rules set on these subscriptions.","timestamp":"1662115020.0","upvote_count":"15","comment_id":"657311","poster":"Gowind"}],"upvote_count":"7","content":"Selected Answer: C\nAnswer is C. \nThe shipping must be handled by only ONE receiver at a time. If you use D (Topic) several subscribers can receive the message and processes the shipment resulting in several shipments.\n\nQueue does not mean only one receiver but only ONE AT A TIME to process the message.\nhttps://medium.com/awesome-azure/azure-difference-between-azure-service-bus-queues-and-topics-comparison-azure-servicebus-queue-vs-topic-4cc97770b65\n\nQueues\nQueues offer First In, First Out (FIFO) message delivery to one or more competing consumers. That is, receivers typically receive and process messages in the order in which they were added to the queue. And, only one message consumer receives and processes each message.\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions"},{"content":"Selected Answer: D\nAzure service bus topic is support many application","timestamp":"1657788240.0","comment_id":"631257","upvote_count":"2","poster":"princessgalz"},{"timestamp":"1656225720.0","upvote_count":"1","content":"A queue allows processing of a message by a single consumer. In contrast to queues, topics and subscriptions provide a one-to-many form of communication in a publish and subscribe pattern.","comment_id":"622384","poster":"tictaclu"},{"timestamp":"1655889420.0","upvote_count":"6","comment_id":"620278","poster":"al608","content":"did my Exam today. This was on there."},{"upvote_count":"1","timestamp":"1653339060.0","poster":"Gor","comment_id":"606311","content":"Selected Answer: D\nCorrect answer: D"},{"timestamp":"1650994020.0","poster":"Teringzooi","content":"Selected Answer: D\nCorrect answer - D\nService Bus Topic","comment_id":"592597","upvote_count":"2"},{"upvote_count":"4","content":"Came in exam today 04/04/2022","comment_id":"580777","timestamp":"1649081220.0","poster":"Contactfornitish"},{"content":"Correct answer is D","upvote_count":"2","comment_id":"576299","poster":"Suwani","timestamp":"1648396200.0"},{"timestamp":"1646952360.0","upvote_count":"4","comment_id":"565072","content":"Appeared in my exam, March 10th, 2022. I chose D.","poster":"Insanewhip"},{"upvote_count":"6","content":"Selected Answer: D\nD seems right. not sure what is C ? Definitely a broker solution is needed. Like pub/sub","poster":"[Removed]","comment_id":"512459","timestamp":"1640796900.0"},{"comment_id":"500763","upvote_count":"4","poster":"default_wizard","content":"correct answer: service bus topic","timestamp":"1639415520.0"}],"topic":"2","unix_timestamp":1639415520,"choices":{"C":"one Azure Service Bus queue","B":"multiple storage account queues","A":"one Azure Data Factory pipeline","D":"one Azure Service Bus topic"},"answers_community":["D (91%)","9%"],"url":"https://www.examtopics.com/discussions/microsoft/view/67833-exam-az-305-topic-2-question-3-discussion/","answer_ET":"D","question_id":100,"answer_description":"","question_images":[],"answer":"D","isMC":true,"exam_id":54,"question_text":"You have an Azure subscription that contains two applications named App1 and App2. App1 is a sales processing application. When a transaction in App1 requires shipping, a message is added to an Azure Storage account queue, and then App2 listens to the queue for relevant transactions.\nIn the future, additional applications will be added that will process some of the shipping requests based on the specific details of the transactions.\nYou need to recommend a replacement for the storage account queue to ensure that each additional application will be able to read the relevant transactions.\nWhat should you recommend?"}],"exam":{"id":54,"lastUpdated":"12 Apr 2025","isImplemented":true,"isBeta":false,"provider":"Microsoft","name":"AZ-305","isMCOnly":false,"numberOfQuestions":286},"currentPage":20},"__N_SSP":true}