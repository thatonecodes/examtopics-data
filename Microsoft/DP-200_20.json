{"pageProps":{"questions":[{"id":"px9xeNAxVNYyhvAbkwwN","question_id":96,"isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a solution that will use Azure Stream Analytics. The solution will accept an Azure Blob storage file named Customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address.\nYou have a file in Blob storage named LocationIncomes that contains median incomes based on location. The file rarely changes.\nYou need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure\nData Lake Storage Gen2 for long-term retention.\nSolution: You implement a Stream Analytics job that has one streaming input, one reference input, two queries, and four outputs.\nDoes this meet the goal?","timestamp":"2020-07-14 21:46:00","url":"https://www.examtopics.com/discussions/microsoft/view/25766-exam-dp-200-topic-2-question-27-discussion/","choices":{"B":"No","A":"Yes"},"answer_images":[],"question_images":[],"exam_id":65,"answer":"A","answer_ET":"A","unix_timestamp":1594755960,"answer_description":"We need one reference data input for LocationIncomes, which rarely changes.\nWe need two queries, on for in-store customers, and one for online customers.\nFor each query two outputs is needed.\nNote: Stream Analytics also supports input known as reference data. Reference data is either completely static or changes slowly.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#stream-and-reference-inputs https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs","answers_community":[],"topic":"2","discussion":[{"content":"It would meet the goal, but why would you even create 2 queries? You can get there with just 1 query and two outputs.","comment_id":"172722","upvote_count":"11","poster":"BungyTex","comments":[{"comment_id":"295107","content":"no you can''t, you have 2 \"INTO\"","poster":"H_S","timestamp":"1613831400.0","upvote_count":"1"}],"timestamp":"1599143220.0"},{"comments":[{"timestamp":"1620360480.0","content":"Here is the scenario, there is a file that contains two dataset in-store and online customer and it requires to load it into Azure SQL and Data Lake. In coding perspective each query is equivalent to ONE select into which results to two queries to load the requirement. However, if we are going to combine the in-store and online customers then it is only ONE query. Since it was not stated here to unify the datasets then it is best to assume that they will be loaded into two tables. In azure SQL it will be loaded to two tables then in Azure Datalake there will be also two placeholder. Therefore, there are 2 queries and 4 outputs in total.","comment_id":"351626","upvote_count":"2","poster":"cadio30"}],"timestamp":"1620006780.0","poster":"cadio30","comment_id":"348192","upvote_count":"1","content":"NO is the answer\n\nReference: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#query-example-send-data-to-multiple-outputs"},{"comment_id":"338718","poster":"azure_emumba","upvote_count":"1","timestamp":"1618821600.0","content":"\"NO\". we can handle two inputs i.e two SELECT statements in one query. so no need of two queries.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data#joining-multiple-reference-datasets-in-a-job"},{"comments":[{"poster":"mohowzeh","upvote_count":"1","content":"Good point about \"what is a query\"... \nTwo SELECT clauses are needed if there are two INTO statements, as per this link: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns: \n(quote from above link) \"Multiple SELECT statements can be used to output data to different output sinks. For example, one SELECT can output a threshold-based alert while another one can output events to blob storage.\"","timestamp":"1608645420.0","comment_id":"250150"}],"content":"I think the problem is that its a bit confusing what is \"one query\". Is it all you can do into query editor or is each time you use separated \"select - into - from\" ? If its the second one, so you need to create 2 queries, each one with different \"INTO output\"","comment_id":"241684","upvote_count":"1","poster":"maynard13x8","timestamp":"1607785380.0"},{"upvote_count":"3","content":"hi to all,\nfor it's false\njust because we have 1 query and not 2 queries; the query already joins the main data with the reference data so it's one query\nthe rest is OK\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-troubleshoot-query\n\nregards","comment_id":"241470","comments":[{"upvote_count":"1","comment_id":"295109","timestamp":"1613831520.0","poster":"H_S","content":"it's one query, with 2 outputs and not 4 ?"}],"poster":"dumpsm42","timestamp":"1607766240.0"},{"timestamp":"1606821300.0","content":"Hmm...so long as it meets the goal the answer then the answer is yes then.\nYes the suggested solution does kind of \"overdo\" it but no harm in it I suppose.\nJust my own thoughts about it so I will go with Yes in this case.","upvote_count":"3","comment_id":"231863","poster":"syu31svc"},{"content":"There is only one requirement in teh question \"You need to use an address to look up a median income based on location\". 1 Query and 2 outputs makes sense. Not sure why we need the second query for? anyone?","timestamp":"1601543100.0","upvote_count":"2","comment_id":"190870","poster":"hart232"},{"timestamp":"1599528000.0","comment_id":"175511","poster":"Varma_Saraswathula","content":"The online customers will provide a mailing address.\n\nYou need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure\nDoes this mean that we need to have 2 diferent queries to get this, if there are 2 queries then 4 outputs","upvote_count":"1"},{"timestamp":"1596971880.0","comment_id":"153565","poster":"dcpavelescu","content":"Not clear why 4 outputs are required, as long as no specific requirement to provide separate outputs per customer type (in-store and online customers)\n2 query, 2 outputs shall be the right answer accordingly also to:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns\n\nAnyone, is an option with 2 queries and 2 outputs in the exam?","upvote_count":"3"},{"comments":[{"comment_id":"137331","timestamp":"1595002440.0","upvote_count":"6","poster":"nivas143srinivas","content":"The query output need to be stored in two places: SQL DW and Data lake both.\nSo it's 2x2"}],"content":"There are only two queries, why need for four outputs? I think the answer should be No.","comment_id":"135221","upvote_count":"1","poster":"Yan2x","timestamp":"1594755960.0"}]},{"id":"N5cJGBkkwa5CiBXAs3No","timestamp":"2020-07-18 21:21:00","question_images":[],"exam_id":65,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/26059-exam-dp-200-topic-2-question-28-discussion/","topic":"2","answer":"B","answer_description":"We need a High Concurrency cluster for the data engineers and the jobs.\nNote:\nStandard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.\nA high concurrency cluster is a managed cloud resource. The key benefits of high concurrency clusters are that they provide Apache Spark-native fine-grained sharing for maximum resource utilization and minimum query latencies.\nReferences:\nhttps://docs.azuredatabricks.net/clusters/configure.html","question_id":97,"discussion":[{"content":"As job notebooks include scala and high concurrency clusters do not support scala, the answer should be no.","comment_id":"138093","poster":"Nieswurz","upvote_count":"26","timestamp":"1595100060.0","comments":[{"comment_id":"171184","poster":"Equalizer","timestamp":"1598937900.0","content":"Correct, check: https://docs.microsoft.com/en-us/azure/databricks/clusters/configure","upvote_count":"3"},{"timestamp":"1662792180.0","upvote_count":"1","content":"this is 100% correct, it's as simple as this, job requires scala, high concurrency does not support scala. The answer is no.","comment_id":"665159","poster":"TashaP"}]},{"upvote_count":"9","timestamp":"1596055140.0","content":"If job needs to use scala then high concurrency cluster can't be used as that wont support Scala","comment_id":"146925","poster":"avix"},{"poster":"ffgghhjj","content":"B. as high concurrency clusters do not support Scala.","timestamp":"1723507260.0","upvote_count":"1","comment_id":"1264867"},{"content":"B is corect \nThere are two options for cluster mode:\n\nStandard: Single user / small group clusters - can use any language.\nHigh Concurrency: A cluster built for minimizing latency in high concurrency workloads.\nThere are a few main reasons you would use a Standard cluster over a high concurrency cluster. The first is if you are a single user of Databricks exploring the technology. For most PoCs and exploration, a Standard cluster should suffice. The second is if you are a Scala user, as high concurrency clusters do not support Scala. The third is if your use case simply does not require high concurrency processes.\n\nHigh concurrency clusters, in addition to performance gains, also allow you utilize table access control, which is not supported in Standard clusters.\n\nPlease note that High Concurrency clusters do not automatically set the auto shutdown field, whereas standard clusters default it to 120 minutes.\nhttps://www.mssqltips.com/sqlservertip/6604/azure-databricks-cluster-configuration/","timestamp":"1621219140.0","upvote_count":"2","comment_id":"359203","poster":"hoangton"}],"answer_images":[],"isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\n✑ A workload for data engineers who will use Python and SQL\n✑ A workload for jobs that will run notebooks that use Python, Scala, and SQL\n✑ A workload that data scientists will use to perform ad hoc analysis in Scala and R\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\n✑ The data engineers must share a cluster.\n✑ The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\n✑ All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\nYou need to create the Databricks clusters for the workloads.\nSolution: You create a Standard cluster for each data scientist, a Standard cluster for the data engineers, and a High Concurrency cluster for the jobs.\nDoes this meet the goal?","choices":{"A":"Yes","B":"No"},"answer_ET":"B","unix_timestamp":1595100060},{"id":"ZZlJCA3rVW2jeYQBvzRv","exam_id":65,"answer":"B","question_id":98,"unix_timestamp":1617440160,"answer_images":[],"isMC":true,"question_images":[],"choices":{"A":"Yes","B":"No"},"url":"https://www.examtopics.com/discussions/microsoft/view/48906-exam-dp-200-topic-2-question-29-discussion/","question_text":"Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.\nYou need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.\nSolution:\n1. Use Azure Data Factory to convert the parquet files to CSV files\n2. Create an external data source pointing to the Azure Data Lake Gen 2 storage account\n3. Create an external file format and external table using the external data source\n4. Load the data using the CREATE TABLE AS SELECT statement\nDoes the solution meet the goal?","answer_description":"It is not necessary to convert the parquet files to CSV files.\nYou need to create an external file format and external table using the external data source.\nYou load the data using the CREATE TABLE AS SELECT statement.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store","answer_ET":"A","timestamp":"2021-04-03 10:56:00","answers_community":["B (100%)"],"discussion":[{"comment_id":"328510","upvote_count":"9","timestamp":"1617613740.0","comments":[{"poster":"Mily94","content":"indeed, it is not necessary to convert parquet to CSV, but the question is if that apporach meet the goal - A: yes","timestamp":"1619370840.0","comment_id":"342739","upvote_count":"14"}],"content":"It is not necessary to convert the parquet files to CSV files.--> Answer No","poster":"vaseva1"},{"timestamp":"1637692860.0","upvote_count":"1","content":"Selected Answer: B\nNo need to convert to CSV File","comment_id":"485327","poster":"FredNo"},{"timestamp":"1623059940.0","upvote_count":"1","poster":"nishant_678993","comment_id":"376667","content":"why would i convert to csv ..When my further steps are going to read data from ADLS.\nExplanation says not required and ANSWER says YES."},{"content":"You can just use data factory to read parquest and ingest to Synapse with auto creating table, but per se the given solution meets the goal even if in not optimal way","comment_id":"364260","timestamp":"1621756020.0","upvote_count":"1","poster":"maciejt"},{"content":"Correct answer is ( B-------- No )","poster":"Qrm_1972","comment_id":"351199","timestamp":"1620318540.0","upvote_count":"3"},{"content":"The answer should be A: Yes. Since it meets the goal though it is not necessary to change the parquet to csv but you can change it.","upvote_count":"2","comment_id":"346533","timestamp":"1619835600.0","poster":"KpKo"},{"timestamp":"1618879800.0","comment_id":"339237","upvote_count":"1","poster":"Wendy_DK","content":"Correct Answer is B"},{"timestamp":"1617440160.0","comment_id":"327241","poster":"Ash001","content":"It should be No as per the details provided.","upvote_count":"2"}],"topic":"2"},{"id":"IfhjpX6Chp2sMeJmg9qJ","topic":"2","question_id":99,"unix_timestamp":1617776640,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0014600001.png"],"timestamp":"2021-04-07 08:24:00","answers_community":[],"question_text":"HOTSPOT -\nA company runs Microsoft Dynamics CRM with Microsoft SQL Server on-premises. SQL Server Integration Services (SSIS) packages extract data from Dynamics\nCRM APIs, and load the data into a SQL Server data warehouse.\nThe datacenter is running out of capacity. Because of the network configuration, you must extract on premises data to the cloud over https. You cannot open any additional ports. The solution must implement the least amount of effort.\nYou need to create the pipeline system.\nWhich component should you use? To answer, select the appropriate technology in the dialog box in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/49461-exam-dp-200-topic-2-question-3-discussion/","answer":"","answer_ET":"","answer_description":"Box 1: Source -\nFor Copy activity, it requires source and sink linked services to define the direction of data flow.\nCopying between a cloud data source and a data source in private network: if either source or sink linked service points to a self-hosted IR, the copy activity is executed on that self-hosted Integration Runtime.\nBox 2: Self-hosted integration runtime\nA self-hosted integration runtime can run copy activities between a cloud data store and a data store in a private network, and it can dispatch transform activities against compute resources in an on-premises network or an Azure virtual network. The installation of a self-hosted integration runtime needs on an on-premises machine or a virtual machine (VM) inside a private network.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime","discussion":[{"content":"For me are:\n- Self Hosted IR\n- Sink","poster":"Pairon","comment_id":"330092","timestamp":"1617776640.0","upvote_count":"17"},{"upvote_count":"7","poster":"Williammm","comment_id":"390446","timestamp":"1624622520.0","content":"So...what is the answer??? so anoying!"},{"poster":"Nnuujj","upvote_count":"1","timestamp":"1693211760.0","comment_id":"991934","content":"For me:\nExtract SQL data on-premises: Use Azure-SSIS Integration runtime to run SSIS packages in ADF to extract data from Dynamics.\nLoad SQL data warehouse: Use Self-hosted integration runtime to perform operations between on premises and azure. In this case loading data using ADF to SQL Server data warehouse"},{"upvote_count":"3","poster":"Marcus1612","comment_id":"462135","content":"Bloc 1= Azure SSIS Integration runtine\".\nBloc 2 = Azure SSIS Integration runtine\".\nThe main problem is that The datacenter is running out of capacity. So we just need the cloud infrastructure with the underlying ressources (cpu, memory) to transfert data between the CRM and the Data warehouse. Since an existing SSIS package exists already, the minimum effort would be to lift and shift this exiting SSIS package to the cloud. To do that, the technology behind the scene is \"Azure SSIS Integration runtine\".","timestamp":"1634223900.0"},{"timestamp":"1624290840.0","comment_id":"387252","content":"The answer is correct!","poster":"eng1","upvote_count":"1"},{"timestamp":"1622910960.0","poster":"vrmei","comment_id":"375324","content":"Self Hosted IR for both answer\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime","upvote_count":"2"},{"comment_id":"358872","timestamp":"1621180140.0","content":"-Self Hosted\n-Self Hosted","upvote_count":"2","poster":"hoangton"},{"poster":"cadio30","comments":[{"content":"On second though, the question states that there is a SSIS involve and it requires minimal effort, hence, Azure SSIS and Self Hosted are the answers","poster":"cadio30","upvote_count":"2","timestamp":"1620353340.0","comment_id":"351557"}],"upvote_count":"4","comment_id":"345903","content":"The scenario retrieves data from on-prem database and loads it to on-prem data warehouse.\nADF function would be to orchestrate data and their link services would use \"SELF-HOSTED\" and this requires to implement gateway configuration on the server where the on-prem resides. Therefore, both SELF-HOSTED is the appropriate answer","timestamp":"1619761260.0"},{"poster":"Aragorn_2021","content":"- Self Hosted IR ( to transfer the data from on premise to Azure)\n- Azure-SSIS ( to take care of the load from source to target warehouse)","upvote_count":"7","timestamp":"1618893420.0","comment_id":"339321"},{"comment_id":"338796","poster":"Prabhakaran94","upvote_count":"4","timestamp":"1618830720.0","content":"Correct Answer is:\n - Azure-SSIS integration runtime\n - Azure integration runtime"},{"poster":"LuBarba","comment_id":"332042","content":"Correct answer is:\n- Self Hosted IR\n- Sink\nSource: https://docs.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-data-tool","comments":[{"timestamp":"1621180560.0","upvote_count":"3","content":"source is irrelevant. for me its;\nself host ir\nazure ssis","poster":"memo43","comment_id":"358880"}],"upvote_count":"4","timestamp":"1617983760.0"},{"content":"I'm not sure that \"Self Hosted IR\" can be the right choice for the first option. \nKeep in mind that on-premise data-center is already running \"SQL Server Integration Services (SSIS)\". \nSome related doc: https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime","comment_id":"331737","poster":"tucho","timestamp":"1617945420.0","upvote_count":"3"}],"exam_id":65,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0014500001.png"]},{"id":"hHH7bA6cvyqWRhI3SmHP","exam_id":65,"answer_ET":"C","answer_description":"Azure Stream Analytics supports user-defined aggregates (UDA) written in JavaScript, it enables you to implement complex stateful business logic. Within UDA you have full control of the state data structure, state accumulation, state decumulation, and aggregate result computation.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-javascript-user-defined-aggregates","choices":{"C":"JavaScript user-defined aggregates (UDA)","B":"Azure Machine Learning","A":"JavaScript user-define functions (UDFs)"},"discussion":[{"poster":"H_S","comment_id":"295116","content":"keyword statufull, otherwise it would be 1 \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-javascript-user-defined-functions\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-javascript-user-defined-aggregates","timestamp":"1613831760.0","upvote_count":"2"},{"upvote_count":"4","comment_id":"228289","timestamp":"1606393320.0","poster":"syu31svc","content":"Link provided supports C as the answer"}],"topic":"2","question_id":100,"question_images":[],"timestamp":"2020-11-26 13:22:00","question_text":"You need to implement complex stateful business logic within an Azure Stream Analytics service.\nWhich type of function should you create in the Stream Analytics topology?","answers_community":[],"isMC":true,"unix_timestamp":1606393320,"url":"https://www.examtopics.com/discussions/microsoft/view/37836-exam-dp-200-topic-2-question-30-discussion/","answer_images":[],"answer":"C"}],"exam":{"isMCOnly":false,"numberOfQuestions":228,"isImplemented":true,"id":65,"name":"DP-200","isBeta":false,"provider":"Microsoft","lastUpdated":"12 Apr 2025"},"currentPage":20},"__N_SSP":true}