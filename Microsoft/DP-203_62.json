{"pageProps":{"questions":[{"id":"5QlB2EVrh99ydYqczYj0","isMC":true,"answers_community":["CD (91%)","4%"],"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/60841-exam-dp-203-topic-4-question-13-discussion/","choices":{"B":"Use built-in SQL functions to extract date attributes.","E":"Use DateTime columns for the date fields.","A":"Create a date dimension table that has a DateTime key.","D":"In the fact table, use integer columns for the date fields.","C":"Create a date dimension table that has an integer key in the format of YYYYMMDD."},"question_text":"You are designing a star schema for a dataset that contains records of online orders. Each record includes an order date, an order due date, and an order ship date.\nYou need to ensure that the design provides the fastest query times of the records when querying for arbitrary date ranges and aggregating by fiscal calendar attributes.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","timestamp":"2021-08-27 11:46:00","unix_timestamp":1630057560,"discussion":[{"comments":[{"poster":"anto69","content":"Yup, that makes sense","timestamp":"1643802180.0","comment_id":"538631","upvote_count":"4"}],"upvote_count":"65","timestamp":"1630101240.0","content":"Should be C and D","poster":"echerish","comment_id":"433355"},{"timestamp":"1630759980.0","poster":"GervasioMontaNelas","content":"100% CD","upvote_count":"14","comment_id":"439145"},{"upvote_count":"1","comments":[{"upvote_count":"1","content":"Also, the use of built-in SQL functions would require the database to do a full tablescan on the fact table anyways.","poster":"7082935","timestamp":"1722619920.0","comment_id":"1259969"}],"comment_id":"1259968","content":"Selected Answer: CD\nWhile B is a valid solution, it presents a serious performance issue when SQL Server has to dynamically convert DATE datatypes to INTEGER on-the-fly when joining to a date dimension that uses INTEGER for its surrogate key. Therefore, options C and D present the most efficient way of querying/filtering this fact table.","timestamp":"1722619800.0","poster":"7082935"},{"poster":"Dusica","comment_id":"1202501","content":"C and D but also B - should have asked for 3 steps","upvote_count":"2","timestamp":"1714122480.0"},{"content":"Selected Answer: CD\nthat makes sense , we don't use datetime if it's possible for performance","upvote_count":"1","timestamp":"1709585460.0","comment_id":"1165994","poster":"MohamedBI12"},{"timestamp":"1707039120.0","upvote_count":"1","poster":"Azure_2023","comment_id":"1139942","content":"Selected Answer: CD\nStar schema - Fact and Dim tables - they have to be connected using Date Key (int)."},{"poster":"kkk5566","comment_id":"994928","timestamp":"1693469940.0","content":"Selected Answer: CD\nC&D. You can see some exapmles from modules of MS' dp-203 training.","upvote_count":"1"},{"comment_id":"935066","upvote_count":"2","content":"Selected Answer: CD\nBecause of fixed date ranges used to query.","timestamp":"1687846500.0","poster":"auwia"},{"comment_id":"903025","poster":"BPW","content":"Should be A and E","upvote_count":"2","timestamp":"1684656840.0"},{"upvote_count":"2","poster":"esaade","comment_id":"841186","comments":[{"comment_id":"890138","upvote_count":"1","poster":"gogosgh","timestamp":"1683301800.0","content":"we are not querying against time. the fact table has only dates"}],"timestamp":"1678984680.0","content":"Selected Answer: AB\nA. Create a date dimension table that has a DateTime key. A date dimension table that has a DateTime key can provide fast query times when querying for arbitrary date ranges and aggregating by fiscal calendar attributes. The DateTime key allows for easy sorting and filtering of dates, and can be used to join with the fact table on the order date, order due date, and order ship date fields.\n\nB. Use built-in SQL functions to extract date attributes. Using built-in SQL functions to extract date attributes (such as year, quarter, month, week, day) from the DateTime key in the date dimension table can help with aggregating data by fiscal calendar attributes. This can improve query performance by reducing the amount of data that needs to be scanned and aggregated.\n\nTherefore, the correct actions to perform are A and B."},{"content":"For sure its CD","timestamp":"1670200140.0","upvote_count":"2","poster":"XiltroX","comment_id":"735539"},{"timestamp":"1666125720.0","content":"Selected Answer: CD\nCD with no doubt.","poster":"Xinyuehong","upvote_count":"2","comment_id":"698511"},{"comment_id":"646190","content":"correct - C&D agree with StudentFromAus M","upvote_count":"4","poster":"Deeksha1234","timestamp":"1660375980.0"},{"timestamp":"1656367320.0","upvote_count":"5","content":"Selected Answer: CD\nTHe question has many clues, it states fiscal calendar year and then star schema which hints we need proper fact and dim tables and appropriate date keys to link these.","comment_id":"623511","poster":"StudentFromAus"},{"poster":"Davico93","content":"Selected Answer: CD\nbasic knowledge for fact and dim tables","comment_id":"619552","timestamp":"1655774640.0","upvote_count":"3"},{"poster":"AlCubeHead","upvote_count":"7","comment_id":"581525","content":"Selected Answer: CD\nWho gives these answers?? It's so obviously C and D. You want a Date Dim with an Integer key and the fact table also with that integer key","timestamp":"1649207400.0"},{"content":"Should be CD!","timestamp":"1647541140.0","comment_id":"570003","poster":"wwdba","upvote_count":"2"},{"upvote_count":"2","poster":"Boumisasound","content":"Selected Answer: CD\nI'm agree for CD","timestamp":"1646826240.0","comment_id":"564004"},{"upvote_count":"2","timestamp":"1645798920.0","content":"Selected Answer: AE\nthis makes the most sense","poster":"ovokpus","comment_id":"556023"},{"comment_id":"543047","timestamp":"1644325860.0","content":"Selected Answer: CD\nC & D should be correct","upvote_count":"2","poster":"kanak01"},{"poster":"bahamutedean","timestamp":"1642290120.0","content":"should be CD","comment_id":"524498","upvote_count":"2"},{"timestamp":"1639480500.0","poster":"BusinessApps","comment_id":"501313","upvote_count":"4","content":"Selected Answer: CD\nAnswer C and D"},{"poster":"alexleonvalencia","timestamp":"1639179360.0","comment_id":"499006","upvote_count":"3","content":"Selected Answer: CD\nRespuesta correcta CD"},{"timestamp":"1638089400.0","comment_id":"488998","upvote_count":"3","content":"Selected Answer: CD\nC and D","poster":"dija123"},{"comments":[{"content":"You dont want to have to run a function everytime you look up a date. That will cause your queries to perform poorly. Also, there is nothing in the scenario that requires time (just dates), so a simple date dimension will do. If this was a real data warehouse, I would implement a separate time dimension.","poster":"SQLDev0000","upvote_count":"1","comment_id":"582005","timestamp":"1649273580.0"}],"timestamp":"1637514120.0","content":"I think it's A and B since we need to design a star schema and we're aggregating by fiscal calendar attributes (we can use DATETIME functions to fetch year, month... attributes)","poster":"simo40010","comment_id":"483467","upvote_count":"1"},{"comment_id":"482643","timestamp":"1637421000.0","poster":"captainpike","upvote_count":"2","content":"https://community.idera.com/database-tools/blog/b/community_blog/posts/why-use-a-date-dimension-table-in-a-data-warehouse"},{"comment_id":"480034","content":"Selected Answer: CD\nit is C and D. You need a dimension table for the date","upvote_count":"4","timestamp":"1637154780.0","poster":"FredNo"},{"content":"Why not A and E?","poster":"sdnv89","upvote_count":"4","timestamp":"1635404040.0","comment_id":"469045"},{"poster":"AAmur","upvote_count":"2","comment_id":"467841","content":"Why not use DateTime data type for keys? Despite some storage overhead, it will allow us to use specific functions during queries. Also, it is probable that the optimizer incorporates date type intelligence and knows there are 31 values between March 1 and April 1, as opposed to the apparent 100 values between 20210301 and 20210401.","timestamp":"1635229740.0"},{"upvote_count":"1","timestamp":"1633078140.0","poster":"yolap31172","comment_id":"455421","content":"Built-in SQL functions will not cover fiscal calendar attributes, you will not get it without custom date dimension. Therefore D and E (I doubt there's serious performance gain when using int for key instead)."},{"comments":[{"comment_id":"471546","upvote_count":"1","poster":"eoicp","timestamp":"1635840660.0","content":"Yes, definitely"}],"upvote_count":"3","content":"I think C and D, as we are designing a star schema so we need a dimension table","timestamp":"1630311120.0","comment_id":"435266","poster":"Amalbenrebai"},{"content":"Why is that the answer?","poster":"[Removed]","timestamp":"1630057560.0","upvote_count":"2","comment_id":"432970"}],"question_id":306,"exam_id":67,"answer":"CD","answer_ET":"CD","topic":"4","question_images":[]},{"id":"NzzcbHrlDMEyGoS7o6Ko","answer_description":"","question_text":"A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\nThe company must be able to monitor the devices in real-time.\nYou need to design the solution.\nWhat should you recommend?","answer":"C","answer_ET":"C","unix_timestamp":1640929620,"choices":{"C":"Azure Stream Analytics cloud job using Azure Portal","A":"Azure Analysis Services using Azure Portal","D":"Azure Data Factory instance using Microsoft Visual Studio","B":"Azure Analysis Services using Azure PowerShell"},"answer_images":[],"answers_community":["C (100%)"],"question_images":[],"timestamp":"2021-12-31 06:47:00","topic":"4","exam_id":67,"isMC":true,"question_id":307,"url":"https://www.examtopics.com/discussions/microsoft/view/69121-exam-dp-203-topic-4-question-14-discussion/","discussion":[{"poster":"Raghu108","timestamp":"1674155460.0","content":"Repeated question","upvote_count":"12","comment_id":"527839"},{"upvote_count":"8","timestamp":"1674023460.0","content":"Selected Answer: C\nC is correct","poster":"PallaviPatel","comment_id":"526342"},{"poster":"kkk5566","content":"Selected Answer: C\nC is correct","timestamp":"1725092460.0","comment_id":"994929","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: C\nC is correct","poster":"ZIMARAKI","timestamp":"1705236660.0","comment_id":"775418"},{"upvote_count":"1","timestamp":"1691912460.0","content":"correct","comment_id":"646194","poster":"Deeksha1234"},{"timestamp":"1672465620.0","poster":"SabaJamal2010AtGmail","content":"C is correct","comment_id":"513900","upvote_count":"4"}]},{"id":"QgcNw68kJpNi3RIVvxZT","answer_description":"","question_text":"You have a SQL pool in Azure Synapse.\nA user reports that queries against the pool take longer than expected to complete. You determine that the issue relates to queried columnstore segments.\nYou need to add monitoring to the underlying storage to help diagnose the issue.\nWhich two metrics should you monitor? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer_ET":"BD","answer":"BD","unix_timestamp":1641122880,"choices":{"D":"Cache hit percentage","B":"Cache used percentage","C":"DWU Limit","A":"Snapshot Storage Size"},"answer_images":[],"answers_community":["BD (79%)","AD (21%)"],"question_images":[],"timestamp":"2022-01-02 12:28:00","topic":"4","exam_id":67,"isMC":true,"question_id":308,"url":"https://www.examtopics.com/discussions/microsoft/view/69247-exam-dp-203-topic-4-question-15-discussion/","discussion":[{"poster":"Deeksha1234","upvote_count":"6","comment_id":"646193","content":"Selected Answer: BD\nseems correct","timestamp":"1660376460.0"},{"poster":"Alongi","timestamp":"1714119960.0","comments":[{"comment_id":"1266313","timestamp":"1723710900.0","content":"Snapshot size has not effect on query performance. Answer is BD.","upvote_count":"2","poster":"ramnathshenoy10"}],"content":"Selected Answer: AD\nSnapshot Storage Size: This metric indicates the size of the snapshot used by the SQL pool. Monitoring this metric can provide information on how much storage space is being utilized by the pool, which can be useful in understanding if there are any storage overutilization issues.\n\nCache hit percentage: This metric indicates the percentage of column cache used for queries. Monitoring this metric can provide insights into how effectively the columns used in queries are being stored in the column cache, which is critical for query performance.","upvote_count":"3","comment_id":"1202470"},{"comment_id":"994931","timestamp":"1693470240.0","upvote_count":"1","content":"Selected Answer: BD\ngo b &D","poster":"kkk5566"},{"timestamp":"1691655720.0","poster":"Matt2000","upvote_count":"1","content":"This link might be useful. It explains cache hit percentage and cache used percentage:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity","comment_id":"977429"},{"comment_id":"783714","poster":"yogiazaad","content":"This article is more relevant here.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache","upvote_count":"3","timestamp":"1674332100.0"},{"comment_id":"526345","poster":"PallaviPatel","upvote_count":"4","content":"Selected Answer: BD\nCorrect Answer","timestamp":"1642487520.0"},{"upvote_count":"4","content":"correct","poster":"HaBroNounen","comment_id":"514948","timestamp":"1641122880.0"}]},{"id":"pTteloqpepopOZWVrL8j","question_id":309,"timestamp":"2022-01-02 12:29:00","question_text":"You manage an enterprise data warehouse in Azure Synapse Analytics.\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\nYou need to monitor resource utilization to determine the source of the performance issues.\nWhich metric should you monitor?","isMC":true,"choices":{"B":"Cache hit percentage","C":"DWU limit","A":"DWU percentage","D":"Data IO percentage"},"answer":"A","answer_description":"Monitor and troubleshoot slow query performance by determining whether your workload is optimally leveraging the adaptive cache for dedicated SQL pools.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache","question_images":[],"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/69248-exam-dp-203-topic-4-question-16-discussion/","unix_timestamp":1641122940,"answer_ET":"B","exam_id":67,"discussion":[{"poster":"HaBroNounen","content":"correct","timestamp":"1672658940.0","comment_id":"514949","upvote_count":"8"},{"poster":"nadavw","comment_id":"1364823","timestamp":"1741079280.0","content":"Selected Answer: A\nDWU percentage will show issues in resources","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: A\nCache hit percentage: This measures how often data is retrieved from cache, but it won't directly indicate issues with resource utilization causing slow queries.\n\nDWU percentage\n\nThe DWU (Data Warehouse Units) percentage represents the resource utilization in terms of the computational power allocated to the data warehouse. Monitoring this metric can help identify if the current DWU allocation is insufficient for handling the load of commonly used queries, leading to slow performance.\n\nplease correct if my thinking is not wrong","poster":"Asheesh1909","timestamp":"1739983740.0","comment_id":"1358856"},{"content":"Selected Answer: B\nrepeted","upvote_count":"2","poster":"kkk5566","timestamp":"1725092700.0","comment_id":"994932"},{"poster":"Deeksha1234","timestamp":"1691912580.0","upvote_count":"4","content":"correct","comment_id":"646196"}],"topic":"4","answers_community":["A (60%)","B (40%)"]},{"id":"2ww8THzRFTE5dyxWfLqy","answers_community":["A (88%)","12%"],"question_images":[],"topic":"4","question_text":"You have an Azure Databricks resource.\nYou need to log actions that relate to changes in compute for the Databricks resource.\nWhich Databricks services should you log?","isMC":true,"answer_images":[],"exam_id":67,"unix_timestamp":1639315320,"choices":{"B":"workspace","E":"jobs","C":"DBFS","A":"clusters","D":"SSH"},"answer_description":"","timestamp":"2021-12-12 14:22:00","answer_ET":"A","answer":"A","question_id":310,"discussion":[{"timestamp":"1639315320.0","comment_id":"500023","content":"It shall be A:Clusters, workspace logs does not have any cluster related resource change.","comments":[{"timestamp":"1687702920.0","upvote_count":"5","content":"A cluster is defined within the workspace and cluster events are logged at the workspace level. See \"Cluster Events\" in the following doc:https://docs.databricks.com/administration-guide/account-settings/audit-logs.html","comment_id":"933676","poster":"Abakwagirl"}],"poster":"azure9876","upvote_count":"23"},{"poster":"Deeksha1234","comment_id":"646202","upvote_count":"5","content":"Selected Answer: A\nA is correct","timestamp":"1660377180.0"},{"comment_id":"1263318","timestamp":"1723267200.0","content":"ChatGPT : clusters\n\nExplanation:\nClusters: In Azure Databricks, clusters are the key compute resource. Changes to clusters, such as creation, termination, and resizing, directly relate to compute changes. Logging actions related to clusters will help you track compute-related events.\nWorkspace: Logging related to workspace typically involves monitoring user activities within the Databricks environment, such as notebook access, but it does not directly relate to compute changes.\nDBFS: The Databricks File System (DBFS) relates to storage, not compute resources.\nSSH: SSH logging is related to Secure Shell (SSH) connections, not specifically to Databricks compute resources.\nJobs: While jobs involve executing tasks on clusters, they don't specifically log changes in the compute resources themselves.\nThus, to monitor and log compute-related changes in Databricks, focusing on the clusters service is the appropriate choice.","poster":"RG_123","upvote_count":"2"},{"upvote_count":"1","comment_id":"1187983","timestamp":"1712056020.0","content":"Selected Answer: A\nYou can't take logs from the workspace! Audit Logs are cluster-scoped: you need clusters.","poster":"MBRSDG"},{"content":"Selected Answer: A\nClusters are the most important service to log for changes in compute, as they represent the compute resources that are used to run workloads. By logging cluster-related actions, you can track when clusters are created, deleted, or modified. This information can be used to troubleshoot performance issues and ensure that compute resources are being used efficiently.","poster":"Azure_2023","timestamp":"1705248180.0","comment_id":"1122655","upvote_count":"2"},{"poster":"matiandal","comment_id":"1065692","timestamp":"1699453680.0","content":"Answer : A.Clusters\n\nwhy not workspace ?\n \n Workspace is not a service that you should log to track changes in compute for the Databricks resource because it does not record events related to creating, editing, deleting, starting, or stopping clusters or jobs. Workspace events are related to actions performed on the workspace itself, such as creating, renaming, deleting, or importing notebooks, folders, libraries, or repos1. These events do not affect the compute \nresources used by the Databricks resource, but rather the workspace content and configuration. \nTherefore, workspace is not a relevant service for logging compute changes.","upvote_count":"1"},{"timestamp":"1693470360.0","upvote_count":"1","poster":"kkk5566","comment_id":"994933","content":"Selected Answer: A\nA is correct"},{"comment_id":"981287","poster":"[Removed]","timestamp":"1692074820.0","content":"https://learn.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/audit-log-delivery\n\nMS document says cluster","upvote_count":"1"},{"content":"Selected Answer: A\nCluster is the only compute first class resource","comment_id":"946215","timestamp":"1688797020.0","poster":"[Removed]","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: A\nA. Clusters: Databricks clusters are the primary compute resources in Azure Databricks. Monitoring and logging cluster-related actions will help you track changes in cluster creation, termination, resizing, and other cluster-related activities.","poster":"vctrhugo","timestamp":"1687380180.0","comment_id":"929907"},{"poster":"Ast999","upvote_count":"3","comment_id":"828769","timestamp":"1677920880.0","content":"Selected Answer: A\n100% SURE A IS A CORRECT ANSWER."},{"content":"Selected Answer: A\ndefinitely A","comment_id":"609773","poster":"demirsamuel","upvote_count":"3","timestamp":"1654007760.0"},{"comment_id":"609753","timestamp":"1654004820.0","poster":"upliftinghut","upvote_count":"1","content":"Selected Answer: B\nWorkspace is correct. Detail is here: \nSet-AzDiagnosticSetting -ResourceId $databricks.ResourceId -WorkspaceId $logAnalytics.ResourceId -Enabled $true -name \"<diagnostic setting name>\" -Category <comma separated list>\n\n\nLink: https://docs.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/azure-diagnostic-logs#configure-diagnostic-log-delivery"},{"upvote_count":"3","comment_id":"602358","timestamp":"1652666340.0","content":"I thought compute is related to cluster.","poster":"Mckay_"},{"timestamp":"1652524020.0","upvote_count":"4","comments":[{"comment_id":"646198","timestamp":"1660376700.0","poster":"Deeksha1234","content":"agree A should be the answer","upvote_count":"2"}],"comment_id":"601516","poster":"KashRaynardMorse","content":"Selected Answer: A\nAnswer: A (clusters)\nDispite using workspace to enable logging, from there you need to select clusters form the list if you want to satisfy the \"changes in compute for the Databricks resource\" question, hence the service you sould log is clusters. See link from Amsterliese.\nBeware of links to databricks.com vs links to microsoft because they are two slightly different products (i.e. Databricks (on AWS) vs Azure Databricks).\nFor the other comment referencing dp200; the answer description only gives the defintions but no explanation."},{"upvote_count":"2","comment_id":"585282","timestamp":"1649863200.0","poster":"Amsterliese","content":"From what I understand from MS documentation, it should be \nA - clusters\nhttps://docs.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/azure-diagnostic-logs#configure-diagnostic-log-delivery\nThe links in previous comments here which support answer B - workspace refer to AWS databricks. I tried to find a similar setup in the MS documentation, but couldn't find anything. Please tell me if my thinking is wrong. (Always happy to learn ;)"},{"timestamp":"1645799220.0","content":"Selected Answer: A\nAgreed with clusters!","poster":"ovokpus","upvote_count":"2","comment_id":"556030"},{"comment_id":"543043","timestamp":"1644325200.0","content":"A clusters","upvote_count":"1","poster":"kanak01"},{"comment_id":"530440","timestamp":"1642933260.0","content":"Selected Answer: A\ncompute is related to the cluster","upvote_count":"3","poster":"svik"},{"poster":"PallaviPatel","timestamp":"1642488540.0","comment_id":"526353","upvote_count":"4","content":"Selected Answer: A\nA is correct answer, as compute relates to clusters."},{"content":"Selected Answer: A\nA should be correct answer.\nhttps://www.examtopics.com/exams/microsoft/dp-200/view/17/","upvote_count":"4","comment_id":"520421","poster":"Canary_2021","timestamp":"1641756300.0"},{"upvote_count":"1","timestamp":"1640717820.0","poster":"Canary_2021","comment_id":"511495","content":"What kind of changes belong to 'changes in compute for the Databricks resource'? Any example?"},{"poster":"ItHYMeRIsh","upvote_count":"4","content":"Selected Answer: B\nThe answer is correct. The workspace logs contain information about cluster events.\n\nhttps://docs.databricks.com/administration-guide/account-settings/audit-logs.html#audit-events","timestamp":"1639420380.0","comments":[{"poster":"azure9876","comment_id":"501868","timestamp":"1639543320.0","content":"Please check this table:\nhttps://docs.databricks.com/administration-guide/account-settings/audit-logs.html#workspace-level-audit-log-events\nIn general, all logs are belongs to workspace-level audit log if you check the title of the table. But if you check in details, cluster related logs belongs to clusters part.","upvote_count":"3"}],"comment_id":"500817"}],"url":"https://www.examtopics.com/discussions/microsoft/view/67733-exam-dp-203-topic-4-question-17-discussion/"}],"exam":{"isImplemented":true,"numberOfQuestions":384,"name":"DP-203","isBeta":false,"provider":"Microsoft","lastUpdated":"12 Apr 2025","isMCOnly":false,"id":67},"currentPage":62},"__N_SSP":true}