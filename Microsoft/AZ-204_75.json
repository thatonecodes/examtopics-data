{"pageProps":{"questions":[{"id":"OHLMOdLxDxiphd17DnuG","topic":"5","question_images":[],"choices":{"A":"Yes","B":"No"},"isMC":true,"timestamp":"2024-10-30 13:40:00","exam_id":48,"discussion":[{"content":"Selected Answer: B\nNo,\nA funnel in Application Insights is used to analyze user flow and conversion paths, helping you understand how users navigate through your application.\n\nonfigure an action group in Azure Monitor","upvote_count":"4","timestamp":"1730292000.0","poster":"Mattt","comment_id":"1304987"}],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure App Service web app named WebApp1 and an Azure Functions app named Function1. WebApp1 is associated with an Application Insights instance named appinsights1.\n\nYou configure a web test and a corresponding alert for WebApp1 in appinsights1. Each alert triggers a delivery of email to your mailbox.\n\nYou need to ensure that each alert also triggers execution of Function1.\n\nSolution: Configure an Application Insights funnel.\n\nDoes the solution meet the goal?","answer_ET":"B","unix_timestamp":1730292000,"answer":"B","answers_community":["B (100%)"],"answer_images":[],"answer_description":"","question_id":371,"url":"https://www.examtopics.com/discussions/microsoft/view/150512-exam-az-204-topic-5-question-55-discussion/"},{"id":"fiwxIwFIGzlHYAiIL3gQ","isMC":true,"exam_id":48,"answer_ET":"BC","answer_description":"","answer_images":[],"timestamp":"2024-10-30 13:58:00","topic":"5","question_id":372,"unix_timestamp":1730293080,"answers_community":["AB (40%)","BC (30%)","BD (30%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/150514-exam-az-204-topic-5-question-56-discussion/","question_text":"Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground -\n\nFourth Coffee is a global coffeehouse chain and coffee company recognized as one of the world’s most influential coffee brands. The company is renowned for its specialty coffee beverages, including a wide range of espresso-based drinks, teas, and other beverages. Fourth Coffee operates thousands of stores worldwide.\n\n\nCurrent environment -\n\nThe company is developing cloud-native applications hosted in Azure.\n\n\nCorporate website -\nThe company hosts a public website located at http://www.fourthcoffee.com/. The website is used to place orders as well as view and update inventory items.\n\n\nInventory items -\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its menu to include inventory items such as lunch items, snacks, and merchandise. Corporate team members constantly update inventory. Users can customize items. Corporate team members configure inventory items and associated images on the website.\n\n\nOrders -\nAssociates in the store serve customized beverages and items to customers. Orders are placed on the website for pickup.\n\nThe application components process data as follows:\n\n1. Azure Traffic Manager routes a user order request to the corporate website hosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for customers tenant.\n4. Users search for items and place an order on the website as item images are pulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized items to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve performance.\n\nThe application consists of the following Azure services:\n\n//IMG//\n\n\n\nRequirements -\n\nThe application components must meet the following requirements:\n\n• Azure Cosmos DB development must use a native API that receives the latest updates and stores data in a document format.\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before deployment to Azure. Testing must support the latest versions of the Azure Storage APIs.\n\n\nCorporate website -\n• User authentication and authorization must allow one-time passcode sign-in methods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network latency.\n\n\nInventory items -\n• Customized items read from Azure Cosmos DB must maximize throughput while ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable updates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to include exchangeable image file format (data) stored with the blob data upon upload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\n\n\nOrders -\n• Orders must receive inventory item changes automatically after inventory items are updated or saved.\n\n\nIssues -\n\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear text manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected application performance.\n\n\nYou need to mitigate the Azure Cache for Redis issue.\n\nWhat are two possible ways to achieve this goal? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.","question_images":["https://img.examtopics.com/az-204/image535.png"],"discussion":[{"comment_id":"1344257","poster":"Zezere","upvote_count":"3","content":"Selected Answer: BD\nA: cannot be true. Rebooting the test instance will not imrpov the state of prod.\nB: true (https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-failover)\nC: The failover is due to too much load, at one point in time (during the maintenance). Eviction is not pertinent since all keys are correct at this point in time\nD: Correct (A: cannot be true. Rebooting the test instance will not imrpov the state of prod.\nB: true (https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-failover)\nC: )\nE: Testing something will not solve the problem","timestamp":"1737481680.0"},{"comment_id":"1336166","poster":"fivestarfrog","content":"Selected Answer: AB\nCorrect answer should be A & B. smetr has provided good explanation of this issue.","upvote_count":"1","timestamp":"1735941300.0"},{"timestamp":"1732439400.0","upvote_count":"3","content":"A-B\nServer maintenance\n\nIf your Azure Cache for Redis underwent a failover, all client connections from the node that went down are transferred to the node that is still running. The server load could spike because of the increased connections. You can try rebooting your client applications so that all the client connections get recreated and redistributed among the two nodes.\nSee Server maintenance section in following link\nhttps://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-troubleshoot-server","poster":"smetr","comment_id":"1316980"},{"timestamp":"1731853500.0","comment_id":"1313568","content":"Selected Answer: AB\nA. Test application code by rebooting all nodes in the test environment.\n\nB. Configure client connections to retry commands with exponential backoff.","poster":"Vichu_1607","upvote_count":"3"},{"comment_id":"1304994","content":"Selected Answer: BC\nissue:\n• Production Azure Cache for Redis maintenance has negatively affected application performance.\n\n\nB is correct\nC. Modify the maxmemory policy to evict the least frequently used keys out of all keys.\nBy configuring the (LFU) keys, you can ensure that the cache retains the most relevant data while removing less frequently accessed data. This can help maintain cache performance and availability during maintenance windows.","upvote_count":"3","timestamp":"1730293080.0","comments":[{"upvote_count":"1","timestamp":"1731499380.0","poster":"overhill","content":"I don't think the problem is the eviction policy but rather the Redis being in maintenance and thus the cache is unavailable, and repeteadly query the redis wont make it better for the app, so exponentially backoff will prevent crazy cycles of retrying.\n\nThat's what I think","comment_id":"1311234"}],"poster":"Mattt"}],"choices":{"B":"Configure client connections to retry commands with exponential backoff.","D":"Increase the maxmemory-reserved and maxfragmentationmemory-reserved values.","A":"Test application code by rebooting all nodes in the test environment.","E":"Test application code by purging the cache in the test environment.","C":"Modify the maxmemory policy to evict the least frequently used keys out of all keys."},"answer":"AB"},{"id":"kGMuYFjPKSEJYARIgmOe","isMC":true,"exam_id":48,"answer_ET":"AB","timestamp":"2024-10-30 14:04:00","answer_description":"","answer_images":[],"topic":"5","question_id":373,"answers_community":["AB (100%)"],"unix_timestamp":1730293440,"url":"https://www.examtopics.com/discussions/microsoft/view/150515-exam-az-204-topic-5-question-57-discussion/","question_images":["https://img.examtopics.com/az-204/image535.png"],"question_text":"Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground -\n\nFourth Coffee is a global coffeehouse chain and coffee company recognized as one of the world’s most influential coffee brands. The company is renowned for its specialty coffee beverages, including a wide range of espresso-based drinks, teas, and other beverages. Fourth Coffee operates thousands of stores worldwide.\n\n\nCurrent environment -\n\nThe company is developing cloud-native applications hosted in Azure.\n\n\nCorporate website -\nThe company hosts a public website located at http://www.fourthcoffee.com/. The website is used to place orders as well as view and update inventory items.\n\n\nInventory items -\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its menu to include inventory items such as lunch items, snacks, and merchandise. Corporate team members constantly update inventory. Users can customize items. Corporate team members configure inventory items and associated images on the website.\n\n\nOrders -\nAssociates in the store serve customized beverages and items to customers. Orders are placed on the website for pickup.\n\nThe application components process data as follows:\n\n1. Azure Traffic Manager routes a user order request to the corporate website hosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for customers tenant.\n4. Users search for items and place an order on the website as item images are pulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized items to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve performance.\n\nThe application consists of the following Azure services:\n\n//IMG//\n\n\n\nRequirements -\n\nThe application components must meet the following requirements:\n\n• Azure Cosmos DB development must use a native API that receives the latest updates and stores data in a document format.\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before deployment to Azure. Testing must support the latest versions of the Azure Storage APIs.\n\n\nCorporate website -\n• User authentication and authorization must allow one-time passcode sign-in methods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network latency.\n\n\nInventory items -\n• Customized items read from Azure Cosmos DB must maximize throughput while ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable updates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to include exchangeable image file format (data) stored with the blob data upon upload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\n\n\nOrders -\n• Orders must receive inventory item changes automatically after inventory items are updated or saved.\n\n\nIssues -\n\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear text manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected application performance.\n\n\nYou need to serve static content from the corporate website.\n\nWhat are two possible ways to achieve this goal? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.","discussion":[{"upvote_count":"6","content":"Selected Answer: AB\nA. Store all static content in Azure Blob Storage. Enable Azure Content Delivery Network for the storage account.\n\nB. Configure App Service networking to create a Content Delivery Network profile and endpoint.","comment_id":"1313570","timestamp":"1731853980.0","poster":"Vichu_1607"},{"poster":"Mattt","comment_id":"1305000","timestamp":"1730293440.0","upvote_count":"3","content":"Selected Answer: AB\nRequirment:\n- Static web content must be stored closest to end users to reduce network latency.\n\nA. is clear\nB.This option enables you to directly link your Azure App Service with a CDN profile, allowing the static content hosted in the App Service to be served efficiently. This setup leverages the benefits of CDN, such as caching and reduced latency for users accessing the static content.\n\n\n\nD is Not Directly Related to Static Content Delivery.\nraffic Manager is designed for routing requests based on performance, priority, or geographic location among different services (e.g., multiple Azure App Services, Azure Functions). It does not store or serve static content itself. The requirement is about serving static content efficiently, which is best achieved with Azure Blob Storage and Azure CDN."}],"choices":{"D":"Create a nested Azure Traffic Manager profile. Configure the parent profile to the performance traffic routing method and the child profile to the priority traffic routing method.","B":"Configure App Service networking to create a Content Delivery Network profile and endpoint.","A":"Store all static content in Azure Blob Storage. Enable Azure Content Delivery Network for the storage account.","C":"Configure the Azure App Service Local Cache feature and set the app setting WEBSITE_LOCAL_CACHE_SIZEINMB value.","E":"Update the Azure Traffic Manager routing method to priority."},"answer":"AB"},{"id":"zB4kt8Xdqf5VqJ1XulUH","unix_timestamp":1730359260,"choices":{"C":"Update the application code to reduce the number of DiagnosticSource events. Use filtering to exclude these events.","B":"Modify the pricing tier for the Log Analytics workspace.","D":"Disable adaptive sampling. Enable and configure the fixed-rate sampling module.","A":"Set a daily cap on the Log Analytics workspace. Create an Activity log alert rule."},"url":"https://www.examtopics.com/discussions/microsoft/view/150551-exam-az-204-topic-5-question-58-discussion/","answer":"D","question_images":[],"timestamp":"2024-10-31 08:21:00","answer_ET":"D","topic":"5","answers_community":["D (86%)","14%"],"answer_description":"","exam_id":48,"question_id":374,"answer_images":[],"discussion":[{"poster":"Zezere","comment_id":"1344895","timestamp":"1737568620.0","comments":[{"upvote_count":"2","content":"Sorry I understood wrong what are adaptive and fixed rate sampling. So now I think D would be correct. \nIf we would like to stay with adaptive sampling, we should set maxTelemetryItemsPerSeconds to a lower value ike it is proposed in thenexct question.","comment_id":"1344912","poster":"Zezere","timestamp":"1737570780.0"}],"content":"Selected Answer: C\nIt cannot be D. We want to reduce the volume of telemetry. \nTo have a constant volume of telemtry, we need to adapt the sampling rate -> adaptive sampling should be activated\n\nC would do the job. We have less source of telemtry, but it will still be statistically correct","upvote_count":"1"},{"poster":"Vichu_1607","upvote_count":"2","comment_id":"1313573","content":"Selected Answer: D\nD: Disable adaptive sampling. Enable and configure the fixed-rate sampling module.","timestamp":"1731854100.0"},{"timestamp":"1731567300.0","poster":"Jay456","content":"Selected Answer: D\nI think it should be D as well. \"Reduce telemetry rates\" is exactly the phrase used on https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling-classic-api#types-of-sampling","upvote_count":"2","comment_id":"1311775"},{"upvote_count":"1","poster":"overhill","comment_id":"1311240","content":"After investigation I think C is the best answer becuse fix rate sampling will probably affect a statistically correct result, limiting diagnostics doesn't hurt statistics","timestamp":"1731500460.0"},{"poster":"overhill","timestamp":"1731500100.0","content":"D) Fixed-rate sampling reduces the volume of telemetry sent from both your ASP.NET or ASP.NET Core or Java server and from your users' browsers. You set the rate. The client and server synchronize their sampling so that, in Search, you can navigate between related page views and requests.\n\nhttps://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling-classic-api","comment_id":"1311238","upvote_count":"1"},{"poster":"Mattt","upvote_count":"2","content":"Selected Answer: D\nI think it should be D\n\nFixed-rate sampling allows you to set a specific sampling rate","timestamp":"1730359260.0","comment_id":"1305312"}],"isMC":true,"question_text":"You develop an ASP. Net Care application by integrating the Application Insights SDK into your solution.\n\nThe application sends a very high rate of telemetry in a short time interval. You observe a reduced number of events, traces, and metrics being recorded and increased error rates for telemetry ingestion. Telemetry data must synchronize the client and server information to allow HTTP request and response correlation.\n\nYou need to reduce telemetry traffic, data costs, and storage costs while preserving a statistically correct analysis of application telemetry data.\n\nWhat should you do?"},{"id":"MMSRaUpBiWjwp04XQFnR","topic":"5","answers_community":["C (83%)","D (17%)"],"discussion":[{"upvote_count":"5","content":"Selected Answer: C\nOption C seems to be the only possible option here. Options A, B and D do nothing to reduce telemetry traffic. Microsoft Documentation states that **all** sampling methods preserve statistically correct analysis of data (https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling-classic-api)","timestamp":"1737450660.0","comment_id":"1344076","poster":"Regex37"},{"content":"Selected Answer: D\nAdaptive sampling should be disabled","poster":"c01efe8","timestamp":"1735648140.0","comment_id":"1334790","upvote_count":"1"}],"unix_timestamp":1735648140,"question_text":"You develop an ASP. Net Care application by integrating the Application Insights SDK into your solution.\n\nThe application sends a very high rate of telemetry in a short time interval. You observe a reduced number of events, traces, and metrics being recorded and increased error rates for telemetry ingestion. Telemetry data must synchronize the client and server information to allow HTTP request and response correlation.\n\nYou need to reduce telemetry traffic, data costs, and storage costs while preserving a statistically correct analysis of application telemetry data.\n\nWhat should you do?","answer_description":"","isMC":true,"timestamp":"2024-12-31 13:29:00","question_images":[],"answer":"C","choices":{"A":"Set a daily cap on the Log Analytics workspace. Create an Activity log alert rule.","B":"Modify the pricing tier for the Log Analytics workspace.","D":"Set retention and archive policies by table in the Log Analytics workspace. Purge retained data beyond 30 days.","C":"Verify adaptive sampling is enabled. Set the maxTelemetryItemsPerSecond value."},"answer_ET":"C","question_id":375,"exam_id":48,"url":"https://www.examtopics.com/discussions/microsoft/view/153714-exam-az-204-topic-5-question-59-discussion/","answer_images":[]}],"exam":{"name":"AZ-204","id":48,"isMCOnly":false,"isBeta":false,"provider":"Microsoft","isImplemented":true,"numberOfQuestions":452,"lastUpdated":"12 Apr 2025"},"currentPage":75},"__N_SSP":true}