{"pageProps":{"questions":[{"id":"Ai8on5G5K3HZd6xvGz6Q","choices":{"A":"Yes","B":"No"},"isMC":true,"answers_community":[],"answer":"B","timestamp":"2021-04-27 05:13:00","url":"https://www.examtopics.com/discussions/microsoft/view/51042-exam-ai-100-topic-2-question-53-discussion/","exam_id":39,"answer_ET":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an app named App1 that uses the Face API.\nApp1 contains several PersonGroup objects.\nYou discover that a PersonGroup object for an individual named Ben Smith cannot accept additional entries. The PersonGroup object for Ben Smith contains\n10,000 entries.\nYou need to ensure that additional entries can be added to the PersonGroup object for Ben Smith. The solution must ensure that Ben Smith can be identified by all the entries.\nSolution: You delete 1,000 entries from the PersonGroup object for Ben Smith.\nDoes this meet the goal?","unix_timestamp":1619493180,"topic":"2","answer_images":[],"question_id":96,"discussion":[{"upvote_count":"1","timestamp":"1687247040.0","content":"B. No\n\nExplanation:\n\nThe given solution does not meet the goal. Deleting 1,000 entries from the PersonGroup object for Ben Smith will not make room for additional entries because the limit for a PersonGroup in the Face API is 10,000 entries. Even if you delete some entries, the PersonGroup will still be at its maximum capacity.","poster":"rveney","comment_id":"928248"},{"comment_id":"360093","upvote_count":"3","timestamp":"1621316280.0","content":"It is \"No\" because Person group can only hold up to 10000 persons. To hold more than this , we would need to create a LargePersonGroup, which holds up to 1000000 people.\nhttps://docs.microsoft.com/en-us/rest/api/faceapi/persongroup","poster":"YSBINW","comments":[{"content":"It's possible to delete faces entries for a person. However the question states \"The solution must ensure that Ben Smith can be identified by all the entries.\" This seems like you need to keep the others so moving to a LargePersonGroup is required to keep the existing 10k. If you think it doesn't require keeping the 10k and just making space for new entries the answer is Yes.\n\nDELETE {Endpoint}/face/v1.0/persongroups/{personGroupId}/persons/{personId}/persistedfaces/{persistedFaceId}","timestamp":"1621511400.0","upvote_count":"2","comment_id":"362110","poster":"soren"}]},{"poster":"PinkUnicorns","comment_id":"358265","content":"I had this in the exam - I selected NO but this was confusing. If anyone can explain. I will come back and check.","timestamp":"1621127220.0","upvote_count":"1"},{"upvote_count":"1","poster":"hkbbboy","timestamp":"1619493180.0","comment_id":"343716","content":"I don't understand the answer, can anyone share your opinion?"}],"answer_description":"","question_images":[]},{"id":"4T9SrCwXfpLKISipgz7M","answer_description":"Using the Batch Execution Activity in an Azure Data Factory pipeline, you can invoke an Azure Machine Learning Studio (classic) web service to make predictions on the data in batch\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning","answer":"A","choices":{"A":"Yes","B":"No"},"exam_id":39,"timestamp":"2021-04-19 16:55:00","isMC":true,"topic":"2","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure SQL database, an Azure Data Lake Storage Gen 2 account, and an API developed by using Azure Machine Learning Studio.\nYou need to ingest data once daily from the database, score each row by using the API, and write the data to the storage account.\nSolution: You create an Azure Data Factory pipeline that contains the Machine Learning Batch Execution activity.\nDoes this meet the goal?","url":"https://www.examtopics.com/discussions/microsoft/view/50475-exam-ai-100-topic-2-question-54-discussion/","answer_images":[],"answer_ET":"A","discussion":[{"poster":"mingchieh2","content":"Question54 & 58, same series, solution only 1 word diff, need to take care \n\nYou create an Azure Data Factory pipeline that contains the Machine Learning Batch Execution activity\n\nYou create an Azure Data Factory pipeline that contains a Machine Learning Execute Pipeline activity\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning","comment_id":"338890","timestamp":"1618844100.0","upvote_count":"7"},{"comment_id":"928250","timestamp":"1687247160.0","upvote_count":"1","poster":"rveney","content":"B. No\n\nExplanation:\n\nThe given solution does not meet the goal. Although Azure Data Factory (ADF) can be used for orchestrating data movement and transformation workflows, it does not directly support the execution of Azure Machine Learning Studio (classic) APIs through its built-in activities.\n\nTo ingest data from the Azure SQL database, score each row using the Azure Machine Learning Studio API, and write the data to the Azure Data Lake Storage Gen2 account, you would need a different approach."}],"answers_community":[],"question_id":97,"question_images":[],"unix_timestamp":1618844100},{"id":"Phh0IbgaNVshQxsHjrF7","answer_description":"We need to schedule the job in Azure Data Factory.","isMC":true,"timestamp":"2020-12-06 16:55:00","discussion":[{"poster":"littleaznman","upvote_count":"5","content":"The other two options were something along the lines of\n- You should use Azure Data Factory with a Jupyter Notebook.\n- Use Azure Data Factory with Machine Learning ML something.","comment_id":"236601","timestamp":"1607270100.0"},{"content":"A. Yes\n\nExplanation:\n\nThe given solution meets the goal. By creating a scheduled Jupyter Notebook in Azure Databricks, you can implement the necessary steps to ingest data from the Azure SQL database, score each row using the Azure Machine Learning Studio API, and write the data to the Azure Data Lake Storage Gen2 account.\n\nAzure Databricks provides a collaborative environment for developing and running Apache Spark-based analytics workloads. With a Jupyter Notebook, you can write Python or Scala code to interact with different data sources, including the Azure SQL database, and utilize the Azure Machine Learning Studio API for scoring.","poster":"rveney","timestamp":"1687247280.0","upvote_count":"1","comment_id":"928252"},{"poster":"YSBINW","comment_id":"360102","content":"I Believe this is a yes, as we can create a scheduled notebook inside of the Databricks.\nAlso, I think that it is possible to make API calls inside of the notebook.\nhttps://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage","timestamp":"1621316760.0","upvote_count":"3"},{"poster":"PinkUnicorns","content":"I selected NO in the exam. But I have a feeling it should have been Yes.","comment_id":"358266","upvote_count":"1","timestamp":"1621127580.0"},{"upvote_count":"1","timestamp":"1613995200.0","content":"Why can't this be true?","comment_id":"296619","poster":"renuka1234"},{"content":"Found the exact text for the other two options:\n- You create an Azure Data Factory pipeline that contains a Jupyter notebook activity.\n- You create an Azure Data Factory pipeline that contains a Machine Learning Execute Pipeline activity.\n\nThe site that I found this in marks both as NO. If ADF is the right choice, then which one is it?","timestamp":"1613359440.0","poster":"Cornholioz","upvote_count":"3","comment_id":"290660"},{"poster":"Cornholioz","upvote_count":"1","comment_id":"290656","content":"Agree ADF can do this. And thanks for the other options littleaznman. But wouldn't Databricks (SQL Analytics) be able to do this too? And which one of the ADF answers is right: Jupyter Notebook or the ML something option?","timestamp":"1613359080.0"},{"upvote_count":"1","content":"Yes, ADF is the best choice!","timestamp":"1611885900.0","poster":"ajforu","comment_id":"278942"}],"exam_id":39,"topic":"2","answer":"B","unix_timestamp":1607270100,"question_images":[],"choices":{"A":"Yes","B":"No"},"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure SQL database, an Azure Data Lake Storage Gen 2 account, and an API developed by using Azure Machine Learning Studio.\nYou need to ingest data once daily from the database, score each row by using the API, and write the data to the storage account.\nSolution: You create a scheduled Jupyter Notebook in Azure Databricks.\nDoes this meet the goal?","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/39022-exam-ai-100-topic-2-question-55-discussion/","answer_ET":"B","question_id":98,"answers_community":[]},{"id":"ZQI4H2oV8ossFJpYq5yO","answer_ET":"B","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/112953-exam-ai-100-topic-2-question-56-discussion/","choices":{"A":"Yes","B":"No"},"timestamp":"2023-06-22 16:19:00","answer_description":"","answer_images":[],"answer":"B","question_images":[],"question_id":99,"discussion":[{"upvote_count":"1","comment_id":"930658","content":"B. No\n\nThe solution mentioned does not meet the goal of ingesting data from the Azure SQL database, scoring each row using the API, and writing the data to the Azure Data Lake Storage Gen2 account.","poster":"rveney","timestamp":"1687443540.0"}],"topic":"2","isMC":true,"exam_id":39,"unix_timestamp":1687443540,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure SQL database, an Azure Data Lake Storage Gen 2 account, and an API developed by using Azure Machine Learning Studio.\nYou need to ingest data once daily from the database, score each row by using the API, and write the data to the storage account.\nSolution: You create an Azure Data Factory pipeline that contains a Jupyter Notebook activity.\nDoes this meet the goal?"},{"id":"xn55DOr0CgYmIiJ8bpUW","unix_timestamp":1627347660,"topic":"2","answers_community":[],"question_id":100,"answer":"","answer_description":"Box 1:\n\nProject type: Classification -\nBox 2:\nDomain: Food (compact)\nIf you want to classify photographs of individual fruits or vegetables, use the Food domain. Compact domains are optimized for the constraints of real-time classification on mobile devices.\nBox 3:\nExport capability: Vision AI Dev Kit\nCustom Vision Service supports the following exports:\n✑ Tensorflow for Android.\n✑ CoreML for iOS11.\n✑ ONNX for Windows ML.\n✑ Vision AI Developer Kit.\n✑ A Docker container for Windows, Linux, or ARM architecture.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/select-domain https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/export-your-model","question_images":["https://www.examtopics.com/assets/media/exam-media/03857/0012400001.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03857/0012500001.png"],"timestamp":"2021-07-27 03:01:00","isMC":false,"answer_ET":"","discussion":[{"upvote_count":"1","content":"box3 is basic platform","timestamp":"1629723960.0","comment_id":"429982","poster":"dijaa"},{"content":"\"For Vision AI Dev Kit, the project must be created with the General (Compact) domain.\"","poster":"YipingRuan","timestamp":"1627347780.0","comment_id":"415004","upvote_count":"1"},{"upvote_count":"1","timestamp":"1627347660.0","comment_id":"415003","content":"\"perishable products in grocery stores\" means fruit?","poster":"YipingRuan"}],"question_text":"HOTSPOT -\nYou are designing a Custom Vision Service solution to identify perishable products in grocery stores. The solution will be deployed as part of a mobile app.\nYou need to recommend the configurations for the Custom Vision API. The solution must minimize the size of the mobile app.\nWhat should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/58740-exam-ai-100-topic-2-question-57-discussion/","exam_id":39}],"exam":{"provider":"Microsoft","numberOfQuestions":206,"lastUpdated":"12 Apr 2025","isMCOnly":false,"name":"AI-100","id":39,"isImplemented":true,"isBeta":false},"currentPage":20},"__N_SSP":true}