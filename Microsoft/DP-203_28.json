{"pageProps":{"questions":[{"id":"l4ygfqThUkQ4gVaizpaq","question_id":136,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/118994-exam-dp-203-topic-2-question-114-discussion/","discussion":[{"poster":"Paulkuzzio","comment_id":"989611","upvote_count":"9","content":"Answer is highly correct. I know this one for sure.","timestamp":"1692927480.0"},{"comment_id":"1286482","poster":"renan_ineu","timestamp":"1726759920.0","content":"Selected Answer: CE\nOne of those questions, let's try to see the catch.\n\n1. To set up or configure source code control in ADF, the source code repository must be created in advance.\n2. Once source code is enabled, pipelines will run from a publishing branch, from the repo.\n\nSo...\nC. You have to create a repo before setting it up in ADF. ADF does not create repos.\nE. You link ADF to a repo by hitting \"Set up\" from the main page. But from the Manager tab, the button's label is \"Configure\".\nF. Once git is set, it runs pipelines from the publishing branch it creates when you set git. If you don't Publish and try to run the pipeline, ADF yields an error. The question does not require you to run right away but if you don't Publish, it will never run.","upvote_count":"1"},{"comment_id":"1148370","poster":"Alongi","content":"Selected Answer: CE\nIt's correct","upvote_count":"1","timestamp":"1707755700.0"},{"poster":"vernillen","comments":[{"poster":"renan_ineu","upvote_count":"1","comment_id":"1286487","timestamp":"1726760100.0","content":"Can be Azure DevOps Git too. ;-)"}],"comment_id":"1111880","upvote_count":"3","content":"Selected Answer: CE\nVersion control = always github","timestamp":"1704198780.0"}],"unix_timestamp":1692927480,"timestamp":"2023-08-25 03:38:00","answers_community":["CE (100%)"],"topic":"2","answer_images":[],"isMC":true,"choices":{"E":"From the Azure Data Factory Studio, select Set up code repository.","F":"From the Azure Data Factory Studio, select Publish.","D":"Create a GitHub action.","B":"Create an Azure Data Factory trigger.","A":"From the Azure Data Factory Studio, run Publish All.","C":"Create a Git repository."},"answer_description":"","question_text":"You have an Azure data factory named ADF1.\n\nYou currently publish all pipeline authoring changes directly to ADF1.\n\nYou need to implement version control for the changes made to pipeline artifacts. The solution must ensure that you can apply version control to the resources currently defined in the Azure Data Factory Studio for ADF1.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.","answer":"CE","exam_id":67,"answer_ET":"CE"},{"id":"pb3380FdNdl0cGQrBgPm","exam_id":67,"answer_images":[],"answer":"D","timestamp":"2023-08-25 03:41:00","isMC":true,"discussion":[{"timestamp":"1739702160.0","content":"Selected Answer: D\ncomparison between schedule and tumbling","comment_id":"1357202","poster":"nadavw","upvote_count":"1"},{"timestamp":"1729952640.0","poster":"swathi_rs","content":"almost always it's tumbling window","comment_id":"1202657","upvote_count":"1"},{"timestamp":"1719750120.0","content":"Selected Answer: D\nTumbling window because of the backfill","comment_id":"1109780","poster":"[Removed]","upvote_count":"2"},{"upvote_count":"1","timestamp":"1718181180.0","comment_id":"1094373","poster":"Lucasmh","content":"Selected Answer: C\ndata is expected to arrive at regular intervals, and you want to trigger a pipeline with a fixed window size.\n\nFor the specific requirements mentioned, especially the need to execute every 30 minutes with a 15-minute offset and backfill data from the beginning of the day to the current time, a schedule trigger is more suitable. The tumbling window trigger is generally used for scenarios where you want to process data in fixed windows based on its arrival time."},{"timestamp":"1711093260.0","content":"Tumbling window is correct.\nBackfill scenario: https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger?tabs=data-factory%2Cazure-powershell#execution-order-of-windows-in-a-backfill-scenario\noffset, concurrency, ... :\nhttps://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger?tabs=data-factory%2Cazure-powershell#tumbling-window-trigger-type-properties","poster":"msb","upvote_count":"3","comment_id":"1013634"},{"upvote_count":"2","poster":"DataEngDP","timestamp":"1710176280.0","content":"schedule because it occurs every 30 minutes","comment_id":"1004929"},{"poster":"kkk5566","upvote_count":"2","content":"Selected Answer: D\nseem to correct","comment_id":"993936","timestamp":"1709208540.0"},{"comment_id":"989613","poster":"Paulkuzzio","content":"It seems correct but the 15mins offset is throwing me off. Somebody please explain. Thanks","timestamp":"1708832460.0","upvote_count":"4","comments":[{"upvote_count":"1","comment_id":"1116960","timestamp":"1720461360.0","content":"offset refers to the delay of start of the trigger : A timespan value that must be negative in a self-dependency. If no value specified, the window is the same as the trigger itself.","poster":"Lewiasskick"},{"timestamp":"1713896880.0","upvote_count":"2","poster":"g2000","comment_id":"1052150","content":"Ensure that only one concurrent pipeline execution can occur <--- this suggests the tumbling window"}]}],"answer_description":"","question_id":137,"topic":"2","question_images":[],"answers_community":["D (83%)","C (17%)"],"answer_ET":"D","question_text":"You have an Azure data factory named ADF1 that contains a pipeline named Pipeline1.\n\nPipeline1 must execute every 30 minutes with a 15-minute offset.\n\nYou need to create a trigger for Pipeline1. The trigger must meet the following requirements:\n\n• Backfill data from the beginning of the day to the current time.\n• If Pipeline1 fails, ensure that the pipeline can re-execute within the same 30-minute period.\n• Ensure that only one concurrent pipeline execution can occur.\n• Minimize development and configuration effort.\n\nWhich type of trigger should you create?","url":"https://www.examtopics.com/discussions/microsoft/view/118995-exam-dp-203-topic-2-question-115-discussion/","choices":{"A":"schedule","C":"manual","B":"event-based","D":"tumbling window"},"unix_timestamp":1692927660},{"id":"FjcIdTuxG3iMwtlHF52u","isMC":true,"choices":{"C":"JSON","B":"ORC","A":"Parquet -\nO. Avro"},"answers_community":["A (100%)"],"exam_id":67,"timestamp":"2023-11-02 11:45:00","answer_description":"","discussion":[{"content":"Answer is B, here is why:\n\nAvro and Parquet are both binary compressed file formats which makes them preferred over CSV which stores the data as strings (much larger files).\n\nNow the difference between Parquet and Avro is the format, as Parquet is column based while Avro provides row based store. Since the requirement is to retrieve rows in their entirety, it is better to use Avro. Scenarios where we only retrieve a subset of columns for analysis would favour the use of Parquet.","upvote_count":"8","timestamp":"1704194100.0","comment_id":"1111801","poster":"jongert"},{"content":"Selected Answer: A\nAvro is row-based, meaning entire rows must be read even if only five columns are needed. This increases query time and reduces efficiency.","comment_id":"1360999","timestamp":"1740400260.0","poster":"Pey1nkh","upvote_count":"1"},{"poster":"JyotiVerma","content":"Avro, because its a row oriented storage offer slight advantage over Parquet which is columnar storage.","timestamp":"1711351260.0","upvote_count":"2","comment_id":"1182277"},{"timestamp":"1708191060.0","poster":"Alongi","content":"Selected Answer: A\nAVRO, because parquet is better for columnar mode","comment_id":"1152726","upvote_count":"1"},{"timestamp":"1706748840.0","content":"AVRO ( just because we want to retrieve the data in its entirety )","comments":[{"upvote_count":"2","poster":"mghf61","content":"Avro is a row-based format and can be a good choice when you need to read or write individual records in a stream. However, for analytical queries that often involve reading multiple rows at once, columnar formats like Parquet are typically more efficient.","comment_id":"1171551","timestamp":"1710233100.0"}],"comment_id":"1137172","poster":"be8a152","upvote_count":"3"},{"timestamp":"1705789620.0","poster":"Azure_2023","comments":[{"comment_id":"1234965","timestamp":"1719004800.0","poster":"Sr18","upvote_count":"1","content":"Parquet will be performing always better considering the fact of read operation. Avro are good for write performance. But parquets are best for analytical and read operations"}],"content":"Parquet is the clear winner","comment_id":"1127529","upvote_count":"2"},{"comment_id":"1060481","timestamp":"1698921900.0","upvote_count":"3","content":"Parquet showed either similar or better results on every test [than Avro]. The query-performance differences on the larger datasets in Parquet’s favor are partly due to the compression results; when querying the wide dataset, Spark had to read 3.5x less data for Parquet than Avro. Avro did not perform well when processing the entire dataset, as suspected.\"\n\nR: https://blog.cloudera.com/benchmarking-apache-parquet-the-allstate-experience/","poster":"matiandal"}],"answer":"A","question_id":138,"url":"https://www.examtopics.com/discussions/microsoft/view/125170-exam-dp-203-topic-2-question-116-discussion/","question_text":"You have an Azure Data Lake Storage Gen2 account named account1 and an Azure event hub named Hub1. Data is written to account1 by using Event Hubs Capture.\n\nYou plan to query account by using an Apache Spark pool in Azure Synapse Analytics.\n\nYou need to create a notebook and ingest the data from account1. The solution must meet the following requirements:\n\n• Retrieve multiple rows of records in their entirety.\n• Minimize query execution time.\n• Minimize data processing.\n\nWhich data format should you use?","topic":"2","unix_timestamp":1698921900,"answer_images":[],"answer_ET":"A","question_images":[]},{"id":"ggE0gkvRX6tQuFvRUn3s","answer":"B","discussion":[{"upvote_count":"5","content":"Selected Answer: B\n\"A Storage Event trigger in Azure Data Factory is designed to initiate a pipeline in response to an event happening in Azure Blob Storage, such as the deletion of a file\"\n\nSo B.","poster":"vernillen","comment_id":"1068614","timestamp":"1699803000.0"},{"upvote_count":"1","poster":"renan_ineu","timestamp":"1726679040.0","content":"Selected Answer: B\nSource: https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory","comment_id":"1285927"},{"timestamp":"1709460600.0","comment_id":"1164693","content":"Selected Answer: B\nI would go with storage event as well","poster":"Delphin_8150","upvote_count":"1"},{"comment_id":"1157716","content":"correct","poster":"efeee333","upvote_count":"1","timestamp":"1708761000.0"}],"choices":{"D":"custom event","C":"tumbling window","A":"schedule","B":"storage event"},"url":"https://www.examtopics.com/discussions/microsoft/view/125855-exam-dp-203-topic-2-question-117-discussion/","question_images":[],"isMC":true,"answer_description":"","answer_ET":"B","question_text":"You have an Azure Blob Storage account named blob1 and an Azure Data Factory pipeline named pipeline1.\n\nYou need to ensure that pipeline1 runs when a file is deleted from a container in blob1. The solution must minimize development effort.\n\nWhich type of trigger should you use?","exam_id":67,"question_id":139,"answers_community":["B (100%)"],"timestamp":"2023-11-12 16:30:00","unix_timestamp":1699803000,"topic":"2","answer_images":[]},{"id":"Wym1GuHwAZF7CZfm3b3x","answer":"","discussion":[{"upvote_count":"5","timestamp":"1730137080.0","comment_id":"1203644","poster":"Alongi","content":"Feature1 & adf_publish"},{"comment_id":"1154782","timestamp":"1724157420.0","content":"The given answer is correct. The diagram in this documentation gives a clear picture of the CI/CD process: https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery#cicd-lifecycle","upvote_count":"4","poster":"ExamKiller42"},{"poster":"jongert","timestamp":"1719912900.0","comment_id":"1111818","upvote_count":"3","content":"Correct, publish branch contains only ADF related code in JSON format. All the source code can be found in the collaboration branch which is by default the main branch.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/source-control"},{"timestamp":"1715091420.0","content":"Seems correct, as there was not a PR of the changes made in the feature branch. Thus, if you \"Publish\" the built will be based on what is in the collab branch, not in the feature branch.","upvote_count":"2","comment_id":"1064986","poster":"y154707"}],"url":"https://www.examtopics.com/discussions/microsoft/view/125594-exam-dp-203-topic-2-question-118-discussion/","question_images":["https://img.examtopics.com/dp-203/image348.png"],"question_text":"HOTSPOT\n-\n\nYou have Azure Data Factory configured with Azure Repos Git integration. The collaboration branch and the publish branch are set to the default values.\n\nYou have a pipeline named pipeline1.\n\nYou build a new version of pipeline1 in a branch named feature1.\n\nFrom the Data Factory Studio, you select Publish.\n\nThe source code of which branch will be built, and which branch will contain the output of the Azure Resource Manager (ARM) template? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_ET":"","isMC":false,"answer_description":"","question_id":140,"exam_id":67,"answers_community":[],"timestamp":"2023-11-07 17:17:00","unix_timestamp":1699373820,"topic":"2","answer_images":["https://img.examtopics.com/dp-203/image349.png"]}],"exam":{"provider":"Microsoft","numberOfQuestions":384,"isMCOnly":false,"name":"DP-203","isImplemented":true,"id":67,"isBeta":false,"lastUpdated":"12 Apr 2025"},"currentPage":28},"__N_SSP":true}