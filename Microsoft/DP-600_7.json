{"pageProps":{"questions":[{"id":"TbnA0ciUbR4UTImCWyOM","topic":"1","answer":"D","answer_ET":"D","exam_id":71,"choices":{"A":"spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")","C":"external_table(ResearchProduct)","B":"spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")","D":"spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")"},"timestamp":"2024-12-01 17:19:00","answers_community":["D (100%)"],"answer_images":[],"question_id":31,"url":"https://www.examtopics.com/discussions/microsoft/view/152436-exam-dp-600-topic-1-question-126-discussion/","question_text":"Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\n\nOverview -\n\nContoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.\n\n\nExisting Environment -\n\n\nIdentity Environment -\n\nContoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.\n\n\nData Environment -\n\nContoso has the following data environment:\n\n• The Sales division uses a Microsoft Power BI Premium capacity.\n• The semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.\n• The Research department uses an on-premises, third-party data warehousing product.\n• Fabric is enabled for contoso.com.\n• An Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.\n• A Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.\n\n\nRequirements -\n\n\nPlanned Changes -\n\nContoso plans to make the following changes:\n\n• Enable support for Fabric in the Power BI Premium capacity used by the Sales division.\n• Make all the data for the Sales division and the Research division available in Fabric.\n• For the Research division, create two Fabric workspaces named Productline1ws and Productline2ws.\n• In Productline1ws, create a lakehouse named Lakehouse1.\n• In Lakehouse1, create a shortcut to storage1 named ResearchProduct.\n\n\nData Analytics Requirements -\n\nContoso identifies the following data analytics requirements:\n\n• All the workspaces for the Sales division and the Research division must support all Fabric experiences.\n• The Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.\n• The Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.\n• For the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.\n• For the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.\n• All the semantic models and reports for the Research division must use version control that supports branching.\n\n\nData Preparation Requirements -\n\nContoso identifies the following data preparation requirements:\n\n• The Research division data for Productline2 must be retrieved from Lakehouse1 by using Fabric notebooks.\n• All the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.\n\n\nSemantic Model Requirements -\n\nContoso identifies the following requirements for implementing and managing semantic models:\n\n• The number of rows added to the Orders table during refreshes must be minimized.\n• The semantic models in the Research division workspaces must use Direct Lake mode.\n\n\nGeneral Requirements -\n\nContoso identifies the following high-level requirements that must be considered for all solutions:\n\n• Follow the principle of least privilege when applicable.\n• Minimize implementation and maintenance effort when possible.\n\n\nWhich syntax should you use in a notebook to access the Research division data for Productline1?","unix_timestamp":1733069940,"isMC":true,"discussion":[{"poster":"Pegooli","content":"Selected Answer: D\nB is not correct because it specifies an incorrect path","upvote_count":"3","timestamp":"1733792460.0","comment_id":"1324294"},{"poster":"MultiCloudIronMan","comment_id":"1320621","timestamp":"1733069940.0","content":"Selected Answer: D\nThis syntax correctly specifies the format as Delta and loads the data from the specified table in the lakehouse.","upvote_count":"1"}],"answer_description":"","question_images":[]},{"id":"7i5QGR2rJWN615bAgAeX","timestamp":"2024-11-30 16:30:00","answer":"B","answer_images":[],"choices":{"D":"From the Endorsement and discovery settings of LH1, select Make discoverable.","A":"Disable Query Caching for the default semantic model.","C":"Enable Refresh for the default semantic model.","B":"From the settings pane of LH1, enable Sync the default Power BI semantic model."},"question_images":[],"exam_id":71,"answer_description":"","isMC":true,"answers_community":["B (100%)"],"answer_ET":"B","discussion":[{"poster":"4d8d76c","upvote_count":"3","timestamp":"1734936660.0","comment_id":"1330704","content":"Manually enable the Sync the default Power BI semantic model setting that will automatically add objects to the semantic model. For more information, see Sync the default Power BI semantic model.\nManually add objects to the semantic model.\nThe auto detect experience determines any tables or views and opportunistically adds them.\n\nThe manually detect option in the ribbon allows fine grained control of which object(s), such as tables and/or views, should be added to the default Power BI semantic model:\n\nSelect all\nFilter for tables or views\nSelect specific objects"},{"upvote_count":"2","timestamp":"1734070920.0","content":"Selected Answer: B\nAccording to the documentation on Learn:\n\"To add objects such as tables or views to the default Power BI semantic model, you have options:\n 1. Manually enable the Sync the default Power BI semantic model setting that will automatically add objects to the semantic model. For more information, see Sync the default Power BI semantic model.\n 2. Manually add objects to the semantic model.\n\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/default-power-bi-semantic-model","comment_id":"1326030","poster":"403a308"},{"comment_id":"1320275","content":"Selected Answer: B\nB is correct","timestamp":"1732980600.0","upvote_count":"2","poster":"MultiCloudIronMan"}],"topic":"1","unix_timestamp":1732980600,"question_id":32,"url":"https://www.examtopics.com/discussions/microsoft/view/152408-exam-dp-600-topic-1-question-127-discussion/","question_text":"You have a Fabric tenant that contains a lakehouse named LH1.\n\nYou create new tables in LH1.\n\nYou need to ensure that the tables are added automatically to the default semantic model.\n\nWhat should you do?"},{"id":"UsWer1auU5WzqKE5lvJz","question_text":"You have a Fabric tenant.\n\nYou are creating a Fabric Data Factory pipeline.\n\nYou have a stored procedure that returns the number of active customers and their average sales for the current month.\n\nYou need to add an activity that will execute the stored procedure in a warehouse. The returned values must be available to the downstream activities of the pipeline.\n\nWhich type of activity should you add?","choices":{"D":"KQL","A":"Append variable","C":"Copy data","B":"Lookup"},"answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/152410-exam-dp-600-topic-1-question-128-discussion/","question_images":[],"exam_id":71,"unix_timestamp":1732980660,"topic":"1","discussion":[{"poster":"MultiCloudIronMan","content":"Selected Answer: B\nAsked multiple times","upvote_count":"5","timestamp":"1732980660.0","comment_id":"1320277"},{"timestamp":"1733790900.0","comment_id":"1324285","content":"Selected Answer: B\nB Lookup activity is the correct answer","upvote_count":"1","poster":"Pegooli"}],"timestamp":"2024-11-30 16:31:00","answers_community":["B (100%)"],"question_id":33,"answer":"B","isMC":true,"answer_ET":"B"},{"id":"p97YU3yBP7wAdpJ1gIua","answer_images":[],"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/152306-exam-dp-600-topic-1-question-129-discussion/","answer_description":"","choices":{"A":"Yes","B":"No"},"answer":"B","question_images":[],"unix_timestamp":1732813200,"timestamp":"2024-11-28 18:00:00","exam_id":71,"question_id":34,"isMC":true,"question_text":"Note: This section contains one or more sets of questions with the same scenario and problem. Each question presents a unique solution to the problem. You must determine whether the solution meets the stated goals. More than one solution in the set might solve the problem. It is also possible that none of the solutions in the set solve the problem.\n\nAfter you answer a question in this section, you will NOT be able to return. As a result, these questions do not appear on the Review Screen.\n\nYour network contains an on-premises Active Directory Domain Services (AD DS) domain named contoso.com that syncs with a Microsoft Entra tenant by using Microsoft Entra Connect.\n\nYou have a Fabric tenant that contains a semantic model.\n\nYou enable dynamic row-level security (RLS) for the model and deploy the model to the Fabric service.\n\nYou query a measure that includes the USERNAME() function, and the query returns a blank result.\n\nYou need to ensure that the measure returns the user principal name (UPN) of a user.\n\nSolution: You update the measure to use the USEROBJECTID() function.\n\nDoes this meet the goal?","answer_ET":"B","discussion":[{"comment_id":"1330602","content":"Selected Answer: B\nuser principal name (UPN) of a user is returned by the USERPRINCIPALNAME() function","timestamp":"1734906600.0","upvote_count":"2","poster":"nappi1"},{"content":"Selected Answer: B\nUSERPRINCIPALNAME() is the correct answer. So B is correct","timestamp":"1733790720.0","poster":"Pegooli","upvote_count":"2","comment_id":"1324283"},{"comment_id":"1319375","timestamp":"1732813200.0","upvote_count":"2","poster":"shorymor","content":"Selected Answer: B\nAnswer is correct. you should use USERPRINCIPALNAME()"}],"topic":"1"},{"id":"3WHKia2AtqvYFKnQ8KA6","answer_images":[],"answers_community":["A (98%)","1%"],"question_images":[],"timestamp":"2024-02-09 12:09:00","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/133434-exam-dp-600-topic-1-question-13-discussion/","answer_ET":"A","topic":"1","answer":"A","question_id":35,"choices":{"B":"Update the data Table3.","C":"Read Table2.","A":"Read Table3.","D":"Update the data in Table1."},"discussion":[{"poster":"conie","upvote_count":"60","comment_id":"1146559","timestamp":"1707588120.0","comments":[{"content":"It seems there is a way to have an external table created using Spark viewed using the Lkehouse SQL End point .\nThis is the spark code I used for testing: \ndf.write.format(\"delta\").option(\"path\",\"Tables/externalfolder\").saveAsTable(\"testmanagedcsveditors\")\nWhen I run the catalog query :\nspark.catalog.listTables(), it is returning as below :\nTable(name='unmanagedcsveditors', catalog='spark_catalog', namespace=['training_lakehouse'], description=None, tableType='EXTERNAL', isTemporary=False)\nNow if I go to the SQL end point, I can see a table named \"externalfolder\" .\nI agree that the table name should be \"umanagedcsveditors\", I can infact see 2 tables in the Lakehouse explorer - externalfolder and unmanagedcsveditors","timestamp":"1717065420.0","poster":"i_have_a_name","comments":[{"upvote_count":"2","timestamp":"1730399220.0","poster":"semauni","content":"Usually if it's this complicated, it's not the answer ;) even if you're right","comment_id":"1305551"}],"comment_id":"1221509","upvote_count":"2"}],"content":"Selected Answer: A\nB & D is out as you can’t update a table in lakehouse using SQL endpoint as this is read only. You will need to use spark or dataflows. \nC is out because when you create external table using spark, you can see the table from the lakehouse but you can’t see the table from SQL endpoint let alone ready.\n\nA is the answer, as I was able to see and read a managed table using SQL Endpoint"},{"content":"Selected Answer: A\nThe right answer is A. A managed table, it is stored within the Fabric storage and becomes immediately accessible through the SQL endpoint upon connection.","upvote_count":"1","timestamp":"1741554060.0","comment_id":"1373418","poster":"contactodonuno"},{"content":"Selected Answer: A\nRead Table3 (A managed table)","timestamp":"1731650220.0","comment_id":"1312427","poster":"Rakesh16","upvote_count":"1"},{"poster":"jass007_k","upvote_count":"1","content":"Correct option is A) as Lakehouse SQL endpoint is read only\nB) and D) are incorrect as Lakehouse SQL endpoint is read only\nOption C) is also incorrect as external tables created using spark in lakehouse are not visible to Lakehouse SQL endpoint\nRefer this document for option c: https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-visualization","timestamp":"1729951980.0","comment_id":"1303299"},{"timestamp":"1724768700.0","poster":"cafb698","upvote_count":"1","comment_id":"1273495","content":"The SQL analytics endpoint operates in read-only mode over lakehouse Delta tables. You can only read data from Delta tables using the SQL analytics endpoint. They can save functions, views, and set SQL object-level security.\nHence, B & D are out. \nExternal Delta tables created with Spark code won't be visible to the SQL analytics endpoint. Use shortcuts in Table space to make external Delta tables visible to the SQL analytics endpoint. \nHence C is out. \n\nCorrect Answer: A"},{"comment_id":"1257485","poster":"Sara0724","timestamp":"1722257580.0","content":"Selected Answer: A\nThe SQL analytics endpoint operates in read-only mode over lakehouse Delta tables.\nExternal Delta tables created with Spark code won't be visible to the SQL analytics endpoint. https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint","upvote_count":"3"},{"timestamp":"1721701320.0","poster":"Kang_24","comment_id":"1253369","content":"Selected Answer: A\nSQL Endpoint cannot update table --> B & D are outed.\nTable 2 is a external table so it won't be displayed in SQL Endpoint view --> C is outed.","upvote_count":"2"},{"timestamp":"1720972740.0","content":"D is the correct answer.\nIn a lakehouse, the tables that can be edited using SQL endpoints are primarily Delta tables. These tables are specifically designed to be compatible with SQL analytics endpoints, allowing you to perform various SQL operations such as querying, creating views, and applying SQL security.\n\nOther table formats like Parquet, CSV, and JSON are not directly editable through SQL endpoints; they need to be converted to Delta format first.","upvote_count":"1","comment_id":"1247841","poster":"Ahmadpbi"},{"upvote_count":"2","content":"Selected Answer: A\nExternal Delta tables created with Spark code won't be visible to the SQL analytics endpoint. \nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint\n\nSo C is out. \n\nB and D are also out since with a SQL endpoint we can't update tables.","timestamp":"1720156560.0","poster":"b65ecca","comment_id":"1242492"},{"timestamp":"1718196960.0","upvote_count":"1","content":"We can read table 1 and table 3 not table 2\nand can't write any of them....\nso ans will be A","comment_id":"1229144","poster":"Parth_Mehta"},{"poster":"282b85d","upvote_count":"2","comment_id":"1219658","content":"Managed tables are fully controlled by the database system. These tables typically allow both read and write operations, including updates, through SQL endpoints.\n\nAfter connecting to Lakehouse1 using its SQL endpoint, you will be able to:\n\n- **Read Table3** (A)\n- **Update the data in Table3** (B)\n\nSo, the correct options are:\n\n**A. Read Table3.**\n**B. Update the data in Table3.**","timestamp":"1716824760.0"},{"upvote_count":"4","comment_id":"1208059","content":"Selected Answer: A\nIMHO, \nthe answer is A.\n\nupdates are prohibited at all. reading spark external table is a big no-no.\nLink: https://www.linkedin.com/pulse/use-shortcuts-instead-external-tables-reference-data-fabric-popovic/","timestamp":"1715119380.0","poster":"stilferx"},{"upvote_count":"2","poster":"rmeng","comment_id":"1205551","content":"Selected Answer: A\nC is not correct because the external tables are not accessible from the endpoint.\nB & D is out of question cause as SQL endpoint is read only.","timestamp":"1714654860.0"},{"poster":"Chrys941","upvote_count":"2","comment_id":"1203288","content":"Selected Answer: A\nD is completely wrong is not typically feasible through a shortcut in SQL endpoint setup. \nB Generally Supported but depends on the SQL endpoint permissions for such operations \n\nA you should be able to read data from table3 since it is a managed table and such operations are standard through Sql Endpoints","timestamp":"1714252380.0"},{"upvote_count":"2","timestamp":"1712757900.0","poster":"GPerez73","comment_id":"1193046","content":"Selected Answer: A\nA is correct. Tested!"},{"content":"Selected Answer: A\nA is correct, D is not. SQL endpoint is read-only.\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint","poster":"a_51","timestamp":"1710872640.0","upvote_count":"4","comments":[{"timestamp":"1710872700.0","content":"In that link it tells you as well to modify you have to switch\n\"To modify data in Lakehouse delta tables, you have to switch to lakehouse mode and use Apache Spark.\"","upvote_count":"1","comment_id":"1177615","poster":"a_51"}],"comment_id":"1177613"},{"poster":"mtroyano","upvote_count":"2","content":"Selected Answer: A\nOptions B and D are incorrect because an endpoint does not support DML operations. Option C is not correct because the external tables are not accessible from the endpoint. The correct answer is A, the managed tables can be read from the connection point.","comment_id":"1177557","timestamp":"1710867960.0"},{"poster":"Aruuu6","comment_id":"1167026","upvote_count":"3","content":"Selected Answer: A\nA is answer","timestamp":"1709715780.0"},{"upvote_count":"3","content":"With SQL endpoint in a Lakehouse you can only read tables not update them. But that only applies to managed tables. External tables have to be read by logging inside the Lakehouse and using Spark dataframe commands.","comment_id":"1159883","poster":"XiltroX","timestamp":"1708961580.0"},{"content":"A. Read Table 3","timestamp":"1708876440.0","upvote_count":"2","poster":"TashaP","comment_id":"1158932"},{"timestamp":"1708259400.0","comment_id":"1153262","content":"I can confirm it is A. I manually went to my fabric tenant to investigate.","upvote_count":"3","poster":"BrandonPerks"},{"poster":"Momoanwar","timestamp":"1708207080.0","upvote_count":"1","comment_id":"1152859","content":"Selected Answer: A\nEnd point dont modify and not read external"},{"timestamp":"1708181280.0","comment_id":"1152614","upvote_count":"1","content":"Answer A is correct\nUsing SQL Endpoint we can only READ tables, so B & D is out\nMoreover for now we can't read external tables using SQL enpoint (https://community.fabric.microsoft.com/t5/General-Discussion/Fabric-SQL-end-point-not-showing-external-delta-tables/m-p/3475969)","poster":"wojciech_wie"},{"comment_id":"1145782","content":"Selected Answer: D\nD is correct because you can update a Delta table.","poster":"IshtarSQL","comments":[{"upvote_count":"2","content":"You can update Delta tables, but not through a SQL endpoint, as that is read-only.","timestamp":"1708610640.0","comment_id":"1156411","poster":"thuss"}],"upvote_count":"1","timestamp":"1707511140.0"},{"comment_id":"1145500","upvote_count":"1","poster":"theseon","content":"Selected Answer: C\nC\n\"The SQL analytics endpoint operates in read-only mode over lakehouse delta tables\"\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint","timestamp":"1707485220.0"},{"comment_id":"1145397","poster":"Fermd","upvote_count":"2","timestamp":"1707476940.0","content":"Selected Answer: A\nThe right answer is A. A managed table, it is stored within the Fabric storage and becomes immediately accessible through the SQL endpoint upon connection.\n\nD is not right becouse tables created using shortcuts might not be immediately accessible through the SQL endpoint for updates."}],"answer_description":"","question_text":"You are the administrator of a Fabric workspace that contains a lakehouse named Lakehouse1. Lakehouse1 contains the following tables:\nTable1: A Delta table created by using a shortcut\nTable2: An external table created by using Spark\n\nTable3: A managed table -\nYou plan to connect to Lakehouse1 by using its SQL endpoint.\nWhat will you be able to do after connecting to Lakehouse1?","unix_timestamp":1707476940,"exam_id":71}],"exam":{"name":"DP-600","provider":"Microsoft","id":71,"numberOfQuestions":179,"isBeta":false,"lastUpdated":"12 Apr 2025","isImplemented":true,"isMCOnly":false},"currentPage":7},"__N_SSP":true}