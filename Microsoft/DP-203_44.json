{"pageProps":{"questions":[{"id":"dljEsw8vaUfwagaX5deG","timestamp":"2021-12-25 03:26:00","discussion":[{"timestamp":"1656116760.0","comment_id":"508935","content":"Selected Answer: B\nAnswer should be B.\nAn external table is based on a source flat file structure. It seems to make no sense to add additional date time columns to such a table.","poster":"Canary_2021","upvote_count":"20"},{"comment_id":"1149635","upvote_count":"1","content":"B is correct","poster":"mghf61","timestamp":"1723580040.0"},{"comment_id":"1076857","poster":"AlejandroU","upvote_count":"1","timestamp":"1716336660.0","content":"Answer B. The answer is incomplete because 2 additional steps were missing. After the 1st step which is creating the external table in the dedicated SQL pool with the additional DateTime column, the 2nd step is to load data using for example PolyBase to load data from the files in container1 into the external table of your dedicated SQL pool. 3rd step is to transform and insert once the data is in the dedicated SQL pool, and then insert the transformed data into your actual Table1, including the additional DateTime column."},{"content":"Selected Answer: B\nB is correct","comment_id":"643607","timestamp":"1675756320.0","poster":"Deeksha1234","upvote_count":"1"},{"poster":"youngbug","upvote_count":"3","content":"From the words in the Solution part, it seems to use PolyBase to read external tables. PolyBase can't change the schemas of external tables(files). You can only transform the data after loading data in the staging directory. And then load the data into tables","timestamp":"1675744260.0","comment_id":"643564"},{"upvote_count":"2","poster":"sdokmak","comment_id":"608945","timestamp":"1669767900.0","content":"Selected Answer: B\nserverless works for data lake\ndedicated doesn't"},{"comment_id":"535654","content":"Its clearly mentioned \"You plan to insert data from the files in container1 into Table1\". External tables dont get the data inserted into themselves, but instead refer outside data.","upvote_count":"4","timestamp":"1659113880.0","poster":"GDJ2022"},{"comment_id":"519849","timestamp":"1657321980.0","poster":"edba","content":"If using dedicated SQL pool, after creating an external table, need a further CTAS for adding derived columns.","upvote_count":"3"}],"answer_description":"","isMC":true,"choices":{"B":"No","A":"Yes"},"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.\nYou have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of\nTable1.\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\nSolution: You use a dedicated SQL pool to create an external table that has an additional DateTime column.\nDoes this meet the goal?","question_images":[],"answers_community":["B (100%)"],"answer":"B","question_id":216,"topic":"2","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/68559-exam-dp-203-topic-2-question-60-discussion/","exam_id":67,"unix_timestamp":1640399160,"answer_ET":"B"},{"id":"huHlYggheNKoDi6oSZ5w","timestamp":"2022-01-03 22:07:00","isMC":true,"answer_description":"","question_id":217,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.\nYou have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of\nTable1.\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\nSolution: You use an Azure Synapse Analytics serverless SQL pool to create an external table that has an additional DateTime column.\nDoes this meet the goal?","exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/69389-exam-dp-203-topic-2-question-61-discussion/","question_images":[],"topic":"2","unix_timestamp":1641244020,"answer_ET":"B","discussion":[{"content":"You can't use serverless pool to create table in dedicate pool","upvote_count":"22","comment_id":"530384","poster":"rainbowyu","timestamp":"1658558340.0"},{"timestamp":"1683205620.0","content":"Selected Answer: B\nTable1 is in dedicated sql pool","poster":"Knoushore1","upvote_count":"7","comment_id":"711233"},{"content":"Selected Answer: B\nThe solution does not meet the goal. While an Azure Synapse Analytics serverless SQL pool can be used to create an external table, this alone will not ensure that the DateTime is stored as an additional column in Table1 when the source data files are loaded to container1. The DateTime would need to be included in the data files themselves or added during the transformation process before loading the data into Table1. Therefore, the proposed solution is incorrect.","poster":"jppdks","comment_id":"1181528","upvote_count":"2","timestamp":"1727174760.0"},{"upvote_count":"1","timestamp":"1723580640.0","comment_id":"1149639","content":"Answer is Yes,","poster":"mghf61"},{"poster":"Azure_2023","content":"Selected Answer: B\nNo. Because of the serverless pool.","upvote_count":"1","timestamp":"1723359480.0","comment_id":"1147078"},{"comments":[{"comment_id":"1003982","content":"correct to yes","poster":"kkk5566","upvote_count":"2","timestamp":"1710080460.0"}],"timestamp":"1709821740.0","upvote_count":"2","content":"Selected Answer: B\nshould be no","poster":"kkk5566","comment_id":"1001547"},{"comment_id":"979275","poster":"[Removed]","content":"Selected Answer: B\nYou can't use serverless pool to create table in dedicate pool","timestamp":"1707736800.0","upvote_count":"1"},{"timestamp":"1707663420.0","comment_id":"978627","poster":"AliakseiM","upvote_count":"2","content":"Selected Answer: B\nB since Table1 is in dedicated pool"},{"comment_id":"971509","content":"Selected Answer: A\nYou can use external tables to read external data using dedicated SQL pool or serverless SQL pool.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","poster":"g2000","upvote_count":"2","timestamp":"1707007980.0"},{"poster":"OldSchool","timestamp":"1685955600.0","comment_id":"735867","content":"Selected Answer: A\nQ:\"You have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of\nTable1.\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\"\nPark for a while Table1 and dedicated SQL pool, that is where the transformation will happen AFTER loading from container1 to Table1.\nHere is about loading data to ADLSG2 continer1 and adding a column which can be done with serverless SQL as an external table.","upvote_count":"3"},{"content":"if table 1 would be serverless, yes, now no","poster":"berend1","timestamp":"1682406240.0","comment_id":"703635","upvote_count":"1"},{"timestamp":"1678959300.0","poster":"emna2022","comment_id":"670573","content":"The job is to insert data from the files in container1 into Table1 (in the dedicated sql pool) and transform the data after that and we need to add a new additional column.\n\nExternal table are just references to the data, only metadata is really stored in the sql pool.\nHence anything including external table will be not a solution.\nIf you follow the different proposed solutions from previous questions, the most efficient solution is to use derived column transformation.","upvote_count":"4"},{"content":"Selected Answer: A\nyes, with serverless pool we can add a new column while creating an external table","timestamp":"1675756860.0","poster":"Deeksha1234","comment_id":"643608","upvote_count":"1"},{"timestamp":"1675745340.0","comment_id":"643567","poster":"youngbug","upvote_count":"1","content":"The aim of the solution is to load data from Data Lake's files to dedicated SQL pool's tables. There are three ways: DF's Copy Activity, PolyBase and Bulk insert. It's not serverless SQL pool's business..."},{"upvote_count":"1","comment_id":"621868","poster":"StudentFromAus","content":"The answer should be yes as we can create an additional column using CETAS in a serverless SQL pool though it is not a complete solution but a step closer to the required result.","timestamp":"1671925320.0"},{"poster":"sdokmak","upvote_count":"2","comment_id":"608944","content":"Serverless pool works for data lake\nDedicated doesn't","timestamp":"1669767840.0"},{"comment_id":"601892","timestamp":"1668492060.0","poster":"nefarious_smalls","content":"Apparently when dealing with dedicated sql pools you can only create an external table by importing the data from source using ctas. However, when using serverless using cetas will actually export a new file to your data source as well as create an external table. With that being said I think the answer is A.","upvote_count":"3"},{"comments":[{"content":"it doesn't say in the link you can add a column using external table, so no.","upvote_count":"3","timestamp":"1685882340.0","comment_id":"735146","poster":"Billybob0604"}],"comment_id":"596791","content":"Selected Answer: A\nAnswer should be Yes\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas#examples","upvote_count":"3","timestamp":"1667571960.0","poster":"Andushi"},{"upvote_count":"1","poster":"ranjsi01","comment_id":"571401","content":"answer is Yes \n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas","comments":[{"upvote_count":"1","comment_id":"588842","timestamp":"1666283340.0","content":"Table1 is not an externa table","poster":"g2000"}],"timestamp":"1663643280.0"},{"timestamp":"1656875220.0","comments":[{"content":"I think it's possible modify the files using cetas, but you have to create very much cetas to modify the files, so I think thw answer is no","upvote_count":"1","timestamp":"1659123360.0","comment_id":"535730","poster":"alex623"},{"comments":[{"content":"Ignore my comments, I got your point thanks :)","comment_id":"615838","poster":"Aditya0891","upvote_count":"1","timestamp":"1670952600.0"},{"content":"edba can you please suggest where in the link is it mentioned that you can use extra columns ?","upvote_count":"1","timestamp":"1670498460.0","comment_id":"613173","poster":"Aditya0891"}],"upvote_count":"6","comment_id":"519851","timestamp":"1657322220.0","poster":"edba","content":"after further looking into it, I think the answer should be YES. pls refer to https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas#examples"}],"poster":"edba","comment_id":"516050","content":"correct to me.","upvote_count":"4"}],"answer":"B","answer_images":[],"answers_community":["B (63%)","A (38%)"],"choices":{"B":"No","A":"Yes"}},{"id":"QuOAJxStfIsbFMAC0LrL","timestamp":"2022-04-20 20:10:00","question_images":[],"answers_community":["B (72%)","A (28%)"],"discussion":[{"upvote_count":"21","comment_id":"591128","timestamp":"1650813000.0","poster":"juanlu46","content":"Selected Answer: B\nIs part of a posible solution, but it isn't sufficient to meet the goal, yo need to pass the \"Get metadata\"'s output as a parameter to the ingest process, processing each file inside a \"for\" loop, for example.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"},{"poster":"oldpony","timestamp":"1654150680.0","comment_id":"610466","content":"Selected Answer: A\nhttps://docs.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity\npoints that Get Metadata activity can retrieve the corresponding Matadata type of: Created datetime of the file or folder.","upvote_count":"11"},{"upvote_count":"3","timestamp":"1719717120.0","content":"Selected Answer: B\nThis is my take on the answer. The question clearly states that \"You need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\" Here the focus should be on the term \"additional column\" which helps us understand that the datetime column does not already exist in the files in container1. \nNow what is Get Metadata activity? As per documentation link https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity \"You can use the Get Metadata activity to retrieve the metadata of any data in Azure Data Factory or a Synapse pipeline\". This will work only if the underlying data files contains the DateTime column which is not the case here. Hence to add an additional column to Table1 in Synapse, Get Metadata activity won't be of help and we have to derive the column through data flow transformation which is the result for Question 59.","comment_id":"1239500","poster":"learnwell"},{"upvote_count":"1","comments":[{"comment_id":"1213207","content":"yes but it won't map as a column","timestamp":"1716019800.0","poster":"jpgsa11","upvote_count":"2"}],"comment_id":"1149640","poster":"mghf61","content":"The Get Metadata activity in Azure Synapse Analytics pipeline can retrieve the metadata of any data, including the DateTime of the files. So Answer is Yes","timestamp":"1707863100.0"},{"timestamp":"1694089860.0","comment_id":"1001550","poster":"kkk5566","content":"Selected Answer: B\nnonono","upvote_count":"1"},{"timestamp":"1691832240.0","upvote_count":"1","poster":"[Removed]","comment_id":"979277","content":"Selected Answer: B\nDoes not meet the goal"},{"timestamp":"1686776880.0","comment_id":"923568","comments":[{"upvote_count":"2","comment_id":"1019301","content":"chatGpt is not to be trusted at all.","comments":[{"upvote_count":"2","poster":"rlnd2000","timestamp":"1711551300.0","content":"But in this case is right :)","comment_id":"1184183"}],"timestamp":"1695861060.0","poster":"Qordata"}],"poster":"vctrhugo","upvote_count":"3","content":"According to ChatGPT:\n\nB. No\n\nThe proposed solution of using a Get Metadata activity in an Azure Synapse Analytics pipeline will retrieve the DateTime of the files, but it does not address the requirement of storing the DateTime as an additional column in Table1."},{"poster":"esaade","comment_id":"831881","content":"No, using a Get Metadata activity in an Azure Synapse Analytics pipeline to retrieve the DateTime of the files does not meet the goal of storing the DateTime as an additional column in Table1. The Get Metadata activity retrieves metadata about the files, such as file size, file name, or last modified date, but it does not provide the file content needed to extract the DateTime value and store it as an additional column in Table1. To achieve the goal, you need to use a data flow in the pipeline that loads the data from container1, extracts the DateTime value, and transforms the data by adding the DateTime column to Table1.","timestamp":"1678192620.0","upvote_count":"5"},{"poster":"OldSchool","content":"If DateTime is part of data in files in container1 than answer is A, but if it is not part of data in files but only Meta data of files then B. Wording in question is really strange but I think it is A because it says \"data from files in container1\"","timestamp":"1669107180.0","upvote_count":"2","comment_id":"724190"},{"timestamp":"1659852960.0","upvote_count":"2","comments":[{"poster":"Dusica","timestamp":"1673583120.0","content":"AGREED","upvote_count":"1","comment_id":"774089"}],"comment_id":"643611","poster":"Deeksha1234","content":"Its confusing, if we need to insert the dateTime of insertion then answer should be No, but if we need to insert the datetime of file modified then answer should be yes.\n\nTo me looks like the question is about 1st case so the answer should be No"},{"poster":"Strix","content":"Selected Answer: B\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables","upvote_count":"2","timestamp":"1658770140.0","comment_id":"636859"},{"content":"I'm confusing more every time a read the solution, I don't know if it says that you have to do it in two setps, that changes everything","poster":"Davico93","comment_id":"622178","upvote_count":"1","timestamp":"1656171600.0"},{"timestamp":"1655209980.0","comment_id":"616222","upvote_count":"3","poster":"MvanG","content":"It seems rather odd that in the same two previous questions \"Use the derived column transformation to generate new columns in your data flow or to modify existing fields.\" was the answer. This is very confusing.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"},{"upvote_count":"2","timestamp":"1650478200.0","poster":"g2000","comment_id":"588880","content":"Get Metadata seems possible\nhttps://www.mssqltips.com/sqlservertip/6246/azure-data-factory-get-metadata-example/"}],"choices":{"A":"Yes","B":"No"},"answer_description":"","question_id":218,"answer_ET":"B","answer_images":[],"answer":"B","exam_id":67,"topic":"2","unix_timestamp":1650478200,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.\nYou have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of\nTable1.\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\nSolution: In an Azure Synapse Analytics pipeline, you use a Get Metadata activity that retrieves the DateTime of the files.\nDoes this meet the goal?","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/73939-exam-dp-203-topic-2-question-62-discussion/"},{"id":"3PzWeOjcl301cWLqQrYx","question_id":219,"discussion":[{"content":"Selected Answer: A\nI think is A. Yes.\nYou can execute R code in a notebook, and then call it from Data Factory.\nYou can check it at \"Databricks Notebook activity\" header:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data\nAnd also: \nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/sparkr/overview","comments":[{"comments":[{"upvote_count":"2","content":"I tried it. I was wrong.","comment_id":"1135954","comments":[{"poster":"biafko","upvote_count":"10","comment_id":"1138473","timestamp":"1722590220.0","content":"Next time only comment when you are sure. People like you are making it more confusing for people who are trying to learn and prepare for the exam."}],"poster":"Gikan","timestamp":"1722349440.0"}],"upvote_count":"1","content":"No is the answer: It is all true, what you wrote down, but Synapse Analytics has got its own \"data factory\". Its name is \"pipeline\". I do not see a chance to set up to data factory the sick as DWH in Synapse.","comment_id":"1135932","poster":"Gikan","timestamp":"1722347400.0"}],"upvote_count":"22","timestamp":"1666623300.0","poster":"juanlu46","comment_id":"591118"},{"comment_id":"1114084","content":"Selected Answer: A\n**VARIATIONS OF THIS QUESTION**\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. **NO**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse. **YES**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse. **NO**\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse. **YES**","upvote_count":"6","poster":"ExamDestroyer69","timestamp":"1720121700.0"},{"upvote_count":"1","timestamp":"1709822040.0","comment_id":"1001555","poster":"kkk5566","content":"Selected Answer: A\nyes can do it"},{"comment_id":"923570","upvote_count":"3","poster":"vctrhugo","content":"Selected Answer: A\nA. Yes\n\nThe proposed solution meets the goal of designing a daily process to ingest incremental data from the staging zone, transform the data using an R script, and insert the transformed data into a data warehouse in Azure Synapse Analytics. The solution involves using an Azure Data Factory (ADF) schedule trigger to execute a pipeline that executes an Azure Databricks notebook and then inserts the data into the data warehouse.","timestamp":"1702595340.0"},{"upvote_count":"4","content":"Yes, this solution meets the goal of ingesting incremental data from the staging zone, transforming the data by executing an R script, and inserting the transformed data into a data warehouse in Azure Synapse Analytics. By using an Azure Data Factory schedule trigger, you can schedule the pipeline to run on a daily basis. The pipeline can execute an Azure Databricks notebook, which can perform the transformation using R scripts, and then insert the transformed data into the data warehouse.","poster":"esaade","comment_id":"831885","timestamp":"1694083080.0"},{"poster":"vrodriguesp","comment_id":"766510","upvote_count":"4","content":"Selected Answer: A\nyes, you can execute R script in notebook and call it via adf","timestamp":"1688544540.0"},{"comment_id":"728363","timestamp":"1685193180.0","content":"Selected Answer: A\nthe answer is YES. I already used this solution in a previous project.","upvote_count":"4","poster":"urielramoss"},{"upvote_count":"2","comment_id":"706247","timestamp":"1682668260.0","content":"should be YES","poster":"rzeng"},{"upvote_count":"4","content":"Selected Answer: A\nWe do sth like it in my company","comment_id":"652527","poster":"dom271219","timestamp":"1677499800.0"},{"timestamp":"1675758000.0","comment_id":"643613","content":"Selected Answer: A\nanswer should be A","poster":"Deeksha1234","upvote_count":"5"},{"timestamp":"1674676080.0","upvote_count":"3","content":"Selected Answer: A\nA.\nR Language is supported in ADB.\nADB notebooks, can be called from ADF pipeline(Use Notebook Activity) to link to the ADB notebook","comment_id":"636877","poster":"Sriramiyer92"},{"timestamp":"1671855420.0","content":"I don't know guys, it's kind of tricky, in 2 next questions, it says \"inser the TRANSFORMED data\" and here it says jus \"DATA\".... what do you think?","poster":"Davico93","upvote_count":"2","comment_id":"621405"},{"upvote_count":"3","timestamp":"1670599020.0","content":"Para mi es la respuesta A. En un pipeline de ADF puede tener una actividad de notebook para databricks, el cual permitirá ejecutar el notebook una vez al día a través de un trigger.","poster":"evega","comment_id":"614007"},{"timestamp":"1668444240.0","content":"Selected Answer: A\nR in notebook and call via Data Factory","comment_id":"601640","poster":"OCHT","upvote_count":"4"},{"content":"Selected Answer: A\nYou can execute R code in a notebook.","poster":"MS_Nikhil","comments":[{"content":"The correct answer should be No, based on the how it is worded and the following logic:\n\nIn Azure Data Factory a Databricks Activity can be used to execute a Databricks notebook. However, it cannot pass the data along to the next activity ( dbutils.notebook.exit(\"returnValue\") only passes a string). Given that the way this is worded it says \" execute a pipeline that executes an Azure Databricks notebook, and then inserts the data \" the \"then\" implies a next step which wont work as cant pass the data along. If the transformation and insert both happened in the notebook only then it would work.\n\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data-databricks-notebook","comments":[{"content":"Yea but you do not have to pass the data along in ADF. You can insert it into Synapse from the notebook.","timestamp":"1668492300.0","comments":[{"timestamp":"1668695760.0","content":"precisely my point, either both things ( R and Insert) should be in the one workbook OR you need two workbooks. The wording indicates 2 steps rather than all in one book: \"notebook\" being the first step and “then” indicating another step.","comment_id":"602918","upvote_count":"2","poster":"hbad","comments":[{"timestamp":"1684960860.0","poster":"Igor85","comment_id":"726249","upvote_count":"2","content":"i don't see any problem to run R and write Synapse dedicated SQL pool in the same notebook\n\nhttps://learn.microsoft.com/en-us/azure/databricks/external-data/synapse-analytics"},{"upvote_count":"1","content":"*2 notebooks =if possible to transfer info between them Imeant","poster":"hbad","comments":[{"content":"It says what the process need, but not necessary in 2 steps","upvote_count":"2","timestamp":"1671989520.0","comment_id":"622175","poster":"Davico93"}],"comment_id":"602921","timestamp":"1668696240.0"}]}],"comment_id":"601893","poster":"nefarious_smalls","upvote_count":"2"}],"timestamp":"1668415380.0","upvote_count":"2","comment_id":"601456","poster":"hbad"}],"comment_id":"600518","upvote_count":"4","timestamp":"1668246840.0"},{"comment_id":"595977","content":"Selected Answer: A\nI agree that Yes","timestamp":"1667389620.0","poster":"romega2","upvote_count":"2"},{"comment_id":"591238","content":"I think it should be Yes. i.e. A\nR Script is well supported by databricks notepad","poster":"gauravgogs","upvote_count":"3","timestamp":"1666647120.0"}],"choices":{"A":"Yes","B":"No"},"unix_timestamp":1650812100,"isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse.\nDoes this meet the goal?","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/74332-exam-dp-203-topic-2-question-63-discussion/","question_images":[],"answer":"A","topic":"2","answers_community":["A (100%)"],"answer_images":[],"timestamp":"2022-04-24 16:55:00","exam_id":67,"answer_ET":"A"},{"id":"WWvXu6R395nGzeMM2aJH","answer_ET":"B","question_id":220,"timestamp":"2022-04-24 17:05:00","answer_description":"","choices":{"B":"No","A":"Yes"},"unix_timestamp":1650812700,"topic":"2","question_images":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse.\nDoes this meet the goal?","discussion":[{"upvote_count":"8","comment_id":"591122","content":"Selected Answer: B\nIs correct.\nMapping Dataflows can't execute R code that is a requeriment, so not meet the goal.","poster":"juanlu46","timestamp":"1666623900.0"},{"comment_id":"1114085","content":"Selected Answer: B\n**VARIATIONS OF THIS QUESTION**\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. **NO**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse. **YES**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse. **NO**\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse. **YES**","poster":"ExamDestroyer69","timestamp":"1720121760.0","upvote_count":"5"},{"upvote_count":"1","poster":"kkk5566","content":"Selected Answer: B\nno is correct","timestamp":"1709822100.0","comment_id":"1001557"},{"comment_id":"652529","poster":"dom271219","content":"Selected Answer: B\nThere is no R in ADF dataflow","timestamp":"1677499980.0","upvote_count":"4"},{"poster":"Deeksha1234","timestamp":"1675758300.0","content":"Selected Answer: B\nB is right","upvote_count":"2","comment_id":"643614"},{"timestamp":"1671653820.0","poster":"Remedios79","content":"The answer is no.","upvote_count":"3","comment_id":"620001"}],"answer_images":[],"answer":"B","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/74334-exam-dp-203-topic-2-question-64-discussion/","exam_id":67,"isMC":true}],"exam":{"isMCOnly":false,"lastUpdated":"12 Apr 2025","isImplemented":true,"provider":"Microsoft","name":"DP-203","numberOfQuestions":384,"id":67,"isBeta":false},"currentPage":44},"__N_SSP":true}