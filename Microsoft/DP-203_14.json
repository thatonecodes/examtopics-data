{"pageProps":{"questions":[{"id":"PltMgyOwfh2Jday0QyMm","question_text":"HOTSPOT -\nYou have the following Azure Stream Analytics query.\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer":"","answer_ET":"","isMC":false,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/75015-exam-dp-203-topic-1-question-52-discussion/","timestamp":"2022-05-01 13:56:00","discussion":[{"content":"I feel its all YES. Since it does use a UNION and UNION combines. No matter it repartitions the result is the combination of two sources, a UNION of two sources. Am I missing something here?","comment_id":"615220","comments":[{"content":"I believe the answer to the first question heavily relies on creator's understanding \"what is query\". If it is the last part only (without CTEs) than the answer should be \"yes\", because you have partitioned data that come from CTEs as input for the main query. But if creator's understaning that query is the whole thing, than probably answer should be \"no\", because you're receiving non-partitioned data from sensors.","comment_id":"1023121","poster":"oleg25","timestamp":"1696252980.0","upvote_count":"1"}],"poster":"objecto","timestamp":"1655022360.0","upvote_count":"40"},{"comment_id":"935758","content":"False, True, False.\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/repartition\nThe first is False, because this:\n\"The following example query joins two streams of repartitioned data.\" \nIt's extracted from the link above, and it's pointing to our query! Repartitioned and not partitioned.\nSecond is True, it's explicitly written\nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently.\nThird is False, \n\"In general, six SUs are needed for each partition.\"\n In the example we have 10 positions for step 1 and 10 for step 2, it should be 120 and not 60.","comments":[{"comment_id":"1204507","timestamp":"1714474560.0","content":"they are unioned not joined","upvote_count":"2","poster":"Dusica"}],"upvote_count":"26","poster":"auwia","timestamp":"1687893180.0"},{"content":"yes -yes -no","timestamp":"1738014960.0","upvote_count":"2","comment_id":"1347594","poster":"samianae"},{"timestamp":"1729839720.0","comment_id":"1302790","poster":"ff5037f","upvote_count":"2","content":"the first question is definetly yes \nhttps://learn.microsoft.com/en-us/stream-analytics-query/union-azure-stream-analytics"},{"upvote_count":"2","content":"1) Y\n2) Y The output scheme should match the stream scheme key and count so that each substream can be flushed independently. https://learn.microsoft.com/en-us/azure/stream-analytics/repartition\n3) Y","poster":"dgerok","timestamp":"1713160320.0","comment_id":"1195838"},{"timestamp":"1707346320.0","poster":"rocky48","content":"False, True, False.\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/repartition\nThe first is False, because this:\n\"The following example query joins two streams of repartitioned data.\"\nIt's extracted from the link above, and it's pointing to our query! Repartitioned and not partitioned.\nSecond is True, it's explicitly written\nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently.\nThird is False,\n\"In general, six SUs are needed for each partition.\"\nIn the example we have 10 positions for step 1 and 10 for step 2, it should be 120 and not 60.","upvote_count":"3","comment_id":"1143880"},{"comment_id":"1139160","poster":"Ha_Tran","timestamp":"1706956320.0","upvote_count":"1","content":"The first question should be yes. \"Partitioning lets you divide data into subsets based on a partition key\". https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization"},{"poster":"Anto____","content":"Yes NO NO","comment_id":"1136311","timestamp":"1706665560.0","upvote_count":"3"},{"poster":"Momoanwar","timestamp":"1701891780.0","upvote_count":"8","comment_id":"1089716","content":"Chatgpt say : yes tes no.\nBased on the provided information and additional documentation on Azure Stream Analytics:\n\n1. **Yes**: The query combines two streams of partitioned data [[❞]](https://azure.microsoft.com/fr-fr/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/).\n2. **Yes**: The stream scheme key and count should match the output scheme for optimal independent processing of each substream [[❞]](https://azure.microsoft.com/fr-fr/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/).\n3. **No**: The documentation does not specify that providing 60 streaming units will optimize the query's performance. The appropriate number of streaming units depends on experimentation and resource usage observation [[❞]](https://azure.microsoft.com/fr-fr/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/).","comments":[{"timestamp":"1704869760.0","comment_id":"1118237","content":"You gice one reasoning to chatgpt it will change its answer so i dont think it is reliable resource","poster":"dakku987","upvote_count":"7"}]},{"comments":[{"comments":[{"comment_id":"1063380","poster":"ahmadsayeed","upvote_count":"1","timestamp":"1699226760.0","content":"Calculate the max streaming units for a job\nAll non-partitioned steps together can scale up to one streaming unit (SU V2s) for a Stream Analytics job. In addition, you can add 1 SU V2 for each partition in a partitioned step. You can see some examples in the table below.\n\nQuery Max SUs for the job\nThe query contains one step.\nThe step isn't partitioned.\n1 SU V2\n\n\nThe input data stream is partitioned by 16.\nThe query contains one step.\nThe step is partitioned.\n16 SU V2 (1 * 16 partitions)\n\n\nThe query contains two steps.\nNeither of the steps is partitioned.\n1 SU V2\n\n\nThe input data stream is partitioned by 3.\nThe query contains two steps. The input step is partitioned and the second step isn't.\nThe SELECT statement reads from the partitioned input.\n4 SU V2s (3 for partitioned steps + 1 for non-partitioned steps\n\nBase on the above from MS documentation, why do we need to multiply by 6SUs?"}],"timestamp":"1696763940.0","poster":"ellala","content":"I will correct my previous message. After reading through SU calculation documentation, I concluded it should be 120 for SU V1 or 20 for SU V2. Therefore none of them would be 60. \nExplanation is this sentence here:\n\"All non-partitioned steps together can scale up to one streaming unit (SU V2s) for a Stream Analytics job. In addition, you can add 1 SU V2 for each partition in a partitioned step. \nSo tecnically, it would be 21 SU V2. \nTherefore, 60 SU is not correct. \n\nShould be \nFALSE\nTRUE\nFALSE","upvote_count":"3","comment_id":"1027905"}],"comment_id":"1027892","content":"The answer you need for first and second questions is in Microsoft Documentation:\n\"The following example query joins two streams of repartitioned data. When joining two streams of repartitioned data, the streams must have the same partition key and count. The outcome is a stream that has the same partition scheme. (...) The output scheme should match the stream scheme key and count so that each substream can be flushed independently. \" https://learn.microsoft.com/en-us/azure/stream-analytics/repartition#repartition-input-within-a-single-stream-analytics-job\n\nSo its not two streams of partitioned data, but two streams of REpartitioned data. \nAnd the output stream must have the same partition key and count.\n\nFor the third question, a bit lower in the same link, we get: In general, six SUs are needed for each partition. Therefore, if we have 10 partitions, 6*10 = 60.","poster":"ellala","upvote_count":"3","timestamp":"1696762500.0"},{"upvote_count":"1","comment_id":"1024638","poster":"Vanq69","content":"I think this is an older question since there is SU V2 now.\nAccording to this: https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization\nEach partition would consume 1 Streamiung Unit (SU) V2 and since we have 2 inputs with 10 partitions each it would add up to 20 SU V2, now we have 2 Select statements after the WITH Step which each consume 1 SU V2, so it should add up to 22 SU V2 which would equal 22*6=132 SU V1.","timestamp":"1696413600.0"},{"upvote_count":"1","timestamp":"1696170120.0","comment_id":"1022355","poster":"mav2000","content":"Based on recommended Streaming units,\nStep 1: 10 partitions\nStep 2: 10 partitions\n\n(1*10+1*10) = 20 SU's is the optimal, if you have more, it's not ideal because some SU's are inactive and if you have less, it can cause a bottleneck\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stre\nam-analytics-parallelization#calculate-the-maximum-streaming-units-of-a-job"},{"comment_id":"990213","upvote_count":"1","content":"Question 1 is false; the question says the union of streams and not data. The union combines 2 streams which are the same and thus, the output is the same stream.","poster":"Saintu","timestamp":"1692980580.0"},{"comment_id":"929126","comments":[{"timestamp":"1687893240.0","upvote_count":"4","poster":"auwia","content":"False, true, false.\nI've changed mind completely looking and reading accurately this official link where all 3 questions are answered:\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/repartition\n\nThe first is False, because this:\n\"The following example query joins two streams of repartitioned data.\" \nIt's extracted from the link above, and it's pointing to our query! Repartitioned and not partitioned.\nSecond is True, it's explicitly written\nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently.\nThird is False, \n\"In general, six SUs are needed for each partition.\"\n In the example we have 10 positions for step 1 and 10 for step 2, it should be 120 and not 60.","comment_id":"935759"},{"content":"Again for point 2-False:\nhttps://learn.microsoft.com/en-us/stream-analytics-query/union-azure-stream-analytics\nThe following are basic rules for combining the result sets of two queries by using UNION:\n- The number and the order of the columns must be the same in all queries.\n- The data types must be compatible.\n- Streams must have the same partition and partition count (not scheme key and count! :-) )","timestamp":"1687329960.0","poster":"auwia","comment_id":"929128","upvote_count":"1","comments":[{"content":"All answer is no.","comments":[{"upvote_count":"1","timestamp":"1693813500.0","content":"forgot it, no, yes, no","poster":"kkk5566","comment_id":"998319"}],"upvote_count":"1","comment_id":"998300","poster":"kkk5566","timestamp":"1693813020.0"}]}],"poster":"auwia","timestamp":"1687329660.0","upvote_count":"2","content":"1. True\n2. False:\nIn the context of a UNION operation in Azure Stream Analytics, the stream scheme key and count do not need to match the output schema. The key and count of the output schema are determined based on the input streams being unioned.\n\nWhen performing a UNION operation, the input streams must have compatible schemas, which means that the data types and field names should align. However, the key and count are determined by the input streams themselves and do not need to match the output schema.\n3. True"},{"upvote_count":"1","comment_id":"770570","poster":"UristMcFarmer","content":"I believe the answer to the First Question is No because the execution of the last statement will result in an error because the second query has \"SELECT INTO output\" rather than a straight SELECT. Can anyone confirm that you can UNION two data sets both directed to the same stream with SELECT INTO? It's not functionality shown in any example I've been able to find (in the given answer, linked in other comments on here, and my own research).","timestamp":"1673278140.0"},{"timestamp":"1672038060.0","comment_id":"757150","upvote_count":"5","content":"Is the first option NO because it mentions partitioned data instead of repartitioned data?\n\nReference : https://learn.microsoft.com/en-us/azure/stream-analytics/repartition\n\nThe following example query joins two streams of repartitioned data. When joining two streams of repartitioned data, the streams must have the same partition key and count. The outcome is a stream that has the same partition scheme.\n\nWITH step1 AS (SELECT * FROM input1 PARTITION BY DeviceID),\nstep2 AS (SELECT * FROM input2 PARTITION BY DeviceID)\n\nSELECT * INTO output FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID","poster":"rohanb1986","comments":[{"poster":"vrodriguesp","comment_id":"795945","timestamp":"1675332960.0","content":"yes, the only difference I see is this:\n\n1)this example:\nSELECT * \nINTO output \nFROM \nstep1 PARTITION BY StateID \nUNION \nSELECT * INTO output FROM step2 PARTITION BY StateID #is using another select * on top of step2\n\n2)the doc example:\nSELECT * INTO output \nFROM \nstep1 PARTITION BY DeviceID \nUNION \nstep2 PARTITION BY DeviceID\n\nI think both query joins two streams of repartitioned/partitioned data, so first answer should be yes","upvote_count":"2"}]},{"upvote_count":"3","poster":"XiltroX","comment_id":"735355","timestamp":"1670178840.0","content":"All 3 are YES"},{"comment_id":"731562","poster":"OldSchool","timestamp":"1669816620.0","upvote_count":"1","content":"At first I thought all 3 are Y but then; Strems are Input and UNION combines query results only so in that case first is No."},{"poster":"dduque10","comment_id":"730395","content":"The explanation of the second Yes indicates that the first one is also Yes","upvote_count":"2","timestamp":"1669726440.0"},{"comments":[{"poster":"Bro111","content":"do you have an example of combining two streams, please.","timestamp":"1669397400.0","comment_id":"726982","upvote_count":"3"}],"content":"for the first box, I voted no. I think the correct expression may be something like \"the query combine two partitioned data into one stream\". I do not think there are two streams","timestamp":"1669101900.0","poster":"SomethingRight100","comment_id":"724147","upvote_count":"1"},{"timestamp":"1665980040.0","content":"The reasoning behind the first one could be: union combines two RESULTSETS, not two streams.","poster":"ca_acc","comment_id":"696798","upvote_count":"1"},{"comment_id":"647013","timestamp":"1660535940.0","upvote_count":"4","content":"all three yes","poster":"Deeksha1234"},{"content":"Should be ALL 3 YES.. Union will combine 2 stream output.","comment_id":"631875","upvote_count":"6","timestamp":"1657903860.0","poster":"dsp17"},{"content":"for statement 3,max SU can be calculated as (10*6 + 10*6 + 6) 126. So shouldn't this value be considered optimum and not 60.","poster":"SD_2021","upvote_count":"4","timestamp":"1655985480.0","comments":[{"upvote_count":"3","comment_id":"620950","content":"Based on - https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization","comments":[{"comment_id":"639922","timestamp":"1659246180.0","upvote_count":"1","poster":"Sriramiyer92","content":"Second that thought.\n1. SU count 60 would only be one step.\n2.Needs to add SU count for second step Which is additional 60 SU.\n3.In addition, you can add 6 SUs for each partition in a partitioned step. which would be addition of 6 SU for both steps = 6 x 2 = 12 SU.\nTotal would be > (60 + 60 + 12) = 132 SUs"},{"comment_id":"686615","upvote_count":"2","content":"So, the right answer should be Y, Y, and N. Hope they modify this. Have just shared your answer by reporting to ExamTopics","timestamp":"1664948700.0","poster":"ExamTopicsAshwin"},{"upvote_count":"2","timestamp":"1669814940.0","content":"Both streams are partitioned on the same Partition StreamID so 60 SU is enough","poster":"OldSchool","comment_id":"731519"}],"poster":"SD_2021","timestamp":"1655985540.0"}],"comment_id":"620949"},{"poster":"flaviodiasps","timestamp":"1655660880.0","comment_id":"618770","upvote_count":"7","content":"No reason for NO in the first box.\nShould be all yes"},{"content":"Here, the output is 10 separate partitions where each partition is individually combined but the partitions are not combined. You would have to remove PARTITION BY on both sides of UNION for answer to be Yes","comment_id":"615499","upvote_count":"1","timestamp":"1655068620.0","poster":"Backy"},{"content":"Reading https://docs.microsoft.com/en-us/stream-analytics-query/union-azure-stream-analytics, it is a claer YES to box 1. UNION Combines the results of two or more queries into a single result set that includes all the rows that belong to all queries in the union. The UNION operation is different from using joins that combine columns from two tables.","timestamp":"1654946880.0","comment_id":"614958","poster":"MvanG","upvote_count":"3"},{"timestamp":"1653631020.0","comment_id":"607932","upvote_count":"1","poster":"Rrk07","content":"Agree with explanation"},{"content":"Reading https://docs.microsoft.com/en-us/stream-analytics-query/union-azure-stream-analytics and the second sample given in there I would expect the first one to be No.","comments":[{"poster":"Dicer","comment_id":"635140","upvote_count":"1","content":"First one should yes. Union means combining tables.","timestamp":"1658485140.0"}],"comment_id":"597638","poster":"TacoB","upvote_count":"1","timestamp":"1651825080.0"},{"timestamp":"1651406160.0","content":"Correct","poster":"Akshay_1995","upvote_count":"1","comment_id":"595584"}],"answer_description":"Box 1: No -\nNote: You can now use a new extension of Azure Stream Analytics SQL to specify the number of partitions of a stream when reshuffling the data.\nThe outcome is a stream that has the same partition scheme. Please see below for an example:\nWITH step1 AS (SELECT * FROM [input1] PARTITION BY DeviceID INTO 10), step2 AS (SELECT * FROM [input2] PARTITION BY DeviceID INTO 10)\nSELECT * INTO [output] FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID\nNote: The new extension of Azure Stream Analytics SQL includes a keyword INTO that allows you to specify the number of partitions for a stream when performing reshuffling using a PARTITION BY statement.\n\nBox 2: Yes -\nWhen joining two streams of data explicitly repartitioned, these streams must have the same partition key and partition count.\n\nBox 3: Yes -\nStreaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. The higher the number of SUs, the more CPU and memory resources are allocated for your job.\nIn general, the best practice is to start with 6 SUs for queries that don't use PARTITION BY.\nHere there are 10 partitions, so 6x10 = 60 SUs is good.\nNote: Remember, Streaming Unit (SU) count, which is the unit of scale for Azure Stream Analytics, must be adjusted so the number of physical resources available to the job can fit the partitioned flow. In general, six SUs is a good number to assign to each partition. In case there are insufficient resources assigned to the job, the system will only apply the repartition if it benefits the job.\nReference:\nhttps://azure.microsoft.com/en-in/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0010800002.jpg"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0010700001.png","https://www.examtopics.com/assets/media/exam-media/04259/0010800001.jpg"],"unix_timestamp":1651406160,"question_id":66,"topic":"1","exam_id":67},{"id":"TU3nRHdTOzAq21OeRkwD","question_text":"HOTSPOT -\nYou are building a database in an Azure Synapse Analytics serverless SQL pool.\nYou have data stored in Parquet files in an Azure Data Lake Storege Gen2 container.\nRecords are structured as shown in the following sample.\n{\n\"id\": 123,\n\"address_housenumber\": \"19c\",\n\"address_line\": \"Memory Lane\",\n\"applicant1_name\": \"Jane\",\n\"applicant2_name\": \"Dev\"\n}\nThe records contain two applicants at most.\nYou need to build a table that includes only the address fields.\nHow should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2022-05-05 07:42:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011000001.jpg"],"answer_ET":"","answer":"","isMC":false,"question_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/75193-exam-dp-203-topic-1-question-53-discussion/","discussion":[{"poster":"Deeksha1234","content":"Correct answer","timestamp":"1659025500.0","upvote_count":"20","comment_id":"638793"},{"upvote_count":"1","poster":"Piantoni","comment_id":"1311003","timestamp":"1731457380.0","content":"1) CREATE EXTERNAL TABLE\n2) OPENROWSET"},{"content":"1) CREATE EXTERNAL TABLE (because this is SERVERLESS sql pool)\n2) OPENROWSET","timestamp":"1712827620.0","poster":"dgerok","comment_id":"1193642","upvote_count":"3"},{"comment_id":"998321","timestamp":"1693813560.0","content":"correct","upvote_count":"1","poster":"kkk5566"},{"comments":[{"poster":"vctrhugo","timestamp":"1687639200.0","upvote_count":"2","comment_id":"932914","content":"\"You have data stored in Parquet files in an Azure Data Lake Storege Gen2 container.\""},{"timestamp":"1663924920.0","upvote_count":"15","content":"because serverless SQL pool does not have internal tables","comment_id":"676953","poster":"NORLI"}],"upvote_count":"2","comment_id":"651273","poster":"cjb0","content":"Why is it External Table if it's a Serverless SQL pool?","timestamp":"1661347560.0"},{"poster":"dkamat","content":"correct","timestamp":"1656397740.0","comment_id":"623778","upvote_count":"3"},{"upvote_count":"3","timestamp":"1652158440.0","poster":"SandipSingha","comment_id":"599400","content":"correct"},{"upvote_count":"4","timestamp":"1651729320.0","poster":"Feljoud","comment_id":"597132","content":"correct"}],"topic":"1","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011100001.jpg"],"answer_description":"Box 1: CREATE EXTERNAL TABLE -\nAn external table points to data located in Hadoop, Azure Storage blob, or Azure Data Lake Storage. External tables are used to read data from files or write data to files in Azure Storage. With Synapse SQL, you can use external tables to read external data using dedicated SQL pool or serverless SQL pool.\nSyntax:\nCREATE EXTERNAL TABLE { database_name.schema_name.table_name | schema_name.table_name | table_name }\n( <column_definition> [ ,...n ] )\nWITH (\nLOCATION = 'folder_or_filepath',\nDATA_SOURCE = external_data_source_name,\nFILE_FORMAT = external_file_format_name\n\nBox 2. OPENROWSET -\nWhen using serverless SQL pool, CETAS is used to create an external table and export query results to Azure Storage Blob or Azure Data Lake Storage Gen2.\nExample:\n\nAS -\nSELECT decennialTime, stateName, SUM(population) AS population\n\nFROM -\nOPENROWSET(BULK 'https://azureopendatastorage.blob.core.windows.net/censusdatacontainer/release/us_population_county/year=*/*.parquet',\nFORMAT='PARQUET') AS [r]\nGROUP BY decennialTime, stateName\n\nGO -\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables","unix_timestamp":1651729320,"answers_community":[],"exam_id":67},{"id":"LmDzPpQ4cOmk747LxHnK","exam_id":67,"topic":"1","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011200001.jpg"],"timestamp":"2022-04-27 12:19:00","answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/74671-exam-dp-203-topic-1-question-54-discussion/","question_id":68,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011300001.jpg"],"answer_description":"Box 1: blob -\nThe following example creates an external data source for Azure Data Lake Gen2\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\nTYPE = HADOOP)\n\nBox 2: HADOOP -\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables","discussion":[{"comment_id":"593044","timestamp":"1651054740.0","content":"1. dfs (for Azure Data Lake Storage Gen2)","upvote_count":"69","poster":"galacaw","comments":[{"content":"Correct, https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated#location--prefixpath","poster":"Rob77","comment_id":"904856","timestamp":"1684842480.0","upvote_count":"5"},{"poster":"panda_azzurro","comments":[{"poster":"suvec","comment_id":"866079","upvote_count":"7","content":"dfs is valid \nData Lake Storage Gen2\nabfs[s] <container>@<storage_account>.dfs.core.windows.net\nhttp[s] <storage_account>.dfs.core.windows.net/<container>/subfolders\nwasb[s] <container>@<storage_account>.blob.core.windows.net","timestamp":"1681110960.0"}],"content":"dfs is not valid","upvote_count":"1","timestamp":"1674821880.0","comment_id":"789575"},{"poster":"jds0","content":"This table corroborates that \"dfs\" should be used for ADLS Gen 2:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#location","upvote_count":"1","timestamp":"1680079380.0","comment_id":"854181"},{"poster":"Vedjha","comment_id":"794076","timestamp":"1675165380.0","upvote_count":"9","content":"CREATE EXTERNAL DATA SOURCE mydatasource\nWITH ( LOCATION = 'abfss://data@storageaccount.dfs.core.windows.net',\n CREDENTIAL = AzureStorageCredential,\n TYPE = HADOOP\n)"}]},{"poster":"Kure87","upvote_count":"39","timestamp":"1669240380.0","content":"1. blob. Acoording with this article https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop we only use DFS (abfss endpoint) when your account has secure transfer enabled. \n\nOn the question the location starts with \"https://account1.\" not \"abfss://\"","comment_id":"725408","comments":[{"poster":"tlb_20","upvote_count":"5","comment_id":"1196561","timestamp":"1713268620.0","content":"As it is written in this MSF example on how to create an External data source based on Azure storage account:\n\"\nTYPE = HADOOP, -- For dedicated SQL pool\n-- TYPE = BLOB_STORAGE, -- For serverless SQL pool\n\"\nhttps://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/2-transform-data-using-create-external-table-select-statement"},{"poster":"vadiminski_a","timestamp":"1679655360.0","content":"That's not correct, you use abfss:// if you have secure transfer enabled. There is nothing wrong with using https:// when you don't have secure transfer enabled. However, for DLSv2 you need to specify .dfs. ...\nThe correct answer is is:\ndfs\nhadoop","upvote_count":"7","comment_id":"849232"},{"comment_id":"1343013","content":"I agree with DFS but remember this is possible on blob storage:\nhttp[s] <storage_account>.blob.core.windows.net/<container>/subfolders\n\nI created an linked service from my synapse to storage account explicitly using blob.","upvote_count":"1","poster":"JustImperius","timestamp":"1737293040.0"},{"comment_id":"750541","timestamp":"1671515220.0","upvote_count":"3","poster":"Sebastian1677","content":"please upvote this"}]},{"poster":"samianae","timestamp":"1738015800.0","content":"dfs and hadoop","comment_id":"1347603","upvote_count":"1"},{"comment_id":"1302799","upvote_count":"2","timestamp":"1729841640.0","poster":"ff5037f","content":"answer is correct. as location prefix is https. if the container location prefix is abfs then it is dfs, if it is wbfs then it is blob .\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated#location--prefixpath\n\nData Lake Storage Gen2 http[s] <storage_account>.dfs.core.windows.net/<container>/subfolders"},{"content":"Dedicated SQL Pool: Use CREATE EXTERNAL DATA SOURCE with TYPE = HADOOP for accessing Azure Data Lake Storage Gen2.\nServerless SQL Pool: Use OPENROWSET for direct querying of the external data.","timestamp":"1719757080.0","comment_id":"1239681","poster":"ypan","upvote_count":"1"},{"upvote_count":"2","poster":"Charley92","comment_id":"1197322","content":"CREATE EXTERNAL DATA SOURCE MyDataSource\nWITH (\n TYPE = HADOOP,\n LOCATION = 'abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/',\n CREDENTIAL = <your-credential-name>\n);","timestamp":"1713405240.0"},{"timestamp":"1713160800.0","content":"answer is correct\nsee the MS example \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#example-for-create-external-data-source","upvote_count":"1","comment_id":"1195841","poster":"dgerok"},{"poster":"ankeshpatel2112","content":"Correct Answer : DFS \nExplnation : If your source is ADLS Gen2 then it would be \"DFS\" and if your source is Azure Blob Storage then \"Blob\". \n\nPlease refer below table from Microsoft Documentation. \nExternal Data Source Connector location prefix Location path\nData Lake Storage* Gen1 adl <storage_account>.azuredatalake.net\nData Lake Storage Gen2 abfs[s] <container>@<storage_account>.dfs.core.windows.net\nAzure Blob Storage wasbs <container>@<storage_account>.blob.core.windows.net\nAzure Blob Storage https <storage_account>.blob.core.windows.net/<container>/subfolders\nData Lake Storage Gen1 http[s] <storage_account>.azuredatalakestore.net/webhdfs/v1\nData Lake Storage Gen2 http[s] <storage_account>.dfs.core.windows.net/<container>/subfolders\nData Lake Storage Gen2 wasb[s] <container>@<storage_account>.blob.core.windows.net","upvote_count":"2","comment_id":"1186570","timestamp":"1711858200.0"},{"timestamp":"1711310520.0","upvote_count":"1","content":"Should be DFS for Datalake Gen2","comment_id":"1181951","poster":"Alongi"},{"upvote_count":"1","poster":"Momoanwar","timestamp":"1703052180.0","content":"Both `blob` and `dfs` endpoints work when connecting to Azure Data Lake Storage Gen2, but they serve different purposes. The `blob` endpoint is typically used for standard storage operations, while the `dfs` endpoint is optimized for hierarchical file system operations and is preferred for analytics workloads with Azure Synapse Analytics.","comment_id":"1101289","comments":[{"timestamp":"1703052240.0","poster":"Momoanwar","upvote_count":"2","comment_id":"1101291","content":"To simply access files in Azure Data Lake Storage Gen2 for reading and analysis, without the need for Data Lake specific features like directory management or fine-grained ACLs, using the `blob` endpoint is sufficient. If your operations are primarily related to accessing files for reading, the `blob` endpoint can be used in the external data source definition within Azure Synapse Analytics."}]},{"upvote_count":"3","content":"Answer is CORRECT: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#example-for-create-external-data-source\n\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP)","poster":"Qordata","comment_id":"1018647","timestamp":"1695806040.0"},{"poster":"fahfouhi94","comment_id":"1013002","content":"Ans : dfs & hadoop\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated","timestamp":"1695293880.0","upvote_count":"1"},{"upvote_count":"1","poster":"kkk5566","content":"CREATE EXTERNAL DATA SOURCE AzureDataLakeStore\nWITH\n -- Please note the abfss endpoint when your account has secure transfer enabled\n ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' ,\n CREDENTIAL = ADLS_credential ,\n TYPE = HADOOP\n ) ;\n\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TY\n\n\nHADOOP, blob","timestamp":"1693813740.0","comment_id":"998326"},{"content":"dfs should be the correct answer (ADLS Gen2)","upvote_count":"1","comment_id":"990165","poster":"kdp203","timestamp":"1692975180.0"},{"content":"Confirmed HADOOP and DFS:\nExternal Data Source | Connector | Location path\n----------------------------------------------------------------------------------------------------------------------------\nData Lake Storage Gen1 | adl | <storage_account>.azuredatalake.net\nData Lake Storage Gen2 | abfs[s] | <container>@<storage_account>.dfs.core.windows.net\nAzure Blob Storage | wasbs | <container>@<storage_account>.blob.core.windows.net\nAzure Blob Storage | https | <storage_account>.blob.core.windows.net/<container>/subfolders\nData Lake Storage Gen1 | http[s] | <storage_account>.azuredatalakestore.net/webhdfs/v1\nData Lake Storage Gen2 | http[s] | <storage_account>.dfs.core.windows.net/<container>/subfolders\nData Lake Storage Gen2 | wasb[s] | <container>@<storage_account>.blob.core.windows.net","upvote_count":"3","comment_id":"929175","timestamp":"1687334220.0","poster":"auwia"},{"content":"CREATE EXTERNAL DATA SOURCE source1\nWITH (\n LOCATION = 'https://account1.dfs.core.windows.net',\n TYPE = HADOOP\n)","poster":"auwia","upvote_count":"1","timestamp":"1687162260.0","comment_id":"927293"},{"poster":"aga444","timestamp":"1686212520.0","upvote_count":"1","comment_id":"918006","content":"CREATE EXTERNAL DATA SOURCE DataSourceName\nWITH (\n TYPE = HADOOP,\n LOCATION = 'adl://Account1.dfs.core.windows.net/',\n CREDENTIAL = SqlPoolCredential\n);"},{"poster":"janaki","timestamp":"1684863960.0","upvote_count":"1","content":"CREATE EXTERNAL DATA SOURCE <datasource_name>\nWITH (\n TYPE = HADOOP,\n LOCATION = 'adl://<account_name>.dfs.core.windows.net',\n CREDENTIAL = <credential_name>\n);\n\nSo answer is dfs and Type = Hadoop","comment_id":"905121"},{"content":"1. blob\n2. TYPE=HADOOP\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\nThe following example creates an external data source for Azure Data Lake Gen2 pointing to the publicly available New York data set:\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP)","timestamp":"1684030980.0","comment_id":"897205","upvote_count":"3","poster":"Reloadedvn"},{"comment_id":"881073","timestamp":"1682481540.0","poster":"Fredward_95","upvote_count":"2","content":"Hadoop is the only allowed type in dedicated SQL pools and for ADLS Gen2 with http[s] prefix it's definitely dfs as you can lookup here \nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated#location--prefixpath"},{"timestamp":"1681110840.0","poster":"suvec","comment_id":"866076","content":"Data Lake Storage Gen2\nabfs[s] <container>@<storage_account>.dfs.core.windows.net\nhttp[s] <storage_account>.dfs.core.windows.net/<container>/subfolders\nwasb[s] <container>@<storage_account>.blob.core.windows.net","upvote_count":"1"},{"timestamp":"1681045020.0","content":"Answer is: Blob and Hadoop\n\nI found this in Azure documentation: \nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP)\n\nLink:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","poster":"deutscher","comment_id":"865513","upvote_count":"3"},{"content":"correct answer","upvote_count":"1","poster":"[Removed]","comment_id":"850116","timestamp":"1679747700.0"},{"poster":"Camarade_Emile","comment_id":"837415","timestamp":"1678656840.0","content":"1) dfs because of \"Dedicated SQL Pool\n2) HADOOP\nreference: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","upvote_count":"2"},{"content":"dfs, HADOOP\nSince the data is stored in DL Gen2\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","timestamp":"1676968740.0","poster":"baiy","upvote_count":"1","comment_id":"816370"},{"content":"LOCATION = 'https://account1.blob.core.windows.net',\nTYPE = HADOOP","upvote_count":"1","timestamp":"1675137900.0","comment_id":"793616","poster":"akk_1289"},{"comment_id":"787917","content":"The following example creates an external data source for Azure Data Lake Gen2 pointing to the publicly available New York data set:\n\nSQL\n\nCopy\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP)\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#tabpanel_2_hadoop\n\npublic sets/no credentials = blob otherwise dfs","poster":"Lestrang","upvote_count":"1","timestamp":"1674665520.0"},{"upvote_count":"2","content":"1. blob\n2. Hadoop \nI think correct answer is blob here because the main point is that credentials are not given and so public access to account1 should have been already given previously. \nHadoop is the driver use in dedicated SQL pool to connect with external tables.","timestamp":"1673258100.0","comment_id":"770219","poster":"akad9"},{"poster":"pk07","content":"TYPE=HADOOP as it's a dedicated pool. \ndfs \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-data-source:\nAzure Data Lake Store Gen 2 http[s] storage_account>.dfs.core.windows.net/<container>/subfolders","timestamp":"1673199720.0","upvote_count":"2","comment_id":"769673"},{"timestamp":"1673014560.0","comment_id":"767785","content":"dfs for datalake gen2, (data file system ):\n\nAzure Data Lake Store Gén. 2 http[s] <storage_account>.dfs.core.windows.net/<container>/subfolders","poster":"aws123","upvote_count":"2"},{"upvote_count":"2","comment_id":"735786","poster":"vigilante89","timestamp":"1670231220.0","content":"PLEASE CHECK THIS LINK:\nhttps://stackoverflow.com/questions/64566781/azure-blob-use-cases-for-abfss-vs-https\n\nCorrect Answer:\n1) blob\n2) Hadoop"},{"comment_id":"720697","poster":"OldSchool","content":"1. dfs because we don't have blob storage but ADLS G2 see here: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#:~:text=Azure%20Data%20Lake%20Store%20Gen%202,dfs.core.windows.net/%3Ccontainer%3E/subfolders\n2. HADOOP","comments":[{"comment_id":"733625","timestamp":"1669976280.0","poster":"OldSchool","upvote_count":"1","content":"Disregard previous answer. Given answer is correct, blob and hadoop."}],"upvote_count":"1","timestamp":"1668705900.0"},{"timestamp":"1668673200.0","comment_id":"720320","upvote_count":"2","content":"Correct Answer :\n - Blob, because the URL of Location does not contains abfss endpoint\n- Type = HADOOP","poster":"shyemko"},{"timestamp":"1660504380.0","poster":"memoje","content":"1. dfs when the Storage Account is an ADLS\n2. Hadoop, it is stated in the documentation bellow\n\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=sql-server-ver16&tabs=dedicated#type---hadoop--blob_storage--2","comment_id":"646891","upvote_count":"2"},{"timestamp":"1659258180.0","poster":"strato","content":"https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=sql-server-ver16&tabs=dedicated\n\nAccording to this it's:\n1. adls (not hadoop)\n2. dfs","comments":[{"content":"Your link is from SQL Server not Azure Synapse Analytics. There're several differences.","timestamp":"1661632440.0","poster":"youngbug","comment_id":"652691","upvote_count":"1"}],"comment_id":"640013","upvote_count":"1"},{"timestamp":"1659082200.0","content":"The following example creates an external data source for Azure Data Lake Gen2 pointing to the publicly available New York data set:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP)","comment_id":"639079","poster":"vishal10","upvote_count":"5"},{"timestamp":"1659026520.0","poster":"Deeksha1234","comment_id":"638803","upvote_count":"9","content":"blob and Hadoop is correct, since data source is being created without credentials \n\nThe following example creates an external data source for Azure Data Lake Gen2 pointing to the publicly available New York data set:\n\nSQL\n\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP)\n\nref- https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop"},{"upvote_count":"1","timestamp":"1658846100.0","content":"I think the blob is the correct answer. Possible location: Bulk Operations https <storage_account>.blob.core.windows.net/<container> \nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=sql-server-ver16&tabs=dedicated","comment_id":"637467","poster":"Janisys"},{"comment_id":"625208","upvote_count":"3","timestamp":"1656587460.0","poster":"SKN0865","content":"Acc to https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop you reference to a ADLS Gen 2 with Location prefix: \"http[s]\" and Location Path: \"<storage_account>.dfs.core.windows.net/<container>/subfolders\". So dfs sounds right."},{"poster":"sethuramansp","comment_id":"623382","content":"The answer would be Type = Hadoop and location = dfs given the data source is ADLS Gen2. If the Data source is Blob storage then location of blob will fit in.Please refer the following link for the same.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","timestamp":"1656344940.0","upvote_count":"3"},{"poster":"NamitSehgal","timestamp":"1656033960.0","content":"dfs is correct\nCREATE EXTERNAL DATA SOURCE AzureDataLakeStore\nWITH\n -- Please note the abfss endpoint when your account has secure transfer enabled\n ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' ,\n CREDENTIAL = ADLS_credential ,\n TYPE = HADOOP\n ) ;","comment_id":"621394","upvote_count":"3"},{"comment_id":"619503","content":"DFS is correct I think","poster":"jalwinjs","timestamp":"1655769060.0","upvote_count":"1"},{"timestamp":"1654433100.0","comment_id":"611827","content":"From the Microsoft documentation it seems.......To create data source for publicly available dataset blob is correct and if dataset need credential to access then dfs is used.\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","poster":"kiranSargar","upvote_count":"4"},{"comment_id":"602876","timestamp":"1652782440.0","upvote_count":"1","poster":"Jmanuelleon","content":"Es confuso.... en la definición para location, indica usar DFS,https://docs.microsoft.com/es-es/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#location, pero en el ejemplo que aparece mas abajo, usa lo contrario, https://docs.microsoft.com/es-es/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#example-for-create-external-data-source (En el ejemplo siguiente se crea un origen de datos externo para Azure Data Lake Gen2 que apunta al conjunto de datos de Nueva York disponible públicamente: CREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP))"},{"comment_id":"602668","upvote_count":"2","poster":"hbad","timestamp":"1652726400.0","content":"It is hadoop and dfs. For dfs see link below location section: \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop"},{"timestamp":"1651828560.0","content":"From https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#example-for-create-external-data-source\n\nThe following example creates an external data source for Azure Data Lake Gen2 pointing to the publicly available New York data set:\n\nSQL\n\nCopy\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP)","upvote_count":"14","poster":"LetsPassExams","comment_id":"597650"},{"poster":"LetsPassExams","comment_id":"597649","content":"I thin answer is correct:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#example-for-create-external-data-source","upvote_count":"1","timestamp":"1651828380.0"},{"poster":"shrikantK","timestamp":"1651223520.0","upvote_count":"3","comment_id":"594346","content":"dfs is the answer as question is about Azure Data Lake Storage Gen2 . if question was about blob storage then answer would have been blob."},{"content":"1. is DFS","poster":"Andushi","comment_id":"593874","timestamp":"1651156080.0","upvote_count":"3"},{"upvote_count":"4","timestamp":"1651089240.0","poster":"Andushi","content":"I agree with galacaw is dfs and type Hadoop","comment_id":"593377"}],"unix_timestamp":1651054740,"answers_community":[],"answer_ET":"","isMC":false,"question_text":"HOTSPOT -\nYou have an Azure Synapse Analytics dedicated SQL pool named Pool1 and an Azure Data Lake Storage Gen2 account named Account1.\nYou plan to access the files in Account1 by using an external table.\nYou need to create a data source in Pool1 that you can reference when you create the external table.\nHow should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//"},{"id":"ivGCnMwPr1MewDyooF0R","topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/75450-exam-dp-203-topic-1-question-55-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011300002.png"],"discussion":[{"upvote_count":"29","poster":"namtn6","timestamp":"1656653640.0","content":"If the answer has Parquet. Of course, you should choose that answer. :D","comment_id":"625586"},{"comment_id":"602963","upvote_count":"15","comments":[{"content":"Good point, also better cost","comment_id":"606959","timestamp":"1653439200.0","upvote_count":"3","poster":"sdokmak"}],"timestamp":"1652799000.0","poster":"ClassMistress","content":"Selected Answer: B\nAutomatic creation of statistics is turned on for Parquet files. For CSV files, you need to create statistics manually until automatic creation of CSV files statistics is supported."},{"content":"Selected Answer: B\nIf it has Parquet, odds are very good.","timestamp":"1734425460.0","poster":"GigachadPrime","comment_id":"1327816","upvote_count":"1"},{"timestamp":"1732778460.0","comment_id":"1319067","content":"Selected Answer: B\nSelected Answer: B","poster":"EmnCours","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nPARQUET","poster":"dgerok","timestamp":"1712828520.0","comment_id":"1193654"},{"poster":"phydev","comment_id":"1056891","content":"Parquet is always the answer ;-)","timestamp":"1698591000.0","upvote_count":"5"},{"content":"Selected Answer: B\nis correct","upvote_count":"1","timestamp":"1693813800.0","poster":"kkk5566","comment_id":"998328"},{"content":"Selected Answer: B\nParquet","upvote_count":"1","timestamp":"1691392800.0","comment_id":"974472","poster":"akhil5432"},{"poster":"ajhak","content":"Selected Answer: B\nWhen in doubt, select Parquet.","comment_id":"900645","timestamp":"1684372260.0","upvote_count":"8"},{"comment_id":"692059","upvote_count":"5","content":"Selected Answer: B\nWhen reading from Parquet files, you can specify only the columns you want to read and skip the rest.","poster":"greenlever","timestamp":"1665488880.0"},{"comment_id":"653767","upvote_count":"2","content":"CORRECT","timestamp":"1661827860.0","poster":"Rahuar"},{"upvote_count":"3","poster":"monibun","timestamp":"1661405340.0","comment_id":"651651","content":"Question is bit contradictory: it mentions reading blob storage data in dedicated sql , which could be done by External Tables, however, dedicated sql pool do NOT support automatic stats for external tables (as mentioned on \"automatic stats creation for dedicated sql pool\" section- https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-statistics","comments":[{"upvote_count":"1","comment_id":"1111062","content":"@monibun : great point. I agree with you . I am not sure if anyone wants to advise on your point .","timestamp":"1704101640.0","poster":"Ranjan6214"}]},{"poster":"Deeksha1234","upvote_count":"2","comment_id":"638797","timestamp":"1659026040.0","content":"Parquet"},{"comment_id":"599818","poster":"shachar_ash","content":"Correct","upvote_count":"2","timestamp":"1652233260.0"}],"answer_ET":"B","choices":{"C":"Avro","D":"CSV","B":"Parquet","A":"JSON"},"answers_community":["B (100%)"],"exam_id":67,"answer_description":"","answer_images":[],"timestamp":"2022-05-11 03:41:00","question_id":69,"unix_timestamp":1652233260,"question_text":"You have an Azure subscription that contains an Azure Blob Storage account named storage1 and an Azure Synapse Analytics dedicated SQL pool named\nPool1.\nYou need to store data in storage1. The data will be read by Pool1. The solution must meet the following requirements:\nEnable Pool1 to skip columns and rows that are unnecessary in a query.\n//IMG//\n\n✑ Automatically create column statistics.\n✑ Minimize the size of files.\nWhich type of file should you use?","isMC":true,"answer":"B"},{"id":"5t542yysZs7g69e0cr3Y","discussion":[{"upvote_count":"23","timestamp":"1668704640.0","comment_id":"602968","poster":"ClassMistress","content":"I think it is Hash because the question refer to a Fact table."},{"content":"1. Hash -> Fact Table\n2. DateKey -> for Partition","poster":"hereiamken","timestamp":"1692714120.0","comment_id":"818033","upvote_count":"17"},{"content":"1) HASH, because this is a fact table\n2) OrderDateKey (simply see the partition values - these are dates)","poster":"dgerok","timestamp":"1728640980.0","comment_id":"1193671","upvote_count":"2"},{"timestamp":"1709545920.0","content":"the syntax is ok only for HASH &\nDatekey","comment_id":"998334","upvote_count":"3","poster":"kkk5566"},{"content":"Why not 'Product Key' for partition? can anyone explain me please.","timestamp":"1709208240.0","comment_id":"994987","upvote_count":"1","comments":[{"timestamp":"1711218120.0","poster":"MJamesP","content":"Because partitioning on the date key will help in deleting older data quickly since the older records' partition can be moved to a different table and the table truncated.","upvote_count":"4","comment_id":"1015099"}],"poster":"kumarsunny"},{"timestamp":"1703433120.0","poster":"VittalManikonda","comment_id":"932611","content":"if it is round robin, there is no key to specify, so hash","upvote_count":"2"},{"comment_id":"850394","timestamp":"1695662220.0","content":"It must be HASH because of syntax.","upvote_count":"5","poster":"[Removed]"},{"timestamp":"1691070240.0","comment_id":"797167","upvote_count":"3","content":"The Answer is correct","poster":"SHENOOOO"},{"upvote_count":"2","comment_id":"789023","timestamp":"1690390320.0","poster":"astone42","content":"The answer is correct."},{"poster":"DindaS","upvote_count":"3","content":"Should be Round Robin as the requirement is to have the data evenly. the second one should be on the date","comment_id":"783804","comments":[{"content":"You would use Round-Robin for staging table and not Fact table.","timestamp":"1702981500.0","upvote_count":"3","comment_id":"927300","poster":"auwia"},{"upvote_count":"1","comment_id":"842599","content":"It should, but they have given attributes. So only hash supports attribute","timestamp":"1695019800.0","poster":"Ritik37"}],"timestamp":"1689973080.0"},{"upvote_count":"4","timestamp":"1682682660.0","comment_id":"706416","poster":"allagowf","content":"<distribution_option> ::=\n { \n DISTRIBUTION = HASH ( distribution_column_name ) \n | DISTRIBUTION = ROUND_ROBIN \n | DISTRIBUTION = REPLICATE\n } \n\n+ fact table\nit's for sure ::: hash"},{"comment_id":"690556","timestamp":"1681076340.0","content":"data is distributed evenly across partitions and data is deleted once a year not frequently. So it should be Round-robin distribution.","upvote_count":"3","poster":"greenlever"},{"timestamp":"1677204360.0","poster":"Rajashekharc","upvote_count":"5","comment_id":"651030","content":"Cannot be Round Robin, the syntax of distribution for round robin don't mention/include Column Name. So it has to be HASH"},{"poster":"Deeksha1234","timestamp":"1675066020.0","upvote_count":"2","comment_id":"639498","content":"Answer is correct"},{"comments":[{"timestamp":"1678055520.0","comment_id":"660536","upvote_count":"4","poster":"anks84","content":"syntax used is for HASH distribution."}],"comment_id":"639108","poster":"Dicer","timestamp":"1674991920.0","upvote_count":"1","content":"should be round robin because of distributed evenly."},{"upvote_count":"1","comment_id":"636702","poster":"Franz58","content":"correct","timestamp":"1674657240.0"},{"content":"I think the first answer should be Round-Robin as it should be distributed evenly.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","poster":"jebias","comment_id":"591142","upvote_count":"2","timestamp":"1666625760.0","comments":[{"upvote_count":"26","timestamp":"1666703160.0","poster":"Feljoud","comment_id":"591723","content":"While you are right, that Round-Robin guarantees an even distribution, it is only recommended to use on small tables < 2 GB (see your link). Using the Hash of the ProductKey will also allow for an even distribution but in a more efficient manner.\nAlso, the Syntax here would be wrong if you would insert Round-Robin. As in that case it would only say: \"DISTRIBUTION = ROUND-ROBIN\" (no ProductKey)","comments":[{"comment_id":"730437","content":"For small tables is recommended replicated, not round robin","upvote_count":"1","timestamp":"1685359380.0","poster":"dduque10"},{"timestamp":"1673890020.0","upvote_count":"2","content":"@Feljoud : Thanks for the clarification. Even I opted for Roundrobin, considering the keywords = \"distributed evenly\", but that's incorrect.","comment_id":"632239","poster":"sivva"},{"content":"You are exactly righty","timestamp":"1667885580.0","comment_id":"598425","poster":"nefarious_smalls","upvote_count":"1"}]},{"upvote_count":"6","comment_id":"594526","timestamp":"1667054880.0","poster":"Massy","content":"the syntax is ok only for HASH"},{"timestamp":"1666952760.0","poster":"Muishkin","comment_id":"593746","content":"yes i think so too","upvote_count":"1"}]}],"answers_community":[],"answer":"","timestamp":"2022-04-24 17:36:00","question_id":70,"answer_ET":"","topic":"1","exam_id":67,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011600001.jpg"],"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/74340-exam-dp-203-topic-1-question-56-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011500001.jpg"],"unix_timestamp":1650814560,"question_text":"DRAG DROP -\nYou plan to create a table in an Azure Synapse Analytics dedicated SQL pool.\nData in the table will be retained for five years. Once a year, data that is older than five years will be deleted.\nYou need to ensure that the data is distributed evenly across partitions. The solution must minimize the amount of time required to delete old data.\nHow should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","answer_description":"Box 1: HASH -\n\nBox 2: OrderDateKey -\nIn most cases, table partitions are created on a date column.\nA way to eliminate rollbacks is to use Metadata Only operations like partition switching for data management. For example, rather than execute a DELETE statement to delete all rows in a table where the order_date was in October of 2001, you could partition your data early. Then you can switch out the partition with data for an empty partition from another table.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool"}],"exam":{"name":"DP-203","isMCOnly":false,"lastUpdated":"12 Apr 2025","provider":"Microsoft","numberOfQuestions":384,"isImplemented":true,"isBeta":false,"id":67},"currentPage":14},"__N_SSP":true}