{"pageProps":{"questions":[{"id":"hQF3hiqR0c3FHiBlJkKW","answer":"BCF","topic":"1","discussion":[{"comment_id":"1401891","comments":[{"upvote_count":"1","content":"in this case, the transaction date is referenced. you cannot use it as it is. If you have to track SCD, you have to create your own Date Column, and populate it differently, not from this table","comment_id":"1410135","timestamp":"1742927160.0","poster":"shmmini"}],"upvote_count":"1","timestamp":"1742645640.0","content":"Selected Answer: ABF\nThe dimension table has to track the changes also. Then date field is important for SCD. So Product color takes second priority in this list","poster":"maham"},{"comment_id":"1359381","content":"Selected Answer: BCF\nBCF are the attributes of Product. TranId , Date and SalesAmt quaify for fact table instead","poster":"Madhu2023","timestamp":"1740071940.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"1345366","poster":"robertlavigne","content":"Selected Answer: BCF\nDate: Should be in the FactSales table\nProductName: Attribute of the product and belongs in DimProduct\nProductColour: Attribute of the product and belongs in DimProduct\nTransactionID: Belongs in the FactSales Table\nSales Amount: Belongs in the FactSales Table\nProductID: Will be the primary key for DimProduct","timestamp":"1737636540.0"},{"timestamp":"1734247140.0","content":"Selected Answer: BCF\nColumns to Include in DimProduct\nProductID - This is the unique identifier for each product and acts as the primary key in the DimProduct table.\nProductName - Contains the name of the product, which is a descriptive attribute.\nProductColor - Contains the color description of the product, which is also a descriptive attribute.","comment_id":"1326738","upvote_count":"4","poster":"QAZdbarhate12345678"},{"upvote_count":"3","comment_id":"1323613","content":"Selected Answer: BCF\nI believe the provided answer is correct","timestamp":"1733672460.0","poster":"Tuki93"}],"url":"https://www.examtopics.com/discussions/microsoft/view/152700-exam-dp-700-topic-1-question-7-discussion/","answer_ET":"BCF","exam_id":72,"answer_description":"","question_images":["https://img.examtopics.com/dp-700/image4.png"],"answers_community":["BCF (91%)","9%"],"timestamp":"2024-12-08 16:41:00","isMC":true,"answer_images":[],"question_text":"You have a Fabric workspace that contains a lakehouse named Lakehouse1. Data is ingested into Lakehouse1 as one flat table. The table contains the following columns.\n//IMG//\n\nYou plan to load the data into a dimensional model and implement a star schema. From the original flat table, you create two tables named FactSales and DimProduct. You will track changes in DimProduct.\nYou need to prepare the data.\nWhich three columns should you include in the DimProduct table? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","unix_timestamp":1733672460,"choices":{"D":"TransactionID","A":"Date","B":"ProductName","E":"SalesAmount","C":"ProductColor","F":"ProductID"},"question_id":51},{"id":"txbEUzqOkCmp7nmJC6Ey","timestamp":"2024-12-08 16:43:00","topic":"1","choices":{"C":"Change the runtime version.","D":"Increase the number of executors.","A":"Enable high concurrency for notebooks.","B":"Enable dynamic allocation for the Spark pool."},"exam_id":72,"answer_ET":"A","question_text":"You have a Fabric workspace named Workspace1 that contains a notebook named Notebook1.\nIn Workspace1, you create a new notebook named Notebook2.\nYou need to ensure that you can attach Notebook2 to the same Apache Spark session as Notebook1.\nWhat should you do?","url":"https://www.examtopics.com/discussions/microsoft/view/152701-exam-dp-700-topic-1-question-8-discussion/","isMC":true,"discussion":[{"comment_id":"1323615","poster":"Tuki93","content":"Selected Answer: A\nEnabling high concurrency allows multiple notebooks to share the same Spark session.\n\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/configure-high-concurrency-session-notebooks\n\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/high-concurrency-overview","timestamp":"1733672580.0","upvote_count":"11"},{"comment_id":"1326739","upvote_count":"6","timestamp":"1734247500.0","poster":"QAZdbarhate12345678","content":"Selected Answer: A\nIn Azure Fabric, when you want to share the same Apache Spark session across multiple notebooks, enabling high concurrency mode is essential. High concurrency mode allows multiple notebooks to share the same Spark session, facilitating collaboration and efficient resource usage."}],"answer_description":"","answers_community":["A (100%)"],"unix_timestamp":1733672580,"answer":"A","question_id":52,"answer_images":[],"question_images":[]},{"id":"nC1HZhsp0wcq5a3VZZ1Q","question_text":"You have a Fabric workspace named Workspace1 that contains a lakehouse named Lakehouse1. Lakehouse1 contains the following tables:\n\nOrders -\n\nCustomer -\n\nEmployee -\nThe Employee table contains Personally Identifiable Information (PII).\nA data engineer is building a workflow that requires writing data to the Customer table, however, the user does NOT have the elevated permissions required to view the contents of the Employee table.\nYou need to ensure that the data engineer can write data to the Customer table without reading data from the Employee table.\nWhich three actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","timestamp":"2024-12-06 22:03:00","topic":"1","answers_community":["DEF (82%)","Other"],"unix_timestamp":1733518980,"question_id":53,"answer":"DEF","url":"https://www.examtopics.com/discussions/microsoft/view/152633-exam-dp-700-topic-1-question-9-discussion/","answer_ET":"DEF","exam_id":72,"choices":{"E":"Migrate the Employee table from Lakehouse1 to Lakehouse2.","A":"Share Lakehouse1 with the data engineer.","B":"Assign the data engineer the Contributor role for Workspace2.","C":"Assign the data engineer the Viewer role for Workspace2.","D":"Assign the data engineer the Contributor role for Workspace1.","F":"Create a new workspace named Workspace2 that contains a new lakehouse named Lakehouse2.","G":"Assign the data engineer the Viewer role for Workspace1."},"answer_images":[],"isMC":true,"question_images":[],"discussion":[{"poster":"Etensel","upvote_count":"1","content":"Selected Answer: ADE\nbelive me..","comment_id":"1401407","timestamp":"1742517480.0"},{"poster":"fassil","upvote_count":"4","content":"Selected Answer: DEF\nAssign the data engineer the Contributor role for Workspace1 ( D) \n- This will provide the necessary permissions to write to the Customer table.\nMigrate the Employee table from Lakehouse1 to Lakehouse2 (E) \n- This will isolate the table with PII in a different lakehouse.\nCreate a new workspace named Workspace2 that contains a new lakehouse named Lakehouse2 (Option F) \n- This is necessary to separate the Employee table into a different workspace to restrict access.","comment_id":"1362526","timestamp":"1740660840.0"},{"content":"Selected Answer: CEF\nC. Assign the Viewer role for Workspace2:\n\nThis ensures the data engineer can see the structure and metadata of the workspace without accessing sensitive Employee table data. They can still write to the Customer table without elevated permissions.\nE. Migrate the Employee table from Lakehouse1 to Lakehouse2:\n\nThis separates Personally Identifiable Information (PII) from the original lakehouse, ensuring the data engineer does not have access to sensitive data while working with non-PII data.\nF. Create Workspace2 with Lakehouse2:\n\nThis creates an isolated environment specifically for sensitive data like the Employee table. It ensures workspace-level access control and adheres to privacy and security best practices.","upvote_count":"1","poster":"ChenFu","comment_id":"1359668","timestamp":"1740125040.0"},{"timestamp":"1739740500.0","upvote_count":"1","comment_id":"1357393","poster":"henryphchan","content":"Selected Answer: DEF\nThe steps are F, E, D"},{"content":"Selected Answer: DEF\nSo, in this case, your answer DEF makes sense as it covers the necessary steps:\n\nCreate a new workspace and lakehouse (Workspace2 and Lakehouse2).\n\nMigrate the Employee table to the new lakehouse.\n\nAssign the data engineer the Contributor role in the original workspace.","poster":"MultiCloudIronMan","upvote_count":"1","timestamp":"1739632740.0","comment_id":"1356898"},{"content":"Selected Answer: DEF\nFirst we need to do F and E to separate the Employee table from the other tables. Then they need contributor access to be able to write to the Customer table which is in Workspace1\n\n\nA is incorrect as sharing only gives read access. https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sharing\nB is incorrect as Workspace2 has the Employee table which the data engineer shouldn’t access.\nC is incorrect as viewer won’t allow them to write and the Customer table isn’t even in that workspace\nE is incorrect as viewer won’t allow them to write","timestamp":"1737637560.0","upvote_count":"3","poster":"robertlavigne","comment_id":"1345388"},{"poster":"i_have_a_name","comment_id":"1342087","content":"Selected Answer: AEF\nIf we are migrating the Employee table to Lakehouse 2 in the workspace 2 , what is wrong in sharing the Lakehouse 1 with the data engineer ? \nMy answer would be A, E, F","comments":[{"poster":"dcprice","comment_id":"1354499","content":"Currently uou can't give write access via Lakehouse sharing. https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sharing. So it is DEF","upvote_count":"1","timestamp":"1739198040.0"},{"poster":"prabhjot","comment_id":"1352470","upvote_count":"1","content":"but then you ultimately would have to give the data engineer a contributor access to workspace 1 for him to work on the lakehouse as far as i think.","timestamp":"1738854660.0"}],"upvote_count":"1","timestamp":"1737108180.0"},{"comment_id":"1323617","content":"Selected Answer: DEF\nI think D, E, and F are the working solution.","upvote_count":"2","timestamp":"1733672760.0","poster":"Tuki93"},{"poster":"IshtarSQL","upvote_count":"3","comment_id":"1322913","content":"Selected Answer: DEF\nWhy A is incorrect: A. Share Lakehouse1 with the data engineer.\n\nSharing Lakehouse1 directly grants access to all its contents, including the Employee table, which violates the requirement.","timestamp":"1733518980.0"}],"answer_description":""},{"id":"4vCk0MPd4TBapYYQCxDy","answers_community":["B (100%)"],"answer_description":"","answer":"B","choices":{"A":"Create a Dataflow Gen2 dataflow.","C":"Enable external data sharing.","D":"Create a data pipeline.","B":"Create a shortcut."},"question_text":"Case Study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nLitware, Inc. is a publishing company that has an online bookstore and several retail bookstores worldwide. Litware also manages an online advertising business for the authors it represents.\nExisting Environment. Fabric Environment\nLitware has a Fabric workspace named Workspace1. High concurrency is enabled for Workspace1.\nThe company has a data engineering team that uses Python for data processing.\nExisting Environment. Data Processing\nThe retail bookstores send sales data at the end of each business day, while the online bookstore constantly provides logs and sales data to a central enterprise resource planning (ERP) system.\nLitware implements a medallion architecture by using the following three layers: bronze, silver, and gold. The sales data is ingested from the ERP system as Parquet files that land in the Files folder in a lakehouse. Notebooks are used to transform the files in a Delta table for the bronze and silver layers. The gold layer is in a warehouse that has V-Order disabled.\nLitware has image files of book covers in Azure Blob Storage. The files are loaded into the Files folder.\nExisting Environment. Sales Data\nMonth-end sales data is processed on the first calendar day of each month. Data that is older than one month never changes.\nIn the source system, the sales data refreshes every six hours starting at midnight each day.\nThe sales data is captured in a Dataflow Gen1 dataflow. When the dataflow runs, new and historical data is captured. The dataflow captures the following fields of the source:\n\nSales Date -\n\nAuthor -\n\nPrice -\n\nUnits -\n\nSKU -\nA table named AuthorSales stores the sales data that relates to each author. The table contains a column named AuthorEmail. Authors authenticate to a guest Fabric tenant by using their email address.\nExisting Environment. Security Groups\nLitware has the following security groups:\n\nSales -\n\nFabric Admins -\n\nStreaming Admins -\nExisting Environment. Performance Issues\nBusiness users perform ad-hoc queries against the warehouse. The business users indicate that reports against the warehouse sometimes run for two hours and fail to load as expected. Upon further investigation, the data engineering team receives the following error message when the reports fail to load: “The SQL query failed while running.”\nThe data engineering team wants to debug the issue and find queries that cause more than one failure.\nWhen the authors have new book releases, there is often an increase in sales activity. This increase slows the data ingestion process.\nThe company’s sales team reports that during the last month, the sales data has NOT been up-to-date when they arrive at work in the morning.\n\nRequirements. Planned Changes -\nLitware recently signed a contract to receive book reviews. The provider of the reviews exposes the data in Amazon Simple Storage Service (Amazon S3) buckets.\nLitware plans to manage Search Engine Optimization (SEO) for the authors. The SEO data will be streamed from a REST API.\n\nRequirements. Version Control -\nLitware plans to implement a version control solution in Fabric that will use GitHub integration and follow the principle of least privilege.\nRequirements. Governance Requirements\nTo control data platform costs, the data platform must use only Fabric services and items. Additional Azure resources must NOT be provisioned.\n\nRequirements. Data Requirements -\nLitware identifies the following data requirements:\nProcess the SEO data in near-real-time (NRT).\nMake the book reviews available in the lakehouse without making a copy of the data.\nWhen a new book cover image arrives in the Files folder, process the image as soon as possible.\nYou need to implement the solution for the book reviews.\nWhich should you do?","exam_id":72,"timestamp":"2024-12-31 09:55:00","discussion":[{"comment_id":"1334707","upvote_count":"6","timestamp":"1735635300.0","poster":"GHill1982","content":"Selected Answer: B\nA Shortcut meets the requirement to make the book reviews in the S3 bucket available in the lakehouse without making a copy of the data."},{"upvote_count":"1","poster":"5e89616","timestamp":"1744174800.0","comment_id":"1559143","content":"Selected Answer: B\nA shortcut in Fabric allows you to reference external data (e.g., from Amazon S3 or OneLake) without duplicating or copying it into the Lakehouse. It creates a virtual pointer to the data source."},{"comment_id":"1366045","upvote_count":"1","timestamp":"1741299240.0","content":"Selected Answer: B\nCorrect","poster":"MultiCloudIronMan"}],"answer_ET":"B","question_images":[],"question_id":54,"url":"https://www.examtopics.com/discussions/microsoft/view/153709-exam-dp-700-topic-2-question-1-discussion/","unix_timestamp":1735635300,"topic":"2","answer_images":[],"isMC":true},{"id":"mfJmAhKgtgrp7jPyd9mV","unix_timestamp":1735671840,"answers_community":["B (79%)","D (21%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/153737-exam-dp-700-topic-2-question-10-discussion/","answer":"B","timestamp":"2024-12-31 20:04:00","isMC":true,"choices":{"C":"a streaming dataset","B":"an eventstream","A":"a KQL queryset","D":"Apache Spark Structured Streaming"},"topic":"2","discussion":[{"timestamp":"1735671840.0","poster":"GHill1982","content":"Selected Answer: B\nEventstream would be most suitable for this.","comment_id":"1335016","upvote_count":"6"},{"poster":"MultiCloudIronMan","content":"Selected Answer: B\nAn eventstream is designed to handle this type of real-time data processing. It provides the necessary capabilities to:\n\nIngest data from sources like Azure Event Hubs.\n\nFilter and transform the data in real-time.\n\nStore the processed data directly into a destination like a Fabric lakehouse.","timestamp":"1740250440.0","upvote_count":"2","comment_id":"1360221"},{"timestamp":"1737790740.0","content":"Selected Answer: B\nEventstream is the way to go here. It allows for transformation activities such as filter.\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-source-azure-event-hubs?pivots=enhanced-capabilities","comment_id":"1346374","upvote_count":"3","poster":"4371883"},{"content":"Selected Answer: D\nThe questions says to store the data in a Fabric lakehouse -> correct answer should be a apache spark structured streaming. Using an eventstream stored the data in KQL database, with optional availability in OneLake, but the questions doesnt state anything about KQL databases, so structured streaming should be the correct answer.","timestamp":"1737302640.0","upvote_count":"3","comment_id":"1343075","poster":"amli123","comments":[{"content":"It's not true. Eventstream coud store into Lakehouse","poster":"38578c4","upvote_count":"3","timestamp":"1740406020.0","comment_id":"1361041"}]}],"answer_images":[],"question_id":55,"question_images":[],"answer_ET":"B","question_text":"You have an Azure event hub. Each event contains the following fields:\n\nBikepointID -\n\nStreet -\n\nNeighbourhood -\n\nLatitude -\n\nLongitude -\n\nNo_Bikes -\n\nNo_Empty_Docks -\nYou need to ingest the events. The solution must only retain events that have a Neighbourhood value of Chelsea, and then store the retained events in a Fabric lakehouse.\nWhat should you use?","answer_description":"","exam_id":72}],"exam":{"provider":"Microsoft","isImplemented":true,"name":"DP-700","isBeta":false,"numberOfQuestions":97,"lastUpdated":"12 Apr 2025","id":72,"isMCOnly":false},"currentPage":11},"__N_SSP":true}