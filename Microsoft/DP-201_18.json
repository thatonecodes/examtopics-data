{"pageProps":{"questions":[{"id":"0xDavfD5lj0gaUlfq0td","answer":"AC","isMC":true,"question_id":86,"exam_id":66,"discussion":[{"content":"This question is also in the DP-200 exam. Same with the previous question.","comment_id":"268488","upvote_count":"13","timestamp":"1610766300.0","poster":"DannyDaj"},{"content":"Agreed this is a question from DP-200 but wondering if this is part of DP-201 as well?","upvote_count":"1","timestamp":"1623688620.0","poster":"azurrematt123","comment_id":"381997"}],"answer_ET":"AC","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/42500-exam-dp-201-topic-2-question-25-discussion/","answers_community":[],"answer_description":"A: Typically, analytics engines such as HDInsight and Azure Data Lake Analytics has a per-five overhead. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256MB to 100GB in size). Some engines and applications might have trouble efficiently processing files that are greater than 100GB in size.\nC: For Hive workloads, partition pruning of time-series data can help some queries read only a subset of the data which improves performance.\nThose pipelines that ingest time-series data, often place their files with a very structured naming for files and folders. Below is a very common example we see for data is structured by date:\n\\DataSet\\YYYY\\MM\\DD\\datafile_YYYY_MM_DD.tsv\nNotice that the datetime information appears both as folders and in the filename.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance","choices":{"C":"Move the log files into folders so that each day's logs are in their own folder","B":"Increase the value of the mapreduce.map.memory parameter","A":"Combine the daily log files for all servers into one file","E":"Increase the value of the hive.tez.container.size parameter","D":"Increase the number of worker nodes"},"timestamp":"2021-01-16 04:05:00","question_images":[],"topic":"2","unix_timestamp":1610766300,"question_text":"You manage a process that performs analysis of daily web traffic logs on an HDInsight cluster. Each of the 250 web servers generates approximately\n10megabytes (MB) of log data each day. All log data is stored in a single folder in Microsoft Azure Data Lake Storage Gen 2.\nYou need to improve the performance of the process.\nWhich two changes should you make? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point."},{"id":"WhhljMDfGl5xnE9W9d5L","question_images":[],"answer":"C","answers_community":[],"unix_timestamp":1618750080,"answer_images":[],"answer_description":"Stream Analytics is a cost-effective event processing engine that helps uncover real-time insights from devices, sensors, infrastructure, applications and data quickly and easily.\nVisual Studio 2019 and Visual Studio 2017 support Stream Analytics Tools.\nNote: You can also monitor and manage Stream Analytics resources with Azure PowerShell cmdlets and powershell scripting that execute basic Stream Analytics tasks.\nReference:\nhttps://cloudblogs.microsoft.com/sqlserver/2014/10/29/microsoft-adds-iot-streaming-analytics-data-production-and-workflow-services-to-azure/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-for-visual-studio-install","choices":{"A":"Azure Analysis Services using Azure Portal","B":"Azure Analysis Services using Microsoft Visual Studio","C":"Azure Stream Analytics Edge application using Microsoft Visual Studio","D":"Azure Data Factory instance using Microsoft Visual Studio"},"question_text":"A company purchases IoT devices to monitor manufacturing machinery. The company uses an IoT appliance to communicate with the IoT devices.\nThe company must be able to monitor the devices in real-time.\nYou need to design the solution.\nWhat should you recommend?","discussion":[{"comment_id":"387924","poster":"ZodiaC","content":"3 times DAMN","upvote_count":"6","timestamp":"1624363200.0"},{"timestamp":"1672677780.0","comment_id":"763855","poster":"Harshit1905","upvote_count":"2","content":"Now I cannot miss this question in exam . Lol"},{"upvote_count":"1","comments":[{"content":"even twice","poster":"dbdev","comment_id":"370234","timestamp":"1622379720.0","upvote_count":"7"}],"timestamp":"1621696200.0","comment_id":"363761","poster":"IAMKPR","content":"It's a repeated question. Already appeared once before."}],"answer_ET":"C","exam_id":66,"timestamp":"2021-04-18 14:48:00","question_id":87,"isMC":true,"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/50396-exam-dp-201-topic-2-question-26-discussion/"},{"id":"vK5Y7t19F8Alg5gZUvn4","answers_community":[],"isMC":true,"question_id":88,"discussion":[{"timestamp":"1621926060.0","comment_id":"366191","poster":"cadio30","upvote_count":"4","content":"Propose solution is correct and better to perform hands-on to identify the other possible destination of data using Azure Stream Analytics"},{"timestamp":"1617817740.0","poster":"aksoumi","comments":[{"upvote_count":"4","poster":"Sherinm","comment_id":"332048","timestamp":"1617985080.0","content":"It is possible.\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/azure-synapse-analytics-output#:~:text=Azure%20Stream%20Analytics%20jobs%20can,rates%20up%20to%20200MB%2Fsec.&text=To%20use%20Azure%20Synapse%20as,have%20the%20storage%20account%20configured."}],"comment_id":"330594","content":"I am not sure if this is the correct answer. Never heard of Azure streaming analytics output being sent to Azure synapse/Data warehouse","upvote_count":"2"},{"poster":"sdas1","comment_id":"322012","content":"answer is correct.\nhttps://azure.github.io/iot-workshop-asset-tracking/step-003-anomaly-detection/","upvote_count":"4","timestamp":"1616866500.0"}],"url":"https://www.examtopics.com/discussions/microsoft/view/48316-exam-dp-201-topic-2-question-27-discussion/","topic":"2","answer_images":[],"timestamp":"2021-03-27 18:35:00","answer_ET":"B","question_images":[],"answer_description":"You can identify anomalies by routing data via IoT Hub to a built-in ML model in Azure Stream Analytics.\nReference:\nhttps://docs.microsoft.com/en-us/learn/modules/data-anomaly-detection-using-azure-iot-hub/","unix_timestamp":1616866500,"question_text":"You are designing an anomaly detection solution for streaming data from an Azure IoT hub. The solution must meet the following requirements:\n✑ Send the output to Azure Synapse.\n✑ Identify spikes and dips in time series data.\n✑ Minimize development and configuration effort\nWhich should you include in the solution?","choices":{"B":"Azure Stream Analytics","A":"Azure Databricks","C":"Azure SQL Database"},"exam_id":66,"answer":"B"},{"id":"jgYWznjlTkkScazO2nDD","answer_images":[],"answer":"C","discussion":[{"poster":"GabiN","upvote_count":"53","content":"According to Microsoft documentation: https://docs.microsoft.com/en-us/azure/data-factory/transform-data only 4 external transformations can be executed on-demand: HDInsight MapReduce Activity, HDInsight Hive Activity, HDInsight Pig Activity and HDInsight Streaming Activity. On-demand means that the computing environment is automatically created by the Data Factory service before a job is submitted to process data and removed when the job is completed. Therefore, the correct answer is C.","timestamp":"1582435920.0","comment_id":"54000"},{"content":"I agree with the solution C : \"With on-demand HDInsight linked service, a HDInsight cluster is created every time a slice needs to be processed unless there is an existing live cluster (timeToLive) and is deleted when the processing is done.\" But why are the others false ?","comment_id":"38350","poster":"methodidacte","timestamp":"1578895560.0","upvote_count":"7"},{"content":"NOT IN THE DP-201 ANY MORE","comment_id":"311604","timestamp":"1615828140.0","upvote_count":"6","poster":"H_S"},{"poster":"Deepu1987","content":"I would go with HDInsight Pig activity - rather than option A as per the given condition in the question where we're using ADLS n data bricks is ideally used during ADLS Gen2","upvote_count":"1","comment_id":"300235","timestamp":"1614427020.0"},{"comment_id":"236309","upvote_count":"1","content":"I would agree with the answer\nFrom https://docs.microsoft.com/en-us/azure/data-factory/v1/data-factory-compute-linked-services#:~:text=When%20the%20job%20is%20finished,cluster%20management%2C%20and%20bootstrapping%20actions.:\n\"Data Factory automatically creates the compute environment before a job is submitted for processing data. When the job is finished, Data Factory removes the compute environment.\"\n\"The Azure Storage linked service to be used by the on-demand cluster for storing and processing data. The HDInsight cluster is created in the same region as this storage account.\nCurrently, you can't create an on-demand HDInsight cluster that uses Azure Data Lake Store as the storage. If you want to store the result data from HDInsight processing in Data Lake Store, use Copy Activity to copy the data from Blob storage to Data Lake Store.\"","poster":"syu31svc","timestamp":"1607247600.0"},{"upvote_count":"2","timestamp":"1606561920.0","content":"HDinsight is not in dp201 anymore","poster":"GraceCyborg","comment_id":"229569"},{"poster":"Abhilvs","content":"Azure Databricks also supports on-demand. when running from Az Datafactory, Databricks cluster gets created as an Automated cluster and destroyed after completion. The question is ambiguous.","comment_id":"115345","upvote_count":"2","timestamp":"1592725320.0"},{"upvote_count":"1","comment_id":"107136","timestamp":"1591818360.0","poster":"Runi","content":"The HDInsight Pig activity in a Data Factory pipeline executes Pig queries on your own or on-demand Windows/Linux-based HDInsight cluster. See Pig activity article for details about this activity.\n\nSame as Mapreduce , streaming and hive activity - mentioned explicitly \"on your own or on-demand\" and based on on demand \"On-Demand: In this case, the computing environment is fully managed by Data Factory. It is automatically created by the Data Factory service before a job is submitted to process data and removed when the job is completed. You can configure and control granular settings of the on-demand compute environment for job execution, cluster management, and bootstrapping actions.\" However, python or jar activities doesn't do any on-demand process. So answer is C."},{"comments":[{"poster":"azurearch","content":"The Azure Databricks Python Activity in a Data Factory pipeline runs a Python file in your Azure Databricks cluster. This article builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. Azure Databricks is a managed platform for running Apache Spark.","timestamp":"1589693280.0","comment_id":"90342","upvote_count":"1"}],"comment_id":"80856","content":"It's the strange question. Every one of them could answer the demand.","upvote_count":"3","poster":"Leonido","timestamp":"1588078320.0"},{"upvote_count":"2","poster":"Narender_Bhadrecha","comment_id":"48708","timestamp":"1581340200.0","content":"A is also correct answer."},{"content":"A and D are correct too, u can use automatic created cluster option in linked services","upvote_count":"1","poster":"mustaphaa","timestamp":"1578991980.0","comment_id":"38815"}],"url":"https://www.examtopics.com/discussions/microsoft/view/11871-exam-dp-201-topic-2-question-28-discussion/","answer_description":"The HDInsight Pig activity in a Data Factory pipeline executes Pig queries on your own or on-demand HDInsight cluster.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-hadoop-pig","isMC":true,"timestamp":"2020-01-13 07:06:00","answers_community":[],"choices":{"C":"HDInsight Pig activity","D":"Databricks Jar activity","B":"Data Lake Analytics U-SQL activity","A":"Databricks Python activity"},"exam_id":66,"question_images":[],"question_text":"You are designing an Azure Data Factory pipeline for processing data. The pipeline will process data that is stored in general-purpose standard Azure storage.\nYou need to ensure that the compute environment is created on-demand and removed when the process is completed.\nWhich type of activity should you recommend?","answer_ET":"C","question_id":89,"unix_timestamp":1578895560,"topic":"2"},{"id":"GTX11utXjxyfNU1aN51N","answer":"C","isMC":true,"question_images":[],"question_id":90,"unix_timestamp":1587578700,"choices":{"D":"Azure HDInsight with Storm","A":"Azure HDInsight with Spark Streaming","C":"Azure Stream Analytics","B":"Apache Spark in Azure Databricks"},"answer_description":"Step 1: Get your IoT hub ready for data access by adding a consumer group.\nStep 2: Create, configure, and run a Stream Analytics job for data transfer from your IoT hub to your Power BI account.\nStep 3: Create and publish a Power BI report to visualize the data.\nReference:\nhttps://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-live-data-visualization-in-power-bi","url":"https://www.examtopics.com/discussions/microsoft/view/18937-exam-dp-201-topic-2-question-29-discussion/","topic":"2","answer_images":[],"answers_community":[],"exam_id":66,"question_text":"A company installs IoT devices to monitor its fleet of delivery vehicles. Data from devices is collected from Azure Event Hub.\nThe data must be transmitted to Power BI for real-time data visualizations.\nYou need to recommend a solution.\nWhat should you recommend?","discussion":[{"content":"data is already collected from event hub as per question, hence stream analytics is correct","upvote_count":"33","timestamp":"1589003760.0","comment_id":"85951","poster":"azurearch"},{"upvote_count":"2","comment_id":"300865","poster":"Deepu1987","content":"It's ASA - Azure Stream Analytics where it's fastest way to view real time data visualizations","timestamp":"1614526680.0"},{"upvote_count":"2","timestamp":"1608366420.0","comment_id":"247834","poster":"chaoxes","content":"C. Azure Stream Analytics\n\nIt is most efficient with Event Hubs"},{"poster":"syu31svc","content":"It can only be stream analytics","timestamp":"1607436960.0","comment_id":"238332","upvote_count":"1"},{"comments":[{"timestamp":"1592725800.0","content":"Azure Databricks is suitable for complex analysis. With Strem analytics, one can query event data and perform the required analysis on it. upon the data can directly send to Power BI without any other interface between, SA has Power BI as an output stream.","comment_id":"115350","upvote_count":"5","poster":"Abhilvs"}],"comment_id":"93913","timestamp":"1590149820.0","content":"Is 'B' wrong/no good because it says Apache Spark with Databricks? If 'Databricks' was by itself, wouldn't that be an acceptable answer? Databricks can model and serve to BI.","upvote_count":"2","poster":"runningman"},{"timestamp":"1587578700.0","content":"The given reference is incorrect. Should use Event Hub.\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-tutorial-visualize-anomalies","comment_id":"78015","poster":"Yuri1101","upvote_count":"2"}],"answer_ET":"C","timestamp":"2020-04-22 20:05:00"}],"exam":{"name":"DP-201","isMCOnly":false,"provider":"Microsoft","numberOfQuestions":206,"isBeta":false,"isImplemented":true,"lastUpdated":"12 Apr 2025","id":66},"currentPage":18},"__N_SSP":true}