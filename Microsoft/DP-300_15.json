{"pageProps":{"questions":[{"id":"aPhymqm5oaNkHILxDTPk","question_id":71,"question_images":["https://img.examtopics.com/dp-300/image351.png"],"topic":"1","discussion":[{"upvote_count":"1","poster":"turbokun","content":"answer is correct","timestamp":"1730895960.0","comments":[{"timestamp":"1736956920.0","comment_id":"1341110","poster":"voodoo_sh","content":"Agree:\n\n1. Stop and deallocate VM (Step C)\n2. Resize VM (Step A) - IF necessary to a size that supports Ultra Disks\n3. Set Enable Ultra disk compatibility to Yes (Step B)\n4. Attach the ultra disk (Step D)\n5. Start the VM (Step E)","upvote_count":"1"}],"comment_id":"1307830"}],"question_text":"DRAG DROP\n-\n\nYou have a burstable Azure virtual machine named VM1 that hosts an instance of Microsoft SQL Server.\n\nYou need to attach an Azure ultra disk to VM1. The solution must minimize downtime on VM1.\n\nIn which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.\n\n//IMG//","answers_community":[],"answer_images":["https://img.examtopics.com/dp-300/image352.png"],"timestamp":"2024-11-06 13:26:00","isMC":false,"unix_timestamp":1730895960,"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/150866-exam-dp-300-topic-1-question-77-discussion/","exam_id":68,"answer_description":"","answer":""},{"id":"sGlPCEt45wSlbWRAJbnK","exam_id":68,"answers_community":[],"answer":"","discussion":[{"upvote_count":"25","timestamp":"1708675020.0","comments":[{"content":"-- Please note the abfss endpoint when your account has secure transfer enabled\n ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' ,\n CREDENTIAL = ADLS_credential ,\n TYPE = HADOOP\n ) ;","timestamp":"1712861700.0","upvote_count":"2","comment_id":"460770","poster":"U_C"},{"content":"You are right:\n\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP)","comment_id":"460769","timestamp":"1712861640.0","upvote_count":"2","poster":"U_C"},{"content":"Why \"dfs\"? For the reference in the answer seems that the first answer is \"blob\".","upvote_count":"2","poster":"Dawn7","comment_id":"437190","timestamp":"1709308020.0"}],"comment_id":"429691","content":"dfs\nhadoop","poster":"azure2022"},{"content":"datalake gen2 is dfs.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-data-source","timestamp":"1725613140.0","comment_id":"561949","poster":"Appajip","upvote_count":"5"},{"poster":"sincerebb","upvote_count":"1","timestamp":"1744303380.0","comment_id":"1559643","content":"is this for DP-300?"},{"comment_id":"510697","poster":"CellCS","content":"datalake gen2 is dfs. \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-data-source","upvote_count":"5","timestamp":"1719527700.0"},{"poster":"ramelas","upvote_count":"2","content":"dfs - gen2\ndatalakestorage - gen1\nblob - blob storage","timestamp":"1718443560.0","comment_id":"502081"},{"timestamp":"1718261700.0","upvote_count":"2","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\nAzure Data Lake Storage Gen2 ===> dfs\n\nTYPE = HADOOP","poster":"sqljuanito","comment_id":"500471"},{"poster":"U_C","content":"-- Creates a Hadoop external data source in dedicated SQL pool\nCREATE EXTERNAL DATA SOURCE AzureDataLakeStore\nWITH\n ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' ,\n CREDENTIAL = ADLS_credential ,\n TYPE = HADOOP\n ) \n\n\n-- Creates an external data source for Azure Data Lake Gen2\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH \n ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n TYPE = HADOOP\n )\n\nThe question asks to create a data source in Pool1. So the answer is dfs & HADOOP.","upvote_count":"1","timestamp":"1712862240.0","comment_id":"460774"},{"timestamp":"1712618640.0","content":"dfs and hadoop.\nAzure Data Lake Store Gen 2 http[s] <storage_account>.dfs.core.windows.net/<container>/subfolders","comment_id":"459425","upvote_count":"1","poster":"panjie_s"},{"upvote_count":"2","timestamp":"1712161140.0","poster":"matongax","comment_id":"456668","content":"Azure Data Lake Store Gen 2 =dfs\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#:~:text=Azure%20Data%20Lake%20Store%20Gen%202"},{"upvote_count":"3","poster":"lorenzopp","timestamp":"1709686320.0","comment_id":"439974","content":"'dfs' is required to access data files with a DLS Gen2 storage account"},{"poster":"Mladen_66","comment_id":"434031","upvote_count":"1","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","timestamp":"1709144880.0"}],"isMC":false,"timestamp":"2021-08-23 07:57:00","topic":"1","answer_description":"Box 1: dfs -\nFor Azure Data Lake Store Gen 2 used the following syntax:\nhttp[s] <storage_account>.dfs.core.windows.net/<container>/subfolders\nIncorrect:\nNot blob: blob is used for Azure Blob Storage. Syntax:\nhttp[s] <storage_account>.blob.core.windows.net/<container>/subfolders\n\nBox 2: TYPE = HADOOP -\nSyntax for CREATE EXTERNAL DATA SOURCE.\nExternal data sources with TYPE=HADOOP are available only in dedicated SQL pools.\nCREATE EXTERNAL DATA SOURCE <data_source_name>\n\nWITH -\n( LOCATION = '<prefix>://<path>'\n[, CREDENTIAL = <database scoped credential> ]\n, TYPE = HADOOP\n)\n[;]\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables","question_id":72,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0003400001.png"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0003300001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/60328-exam-dp-300-topic-1-question-8-discussion/","answer_ET":"","question_text":"HOTSPOT -\nYou have an Azure Synapse Analytics dedicated SQL pool named Pool1 and an Azure Data Lake Storage Gen2 account named Account1.\nYou plan to access the files in Account1 by using an external table.\nYou need to create a data source in Pool1 that you can reference when you create the external table.\nHow should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","unix_timestamp":1629698220},{"id":"JjW1vdjXoVRCVAzXVxSk","unix_timestamp":1629713880,"exam_id":68,"question_id":73,"answer":"","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/60365-exam-dp-300-topic-1-question-9-discussion/","answer_description":"Box 1: .partitionBy -\nExample:\ndf.write.partitionBy(\"y\",\"m\",\"d\")\n.mode(SaveMode.Append)\n.parquet(\"/data/hive/warehouse/db_name.db/\" + tableName)\nBox 2: (\"Year\",\"Month\",\"Day\",\"Hour\",\"StoreID\")\nBox 3: .parquet(\"/Purchases\")\nReference:\nhttps://intellipaat.com/community/11744/how-to-partition-and-write-dataframe-in-spark-without-deleting-partitions-with-no-new-data","discussion":[{"comment_id":"429873","content":"Question FOR DP-203 , Not For DBA (DP-300)","timestamp":"1692785880.0","upvote_count":"19","poster":"HichemZe","comments":[{"timestamp":"1695915120.0","comments":[{"comment_id":"585633","content":"Answer should be (\"StoreID\",\"Year\",\"Month\",\"Day\",\"Hour\") and this indeed a question from DP-203 (recently passed this one)","upvote_count":"5","timestamp":"1713084540.0","poster":"ladywhiteadder"}],"comment_id":"453488","upvote_count":"2","content":"Is it correct then?","poster":"valente_sven1"}]},{"poster":"Backy","content":".partitionBy\n\n(\"StoreID\", \"Year\",\"Month\",\"Day\",\"Hour\") or (\"StoreID\", \"Hour\")\n\n.parquet(\"/Purchases\")\n\n\n// The problem is that (\"StoreID\", \"Year\",\"Month\",\"Day\",\"Hour\") and (\"Year\",\"Month\",\"Day\",\"Hour\", \"StoreID\") are basically the same\n\n// (\"StoreID\", \"Hour\") or even better (\"StoreID\") (not on the list) are also good. The problem is that you would have to keep offset of the last read\n\n// I would choose (\"StoreID\", \"Year\",\"Month\",\"Day\",\"Hour\") because it is the cleanest","comment_id":"621104","timestamp":"1719159000.0","upvote_count":"5"},{"timestamp":"1744303440.0","upvote_count":"1","poster":"sincerebb","content":"Question FOR DP-203 , Not For DBA (DP-300)","comment_id":"1559644"},{"poster":"reachmymind","timestamp":"1709889120.0","upvote_count":"2","comment_id":"563103","content":".partitionBy\n(\"Year\",\"Month\",\"Day\",\"Hour\",\"StoreID\")\n.parquet(\"/Purchases\")\n\nCorrect at the expectation is incremental load pipelines so the smallest partition will be achieved by \ndf.write.partitionBy (\"Year\",\"Month\",\"Day\",\"Hour\",\"StoreID\")\n.mode(\"append\")\n.parquet(\"/Purchases\") as parquet has the least data footprint"},{"content":"IMHO, as far as hourly load should vary by Store, it should be \"StoreID, Year, Month, Day, Hour\".","timestamp":"1706250540.0","comment_id":"532659","upvote_count":"2","poster":"Daba"},{"timestamp":"1697768040.0","poster":"Cindy_Lo","upvote_count":"3","content":"answer is correct.\n\nReference:\nhttps://stackoverflow.com/questions/59278835/pyspark-how-to-write-dataframe-partition-by-year-month-day-hour-sub-directory","comment_id":"464887"},{"timestamp":"1697593800.0","content":"The given answer is correct.","comment_id":"463814","poster":"learnazureportal","upvote_count":"2"},{"content":"is the answer correct?","timestamp":"1696096080.0","comment_id":"455066","poster":"o2091","upvote_count":"1"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0003600001.png"],"answers_community":[],"timestamp":"2021-08-23 12:18:00","question_text":"HOTSPOT -\nYou plan to develop a dataset named Purchases by using Azure Databricks. Purchases will contain the following columns:\n✑ ProductID\n✑ ItemPrice\n✑ LineTotal\n✑ Quantity\n✑ StoreID\n✑ Minute\n✑ Month\n✑ Hour\n✑ Year\n✑ Day\nYou need to store the data to support hourly incremental load pipelines that will vary for each StoreID. The solution must minimize storage costs.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","isMC":false,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0003700001.png"],"topic":"1"},{"id":"SaXmZO3z7KI1dH2FrVij","timestamp":"2021-11-03 22:34:00","url":"https://www.examtopics.com/discussions/microsoft/view/65419-exam-dp-300-topic-10-question-1-discussion/","topic":"10","discussion":[{"poster":"phmartins","content":"I think you're wrong, because before creating and setting up a schedule, we have to publish the runbook.\n1. Create an Azure Automation account;\n2. Import the SqlServer module\n3. Create a runbook that runs a PowerShell script;\n4. Publish the runbook.","comment_id":"572588","upvote_count":"9","timestamp":"1647905760.0","comments":[{"poster":"voodoo_sh","timestamp":"1739983920.0","comment_id":"1358859","content":"Agree with this:\n\n1. Create an Azure Automation account;\n2. Import the SqlServer module\n3. Create a runbook that runs a PowerShell script;\n4. Publish the runbook.\n\nThen 5th step is to configure a schedule.","upvote_count":"1"},{"comment_id":"572590","upvote_count":"2","comments":[{"content":"Before Schedule, we need to Save and Publish it. Then its good to give it a test. Then only we need to Schedule it. Very good question, I will say.","upvote_count":"1","poster":"Sr18","timestamp":"1719781020.0","comment_id":"1239840"}],"timestamp":"1647905940.0","poster":"phmartins","content":"https://docs.microsoft.com/en-us/azure/automation/manage-runbooks"},{"comment_id":"595978","upvote_count":"2","poster":"eric0718","content":"You are wrong. Create runbook includes publish.","timestamp":"1651484940.0"}]},{"poster":"bingomutant","upvote_count":"1","timestamp":"1729076280.0","content":"given answer looks OK - there seems to be some doubt over step 4 - my view is Create and configure a schedule: Finally, configure the runbook to execute automatically by setting up a schedule, ensuring that statistics are regularly maintained in the database.","comment_id":"1298663"},{"content":"Create an Azure Automation account.\nImport the SqlServer module.\nCreate a runbook that runs a PowerShell script.\nPublish the runbook.","timestamp":"1716636300.0","poster":"ae8a90c","upvote_count":"1","comment_id":"1218248"},{"timestamp":"1713517440.0","upvote_count":"3","content":"Walkthrough: https://tracyboggiano.com/archive/2023/06/using-azure-automation-and-runbooks-to-run-azure-sql-database-maintenance-tasks/","comment_id":"1198468","poster":"Dalamain"},{"poster":"amazonalex","content":"runbook needs to be published before you can run it:\nWhen you create or import a new runbook, you have to publish it before you can run it. Each runbook in Azure Automation has a Draft version and a Published version. Only the Published version is available to be run, and only the Draft version can be edited. The Published version is unaffected by any changes to the Draft version. When the Draft version should be made available, you publish it, overwriting the current Published version with the Draft version.\nhttps://learn.microsoft.com/en-us/azure/automation/manage-runbooks","timestamp":"1682576640.0","upvote_count":"4","comment_id":"882284"},{"content":"Answer is correct\nCreate Azure automation account \nImport SQLServer module\n Add Credentials to access SQL DB\n Add a runbook to run the maintenance\n Schedule task","upvote_count":"1","timestamp":"1651485060.0","poster":"eric0718","comment_id":"595979"},{"comment_id":"526136","content":"Looks correct.","poster":"Singii","upvote_count":"2","timestamp":"1642455960.0"},{"content":"Is it correct?","comment_id":"472297","upvote_count":"1","timestamp":"1635975240.0","poster":"o2091"}],"answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0022900001.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0023000001.png","https://www.examtopics.com/assets/media/exam-media/04275/0023100001.png"],"answer_description":"Automating Azure SQL DB index and statistics maintenance using Azure Automation:\n1. Create Azure automation account (Step 1)\n2. Import SQLServer module (Step 2)\n3. Add Credentials to access SQL DB\nThis will use secure way to hold login name and password that will be used to access Azure SQL DB\n4. Add a runbook to run the maintenance (Step 3)\nSteps:\n1. Click on \"runbooks\" at the left panel and then click \"add a runbook\"\n2. Choose \"create a new runbook\" and then give it a name and choose \"Powershell\" as the type of the runbook and then click on \"create\"\n\n5. Schedule task (Step 4)\nSteps:\n1. Click on Schedules\n2. Click on \"Add a schedule\" and follow the instructions to choose existing schedule or create a new schedule.\nReference:\nhttps://techcommunity.microsoft.com/t5/azure-database-support-blog/automating-azure-sql-db-index-and-statistics-maintenance-using/ba-p/368974","question_id":74,"unix_timestamp":1635975240,"question_text":"DRAG DROP -\nYou need to implement statistics maintenance for SalesSQLDb1. The solution must meet the technical requirements.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","isMC":false,"exam_id":68,"answers_community":[],"answer_ET":""},{"id":"8VrzAd9DtZg3jHAVbaqA","discussion":[{"comment_id":"472298","content":"it looks good, what do you think?","upvote_count":"6","poster":"o2091","timestamp":"1635975360.0"},{"timestamp":"1729076580.0","upvote_count":"1","content":"D. Configure the Manage Backups settings for ResearchSrv01\nHere's why:\n• Manage Backups settings allow you to configure automatic backups and retention policies for Azure SQL databases, which aligns with the requirement to retain backups for two months.\n• It provides a simple and cost-effective way to ensure that backups are automatically retained for the desired time period without needing additional infrastructure like an Azure Backup Server.\nOther options do not directly address the requirement for setting a backup retention period:","comment_id":"1298668","poster":"bingomutant"},{"timestamp":"1651485240.0","comment_id":"595980","upvote_count":"3","poster":"eric0718","content":"Selected Answer: D\nD is correct.\nYou can configure SQL Database to retain automated backups for a period longer than the retention period for your service tier.\nIn the Azure portal, navigate to your server and then select Backups. Select the Retention policies tab to modify your backup retention settings."},{"timestamp":"1646236620.0","content":"Selected Answer: D\nCorrect answer is D:","upvote_count":"1","comment_id":"559545","poster":"Chunchi"}],"answer_images":[],"exam_id":68,"answer":"D","answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/65420-exam-dp-300-topic-11-question-1-discussion/","answer_ET":"D","isMC":true,"choices":{"B":"Deploy and configure an Azure Backup server.","C":"Configure the Advanced Data Security settings for ResearchDB1.","D":"Configure the Manage Backups settings for ResearchSrv01.","A":"Configure the Deleted databases settings for ResearchSrv01."},"unix_timestamp":1635975360,"timestamp":"2021-11-03 22:36:00","question_images":[],"question_id":75,"question_text":"You need to provide an implementation plan to configure data retention for ResearchDB1. The solution must meet the security and compliance requirements.\nWhat should you include in the plan?","topic":"11","answer_description":""}],"exam":{"lastUpdated":"12 Apr 2025","numberOfQuestions":360,"isImplemented":true,"isMCOnly":false,"id":68,"provider":"Microsoft","name":"DP-300","isBeta":false},"currentPage":15},"__N_SSP":true}