{"pageProps":{"questions":[{"id":"cdGwUdNsZTgHRY0bvbVh","question_text":"HOTSPOT -\nYou need to design storage for the solution.\nWhich storage services should you recommend? To answer, select the appropriate configuration in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","exam_id":66,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/46901-exam-dp-201-topic-16-question-4-discussion/","answer_ET":"","unix_timestamp":1615649880,"answer_description":"Images: Azure Data Lake Storage -\nScenario: Image data must be stored in a single data store at minimum cost.\nCustomer data: Azure Blob Storage\nScenario: Customer data must be analyzed using managed Spark clusters.\nSpark clusters in HDInsight are compatible with Azure Storage and Azure Data Lake Storage.\nAzure Storage includes these data services: Azure Blob, Azure Files, Azure Queues, and Azure Tables.\nReference:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview\nDesign Azure data storage solutions","topic":"16","question_id":61,"discussion":[{"timestamp":"1618551120.0","poster":"Mittun","comments":[{"content":"Find it out, and discuss ;)","poster":"ZodiaC","timestamp":"1623590580.0","upvote_count":"4","comment_id":"381125"}],"content":"most of the case study answers are incorrect for DP-201. Need to be very careful. I am diapponted. Waste of time.","comment_id":"336772","upvote_count":"26"},{"comments":[{"comment_id":"311067","timestamp":"1615771080.0","content":"More than 2 TB of image data is added each day\nimages - Data Lake","poster":"AlexD332","upvote_count":"10"}],"poster":"kz_data","upvote_count":"11","timestamp":"1615720800.0","content":"Shouldn't be A- Blob Storage (minimum cost) and B- DW?","comment_id":"310481"},{"timestamp":"1630573920.0","poster":"michalS","content":"Wouldn't it depend on how often are these analytical queries made on customer data? If they are infrequent then it would be probably cheaper to hold the data on hyperscale Azure SQL and use polybase to load it to non-dedicated synapse pool","upvote_count":"1","comment_id":"437753"},{"content":"Images: data lake\nCustomer: synapse analytics\nBecause\nparallel processing of customer data\nhyper-scale storage of images","upvote_count":"2","poster":"hoangton","comment_id":"391740","timestamp":"1624768200.0"},{"timestamp":"1624435920.0","poster":"vrmei","upvote_count":"1","comment_id":"388604","content":"Azure SQL Database support HyperScale Storage\ncheck this:\nhyper-scale storage of images"},{"content":"Customer Data Should be stored Azure Synapse Analytics\nCheck this:\nThe New York office hosts SQL Server databases that stores massive amounts of customer data\nparallel processing of customer data","upvote_count":"1","poster":"vrmei","comment_id":"388597","timestamp":"1624435380.0"},{"upvote_count":"4","poster":"Qrm_1972","timestamp":"1621846920.0","comment_id":"365444","content":"From Topic 17 / Question 2, you will know the answer to Topic 16/ Question 4/ part 2 ( Customer Data ) is : Azure Synapse Analytics","comments":[{"comment_id":"365451","poster":"Qrm_1972","upvote_count":"2","timestamp":"1621847400.0","content":"Topic 17/Q2: You plan to use an enterprise data warehouse in Azure Synapse Analytics to store the customer data. You need to recommend a disaster recovery solution for the data warehouse."}]},{"poster":"Qrm_1972","upvote_count":"2","content":"From Topic 17 / Question 1, you will know the answer to Topic 16/ Question 4/ part 2 ( Customer Data ) is : Azure Synapse Analytics.","comment_id":"365442","timestamp":"1621846800.0"},{"comment_id":"362434","upvote_count":"1","timestamp":"1621536240.0","content":"Spark clusters in HDInsight are compatible with Azure Blob storage, Azure Data Lake Storage Gen1, or Azure Data Lake Storage Gen2. So you can use HDInsight Spark clusters to process your data stored in Azure\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview","poster":"Qrm_1972"},{"poster":"Qrm_1972","content":"You can see the solution in \" Support for Azure Storage Spark clusters in HDInsight can use Azure Data Lake Storage Gen1/Gen2 as both the primary storage or additional storage. \"\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview \nThe answer is correct","timestamp":"1621535940.0","comment_id":"362433","upvote_count":"1"},{"timestamp":"1621388220.0","content":"Before moving to Azure, The New York office hosts SQL Server databases that stores massive amounts of customer data. \nAfter moving to Azure Customer data must be analyzed using managed Spark clusters. and Power BI must be used to visualize transformed customer data.\nFor Images: Azure Data Lake Storage or Azure Blob Storage both are ok\nFor Customer data: Azure SQL database or Azure Synapse Analytics both are ok","comment_id":"360934","upvote_count":"1","poster":"Wendy_DK"},{"poster":"anamaster","timestamp":"1619258340.0","content":"images -> data lake (2TB each day)\ncustomer data -> synapse (massive amount of data, case requires parallel processing)","upvote_count":"10","comment_id":"341953"},{"timestamp":"1615650060.0","content":"Q1 Topic 17\nYou plan to use an enterprise data warehouse in Azure Synapse Analytics to store the customer data.","upvote_count":"2","poster":"AlexD332","comment_id":"309812"},{"comment_id":"309810","poster":"AlexD332","upvote_count":"2","content":"Azure Databricks provides the latest versions of Apache Spark and allows you to seamlessly integrate with open source libraries. Spin up clusters and build quickly in a fully managed Apache Spark environment\nI would say DW for customer data","timestamp":"1615649880.0"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0001700001.jpg"],"isMC":false,"answer":"","timestamp":"2021-03-13 16:38:00","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0001600001.jpg"]},{"id":"INCYDVvsJaMmQOC2TMiD","discussion":[{"comment_id":"145898","timestamp":"1595949660.0","poster":"dcpavelescu","content":"Answer C is correct.\n\nAssuming:\n- Inferred from case: processed customer data is stored in SQL DWS\n- Technical requirements: \"All data must be backed up in case disaster recovery is required\"\n\nSQL DWS has the following solutions for restore/disaster revovery:\n- snapshots (automatic, user defined) - cannot fulfill the need that all data is backed up RPO <= 8h\n- geo-backups - cannot fulfill the need that all data is backed up RPO <= 24h\n(https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/backup-and-restore#:~:text=A%20geo%2Dbackup%20is%20created,points%20in%20your%20primary%20region.)\n- geo-redundant storage for disaster recovery - fulfill the need that all data is backed up\n(https://azure.microsoft.com/en-us/pricing/details/synapse-analytics/)","upvote_count":"10"},{"comments":[{"content":"you could have said, the given answer is correct - C. Geo-Redundancy\nand then posted your comment.....\nas the comment is confusing people..","upvote_count":"21","comment_id":"190861","poster":"groy","timestamp":"1601541300.0"}],"comment_id":"52781","upvote_count":"8","poster":"danseol","content":"Replication is not a backup solution. Erroneous deletes will be replicated as well.","timestamp":"1582154760.0"},{"content":"C for correct\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/disaster-recovery-guidance\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy\ngeo-redundancy for backup\ngeo-replication for disaster recovery","comment_id":"240075","upvote_count":"3","poster":"syu31svc","timestamp":"1607601780.0"},{"upvote_count":"2","poster":"M0e","timestamp":"1603791120.0","comment_id":"206939","content":"Assuming the processed customer data is stored in Azure DW (following the previous discussions) and as per this document: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/backup-and-restore, the given answer is correct. Although, the recent wording in Synapse Analytics is Geo-Backups."},{"comment_id":"132834","upvote_count":"3","poster":"Sudipta3009","content":"-Geo-redundancy signifies a computer system operating at two or more geographical locations as a redundancy in case the primary system fails due to any reason.\n-Active geo-replication is designed as a business continuity solution that allows the application to perform quick disaster recovery of individual databases in case of a regional disaster or large scale outage.\nBoth the concept sounds relatable to the context of the question which mentions \"All data must be backed up in case disaster recovery is required\"\n\nSo anyone has the correct explanation to this and what would be the correct answer","timestamp":"1594545420.0"},{"content":"so geo redundancy is the right answer?","upvote_count":"4","comment_id":"99291","timestamp":"1590929400.0","poster":"CNBOOST2"},{"comment_id":"83510","content":"In other words, Geo-Rep is for HA and Geo-Backup is for DR","poster":"Leonido","timestamp":"1588582860.0","upvote_count":"7"}],"timestamp":"2020-02-20 00:26:00","isMC":true,"topic":"17","question_images":[],"answer_ET":"C","answers_community":[],"answer_images":[],"answer_description":"Scenario: All data must be backed up in case disaster recovery is required.\nGeo-redundant storage (GRS) is designed to provide at least 99.99999999999999% (16 9's) durability of objects over a given year by replicating your data to a secondary region that is hundreds of miles away from the primary region. If your storage account has GRS enabled, then your data is durable even in the case of a complete regional outage or a disaster in which the primary region isn't recoverable.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy-grs","choices":{"B":"AdlCopy","C":"Geo-Redundancy","D":"Geo-Replication","A":"AzCopy"},"unix_timestamp":1582154760,"question_text":"You need to design a backup solution for the processed customer data.\nWhat should you include in the design?","answer":"C","url":"https://www.examtopics.com/discussions/microsoft/view/14478-exam-dp-201-topic-17-question-1-discussion/","exam_id":66,"question_id":62},{"id":"z7FkZkcpV4UITH3GvdOE","answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/backup-and-restore\nDesign for high availability and disaster recovery","timestamp":"2020-09-12 12:13:00","answer":"D","exam_id":66,"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/31133-exam-dp-201-topic-17-question-2-discussion/","unix_timestamp":1599905580,"discussion":[{"comment_id":"178153","poster":"Kashan_Ali","upvote_count":"10","content":"GEO Replication is for High Availability (HA)\nGEO Backup is for Disaster Recovery (DR)","timestamp":"1599905580.0"},{"timestamp":"1607416740.0","upvote_count":"3","poster":"syu31svc","comment_id":"238054","content":"It can only be D"}],"choices":{"B":"Read-only replicas","D":"Geo-Redundant backups","A":"AzCopy","C":"AdlCopy"},"question_text":"You plan to use an enterprise data warehouse in Azure Synapse Analytics to store the customer data.\nYou need to recommend a disaster recovery solution for the data warehouse.\nWhat should you include in the recommendation?","question_id":63,"question_images":[],"answers_community":[],"topic":"17","answer_images":[],"answer_ET":"D"},{"id":"kMjeq3k6N7d7of3ntULF","choices":{"A":"Use AzCopy and store the data in Azure.","B":"Configure Azure SQL Database long-term retention for all databases.","D":"Use DWLoader.","C":"Configure Accelerated Database Recovery."},"answer_description":"Scenario: The database backups have regulatory purposes and must be retained for seven years.","topic":"18","exam_id":66,"discussion":[{"content":"Answer is 100% correct","poster":"syu31svc","comment_id":"238022","upvote_count":"14","timestamp":"1607415300.0","comments":[{"timestamp":"1608591660.0","upvote_count":"1","comment_id":"249724","content":"yes, thank you","poster":"Kampai787"}]}],"answers_community":[],"unix_timestamp":1607415300,"question_text":"You need to recommend a backup strategy for CONT_SQL1 and CONT_SQL2.\nWhat should you recommend?","answer_images":[],"answer":"B","timestamp":"2020-12-08 09:15:00","isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/39180-exam-dp-201-topic-18-question-1-discussion/","answer_ET":"B","question_id":64},{"id":"izKEeH9fJ8wuelECQHEd","question_text":"You need to design the disaster recovery solution for customer sales data analytics.\nWhich three actions should you recommend? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","topic":"18","answer_description":"Scenario: The analytics solution for customer sales data must be available during a regional outage.\nTo create your own regional disaster recovery topology for databricks, follow these requirements:\n1. Provision multiple Azure Databricks workspaces in separate Azure regions\n2. Use Geo-redundant storage.\n3. Once the secondary region is created, you must migrate the users, user folders, notebooks, cluster configuration, jobs configuration, libraries, storage, init scripts, and reconfigure access control.\nNote: Geo-redundant storage (GRS) is designed to provide at least 99.99999999999999% (16 9's) durability of objects over a given year by replicating your data to a secondary region that is hundreds of miles away from the primary region. If your storage account has GRS enabled, then your data is durable even in the case of a complete regional outage or a disaster in which the primary region isn't recoverable.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy-grs\nDesign for high availability and disaster recovery","question_id":65,"exam_id":66,"choices":{"A":"Provision multiple Azure Databricks workspaces in separate Azure regions.","F":"Provision a second Azure Databricks workspace in the same region.","D":"Migrate users, notebooks, and cluster configurations from one region to another.","B":"Migrate users, notebooks, and cluster configurations from one workspace to another in the same region.","E":"Use Geo-redundant storage.","C":"Use zone redundant storage."},"discussion":[{"poster":"ade2020","timestamp":"1590485280.0","comments":[{"upvote_count":"11","content":"Translated from the link:\nDeploy multiple Azure Databricks workspaces in separate Azure regions\nUse geo-redundant storage . \nAfter creating the secondary region, you will need to migrate users, home folders, notebooks, cluster configuration, job configuration, libraries, storage, initialization scripts, and reconfigure access control","poster":"syu31svc","timestamp":"1607415840.0","comment_id":"238030"}],"upvote_count":"11","content":"https://docs.microsoft.com/de-de/azure/azure-databricks/howto-regional-disaster-recovery","comment_id":"95915"},{"poster":"ZodiaC","comment_id":"381134","timestamp":"1623591780.0","upvote_count":"3","content":"CORRECT !"}],"timestamp":"2020-05-26 11:28:00","question_images":[],"answer_ET":"ADE","unix_timestamp":1590485280,"answers_community":[],"answer_images":[],"isMC":true,"answer":"ADE","url":"https://www.examtopics.com/discussions/microsoft/view/21368-exam-dp-201-topic-18-question-2-discussion/"}],"exam":{"lastUpdated":"12 Apr 2025","isImplemented":true,"id":66,"isMCOnly":false,"isBeta":false,"name":"DP-201","numberOfQuestions":206,"provider":"Microsoft"},"currentPage":13},"__N_SSP":true}