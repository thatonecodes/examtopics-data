{"pageProps":{"questions":[{"id":"Kc2K3YmoONdf84SFMuMz","answer_description":"Accessing the client certificate from App Service.\nIf you are using ASP.NET and configure your app to use client certificate authentication, the certificate will be available through the HttpRequest.ClientCertificate property. For other application stacks, the client cert will be available in your app through a base64 encoded value in the \"X-ARR-ClientCert\" request header. Your application can create a certificate from this value and then use it for authentication and authorization purposes in your application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-web-configure-tls-mutual-auth","topic":"2","question_images":["https://www.examtopics.com/assets/media/exam-media/04273/0010000001.jpg"],"question_text":"HOTSPOT -\nYou are developing an Azure Web App. You configure TLS mutual authentication for the web app.\nYou need to validate the client certificate in the web app. To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answers_community":[],"answer":"","exam_id":48,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04273/0010100001.jpg"],"timestamp":"2020-11-14 12:20:00","question_id":141,"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/36993-exam-az-204-topic-2-question-9-discussion/","unix_timestamp":1605352800,"answer_ET":"","discussion":[{"content":"Box 1: HTTP request header\nIf you are using ASP.NET and configure your app to use client certificate authentication, the certificate will be available through the HttpRequest.ClientCertificate property.\n\nBox 2: Base64\nFor other application stacks, the client cert will be available in your app through a base64 encoded value in the \"X-ARR-ClientCert\" request header. Your application can create a certificate from this value and then use it for authentication and authorization purposes in your application.\n\n\nReference:\n\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-web-configure-tls-mutual-auth","upvote_count":"67","timestamp":"1638269760.0","comment_id":"369994","poster":"mlantonis"},{"upvote_count":"16","timestamp":"1620984000.0","content":"With client certificates enabled, App Service injects an X-ARR-ClientCert request header with the client certificate.","comment_id":"219079","poster":"27close"},{"timestamp":"1730104620.0","content":"Got it on 20 April 2024...Marks > 950...answer is correct....\nBox 1: HTTP request header\nBox 2: Base64","upvote_count":"6","comment_id":"1203399","poster":"neelkanths"},{"comment_id":"1127857","poster":"Nihilist11","content":"Jan-21-2024 - This was in my exam, went with selected answer Score 740\nContosso case study [Couldnt find here]","upvote_count":"3","timestamp":"1721562960.0"},{"timestamp":"1719126120.0","poster":"bgbgvfvf","upvote_count":"1","content":"Answer is correct.Base64","comment_id":"1103942"},{"timestamp":"1706242980.0","comment_id":"963253","poster":"NightshadeRC","content":"Had this question today: 2023-07-26","upvote_count":"4"},{"poster":"[Removed]","upvote_count":"3","timestamp":"1696474320.0","comment_id":"861699","content":"Got this in exam today (5 April 2023)"},{"comments":[{"comment_id":"1107941","upvote_count":"1","content":"\"or other application stacks (Node.js, PHP, etc.), the client cert is available in your app through a base64 encoded value in the X-ARR-ClientCert request header.\"","timestamp":"1719583500.0","poster":"yyandrakk"}],"comment_id":"832763","poster":"ucskips","timestamp":"1694157900.0","upvote_count":"3","content":"why is it base64?"},{"comment_id":"824612","content":"Got this in the exam today! Feb 28, 2023","poster":"CODE_STS","upvote_count":"1","timestamp":"1693210440.0"},{"content":"Received this in my exam today (22/02/2023). Selected HTTP request header, and Base64. Score 927.","timestamp":"1692651060.0","comments":[{"timestamp":"1693643760.0","content":"Are the questions coming from dump","poster":"Jyo221","upvote_count":"1","comment_id":"826744"}],"upvote_count":"4","comment_id":"817254","poster":"uffuchsi"},{"timestamp":"1691924100.0","upvote_count":"2","comment_id":"807399","poster":"Esward","content":"It is there in 13 Feb 2023 exam"},{"upvote_count":"4","content":"Got this Dec 28 2022","timestamp":"1687998300.0","comment_id":"760505","poster":"zb1234"},{"content":"Got this question on 2-Dec-2022 exam.\nAnswer is correct. Passed with 857 score.","comment_id":"733380","timestamp":"1685668140.0","poster":"nvtienanh","upvote_count":"6"},{"timestamp":"1682835840.0","content":"Got this question on 30-Oct-2022 exam.\nAnswer is correct. Passed with 875 score","comment_id":"708290","poster":"ms_master","upvote_count":"2"},{"comment_id":"683531","poster":"AbdulMannan","content":"Got this question on 30-Sep-2022 exam.\nAnswer is correct. Passed with 870 score.","timestamp":"1680177420.0","upvote_count":"2"},{"poster":"serpevi","comment_id":"665853","upvote_count":"2","timestamp":"1678522800.0","content":"Got this in 09/22 , went with the most voted answers, score 927."},{"poster":"N9","upvote_count":"2","comment_id":"641243","content":"Box 1: HTTP request header\nBox 2: Base64","timestamp":"1675344900.0"},{"timestamp":"1670929200.0","content":"Statement 1: HTTP request header\nIf you are using ASP.NET and configure your app to use client certificate authentication, the certificate will be available through the HttpRequest.ClientCertificate property.\n\nStatement 2: Base64\nFor other application stacks, the client cert will be available in your app through a base64 encoded value in the \"X-ARR-ClientCert\" request header. Your application can create a certificate from this value and then use it for authentication and authorization purposes in your application.\n\n\nReference:\n\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-web-configure-tls-mutual-auth","poster":"HafizSalmanMalik","upvote_count":"1","comment_id":"615717"},{"comment_id":"613301","upvote_count":"1","timestamp":"1670513700.0","content":"HTTP request header\nBase64","poster":"Eltooth"},{"comment_id":"584298","upvote_count":"3","poster":"AZ204Cert","timestamp":"1665504480.0","content":"Got it in my exam 04/05/22 (selected HTTP request header, Base64)"},{"content":"Got it in my exam 03/22","comment_id":"564321","upvote_count":"2","poster":"petitbilly","timestamp":"1662749460.0"},{"comment_id":"535546","content":"This came out on 29/1(today)","poster":"Chiboy","timestamp":"1659100740.0","upvote_count":"4"},{"timestamp":"1658082120.0","comment_id":"526081","content":"It's correct","upvote_count":"2","poster":"vladhof"},{"timestamp":"1653342840.0","upvote_count":"3","comment_id":"485504","poster":"rick_cschen","content":"have this on exam"},{"content":"The correct answer is \"HTTP request header\" and \"Base64\".","poster":"[Removed]","upvote_count":"9","timestamp":"1640020500.0","comment_id":"386351"},{"upvote_count":"5","content":"Answer is correct..","timestamp":"1636557360.0","comment_id":"353724","poster":"glam"},{"timestamp":"1620984120.0","upvote_count":"6","poster":"27close","content":"The following Node.js sample code gets the X-ARR-ClientCert header and uses node-forge to convert the base64-encoded PEM st","comment_id":"219080"}]},{"id":"Y4U0c7w95nesVZ4j1WdD","isMC":true,"topic":"20","question_images":[],"answer_ET":"D","question_id":142,"answer_images":[],"answer_description":"","answer":"D","discussion":[{"comments":[{"comments":[{"content":"A lease does not lock a file for read operations, just for write and delete operations. The lock duration can be 15 to 60 seconds, or can be infinite. So I guess the effect is the same?\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/lease-blob","timestamp":"1648101420.0","comment_id":"318823","upvote_count":"3","poster":"Bengkel"},{"comment_id":"318970","content":"Why should you avoid to use blob lease to guarantee access the blob file from the report link? Generally to give access to a blob from http you use SAS. In the SAS you can specify an expiration datetime according to your need. Blob lease is instead a way just to prevent concurrent access and puts a lock on blob only for write and delete operations. Any other can still view the content but can't modify or delete it. So in my opinion the correct answer is Blob lease. In addition I have found the same question on a udemy course test and the answer is just \"Blob Lease\".\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/lease-blob","comments":[{"upvote_count":"5","poster":"MrZoom","content":"Agreed. The question is about the receipt processing. The case states \"Concurrent processing of a receipt must be prevented\", which can't be done with snapshots and leases are made for this specifically. So answer should be D.","comment_id":"324775","timestamp":"1648705740.0"}],"upvote_count":"4","timestamp":"1648113120.0","poster":"rdemontis"}],"upvote_count":"3","comment_id":"316369","content":"I guess this could be a problem with the \"Processing\" scenario:\n\"Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\"\nIt seems a lease will only be kept alive for 60 seconds, so it shouldn't be an option.","timestamp":"1647870360.0","poster":"Shion2009"}],"content":"Answer is D: Use blob leases to prevent concurrency problems","comment_id":"313811","poster":"Arul1705","timestamp":"1647578520.0","upvote_count":"61"},{"content":"I think it is lease (see this link)\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet","timestamp":"1648203600.0","upvote_count":"11","comment_id":"320055","poster":"Javierzgz"},{"content":"Selected Answer: D\nmust be lease because if you use snapshot to do the processing another thread could write or update the original which could mean you are actually processing out of date data","upvote_count":"1","comment_id":"940978","poster":"kabbas","timestamp":"1719931560.0"},{"content":"D. Use blob leases to prevent concurrency problems.\n\nTo prevent concurrent processing of a receipt, we need to ensure that only one instance of the processor can access the receipt file at a given time. Blob leases can be used to implement this mechanism. A blob lease allows an application to acquire a lease on a blob for a specified period, during which no other application can modify the blob. When the lease expires or is released, another application can acquire the lease and modify the blob.","upvote_count":"3","poster":"adilkhan","comment_id":"839757","timestamp":"1710496680.0"},{"timestamp":"1703774160.0","poster":"AymanAkk","content":"Selected Answer: D\nThe Lease Blob operation creates and manages a lock on a blob for write and delete operations. The lock duration can be 15 to 60 seconds, or can be infinite. In versions prior to 2012-02-12, the lock duration is 60 seconds \nhttps://learn.microsoft.com/en-us/rest/api/storageservices/lease-blob","comments":[{"timestamp":"1703774220.0","poster":"AymanAkk","content":"also this \nPessimistic concurrency for blobs\nTo lock a blob for exclusive use, you can acquire a lease on it. When you acquire the lease, you specify the duration of the lease. A finite lease may be valid from between 15 to 60 seconds. A lease can also be infinite, which amounts to an exclusive lock. \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet","comment_id":"759952","upvote_count":"1"}],"comment_id":"759949","upvote_count":"2"},{"upvote_count":"1","comments":[{"poster":"Knightie","upvote_count":"2","comment_id":"737256","timestamp":"1701901680.0","content":"my bad, lease is right, it's a lock.\nhttps://learn.microsoft.com/en-us/rest/api/storageservices/lease-blob"}],"poster":"Knightie","content":"Selected Answer: B\nB is correct, the issue is that one staff sent out the receipt but another staff deleted it. When the customer open, the receipt is gone. Extending the lease only give a limited time to access but it does not retain a snapshot of a copy for the customer to retrieve. So Snapshot is right.","timestamp":"1701901380.0","comment_id":"737252"},{"content":"Selected Answer: D\nD. Use blob leases to prevent concurrency problem","timestamp":"1700315880.0","upvote_count":"1","poster":"OPT_001122","comment_id":"721312"},{"content":"Sure, BlobLease is an option but that does not require me to read a huge scenario. I wonder what the scenario mean for this question. And I wonder why not A as well because Etag is a \"property\" !!","upvote_count":"1","timestamp":"1696245720.0","comment_id":"684768","poster":"gmishra88"},{"timestamp":"1693312020.0","content":"I think this one will close the discussion\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet\n\nTo lock a blob for exclusive use, you can acquire a lease on it.\nAnswer is D","poster":"OlivierPaudex","comment_id":"653478","upvote_count":"1"},{"comment_id":"617292","upvote_count":"1","timestamp":"1686925080.0","content":"Selected Answer: D\nD is correct, although I have managed to solve such problem in real life using blobs metadata so I am not sure whether C could be correct aswell. But D is safer bet","poster":"Shotare"},{"poster":"SivajiTheBoss","comment_id":"561947","content":"Answer: D. Use blob leases to prevent concurrency problems \nReason: To lock a blob for exclusive use, you can acquire a lease on it. When you acquire the lease, you specify the duration of the lease. A finite lease may be valid from between 15 to 60 seconds. \n\nReference: https://docs.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet#pessimistic-concurrency-for-blobs","upvote_count":"2","timestamp":"1678100100.0"},{"timestamp":"1677163500.0","poster":"massnonn","comment_id":"554567","upvote_count":"3","content":"Selected Answer: D\nAnswer is D"},{"poster":"vilainchien","upvote_count":"2","comment_id":"553749","timestamp":"1677077160.0","content":"Selected Answer: D\nUse blob leases to prevent concurrency problems"},{"poster":"leonidn","upvote_count":"2","content":"Selected Answer: D\nFor processing purposes \"lease\" is the simplest option.","timestamp":"1674973080.0","comment_id":"535230"},{"comments":[{"upvote_count":"1","comment_id":"524842","content":"I guess the concurrency is caused by the CRON job every 5 minutes. If processing of the available files takes longer than 5 minutes, concurrency issues may be caused.","poster":"PhilLI","timestamp":"1673865420.0"}],"comment_id":"524839","timestamp":"1673865060.0","poster":"PhilLI","content":"It's quite vague. Receipts are uploaded to Azure Files. Some people place a file, wait for a while, then delete it, and upload it via the webinterface. Should we cater for this situation as well where duplicates may be introduced?\nFile will be copied to blob by the code, but it is not clear how long it will stay there, and how to prevent rereading the same file. I would either use blob storage events, or timestamps to filter the files to be processed. If there is a risk to read the same file twice (shortly after upload when filtering on timestamp) then a lease seems ok: if you can't acquire the lease then another instance of your code is already working on it.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1663579860.0","comments":[{"poster":"RajMasilamani","content":"Answer is D","upvote_count":"1","comment_id":"448523","timestamp":"1663716300.0"}],"comment_id":"447514","poster":"RajMasilamani","content":"https://docs.microsoft.com/en-us/rest/api/storageservices/lease-blob\n\nThe answer would be Blob lease.\n\nRelease, to free the lease if it is no longer needed so that another client may immediately acquire a lease against the blob."},{"upvote_count":"2","comments":[{"upvote_count":"2","content":"No, it's not. It's D, use blob leases.","comment_id":"387240","timestamp":"1655825040.0","poster":"azurelearner666"}],"timestamp":"1649557680.0","content":"Given answer is correct","poster":"rhr","comment_id":"332241"},{"content":"Can't blob leases be used to prevent concurrency issues?","timestamp":"1647276900.0","upvote_count":"7","comment_id":"310688","poster":"inputoutput"}],"question_text":"You need to ensure receipt processing occurs correctly.\nWhat should you do?","choices":{"C":"Use blob metadata to prevent concurrency problems","D":"Use blob leases to prevent concurrency problems","B":"Use blob SnapshotTime to prevent concurrency problems","A":"Use blob properties to prevent concurrency problems"},"url":"https://www.examtopics.com/discussions/microsoft/view/47073-exam-az-204-topic-20-question-1-discussion/","unix_timestamp":1615740900,"exam_id":48,"answers_community":["D (92%)","8%"],"timestamp":"2021-03-14 17:55:00"},{"id":"JmZuEhbEuyQpwh6eDxsf","question_text":"You need to resolve the capacity issue.\nWhat should you do?","isMC":true,"discussion":[{"comment_id":"328715","content":"Receipts are uploaded to the File Storage (not Blob Storage) which does not support triggers.\nConcurrent processing of a (SINGLE!) receipt must be prevented - so parallel processing is OK.\nSo answer D.","poster":"trance13","timestamp":"1617630300.0","upvote_count":"31","comments":[{"timestamp":"1627405020.0","poster":"ZodiaC","content":"1000% D !!!!!!!! CORRECT!","upvote_count":"3","comment_id":"415603"}]},{"content":"Cleared AZ-204 today, the question appeared, the option \"D\" was not there, but a \"replace the solution with durable functions\". I went for that.","poster":"PaulMD","comments":[{"comment_id":"367647","timestamp":"1622095140.0","upvote_count":"4","content":"Durable functions will let the consumer get an immediate (async) response, but the processing remains. The duration till the file appears on the website doesn't change. \nDoing the processing in parallel will make a change.","poster":"ferut"},{"poster":"ning","timestamp":"1629819660.0","content":"Correct, if one instance of time trigger function is running, then there will not be a second instance starts, even when 5 minutes pass ... For a durable function, it can make sure immediate returns to allow second instance to start ...","upvote_count":"3","comment_id":"430875"},{"timestamp":"1643437680.0","comment_id":"535236","upvote_count":"2","poster":"leonidn","content":"That makes sense. Running parallel tasks is not good practice for functions. Here we cannot predict the degree of parallelizm. But using durable function is the best choice."},{"upvote_count":"1","poster":"edengoforit","content":"If that is the case, the answer should be C?","comment_id":"530730","timestamp":"1642961280.0"},{"upvote_count":"7","timestamp":"1620916800.0","comments":[{"timestamp":"1656006060.0","upvote_count":"1","comment_id":"621171","poster":"HumbleYolo","content":"in consumption plan, scaling is automatic."}],"poster":"SnakePlissken","comment_id":"356470","content":"Durable Functions is just an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. It's not the solution for this problem. It proves that another answer is correct. I think that answer B, scaling, is the best option."}],"timestamp":"1619693940.0","upvote_count":"24","comment_id":"345269"},{"comment_id":"1314844","poster":"overhill","timestamp":"1732041480.0","content":"Why not B????","upvote_count":"1"},{"upvote_count":"4","content":"I believe it's \"B\" - since issue happens in busy period when CPU is over-utilized. Then only reasonable action will be to scale. And for that we should properly configure Consumption Plan. \n\"D\" - could be an answer if Q was about slow speed of processing in normal situation, when CPU resources in enough. In this case, I/O operations are the bottleneck. But, if we try to spawn more thread when CPU is already super-busy, it would even worsen user experience.\nAnd it's not \"C\" since Dedicated Plan is used in very specific situation. Exerpt:\n\"Consider a dedicated App Service plan in the following situations:\n- You have existing, underutilized VMs that are already running other App Service instances.\n- You want to provide a custom image on which to run your functions.\"","comments":[{"upvote_count":"1","timestamp":"1732041540.0","comment_id":"1314846","poster":"overhill","content":"D can cause troubles with Bandwidth"}],"timestamp":"1697798700.0","comment_id":"1048684","poster":"AndySmith"},{"upvote_count":"1","content":"Selected Answer: D\nD. Update the loop starting on line PC09 to process items in parallel","comment_id":"721314","timestamp":"1668780000.0","poster":"OPT_001122"},{"upvote_count":"3","poster":"gmishra88","content":"\"Concurrent processing of a receipt must be prevented.\" \nMicrosoft has added this line as a red-herring to make the question taker not think parallelism as an option? What does \"processing\" mean here? What is \"a receipt\"? That in combination with the listFiles() method. Does \"a receipt\" contain multiple files? Does \"processing of a receipt\" in Microsoft dictionary mean uploading (processing, microsoft?) of the multiple files in \"a receipt\"\nIf the answer has durable functions then go for it without thinking deep. The requirements looks like a requirement for asynchronous processing because employees get an email (asynchronous) later. But any other answer is just not right and the question could send an intelligent developer (Microsoft excluded) into a loop of thoughts.","comment_id":"684784","timestamp":"1664711160.0"},{"timestamp":"1664710560.0","comment_id":"684776","content":"Appreciate all the Microsoft-Technology-developers finding innovative reasons for the answers. But what is not clear is what that listFiles() method do. Which files are returned. That's a lot of assumptions to say you can do upload in parallel without knowing what files and their sizes. No wonder Microsoft-technologies are so buggy","poster":"gmishra88","upvote_count":"1"},{"comments":[{"upvote_count":"1","poster":"0cc50bf","timestamp":"1723718340.0","content":"The trigger is every 5 hours, not seconds.","comment_id":"1266375"}],"content":"Selected Answer: D\nA. Convert the trigger on the Azure Function to an Azure Blob storage trigger\n=> won't help because we have Azure Fileshare\nB. Ensure that the consumption plan is configured correctly to allow scaling\n=> Trigger is time based. Multiple instances scanning the same folder => bad idea; also clearly stated in the requirements that parallel processing is not allowed\nC. Move the Azure Function to a dedicated App Service Plan\n=> the Trigger every 5 seconds should keep the function \"alive\". The function work is also not CPU bound so I cannot see a real benefit for ASP in this scenario\nD. Update the loop starting on line PC09 to process items in parallel\n=> might help. \nD2 (alternative to D as by PaulMD) replace the solution with durable functions\n=> looks even better than D\n\nIf D2 is an option I'd go for that. \n Maybe they realized that the current \"D\" is not a really good solution and D2 is also way more \"azure\"\nOtherwise D.","upvote_count":"15","timestamp":"1646646480.0","comment_id":"562515","poster":"ReniRechner"},{"upvote_count":"1","content":"The answer is C since this is a cold start problem.\n\"When using Azure Functions in the dedicated plan, the Functions host is always running, which means that cold start isn’t really an issue.\"\nhttps://azure.microsoft.com/en-us/blog/understanding-serverless-cold-start/","timestamp":"1646162940.0","comments":[{"timestamp":"1666587540.0","comment_id":"702701","poster":"coffecold","upvote_count":"1","content":"No, trigger is timed [Timertrigger....], so function execution never sleeps.."}],"comment_id":"558972","poster":"kozchris"},{"poster":"eMax","timestamp":"1642647000.0","content":"The answer reference is about JavaScript, not C# :))))))","comment_id":"528094","upvote_count":"1"},{"timestamp":"1641470820.0","content":"D is not correct - while this would speed up performance, the prompt states that users report high delay during BUSY PERIODS. Clearly, the fact that it does not upload files in parallel would not solve that.\n\nThe problem must be that the consumption plan is not scaling the function app correctly to handle the load. C could theoretically help, but B is better.\n\nCorrect answer: B","upvote_count":"2","comment_id":"518201","poster":"asdasdasg2"},{"poster":"ning","upvote_count":"4","content":"Only thing possible is D ...\nFile mount, is not blob storage, so cannot be trigger ...\nThis is a time trigger, so scale up will not help, only one instance will run ...\nOnly leave us with D","timestamp":"1629812640.0","comment_id":"430800"},{"timestamp":"1622195580.0","content":"Nobody is given us a correct answer","upvote_count":"2","comment_id":"368631","poster":"Onuoa92","comments":[{"upvote_count":"1","comment_id":"415604","content":"D is 1000% correct","timestamp":"1627405080.0","poster":"ZodiaC","comments":[{"content":"your 1000% comments under every single question does not help at all!","comment_id":"527306","timestamp":"1642579680.0","poster":"Molte","upvote_count":"16"}]}]},{"comment_id":"348443","timestamp":"1620032160.0","content":"I vote for B. Reasoning: \n\nA. Convert the trigger on the Azure Function to an Azure Blob storage trigger\n> We are not dealing with a defect, but a performance degradation, so this would not help. \n\nB. Ensure that the consumption plan is configured correctly to allow scaling\n> It seems that \"Maximum Scale Out Limit\" is set to a value not appropriate for the usage pattern \n\nC. Move the Azure Function to a dedicated App Service Plan\n> Wont help. \n\nD. Update the loop starting on line PC09 to process items in parallel\n> I don't think it is a good idea to call an async method from within a foreach loop, also not from within Parallel.ForEach. \n\nhttps://stackoverflow.com/questions/23137393/parallel-foreach-and-async-await","comments":[{"timestamp":"1621992660.0","upvote_count":"2","comment_id":"366778","content":"well the function is started by a timer, meaning that the \"event\" that should trigger the scaling won't increase. Hence I do not think B is the correct choice (Ref: https://docs.microsoft.com/en-us/azure/azure-functions/event-driven-scaling). \nConsidering that we are uploading receipts to a Azure file storage A is also incorrect.\nIn the given scenario D is the one that makes the most sense.","poster":"anirbanzeus"},{"poster":"warchoon","comment_id":"862850","content":"Don't use parallel extensions in Azure. There are special Azure constructions for it.","timestamp":"1680776580.0","upvote_count":"1"}],"poster":"[Removed]","upvote_count":"5"},{"poster":"VR","timestamp":"1618585860.0","comment_id":"337097","upvote_count":"4","content":"So what is the answer?"},{"poster":"kwaazaar","upvote_count":"2","timestamp":"1617874440.0","content":"D is the right answer, since the loop picks up all files in the container and scaling would make the files being processed more than once, potentially.\nChange feed is not supported for file shares, so D is the only remaining option (though ugly as hell).","comment_id":"331048"},{"comment_id":"327341","timestamp":"1617452700.0","poster":"jokergester","upvote_count":"1","content":"A and C - converting to blob trigger with dedicated plan not consumption to avoid cold start and high availability of the function\nD - is not enough since the the trigger is scheduled to every 5 mins - so users will still need to wait even it is already have been processed.","comments":[{"timestamp":"1621182420.0","content":"Answer is C. A is not possible as reports can also be uploaded using Azure Files. Consumption plan has a cold start (up to 10 minutes), so moving to dedicated plan will help","poster":"nicolaus","upvote_count":"6","comment_id":"358912","comments":[{"poster":"PhilLI","timestamp":"1642333080.0","comment_id":"524898","upvote_count":"2","content":"2 questions about C:\nWill a cold start be an issue at all when it is triggered by a time trigger?\nCould it be a dedicated App Service plan has stronger CPU allowing to process the files faster?\nBesides that: if parallel processing is an option, I would go for that specially with the autoscaling options of a consumption plan (but where time trigger doesn't help?)"},{"comment_id":"940998","content":"I agree, parallel processing is not going to help much here since the listfile() is already doing that. A dedicated plan will provide more resources","upvote_count":"1","timestamp":"1688310300.0","poster":"kabbas"}]}]},{"comment_id":"325238","timestamp":"1617210420.0","poster":"aperez1979","content":"I think is better option change the trigger. A","upvote_count":"4","comments":[{"comment_id":"326791","upvote_count":"3","timestamp":"1617379680.0","poster":"Beitran","comments":[{"timestamp":"1617630420.0","content":"No one wants to process a single receipt concurrently, each distinct file will be processed in parallel.","upvote_count":"6","poster":"trance13","comment_id":"328719"}],"content":"Indeed: \"Concurrent processing of a receipt must be prevented.\""},{"comment_id":"331047","timestamp":"1617874260.0","content":"Only blobs support change feed, not fileshare, which is used here.\nhttps://docs.microsoft.com/nl-nl/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal","upvote_count":"3","poster":"kwaazaar"}]}],"answer":"D","answer_description":"","unix_timestamp":1617210420,"question_images":[],"question_id":143,"answer_ET":"D","answer_images":[],"topic":"20","answers_community":["D (100%)"],"exam_id":48,"choices":{"A":"Convert the trigger on the Azure Function to an Azure Blob storage trigger","B":"Ensure that the consumption plan is configured correctly to allow scaling","D":"Update the loop starting on line PC09 to process items in parallel","C":"Move the Azure Function to a dedicated App Service Plan"},"url":"https://www.examtopics.com/discussions/microsoft/view/48600-exam-az-204-topic-20-question-2-discussion/","timestamp":"2021-03-31 19:07:00"},{"id":"NSMlCClGdE729jZfz1WF","answer_ET":"B","url":"https://www.examtopics.com/discussions/microsoft/view/48172-exam-az-204-topic-20-question-3-discussion/","question_images":[],"answer":"B","answer_description":"","isMC":true,"choices":{"C":"Implement Application Insights Sampling","B":"Change the minimum log level in the host.json file for the function","D":"Set a LogCategoryFilter during startup","A":"Create an Application Insights Telemetry Filter"},"unix_timestamp":1616696340,"timestamp":"2021-03-25 19:19:00","answers_community":["B (55%)","C (45%)"],"discussion":[{"timestamp":"1622438340.0","content":"I had the exam last Friday and I notice that they change one sentence in case study from:\nApplication Insights must always contain all log messages.\nto:\nApplication Insights currently contain all log messages.\nSo I chosen C. Implement Application Insights Sampling.","comments":[{"comments":[{"upvote_count":"3","poster":"coffecold","timestamp":"1666590120.0","content":"I don't agree. Sampling does not meet \"Application Insights must always contain ALL log messages\" since it drops items. \nhttps://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling#how-sampling-works\nI think it would be simply adjusting the log level in the app.","comment_id":"702721"},{"poster":"coffecold","upvote_count":"1","timestamp":"1666589580.0","comment_id":"702717","content":"The fourth means : filtering on log level (in webjobs SDK 2.0).\nhttps://learn.microsoft.com/en-us/azure/app-service/webjobs-sdk-how-to"}],"timestamp":"1664775240.0","comment_id":"685225","content":"If that is the case then all three options are back on the table:\n1. Sampling\n2. Filtering\n3. Adjusting the log level\n4. Whatever the 4th mean\n\nIt totally depend on the situation. Practically you try the option and find the best solution. But I think Microsoft wants us to say Sampling.","poster":"gmishra88","upvote_count":"2"},{"comment_id":"721319","poster":"OPT_001122","upvote_count":"2","content":"so you mean \nIF Application Insights must always contain all log messages. then A. Create an Application Insights Telemetry Filter\n\nIf Application Insights currently contain all log messages then C. Implement Application Insights Sampling.","timestamp":"1668780480.0"}],"poster":"MikiStieger","upvote_count":"32","comment_id":"370654"},{"comments":[{"content":"agreed, it also said \"trace output for the processor is too high, resulting in lost log messages\" so it seems that sampling is already enabled and the logs are too many so resulting in lost messages since sampling only takes only portion of all the logs, so it helps if changing the minimum log level to reduce the logs or disable sampling but this is not in the answer selection.","comments":[{"content":"also this requirement \"Application Insights must always contain all log messages.\" will eliminate enable sampling answer because app insights will filter some logs during sampling","poster":"chingdm","timestamp":"1645915860.0","upvote_count":"2","comment_id":"557054"},{"content":"That's right, sherlock. But I don't think that is what Microsoft is looking for here. Those are red-herring options given to doom developers who never tried their flashy, state-of-the-art sampling feature. Yes, but these options depends on the developer and what exactly is shown in the logs and the level of logs.","comment_id":"684825","poster":"gmishra88","upvote_count":"1","timestamp":"1664716260.0"}],"poster":"chingdm","timestamp":"1645915260.0","comment_id":"557045","upvote_count":"2"},{"timestamp":"1708259280.0","poster":"InversaRadice","content":"The purpose of Sampling is reducing the process of ingest log data...","upvote_count":"1","comment_id":"1153261"}],"content":"The answer is \"B\".\n\nFrom the problem description: \n\"Developers report that the number of log messages in the trace output for the processor is too high\"\n\nThe keywords are \"too high\".\n\nThe answer is to change the minimum logging level.","timestamp":"1645736940.0","poster":"kozchris","comment_id":"555561","upvote_count":"6"},{"upvote_count":"3","timestamp":"1732950840.0","comment_id":"1320109","content":"Selected Answer: B\nThe developers claim that they are \"losing log messages\". Sounds to me that they will not be satisfied with sampling because then they will be losing log messages even when they are not losing log messages :)","poster":"Jay456"},{"content":"Selected Answer: B\nApplication Insights must always contain all log messages. So I would vote B","poster":"Jay456","timestamp":"1732184880.0","upvote_count":"1","comment_id":"1315761"},{"upvote_count":"1","content":"Selected Answer: B\n\"Application Insights must always contain all log messages\". Sampling will mean that not all log messages will get logged. Therefore changing log level from Trace to eg Information is the solution.","timestamp":"1715616900.0","comment_id":"1210953","poster":"lednari"},{"upvote_count":"1","content":"Selected Answer: B\nB : TraceWriter and Application are distincts","poster":"dom271219","comment_id":"1100019","timestamp":"1702930800.0"},{"upvote_count":"2","content":"Selected Answer: C\nUsing sampling also allows you to preserve all your logs but reduces throttling","comment_id":"941007","poster":"kabbas","timestamp":"1688311080.0"},{"content":"Selected Answer: C\n\"Application Insights must always contain all log messages.\" is a stupid requirement. What does that even mean? It would help if it indicated to what level. If some jr programmer set the logging level to Verbose then that's the problem and the answer should be B change the minimum logging level. A and C would indicate you are not meeting the requirement to log all messages. D is not a real answer. But I guess C sampling is what MS is looking for since \"\"Sampling also helps you avoid Application Insights throttling your telemetry.\" which is the issue. https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling","poster":"JH81","comment_id":"928456","timestamp":"1687264500.0","upvote_count":"2"},{"upvote_count":"1","poster":"halfway","timestamp":"1680612060.0","comment_id":"861033","content":"Selected Answer: C\nAccording to the reference below: \"Sampling is a feature in Application Insights. It's the recommended way to reduce telemetry traffic, data costs, and storage costs, while preserving a statistically correct analysis of application data. Sampling also helps you avoid Application Insights throttling your telemetry. \"\nRef: https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling?tabs=net-core-new"},{"timestamp":"1664717460.0","comment_id":"684834","upvote_count":"3","comments":[{"upvote_count":"1","poster":"coffecold","content":"Agree, exactly because of that: \nFiltering and Sampling drop messages. So if you must not drop messages between app and app Insights, the only thing that is left is to reduce logged messages (C)","timestamp":"1666590420.0","comment_id":"702724","comments":[{"poster":"coffecold","upvote_count":"1","comment_id":"702725","timestamp":"1666590540.0","content":"I mean option B."}]}],"poster":"gmishra88","content":"Filtering or Sampling cannot be the answers considering, as far as I can guess from Microsoft documentation, that it is about NOT sending the telemetry (log entries?) to Application Insights. So, how can the argument that the requirement says \"must contain all log message\" (whatever that means in the real world). Sampling/Filtering and Chaning-log-levels have all the same effect. From the Microsoft documentation I could not really be clear of what exactly \"sampling\" is but it looks like it is not about \"trace\" but about metrics."},{"comment_id":"505885","content":"We had similar a few weeks ago \nsampling worked fine","poster":"mandusya","upvote_count":"1","timestamp":"1640071680.0"},{"comment_id":"471073","upvote_count":"2","timestamp":"1635752220.0","poster":"phvogel","content":"From https://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling\n\"Sampling reduces the volume of telemetry without affecting your statistics. \"\n\"Filtering with telemetry processors lets you filter out telemetry in the SDK before it's sent to the server\" (and the other options would also eliminate the log traces)\nSo only sampling will meet the requirement of containing all log messages."},{"timestamp":"1629813420.0","content":"I am thinking of A to exclude TraceWrite logging ...\nSince all log messages are required, so you cannot sampling or change log levels, those are leading to lose log entries ...\nLogCategoryFilter I cannot find anything with that from documentation ...","comment_id":"430812","poster":"ning","upvote_count":"3"},{"comments":[{"upvote_count":"3","comment_id":"684827","poster":"gmishra88","content":"Bingo. It does say \"Application Insights must always contain all log messages.\" I did miss that sentence. This is an eyesight test than a developer exam.","timestamp":"1664716440.0"}],"content":"A. Create an Application Insights Telemetry Filter\n\n> A filter can be created by either implementing ITelemetryProcessor or by implementing ITelemetryInitializer. However MS recommends to use sampling\n\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling#filtering\n\nB. Change the minimum log level in the host.json file for the function\n\n> Can be ruled out because req. says app insights must always contain all log messages\n\nC. Implement Application Insights Sampling\n\n> Is the recommended way to reduce telemetry traffic, data costs and storage costs\n> https://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling\n> https://blog.ramondeklein.nl/2017/05/05/filtering-application-insights/\n\nD. Set a LogCategoryFilter during startup\n\n> No idea what that is, but feels like pointint in the same direction as the B does","upvote_count":"5","poster":"[Removed]","comment_id":"348515","timestamp":"1620037320.0"},{"comment_id":"347286","timestamp":"1619910120.0","upvote_count":"3","poster":"BrettusMaximus","content":"It can't be C. - AI Sampling.\nRule 1: Application Insights must always contain all log messages\nIf you read the Sampling doco carefully. It does filter logs and does not record all the transaction records (but yes it keeps a count for statistics only (See adaptive and fixed sampling)) https://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling\n\nFact 2: The processor also has TraceWriter logging enabled.\nTraceWriter logs are generally used for debugging and are not \"Official\" transactional logs. https://stackify.com/logging-azure-functions/\nOptions A, B or D are candidates to filter these logs.\nOption B and D would stop the actual logs being generated but may also remove some transactional logs. It would also not let the developers do their debugging (the purpose of TraceWriter in the first place).\n\nThus this leaves the only option A.\nA. Create an Application Insights Telemetry Filter (to filter the trace writer logging)"},{"upvote_count":"1","timestamp":"1616696340.0","poster":"aperez1979","content":"I think it could be the b","comments":[{"comment_id":"324788","content":"The case states \"The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\"\n\nFor this reason B isn't an option, and neither is A or D. These would all change what log messages are sent to AI, which isn't according to reqs. So this leaves C. Sampling just groups messages together with a count, causing less traffic to AI but the same results.","poster":"MrZoom","upvote_count":"9","timestamp":"1617172680.0","comments":[{"comment_id":"335568","upvote_count":"2","content":"B is correct.\nBecause it says \"Application Insights must always contain all log messages.\", sampling is not a valid answer. In fact, sampling could be the very reason that log messages are lost.\nSee: https://docs.microsoft.com/en-us/azure/azure-monitor/app/asp-net-trace-logs\n\"I don't see some log entries that I expected\".","timestamp":"1618412280.0","poster":"clarionprogrammer","comments":[{"comments":[{"comment_id":"752810","upvote_count":"3","timestamp":"1671661020.0","content":"Thank you gmishra88 for the funny comments you left along the way throughout this painful study session.","poster":"oceane0316","comments":[{"upvote_count":"2","poster":"naivecoder786","timestamp":"1677933240.0","content":"Haha so true ! I think he is the same guy who was from start but is now showin as \"Removed User\" xD","comment_id":"828916"}]}],"content":"That's an excellent point. Kudos. I will need three days to prepare a legal answer for this question. How much did you say the exam allows? 180 minutes? \nSo, according to you the filter is also not an option? Nice, but I do not know how can I not see the vague requirement that says \"must contain all log messages\". Hope I do not read that line in the certification exam and so, I can say B is the right answer like a good developer's natural instinct.","poster":"gmishra88","timestamp":"1664718240.0","comment_id":"684844","upvote_count":"2"}]},{"timestamp":"1617874980.0","comment_id":"331053","upvote_count":"2","content":"Sampling is enabled by default and can be turned off (althoiugh AI ingress may still drop entries when overloaded). For metrics sampling does exactly as MrZoom describes.\n\nAs I see it, irrelevant logs must be prevented. My first step would be to adjust the minimum loglevel,if possible. A telemetry processor to filter telemetry/logs technically could work to, but is intended for filtering specific entries.","poster":"kwaazaar"}]}],"comment_id":"320432"}],"answer_images":[],"question_id":144,"topic":"20","question_text":"You need to resolve the log capacity issue.\nWhat should you do?","exam_id":48},{"id":"FoOOXt7PYzj5FXueoiag","answer_ET":"","discussion":[{"upvote_count":"41","content":"Shouldn't it be Azure Function App for the handler?","timestamp":"1662057120.0","comments":[{"upvote_count":"6","content":"Agree, there is no logic app in the solution.","poster":"willchenxa","timestamp":"1662097080.0","comment_id":"657017"},{"upvote_count":"6","poster":"Dani_ac7","comment_id":"658115","timestamp":"1662184800.0","content":"Yes, logic app is not mentioned"}],"poster":"le129","comment_id":"656557"},{"content":"This was on the exam (July 2023). Went with blob/grid/function. Scored 917","comment_id":"944406","upvote_count":"9","poster":"juanckar","timestamp":"1688628660.0"},{"timestamp":"1723719600.0","poster":"0cc50bf","content":"The answer is Blob Storage, Event Grid, Function app. You maniacs want to send the entire blob through the Service Bus, don't you? Fools. What you want is for Blob Storage to emit an event saying \"a new blob is here\" to the function app, and then the Function app reads the blob and potentially saves the data to Cosmos. Please don't use Service bus to send data when it's already been stored and can just as easily be read by a function.","upvote_count":"4","comment_id":"1266385","comments":[{"content":"Good argument in favor of Event Grid","comment_id":"1315507","poster":"overhill","upvote_count":"1","timestamp":"1732136700.0"}]},{"comment_id":"1208665","content":"For the Handler, Logic Apps are not part of the test anymore, so Function App is probably the preferred option here, regardless of what it was before.","upvote_count":"1","poster":"robertical","timestamp":"1715223300.0"},{"content":"Event data --> Configuration\nSource = Azure Blob Storage\nReceiver = Azure Event Grid \nHandler = Azure Logic App","upvote_count":"3","comment_id":"1172377","poster":"james2033","timestamp":"1710318540.0","comments":[{"upvote_count":"1","timestamp":"1723207080.0","comment_id":"1262980","poster":"Christian_garcia_martin","content":"logic app is not used in this case , Handler is the function"}]},{"comments":[{"upvote_count":"1","content":"Agreed. Blob storage dispatch heavy-weight payload data that fall under the category of \"messages\" - the dispatched data packets are not light-weight events. We should use Service Bus here","timestamp":"1713758400.0","poster":"abhishekgraphs","comment_id":"1199941"}],"content":"locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services.\nThis data might be more than event grid max message size of 512k, so survicebus should be here","upvote_count":"2","poster":"egaws","comment_id":"1147197","timestamp":"1707652440.0"},{"timestamp":"1680926340.0","comment_id":"864389","upvote_count":"2","poster":"tawanda_belkis","content":"Is this microsoft question or Azure Function app"},{"comments":[{"comment_id":"864363","timestamp":"1680922680.0","content":"ok, this is wrong. Because 21/1 has no Service Bus option and \"financial\" is probably for the audit trigger only.\nSo Box:2 must be Event Grid","upvote_count":"2","poster":"warchoon","comments":[{"poster":"warchoon","comment_id":"864370","upvote_count":"2","timestamp":"1680922980.0","content":"... I mean 29/1 has no Service Bus"}]},{"comment_id":"870383","content":"exactly. Event should not carry much information, so Azure Service Bus should be utilized","upvote_count":"2","poster":"surprise0011","timestamp":"1681494840.0"}],"timestamp":"1680920160.0","poster":"warchoon","content":"\"process sales financials\" needs messages not events, \"immediately\" => Box 2: Azure Service Bus\n\"Azure Functions must\" => Box 3: Azure Function App","upvote_count":"2","comment_id":"864352"},{"content":"Retail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nLogic app is not mentioned anywhere in the case study.\nSo #3 should be Function app?","comment_id":"721335","poster":"OPT_001122","upvote_count":"8","timestamp":"1668782880.0"}],"unix_timestamp":1662057120,"answers_community":[],"timestamp":"2022-09-01 20:32:00","isMC":false,"topic":"21","answer":"","question_text":"HOTSPOT -\nYou need to implement event routing for retail store location data.\nWhich configurations should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/04273/0050900001.jpg"],"url":"https://www.examtopics.com/discussions/microsoft/view/79135-exam-az-204-topic-21-question-1-discussion/","exam_id":48,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04273/0051000001.jpg"],"question_id":145,"answer_description":"Box 1: Azure Blob Storage -\nAzure event publishers and event handlers are at the core of the Event Grid routing-service. Event Grid listens to Azure event publishers, such as Blog Storage, then reacts by routing specific events to Azure event handlers, such as WebHooks. You can easily control this entire process at a granular level through event subscriptions and event filters.\n\nBox 2: Azure Event Grid -\nAzure Event Grid is a highly scalable event-routing service that listens for specific system events, then reacts to them according to your precise specifications. In the past, event handling has relied largely on polling ג€\" a high latency, low-efficiency approach that can prove prohibitively expensive at scale.\n\nBox 3: Azure Logic App -\nEvent Grid's supported event handlers currently include Event Hubs, WebHooks, Logic Apps, Azure Functions, Azure Automation and Microsoft Flow.\nReference:\nhttps://www.appliedi.net/blog/using-azure-event-grid-for-highly-scalable-event-routing"}],"exam":{"isBeta":false,"id":48,"provider":"Microsoft","name":"AZ-204","isMCOnly":false,"isImplemented":true,"lastUpdated":"12 Apr 2025","numberOfQuestions":452},"currentPage":29},"__N_SSP":true}