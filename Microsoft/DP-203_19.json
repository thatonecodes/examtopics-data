{"pageProps":{"questions":[{"id":"mgCDZ1GnWsRh8zttCzbE","answer":"","unix_timestamp":1680529740,"timestamp":"2023-04-03 15:49:00","question_id":91,"answer_ET":"","answers_community":[],"answer_description":"","question_text":"HOTSPOT\n-\n\nYou have an Azure Synapse Analytics serverless SQL pool that contains a database named db1. The data model for db1 is shown in the following exhibit.\n\n//IMG//\n\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the exhibit.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","exam_id":67,"answer_images":["https://img.examtopics.com/dp-203/image275.png"],"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/104974-exam-dp-203-topic-1-question-75-discussion/","discussion":[{"poster":"blenau","upvote_count":"95","content":"Correct answer should be join DimGeography and DimCustomer and 5 tables.\n\nYou also need to combine ProductLine and Product in order for the schema to be considered a star schema. This would result in 5 remaining tables: DimCustomer (DimCustomer JOIN DimGeography), DimStore, Date, Product (Product JOIN ProductLine) and FactOrders.","timestamp":"1680529740.0","comments":[{"comment_id":"1260439","content":"This is true, but we are limited by what possible answers the question provides us. So the ExamTopics selected answers are correct.","upvote_count":"2","timestamp":"1722717900.0","poster":"7082935"},{"comment_id":"1137113","timestamp":"1706737020.0","poster":"moneytime","upvote_count":"2","content":"It is a conversion of snow flake schema to star schema.All"}],"comment_id":"859963"},{"timestamp":"1682020260.0","comment_id":"875964","poster":"Dataminer","content":"Agree with explanation. It will still be snowflake if Product and ProductLine is not combined","upvote_count":"20"},{"timestamp":"1726147620.0","content":"I made the exam this week (09/09/2024) and this question was present but slightly different.","upvote_count":"3","comment_id":"1282652","poster":"renan_ineu"},{"upvote_count":"3","comment_id":"1245020","poster":"Nadine_nm","timestamp":"1720547040.0","content":"We should join the FacOrders with the DimGeography\nIn a star schema we have a fact table and dimensions, and dimensions shouldn't be joined \nalso since we are making a join we are keeping the 6 dimensions + the fact table = 7"},{"timestamp":"1714744380.0","upvote_count":"1","content":"I found this question on my exam 30/04/2024, and I put \n- join dimGeography and dimCustomer\n- 5\nI passed the exam with a high score, but I'm not sure if the answer is correct.","poster":"Alongi","comment_id":"1206154"},{"comment_id":"1196632","timestamp":"1713277080.0","poster":"dark_one","upvote_count":"1","content":"If 2 tables are joined there are still 2 tables, we're not UNIONing them so still 7 tables"},{"comments":[{"poster":"chefDE","content":"The answer should be \"join DimGeography and DimCustomer\", as a star schema has no 2nd grade dimensions.\nThe second question is tricky, it asks `Once the data model is converted to a STAR schema..' which insinuates that no 2nd grade dimension tables are left. As said earlier, this means the DimProduct and DimProductCategories should and would also be joined. This would leave us with the 4 dimensions and the fact table, which is 5 tables in total.","timestamp":"1733306100.0","upvote_count":"1","comment_id":"1321798"},{"comment_id":"1194456","timestamp":"1712944080.0","content":"Sorry, the correct is 5, bcs question is tricky and we still have to union ProductLine with Product to have at the end star schema","poster":"Monadire","upvote_count":"2"}],"comment_id":"1194455","content":"Provided answer is correct: why not 5 tables? Please, look at schema, and count once more, how many tables are at all including fact as well :)\nSo Join Geography with Customers = overall 6 tables","upvote_count":"1","timestamp":"1712943540.0","poster":"Monadire"},{"poster":"ChrisGe1234","comment_id":"1131353","timestamp":"1706163060.0","upvote_count":"2","content":"If you design DimGeography as Role Playing Dimension (How its usually done), then join Geography and Fact. Afterwards you could merge the Product Group and Product Table which leads to 6 Tables. So I would argue correct would be \nA: Join DimGeography and Fact\nB: 6","comments":[{"content":"Wrong and wrong","upvote_count":"3","comment_id":"1150190","poster":"Fusejonny1","timestamp":"1707912720.0"}]},{"poster":"sdg2844","content":"I think maybe we all missed something here. If it's star schema, there is nothing hanging off the outside of the outside tables. DimGeography should be joined to FactSales, with the geography placed in the FactSales Table. However, it doesn't solve the problem of Product and ProductCategory, which need to be combined. So there is just part of the answer missing. Once those two items are done, then there are 5 tables remainin.","comment_id":"1114161","upvote_count":"2","timestamp":"1704416820.0","comments":[{"comment_id":"1162019","content":"There would still be 6 tables","poster":"s_unsworth","upvote_count":"2","timestamp":"1709157180.0"}]},{"comment_id":"1076045","content":"it's a tricky question , it says once we join DimGeography and DimCustomer then how many tables will remain in data model. Answer is 6.","timestamp":"1700545740.0","upvote_count":"2","poster":"surajpdh"},{"comment_id":"1040653","content":"num of tables ( dims + facts) == aka ==> 6","upvote_count":"4","timestamp":"1697028360.0","poster":"matiandal"},{"timestamp":"1696775640.0","content":"Correct would be:\n1) join Geography with customer\n2) (then join productline and product - this is not in the question, but must be done to transform into a star schema)\n3) then we have 5 tables since Geography and ProductLine are no longer needed.","comment_id":"1028057","upvote_count":"8","poster":"ellala"},{"timestamp":"1694071620.0","content":"join DimGeography and DimCustomer\n5 tables","comment_id":"1001285","upvote_count":"3","poster":"hassexat"},{"timestamp":"1693980600.0","comment_id":"1000225","poster":"AvSUN","upvote_count":"3","content":"shouldn't it be 5 tables?"},{"timestamp":"1693816920.0","poster":"kkk5566","upvote_count":"3","comment_id":"998385","content":"1 DimGeography and DimCustomer\n\n 2. 5 tables."},{"poster":"ccesarrg","comment_id":"983167","content":"This question is really messy. It doesn't explicit say that by joining or unioning the tables this means they will be combined into a single table, to be it seems like we'll still have 2 tables (DimGeography and DimCustomer) in both options, besides the fact that just fixing DimGeography and DImCustomer won't generate a Star Schema","upvote_count":"1","timestamp":"1692235740.0"},{"upvote_count":"2","timestamp":"1692121200.0","comment_id":"981888","content":"join DimGeography and DimCustomer and 5 tables","poster":"Deeksha1234"},{"upvote_count":"3","timestamp":"1689789540.0","comment_id":"956886","content":"ProductLine and Product also should be joined to switch to a star schema, if not we will be still on Snowflake Schema, so the remained tables should be 5, not 6.","poster":"Zak_Zakaria"},{"comment_id":"949934","content":"Customer is already joined with Geography (see the lines), the only thing needed is to combine it with Orders and ProductLine with Orders too, in order to convert this design to a star schema. \nIn this way we get 6 dimension tables plus the fact tables: Orders.","poster":"DataEngDP","upvote_count":"1","timestamp":"1689175260.0"},{"content":"2. should be 5 tables","upvote_count":"7","comment_id":"897253","poster":"Reloadedvn","timestamp":"1684040280.0"},{"comment_id":"895621","upvote_count":"4","content":"DimGeography and DimCustomer and 5 tables.","poster":"rocky48","timestamp":"1683862980.0"}],"topic":"1","question_images":["https://img.examtopics.com/dp-203/image273.png","https://img.examtopics.com/dp-203/image274.png"]},{"id":"q3BatRlqUF3gx1hCoEQd","question_text":"You have an Azure Databricks workspace and an Azure Data Lake Storage Gen2 account named storage1.\n\nNew files are uploaded daily to storage1.\n\nYou need to recommend a solution that configures storage1 as a structured streaming source. The solution must meet the following requirements:\n\n• Incrementally process new files as they are uploaded to storage1.\n• Minimize implementation and maintenance effort.\n• Minimize the cost of processing millions of files.\n• Support schema inference and schema drift.\n\nWhich should you include in the recommendation?","choices":{"D":"Apache Spark FileStreamSource","A":"COPY INTO","B":"Azure Data Factory","C":"Auto Loader"},"question_images":[],"question_id":92,"discussion":[{"timestamp":"1680693600.0","poster":"Nikiboy","content":"Auto Loader provides a Structured Streaming source called cloudFiles. Plus, it supports schema drift. Hence, Auto Loader is the correct answer.\n https://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/","upvote_count":"18","comment_id":"862026","comments":[{"upvote_count":"1","content":"Auto Loader does not support Azure Data Lake Storage Gen2","comment_id":"894170","comments":[{"content":"It does. Refer this link\nhttps://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/","poster":"cloud_lady","upvote_count":"1","comment_id":"897972","timestamp":"1684113660.0"},{"comment_id":"932949","content":"Auto Loader can load data files from AWS S3 (s3://), Azure Data Lake Storage Gen2 (ADLS Gen2, abfss://), Google Cloud Storage (GCS, gs://), Azure Blob Storage (wasbs://), ADLS Gen1 (adl://), and Databricks File System (DBFS, dbfs:/).","upvote_count":"3","timestamp":"1687642260.0","poster":"vctrhugo"}],"poster":"mr_examers","timestamp":"1683735420.0"}]},{"content":"Selected Answer: C\nAuto Loader","upvote_count":"1","comment_id":"1347634","poster":"samianae","timestamp":"1738021440.0"},{"poster":"moize","content":"Selected Answer: C\nAuto Loader (option C) est la solution recommandée pour configurer storage1 comme source de streaming structurée dans Azure Databricks.","upvote_count":"1","timestamp":"1733325840.0","comment_id":"1321984"},{"timestamp":"1732857780.0","upvote_count":"1","comment_id":"1319549","content":"Selected Answer: C\nAuto Loader is correc","poster":"EmnCours"},{"timestamp":"1725538440.0","upvote_count":"1","poster":"ahana1074","comment_id":"1278902","content":"autoloader is correct-:Incremental processing: Auto Loader can automatically detect and incrementally process new files as they are uploaded to Azure Data Lake Storage Gen2.\nMinimize implementation and maintenance effort: Auto Loader is designed for simplicity, requiring minimal setup and automatically handling file management. It reduces operational overhead by automating many of the tasks required to manage a streaming source.\nMinimize the cost of processing millions of files: Auto Loader efficiently scales to handle millions of files and minimizes costs by using a directory listing mode or a more optimized file notification mode with Azure Event Grid.\nSupport schema inference and schema drift: Auto Loader automatically infers schema and can handle schema drift, which allows it to dynamically adapt to changes in the file structure without requiring constant updates to the processing logic."},{"upvote_count":"3","comment_id":"1182360","timestamp":"1711360140.0","poster":"Alongi","content":"Selected Answer: C\nAuto Loader is correct"},{"poster":"Homer23","content":"Reference:\nhttps://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/#incremental-ingestion-using-auto-loader-with-delta-live-tables","comment_id":"1177537","timestamp":"1710865920.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1137763","timestamp":"1706804880.0","content":"Selected Answer: C\nAuto Loader seems more correct.\nCopy Into focuses on loading from storage to a Delta table","poster":"Bill_Walker"},{"timestamp":"1706425800.0","content":"Selected Answer: D\nTo configure storage1 as a structured streaming source that incrementally processes new files as they are uploaded minimizes implementation and maintenance effort, minimizes the cost of processing millions of files, and supports schema inference and schema drift, you should use Apache Spark FileStreamSource","poster":"Charley92","upvote_count":"1","comment_id":"1133893"},{"comment_id":"1028066","poster":"ellala","content":"Bing explains the following:\nThe best option is C. Auto Loader.\n\nAuto Loader is a feature in Azure Databricks that uses a cloudFiles data source to incrementally and efficiently process new data files as they arrive in Azure Data Lake Storage Gen2. It supports schema inference and schema evolution (drift). It also minimizes implementation and maintenance effort, as it simplifies the ETL pipeline by reducing the complexity of identifying new files for processing.\n\nOther options do not meet the requirements because: \nA. COPY INTO: does not incrementally process new files as they are uploaded, which is one of your requirements.\n\nB. Azure Data Factory: does not natively support schema inference and schema drift. The incremental processing of new files would need to be manually implemented, which could increase implementation and maintenance effort.\n\nD. Apache Spark FileStreamSource: requires manual setup and does not natively support schema inference or schema drift. It also may not minimize the cost of processing millions of files as efficiently as Auto Loader.","upvote_count":"4","timestamp":"1696776240.0"},{"content":"Selected Answer: C\nAuto Loader","poster":"kkk5566","upvote_count":"1","timestamp":"1693816980.0","comment_id":"998386"},{"poster":"Deeksha1234","comment_id":"981889","upvote_count":"1","timestamp":"1692121320.0","content":"Selected Answer: C\nC is correct"},{"content":"To configure Azure Data Lake Storage Gen2 account (storage1) as a structured streaming source in Azure Databricks workspace, while meeting the given requirements, you should include the following in the recommendation:\n\nC. Auto Loader\n\nAuto Loader is a feature provided by Azure Databricks that automatically discovers and processes new files as they are uploaded to a specified directory in Azure Data Lake Storage Gen2. It provides an efficient and cost-effective way to incrementally process new files without the need for manual intervention. Auto Loader also supports schema inference and schema drift, allowing you to handle changes in the file schema over time.\n\nBy using Auto Loader, you can minimize implementation and maintenance effort as it takes care of monitoring the storage directory for new files and processing them in an optimized manner. It also helps to minimize the cost of processing millions of files as it leverages the efficient processing capabilities of Databricks.\n\nTherefore, the correct answer is C. Auto Loader.","poster":"vctrhugo","timestamp":"1686499020.0","upvote_count":"4","comment_id":"920824"},{"poster":"rocky48","content":"Selected Answer: C\nAuto Loader","comments":[{"upvote_count":"1","timestamp":"1707597180.0","poster":"rocky48","comment_id":"1146716","content":"I recommend using Auto Loader. Here’s why:\nIncremental Processing: Auto Loader in Azure Databricks allows you to process new files incrementally as they are uploaded to your storage account. It efficiently identifies and processes only the new data, reducing the need to reprocess entire datasets.\nLow Implementation and Maintenance Effort: Auto Loader simplifies the setup process. You can configure it easily within your Databricks workspace, and it automatically handles file discovery, partitioning, and schema inference.\nCost-Effective: Auto Loader optimizes resource usage by processing only the necessary data. It avoids unnecessary scans of existing files, which helps minimize costs when dealing with millions of files.\nSchema Inference and Schema Drift Support: Auto Loader automatically infers the schema from the data and adapts to schema changes over time (schema drift). This flexibility ensures smooth processing even when the structure of incoming files evolves.\nTherefore, choose C."}],"comment_id":"898040","timestamp":"1684126380.0","upvote_count":"1"},{"timestamp":"1683544020.0","upvote_count":"1","poster":"nicololmen","content":"D according to ChatGPT","comment_id":"892050"},{"content":"Ans : B\n\nDF supports Schema Drift - \nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-schema-drift","comment_id":"861361","poster":"AHUI","timestamp":"1680631500.0","upvote_count":"1","comments":[{"upvote_count":"2","poster":"frankanalysis","comment_id":"870843","content":"Auto Loader is lower cost.","timestamp":"1681555740.0"}]}],"answer_description":"","unix_timestamp":1680631500,"isMC":true,"answer_images":[],"timestamp":"2023-04-04 20:05:00","topic":"1","exam_id":67,"answer":"C","url":"https://www.examtopics.com/discussions/microsoft/view/105169-exam-dp-203-topic-1-question-76-discussion/","answers_community":["C (91%)","9%"],"answer_ET":"C"},{"id":"q4FAHWx47tX0DVA3Nh4Z","choices":{"C":"the DATAFILETYPE bulk option","D":"the DATA_SOURCE parameter","A":"the WITH clause","B":"the ROWSET_OPTIONS bulk option"},"topic":"1","answer_description":"","question_text":"You have an Azure subscription that contains the resources shown in the following table.\n\n//IMG//\n\n\nYou need to read the TSV files by using ad-hoc queries and the OPENROWSET function. The solution must assign a name and override the inferred data type of each column.\n\nWhat should you include in the OPENROWSET function?","answer":"A","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/108481-exam-dp-203-topic-1-question-77-discussion/","answer_ET":"A","isMC":true,"question_id":93,"discussion":[{"content":"Selected Answer: A\nIn the Question \"The solution must assign a name and override the inferred data type of each column\", so we must need a WITH Clause to define the column names and data types.","poster":"henryphchan","comment_id":"890166","timestamp":"1683305100.0","upvote_count":"18"},{"poster":"19c1ee5","timestamp":"1683205080.0","comments":[{"timestamp":"1683271680.0","comment_id":"889857","content":"Agreed - Should be A. \"To specify explicit column names and data types, you can override the default column names and inferred data types by providing a schema definition in a WITH clause\" (https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files)","poster":"jlad26","upvote_count":"6"}],"comment_id":"889356","upvote_count":"9","content":"I think it's A. WITH CLAUSE\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset"},{"comment_id":"1319550","upvote_count":"1","poster":"EmnCours","content":"Selected Answer: A\nSelected Answer: A","timestamp":"1732857900.0"},{"upvote_count":"5","content":"A is correct because-:SELECT * \nFROM OPENROWSET(\n BULK 'https://storage1.blob.core.windows.net/container/file.tsv',\n FORMAT='CSV', \n PARSER_VERSION='2.0', \n FIELDTERMINATOR = '\\t', \n ROWTERMINATOR = '\\n'\n) \nWITH (\n Column1Name datatype1,\n Column2Name datatype2,\n Column3Name datatype3\n) AS result; \nB. ROWSET_OPTIONS bulk option: Configures aspects of data processing like buffering or parallelism, but it doesn’t handle column names or types.\nC. DATAFILETYPE bulk option: Used to specify the file type but not for defining column names or types.\nD. DATA_SOURCE parameter: Points to an external data source, but it doesn't help with naming or overriding data types.\nThus, the WITH clause is the correct option to include in the OPENROWSET function to assign column names and override inferred data types.","poster":"ahana1074","timestamp":"1725538800.0","comment_id":"1278909"},{"content":"DATA_SOURCE is not even an existing parameter option in th OPENROWSET function, the right existing option is DATASOURCE, without underscore, but it is not needed here, so A is the right option, \n\nsee doc:\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/openrowset-transact-sql?view=sql-server-ver16","timestamp":"1720864200.0","comment_id":"1247227","upvote_count":"1","poster":"DanielCruz"},{"content":"Selected Answer: A\nA is correct","timestamp":"1712857320.0","comment_id":"1193947","upvote_count":"1","poster":"[Removed]"},{"content":"Question insists on the point that the file has not the header row! specifying a datasource is not strictly required, since OPENROWSET can connect to storage even with a simple link and managed identity. The WITH clause is the most reasonable recommendation to include in the solution.","comment_id":"1189152","upvote_count":"1","poster":"MBRSDG","timestamp":"1712215680.0"},{"poster":"Alongi","content":"Selected Answer: A\nThere isn't header, so you have to specify columns name manually in WITH clause","comment_id":"1182366","upvote_count":"2","timestamp":"1711360440.0"},{"upvote_count":"1","timestamp":"1711301460.0","content":"Selected Answer: D\nWith clause can be omitted for csv files see automatic schema discovery at https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#automatic-schema-discovery\nGeneric colnames will be assigned","poster":"gplusplus","comment_id":"1181887","comments":[{"poster":"gplusplus","content":"on secodn thought, even if the types CAN be inferred, the question clearly states that the inferred types MUST be overwritten. Therefore WITH","comment_id":"1183974","upvote_count":"2","timestamp":"1711532580.0"}]},{"timestamp":"1709157480.0","poster":"s_unsworth","upvote_count":"1","comment_id":"1162022","content":"Selected Answer: A\nThere is no header row, therefore you should define one using the WITH Clause"},{"timestamp":"1706425980.0","poster":"Charley92","upvote_count":"1","comment_id":"1133895","content":"Selected Answer: A\nThe WITH clause allows you to specify the schema of the data source and override the inferred data type of each column. You can also use the WITH clause to specify the name of the external table and the location of the data source"},{"content":"Selected Answer: A\nWITH CLAUSE","poster":"prshntdxt7","timestamp":"1706348100.0","upvote_count":"1","comment_id":"1133213"},{"comment_id":"1089763","content":"Selected Answer: A\nTo read TSV files without a header row using the `OPENROWSET` function and to assign a name and specify the data type for each column, you should use:\n\nA. the WITH clause\n\nThe WITH clause is used in the `OPENROWSET` function to define the format file or to directly define the structure of the file by specifying the column names and data types.","timestamp":"1701895500.0","upvote_count":"1","poster":"Momoanwar"},{"upvote_count":"1","poster":"hcq31818","comment_id":"1077162","content":"Selected Answer: A\nA - WITH Clause is the correct answer.","timestamp":"1700644500.0"},{"comment_id":"1058205","upvote_count":"1","content":"Selected Answer: D\nD is right.","poster":"Runaj","timestamp":"1698692880.0"},{"timestamp":"1698514140.0","poster":"jhargett1","content":"Selected Answer: A\nTo read TSV (Tab-Separated Values) files using ad-hoc queries and the OPENROWSET function in Azure Synapse Analytics, and to assign a name and override the inferred data type of each column, you should include the following in the OPENROWSET function:\n\nA. the WITH clause\n\nThe WITH clause allows you to specify options for reading the data, including defining the column names and data types. You can use the WITH clause to provide column definitions and specify the data type for each column in the TSV file, which allows you to override the inferred data types.","comment_id":"1056353","upvote_count":"2"},{"content":"Selected Answer: D\nThey ask for \"in the function\", not \"in the query\"","poster":"Metaalverf","timestamp":"1698382920.0","comment_id":"1055119","upvote_count":"5"},{"poster":"ellala","timestamp":"1696778580.0","upvote_count":"1","comment_id":"1028100","content":"Selected Answer: A\nCorrect is A (explained in another comment)"},{"comment_id":"1028092","upvote_count":"4","content":"It is not D because the files are PUBICLY ACCESSIBLE in the storage account, therefore according to documentation:\n\"Any user can use OPENROWSET without DATA_SOURCE to read publicly available files on Azure storage.\" (https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#security) \n\nAlso DATA_SOURCE does not provide the necessary requirements, therefore WITH CLAUSE\n\nExample:\nOPENROWSET \n( { BULK 'unstructured_data_path' , [DATA_SOURCE = <data source name>, ]\n FORMAT= ['PARQUET' | 'DELTA'] } \n) \n[WITH ( {'column_name' 'column_type' }) ]\n[AS] table_alias(column_alias,...n)","poster":"ellala","timestamp":"1696778280.0"},{"content":"Selected Answer: A\nwe can disable the header row in with clause","timestamp":"1696483140.0","poster":"liwangbai123","comment_id":"1025290","upvote_count":"1"},{"upvote_count":"2","poster":"gggqqqqq","timestamp":"1696424220.0","comment_id":"1024771","content":"with clause is not part of OPENROWSET function."},{"upvote_count":"3","timestamp":"1694092440.0","content":"Selected Answer: D\nThe correct answer is D. DATA_SOURCE parameter is the first parameter in OPENROWSET function.","poster":"HSZ","comments":[{"comment_id":"1247229","timestamp":"1720864260.0","content":"DATA_SOURCE it is not even a parameter in OPENROWSET, the one there is DATASOURCE","upvote_count":"1","poster":"DanielCruz"}],"comment_id":"1001595"},{"poster":"hassexat","upvote_count":"1","comment_id":"1001287","content":"Selected Answer: A\nWITH clause","timestamp":"1694071680.0"},{"content":"I think D is the correct answer, it's mentioned that the TSV files have no headers","poster":"AvSUN","comment_id":"1000235","upvote_count":"1","timestamp":"1693980900.0"},{"content":"Selected Answer: A\nA is correct","poster":"kkk5566","comment_id":"998387","upvote_count":"1","timestamp":"1693817040.0"},{"upvote_count":"1","poster":"kkk5566","comment_id":"989791","timestamp":"1692949560.0","content":"A is correct"},{"comment_id":"981894","timestamp":"1692121560.0","content":"Selected Answer: A\nanswer is correct","poster":"Deeksha1234","upvote_count":"1"},{"upvote_count":"2","poster":"ravigolu","comment_id":"939773","content":"Selected Answer: A\nAns is A\nDATA_SOURCE is used to : Use DATA_SOURCE option and define credential that you want to use to access storage.\nThe WITH clause allows you to specify columns that you want to read from files.\nFor CSV data files, to read all the columns, provide column names and their data types.","timestamp":"1688202660.0"},{"comment_id":"936294","content":"Selected Answer: A\nYes we need the WITH clause to define the data type for each column.","poster":"auwia","timestamp":"1687938540.0","upvote_count":"1"},{"upvote_count":"1","poster":"Ankit_Az","comment_id":"907903","content":"Selected Answer: A\nA is correct","timestamp":"1685183700.0"},{"comment_id":"899966","upvote_count":"4","comments":[{"upvote_count":"2","comment_id":"910247","comments":[{"timestamp":"1698663720.0","upvote_count":"1","poster":"AlejandroU","comment_id":"1057577","content":"a \"WITH Clause\" is an argument that should be included in the OPENROWSET function. The TSV files are \"publicly accessible files\" ; thus, can be used without DATA_SOURCE since we don't need any authentication option. Including the WITH Clause argument will allow assigning column names and override the inferred data type of each column."}],"poster":"peches","content":"I agree with you, the way the question is written it seems like is asking for a parameter of the OPENROWSET function and that would discard WITH as an answer (even though we need it in the query to accomplish the overriding of data types). ROWSET_OPTIONS is used to avoid query failures due to constantly appended files, which is not the case here. DATAFILETYPE is used to specify the charset (utf-8 or utf-16) which isn't mentioned in the question. That would leave us with option D, but what is strange is that it clearly says that the files are publicly accesible, so we could pass the whole URL using BULK and not use DATA_SOURCE either. Also, we are not using the information regarding the files not having a header row, which should be taken into account by setting the FIRSTROW parameter of the OPENROWSET function.","timestamp":"1685451060.0"}],"timestamp":"1684317900.0","poster":"ustefan11","content":"Selected Answer: D\nThe question is 'What should you include in the OPENROWSET function?' with the emphasis on the 'in the function', therefore the correct answer is 'D. the DATA_SOURCE parameter' since it's usually amongst the first arguments when using the OPENROWSET function."},{"content":"Selected Answer: A\nOption A seems correct answer as With clause helps to overwrite data types and assign names for columns","comment_id":"899584","timestamp":"1684273680.0","upvote_count":"2","poster":"rocky48"},{"timestamp":"1683281580.0","content":"Selected Answer: A\nOption A seems correct answer as With clause helps to overwrite data types and assign names for columns","comment_id":"889946","upvote_count":"4","poster":"jeroenmouse"}],"exam_id":67,"timestamp":"2023-05-04 14:58:00","answers_community":["A (76%)","D (24%)"],"unix_timestamp":1683205080,"question_images":["https://img.examtopics.com/dp-203/image287.png"]},{"id":"vsRjVTBKm9mUE4NRQPAO","isMC":true,"question_images":[],"exam_id":67,"answer":"D","answer_ET":"D","answer_images":[],"discussion":[{"comments":[{"comments":[{"timestamp":"1711366800.0","poster":"AkosL","content":"Not by default, but always","upvote_count":"1","comment_id":"1182430"},{"content":"60 Million is correct","comment_id":"1088125","upvote_count":"2","timestamp":"1701739680.0","poster":"Lscranio"}],"poster":"Vanq69","content":"You mean the dedicated SQL pool has 60 distributions \"by default\"?","comment_id":"1024883","timestamp":"1696428780.0","upvote_count":"2"}],"content":"Selected Answer: D\nClustered Column Store will by default have 60 partitions. And to achieve best compression we need at least 1 Million rows per partition, hence Option D 60 Millions (1M per partition)","comment_id":"907908","upvote_count":"23","poster":"Ankit_Az","timestamp":"1685183940.0"},{"comment_id":"1319552","upvote_count":"1","content":"Selected Answer: D\nCorrect Answer: D","timestamp":"1732858260.0","poster":"EmnCours"},{"timestamp":"1725538920.0","upvote_count":"1","poster":"ahana1074","content":"D is correct-:Partitioning in a dedicated SQL pool in Azure Synapse Analytics is typically used to manage very large tables, and the recommendation is to start considering partitions when the table contains 60 million rows or more.\n\nPartitioning helps optimize both data compression and query performance by allowing the system to process smaller subsets of data more efficiently. However, partitioning comes with overhead, and if you partition tables that are too small, it can actually degrade performance.\n\nClustered columnstore indexes are designed to provide efficient compression and query performance for large datasets. Partitioning further helps with large fact tables, but for smaller tables (e.g., fewer than 60 million rows), partitioning is usually not necessary and might even be counterproductive.","comment_id":"1278910"},{"poster":"renan_ineu","upvote_count":"1","content":"\"When creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed. Before partitions are created, dedicated SQL pool already divides each table into 60 distributions.\"\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition#partition-sizing","comment_id":"1272523","timestamp":"1724658780.0"},{"upvote_count":"1","content":"Selected Answer: D\n1M * 60 = 60M","comment_id":"1252775","poster":"jaco_0930","timestamp":"1721609700.0"},{"comments":[{"timestamp":"1724658600.0","poster":"renan_ineu","comment_id":"1272518","upvote_count":"1","content":"The question is asking the minimum number of rows in the whole table and not the number per partition. So, in order to have at least 1 million rows per partition, you must have 60 million rows in the fact table."}],"poster":"AA9292","content":"it is 1 Million. the question is clearly asking for minimum number of rows before creating partitions. and in Microsoft document they stated that 1 million rows is the minimum number of rows before partitioning a table","comment_id":"1239951","timestamp":"1719804240.0","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: C\n1 Million (Option C) is correct. You need the minimum number of rows to create a optimized partition. A single optimal partition requires 1 Million rows.","timestamp":"1718362320.0","poster":"Siva_Jsn_23","comment_id":"1230446"},{"content":"I think the correct answer should be 120 million rows. Since splitting the data up into 2 partitions would result in 1 million rows per distribution and partition for 120 million rows","poster":"KarlGardnerDataEngineering","comment_id":"1212075","timestamp":"1715795520.0","upvote_count":"1"},{"poster":"MBRSDG","timestamp":"1712215560.0","upvote_count":"1","content":"Selected Answer: D\neven without knowing how many partitions you're going to create, you know that each partition should contain at least 1million rows. 60million rows are the only case enabling to use partitions.","comment_id":"1189151"},{"timestamp":"1709157840.0","poster":"s_unsworth","comment_id":"1162024","upvote_count":"3","content":"Selected Answer: D\nCluster columnstore tables begin to achieve optimal compression once there is more than 60 million rows. For small lookup tables, less than 60 million rows, consider using HEAP or clustered index for faster query performance. -- Microsoft"},{"timestamp":"1706426160.0","upvote_count":"2","content":"Selected Answer: D\nTo optimize data compression and query performance for Table in Azure Synapse Analytics dedicated SQL pool, you should create partitions when the table contains at least 60 million rows.\n\nPartitioning tables can improve query performance by reducing the amount of data that needs to be scanned. It can also improve data compression by allowing each partition to be compressed separately.\n\nIn general, you should consider partitioning a table when it contains a large amount of data and queries frequently filter on a specific column or set of columns","comment_id":"1133898","poster":"Charley92"},{"comment_id":"1108564","content":"Selected Answer: A\nQuestion says \"What is the minimum number of rows that Table1 should contain before you create (add/new/extra) partitions?\"\nAs per microsoft documentation, each partition will contain 1Million records. So, if there atleast 1million records, we can go for partitioning.\n\nHere is the link for documentation\nhttps://learn.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview?view=sql-server-ver16","upvote_count":"3","poster":"58d2382","timestamp":"1703844660.0"},{"comment_id":"1107020","poster":"6d954df","timestamp":"1703696280.0","upvote_count":"2","content":"60m, see https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"},{"poster":"blazy001","timestamp":"1702374180.0","comment_id":"1094324","content":"A 100mil is correct >>\nFrom the answers to this Q, I see that MS has done a bad job because people don't understand what distributions or partitions are. My explanation: Each table with column store index is auto divided into 60 distributions, on each of these distributions there is auto 1 partition. For good performance (with column store) each partition must have at least 1Mil rows.\nThe question was: \"What is the minimum number of rows that Table1 should contain before you create (add/new/extra) partitions?\"\nSo there is no point in creating partitions with 60M rows,\nbecause then you divide this into 0.5Mil per partition. At least 120Mil would be ideal, but 100Mil already starts.","upvote_count":"1"},{"timestamp":"1694071740.0","poster":"hassexat","comment_id":"1001289","content":"Selected Answer: D\n60 million","upvote_count":"2"},{"content":"Selected Answer: D\nis correct","upvote_count":"2","timestamp":"1693817220.0","poster":"kkk5566","comment_id":"998388"},{"timestamp":"1692123780.0","upvote_count":"3","content":"should be D","poster":"Deeksha1234","comment_id":"981914"},{"upvote_count":"2","comment_id":"974570","poster":"akhil5432","timestamp":"1691403240.0","content":"Selected Answer: C\nWHY People mentioned option D..please explain how?"},{"content":"1 MILLION","upvote_count":"2","poster":"akhil5432","comment_id":"974569","timestamp":"1691403180.0"},{"comment_id":"946625","content":"Selected Answer: C\nTo achieve optimal data compression and query performance with clustered columnstore tables in Azure Synapse Analytics, it is recommended to have a minimum of 1 million rows per distribution and partition.\nAs Synapse Analytics automatically creates 60 distributions per table, to fulfill the 1 million rows per distribution recommendation, the table should ideally contain 60 million rows if no additional partitions are created.\nHowever, the question is asking about the threshold for creating partitions, not necessarily a table of full 60 million rows. Therefore, you would want to ensure you have at least 1 million rows in each partition to maintain the optimal performance and compression. If the number of rows is less than 1 million, it's better to consider fewer partitions in order to increase the number of rows per partition.","comments":[{"comment_id":"998983","timestamp":"1693883100.0","content":"Question is about the minimum number of rows in the table not in a partition. And according to what you explained, which correct, the answer is D.","upvote_count":"4","poster":"tankwayep"}],"poster":"Lukis92","upvote_count":"3","timestamp":"1688833380.0"},{"comment_id":"905223","content":"answer shouls 60 million\nThe minimum number of rows that Table1 should contain before creating partitions in Azure Synapse Analytics dedicated SQL pool depends on various factors such as data size, query patterns, and performance requirements. However, a commonly recommended threshold is typically around 60 million rows before considering partitioning.","poster":"janaki","upvote_count":"3","timestamp":"1684875000.0"},{"content":"The minimum number of rows that Table1 should contain before you consider creating partitions in Azure Synapse Analytics dedicated SQL pool depends on multiple factors, such as the size of each row, the expected data growth rate, and the specific requirements of your workload. However, considering the typical guidelines and best practices, a general rule of thumb is to consider creating partitions when the table size reaches around 60 million rows.\n\nTherefore, the minimum number of rows that Table1 should contain before you create partitions is -\n\nD. 60 million.\n\nThis is a commonly recommended threshold for optimizing data compression and query performance in large-scale data warehouses. However, it's important to note that this number can vary based on your specific scenario, so it's always advisable to conduct performance testing and consider the characteristics of your data and workload to determine the optimal time for partitioning your table.","timestamp":"1684744560.0","comment_id":"903867","upvote_count":"1","poster":"maxstv"},{"upvote_count":"1","timestamp":"1684469580.0","poster":"RamMovvaa","content":"What is the minimum number of rows that Table1 should contain before you create partitions?\n\nAnswer : C","comment_id":"901635"},{"timestamp":"1684318380.0","poster":"ustefan11","comment_id":"899976","upvote_count":"1","content":"Selected Answer: D\nI've seen in the comments the explanation that this question has something to do with distribution and I don't think this is the case here. It's just that for a partition to have optimal compression, it has to be of at least 1 million rows, and since the idea of having a partition is to divide the data into smaller chunks, you need at least 2 partitions. Therefore, since there's no '2 mil' option, the only option left is '60M'."},{"content":"Selected Answer: D\nClustered columnstore has the best compression with 1M rows. So it should be 1M * 60 = 60 million rows","poster":"rocky48","timestamp":"1684273860.0","comment_id":"899586","upvote_count":"2"},{"comment_id":"894176","content":"Selected Answer: D\nFor optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed. Since Synapse Analytics divides each table into 60 distributions by default, the table should contain at least 1 million rows per distribution or 60 million rows in total before considering partitioning the table.","upvote_count":"4","poster":"mr_examers","timestamp":"1683736440.0"},{"content":"Selected Answer: D\nHash-distributed tables work well for large fact tables in a star schema and along with that we need to use column store index for better compression and performance. By default it will have 60 distribution before partition and for better performance it is expected to have 1million rows per distribution.","timestamp":"1683282180.0","poster":"jeroenmouse","comment_id":"889953","upvote_count":"2"},{"content":"Selected Answer: C\nConsider using a clustered columnstore index when:\nEach partition has at least a million rows. Columnstore indexes have rowgroups within each partition. If the table is too small to fill a rowgroup within each partition, you won't get the benefits of columnstore compression and query performance.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-design-guidance?view=sql-server-ver16#use-a-clustered-columnstore-index-for-large-data-warehouse-tables","timestamp":"1683274740.0","upvote_count":"2","poster":"shakes103","comment_id":"889893","comments":[{"poster":"peches","comment_id":"910245","content":"Exactly, when EACH partition has at least a million rows, hence the whole table should have at least 60 million rows since by default each partition is distributed between 60 nodes (6.000.000/60 = 1.000.000).","upvote_count":"2","timestamp":"1685450880.0"}]},{"poster":"jlad26","upvote_count":"2","timestamp":"1683272040.0","comment_id":"889865","content":"Selected Answer: D\n\"For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed. Before partitions are created, dedicated SQL pool already divides each table into 60 distributions. Any partitioning added to a table is in addition to the distributions created behind the scenes. Using this example, if the sales fact table contained 36 monthly partitions, and given that a dedicated SQL pool has 60 distributions, then the sales fact table should contain 60 million rows per month\" (https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition)"},{"comment_id":"889613","poster":"dksks","timestamp":"1683227400.0","content":"When creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed.","upvote_count":"1"},{"comment_id":"889359","poster":"19c1ee5","timestamp":"1683205440.0","content":"Selected Answer: D\nClustered columnstore has the best compression with 1M rows. So it should be 1M * 60.","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/microsoft/view/108483-exam-dp-203-topic-1-question-78-discussion/","question_text":"You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou plan to create a fact table named Table1 that will contain a clustered columnstore index.\n\nYou need to optimize data compression and query performance for Table1.\n\nWhat is the minimum number of rows that Table1 should contain before you create partitions?","answer_description":"","choices":{"D":"60 million","B":"600,000","C":"1 million","A":"100,000"},"topic":"1","unix_timestamp":1683205440,"answers_community":["D (77%)","C (18%)","5%"],"question_id":94,"timestamp":"2023-05-04 15:04:00"},{"id":"n6hFXZnLHC5Ek3BxyF2i","discussion":[{"comments":[{"timestamp":"1736081220.0","comment_id":"1336753","content":"SalesRepID is either business key or surrogate key, and you don't change it... there is no column in this table that acts as active flag, so you only need to change EndDate column.","poster":"hypersam","upvote_count":"2"},{"comments":[{"poster":"robmz","comments":[{"content":"active flag is optional for type 2 and in this scenario does not exist","upvote_count":"6","poster":"Dusica","timestamp":"1714550160.0","comment_id":"1204939"}],"content":"Exactly.\n\nSCD TYPE2 Needs three columns: StartDate, EndDate and isActive flag.","timestamp":"1712329620.0","upvote_count":"2","comment_id":"1189978"}],"comment_id":"905456","upvote_count":"2","content":"This will preserve the history of changes to the salesperson’s last name while keeping the most current information in the table","timestamp":"1684900740.0","poster":"bakamon"}],"upvote_count":"22","comment_id":"905452","poster":"bakamon","content":"Selected Answer: BC\n1) Insert an extra row with the updated last name and the current date as the StartDate.\n2) Update two columns of an existing row: set the EndDate of the previous row for that salesperson to the current date and set the current value of the SalesRepID column to inactive.","timestamp":"1684900440.0"},{"upvote_count":"21","comment_id":"907910","content":"Selected Answer: CD\nCD is correct","timestamp":"1685184060.0","poster":"Ankit_Az"},{"upvote_count":"1","poster":"Stuartsen","timestamp":"1741155240.0","content":"Selected Answer: CD\nCorrect C, D","comment_id":"1365297"},{"content":"Selected Answer: BC\ncorrect answer B and C","upvote_count":"1","timestamp":"1738021800.0","comment_id":"1347637","poster":"samianae"},{"poster":"EmnCours","upvote_count":"1","content":"Selected Answer: CD\nCorrect Answer: CD","comment_id":"1319555","timestamp":"1732858440.0"},{"content":"C - extra row with new surname and new start date\nD-set end date in the existing column\nThat makes it type 2 - one of the variations off","timestamp":"1714550100.0","comment_id":"1204938","upvote_count":"2","poster":"Dusica"},{"content":"I originally thought it would be BC, but then I realised you WOULDN'T update the Last Name, you want to retain the history of the LastName. You would update the EndDate column for that record to mark it as historic and then insert a new row with the new LastName.","poster":"s_unsworth","upvote_count":"6","timestamp":"1709158080.0","comment_id":"1162025"},{"upvote_count":"1","comment_id":"1116494","poster":"dakku987","content":"Selected Answer: CD\nCD is correct as in scd2 we need startdate,enddate that is already present \nwhat we need is \"ISActive/flag\" and one more row thats all its takes to make scd2","timestamp":"1704703680.0"},{"upvote_count":"3","poster":"hassexat","comment_id":"1001293","content":"Selected Answer: CD\nC & D are correct","timestamp":"1694071860.0"},{"content":"Selected Answer: CD\n- Update one column: EndDate to the change date\n- Insert a new record with the new value of LastName, StartDate as the change date.","poster":"tankwayep","timestamp":"1693883760.0","comment_id":"998991","upvote_count":"10"},{"timestamp":"1693817400.0","comment_id":"998389","content":"Selected Answer: CD\ncorrect","upvote_count":"2","poster":"kkk5566"},{"content":"cd is correct","upvote_count":"2","comment_id":"984369","timestamp":"1692353820.0","poster":"lfss"},{"upvote_count":"9","timestamp":"1692137280.0","comment_id":"982022","content":"Selected Answer: CD\nanswer should be CD, since activeRow flag is not present, we need to update only end date.","poster":"Deeksha1234"},{"poster":"[Removed]","timestamp":"1691713140.0","content":"Selected Answer: CD\nCD is correct","comment_id":"978152","upvote_count":"2"},{"timestamp":"1684850100.0","content":"It's SCD Type 2 - you need to update at least three columns in the original raw:\nSurname, StartDate and EndDate. (IsActive if one exists). Then insert new record.\nA and C","upvote_count":"1","poster":"Rob77","comments":[{"comment_id":"904968","timestamp":"1684850160.0","upvote_count":"1","poster":"Rob77","content":"* \"original row\"","comments":[{"comment_id":"910275","upvote_count":"7","content":"but if you update the surname on the original row, don't you lose the previous value?","timestamp":"1685453040.0","poster":"peches"}]},{"content":"No the StartDate stays, you only need to update the EndDate in the original row, the old name also stays to track which names he had, only the new row should have the new name. So you would only need to edit the EndDate column on the old row and since there is no \"IsActive\" flag you ignore it, maybe it's just queried by date and sorted by date and you take the last row which is the newest.","poster":"Vanq69","comment_id":"1024890","upvote_count":"3","timestamp":"1696429260.0"}],"comment_id":"904967"},{"comments":[{"comment_id":"902291","timestamp":"1684543200.0","poster":"ajhak","content":"It's saying \"update on column of an EXISTING row\". AKA you're just changing the IsCurrent part of the existing row, that's it.","upvote_count":"1"}],"timestamp":"1683995520.0","comment_id":"896884","content":"For me this is a little dubious since besides the end date update for the record we could have flg_is_active as well. Making B a possible answer in my opinion","upvote_count":"4","poster":"laurasscastro"},{"comment_id":"890172","content":"Selected Answer: CD\nThe answer is correct","timestamp":"1683305400.0","poster":"henryphchan","upvote_count":"4"},{"upvote_count":"4","content":"Selected Answer: CD\nAns is correct","poster":"[Removed]","timestamp":"1683286500.0","comment_id":"889997"},{"content":"Selected Answer: CD\nSCD Type 2 will have historical changes hence we will have new row and we need to update the existing row's end date. Hence - CD\n\nhttps://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/","comment_id":"889956","timestamp":"1683282420.0","poster":"jeroenmouse","upvote_count":"6"},{"poster":"Yemeral","comment_id":"889909","content":"Selected Answer: CD\nCorrect. You need to insert a new row with the updated data and update the EndDate of the old row","timestamp":"1683276780.0","upvote_count":"7"}],"answers_community":["CD (76%)","BC (24%)"],"answer_images":[],"answer":"CD","timestamp":"2023-05-05 10:53:00","topic":"1","question_images":[],"choices":{"D":"Update one column of an existing row.","B":"Update two columns of an existing row.","A":"Update three columns of an existing row.","C":"Insert an extra row."},"answer_description":"","question_text":"You have an Azure Synapse Analytics dedicated SQL pool that contains a table named DimSalesPerson. DimSalesPerson contains the following columns:\n\n• RepSourceID\n• SalesRepID\n• FirstName\n• LastName\n• StartDate\n• EndDate\n• Region\n\nYou are developing an Azure Synapse Analytics pipeline that includes a mapping data flow named Dataflow1. Dataflow1 will read sales team data from an external source and use a Type 2 slowly changing dimension (SCD) when loading the data into DimSalesPerson.\n\nYou need to update the last name of a salesperson in DimSalesPerson.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.","unix_timestamp":1683276780,"question_id":95,"exam_id":67,"answer_ET":"CD","url":"https://www.examtopics.com/discussions/microsoft/view/108530-exam-dp-203-topic-1-question-79-discussion/","isMC":true}],"exam":{"numberOfQuestions":384,"id":67,"isBeta":false,"provider":"Microsoft","isMCOnly":false,"lastUpdated":"12 Apr 2025","isImplemented":true,"name":"DP-203"},"currentPage":19},"__N_SSP":true}