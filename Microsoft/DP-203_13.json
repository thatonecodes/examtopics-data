{"pageProps":{"questions":[{"id":"EC6PbEJk3Elf8jZyvf2i","timestamp":"2022-04-25 09:53:00","answer_description":"Box 1: Hot -\nHot tier - An online tier optimized for storing data that is accessed or modified frequently. The Hot tier has the highest storage costs, but the lowest access costs.\n\nBox 2: Cool -\nCool tier - An online tier optimized for storing data that is infrequently accessed or modified. Data in the Cool tier should be stored for a minimum of 30 days. The\nCool tier has lower storage costs and higher access costs compared to the Hot tier.\n\nBox 3: Cool -\nNot Archive tier - An offline tier optimized for storing data that is rarely accessed, and that has flexible latency requirements, on the order of hours. Data in the\nArchive tier should be stored for a minimum of 180 days.\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview https://www.altaro.com/hyper-v/azure-archive-storage/","topic":"1","answer":"","discussion":[{"timestamp":"1690304280.0","comment_id":"636836","upvote_count":"23","content":"right , Hot-cool-cool","poster":"Deeksha1234"},{"timestamp":"1682409180.0","upvote_count":"10","content":"Correct answer!\nHot, Cool, Cool","comment_id":"591460","poster":"Andy91"},{"upvote_count":"3","timestamp":"1725691980.0","content":"Hot\nCool\nCool","poster":"hassexat","comment_id":"1001249"},{"timestamp":"1725368400.0","poster":"kkk5566","comment_id":"997637","content":"hot,cool,cool","upvote_count":"3"},{"timestamp":"1715140800.0","poster":"rocky48","upvote_count":"4","comment_id":"891788","content":"Hot, Cool, Cool"},{"upvote_count":"2","comment_id":"625128","comments":[{"poster":"lucasramos","content":"Reads in Cool Tier are more expensive than in Hot Tier. Since the data will be accessed frequently in the first week, it makes sense to store it in hot tier to minimize costs.","timestamp":"1689101340.0","upvote_count":"6","comment_id":"630177"}],"content":"Isn't the cool storage enough for initial requirements and also required for the other options? So shouldn't the answer be cool in all places? that would be cool","poster":"Fishy_Marcy","timestamp":"1688114280.0"},{"comments":[{"comment_id":"598907","upvote_count":"23","poster":"Guincimund","content":"\"After one year, the data will be accessed infrequently but must be accessible within five minutes\"\nThe latency for the first bytes, is \"hours\" for the archive. so because they want to be able to access the data within 5 min, you need to place it in \"cool\"\n\nSo the answer is correct.","timestamp":"1683614820.0"},{"comment_id":"891379","upvote_count":"2","poster":"AnonymousJhb","content":"hydration on the archive tier is hours, not minutes. hence its hot, cool, cool.","timestamp":"1715085840.0"},{"upvote_count":"5","poster":"SandipSingha","timestamp":"1683626400.0","content":"After one year, the data will be accessed infrequently but must be accessible within five minutes.","comment_id":"598969"}],"poster":"nefarious_smalls","content":"Why would it not be be Hot Cool and Archive","timestamp":"1683515640.0","upvote_count":"2","comment_id":"598419"},{"poster":"nefarious_smalls","timestamp":"1683515580.0","upvote_count":"1","content":"I dont know","comment_id":"598418"}],"isMC":false,"unix_timestamp":1650873180,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0009700001.png"],"answers_community":[],"exam_id":67,"answer_ET":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0009800001.png","https://www.examtopics.com/assets/media/exam-media/04259/0009900001.jpg"],"question_text":"HOTSPOT -\nYou are designing an application that will store petabytes of medical imaging data.\nWhen the data is first created, the data will be accessed frequently during the first week. After one month, the data must be accessible within 30 seconds, but files will be accessed infrequently. After one year, the data will be accessed infrequently but must be accessible within five minutes.\nYou need to select a storage strategy for the data. The solution must minimize costs.\nWhich storage tier should you use for each time frame? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_id":61,"url":"https://www.examtopics.com/discussions/microsoft/view/74417-exam-dp-203-topic-1-question-48-discussion/"},{"id":"dRsuBlrUfXQrDr4chXh2","answer_description":"","choices":{"A":"Use a Conditional Split transformation in an Azure Synapse data flow.","D":"Load the data by using PySpark.","C":"Load the data by using the OPENROWSET Transact-SQL command in an Azure Synapse Analytics serverless SQL pool.","B":"Use a Get Metadata activity in Azure Data Factory."},"question_id":62,"unix_timestamp":1651054500,"answer":"D","isMC":true,"answers_community":["D (90%)","10%"],"answer_ET":"D","url":"https://www.examtopics.com/discussions/microsoft/view/74669-exam-dp-203-topic-1-question-49-discussion/","question_images":[],"question_text":"You have an Azure Synapse Analytics Apache Spark pool named Pool1.\nYou plan to load JSON files from an Azure Data Lake Storage Gen2 container into the tables in Pool1. The structure and data types vary by file.\nYou need to load the files into the tables. The solution must maintain the source data types.\nWhat should you do?","answer_images":[],"timestamp":"2022-04-27 12:15:00","discussion":[{"content":"Should be D, it's about Apache Spark pool, not serverless SQL pool.","timestamp":"1651054500.0","upvote_count":"40","comment_id":"593038","poster":"galacaw"},{"content":"Selected Answer: D\nIf your JSON files have a consistent structure and data types, then OPENROWSET is a good option. However, if your JSON files have a varying structure and data types, then PySpark is a better option.","poster":"Joanna0","timestamp":"1704914520.0","upvote_count":"6","comment_id":"1118975"},{"upvote_count":"1","poster":"EmnCours","timestamp":"1732773060.0","content":"Selected Answer: D\nCorrect Answer: D","comment_id":"1319038"},{"timestamp":"1723443900.0","upvote_count":"1","comment_id":"1264510","content":"The answer is C because with external table you can load the data with solution must maintain the source data types.","poster":"vaibhavs120"},{"comment_id":"1243950","timestamp":"1720375020.0","upvote_count":"1","poster":"e56bb91","content":"Selected Answer: D\nChatGPT 4o\nUsing PySpark in an Apache Spark pool within Azure Synapse Analytics is the most flexible and powerful way to handle JSON files with varying structures and data types. PySpark can infer schema and handle complex data transformations, making it well-suited for loading heterogeneous JSON data into tables while preserving the original data types."},{"poster":"Okkier","comment_id":"1243367","upvote_count":"1","content":"Selected Answer: D\nWhen loading data into an Apache Spark pool, especially when dealing with inconsistent file structures, PySpark (the Python API for Spark) is generally the better choice over OpenRowset. This is because PySpark offers greater flexibility, better performance, and more robust handling of varied and complex data structures.","timestamp":"1720270800.0"},{"upvote_count":"1","comment_id":"1239105","content":"should be D","poster":"kldakdlsa","timestamp":"1719646140.0"},{"poster":"ellala","content":"Selected Answer: D\nWe have an \"Azure Synapse Analytics Apache Spark pool\" therefore, we use Spark. There is no information about a serverless SQL Pool","comment_id":"1027834","timestamp":"1696755840.0","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: D\nShould be D","poster":"kkk5566","timestamp":"1693746120.0","comment_id":"997641"},{"content":"Selected Answer: D\nPySpark provides a powerful and flexible programming interface for processing and loading data in Azure Synapse Analytics Apache Spark pools. With PySpark, you can leverage its JSON reader capabilities to infer the schema and maintain the source data types during the loading process.","comment_id":"931932","upvote_count":"3","timestamp":"1687552080.0","poster":"vctrhugo"},{"poster":"vctrhugo","timestamp":"1686234240.0","comment_id":"918356","content":"Selected Answer: D\nTo load JSON files from an Azure Data Lake Storage Gen2 container into tables in an Azure Synapse Analytics Apache Spark pool, you can use PySpark. PySpark provides a flexible and powerful framework for working with big data in Apache Spark.\n\nTherefore, the correct answer is:\n\nD. Load the data by using PySpark.\n\nYou can use PySpark to read the JSON files from Azure Data Lake Storage Gen2, infer the schema, and load the data into tables in the Spark pool while maintaining the source data types. PySpark provides various functions and methods to handle JSON data and perform transformations as needed before loading it into tables.","upvote_count":"4"},{"upvote_count":"1","comment_id":"905089","timestamp":"1684860480.0","poster":"janaki","content":"Option D: Load the data by using PySpark"},{"content":"Selected Answer: D\nThe question stated that \"You have an Azure Synapse Analytics Apache Spark pool named Pool1.\", so this question is about Spark pool","timestamp":"1683044700.0","upvote_count":"1","poster":"henryphchan","comment_id":"887671"},{"content":"Selected Answer: C\nAs stated by Microsoft, \"Serverless SQL pool can automatically synchronize metadata from Apache Spark. A serverless SQL pool database will be created for each database existing in serverless Apache Spark pools.\". So even though the files in Azure Storage were created with Apache Spark, you can still query them using OPENROWSET with a serverless SQL Pool\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-spark-tables","timestamp":"1681923120.0","comment_id":"874924","comments":[{"upvote_count":"1","timestamp":"1713159720.0","poster":"dgerok","comment_id":"1195831","content":"We are dealing with varying JSON. There is nothing about this option by the link you've provided. The correct answer is D..."},{"upvote_count":"2","content":"As the question states that \"You need to load the files into the tables\" , through serverless sql pool we cannot load data. so the answer should be D","comment_id":"1076623","timestamp":"1700597280.0","poster":"Tejashu"}],"poster":"Victor_Kings","upvote_count":"4"},{"comment_id":"840553","timestamp":"1678941540.0","content":"Selected Answer: D\nTo load JSON files from an Azure Data Lake Storage Gen2 container into the tables in an Apache Spark pool in Azure Synapse Analytics while maintaining the source data types, you should use PySpark.","upvote_count":"3","poster":"esaade"},{"comment_id":"815487","timestamp":"1676908680.0","upvote_count":"2","poster":"haidebelognime","content":"Selected Answer: D\nPySpark is the Python API for Apache Spark, which is a distributed computing framework that can handle large-scale data processing."},{"poster":"brzhanyu","timestamp":"1669861560.0","content":"Selected Answer: D\nShould be D, it's about Apache Spark pool, not serverless SQL pool.","upvote_count":"2","comment_id":"732177"},{"upvote_count":"2","content":"Selected Answer: D\nIts a spark pool","poster":"smsme323","timestamp":"1664175480.0","comment_id":"679486"},{"poster":"Deeksha1234","content":"Both C and D looks correct","timestamp":"1658768700.0","comment_id":"636839","upvote_count":"2"},{"comment_id":"636742","comments":[{"comments":[{"content":"but openrowset can also work with different data types","comment_id":"1014110","poster":"jiajiani","timestamp":"1695387480.0","upvote_count":"2"}],"comment_id":"665295","poster":"Gg2","content":"The problem is that \"The solution MUST maintain the source data types.\"","upvote_count":"3","timestamp":"1662804180.0"}],"content":"I agree with D, I already used PySpark for exactly that purpose. But what is the problem with C? Could it be that both answers are correct?","upvote_count":"1","timestamp":"1658756760.0","poster":"Oli388"},{"comment_id":"635122","content":"D\n\nFirst, there is the Apache Spark pool. And Azure Data Lake is based on Apache Spark Delta Lake. The most suitable answer is D.","upvote_count":"3","timestamp":"1658484480.0","poster":"Dicer"},{"comment_id":"620496","poster":"am85","upvote_count":"3","timestamp":"1655913240.0","content":"Selected Answer: D\nIt should be D."},{"poster":"jihenTR13","content":"should be D, apache spark has an option inferschema that can be used to read the file and infer the schema from it automatically without the need to explecitly specify it","comment_id":"617857","timestamp":"1655488140.0","upvote_count":"1"},{"content":"Selected Answer: D\nAgree with ppl below. Should be D since it is not only about the files but also the pool technology. We're not changing pool. It remains Apache Spark","upvote_count":"2","comment_id":"615170","poster":"objecto","timestamp":"1655013600.0"},{"content":"Selected Answer: D\nApache > pyspark","upvote_count":"1","comment_id":"608753","timestamp":"1653827700.0","poster":"sdokmak"},{"content":"Why PySpark?","timestamp":"1653139860.0","comment_id":"604909","poster":"Ben_1010","upvote_count":"1"},{"poster":"Andushi","comment_id":"597583","timestamp":"1651818600.0","upvote_count":"1","content":"Selected Answer: D\nShould be D, I agree with @galacaw"}],"topic":"1","exam_id":67},{"id":"kzXekcQU3ASFexLK4Hw2","answer_description":"Report1: CSV -\nCSV: The destination writes records as delimited data.\n\nReport2: AVRO -\nAVRO supports timestamps.\nNot Parquet, TSV: Not options for Azure Data Lake Storage Gen2.\nReference:\nhttps://streamsets.com/documentation/datacollector/latest/help/datacollector/UserGuide/Destinations/ADLS-G2-D.html","exam_id":67,"unix_timestamp":1620625320,"question_id":63,"question_text":"HOTSPOT -\nYou are planning the deployment of Azure Data Lake Storage Gen2.\nYou have the following two reports that will access the data lake:\n✑ Report1: Reads three columns from a file that contains 50 columns.\n✑ Report2: Queries a single record based on a timestamp.\nYou need to recommend in which format to store the data in the data lake to support the reports. The solution must minimize read times.\nWhat should you recommend for each report? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer":"","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0002300001.png"],"topic":"1","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0002200001.png"],"discussion":[{"timestamp":"1621319580.0","upvote_count":"224","content":"1: Parquet - column-oriented binary file format\n2: AVRO - Row based format, and has logical type timestamp\nhttps://youtu.be/UrWthx8T3UY","comments":[{"content":"This is Correct\nsince we only have to read three columns its best to use column based storage \nAVRO for report2 is fine because as per the AVRO format defintin it already got timestmp","timestamp":"1743018180.0","upvote_count":"2","poster":"technoguy","comment_id":"1410536"},{"timestamp":"1650173040.0","upvote_count":"3","comment_id":"587059","poster":"azurestudent1498","content":"this is correct."},{"poster":"XiltroX","content":"Thanks for the video share, this really helps. Cheers.","timestamp":"1669064700.0","upvote_count":"2","comment_id":"723933"},{"comment_id":"379484","comments":[{"upvote_count":"25","comment_id":"392994","content":"Ok, but in 1st case we need only 3 of 50 columns. Parquet i columnar format. In 2nd Avro because ideal for read full row","timestamp":"1624888560.0","poster":"vlad888"}],"upvote_count":"9","content":"the web is full of old information. timestamp support has been added to parquet","poster":"terajuana","timestamp":"1623390660.0"}],"comment_id":"360159","poster":"alain2"},{"upvote_count":"33","timestamp":"1620649140.0","comment_id":"353671","content":"Shouldn't the answer for Report 1 be Parquet? Because Parquet format is Columnar and should be best for reading a few columns only.","poster":"Himlo24"},{"comment_id":"1400610","content":"1.Parquet\n2.AVRO","poster":"ngabonzic","upvote_count":"1","timestamp":"1742404440.0"},{"comment_id":"1340644","content":"report 1: Parquet (column-based file format optimized for reading specific columns)\nreport 2:Avro(Row-based file format)","poster":"krishna1303","timestamp":"1736910480.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1302433","poster":"ff5037f","content":"the answer should be Parquet for report 1","timestamp":"1729771140.0"},{"content":"The goal is: The solution must minimize read times.\n\nI made small test on Databrick plus DataLake.\nThe same file saved as Parquet and Avro\n9 mln of records.\nParquet ~150 MB\nAvro ~700MB\n\nReading Parquet is always 10 times faster that Avro.\nI checked:\n- for all data or small range of data with condition\n- all or only one column\n\nSo I will select option:\n- Parquet\n- Parquet","timestamp":"1727075700.0","upvote_count":"3","poster":"marcin1212","comments":[{"poster":"dev2dev","upvote_count":"1","comment_id":"515410","timestamp":"1641190500.0","content":"how can be faster read is same as number of reads?"}],"comment_id":"500257"},{"comment_id":"538335","content":"1. Parquet\n2. Avro\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices\n\n\"Consider using the Avro file format in cases where your I/O patterns are more write heavy, or the query patterns favor retrieving multiple rows of records in their entirety. \n\nConsider Parquet and ORC file formats when the I/O patterns are more read heavy or when the query patterns are focused on a subset of columns in the records.\"","poster":"ragz_87","timestamp":"1727075700.0","upvote_count":"9","comments":[{"comment_id":"572441","content":"Thank you.","timestamp":"1647888300.0","upvote_count":"1","poster":"SebK"}]},{"comment_id":"758808","poster":"akk_1289","content":"To minimize read times for the two reports, it is recommended to store the data in the data lake in the parquet format.\n\nParquet is a columnar storage format that is optimized for querying large datasets. It stores data in a compact and efficient manner, allowing for fast querying and filtering of data.\n\nIn this case, Report1 needs to read only three columns from a file that contains 50 columns. Since parquet stores data in a columnar format, the query can skip reading the unnecessary columns and only read the required ones, which can greatly improve the read performance.\n\nReport2 needs to query a single record based on a timestamp. Parquet also supports efficient filtering and querying based on specific values, such as timestamps, making it a good choice for this report as well.\n\nOther formats, such as avro, csv, and tsv, may not provide the same level of performance for these types of queries. Therefore, it is recommended to use parquet to store the data in the data lake.","timestamp":"1727075700.0","upvote_count":"9"},{"upvote_count":"1","content":"Report 1: parquet\nColumnar Storage: Parquet stores data in columns, allowing efficient reading of only the required columns (3 out of 50).\nReport 2: Avro.\nFast Single-Record Access: Optimized row-based formats excel at quickly accessing individual records based on a specific condition, such as a timestamp.\nI don't understand why incorrect answers are being provided, causing confusion.","poster":"lisa710","timestamp":"1727075640.0","comment_id":"1101704"},{"upvote_count":"3","poster":"ypan","content":"Recommendations:\nReport1: Use Parquet\nReason: Parquet is a columnar storage format that is optimized for reading specific columns. Since Report1 needs to read only three columns out of 50, Parquet allows reading just those columns efficiently without scanning the entire file.\n\nReport2: Use Avro\nReason: Avro is a row-based storage format, which is efficient for retrieving entire rows based on a specific condition, such as a timestamp. It allows quick access to individual records, making it suitable for Report2's requirement.","timestamp":"1727075640.0","comment_id":"1232054"},{"comment_id":"1285399","timestamp":"1726596660.0","poster":"ItsPayakan","upvote_count":"1","content":"Why cant it be Parquet for both reports? Parquet supports timestamp too"},{"content":"1- Parquet 2- Avro\n\nParquet for Column selection(stored in columnar format) and Avro for row selection (stored in row format)","comment_id":"1257020","upvote_count":"1","poster":"207680a","timestamp":"1722194580.0"},{"timestamp":"1713851760.0","content":"1. Parquet\n2. Avro","comment_id":"1200493","poster":"Vaibhav251999","upvote_count":"2"},{"comment_id":"1171323","poster":"Alongi","timestamp":"1710193740.0","content":"Parquet\nAvro","upvote_count":"1"},{"comment_id":"1135539","timestamp":"1706593980.0","content":"1. Parquet - Column oriented data store\n2. AVRO - supports timestamps","poster":"mykel71","upvote_count":"1"},{"timestamp":"1704235560.0","content":"Agree:\n1: Parquet - ideal for columnar forma\n2: AVRO: Row-based with logical timestamp","poster":"sdg2844","upvote_count":"2","comment_id":"1112334"},{"upvote_count":"1","comment_id":"1040325","content":"--> TLDR <--\n AVRO PARQUET ORC\nAnal. Queries v v\nWrite Ops (ETL ops) v \nNested Data v \nACID Properties v\nSch.Flexibility v","timestamp":"1697010480.0","poster":"matiandal"},{"upvote_count":"1","comment_id":"1001207","poster":"hassexat","timestamp":"1694065320.0","content":"1. Parquet --> Column format\n2. AVRO --> Row format with timestamp type"},{"upvote_count":"1","poster":"kkk5566","content":"1- Parquet 2- Parquet","comment_id":"993115","timestamp":"1693314720.0","comments":[{"content":"sorry 2 avro","poster":"kkk5566","comment_id":"1003863","upvote_count":"1","timestamp":"1694337180.0"}]},{"timestamp":"1682963220.0","poster":"endeesa","comment_id":"886565","upvote_count":"2","content":"Report 1: parquet\nReport 2: Avro"},{"comment_id":"879902","upvote_count":"3","content":"1: Parquet - column-oriented binary file format\n2: AVRO - Row based format","timestamp":"1682395020.0","poster":"rocky48"},{"content":"It should be Parquet and Avro","timestamp":"1681714380.0","comment_id":"872423","poster":"DipikaChavan","upvote_count":"2"},{"upvote_count":"1","poster":"deutscher","timestamp":"1681024980.0","comment_id":"865340","content":"I completely agree, \nParquet is column based \nAVRO is row based"},{"comment_id":"802226","poster":"bubby248","timestamp":"1675872780.0","upvote_count":"2","content":"Parquet,Avro"},{"content":"Parquet \nAVRO","poster":"Venub28","upvote_count":"2","timestamp":"1673713320.0","comment_id":"775708"},{"timestamp":"1672149300.0","poster":"Yamarh","comment_id":"758586","content":"Parquet - Avro","upvote_count":"1"},{"comment_id":"746794","timestamp":"1671164280.0","content":"Report 1: PARQUET\nBecause parquet is a columnar file format file.\n\nReport 2: AVRO\nBecause Avro is a row-based file format (as Json) which is connected to logical timestamp.","poster":"vigilante89","upvote_count":"2"},{"comment_id":"646401","timestamp":"1660408320.0","poster":"Deeksha1234","upvote_count":"3","content":"1. Parquet\n2. AVRO"},{"comment_id":"623949","timestamp":"1656417540.0","content":"Parquet - Avro makes more sense given the definitios given by Microsoft","upvote_count":"2","poster":"ROLLINGROCKS"},{"comment_id":"619834","poster":"temacc","upvote_count":"1","timestamp":"1655818440.0","content":"1. Parquet. 3 of 50 columns\n2. Parquet. In case with single row work faster, than AVRO."},{"poster":"main616","timestamp":"1652427780.0","content":"1. csv (or json) . csv/json support query accelerate by select specified rows https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration#overview\n2. avro","upvote_count":"5","comment_id":"601013"},{"upvote_count":"3","poster":"Dothy","content":"1: Parquet\n2: AVRO","comment_id":"600059","timestamp":"1652266080.0"},{"timestamp":"1646464020.0","content":"Consider Parquet and ORC file formats when the I/O patterns are more read heavy or when the query patterns are focused on a subset of columns in the records.\n\nthe Avro format works well with a message bus such as Event Hub or Kafka that write multiple events/messages in succession.","comment_id":"561282","poster":"RalphLiang","upvote_count":"3"},{"comments":[{"content":"CSV is not optimized for column reads","poster":"dduque10","timestamp":"1669637820.0","upvote_count":"1","comment_id":"729070"}],"content":"Why NOT csv in report1 ?","poster":"MohammadKhubeb","upvote_count":"2","timestamp":"1643077440.0","comment_id":"531791"},{"upvote_count":"5","content":"This has to be parquet and AVRO , got the answer from Udemy","timestamp":"1641711960.0","poster":"Sandip4u","comment_id":"519993"},{"poster":"Mahesh_mm","upvote_count":"3","timestamp":"1640525040.0","comment_id":"509611","content":"1. Parquet\n2. AVRO"},{"comment_id":"483374","upvote_count":"3","content":"Solution says parquet is not supported for adls gen 2 but it actually is: https://docs.microsoft.com/en-us/azure/data-factory/format-parquet","poster":"Ozzypoppe","timestamp":"1637507280.0"},{"content":"An interesting and complete article that explains the different uses between parquet/avro/csv and gives answers to the question : https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8","comment_id":"464079","upvote_count":"6","timestamp":"1634555880.0","poster":"noranathalie"},{"upvote_count":"1","content":"https://luminousmen.com/post/big-data-file-formats","comment_id":"410842","timestamp":"1626867060.0","poster":"elimey"},{"upvote_count":"1","timestamp":"1626866640.0","content":"Report 1 definitely Parquet","comment_id":"410837","poster":"elimey"},{"poster":"noone_a","comment_id":"402500","upvote_count":"1","content":"report 1 - Parquet as it is columar.\nreport 2 - avro as it is row based and can be compressed further than csv.","timestamp":"1625816520.0"},{"content":"1- Parquet\n2- Parquet\nSince they are all querying; AVRO is good for writing, OLTP, Parquet is good for quering/read","comment_id":"367971","upvote_count":"6","poster":"bc5468521","timestamp":"1622122200.0"},{"timestamp":"1621092240.0","poster":"szpinat","content":"For Report 2 - why not csv?","comment_id":"357996","upvote_count":"1"},{"content":"there is no mention fo aviro in the learning materials provided by Microsoft. not sure about it","timestamp":"1620625320.0","poster":"ehnw","upvote_count":"1","comment_id":"353461"}],"answer_ET":"","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/52254-exam-dp-203-topic-1-question-5-discussion/","timestamp":"2021-05-10 07:42:00"},{"id":"Hb4qUn6WYpvyHsqyz7Dj","choices":{"B":"Create a cluster policy in workspace1.","D":"Create a pool in workspace1.","C":"Upgrade workspace1 to the Premium pricing tier.","A":"Configure a global init script for workspace1."},"url":"https://www.examtopics.com/discussions/microsoft/view/74670-exam-dp-203-topic-1-question-50-discussion/","question_id":64,"exam_id":67,"discussion":[{"content":"Answer D is correct. Azure Databricks pools reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.","comment_id":"609704","timestamp":"1653997920.0","poster":"hkay","upvote_count":"13"},{"upvote_count":"9","timestamp":"1683543600.0","content":"D is accurate answer and this link show this info explicty \nhttps://learn.microsoft.com/en-us/azure/databricks/clusters/cluster-config-best-practices","comment_id":"892043","poster":"kim32"},{"content":"Selected Answer: D\nCorrect Answer: D","upvote_count":"1","poster":"EmnCours","comment_id":"1319040","timestamp":"1732773240.0"},{"timestamp":"1729836180.0","poster":"ff5037f","content":"Selected Answer: D\nboth c and d are correct, but option c is expensive. as question clearly states that solution should be cheaper","comment_id":"1302782","upvote_count":"1"},{"poster":"Alongi","timestamp":"1714549020.0","comment_id":"1204928","content":"Selected Answer: C\nI found this question on my exam yesterday, 30/04/2024, and I put C. I passed the exam with a high score, but I'm not sure if the answer is correct.","upvote_count":"2","comments":[{"timestamp":"1714737420.0","comment_id":"1206090","content":"The answer C seems to be false, given that we have to minimize the costs and the prenium tier has a higher cost","upvote_count":"1","poster":"Slimshaddy"}]},{"poster":"dgerok","timestamp":"1713159900.0","upvote_count":"1","content":"Selected Answer: D\nThe solution must minimize costs, so upgrade to Premium is not an option.","comment_id":"1195832"},{"timestamp":"1706500740.0","content":"This came in my today's exam . Answer D is correct .","poster":"Ajjjiii","upvote_count":"4","comment_id":"1134644"},{"comment_id":"1118977","poster":"Joanna0","timestamp":"1704914640.0","content":"Selected Answer: D\nThe best solution to reduce the time it takes for cluster1 to start and scale up while minimizing costs is to create a pool in workspace1.\n\nA pool is a group of preconfigured clusters that share resources and can be quickly provisioned and scaled up or down as needed. This can significantly reduce the time it takes for cluster1 to start and scale up, as the resources are already provisioned and ready to use.","upvote_count":"4"},{"content":"Got this question today on the exam. Selected Upgrade to Premium tier, was incorrect of course","upvote_count":"3","comment_id":"1102996","poster":"positivitypeople","timestamp":"1703194380.0"},{"timestamp":"1693746120.0","upvote_count":"1","poster":"kkk5566","content":"Selected Answer: D\nD is correct","comment_id":"997642"},{"content":"C is the answer","upvote_count":"1","poster":"Abdullah77","timestamp":"1692344520.0","comment_id":"984253"},{"timestamp":"1687159140.0","poster":"auwia","content":"Selected Answer: D\nTo reduce the time it takes for cluster1 to start and scale up in Azure Databricks workspace, the first step you should take is to create a pool in workspace1.","comment_id":"927254","upvote_count":"1"},{"comment_id":"917289","timestamp":"1686147480.0","content":"The correct answer in this scenario would be B. Create a cluster policy in workspace1.\n\nCreating a cluster policy allows you to define a set of rules and configurations that apply to all clusters within the workspace. By creating a cluster policy, you can optimize the cluster startup and scaling behavior.\n\nWith a cluster policy, you can specify settings such as the minimum and maximum number of worker nodes, the idle timeout duration, and the auto-termination behavior. By tuning these settings, you can reduce the time it takes for cluster1 to start and scale up, ensuring that resources are efficiently utilized.\n\nConfiguring a global init script (option A) or creating a pool (option D) may have other benefits, but they are not directly related to reducing the time it takes for cluster1 to start and scale up. Upgrading the workspace to the Premium pricing tier (option C) may offer additional features but is not necessary to address the specific requirement of minimizing cluster startup and scaling time while minimizing costs.","upvote_count":"2","poster":"aga444"},{"comment_id":"917286","upvote_count":"1","poster":"aga444","content":"How can it be C), as the question clearly states 'minimize costs'","timestamp":"1686147300.0"},{"comment_id":"905095","timestamp":"1684860840.0","upvote_count":"1","poster":"janaki","content":"I think it's \nB. Create a cluster policy in workspace1\nCluster policies in Azure Databricks allow you to define rules and configurations for cluster creation and termination. By creating a cluster policy, you can optimize the cluster start time and scale-up process based on your specific requirements."},{"poster":"mamahani","comment_id":"894962","content":"Selected Answer: D\nD is correct answer","upvote_count":"1","timestamp":"1683802560.0"},{"upvote_count":"1","content":"Selected Answer: D\nCorrect Answer: D","timestamp":"1683518580.0","poster":"rocky48","comment_id":"891791"},{"comment_id":"887678","poster":"henryphchan","timestamp":"1683044940.0","upvote_count":"2","content":"Selected Answer: D\nC and D are practical but the question asked to minimize the costs. Thus, the answer is D."},{"content":"Selected Answer: D\nD is correct","poster":"mehroosali","timestamp":"1680652860.0","comment_id":"861616","upvote_count":"2"},{"timestamp":"1678966260.0","comment_id":"840891","poster":"halamgir15","content":"Selected Answer: C\nI think C makes more sense","upvote_count":"3"},{"content":"Selected Answer: D\nD is the correct answer, as you need to lower the cost","timestamp":"1675351200.0","comment_id":"796159","poster":"SHENOOOO","upvote_count":"1"},{"comment_id":"773984","content":"Selected Answer: D\nCorrect answer is D","upvote_count":"1","poster":"nicky87654","timestamp":"1673573100.0"},{"upvote_count":"2","content":"Selected Answer: C\nCorrect answer is C","poster":"RV123","comment_id":"749617","timestamp":"1671437820.0"},{"timestamp":"1670178780.0","comments":[{"upvote_count":"3","content":"But it asks us to minimize the costs so answer is D. Use the Databricks Pool.","timestamp":"1674318000.0","comment_id":"783526","poster":"Jerrie86"}],"comment_id":"735354","content":"Correct answer is C as premium pricing tier allows faster time to autoscale.","poster":"XiltroX","upvote_count":"3"},{"comment_id":"636842","content":"D is correct","timestamp":"1658768880.0","poster":"Deeksha1234","upvote_count":"4"},{"content":"Selected Answer: D\ncorrect","upvote_count":"2","comment_id":"633306","timestamp":"1658198220.0","poster":"RanjAzure"},{"comments":[{"timestamp":"1653436380.0","upvote_count":"4","poster":"sdokmak","comment_id":"606951","content":"Answer is D: \nlooking at the reference link, pool works for this. Optimized scaling not needed to reduce 'start and scale up' times only."},{"poster":"Davico93","comments":[{"timestamp":"1656113340.0","content":"but... I don't understand, it's a standard tier, but there is an all-in-purpose cluster?","poster":"Davico93","comments":[{"content":"forget about it, Answer is \"D\"..... we need a delete option here....","timestamp":"1656113460.0","upvote_count":"3","poster":"Davico93","comment_id":"621907"},{"upvote_count":"4","comment_id":"635127","poster":"Dicer","content":"Azure Databricks pools reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances. \n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/instance-pools/","timestamp":"1658484660.0"}],"comment_id":"621905","upvote_count":"1"}],"content":"It's all-purpose cluster already!","upvote_count":"2","timestamp":"1655871780.0","comment_id":"620151"}],"upvote_count":"2","comment_id":"601440","content":"Answer should be C","timestamp":"1652506560.0","poster":"Maggiee"},{"content":"Correct","poster":"galacaw","upvote_count":"4","timestamp":"1651054560.0","comment_id":"593039"}],"timestamp":"2022-04-27 12:16:00","isMC":true,"answer_description":"","answer":"D","answers_community":["D (73%)","C (27%)"],"question_images":[],"question_text":"You have an Azure Databricks workspace named workspace1 in the Standard pricing tier. Workspace1 contains an all-purpose cluster named cluster1.\nYou need to reduce the time it takes for cluster1 to start and scale up. The solution must minimize costs.\nWhat should you do first?","answer_images":[],"unix_timestamp":1651054560,"answer_ET":"D","topic":"1"},{"id":"r6eK3ApsaCPZEMK06Rlx","exam_id":67,"answer_ET":"","timestamp":"2022-04-24 08:31:00","question_id":65,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0010500001.jpg"],"isMC":false,"answers_community":[],"discussion":[{"timestamp":"1690020900.0","content":"correct. \n\nReasons:\n1. {dat}/{time}/product.csv\n\nMore detailed things should be put at the last.\n\n2. YYYY-MM-DD\n\nif you choose YYYY/MM/DD, the system will think this is a file path.","upvote_count":"26","poster":"Dicer","comment_id":"635135"},{"upvote_count":"21","comment_id":"782833","timestamp":"1705792980.0","poster":"Rakrah","content":"second box is straight forwarded answer YYYY-MM-DD\nFirst Box = {date}/product.csv - Because the requirement is reference data loaded on daily basis, so it may be once in a day not hourly or timely."},{"comment_id":"998295","poster":"kkk5566","content":"correct","upvote_count":"1","timestamp":"1725435000.0"},{"comment_id":"782840","content":"The recommended way to refresh reference data is to:\nUse {date}/{time} in the path pattern. Box 1 should be {dat}/{time}/product.csv","timestamp":"1705793760.0","upvote_count":"2","poster":"Rakrah"},{"comment_id":"638788","timestamp":"1690560300.0","content":"given answer is correct","upvote_count":"3","poster":"Deeksha1234"},{"timestamp":"1690288020.0","content":"1. {date}/product.csv\n2. YYYY-MM-DD","upvote_count":"2","poster":"Franz58","comment_id":"636697"},{"timestamp":"1689473280.0","comment_id":"631976","poster":"temacc","content":"Path pattern: This required property is used to locate your blobs within the specified container. Within the path, you might choose to specify one or more instances of the variables {date} and {time}.\nExample 1: products/{date}/{time}/product-list.csv\nExample 2: products/{date}/product-list.csv\nExample 3: product-list.csv\n\nIf the blob doesn't exist in the specified path, the Stream Analytics job waits indefinitely for the blob to become available.\n\n#####\nDate format [optional]: If you used {date} within the path pattern you specified, select the date format in which your blobs are organized from the dropdown list of supported formats.\nExample: YYYY/MM/DD or MM/DD/YYYY\n\n##### \nTime format [optional]: If you used {time} within the path pattern you specified, select the time format in which your blobs are organized from the dropdown list of supported formats.\nExample: HH, HH/mm, or HH-mm\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data","upvote_count":"2"},{"comments":[{"upvote_count":"2","content":"The prints don't look consistent with each other, the 2nd image the location is: refdata/2020-03-20/product.csv should be \"YYYY-DD-MM\".\nBut the 1st image shows a different format: YYYY/MM/DD.","comments":[{"content":"first image only shows the default, you need to change them to the actual path shown in image 2","poster":"hypersam","timestamp":"1736029980.0","comment_id":"1336564","upvote_count":"1"}],"timestamp":"1722781440.0","poster":"JayOR","comment_id":"972242"},{"comment_id":"624311","comments":[{"upvote_count":"1","poster":"TriumphMC","timestamp":"1707922260.0","comment_id":"808479","content":"Yes it through me at first because the first graphic refers to the file contents date format. When they looking for directory naming format"}],"timestamp":"1687994220.0","poster":"Revave2","upvote_count":"5","content":"....and to answer my own question, the second exhibit shows the product.csv file in refdata/yyyy-mm-dd . So that'll be the path"}],"poster":"Revave2","content":"I think the {date}/product.csv is correct, however it's formatted as YYYY/MM/DD, so why hyphenate it? In the example provided in the link the date was formatted with - instead of /, but in the question it's all /....","comment_id":"624309","upvote_count":"3","timestamp":"1687994040.0"},{"comment_id":"609441","timestamp":"1685476980.0","upvote_count":"3","poster":"demirsamuel","content":"answers are correct"},{"upvote_count":"9","comment_id":"595970","content":"I should change box 2 to YYYY/MM/DD (as shows 1st exhibit). A bit confusing with time format in the box 1.","poster":"inotbf83","timestamp":"1683019080.0"},{"timestamp":"1682508900.0","content":"The file is updated daily, i think `{date}/product.csv` is correct","poster":"jackttt","comment_id":"592361","upvote_count":"5"},{"poster":"Lotusss","comments":[{"comment_id":"595180","poster":"KashRaynardMorse","content":"See that the file is stored under the date folder, and there is no time folder. \nYour link does recommend the time part, but the the link also says it's optional, and ultimately you need to answer the question, which states the path without the time.","upvote_count":"8","timestamp":"1682876100.0"}],"comment_id":"590910","upvote_count":"3","content":"Wrong! Path Pattern: {dat}/{time}/product.csv\nDat format: yyyy-mm-dd\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data","timestamp":"1682317860.0"}],"unix_timestamp":1650781860,"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/74291-exam-dp-203-topic-1-question-51-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0010200001.jpg","https://www.examtopics.com/assets/media/exam-media/04259/0010300001.png","https://www.examtopics.com/assets/media/exam-media/04259/0010400001.jpg"],"question_text":"HOTSPOT -\nYou are building an Azure Stream Analytics job that queries reference data from a product catalog file. The file is updated daily.\nThe reference data input details for the file are shown in the Input exhibit. (Click the Input tab.)\n//IMG//\n\nThe storage account container view is shown in the Refdata exhibit. (Click the Refdata tab.)\n//IMG//\n\nYou need to configure the Stream Analytics job to pick up the new reference data.\nWhat should you configure? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"Box 1: {date}/product.csv -\nIn the 2nd exhibit we see: Location: refdata / 2020-03-20\nNote: Path Pattern: This is a required property that is used to locate your blobs within the specified container. Within the path, you may choose to specify one or more instances of the following 2 variables:\n{date}, {time}\nExample 1: products/{date}/{time}/product-list.csv\nExample 2: products/{date}/product-list.csv\n\nExample 3: product-list.csv -\n\nBox 2: YYYY-MM-DD -\nNote: Date Format [optional]: If you have used {date} within the Path Pattern that you specified, then you can select the date format in which your blobs are organized from the drop-down of supported formats.\nExample: YYYY/MM/DD, MM/DD/YYYY, etc.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data","answer":""}],"exam":{"isImplemented":true,"lastUpdated":"12 Apr 2025","isMCOnly":false,"id":67,"isBeta":false,"provider":"Microsoft","numberOfQuestions":384,"name":"DP-203"},"currentPage":13},"__N_SSP":true}