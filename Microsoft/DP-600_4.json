{"pageProps":{"questions":[{"id":"ZLsw8YYgtMu2Ac5rk95J","question_text":"HOTSPOT\n-\n\nYou have a Fabric workspace that uses the default Spark starter pool and runtime version 1.2.\n\nYou plan to read a CSV file named Sales_raw.csv in a lakehouse, select columns, and save the data as a Delta table to the managed area of the lakehouse. Sales_raw.csv contains 12 columns.\n\nYou have the following code.\n\n//IMG//\n\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","question_id":16,"answer":"","question_images":["https://img.examtopics.com/dp-600/image129.png","https://img.examtopics.com/dp-600/image130.png"],"answers_community":[],"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/148872-exam-dp-600-topic-1-question-112-discussion/","isMC":false,"answer_images":["https://img.examtopics.com/dp-600/image131.png"],"timestamp":"2024-10-08 15:34:00","unix_timestamp":1728394440,"answer_description":"","answer_ET":"","exam_id":71,"discussion":[{"upvote_count":"25","comments":[{"content":"correct N,N,Y","poster":"Mhunity","upvote_count":"4","timestamp":"1734267720.0","comment_id":"1326863"}],"content":"No, No, Yes\nThe spark reads all columns first before selection , so spark.read , read all columns","comment_id":"1315977","timestamp":"1732217280.0","poster":"mghf61"},{"poster":"Mike5611","comment_id":"1294736","upvote_count":"10","content":"1. YES - The .select() function in the code specifies the exact columns ('SalesOrderNumber', 'OrderDate', 'CustomerName', and 'UnitPrice') to be selected. \n Therefore, only these columns will be read from the CSV.\n2. NO - The withColumn(\"Year\", year(\"OrderDate\")) function adds a new column called \"Year\" by extracting the year from the \"OrderDate\" column. \n However, it does not replace the \"OrderDate\" column—it only adds the new \"Year\" column.\n3. YES - The inferSchema='true' tells Spark to infer the data types of each column in the CSV, which requires an extra scan of the data to determine these types. \n This can indeed increase execution time.","timestamp":"1728394440.0"},{"poster":"nappi1","upvote_count":"5","comment_id":"1330831","content":"Yes, No, Yes is right","timestamp":"1734962940.0"}]},{"id":"vZn7kBuyNMX6Vdy0p54x","answers_community":["A (69%)","B (31%)"],"answer":"A","question_images":[],"answer_images":[],"unix_timestamp":1728200280,"answer_description":"","discussion":[{"poster":"Martin_Nbg","timestamp":"1728200280.0","comments":[{"upvote_count":"3","poster":"Gunstsings","timestamp":"1728810060.0","comment_id":"1296813","content":"DataFrame.Describe = Computes basic statistics for numeric and string columns, including count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns."}],"content":"I think A is correct https://learn.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.describe?view=spark-dotnet","upvote_count":"14","comment_id":"1293781"},{"upvote_count":"5","timestamp":"1728810120.0","content":"Selected Answer: A\nDataFrame.Describe = Computes basic statistics for numeric and string columns, including count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns.","comment_id":"1296815","poster":"Gunstsings"},{"content":"Selected Answer: A\nDescribe as other mentioned shows all fields asked for. if we were to use summary, we would have to specify what fields we want as shown here: https://learn.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.summary?view=spark-dotnet","poster":"jackjack1","upvote_count":"1","comment_id":"1341222","timestamp":"1736968860.0"},{"timestamp":"1734961320.0","poster":"nappi1","upvote_count":"1","comment_id":"1330824","content":"Selected Answer: A\ndf.describe().show() \nreturns\ncount, mean, stddev, min, max \nfor all the columns (numeric and non numeric)"},{"content":"Selected Answer: A\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html","poster":"2fe10ed","comment_id":"1326088","timestamp":"1734087780.0","upvote_count":"2"},{"poster":"maia01","timestamp":"1732963200.0","comment_id":"1320155","content":"Selected Answer: B\ndescribe: only numeric columns with limited summary\nsummary: numeric and non-numeric, broader summary","upvote_count":"4"},{"timestamp":"1729700340.0","poster":"patricck","comment_id":"1302116","upvote_count":"1","content":"There's a difference in numeric and string values. It's applicable for the numeric values but not for the string values and the question mentions both data"},{"upvote_count":"4","content":"A is correct, confirming after trying the command from Notebook. Displays count as well in addition to min, max, mean, and standard deviation.","timestamp":"1728810900.0","comment_id":"1296823","poster":"zeeneuser"},{"timestamp":"1728217920.0","poster":"moteruky","content":"It shows all stat for numeric values but shows only 3 stat for string(count,min and max, it doesnt account for mean and std)","comment_id":"1293834","upvote_count":"3"}],"exam_id":71,"url":"https://www.examtopics.com/discussions/microsoft/view/148730-exam-dp-600-topic-1-question-113-discussion/","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have a Fabric tenant that contains a new semantic model in OneLake.\n\nYou use a Fabric notebook to read the data into a Spark DataFrame.\n\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.\n\nSolution: You use the following PySpark expression:\n\ndf.describe().show()\n\nDoes this meet the goal?","timestamp":"2024-10-06 09:38:00","answer_ET":"A","question_id":17,"isMC":true,"topic":"1","choices":{"B":"No","A":"Yes"}},{"id":"c38fVuq3q208pawDXwzU","exam_id":71,"url":"https://www.examtopics.com/discussions/microsoft/view/148965-exam-dp-600-topic-1-question-114-discussion/","timestamp":"2024-10-10 09:10:00","unix_timestamp":1728544200,"question_id":18,"choices":{"A":"Switch","B":"KQL","C":"Append variable","D":"Lookup"},"answer":"D","topic":"1","answer_description":"","question_text":"You have a Fabric tenant.\n\nYou are creating a Fabric Data Factory pipeline.\n\nYou have a stored procedure that returns the number of active customers and their average sales for the current month.\n\nYou need to add an activity that will execute the stored procedure in a warehouse. The returned values must be available to the downstream activities of the pipeline.\n\nWhich type of activity should you add?","answer_ET":"D","question_images":[],"answer_images":[],"discussion":[{"comment_id":"1330818","poster":"nappi1","content":"Selected Answer: D\nLookup is correct","upvote_count":"1","timestamp":"1734960180.0"},{"poster":"2fe10ed","comment_id":"1326103","upvote_count":"2","content":"Selected Answer: D\nScript or Lookup activity\n\nhttps://learn.microsoft.com/en-us/fabric/data-factory/lookup-activity\n\nhttps://blog.fabric.microsoft.com/id-id/blog/automate-fabric-data-warehouse-queries-and-commands-with-data-factory?ft=All","timestamp":"1734090120.0"},{"timestamp":"1732287540.0","content":"Selected Answer: D\nto execute a stored procedure in a Fabric Data Factory pipeline and make the returned values available for downstream activities, you should use the Lookup activity.\nthe Lookup activity allows you to execute a query or stored procedure and retrieve the results, which can then be used in subsequent activities within the pipeline.","comment_id":"1316366","poster":"nappi1","upvote_count":"2"},{"upvote_count":"3","timestamp":"1728544200.0","poster":"Pegooli","comment_id":"1295454","content":"D is correct - A Lookup activity in Fabric Data Factory is specifically designed to execute a query or stored procedure and retrieve data from a data source."}],"answers_community":["D (100%)"],"isMC":true},{"id":"ixR2KXfg9UAlHBWvFJMK","answer_ET":"","answers_community":[],"answer":"","topic":"1","question_id":19,"unix_timestamp":1728200700,"exam_id":71,"answer_description":"","timestamp":"2024-10-06 09:45:00","isMC":false,"question_text":"HOTSPOT\n-\n\nYou have a Fabric tenant that contains a semantic model named model1. The two largest columns in model1 are shown in the following table.\n\n//IMG//\n\n\nYou need to optimize model1. The solution must meet the following requirements:\n\n• Reduce the model size.\n• Increase refresh performance when using Import mode.\n• Ensure that the datetime value for each sales transaction is available in the model.\n\nWhat should you do on each column? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/148731-exam-dp-600-topic-1-question-115-discussion/","discussion":[{"upvote_count":"12","content":"Correct: Remove + Split","poster":"Martin_Nbg","comment_id":"1293783","timestamp":"1728200700.0"},{"content":"TransactionKey --> Remove the column.\nThere is no need to keep 160 GB for a surrogate key, it can be removed and SaleDateTime can be moved into the SalesTransaction table that will for sure \"Increase refresh performance when using Import mode.\" and \"Reduce the model size.\" since there is no need for an heavy join like this one.\n\nSaleDateTime --> Split the column.\nSplitting the column would lead for sure to \"Ensure that the datetime value for each sales transaction is available in the model.\" and i think it would lead to a better segmentation of the data meaning less cardinality that would lead to \"Increase refresh performance when using Import mode.\" and \"Reduce the model size.\"","timestamp":"1732294440.0","upvote_count":"8","poster":"nappi1","comment_id":"1316406"}],"question_images":["https://img.examtopics.com/dp-600/image132.png","https://img.examtopics.com/dp-600/image133.png"],"answer_images":["https://img.examtopics.com/dp-600/image134.png"]},{"id":"wz8YBaggIr66zQCySaAI","isMC":false,"question_text":"DRAG DROP\n-\n\nYou have a Fabric tenant that contains a data warehouse named DW1. DW1 contains a table named DimCustomer. DimCustomer contains the fields shown in the following table.\n\n//IMG//\n\n\nYou need to identify duplicate email addresses in DimCustomer. The solution must return a maximum of 1,000 records.\n\nWhich four T-SQL statements should you run in sequence? To answer, move the appropriate statements from the list of statements to the answer area and arrange them in the correct order.\n\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/148732-exam-dp-600-topic-1-question-116-discussion/","unix_timestamp":1728200760,"answer_ET":"","exam_id":71,"timestamp":"2024-10-06 09:46:00","answers_community":[],"discussion":[{"timestamp":"1728544620.0","poster":"Pegooli","upvote_count":"10","comment_id":"1295461","content":"Answer is correct"},{"content":"Answer is correct","comment_id":"1293784","poster":"Martin_Nbg","upvote_count":"6","timestamp":"1728200760.0"}],"answer":"","answer_description":"","question_images":["https://img.examtopics.com/dp-600/image135.png","https://img.examtopics.com/dp-600/image136.png"],"answer_images":["https://img.examtopics.com/dp-600/image137.png"],"question_id":20,"topic":"1"}],"exam":{"isImplemented":true,"numberOfQuestions":179,"provider":"Microsoft","isBeta":false,"lastUpdated":"12 Apr 2025","id":71,"name":"DP-600","isMCOnly":false},"currentPage":4},"__N_SSP":true}