{"pageProps":{"questions":[{"id":"TjUSPqoAncWcyvMsEjrN","answer":"C","url":"https://www.examtopics.com/discussions/microsoft/view/154549-exam-dp-203-topic-4-question-45-discussion/","discussion":[{"timestamp":"1736938440.0","poster":"JamieMcD","upvote_count":"5","comment_id":"1340839","content":"Selected Answer: C\nTo create an action group (AG1) and configure an alert in Azure Data Factory (ADF1) to use AG1, you should create AG1 in the same resource group as ADF1. This ensures that the alert configuration and action group are within the same scope for easier management and integration."}],"isMC":true,"question_images":["https://img.examtopics.com/dp-203/image415.png"],"timestamp":"2025-01-15 11:54:00","exam_id":67,"choices":{"A":"RG1","B":"RG2","D":"RG4","C":"RG3"},"unix_timestamp":1736938440,"topic":"4","answer_images":[],"answer_description":"","answers_community":["C (100%)"],"question_text":"You have an Azure subscription that contains the resources shown in the following table.\n\n//IMG//\n\n\nDiagnostic logs from ADF1 are sent to LA1. ADF1 contains a pipeline named Pipeline1 that copies data from DB1 to Dw1.\n\nYou need to perform the following actions:\n\n• Create an action group named AG1.\n• Configure an alert in ADF1 to use AG1.\n\nIn which resource group should you create AG1?","question_id":341,"answer_ET":"C"},{"id":"jDcWCn8rwpykcSwOnouN","answer_description":"","discussion":[{"timestamp":"1683849240.0","comments":[{"upvote_count":"8","content":"so the provided answer is correct.","timestamp":"1684391160.0","comment_id":"900839","poster":"henryphchan"}],"content":"1. To identify trends in queue times, you should focus on the Pipeline activity run logs rather than the Pipeline run logs. \nPipeline activity run logs allows you to track the queue times for individual activities within the pipeline. \nWhile Pipeline run logs logs may provide some information about queue times, they do not provide granular details for each activity within the pipeline.","comment_id":"895492","poster":"vk8880","upvote_count":"11"},{"comment_id":"1276711","content":"The answers are correct. This documentation outlines all the options and describes them clearly. The provided solutions minimize administrative effort, as the goal of these logs is to facilitate further analysis. Sending them to Log Analytics offers a ready-to-use tool for this purpose. In contrast, other solutions store the data as files, such as JSON or Blobs, requiring additional steps to analyze, such as using copy activity in Data Factory to prepare them for further analysis.\n\nhttps://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log?tabs=powershell#send-to-log-analytics-workspace","timestamp":"1725282540.0","upvote_count":"1","poster":"renan_ineu"},{"timestamp":"1711887660.0","comment_id":"1186770","content":"collect pipeline activity runs --> the question asks such a granularity, so it must be included\nsend to log analytics workspace --> this minimized administrative effort in storing logs","comments":[{"timestamp":"1711888200.0","comment_id":"1186775","upvote_count":"2","poster":"MBRSDG","content":"there's another better reason for answer 1: we're interested in measuring how much time activities spend waiting in queue, metric that cannot be analyzed using simple pipelines runs. We must have details about each activity, hence the first, \"collect pipeline activity runs\", is correct."}],"poster":"MBRSDG","upvote_count":"3"},{"timestamp":"1703526180.0","upvote_count":"1","comments":[{"timestamp":"1707349260.0","poster":"axantroff","content":"when will they start deleting all comments marked “chatgpt”... constant aggregation of delirium","comment_id":"1143906","upvote_count":"8"}],"comment_id":"1105439","content":"Wrong activity runs dont minimize efforts, chatgpt :\nTo minimize administrative effort while still being able to identify trends in queue times across pipeline executions and activities, you should collect:\n\n- Pipeline runs log: This log provides a high-level overview of each pipeline execution, which is sufficient for identifying trends in queue times without the need for the more granular detail that would come from collecting activity runs logs.\n\nAnd send to:\n\n- Log Analytics workspace: This will allow for centralized logging and analytics, which is effective for trend analysis with minimal administrative effort. \n\nSo the settings should be:\n\nCollect: Pipeline runs log\nSend to: Log Analytics workspace","poster":"Momoanwar"},{"comments":[{"upvote_count":"1","content":"what about you i am preparing for now","timestamp":"1704218520.0","comment_id":"1112121","poster":"dakku987"}],"timestamp":"1697291520.0","content":"Did any of you completed DP-203 exam here","comment_id":"1043501","poster":"shreembreeze","upvote_count":"1"},{"timestamp":"1693545240.0","poster":"kkk5566","content":"correct","upvote_count":"2","comment_id":"995708"}],"answer_ET":"","question_id":342,"answer_images":["https://img.examtopics.com/dp-203/image307.png"],"exam_id":67,"timestamp":"2023-05-12 01:54:00","unix_timestamp":1683849240,"answers_community":[],"question_text":"HOTSPOT\n-\n\nYou have an Azure data factory named DF1 that contains 10 pipelines.\n\nThe pipelines are executed hourly by using a schedule trigger. All activities are executed on an Azure integration runtime.\n\nYou need to ensure that you can identify trends in queue times across the pipeline executions and activities The solution must minimize administrative effort.\n\nHow should you configure the Diagnostic settings for DF1? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","question_images":["https://img.examtopics.com/dp-203/image306.png"],"isMC":false,"topic":"4","url":"https://www.examtopics.com/discussions/microsoft/view/109009-exam-dp-203-topic-4-question-46-discussion/","answer":""},{"id":"UAJ49n1hdyHKawmUPuOr","choices":{"D":"Data IO percentage","C":"Data Warehouse Units (DWU) used","B":"Cache hit percentage","A":"DWU percentage"},"exam_id":67,"answer":"B","answers_community":["B (83%)","C (17%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/111527-exam-dp-203-topic-4-question-47-discussion/","unix_timestamp":1686221820,"topic":"4","timestamp":"2023-06-08 12:57:00","question_id":343,"answer_images":[],"answer_description":"","question_text":"You manage an enterprise data warehouse in Azure Synapse Analytics.\n\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\n\nYou need to monitor resource utilization to determine the source of the performance issues.\n\nWhich metric should you monitor?","question_images":[],"discussion":[{"timestamp":"1702040220.0","poster":"darshilparmar","upvote_count":"17","content":"Repeated 4 times","comment_id":"918158"},{"timestamp":"1731215100.0","upvote_count":"2","content":"Seems like this question is time travelling. saw it few pages back couple times already and here it is again. lol.","poster":"jjay86","comment_id":"1209183"},{"comment_id":"1129674","timestamp":"1721738220.0","upvote_count":"4","poster":"vernillen","content":"Selected Answer: B\nQuestion is being repeated multiple times, don't know why people select anything other than B. If it's something to do with \"Commonly used queries\" then it's 99% of the time cache"},{"upvote_count":"1","timestamp":"1721480880.0","content":"Selected Answer: C\nThe correct metric to monitor is C. Data Warehouse Units (DWU) used.\n\nDWUs are the units of resource consumption in Azure Synapse Analytics. Each DWU represents a certain amount of processing power, memory, and storage capacity. When users run queries, they consume DWUs.\n\nIn this case, users report slow performance when they run commonly used queries. This suggests that the performance issues are likely due to resource contention. If the commonly used queries are consuming a large amount of DWUs, then there may not be enough resources available for other queries to run efficiently.\n\nBy monitoring DWU usage, you can identify which queries are consuming the most resources and take steps to optimize them or to scale your Azure Synapse Analytics workspace.","comment_id":"1127344","poster":"Azure_2023"},{"comment_id":"1065763","content":"Correct Answer: C\n\nWhy not A ?\nWhile the DWU percentage can provide insights into whether your workload is more CPU or IO intensive, the DWU used can provide a more direct measure of the overall resource utilization1. This can be more helpful in identifying if resource contention (i.e., your workload is demanding more resources than are available) is causing the slow performance of commonly used queries\n\nR: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity","poster":"matiandal","timestamp":"1715178480.0","upvote_count":"1"},{"upvote_count":"4","comment_id":"1006342","timestamp":"1710323100.0","poster":"EliteAllen","content":"Selected Answer: B\nMonitoring DWU used (Option C) can certainly be part of a comprehensive approach to diagnosing the performance issues, focusing on the cache hit percentage (Option B) might offer a more targeted way to address the specific problem described in the scenario."},{"comment_id":"1003032","upvote_count":"2","content":"Selected Answer: B\nRepeated","timestamp":"1709979780.0","poster":"kkk5566"},{"comments":[{"timestamp":"1710489420.0","comment_id":"1008157","poster":"AzureSQLDatabase","content":"says the guy with the wrong answer xD\nI know you know the answer. I know you chose C by mistake which you don't know.\nAAAHHHH how do I know so much yet so little!!!","upvote_count":"2"}],"poster":"kkk5566","timestamp":"1709277240.0","content":"Selected Answer: C\nRepeated","upvote_count":"1","comment_id":"995710"}],"answer_ET":"B","isMC":true},{"id":"tHqU4OJKyR31zvoFhtLM","discussion":[{"comment_id":"1000579","upvote_count":"38","timestamp":"1694002440.0","content":"In the Azure Portal --> Add a role-based access control (RBAC) role to kv1\nIn Synapse Studio --> Create a linked service to kv1","poster":"hassexat","comments":[{"content":"Yep, that's how we do it at our company.","upvote_count":"3","poster":"vernillen","comment_id":"1129676","timestamp":"1706020740.0"}]},{"content":"I'm reporting this comment on purpose so the ExamTopics team can review the answer, as requested below.\n\nExamTopics team, please, do one of the following:\n1) clearly explain and provide the support link or material for the answer you chose, or\n2) correct the answers selected.\nAs stated in previous answers in this discussion, the logic says that first you have to add the role in RBAC to kv1 and then associate the kv1 source as an linked service in synapse.","comment_id":"1063727","upvote_count":"14","timestamp":"1699266300.0","poster":"y154707"},{"comment_id":"1276812","upvote_count":"1","poster":"renan_ineu","content":"Azure Portal: RBAC\n\"Authorization determines which operations the caller can perform. Authorization in Key Vault uses Azure role-based access control (Azure RBAC) on management plane and either Azure RBAC or Azure Key Vault access policies on data plane.\"\nSource: https://learn.microsoft.com/en-us/azure/key-vault/general/security-features\n\nSynapse: Linked Services\nRefer to the Steps number 3: \"Create a linked service pointing to your Azure Key Vault.\"\nSource: https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#steps","timestamp":"1725287520.0"},{"content":"The answer is incorrect. Should be:\n1. Add a role-based access control (RBAC) role to kv\n2. Create a linked service to kv1","timestamp":"1712776140.0","comment_id":"1193248","poster":"Alongi","upvote_count":"1"},{"content":"ExamTopics solution is inverted... you cannot set a linked service from portal, just a detail. \n\nCorrect answers: \nfrom portal --> RBAC on kv1\nfrom synapse studio --> linked service on kv1 (this works exactly as in DataFactory)","timestamp":"1711886520.0","comment_id":"1186754","poster":"MBRSDG","upvote_count":"2"},{"upvote_count":"2","poster":"ArdiShah","content":"Answer is not Correct.","comment_id":"1161749","timestamp":"1709134320.0"},{"timestamp":"1703526480.0","upvote_count":"3","content":"Chatgpt:\nTo enable Spark notebooks in Azure Synapse Analytics workspace `ws1` to retrieve secrets from Azure Key Vault `kv1` using the user-assigned managed identity `UAMI1`, you need to set the appropriate permissions and configurations:\n\nIn the Azure portal:\n- Add a role-based access control (RBAC) role to `kv1`. Assign the user-assigned managed identity `UAMI1` the necessary role, like \"Key Vault Secrets User\", to retrieve secrets from the Key Vault.\n\nIn Synapse Studio:\n- Create a linked service to `kv1`. This linked service should use the user-assigned managed identity `UAMI1` for authentication, allowing the Spark notebooks to use this linked service to access Key Vault secrets.\n\nSo the selections should be:\n\nIn the Azure portal: Add a role-based access control (RBAC) role to `kv1`.\nIn Synapse Studio: Create a linked service to `kv1`.","comment_id":"1105444","poster":"Momoanwar"},{"content":"The boxes should be reversed for the answers as it does not make sense currently","poster":"akshy","comment_id":"1000322","upvote_count":"2","timestamp":"1693983900.0"},{"poster":"kkk5566","comment_id":"995713","timestamp":"1693545420.0","upvote_count":"3","content":"Box1. Add a role-based access control (RBAC) role to kv\nBox2. Create a linked service to kv1"},{"timestamp":"1691254920.0","content":"In the Azure portal:\nAdd a role-based access control (RBAC) role to kv1 - You need to assign the 'Key Vault Secrets User' role to UAMI1 on kv1. This will grant the managed identity the necessary permissions to retrieve secrets from Key Vault.\n\nIn Synapse Studio:\nCreate a linked service to kv1 - You need to create a linked service in Azure Synapse Studio to connect to kv1. The linked service will use the User-Assigned Managed Identity (UAMI1) to authenticate to the Azure Key Vault.","poster":"_Lukas_","upvote_count":"4","comment_id":"973207"}],"answer_ET":"","topic":"4","timestamp":"2023-08-05 19:02:00","answer_images":["https://img.examtopics.com/dp-203/image341.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/117436-exam-dp-203-topic-4-question-48-discussion/","answers_community":[],"question_id":344,"answer_description":"","question_images":["https://img.examtopics.com/dp-203/image339.png","https://img.examtopics.com/dp-203/image340.png"],"question_text":"HOTSPOT\n-\n\nYou have an Azure subscription that contains the resources shown in the following table.\n\n//IMG//\n\n\nYou need to ensure that you can run Spark notebooks in ws1.The solution must ensure that you can retrieve secrets from kv1 by using UAMI1.\n\nWhat should you do? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer":"","exam_id":67,"unix_timestamp":1691254920,"isMC":false},{"id":"LcFm8VgT77hqiR8ht5r1","timestamp":"2023-08-14 19:35:00","question_id":345,"isMC":false,"answer_images":["https://img.examtopics.com/dp-203/image346.png"],"unix_timestamp":1692034500,"discussion":[{"upvote_count":"18","poster":"y154707","content":"ExamTopics team, please, do one of the following:\n1) clearly explain and provide the support link or material for the answer you chose, or\n2) correct the answers selected.\nAs stated in answers below in this discussion, the response are N-Y-N or N-N-N, but there's nothing that can point us to the answer given in the resolution.","timestamp":"1699266840.0","comment_id":"1063737"},{"upvote_count":"15","comment_id":"1002807","content":"No yes no","poster":"[Removed]","timestamp":"1694222940.0"},{"comment_id":"1283557","upvote_count":"2","poster":"renan_ineu","timestamp":"1726303920.0","content":"The common sense leans towards answers being either no/no/no or no/yes/no. Here’s my analysis:\n\nRetry Information: The panel only shows the general status, not specific retry counts. You can't confirm retry settings without checking the Details or activity settings. So, the answer is no if asked if retry is greater than 0.\n\nwaitOnCompletion Property: Comparing activity times suggests waitOnCompletion might be set in the second activity, not the first, which makes sense since the first activity can't have dependencies. Hence, the answer is no for the first activity.\n\nSkipped Activities: The first activity isn’t skipped due to dependency but may be skipped based on parameters or conditions like \"skip if copied less than 3 hours ago.\" Pipelines don’t have retries, only activities do. So, the answer remains no."},{"timestamp":"1713297300.0","poster":"Alongi","content":"No, no, no","comment_id":"1196788","upvote_count":"1"},{"timestamp":"1711886160.0","content":"no --> in the first run, pipline failed, so there's no retry \nyes --> it is a default option for nested pipelines; it doesn't seem to be changed here \nno --> in the second run, pipeline has been restarted from failed activities, causing the skip on the first activity, which succeeded in the last pipeline run","poster":"MBRSDG","upvote_count":"1","comment_id":"1186751"},{"poster":"Matt2000","comment_id":"980997","timestamp":"1692034500.0","comments":[{"poster":"dakku987","timestamp":"1704219180.0","comment_id":"1112126","upvote_count":"8","content":"i think waitOnCompletion is yes bcz only after 11 sec next activity get started if it was not set to true all first and second activity both will be started at same time"}],"content":"No, No, No\n\nThe Retry Property is not set to one for Web_GetIP: Otherwise, we would see a retry of that activity in the first run.\n\nwaitOnCompletion property is not set to true: In the second run, Exec_COPY_BLOB takes as long as in the first one, despite being skipped. So, it could not have been waiting for the pipeline that it had triggered to complete.\n\nExec_COPY_BLOB cannot be skipped due to a pipeline dependency since it is the first activity in the pipeline. Most likely, its activity state was manually set to ‚skipped‘.","upvote_count":"7"}],"question_text":"HOTSPOT\n-\n\nYou have an Azure Data Factory pipeline shown in the following exhibit.\n\n//IMG//\n\n\nThe execution log for the first pipeline run is shown in the following exhibit.\n\n//IMG//\n\n\nThe execution log for the second pipeline run is shown in the following exhibit.\n\n//IMG//\n\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_ET":"","exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/118108-exam-dp-203-topic-4-question-49-discussion/","question_images":["https://img.examtopics.com/dp-203/image342.png","https://img.examtopics.com/dp-203/image343.png","https://img.examtopics.com/dp-203/image344.png","https://img.examtopics.com/dp-203/image345.png"],"answers_community":[],"answer_description":"","topic":"4","answer":""}],"exam":{"name":"DP-203","numberOfQuestions":384,"provider":"Microsoft","id":67,"isMCOnly":false,"isBeta":false,"isImplemented":true,"lastUpdated":"12 Apr 2025"},"currentPage":69},"__N_SSP":true}