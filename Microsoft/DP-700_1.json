{"pageProps":{"questions":[{"id":"OUI8spkv9TSOKmmAmp2t","unix_timestamp":1734244380,"question_text":"Case Study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\n\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions.\nClicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\n\nOverview. Company Overview -\n\nContoso, Ltd. is an online retail company that wants to modernize its analytics platform by moving to Fabric. The company plans to begin using Fabric for marketing analytics.\n\n\nOverview. IT Structure -\n\nThe company’s IT department has a team of data analysts and a team of data engineers that use analytics systems.\n\nThe data engineers perform the ingestion, transformation, and loading of data. They prefer to use Python or SQL to transform the data.\n\nThe data analysts query data and create semantic models and reports. They are qualified to write queries in Power Query and T-SQL.\n\n\nExisting Environment. Fabric -\n\nContoso has an F64 capacity named Cap1. All Fabric users are allowed to create items.\nContoso has two workspaces named WorkspaceA and WorkspaceB that currently use Pro license mode.\n\nExisting Environment. Source Systems\n\nContoso has a point of sale (POS) system named POS1 that uses an instance of SQL Server on Azure Virtual Machines in the same Microsoft Entra tenant as Fabric. The host virtual machine is on a private virtual network that has public access blocked. POS1 contains all the sales transactions that were processed on the company’s website.\n\nThe company has a software as a service (SaaS) online marketing app named MAR1. MAR1 has seven entities. The entities contain data that relates to email open rates and interaction rates, as well as website interactions. The data can be exported from MAR1 by calling REST APIs. Each entity has a different endpoint.\n\nContoso has been using MAR1 for one year. Data from prior years is stored in Parquet files in an Amazon Simple Storage Service (Amazon S3) bucket. There are 12 files that range in size from 300 MB to 900 MB and relate to email interactions.\n\nExisting Environment. Product Data\n\nPOS1 contains a product list and related data. The data comes from the following three tables:\n\nProducts -\n\nProductCategories -\n\nProductSubcategories -\nIn the data, products are related to product subcategories, and subcategories are related to product categories.\n\nExisting Environment. Azure -\nContoso has a Microsoft Entra tenant that has the following mail-enabled security groups:\nDataAnalysts: Contains the data analysts\nDataEngineers: Contains the data engineers\n\nContoso has an Azure subscription.\n\nThe company has an existing Azure DevOps organization and creates a new project for repositories that relate to Fabric.\nExisting Environment. User Problems\nThe VP of marketing at Contoso requires analysis on the effectiveness of different types of email content. It typically takes a week to manually compile and analyze the data. Contoso wants to reduce the time to less than one day by using Fabric.\n\nThe data engineering team has successfully exported data from MAR1. The team experiences transient connectivity errors, which causes the data exports to fail.\n\n\nRequirements. Planned Changes -\n\nContoso plans to create the following two lakehouses:\nLakehouse1: Will store both raw and cleansed data from the sources\nLakehouse2: Will serve data in a dimensional model to users for analytical queries\nAdditional items will be added to facilitate data ingestion and transformation.\nContoso plans to use Azure Repos for source control in Fabric.\n\nRequirements. Technical Requirements\n\nThe new lakehouses must follow a medallion architecture by using the following three layers: bronze, silver, and gold. There will be extensive data cleansing required to populate the MAR1 data in the silver layer, including deduplication, the handling of missing values, and the standardizing of capitalization.\n\nEach layer must be fully populated before moving on to the next layer. If any step in populating the lakehouses fails, an email must be sent to the data engineers.\n\nData imports must run simultaneously, when possible.\nThe use of email data from the Amazon S3 bucket must meet the following requirements:\nMinimize egress costs associated with cross-cloud data access.\nPrevent saving a copy of the raw data in the lakehouses.\n\nItems that relate to data ingestion must meet the following requirements:\nThe items must be source controlled alongside other workspace items.\nIngested data must land in the bronze layer of Lakehouse1 in the Delta format.\n\nNo changes other than changes to the file formats must be implemented before the data lands in the bronze layer.\n\nDevelopment effort must be minimized and a built-in connection must be used to import the source data.\n\nIn the event of a connectivity error, the ingestion processes must attempt the connection again.\n\nLakehouses, data pipelines, and notebooks must be stored in WorkspaceA. Semantic models, reports, and dataflows must be stored in WorkspaceB.\nOnce a week, old files that are no longer referenced by a Delta table log must be removed.\n\nRequirements. Data Transformation\n\nIn the POS1 product data, ProductID values are unique. The product dimension in the gold layer must include only active products from product list. Active products are identified by an IsActive value of 1.\n\nSome product categories and subcategories are NOT assigned to any product. They are NOT analytically relevant and must be omitted from the product dimension in the gold layer.\n\n\nRequirements. Data Security -\n\nSecurity in Fabric must meet the following requirements:\nThe data engineers must have read and write access to all the lakehouses, including the underlying files.\n\nThe data analysts must only have read access to the Delta tables in the gold layer.\n\nThe data analysts must NOT have access to the data in the bronze and silver layers.\n\nThe data engineers must be able to commit changes to source control in WorkspaceA.\n\nYou need to ensure that the data analysts can access the gold layer lakehouse.\n\nWhat should you do?","answer_description":"","timestamp":"2024-12-15 07:33:00","question_images":[],"topic":"1","exam_id":72,"question_id":1,"url":"https://www.examtopics.com/discussions/microsoft/view/152988-exam-dp-700-topic-1-question-1-discussion/","answer_images":[],"isMC":true,"answers_community":["C (100%)"],"discussion":[{"upvote_count":"6","comments":[{"upvote_count":"1","comment_id":"1363848","poster":"oluonimole","content":"Hi QAZdbarhate12345678 I have 2 questions please\n1. Which lakehouse is Answer C referring to: Lakehouse1 or Lakehouse2?\n2. The case study overview says \"The data analysts must NOT have access to the data in the bronze and silver layers.\" If we share the lakehouse with the DataAnalysts group will this not give the data analysts access to the data in the bronze and silver layers? \n\nThank you","timestamp":"1740898440.0"}],"poster":"QAZdbarhate12345678","content":"Selected Answer: C\nBy granting Read all SQL Endpoint data permission, the analysts get the necessary and sufficient access to query the gold layer data while adhering to the principle of least privilege.","comment_id":"1326724","timestamp":"1734244380.0"},{"comment_id":"1401820","content":"Selected Answer: C\nBy granting Read all SQL Endpoint data permission, the analysts get the necessary and sufficient access to query the gold layer data while adhering to the principle of least privilege.\nhttps://docs.google.com/document/d/1oNH9i2ssNi9gISG3JGuxUoUhmAA4xCaGahEKv2dRN6Y/edit?usp=sharing","timestamp":"1742631180.0","poster":"Nityaanantha_Raman","upvote_count":"1"},{"comment_id":"1356481","content":"Selected Answer: C\nSharing the lakehouse with the DataAnalysts group and granting the Read all SQL Endpoint data permission would allow them to query the Delta tables in the gold layer using the SQL endpoint. This aligns with the requirement that they only have read access to the gold layer.","poster":"LasAnsias","comments":[{"upvote_count":"1","timestamp":"1740898320.0","poster":"oluonimole","comment_id":"1363847","content":"Hi LasAnsias. I have 2 questions please?\n1. Which lakehouse is Answer C referring to: Lakehouse1 or Lakehouse2?\n2. The case study overview says \"The data analysts must NOT have access to the data in the bronze and silver layers.\" If we share the lakehouse with the DataAnalysts group will this not give the data analysts access to the data in the bronze and silver layers? \n\nThank you"}],"upvote_count":"2","timestamp":"1739546880.0"},{"poster":"mixonfreddy","timestamp":"1735301280.0","content":"Selected Answer: C\nAnswer is C","comment_id":"1332384","upvote_count":"2"}],"choices":{"B":"Share the lakehouse with the DataAnalysts group and grant the Build reports on the default semantic model permission.","A":"Add the DataAnalyst group to the Viewer role for WorkspaceA.","C":"Share the lakehouse with the DataAnalysts group and grant the Read all SQL Endpoint data permission.","D":"Share the lakehouse with the DataAnalysts group and grant the Read all Apache Spark permission."},"answer_ET":"C","answer":"C"},{"id":"zTI9WxvnRQNfP4FRT4Vp","answer_description":"","question_images":[],"answer_ET":"D","isMC":true,"answers_community":["D (100%)"],"exam_id":72,"question_id":2,"timestamp":"2024-12-08 16:47:00","topic":"1","discussion":[{"upvote_count":"5","content":"Selected Answer: D\nTo implement Row-Level Security (RLS) in a Fabric warehouse like DW1, need to use a FUNCTION to define the filtering logic. Specifically, a user-defined function (UDF) is created and associated with the RLS policy to determine which rows each user can access.","comment_id":"1326749","timestamp":"1734249900.0","poster":"QAZdbarhate12345678"},{"timestamp":"1742918340.0","poster":"Adriel_1996","upvote_count":"1","content":"Selected Answer: D\n-- Creating schema for Security\nCREATE SCHEMA Security;\nGO\n\n-- Creating a function for the SalesRep evaluation\nCREATE FUNCTION Security.tvf_securitypredicate(@UserName AS varchar(50))\n RETURNS TABLE\nWITH SCHEMABINDING\nAS\n RETURN SELECT 1 AS tvf_securitypredicate_result\nWHERE @UserName = USER_NAME()\nOR USER_NAME() = 'BatchProcess@contoso.com';\nGO\n\n-- Using the function to create a Security Policy\nCREATE SECURITY POLICY YourSecurityPolicy\nADD FILTER PREDICATE Security.tvf_securitypredicate(UserName_column)\nON sampleschema.sampletable\nWITH (STATE = ON);\nGO","comment_id":"1410096"},{"comment_id":"1352472","upvote_count":"1","timestamp":"1738854780.0","content":"Selected Answer: D\nyou create a predicate function which gets evaluated to filter user access to certain rows in a Yes or No manner","poster":"prabhjot"},{"comment_id":"1323618","upvote_count":"2","poster":"Tuki93","timestamp":"1733672820.0","content":"Selected Answer: D\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/tutorial-row-level-security#2-define-security-policies"}],"url":"https://www.examtopics.com/discussions/microsoft/view/152702-exam-dp-700-topic-1-question-10-discussion/","question_text":"You have a Fabric warehouse named DW1. DW1 contains a table that stores sales data and is used by multiple sales representatives.\nYou plan to implement row-level security (RLS).\nYou need to ensure that the sales representatives can see only their respective data.\nWhich warehouse object do you require to implement RLS?","choices":{"A":"STORED PROCEDURE","C":"SCHEMA","B":"CONSTRAINT","D":"FUNCTION"},"answer":"D","answer_images":[],"unix_timestamp":1733672820},{"id":"5D6FtvysTJDpyk8AsKZp","timestamp":"2024-12-16 16:57:00","unix_timestamp":1734364620,"answer_ET":"","answer_description":"","answer_images":["https://img.examtopics.com/dp-700/image6.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/153078-exam-dp-700-topic-1-question-11-discussion/","isMC":false,"question_text":"HOTSPOT -\nYou have a Fabric workspace named Workspace1_DEV that contains the following items:\n10 reports\n\nFour notebooks -\n\nThree lakehouses -\n\nTwo data pipelines -\n\nTwo Dataflow Gen1 dataflows -\n\nThree Dataflow Gen2 dataflows -\nFive semantic models that each has a scheduled refresh policy\nYou create a deployment pipeline named Pipeline1 to move items from Workspace1_DEV to a new workspace named Workspace1_TEST.\nYou deploy all the items from Workspace1_DEV to Workspace1_TEST.\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\n\nNOTE: Each correct selection is worth one point.\n//IMG//","question_images":["https://img.examtopics.com/dp-700/image5.png"],"question_id":3,"answers_community":[],"answer":"","discussion":[{"comment_id":"1335389","comments":[{"comment_id":"1361290","poster":"ChenFu","content":"No\nNo\nYes","timestamp":"1740450180.0","upvote_count":"3"}],"poster":"SamuComqi","upvote_count":"19","content":"NO: only metadata will be deployed to the target starge (https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/understand-the-deployment-process?tabs=new#item-properties-copied-during-deployment)\n\nYES: Gen1 dataflows will be deployed\n\nNO: Refresh schedules will not be deployed (https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/understand-the-deployment-process?tabs=new#item-properties-copied-during-deployment)","timestamp":"1735803300.0"},{"poster":"fassil","content":"No/NO/NO\nItem properties that are not copied\nThe following item properties aren't copied during deployment:\nData - Data isn't copied. Only metadata is copied\nURL\nID\nPermissions - For a workspace or a specific item\nWorkspace settings - Each stage has its own workspace\nApp content and settings - To update your apps, see Update content to Power BI apps\nPersonal bookmarks\nThe following semantic model properties are also not copied during deployment:\nRole assignment\nRefresh schedule\nData source credentials\nQuery caching settings (can be inherited from the capacity)\nEndorsement settings\nIncremental refresh settings aren't copied in Gen 1.","timestamp":"1740662580.0","upvote_count":"1","comment_id":"1362536"},{"content":"Selected Answer: A\nYes/Yes/No","upvote_count":"1","timestamp":"1734364620.0","poster":"mixonfreddy","comment_id":"1327494","comments":[{"content":"No/Yes/No","timestamp":"1735502880.0","comments":[{"comments":[{"poster":"GHill1982","upvote_count":"7","content":"I believe Dataflow Gen1 is supported (aka Power BI Dataflows) , see https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/intro-to-deployment-pipelines?tabs=new#supported-items","comment_id":"1337438","comments":[{"content":"GHill is correct. Power BI dataflows are listed in the first link. And here it is called Dataflow Gen1: https://learn.microsoft.com/en-us/fabric/data-factory/dataflows-gen2-overview","timestamp":"1740983520.0","comment_id":"1364278","poster":"SannaLH","upvote_count":"1","comments":[{"timestamp":"1740983760.0","poster":"SannaLH","comment_id":"1364280","content":"No sorry, Semantic models are supported as long as they're not PUSH. https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/intro-to-deployment-pipelines?tabs=new-ui#supported-items","upvote_count":"1"},{"comment_id":"1364287","poster":"SannaLH","content":"NO sorry again. I misread. The first question is - is DATA from semantic model copied. It is not. GHill is correct after all: https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/understand-the-deployment-process?tabs=new-ui#assign-a-workspace-to-an-empty-stage","timestamp":"1740984120.0","upvote_count":"1"}]}],"timestamp":"1736225760.0"}],"comment_id":"1337188","upvote_count":"7","poster":"Marco44","timestamp":"1736175240.0","content":"No/No/No\n\nDeployment pipelines do not support Dataflow Gen1 dataflows. These need to be manually migrated or recreated in the target stage. Only Dataflow Gen2 is supported in deployment pipelines."}],"comment_id":"1333697","upvote_count":"3","poster":"GHill1982"}]}],"exam_id":72,"topic":"1"},{"id":"1ptSX8ravGZcmTB2mvHv","question_images":[],"answer_description":"","isMC":true,"answer_ET":"B","answers_community":["B (88%)","13%"],"exam_id":72,"question_id":4,"topic":"1","timestamp":"2024-12-08 16:48:00","discussion":[{"upvote_count":"5","comment_id":"1326750","content":"Selected Answer: B\nIn Microsoft Fabric, deployment pipelines are the recommended and built-in method for managing and deploying artifacts (like eventhouses, lakehouses, datasets, and reports) across environments such as Dev, Test, and Prod.","timestamp":"1734250380.0","poster":"QAZdbarhate12345678"},{"upvote_count":"1","comment_id":"1362233","poster":"hebertorosillo","content":"Selected Answer: C\nThis is a use case question. The company Contoso already has DevOps.for my is letter C.","timestamp":"1740593940.0"},{"poster":"Tuki93","upvote_count":"2","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/get-started-with-deployment-pipelines?tabs=from-fabric%2Cnew%2Cstage-settings-new\n\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/understand-the-deployment-process?tabs=new","timestamp":"1733672880.0","comment_id":"1323620"}],"url":"https://www.examtopics.com/discussions/microsoft/view/152704-exam-dp-700-topic-1-question-12-discussion/","question_text":"You have a Fabric deployment pipeline that uses three workspaces named Dev, Test, and Prod.\nYou need to deploy an eventhouse as part of the deployment process.\nWhat should you use to add the eventhouse to the deployment process?","answer":"B","choices":{"B":"a deployment pipeline","C":"an Azure DevOps pipeline","A":"GitHub Actions"},"answer_images":[],"unix_timestamp":1733672880},{"id":"F3dxIsyDsLQaNu2B1OGF","url":"https://www.examtopics.com/discussions/microsoft/view/152991-exam-dp-700-topic-1-question-13-discussion/","isMC":true,"answers_community":["B (85%)","Other"],"question_id":5,"answer":"B","answer_ET":"B","exam_id":72,"topic":"1","timestamp":"2024-12-15 09:44:00","question_text":"You have a Fabric workspace named Workspace1 that contains a warehouse named Warehouse1.\nYou plan to deploy Warehouse1 to a new workspace named Workspace2.\nAs part of the deployment process, you need to verify whether Warehouse1 contains invalid references. The solution must minimize development effort.\nWhat should you use?","question_images":[],"answer_images":[],"choices":{"C":"a Python script","D":"a T-SQL script","B":"a deployment pipeline","A":"a database project"},"discussion":[{"timestamp":"1734252240.0","upvote_count":"11","comments":[{"upvote_count":"4","timestamp":"1737798480.0","comments":[{"upvote_count":"3","comment_id":"1360005","content":"agreed with answer A. Here is the hint.\nhttps://community.fabric.microsoft.com/t5/Data-Engineering/Microsoft-fabric-WareHouse-migration-issue-via-deployment/td-p/4281976","poster":"toniteo","timestamp":"1740193680.0"}],"comment_id":"1346397","poster":"tomaszstaroszczyk","content":"What kind of references may a Warehouse have? We don't have shortcuts in a warehouse. It seems to be an indepentent item, there is no possiblity to break a reference to anything external. Therfor I think this relates to internal references, i.e. stored procedure operating on non-existing table. In this case a database project (answer A) would be necessary, but I don't find it related to a Deployment Pipeline in any way."}],"comment_id":"1326757","content":"Selected Answer: B\nMicrosoft Fabric's deployment pipelines provide a built-in mechanism to manage and validate the deployment of artifacts like warehouses. When you use a deployment pipeline to move Warehouse1 from one workspace (Workspace1) to another (Workspace2), the pipeline automatically checks for issues such as invalid references or missing dependencies during the deployment process.","poster":"QAZdbarhate12345678"},{"content":"Selected Answer: A\nA. a database project for minimal effor","timestamp":"1740598260.0","poster":"hebertorosillo","upvote_count":"1","comment_id":"1362256"},{"content":"Selected Answer: C\nQuestion is focusing on \"As part of the deployment process, you need to verify whether Warehouse1 contains invalid references\" for this python script can be written to identify invalid references and can add it as a notebook activity in pipeline. So answer is python script.","poster":"Anushareddysri","timestamp":"1738497240.0","comments":[{"timestamp":"1740193800.0","content":"Python script could be a lengthy solution, while the question asks for minimal effort.","upvote_count":"1","poster":"toniteo","comment_id":"1360006"}],"upvote_count":"1","comment_id":"1350390"}],"unix_timestamp":1734252240,"answer_description":""}],"exam":{"id":72,"isBeta":false,"isImplemented":true,"name":"DP-700","isMCOnly":false,"lastUpdated":"12 Apr 2025","numberOfQuestions":97,"provider":"Microsoft"},"currentPage":1},"__N_SSP":true}