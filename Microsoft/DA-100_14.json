{"pageProps":{"questions":[{"id":"DgCVRShZL6VgYQiV6Bj7","url":"https://www.examtopics.com/discussions/microsoft/view/34958-exam-da-100-topic-3-question-8-discussion/","topic":"3","answer_images":["https://www.examtopics.com/assets/media/exam-media/04207/0010600001.png"],"answer_ET":"","timestamp":"2020-10-21 15:07:00","answer_description":"Box 1: Total Sales -\n\nBox 2: Occupation -\n\nBox 3: City -\nYou can use Expand By to add fields you want to use for setting the level of the analysis without looking for new influencers.\nReference:\nhttps://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-influencers","question_images":["https://www.examtopics.com/assets/media/exam-media/04207/0010400001.jpg","https://www.examtopics.com/assets/media/exam-media/04207/0010500001.png"],"question_text":"HOTSPOT -\nYou have a table that contains the following three columns:\n✑ City\n✑ Total Sales\n✑ Occupation\nYou need to create a key influencers visualization as shown in the exhibit. (Click the Exhibit tab.)\n//IMG//\n\nHow should you configure the visualization? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answers_community":[],"answer":"","question_id":66,"discussion":[{"comment_id":"208825","comments":[{"upvote_count":"3","content":"correct!","comment_id":"352761","poster":"ashishashokan","timestamp":"1620527820.0"},{"timestamp":"1610395920.0","content":"You do not want City to act as Influencer and hence better to use it as Expand By","comment_id":"265013","poster":"Ab5381","comments":[{"timestamp":"1660906140.0","comment_id":"648904","content":"What does Expand By exactly do?","poster":"Lina90","upvote_count":"1"}],"upvote_count":"12"}],"timestamp":"1604000340.0","upvote_count":"94","poster":"Hermenez","content":"Total Sales is what we analyze,we explain by occupation and expand by city.\ncorrect"},{"timestamp":"1606628340.0","upvote_count":"49","comments":[{"poster":"Bagoo","comment_id":"437237","timestamp":"1630508760.0","upvote_count":"1","content":"we need to expand datasets, then regression works"}],"content":"Why do we need city here? I dont see any word mentioning the analysis requirement on city","poster":"jerryl","comment_id":"230099"},{"timestamp":"1653200280.0","upvote_count":"1","content":"Does it matter city first or occupation first?","poster":"fumen","comment_id":"605204"},{"content":"I got it on my exam 03/17/2022. 90-95% questions were from here. 61 questions in 100 min. All 3 case studies that appeared in exam, where from here.","comments":[{"comment_id":"569862","upvote_count":"2","content":"Answered: Analyse: Total Sales, Explain by : Occupation. Just two steps","timestamp":"1647531000.0","poster":"TechDiva"},{"timestamp":"1659326340.0","content":"ok stop commenting to each questions","upvote_count":"5","poster":"syairah86","comment_id":"640332"}],"timestamp":"1647530940.0","comment_id":"569860","poster":"TechDiva","upvote_count":"2"},{"content":"on exam 02/22/2022","upvote_count":"1","poster":"scrumbann","timestamp":"1645642740.0","comment_id":"554791"},{"comment_id":"498755","poster":"PatrickStr","content":"on exam 12/10/2021","upvote_count":"1","timestamp":"1639148700.0"},{"upvote_count":"1","content":"Total Sales is what we analyze,we explain by occupation and expand by city.\nI agree wit this answer because there is City is only one which makes sense. It looks like there is no empty space option. Given the previous answers , somebody can give a clear definition of a measure or summarized column, I think here is the key point.","poster":"ALJOHN","comment_id":"479940","timestamp":"1637144400.0"},{"comment_id":"462066","comments":[{"comment_id":"462940","poster":"Ihueghian","content":"Same 16/10","timestamp":"1634366880.0","upvote_count":"1"}],"timestamp":"1634217360.0","poster":"kcwood94","upvote_count":"2","content":"On exam 10/14/21"},{"comment_id":"450057","poster":"nowwin","upvote_count":"4","timestamp":"1632385860.0","content":"question in exam on 18th September"},{"comment_id":"443807","poster":"maquint","timestamp":"1631518620.0","upvote_count":"1","content":"https://youtu.be/2X1cW8oPtc8"},{"content":"https://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-influencers\nit makes sense to add city(expand by) for a more detailed analysis by geography.","poster":"UsefJuan","comment_id":"434765","timestamp":"1630257600.0","upvote_count":"1"},{"timestamp":"1629328560.0","poster":"jv2120","comment_id":"427111","upvote_count":"2","content":"Given the case only way to use Expand by City is when Total Sales is a measure, given question do not clarify it I will go by City only option left."},{"poster":"Syndhu","content":"can anyone please say whether expand by should be occupation or should be left blanked?","upvote_count":"1","comment_id":"381567","timestamp":"1623647700.0"},{"timestamp":"1621682400.0","content":"This question was in the exam.","poster":"fhqhfhqh","comments":[{"comment_id":"399939","poster":"andimohr","content":"So if you saw it in the exam - could you give us a hint if the expand by area was possible preselected or maybe they took it out? Because I have the impression it is not relevant for the visual that is described. It sounds like \"if it is mentioned in the question then use it someplace\"","timestamp":"1625573460.0","upvote_count":"1"},{"upvote_count":"1","content":"True i saw it","poster":"bpxgeek","comment_id":"388711","timestamp":"1624443240.0"}],"comment_id":"363637","upvote_count":"6"},{"comment_id":"311790","content":"Total Sales is what we analyze,we explain by occupation and expand by city.\nYou can use Expand By to add fields you want to use for setting the level of the analysis without looking for new influencers.\nhttps://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-influencers","upvote_count":"5","timestamp":"1615845120.0","poster":"EMNAB"},{"comments":[{"content":"Here we want to analyze Total Sales and it's a measure (should be), so we need Expand By. But I don't know why City.","poster":"Cherishworth","comment_id":"336299","timestamp":"1618491900.0","upvote_count":"1"}],"comment_id":"303473","timestamp":"1614876000.0","poster":"VM_GCP","content":"Metric you want to investigate into the Analyze field: Here we want to check what drives Total Sale\n\nExplain by field : Move fields that you think might influence Rating into the . We want to check how Occupation drive Total Sale\n\nExpand by: This field is only used when analyzing a measure or summarized field. Is City is measure or summarized field? No. So it should be empty\n\nref: https://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-influencers#analyze-a-metric-that-is-categorical","upvote_count":"5"},{"comment_id":"296483","timestamp":"1613985900.0","comments":[{"content":"As it analyzed the Average of the sales, it should be measure or summarized. \nRef: https://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-influencers#analyze-a-metric-that-is-a-measure-or-a-summarized-column","poster":"manlynn","upvote_count":"1","comment_id":"328371","timestamp":"1617597600.0"}],"content":"I also saw that after test\n\n in test when you want to add a field to \"expanded by \" , Power BI say the field added in the \" Analyze box\" (Total Sales here) is not measure or not summarized. but when you summarize the field you can added after a field in the \"expand box\"\n------------------------------------------------------\nnot sur about \"city\" in the \"expand box\"","poster":"kalyhot","upvote_count":"5"},{"upvote_count":"1","poster":"Narad","content":"Isnt the expand by used only when a measure is being analyzed. In this case, its not mentioned that total sales is a measure. Expand by - what is the correct answer?","comment_id":"272338","timestamp":"1611181200.0"},{"comment_id":"266731","content":"City is one of the columns, Occupation is the Influencer, hence Expand By city","poster":"rhylos","upvote_count":"2","timestamp":"1610599080.0"},{"timestamp":"1609365060.0","content":"correct answer plz...","upvote_count":"4","poster":"chaki","comment_id":"255927"},{"upvote_count":"10","content":"Total Sales, Occupation, Occupation, in the screenshot, the data is expanded by occupation and never shows city","comment_id":"239316","poster":"JACAR","timestamp":"1607527080.0"},{"content":"Why Expand by City? It doesn't refer to City on the screen shot","upvote_count":"7","timestamp":"1606923960.0","comments":[{"comment_id":"289146","content":"you have three tables ✑ City\n✑ Total Sales\n✑ Occupation , so the question is how to analyze the sales trend explained by occupation , and when we say occupation we talk about geographic points , so it should be expanded by city","comments":[{"comments":[{"upvote_count":"1","timestamp":"1623995460.0","comment_id":"384598","content":"The question say:\nYou have a table that contains the following three columns:\n✑ City\n✑ Total Sales\n✑ Occupation\nYou need to create a key influencers visualization as shown in the exhibit. (Click the Exhibit tab.)","poster":"MonBouj"}],"timestamp":"1621399440.0","poster":"nejc","content":"you have one table with 3 columns, please don't comment if you have no idea what you are doing","comment_id":"361003","upvote_count":"5"},{"content":"i dont see a relationship between City and Occupation in the answer you mentioned. i mean, why would the occupation be city related?? nothing in the screenshot says so","upvote_count":"1","timestamp":"1618143120.0","comment_id":"333275","poster":"RehafKH"}],"timestamp":"1613162640.0","upvote_count":"2","poster":"bechir1"}],"poster":"Thibault6974","comment_id":"233136"}],"unix_timestamp":1603285620,"exam_id":63,"isMC":false},{"id":"Mgcr40jAgHlz3k5slxZB","timestamp":"2020-10-29 20:41:00","answer_ET":"A","answers_community":["A (100%)"],"answer_description":"Power BI Top N Filters are useful to display the top performing records, and Bottom N filters are helpful to display the least performing records. For example, we can display top or bottom 10 products by orders or sales.\nNote:\n1. Select the Column you want to display the Top Sales Profit\n2. Then change the Filter Type of that Column to Top N\n3. Fill in Top / Bottom number field\n4. And lastly drag to the By Value filed your Sales Profit\nIncorrect Answers:\nB: You would need a filter as well.\nReference:\nhttps://www.tutorialgateway.org/power-bi-top-10-filters/","question_images":["https://www.examtopics.com/assets/media/exam-media/04207/0010700001.png"],"discussion":[{"poster":"Hermenez","comment_id":"208827","upvote_count":"72","comments":[{"upvote_count":"1","content":"Agreed","comment_id":"571380","timestamp":"1647747600.0","poster":"MohsenY"}],"content":"The question is very specific to the visualization so it is Top N","timestamp":"1604000460.0"},{"upvote_count":"17","comment_id":"330914","timestamp":"1617859680.0","content":"The Answer is A","poster":"sandra1981"},{"timestamp":"1662959880.0","upvote_count":"1","content":"Selected Answer: A\nThe topn filter is the easiest solution","poster":"halfway","comment_id":"666602"},{"comment_id":"609960","poster":"Angell","upvote_count":"3","content":"I just tested and in order to filter the visual by a Top N filter on City field, you must add a field on a box the filter has. I added the sales field and then clicked on Apply filter (this button is grayout until you add the value field) and Woalah, It filtered by Top 10 Cities with highest amt of sales. So I think A is the most reasonable answer for this question.","timestamp":"1654045860.0"},{"comment_id":"558931","upvote_count":"1","poster":"Ashley090521","content":"on exam 3/1/2022","timestamp":"1646159640.0"},{"poster":"Zee10","timestamp":"1636984620.0","content":"There should be mentioned in with DESC order also","upvote_count":"2","comment_id":"478706"},{"comment_id":"425147","comments":[{"comments":[{"content":"I had it in my exam too 16/10, I selected TOP N","comment_id":"462941","timestamp":"1634366940.0","poster":"Ihueghian","upvote_count":"1"}],"poster":"PavanGM80","upvote_count":"5","timestamp":"1629412200.0","comment_id":"427754","content":"its a trap or what, you mentioned this comment for almost all questiosn, everytime it says same comment :-)"}],"timestamp":"1629016080.0","upvote_count":"3","content":"Got this in the exam - Aug 15, 2021.","poster":"francis6170"},{"timestamp":"1627914540.0","comment_id":"418767","upvote_count":"1","content":"It says data is in table as it is not transformation needed so answer is \"A\"","poster":"ali0912"},{"poster":"tramynt","timestamp":"1627221780.0","content":"Tested. Add filter TopN worked perfectly --> A","upvote_count":"1","comment_id":"413980"},{"content":"The answer is A","upvote_count":"4","comment_id":"368444","poster":"PenniWize","timestamp":"1622169240.0","comments":[{"poster":"Praveenkumar0017","content":"How many questions came from this dump","comment_id":"372917","timestamp":"1622655240.0","upvote_count":"1"}]},{"content":"This question was in the exam.","poster":"fhqhfhqh","comment_id":"363638","comments":[{"content":"Hi..I am planning to take exam this week.Could you please let me know if it is possible to pass the exam by referring to the dumps in this site only. I have 2 years experience in Power BI.","upvote_count":"1","poster":"AnnaAbru","comments":[{"content":"I have my exam on Tuesday and started with the MS learning paths on Saturday (learning paths are quite easy if you have experience, exam questions can be tricky) (I am taking 1 day learning paths and 1 day reviewing questions, plus after working hours/in between on monday some revision)","poster":"hendriktytgatpwc","timestamp":"1625384280.0","comment_id":"398104","upvote_count":"2"}],"comment_id":"386918","timestamp":"1624266840.0"}],"timestamp":"1621682400.0","upvote_count":"5"},{"comment_id":"301002","content":"A is the correct answer .","timestamp":"1614549480.0","poster":"JESUSBB","upvote_count":"4"},{"upvote_count":"2","poster":"fergusgarden","timestamp":"1605964440.0","comment_id":"224350","content":"the correct answer is A"},{"upvote_count":"2","poster":"F_Bastiat","timestamp":"1604324040.0","comment_id":"211244","content":"A measure configured in such a way as to return BLANK() for ranks higher than 10 would achieve the same result, but A is an easier approach, and maybe you will need the original measure for other visuals as well."}],"topic":"3","isMC":true,"choices":{"C":"Add a calculated column to the table that uses the TOPN function. In the visual, replace Sales Profit with the calculated column.","A":"Add a Top N filter to the visual.","D":"Add a calculated column to the table that returns the city name if the city is in the top 10, otherwise the calculated column will return \"Not in Top 10\". In the visual, replace Sales Profit with the calculated column.","B":"Configure the Sales Profit measure to use the RANKX function."},"answer":"A","question_id":67,"unix_timestamp":1604000460,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/35449-exam-da-100-topic-3-question-9-discussion/","exam_id":63,"question_text":"You have the dataset shown in the following exhibit.\n//IMG//\n\nYou need to ensure that the visual shows only the 10 cities that have the highest sales profit.\nWhat should you do?"},{"id":"7Zy77BEQSCvzZv7pn6eZ","answer_images":["https://www.examtopics.com/assets/media/exam-media/04207/0013300002.png","https://www.examtopics.com/assets/media/exam-media/04207/0013400001.jpg"],"timestamp":"2020-10-04 11:02:00","topic":"4","question_text":"DRAG DROP -\nYou have the line chart shown in the exhibit. (Click the Exhibit tab.)\n//IMG//\n\nYou need to modify the chart to meet the following requirements:\n✑ Identify months that have order counts above the mean.\n✑ Display the mean monthly order count.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/33580-exam-da-100-topic-4-question-1-discussion/","answers_community":[],"unix_timestamp":1601802120,"answer":"","discussion":[{"upvote_count":"342","comment_id":"194584","content":"My Ans:-\n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label","comments":[{"timestamp":"1625886840.0","upvote_count":"7","content":"9 months later, this still holds true\n\n(tested it locally)","comment_id":"403069","poster":"berserkguts"},{"comment_id":"219764","poster":"AntoanT","content":"Mean = Average. Mean is NOT Median ;)","upvote_count":"15","timestamp":"1605453900.0"},{"timestamp":"1619892180.0","content":"correct","poster":"Zakriya","comment_id":"347164","upvote_count":"1"},{"content":"Does the aggregation has anything to do with this? Doing the average line over that line wouldn't take the daily average and not the monthly? I assume doing the 12 moths rolling means you take the monthly value as source for the new monthly average line. Or is it the same?","upvote_count":"1","comment_id":"461448","poster":"Geyper","timestamp":"1634114760.0"},{"upvote_count":"15","content":"The option available is \"Turn on Data Label for the NEW LINE\". The new line here is the average line which will ideally just give ONE value - the average/mean value (https://docs.microsoft.com/en-us/power-bi/transform-model/desktop-analytics-pane)\nThe next closest answer is to create the rolling measure and add to display - even the question doesn't demand the rolling figure.\n\n1. Create a measure for 12 months moving average and add the measure to the lne chart value. (For the req \"Display the mean monthly order count\")\n2. Select the line chart.\n3. From the Analytics pane, add an average line. (For the req \"Identify months that have order counts above the mean\")","comments":[{"content":"Aggregation I am afraid, is not needed here. Identifying months that have order counts above the mean wont be achieved by a rolling measure..","upvote_count":"3","comment_id":"486416","poster":"vcyc","timestamp":"1637812140.0"},{"comments":[{"comment_id":"398132","upvote_count":"1","timestamp":"1625386860.0","poster":"hendriktytgatpwc","content":"indeed Titi is right (it demands the mean to be shown in the graph)"}],"poster":"Joe212","upvote_count":"4","comment_id":"386589","timestamp":"1624224960.0","content":"One of the goals is to display the mean value and turning on data label on the new line achieves this. Titi08 is right."},{"comment_id":"505520","timestamp":"1640014560.0","comments":[{"poster":"Billybob0604","comment_id":"571539","timestamp":"1647772440.0","content":"Yes, but the visual does not display the mean count per month, so you need the measure for 12 months like bebokhanna states","upvote_count":"1"}],"content":"The line already exists in the visual so adding a new measure '12 months...' is unnecessary.","upvote_count":"4","poster":"mirzotti"}],"comment_id":"367300","poster":"bebokhanna","timestamp":"1622045940.0"},{"upvote_count":"3","content":"average isn't asked, mean is.","comment_id":"195765","comments":[{"poster":"anshi0510","upvote_count":"1","timestamp":"1636461120.0","content":"its basic stats bro. are u an analyst?","comment_id":"474789"},{"content":"mean is average isn't it? https://blog.usejournal.com/average-vs-mean-534b1ac85401?gi=1952bb660b61","upvote_count":"2","poster":"nayarm","comment_id":"196411","timestamp":"1602210300.0","comments":[{"timestamp":"1602267840.0","comment_id":"196934","upvote_count":"8","content":"Yes, Mean is the Average. Basic knowledge and statistics and arithmetics!\nDouble check the article you posted: \"The central value which is called as average in mathematics is called as mean in statistics. Both are synonyms.\"\nArithmetic mean = Average. We don't talk about geometric mean or Harmonic mean! The question refers to the ARTHIMETIC MEAN that indeed is same thing as AVERAGE.\nHopefully this clears up any confusion.","poster":"pinkbadger"},{"timestamp":"1602268320.0","upvote_count":"3","content":"http://mathcentral.uregina.ca/qq/database/qq.09.00/julie1.html\nThis article should clarify even further your confusion :)","poster":"pinkbadger","comment_id":"196945"}]},{"timestamp":"1602208260.0","poster":"pinkbadger","comment_id":"196401","upvote_count":"11","content":"Mean is the Average\nhttps://www.purplemath.com/modules/meanmode.htm"},{"content":"good luck to the company that hired you as their analyst","poster":"berserkguts","comments":[{"comment_id":"643036","content":"hahahaha","poster":"Nitin1209","timestamp":"1659716340.0","upvote_count":"2"},{"upvote_count":"1","poster":"Longest5","timestamp":"1659809160.0","content":"This is just funny.","comment_id":"643470"},{"comment_id":"472877","upvote_count":"5","timestamp":"1636088640.0","poster":"Bibina","content":"I almost laughed to death"},{"poster":"gyeah","content":"I'm dieing loooooooool so funny","upvote_count":"1","comment_id":"649564","timestamp":"1661044740.0"}],"timestamp":"1625886780.0","upvote_count":"15","comment_id":"403067"},{"poster":"ismailqtr","comment_id":"653808","timestamp":"1661833560.0","content":"This is why you need to study first than do practice tests for all exam subjects.. take it as a life tip.","upvote_count":"2"}],"poster":"Ard","timestamp":"1602136020.0"}],"timestamp":"1602011820.0","poster":"titi08"},{"upvote_count":"35","timestamp":"1601802120.0","poster":"VR1","content":"My Ans:-\n1. Select the line chart\n2. Add the Median line\n3. Turn on Data Label ( Display the mean monthly order count)","comments":[{"upvote_count":"37","timestamp":"1607712600.0","comment_id":"241137","poster":"Lhouss","content":"Be careful, Median is not the mean/average"},{"comment_id":"347043","upvote_count":"4","poster":"pmkamleshpartin","content":"Median not mean","timestamp":"1619877900.0"},{"comment_id":"403010","content":"how is this highly voted","upvote_count":"16","timestamp":"1625876460.0","poster":"MaQya"},{"timestamp":"1638862680.0","comment_id":"495739","upvote_count":"3","content":"median not equal to mean","poster":"YTYTYTYTYT"},{"upvote_count":"4","content":"Median is incorrect","comment_id":"486415","timestamp":"1637811900.0","poster":"vcyc"},{"comment_id":"358678","upvote_count":"13","timestamp":"1621166700.0","poster":"jesusdax","content":"Median IS NOT mean. Average equals Mean. Mathematically: Average = Sigma(Values, n) / n; while Median is the 50th (50%) percentile. Therefore. AVG <> Median."},{"content":"how can this be highly voted???? Sheesh.","poster":"berserkguts","comment_id":"400490","upvote_count":"9","timestamp":"1625633280.0"}],"comment_id":"192793"},{"poster":"emp207","upvote_count":"1","content":"WHY select Line Chart its already line chart. Add average line for 1st requirement. 12month roll avg and add labels for new line for 2nd requirement. THATS IT! khaby lame style.","timestamp":"1660542840.0","comment_id":"647049"},{"timestamp":"1659673140.0","comment_id":"642716","upvote_count":"1","content":"1. Select the line chart\n2. Add average line\n3. Turn on Data labels","poster":"QWERTYman"},{"poster":"anbianci","timestamp":"1659011640.0","content":"yes confirm\n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label","upvote_count":"3","comment_id":"638704"},{"timestamp":"1654818000.0","comment_id":"614253","upvote_count":"3","poster":"anurag49","content":"The right order of answere would be:\n1.Create 12 month moving average\n2.Turn on the data lable for new line.\n3. From the analytics pane ,add an average line."},{"comment_id":"608330","poster":"Mad0701","upvote_count":"2","content":"Create a measure for 12 months moving average.\nSelect the line chart \n From the Analytics pane, add a Median line","timestamp":"1653729060.0"},{"upvote_count":"1","content":"In exam 30/12/21","timestamp":"1640811300.0","comment_id":"512720","poster":"Shan"},{"comment_id":"496076","content":"What’s the correct answer?","timestamp":"1638886200.0","upvote_count":"1","poster":"sofiaapedro"},{"upvote_count":"1","timestamp":"1638789900.0","comment_id":"495085","poster":"adbukavu","content":"A measure will give a single value? \nI think there is typo it should have been turn on data labels for the line (not the new line)"},{"timestamp":"1637642040.0","upvote_count":"2","poster":"toaldoe","comment_id":"484742","content":"My Ans:-\n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label"},{"comment_id":"414893","upvote_count":"9","poster":"Prathameshsable","content":"Correct Ans is Create 12 month rolling avg, select line chart, display data labels. There is one important point that we forget here, the question asks to display monthly across every month on the line chart. Adding a average line will just display average line data label and not monthly, so that option wont work.","timestamp":"1627326720.0"},{"upvote_count":"4","timestamp":"1626349620.0","poster":"patricepic","comment_id":"407035","content":"Requirement is \"Display Monthly Mean\" . Mean equals Average but Monthly Average does not equal Rolling Monthly Average."},{"comment_id":"399190","poster":"SillyChili","upvote_count":"1","timestamp":"1625490600.0","content":"what does the \"Select the line chart\" even mean or do? Am I the only one who don't get the answer?","comments":[{"comment_id":"430767","poster":"Vulkany","content":"If you don´t select the Line Chart visual you do not have Analytics options available","upvote_count":"2","timestamp":"1629810060.0"}]},{"timestamp":"1625171880.0","upvote_count":"1","poster":"JoJoJa","comment_id":"396330","content":"So what is the answer if this comes in the exam? 1. Select 2. Average line 3. Data label?"},{"content":"When testing, if first creating and adding 12 months rolling average the chart is already selected, then add the average line and turn data labels on for the line. But if the option was given I'd add the data labels for the chart at first, then the avg line and data labels for it","upvote_count":"1","timestamp":"1624631040.0","comment_id":"390552","poster":"BilJon"},{"timestamp":"1624001040.0","comment_id":"384645","upvote_count":"1","poster":"Daneas","content":"select\ncreate\nturn on label one the new line"},{"upvote_count":"1","timestamp":"1623826020.0","comment_id":"383160","content":"I concur 2. Add the average line.\nIn layman term, Average means Arithmetic Mean, just like the function in Excel.\nIn mathematics / statistics term, Average can be a lot of things, for example, Arithmetic Mean, Geometric Mean, Median, Mode, ...\n\nHence, in statisitcs, we often tell the 'average' more clearly.","poster":"Alivina"},{"timestamp":"1623587280.0","content":"Answer should be \n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label\n\nDon't need step: Create a measure for 12 months moving average and add the measure to the line chart value.","poster":"Canary_2021","upvote_count":"5","comment_id":"381102"},{"comment_id":"373892","upvote_count":"1","content":"Wouldn't the answer be \n1)create the quick measure\n2) select the line chart\n3) turn on data labels for the new line\n\nDirection say display the mean monthly order count","poster":"non20","timestamp":"1622753640.0"},{"poster":"teaparty","comment_id":"352327","upvote_count":"1","content":"If the last option is the average line,Correct!","timestamp":"1620442800.0"},{"content":"I think it should be add average line, Create 12 month rolling average, Turn on data labels. That will cover the 2 requirements.","upvote_count":"4","poster":"Huzak","timestamp":"1617913200.0","comment_id":"331538"},{"poster":"Vikram1840","comment_id":"317023","timestamp":"1616406180.0","content":"Question divided in two part-\n1. Select month having order count more than avg-- \nAdd avg line from analytics pane.\n2. to display monthly avg-- \nCreate a 12 month rolling avg and add in line chart\nEnable data label","upvote_count":"4"},{"comment_id":"310835","content":"For me this is the correct answer \n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label","timestamp":"1615752540.0","upvote_count":"4","poster":"Silia"},{"poster":"ooodrey7","comment_id":"284983","upvote_count":"4","content":"1. Create a 12-mth rolling average \n2. Select the line chart \n3. Turn on data label","timestamp":"1612631040.0"},{"timestamp":"1608708720.0","content":"the question is weird. some people turn data labels on before then slice and dice\nnever the list, select the line chart, add median line or turn on data labels","upvote_count":"3","poster":"P_S09","comment_id":"250716"},{"upvote_count":"4","content":"it should be:\nMy Ans:-\n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label","poster":"hrahal","comment_id":"249541","timestamp":"1608573060.0"},{"upvote_count":"3","content":"The right answer : \n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label","poster":"MBA_1990","comment_id":"244980","timestamp":"1608068760.0"},{"poster":"Lhouss","comment_id":"224332","content":"Right answer is (Agree with titi08) :\n1. Select the line chart\n2. Add the AVERAGE line\n3. Turn on Data Label","timestamp":"1605962340.0","upvote_count":"4"},{"content":"Why right answer is \n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label, but not answer in exams","timestamp":"1605774600.0","upvote_count":"5","poster":"Makar","comment_id":"222589"},{"content":"the median is not always the mean. I suggest that a 12 month rolling average has to be calculated.","poster":"Laredo","comments":[{"timestamp":"1604324880.0","content":"The question does not say anything about the mean being rolling, it implies a simple average value per dataset. A rolling average means that each value will be compared against a distinct average value","comment_id":"211255","poster":"F_Bastiat","upvote_count":"5"}],"timestamp":"1602808920.0","upvote_count":"2","comment_id":"200809"},{"poster":"pg13","comment_id":"200026","timestamp":"1602699540.0","upvote_count":"17","content":"My Ans:-\n1. Select the line chart\n2. Add the average line\n3. Turn on Data Label"}],"question_id":68,"question_images":["https://www.examtopics.com/assets/media/exam-media/04207/0013200001.png","https://www.examtopics.com/assets/media/exam-media/04207/0013300001.png"],"isMC":false,"answer_ET":"","answer_description":"Step 1: Create a 12-month...\nYou can use calculated measure to get the expected result.\n1. Create a calculated column for the date.\n2. Create a measure for 12 months moving average.\n3. Drag the Line Chart into your canvas as below. (step 2 below)\n\n\nStep 2: Select the line chart -\nStep 3: From the Analytics pane, add a Median line\nReference:\nhttps://community.powerbi.com/t5/Desktop/Moving-Average/td-p/43041","exam_id":63},{"id":"V7kcBZ6KQjbc4PAHrKTY","isMC":false,"question_id":69,"answers_community":[],"answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04207/0013500001.jpg","https://www.examtopics.com/assets/media/exam-media/04207/0013600001.png"],"timestamp":"2020-10-02 16:00:00","answer":"","exam_id":63,"answer_description":"","topic":"4","discussion":[{"content":"I think:-\n1. Duplicate the query\n2. Group the query to find the max modified date\n3. Merge the query on inner join","upvote_count":"252","timestamp":"1601801220.0","poster":"VR1","comment_id":"192783","comments":[{"content":"Be careful, cos Person X can have repeated modified date. So a inner join will remain the duplicate (wrongly)...\nThat’s why the “remove duplicated ID” is present.\nIn other words, Inner join will return duplicate if the same customer ID have the same modified date 2 or more times","comment_id":"341211","timestamp":"1619114580.0","comments":[{"timestamp":"1647798540.0","content":"No, modified date is datetime format. using the max you will always get a unique customer id","upvote_count":"2","comment_id":"571772","poster":"Billybob0604"},{"timestamp":"1620997440.0","content":"if you group by ID you doesn't have duplicate id anymore","poster":"Massy","upvote_count":"16","comment_id":"357215"},{"timestamp":"1667514900.0","comment_id":"710818","upvote_count":"1","content":"Maue is correct that duplicates of Customer + Modified Date would not work with the inner join. The Datetime format appears not to use the time part, so duplicates could exist theoretically and even Datetime when fully used would not guarantee uniqueness. However the name of the files in combination with different Modified Dates seems to be unique per file and also does not contain dates before the date of the name of the file, so suggests enough uniqueness. Even so, removing duplicates in the Customer ID column would not solve this. It would have to be removing duplicates in the combined Customer Id + Date Modified Date columns, which is not an option. My assumption here would be that the question does not expect this to occur.\nI would thus go with the answer given by VR1.","poster":"not2smart"}],"poster":"Maue","upvote_count":"1"},{"timestamp":"1619893380.0","upvote_count":"4","content":"correct","comment_id":"347171","poster":"Zakriya"},{"timestamp":"1647197340.0","comment_id":"567134","poster":"TechDiva","upvote_count":"3","content":"Explaination by Lhouss:-\n1) Duplicate Customer query\n2) Group by CustId by Max ModifiedDate (only 2 columns to keep)\n3) Merge two queries on CustId and ModifiedDate inner join (to retreive other customer informations related to latest Date)"}]},{"content":"I thin\nduplicate query\ngroup data on customer id and max date\nmerge two on inner join","upvote_count":"55","comment_id":"193011","comments":[{"comment_id":"200489","poster":"vinaikumar","timestamp":"1602765600.0","content":"Its correct","upvote_count":"6"}],"timestamp":"1601825820.0","poster":"JMona"},{"upvote_count":"1","content":"Answer:\n1. Duplicate\n2. Group max date\n3. Merge inner join","poster":"QWERTYman","comment_id":"642726","timestamp":"1659674280.0"},{"content":"I got it on my exam 03/17/2022. 90-95% questions were from here. 61 questions in 100 min. All 3 case studies that appeared in exam, where from here.","poster":"TechDiva","comments":[{"poster":"Bitan47","timestamp":"1653194340.0","upvote_count":"12","comment_id":"605154","content":"To every question you have replied the same"}],"upvote_count":"2","comment_id":"569866","timestamp":"1647531120.0"},{"timestamp":"1639148700.0","content":"on exam 12/10/2021","poster":"PatrickStr","upvote_count":"1","comment_id":"498758"},{"comment_id":"476487","timestamp":"1636662240.0","poster":"aguilartu1","upvote_count":"4","content":"on exam - Nov 11, 2021. \nmy answers:\nDuplicate the customer query\nGroup the CustomerGrouped\nMerge 2 query using a inner join"},{"comment_id":"462068","content":"On exam 10/14/21","timestamp":"1634217420.0","poster":"kcwood94","upvote_count":"1"},{"comment_id":"437573","timestamp":"1630546740.0","content":"I think the answer should be\n1. Duplicate the query\n2. Filter the query on Modified date is latest\n3. Merge 2 queries on Customer ID and Modified date by inner join","poster":"pashi266","upvote_count":"2"},{"poster":"Harsh_87","upvote_count":"2","timestamp":"1624425960.0","comment_id":"388517","content":"Tested and confused what the correct answer to this \n \n1. Duplicate the Query\n2. Group the Query to find the max modified date \n3. Merge with inner Join\nHere is the best part when I expand the columns I start seeing duplicates \n4. Remove the duplicates ?"},{"content":"It says \"You need to keep only the last modified row for each customer ID.\"\nThe WHOLE row. If you 'group by' WITHOUT then performing an inner join merge, you are only left with 2 columns, but they require the whole row. Therefore you need to:\n1. Duplicate the Query\n2. Group the query by Customer ID, outputting max date\n3. THEN merge using inner join","timestamp":"1623658860.0","comment_id":"381698","poster":"bberries","upvote_count":"1"},{"comment_id":"362265","comments":[{"poster":"MLCL","upvote_count":"3","timestamp":"1621867620.0","comment_id":"365674","content":"An inner join achieves the goald of unique CustomerID rows with the latest modified rows, a left outer join leaves you with multiple rows per customerID."}],"upvote_count":"2","timestamp":"1621520400.0","content":"I don't understand why people think you need any kind of join here. Because the question clearly states: \"You need to keep only the last modified row for each customer ID.\" \nPerforming an inner join or a left outer join will result in a table that will still have multiple rows per CustomerID. And the whole idea is that you only keep the last modified row for each CustomerID right?","poster":"TessieB"},{"poster":"cesar_datamachine","upvote_count":"3","content":"I have some doubts reading the others answer but my choice:\n1.- Duplicate Query\n2.- Group the query to find the max modified date per Customer ID\n3.- Inner join","timestamp":"1620674520.0","comment_id":"354088"},{"poster":"Cherishworth","timestamp":"1618323180.0","upvote_count":"6","content":"I used a dataset with the same structure to test. The result is as same as VR1:\n1. Duplicate the query\n2. Group the query by CustomerID and MAX Modified Date\n3. Merge the queries based on the grouped one, join kind is \"Inner\".\nThis question is a bit tricky that, one option is based on the original query to left join. That's wrong. But if you based on the new grouped query to left join, will get the same outcomes as an inner join. \nI realized the wording was trying to confuse us after reading it carefully.","comment_id":"334731"},{"upvote_count":"2","content":"Better answer?\n1. Sort by customer ID ascending and modification date descending.\n2. Remove duplicates based on customer ID.","timestamp":"1614722760.0","comments":[{"upvote_count":"1","content":"This alone dosnt work in power query , You need to Add a Table.Buffer() between the two steps to preserve the sort order before the duplicates are dropped . So inner join is the correct answer here","poster":"vivekmani2021","timestamp":"1617760440.0","comment_id":"330003"}],"poster":"MicahD","comment_id":"302308"},{"content":"Tested: (not need remove duplicate because Inner join)\n1-duplicate query\n2-group by max on date column\n3-merge - inner join","timestamp":"1613992080.0","comment_id":"296576","poster":"kalyhot","upvote_count":"5"},{"comment_id":"295248","poster":"egyhuj","upvote_count":"1","timestamp":"1613842440.0","content":"Vr1 is correct"},{"comment_id":"288326","upvote_count":"5","timestamp":"1613051100.0","poster":"arahan599","content":"I have tried this on power bi desktop.\n1. Duplicate Query\n2. GroupBy CustomerID to find max modified date\n3. Remove Duplicates","comments":[{"upvote_count":"1","poster":"holySinner","comment_id":"351421","content":"I have tried too and it worked","timestamp":"1620336720.0","comments":[{"content":"But if you GroupBy CustomerID, then why do you need to remove duplicates? Because after GroupBy you don't have any duplicates anymore right?","upvote_count":"3","poster":"TessieB","comment_id":"362256","timestamp":"1621520160.0"}]}]},{"timestamp":"1613050500.0","upvote_count":"1","comment_id":"288316","content":"I have tried this in powe bi desktop.\nThe answer is correct.\n1. duplicate query\n2.groupby customerid and find max modified date\n3. remove duplicates","poster":"arahan599"},{"upvote_count":"1","poster":"Patrikkhalaf","content":"How come everyone disagrees so much? have you come to the right place? validated? I have the test tommorow and want to be sure.","timestamp":"1613034120.0","comment_id":"288153"},{"upvote_count":"1","comment_id":"287060","poster":"smpa01","timestamp":"1612896960.0","content":"last 3 in sequence"},{"timestamp":"1607984940.0","content":"The right answer is : \n1. Duplicate the query\n2. Group the query to find the max modified date\n3. Merge the query on inner join","poster":"MBA_1990","comment_id":"244094","upvote_count":"3"},{"poster":"CDL","comments":[{"poster":"Gabza","timestamp":"1612420080.0","comment_id":"283219","content":"How do you test it","upvote_count":"1"}],"content":"Tested.\n1) duplicate the Cust Query\n2) group by Cust.ID\n3) (in pop up window), aggregator/operation is Max by column \"Mod.Date\"\n4) Merge both queries (selected both \"Cust.ID\" and \"Date\"), inner join or left outer.","timestamp":"1607217660.0","upvote_count":"3","comment_id":"236131"},{"upvote_count":"8","comment_id":"224326","content":"Answer provided is not correct. Here are good steps (agree with VR1) :\n1) Duplicate Customer query \n2) Group by CustId by Max ModifiedDate (only 2 columns to keep)\n3) Merge two queries on CustId and ModifiedDate inner join (to retreive other customer informations related to latest Date)","timestamp":"1605959940.0","poster":"Lhouss"},{"content":"Isn't the easiest thing to do:\nSort by date (descending), add index column, dedupe by customer ID...","upvote_count":"1","timestamp":"1605142020.0","poster":"powerbi84","comment_id":"217609"},{"poster":"RajGoy","upvote_count":"2","timestamp":"1603120140.0","comment_id":"202638","content":"Duplicate will create another query with latest file, so now we have two queries one with older file and one with newer file...how it will help..somebody please advise or can upload pbix somewhere?...Thanks","comments":[{"content":"The Query is a list of steps performed by the Power Query, not an actual file imported. In this case the Query already contains all the files, you duplicate it to do some transformations and then use the transfored Query / table for an inner join with the original Query in order to implicitly filter it.","comment_id":"211248","poster":"F_Bastiat","timestamp":"1604324400.0","upvote_count":"2","comments":[{"timestamp":"1621520820.0","comment_id":"362269","upvote_count":"1","poster":"TessieB","content":"But why do you need to filter that Customer query? Because everything you want (being: \"You need to keep only the last modified row for each customer ID.\") is already in that duplicate table that you transformed. So what is the use of using an inner join to filter the original Customer table?"}]}]},{"upvote_count":"7","content":"I verified all options and found the only right answer should be:\n1. Duplicate Query\n2. Group\n3. Merge inner join","comment_id":"198724","poster":"7Zet","timestamp":"1602528120.0"},{"content":"JMona is right! The third step can't be \"remove duplicate on Customer ID\" as after grouping data on customer Id and Max modified date, duplicates are already removed","comment_id":"195767","poster":"clualaba","upvote_count":"2","timestamp":"1602136140.0"},{"timestamp":"1602090420.0","upvote_count":"6","comment_id":"195419","poster":"brcdbrcd","content":"1-duplicate query\n2-group\n3-merge - inner join"},{"comment_id":"191644","timestamp":"1601647500.0","comments":[{"content":"you need to group not filter, see answer VR1!","comment_id":"273503","timestamp":"1611296940.0","upvote_count":"3","poster":"Deloro"}],"content":"I think:\n1. Duplicate Query\n2. Filter Query on modified as latest\n3. Merge two queries on Inner Join","poster":"Evert23","upvote_count":"31"},{"timestamp":"1601647200.0","content":"Not sure if this is correct, suggestions?","poster":"Evert23","upvote_count":"2","comment_id":"191640"}],"url":"https://www.examtopics.com/discussions/microsoft/view/33440-exam-da-100-topic-4-question-2-discussion/","question_text":"DRAG DROP -\nYou have a query named Customer that imports CSV files from a data lake. The query contains 50,000 rows as shown in the exhibit. (Click the Exhibit tab.)\n//IMG//\n\nEach file contains deltas of any new or modified rows from each load to the data lake. Multiple files can have the same customer ID.\nYou need to keep only the last modified row for each customer ID.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/04207/0013700001.png"],"unix_timestamp":1601647200},{"id":"VQPPvqEYL2OAcEiqe0Rk","answer_ET":"","isMC":false,"timestamp":"2020-10-06 22:16:00","question_id":70,"url":"https://www.examtopics.com/discussions/microsoft/view/33842-exam-da-100-topic-4-question-3-discussion/","unix_timestamp":1602015360,"question_text":"HOTSPOT -\nYou view a query named Transactions as shown in the following exhibit.\n//IMG//\n\nThe query gets CSV files from a folder.\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","topic":"4","exam_id":63,"answer_description":"Box 1: 9 -\n9 distinct CSV files.\n\nBox 2: 10 -\n10 distinct dates.","question_images":["https://www.examtopics.com/assets/media/exam-media/04207/0013800001.jpg","https://www.examtopics.com/assets/media/exam-media/04207/0013900001.png"],"discussion":[{"comment_id":"195762","comments":[{"timestamp":"1639896300.0","upvote_count":"1","poster":"Goooglegen","content":"For 2nd Question remove duplicates: \n\nhttps://docs.microsoft.com/en-us/power-query/working-with-duplicates#remove-duplicates-from-a-single-column","comment_id":"504670"},{"poster":"SaeedJei","timestamp":"1634474520.0","comment_id":"463538","comments":[{"content":"Unless they are appended","upvote_count":"1","timestamp":"1634475960.0","poster":"SaeedJei","comment_id":"463548"},{"comment_id":"467556","poster":"snna4","upvote_count":"1","content":"Filenames have spaces at the beginning (see Source.Name column).","timestamp":"1635179160.0"}],"content":"how is it possible to have 90 rows by 9 files?","upvote_count":"2"},{"timestamp":"1659695820.0","poster":"VeroF","content":"you have right, the correct answer is 9, 10. For first area the answer is 9 because we need to look at the column Source.Name and we can see 9 distinct, 0 unique. For second area the answer is 10 because it is selected <Column Profile> for column Date (View meniu) and also we can identify the same answer with <Column Distribution>.","upvote_count":"1","comment_id":"642920"},{"poster":"mirzotti","content":"This is not correct. The 'Count' is showing 90 rows (meaning csv files). 'Distinct' shows 10. So the answer is: 90 & 10.","comments":[{"comment_id":"620835","timestamp":"1655970840.0","upvote_count":"1","poster":"Matreshka","content":"90 rows are shown for Date column, not for csv files. See the highlighted column"},{"comment_id":"514847","poster":"walkerzjs","content":"The source.name has 9 distinct values. That's why I think there are 9 files.","timestamp":"1641102660.0","upvote_count":"4"}],"upvote_count":"9","timestamp":"1640015460.0","comment_id":"505538"}],"content":"answer is correct","timestamp":"1602135660.0","upvote_count":"176","poster":"Ard"},{"upvote_count":"67","poster":"chakri007","comments":[{"content":"Column stats is shown for Date column, not file name","poster":"ZVV","comment_id":"203263","upvote_count":"2","timestamp":"1603203960.0"},{"content":"yup, even the date column count says 90","timestamp":"1604480880.0","upvote_count":"1","comment_id":"212568","poster":"slalithp"},{"content":"I think this is wrong. The correct one is the answer above by Morti42","comment_id":"277229","upvote_count":"3","poster":"stokazz","timestamp":"1611691200.0"},{"comment_id":"251658","timestamp":"1608828240.0","poster":"VesperKiw","content":"But those 90 are ROWs not FILEs. It states in the question they are coming from a folder. You can't have two files with the same name in a folder, so its 9. \nI personally agree with the answer.","upvote_count":"24"}],"content":"I think the answer should be Box1:90 and Box2:10.\nEach row in the table is one CSV file. The answer for Box1 is about the number of rows(which includes duplicates), not the distinct values of a column. This Count can be found under the Column stats below the table as COUNT 90.\nThe second box is distinct count based on the Date Column which is 10.","timestamp":"1603149720.0","comment_id":"202866"},{"comment_id":"670860","content":"The answer is 90. \nHow can you get 90 rows from a folder if there are only 9 files? doesnt make sense.\nIt could be that the files are from different subfolders, but they have the same name. regardless, they are still separate files, with same name, on different folders.","timestamp":"1663333980.0","poster":"jokstogo","upvote_count":"1"},{"poster":"QWERTYman","upvote_count":"1","timestamp":"1659675300.0","content":"Answer\n1. 9 files\n2. 10 rows","comment_id":"642731"},{"upvote_count":"1","poster":"claudeaboki","timestamp":"1654899120.0","comment_id":"614740","content":"The answer is correct"},{"poster":"BLUE_BUBBLES","comment_id":"605526","upvote_count":"3","timestamp":"1653225000.0","content":"There can't be 9 CSV files for the simple fact that the date is a refresh date happening at different intervals. So every refresh will generate another CSV file. The answer is 90."},{"content":"on exam 3/1/2022","poster":"Ashley090521","upvote_count":"1","timestamp":"1646159640.0","comment_id":"558932"},{"poster":"missmanou","content":"So what's the correct answer here ? Is it 90 and 10 or 9 and 10 distinctive dates ?","comment_id":"557968","timestamp":"1646040900.0","upvote_count":"1"},{"content":"In exam 30/12/21","timestamp":"1640811360.0","poster":"Shan","upvote_count":"2","comment_id":"512723"},{"poster":"Siva2104","comment_id":"508410","upvote_count":"3","content":"9 & 10","timestamp":"1640338080.0"},{"comment_id":"508409","content":"My Answer is 10 & 90::\n1) For each day / date two file (8am & 6pm) - for 5 days min is 2018-01-01 8:00:00AM & max is 2018-05-01 6:00:00 PM (Observe the bar in the columns profile) - each file contains the 9 transactions \n\n2) Date columns (Data type - date/Time) - this filed doesn't have duplicate values - if you use the option to remove duplicate against this columns will not Impact the count\n\nTested","upvote_count":"1","timestamp":"1640337720.0","poster":"Siva2104"},{"upvote_count":"2","timestamp":"1638439140.0","comment_id":"492355","content":"the answer is wrong for box 1. I think it should be 90","poster":"testbbb"},{"comment_id":"482655","poster":"DIPARJ","timestamp":"1637421660.0","content":"Since the total count is 90 and as per value distribution it is uniform for all dates so answer would be 90/10= 9 and removing duplicates would result in 10 distinct values","upvote_count":"3"},{"upvote_count":"5","comment_id":"476488","content":"on exam - Nov 11, 2021. \nmy answers:\n9\n10","poster":"aguilartu1","timestamp":"1636662240.0"},{"poster":"lavI_01","timestamp":"1636652160.0","comment_id":"476383","upvote_count":"2","content":"perhpas the csv files are located in a subfolder inside a folder in which case they can have the same name"},{"content":"why not 10 files as there are 10 distinct id","poster":"katherine_dawalk","timestamp":"1634375520.0","upvote_count":"1","comment_id":"462990"},{"timestamp":"1631872560.0","content":"Correct asnwer is 90 and 10","upvote_count":"4","comment_id":"446520","poster":"yusufuthman57"},{"upvote_count":"2","content":"Asked my teacher and we discussed in class, right answers are 9 and 10","timestamp":"1631791980.0","comments":[{"timestamp":"1634347260.0","poster":"dodis_13","upvote_count":"3","comment_id":"462848","content":"your teacher is the best!"}],"poster":"Beert","comment_id":"445898"},{"poster":"saintpatrick","upvote_count":"3","content":"There actually 90 Transaction recorded on 9 CSV files made based on 10 distinct date/times... It could be that there are 10 transaction per customer at set date/times... Thus the answer of 9 csv files and 10 distinct dates is correct...","comment_id":"439346","timestamp":"1630789680.0"},{"timestamp":"1629554760.0","content":"My answer:\nBox1: 90 - The query gets CSV files from a folder. Since there are 90 rows, the query has retrieved 90 files, and the question does not ask for distinct file names.\nBox2: 10 - There are 10 distinct dates, then removing duplicates on the date column leaves us 10 rows.","upvote_count":"2","poster":"[Removed]","comment_id":"428736"},{"content":"Got this in the exam - Aug 15, 2021.","poster":"francis6170","timestamp":"1629016320.0","comment_id":"425161","upvote_count":"3"},{"upvote_count":"3","poster":"jecapanda","timestamp":"1628480580.0","content":"Guys anyone noticed that there are spaces in the beginning of all the names?","comment_id":"421913"},{"timestamp":"1626866700.0","content":"box1: 90\nbox 2: 10 ( \" remove duplicates based on date column \" )","poster":"tramynt","upvote_count":"1","comment_id":"410839"},{"timestamp":"1625491200.0","poster":"SillyChili","content":"this question is very tricky. at first, it seems to have 9 CSV files, as most people pointed it, if we were to go by the filename. But again, I read MonikaL comment, and come to realize that there are whitespaces in front of the CSV filename, which doesn't make it unique. The answer for Box 1 should be 90 in that case","comments":[{"upvote_count":"2","content":"I think each csv file is merged with more then one customer \nthere are 9 distinct csv files and 10 disticnt customers\nIt looks like each customer is combined with each csv file so there are 10 * 9= 90 rows","poster":"gerard","timestamp":"1626861060.0","comment_id":"410778"}],"comment_id":"399199","upvote_count":"3"},{"timestamp":"1624227240.0","comment_id":"386599","poster":"Joe212","upvote_count":"2","content":"One CSV should not hold multiple dates.\n90\n10"},{"poster":"wellingtonluis","timestamp":"1623955680.0","content":"Guys you don´t take in account the ID.","upvote_count":"1","comment_id":"384402"},{"content":"For me this question is total confusing .... espeecialy the picture, there are few csv files with the same name... how is that even possible? with xlsx i would understand, that there are few diferent lists but with csv? How can you have 1 csv file multiple times?","upvote_count":"2","poster":"MonikaL","comment_id":"364644","timestamp":"1621780740.0"},{"comments":[{"upvote_count":"3","poster":"Maelly","content":"That's what I thought at first... I thought that \"Removing duplicates\" wouldn't remove anything because each row is unique. But here, we are not removing duplicate rows (To achieve that we would need to select every column, and not just Date). Here, we are removing duplicates based on Date column, therefore it will just erase every row where the Date is duplicated. In the exhibit, we would end up with 10 rows, the first 9 all belonging to Transactions201801, and the 10th row belonging to Transactions201802.","timestamp":"1621642800.0","comments":[{"timestamp":"1621643040.0","comments":[{"upvote_count":"2","timestamp":"1622032080.0","content":"Your are right ! after testing same scenario, ended up with 10 rows for second answer","poster":"tiraff","comment_id":"367121"}],"poster":"Maelly","upvote_count":"2","comment_id":"363324","content":"In other words, we are not \"removing duplicates\", we are \"removing rows where Date is duplicated\".\nI wish they changed the wordings :(, it is confusing."}],"comment_id":"363322"}],"content":"From my understanding: \nBox1: 9 because there are 9 distinct files in the dataset\nBox2: 90 because question is about the datasetset size after removing duplicates, So 9 values for files Multiply by 10 values for Dates = 90 rows(cartesian product)","comment_id":"351985","timestamp":"1620394140.0","poster":"tiraff","upvote_count":"4"},{"content":"tota confusing .......","timestamp":"1620378780.0","poster":"ajinnadh","comment_id":"351808","upvote_count":"1"},{"comment_id":"349644","content":"you can have several files with the same names if they are stored in different subfolders of the folder. just checked in the tool. In my example 16 different files with the same name: count = 16, distinct = 1, unique = 0. So, the 90 for 1 question is possible.","upvote_count":"2","poster":"andry_stan","comments":[{"content":"Otherwise, can you provide usecase, when you load 9 files from 1 folder and get 90 rows of data fro them with different timestampts? I can oly imagine it as 10 different folders, each for the different timestempt. And in each folder we have different 9 files.","comment_id":"349646","poster":"andry_stan","timestamp":"1620148800.0","upvote_count":"5"},{"upvote_count":"1","comment_id":"351463","timestamp":"1620343200.0","content":"its possible but not probable; and for this we have to assume one row one file theory but do we have enough information to prove this.","poster":"holySinner"}],"timestamp":"1620148560.0"},{"timestamp":"1618424100.0","poster":"Cherishworth","comment_id":"335735","upvote_count":"3","content":"Go for the answer, 9, 10."},{"timestamp":"1617333300.0","comment_id":"326281","upvote_count":"5","content":"The query gets CSV files from a folder. -> File name cannot be the same. Therefore, the \"count\" doesn't mean anything here. Instead, we should look at \"distinct number\" from \"column distribution\" part in the picture, that is, 9.\n\nThe other question is out of the question. It's 10. With no doubt. Linda Wu confirmed this.","poster":"EmperorWenchang"},{"upvote_count":"2","comment_id":"324835","content":"The answer is correct. 9 and 10 with no doubt...","timestamp":"1617177240.0","poster":"EmperorWenchang"},{"comment_id":"313044","timestamp":"1615970520.0","content":"Answer is 9 then 10. If you look at the distribution of Source.Name you will see there are 9 unique CSV files. Do not confuse with there are 90 rows. Each CSV file is repeated multiple times (I assume 10 times).","poster":"mmustafaicer","upvote_count":"4"},{"content":"90 records from 9 Distinct Sources/Files for 10 Distinct/different days","comment_id":"294482","poster":"Sanin","timestamp":"1613756340.0","upvote_count":"3"},{"poster":"unbeat77","timestamp":"1613149440.0","content":"The answer should be 9, 90 as there are 9 different csv files and 90 is count of rows as same name files are loaded on different datetime","upvote_count":"3","comment_id":"289025"},{"comment_id":"288319","timestamp":"1613050740.0","upvote_count":"1","poster":"arahan599","content":"you can see there are 90 rows \n9 values in source.name 10 values in date"},{"timestamp":"1611762420.0","content":"9 CSV files, 10 distinct dates","upvote_count":"2","comment_id":"277895","poster":"Nugi"},{"comment_id":"277863","poster":"Nastja","timestamp":"1611759120.0","content":"Who wondering, what is the difference between distinct and unique values: \n\"“Distinct” means total number of different values regardless how many times it appears in the dataset. A name appears in the list multiple times is counted as 1 distinct count. Whereas, the “Unique” value is total number of values that only appear once.\"","upvote_count":"5"},{"content":"i think answer its 90,10 \nbecause they havent asked how many \"distinct\" csv files are there. and once you remove duplicates, it doesnt make sense to increase the count","comment_id":"275296","comments":[{"timestamp":"1618358100.0","poster":"mynameistaken","comment_id":"334990","content":"If you put it like that then 90 is wrong too because 90 is the number of total records of csv files names in table and not the actual number of csv files. So 9 makes more sense. The answer is correct.","upvote_count":"1"},{"timestamp":"1620342480.0","comment_id":"351453","poster":"holySinner","upvote_count":"1","content":"i can take that there can be more than 9 files some or all of them have same name coming from subfolders; but the question is how one can establish there are 90 files? so for me 9 makes more sense"}],"poster":"divyak29","upvote_count":"3","timestamp":"1611495960.0"},{"content":"NO DOUBT : 9, 10\n9 distincts CSV file and 10 distinct dates after removing the duplicates","comment_id":"264925","upvote_count":"3","poster":"iaaqq","timestamp":"1610386380.0"},{"comment_id":"248061","poster":"Deibipro","timestamp":"1608393720.0","content":"90, 10. If they were 9 and 10, it is not possible that removing duplicates will appear more rows in the datasets. If you notice the names of the .csv are repeated. in column profile indicates 5 columns 90 rows. There are 90 .csv files (0 unique because all csv files are repeated, in total 90 csv files)","comments":[{"upvote_count":"2","poster":"VesperKiw","comment_id":"251661","timestamp":"1608828420.0","comments":[{"timestamp":"1608921540.0","comment_id":"252227","upvote_count":"2","content":"I think that the csv files are the ones that appear in the source.name column","poster":"Deibipro"}],"content":"No, because the question states they come from a (as in single) folder (not folders) and a file cannot have the same name. It asks how many files in the first question not rows, so IMO the given answer is correct."}],"upvote_count":"3"},{"timestamp":"1607985480.0","upvote_count":"6","content":"Answer is Correct","comment_id":"244100","poster":"MBA_1990"},{"content":"90, 10.\n\nThe main difference between Unique and Distinct in SQL is that Unique helps to ensure that all the values in a column are different while Distinct helps to remove all the duplicate records when retrieving the records from a table.\nhttps://pediaa.com/what-is-the-difference-between-unique-and-distinct-in-sql/#:~:text=Unique%20and%20Distinct%20are%20two%20SQL%20constraints.,the%20records%20from%20a%20table.\n\nSimply put, \"Distinct\" describes an output after removing duplicate , \"Unique\" describes an input with a unique list (no duplicate at the first beginning).\nAs we can see here, all unique is \"Zero\", means there are way more than 9 input source files.","comments":[{"upvote_count":"2","comment_id":"242780","content":"Look at the picture and think a bit more...","poster":"ZVV","timestamp":"1607878740.0"}],"timestamp":"1607231580.0","poster":"CDL","comment_id":"236190","upvote_count":"3"},{"comment_id":"234925","timestamp":"1607086620.0","poster":"Andrexx","content":"I believe the answer is correct. 9 files and 10 distinct dates.","upvote_count":"6"},{"content":"The answer is correct : 9 CSV Files et 10 distinct dates. The 90 rows is a result of appending all rows coming from the 9 CSV files stored in the fold.","upvote_count":"6","comment_id":"227610","timestamp":"1606311060.0","poster":"ZIKO2020"},{"content":"Answer is correct.","upvote_count":"4","timestamp":"1605960960.0","poster":"Lhouss","comment_id":"224328"},{"content":"I agree with Mawii. It doesn't say \"There are (answer choice) DISTINCT CSV files\"","upvote_count":"1","comments":[{"content":"It doesn't say that, but it does implicitly mean that. If the csv files are not distinct, it means they are one and the same file. There might be 90 rows, but they represent a total of 9 csv files according to my logic.","timestamp":"1604829360.0","comment_id":"215139","upvote_count":"9","poster":"sboo92"},{"upvote_count":"1","comment_id":"248465","poster":"ZVV","timestamp":"1608453900.0","content":"Are you really think it asks above non-distinct files? Does it make any sense?"}],"timestamp":"1604373060.0","poster":"Amy_Y","comment_id":"211681"},{"poster":"Mawii","comments":[{"timestamp":"1604741460.0","content":"Look at the column distribution of source.name = 9. There are only 9 source files.","upvote_count":"5","poster":"maybelline_ny","comment_id":"214520"}],"upvote_count":"6","timestamp":"1604111760.0","comment_id":"209664","content":"The answer is Wrong. Answer1 is 90 because the image says \"90 rows\" [as shown in count] and the Answer2 is 10 because there are 10 different files [as shown in distinct]."},{"comment_id":"194611","comments":[{"timestamp":"1602154920.0","upvote_count":"44","comment_id":"195993","poster":"Morti42","content":"No it's indeed 9 and 10. Source.Name is the list of files from the folder. and you can see there are 9 distinct values"}],"timestamp":"1602015360.0","upvote_count":"2","poster":"Rahhal","content":"Answer:10,\n10"}],"answer":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04207/0014000001.png"],"answers_community":[]}],"exam":{"isImplemented":true,"name":"DA-100","isBeta":false,"provider":"Microsoft","isMCOnly":false,"numberOfQuestions":94,"lastUpdated":"12 Apr 2025","id":63},"currentPage":14},"__N_SSP":true}