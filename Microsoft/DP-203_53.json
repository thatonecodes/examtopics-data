{"pageProps":{"questions":[{"id":"aX8jUjWNCnTsPYj0LmnN","answer":"B","question_id":261,"answer_images":[],"question_images":[],"answer_ET":"B","question_text":"You have a data warehouse in Azure Synapse Analytics.\nYou need to ensure that the data in the data warehouse is encrypted at rest.\nWhat should you enable?","answers_community":["B (100%)"],"exam_id":67,"unix_timestamp":1631588760,"answer_description":"","topic":"3","timestamp":"2021-09-14 05:06:00","discussion":[{"content":"Correct!","upvote_count":"22","timestamp":"1647234360.0","poster":"Podavenna","comment_id":"444295"},{"content":"Selected Answer: B\nCorrect!","poster":"juanlu46","comment_id":"591726","timestamp":"1666703280.0","upvote_count":"5"},{"comment_id":"1132812","content":"Selected Answer: B\nTransparent Data Encryption (TDE) is a built-in security feature of SQL Server that automatically encrypts data at rest.","poster":"Azure_2023","timestamp":"1722014280.0","upvote_count":"1"},{"content":"Selected Answer: B\nB is correct","timestamp":"1709962440.0","comment_id":"1002870","poster":"kkk5566","upvote_count":"1"},{"timestamp":"1678163820.0","content":"Selected Answer: B\nCorrect !","upvote_count":"3","poster":"anks84","comment_id":"661873"},{"timestamp":"1675971360.0","content":"Selected Answer: B\ncorrect","comment_id":"644611","upvote_count":"3","poster":"Deeksha1234"}],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/61988-exam-dp-203-topic-3-question-11-discussion/","choices":{"A":"Advanced Data Security for this database","D":"Dynamic Data Masking","B":"Transparent Data Encryption (TDE)","C":"Secure transfer required"}},{"id":"sEsI7WvrOR8AkdPMf367","discussion":[{"comment_id":"509807","upvote_count":"13","content":"Selected Answer: A\nA is the correct Answer.\nYou can specify the number of partitions at the time of creating an event hub. In some scenarios, you may need to add partitions after the event hub has been created. This article describes how to dynamically add partitions to an existing event hub.\n\nDynamic additions of partitions is available only in premium and dedicated tiers of Event Hubs.\n\nhttps://docs.microsoft.com/en-us/azure/event-hubs/dynamically-add-partitions","poster":"Canary_2021","timestamp":"1672086420.0"},{"comment_id":"446007","timestamp":"1663339140.0","upvote_count":"12","content":"Answer is Correct according to given link","poster":"mshakir"},{"comment_id":"993994","poster":"kkk5566","content":"Answer is Correct","upvote_count":"1","timestamp":"1725015960.0"},{"comment_id":"837717","poster":"esaade","upvote_count":"3","timestamp":"1710314700.0","content":"Selected Answer: A\nA. Azure Event Hubs Dedicated would be the best choice to ingest the variable volumes of data and change the partition count after creation.\n\nAzure Event Hubs Dedicated is a highly scalable and fully managed event hub service that can ingest millions of events per second. It allows you to create and manage partitions, and you can dynamically increase or decrease the number of partitions to accommodate changes in data volume or throughput requirements.\n\nAzure Stream Analytics, Azure Data Factory, and Azure Synapse Analytics are not specifically designed to manage the partition count after creation. Although they can be used to ingest streaming data, they may not provide the flexibility to change the partition count dynamically."},{"poster":"shoottheduck","upvote_count":"2","timestamp":"1709360700.0","comment_id":"826540","content":"Selected Answer: A\nCorrect"},{"poster":"Deeksha1234","content":"Selected Answer: A\nA is correct","timestamp":"1691602620.0","comment_id":"644612","upvote_count":"3"},{"comment_id":"600552","content":"As A is correct","poster":"Dothy","upvote_count":"3","timestamp":"1683885480.0"},{"timestamp":"1665126060.0","comments":[{"poster":"dikkieknor","content":"I think you're focusing on the wrong part. It says that the partition count can be increased in a dedicated event hubs cluster. And this question is about event hubs dedicated (cluster?), so I think event hubs is the correct answer.","timestamp":"1666645140.0","comment_id":"467130","upvote_count":"3"}],"upvote_count":"5","content":"From the provided link: \"We recommend that you choose at least as many partitions as you expect that are required during the peak load of your application for that particular event hub. You can't change the partition count for an event hub after its creation except for the event hub in a dedicated cluster. The partition count for an event hub in a dedicated Event Hubs cluster can be increased after the event hub has been created, but the distribution of streams across partitions will change when it's done as the mapping of partition keys to partitions changes, so you should try hard to avoid such changes if the relative order of events matters in your application.\"","poster":"BerendJan","comment_id":"458564"}],"timestamp":"2021-09-16 16:39:00","answer_ET":"A","isMC":true,"topic":"3","choices":{"B":"Azure Stream Analytics","A":"Azure Event Hubs Dedicated","C":"Azure Data Factory","D":"Azure Synapse Analytics"},"answer_images":[],"unix_timestamp":1631803140,"question_text":"You are designing a streaming data solution that will ingest variable volumes of data.\nYou need to ensure that you can change the partition count after creation.\nWhich service should you use to ingest the data?","question_images":[],"question_id":262,"url":"https://www.examtopics.com/discussions/microsoft/view/62215-exam-dp-203-topic-3-question-12-discussion/","answer_description":"","answer":"A","answers_community":["A (100%)"],"exam_id":67},{"id":"DxnVM4isZlavtZNH9OpU","question_images":[],"unix_timestamp":1662126960,"answer_images":[],"answer_description":"","discussion":[{"timestamp":"1667019180.0","content":"Selected Answer: B\ncorrect B","upvote_count":"7","poster":"allagowf","comment_id":"706967"},{"upvote_count":"7","comment_id":"927981","content":"Selected Answer: B\nHASH = Fact/2+Gb table\nREPLICATE = Dimensionn\nROUND_ROBIN = Staging","timestamp":"1687212120.0","poster":"vctrhugo"},{"upvote_count":"1","content":"Selected Answer: B\ndate dimension table is small so choose replicate","poster":"evangelist","comment_id":"1243162","timestamp":"1720237620.0"},{"content":"Selected Answer: B\nReplicate","poster":"hassexat","comment_id":"1001504","timestamp":"1694088180.0","upvote_count":"1"},{"comment_id":"993995","upvote_count":"1","timestamp":"1693393620.0","content":"Selected Answer: B\ncorrect","poster":"kkk5566"},{"timestamp":"1662518340.0","upvote_count":"5","poster":"anks84","comment_id":"661876","content":"Selected Answer: B\nREPLICATE"}],"answer_ET":"B","choices":{"A":"HASH","C":"ROUND_ROBIN","B":"REPLICATE"},"topic":"3","answer":"B","question_id":263,"exam_id":67,"answers_community":["B (100%)"],"isMC":true,"timestamp":"2022-09-02 15:56:00","question_text":"You are designing a date dimension table in an Azure Synapse Analytics dedicated SQL pool. The date dimension table will be used by all the fact tables.\nWhich distribution type should you recommend to minimize data movement during queries?","url":"https://www.examtopics.com/discussions/microsoft/view/79440-exam-dp-203-topic-3-question-13-discussion/"},{"id":"fPdcKbcHhMBQU3LY5mK2","answer":"","question_id":264,"answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0030600001.png"],"unix_timestamp":1629879900,"topic":"3","discussion":[{"timestamp":"1641453420.0","upvote_count":"104","comments":[{"poster":"jpgsa11","timestamp":"1716040260.0","comment_id":"1213337","upvote_count":"1","content":"Partition on date prerably, and hash on non date fields so that it is balanced..."},{"upvote_count":"7","poster":"uzairahm","comment_id":"622640","timestamp":"1656255780.0","content":"regarding point 2 Solution needs to support daily incremental load so having Year, Month, Day first would be more useful"}],"content":"1. Partition by\n2. GeographyRegionID, Year, Month, Day as the pipelines are per region this seems right choice\n3. Parquet","comment_id":"518017","poster":"PallaviPatel"},{"timestamp":"1629901560.0","upvote_count":"52","poster":"petulda","content":"I suggest storing the data in parquet","comment_id":"431546"},{"poster":"e56bb91","content":"ChatGPT 4o\nGiven the requirement to support daily incremental loads for each GeographyRegionID, the optimal partitioning strategy would be:\n\nPartition by (GeographyRegionID, Year, Month, Day)\n\nThis strategy allows the system to efficiently access the data based on GeographyRegionID first, and then further narrows down the partitions based on Year, Month, and Day. This approach is particularly useful for managing and querying large datasets in a time-series fashion, which is typical for sensor data.","comment_id":"1245499","upvote_count":"1","timestamp":"1720617420.0"},{"comment_id":"1241918","content":"The second option given in the answer is correct which is Year, Month, Day, GeographyRegionId\nWhy? The question states that the \"solution must minimize storage cost\". One of the way to do that is to have lesser number of folders getting created.\n\nHere is an example:\nLet's says we go with the option of Year, Month, Day, GeographyRegionId where we consider 1 year of data and say we have 10 different region. So the number of folders that would get created would be 1(Year) + 12(Month) + 365 (Days) + 3650 (No of days multiplied by 10 regions as each day's folder would contain 10 sub folders for 10 regions) which gives 3663 as count.\n\nIf we go with the option of GeographyRegionId, Year, Month, Day then 1(Year) + 12(Month) + 365 (Days) i.e, 378 folders would be repeated 10 times inside 10 different region folder which gives total count as 3780(378*10) which is definitely higher than 3663.","upvote_count":"2","timestamp":"1720082040.0","poster":"learnwell"},{"timestamp":"1706300280.0","upvote_count":"4","comments":[{"content":"Agreed with Geographyregionid first before the Year, month and day","poster":"j888","comment_id":"1153810","upvote_count":"1","timestamp":"1708335660.0"}],"poster":"Azure_2023","content":"1. Partition by\n2. GeographyRegionID, Year, Month, Day\n3. Parquet","comment_id":"1132856"},{"comments":[{"poster":"jpgsa11","timestamp":"1716039660.0","comment_id":"1213335","content":"Completely true. The amount of date folders per RegionID would be huge","upvote_count":"2"}],"timestamp":"1704225360.0","upvote_count":"8","poster":"xmety","comment_id":"1112209","content":"1. Patition by\n2.Year,Month, Day, GeographyRegionID (it said to minimize storage cost, not query performance. if GeographyRegionID goes first, each regionID will have repeated folders for different dates)\n3. Parquet"},{"upvote_count":"3","timestamp":"1696412340.0","poster":"pperf","comment_id":"1024624","content":"https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices"},{"upvote_count":"4","poster":"VikkiC","content":"This question is similar to #36Topic 1, if you reference that question, the answer should be 1. Partitioned By, 2. GeographyRegionID, Year, Month, Day, 3. Parquet","comment_id":"951520","timestamp":"1689337620.0"},{"poster":"JosephVishal","content":"For 3.) if parquet with partitions, then it should \"overwrite\" mode instead of \"append\". Since, it is \"append\" mode, I think saveAsTable sis more appropriate.","timestamp":"1671213600.0","comment_id":"747476","upvote_count":"1"},{"timestamp":"1660139400.0","comment_id":"645007","poster":"Deeksha1234","upvote_count":"8","comments":[{"upvote_count":"2","content":"Agree on 1) & 3) but for 2) it should be year/month/day/GeographyRegionId and for each day we would generate several GeographyRegionId.parquet files","comment_id":"724427","comments":[{"comment_id":"729240","upvote_count":"2","poster":"OldSchool","content":"Disregard my comment on 2). Provided answer is the correct one.","timestamp":"1669646520.0"}],"poster":"OldSchool","timestamp":"1669132140.0"}],"content":"Agree with Pallavi\n1. Partition by\n2. GeographyRegionID, Year, Month, Day \n3. Parquet"},{"upvote_count":"3","timestamp":"1657474140.0","poster":"dsp17","content":"Parquet is must (offer higher compression rates)- \"The solution must minimize storage costs.\"","comment_id":"629658"},{"poster":"Aurelkb","content":"it is the same question on Topic 1 Question 36.\nThen \n1. Partition by\n2. GeographyRegionID, Year, Month, Day\n3. Parquet","upvote_count":"10","timestamp":"1656588540.0","comment_id":"625223"},{"comments":[{"comment_id":"622193","timestamp":"1656173460.0","poster":"Davico93","upvote_count":"1","content":"Agree, but if you choose the first one, you won't have the daily data","comments":[{"poster":"allagowf","upvote_count":"2","comment_id":"706973","content":"no mentionning for daily data in the question","comments":[{"timestamp":"1684194780.0","comment_id":"898774","poster":"Spinozabubble","content":"daily incremental load pipelines","upvote_count":"3"}],"timestamp":"1667019360.0"}]}],"timestamp":"1652533560.0","poster":"Backy","comment_id":"601590","content":"// the correct answer is\n\ndf.write.partitionBy(\"GeographyRegionID\").mode(\"append\").parquet(\"/DBTBL1\")\n\n// or\n\ndf.write.partitionBy(\"GeographyRegionID\",\"Year\",\"Month\",\"Day\").mode(\"append\").parquet(\"/DBTBL1\")\n\n\n// Question says \"minimize storage costs\" so I would select the first one","upvote_count":"4"},{"poster":"Amsterliese","content":"I was wondering if the incremental load is supported for parquet, but since \"append\" mode is used, this should be alright. The question asks to minimize costs, so I go for parquet (not saveAsTable).\npartitionBy\nGeopgraphyRegionID, Year,Month,Day (pipelines per region; daily load)\nparquet","timestamp":"1649836500.0","comment_id":"585095","upvote_count":"2"},{"poster":"dev2dev","comment_id":"532673","content":"its recommend to use partitions first before Y/M/D so that they can be managed eazily such as assigning security, or processing by business unit such as zone/country/area etc., GeographyRegionId/Year/Month/Day and Paraquet are answers","upvote_count":"9","timestamp":"1643179980.0"},{"comment_id":"511585","content":"Mes chers amis: \n1.Sortby\n2.GeographyRegionId, Year, Month, Day\n3.Parquet","upvote_count":"8","poster":"bad_atitude","timestamp":"1640726280.0"},{"content":"only reason for using .parquet is option seems to be dataset path not table else saveastable is right.","upvote_count":"2","poster":"jv2120","comment_id":"507571","timestamp":"1640231400.0"},{"content":"I agreed with @hryniewka, saveAsTable takes db name and table name not path.","comment_id":"472424","poster":"Aslam208","upvote_count":"4","timestamp":"1636009860.0"},{"timestamp":"1635886200.0","upvote_count":"4","comment_id":"471807","comments":[{"poster":"sparkchu","timestamp":"1649203920.0","upvote_count":"1","content":"u got the right answer with wrong reasoning, saveAsTable() can also take file path when a unmanaged table is created in such case. Like rav009 said, the correct answer for this not to choose saveAsTable() is because of the more disk space required for Delta format.","comment_id":"581502"}],"poster":"hryniewka","content":"saveAsTable is wrong as in saveAsTable we specify name for the table and here is a path, so I would suggest that correct answer is parquet"},{"upvote_count":"6","content":"saveAsTable will use the delta format to save the dataset.\ndelta format is based on parquet with versions\nso delta will cost more on storage\nBox 3 should be parquet","timestamp":"1632305160.0","comment_id":"449421","poster":"rav009"},{"content":"saveAsTable is the right option \n\nDataFrameWriter.format(args)\n .option(args)\n .bucketBy(args)\n .partitionBy(args)\n .save(path)\n\nDataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)","upvote_count":"1","comments":[{"poster":"kimalto452","content":"nop nop and nop","timestamp":"1631933280.0","upvote_count":"10","comment_id":"446878"}],"timestamp":"1631645760.0","comment_id":"444766","poster":"A1000"},{"content":"saveAsTable() creates a permanent, physical table stored in S3 using the Parquet format","timestamp":"1630242900.0","poster":"MoDar","upvote_count":"4","comment_id":"434613","comments":[{"comments":[{"poster":"yyyhhh","comment_id":"653825","upvote_count":"1","content":".saveAsTable() is used in spark. for df, the answer is .parquet('...')","timestamp":"1661835720.0"}],"content":"I agree with you. the default format is Parquet when .format() is ignored.\nref: https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html \n- \"If you don’t specify this method, then the default is Parquet or whatever is set in spark.sql.sources.default.\"\n- saveAsTable() : The table to save to.","upvote_count":"1","timestamp":"1661586420.0","poster":"yyyhhh","comment_id":"652503"}]},{"upvote_count":"6","content":"Also for the partitions, i'd say do {sourcetype}always before yyymmdd so you can easily isolate it for security purposes","timestamp":"1629985860.0","comment_id":"432381","poster":"SaferSephy"},{"poster":"SaferSephy","comment_id":"432380","upvote_count":"3","content":"Storage would be cheaper if stored in a datalake, and if you'd use parquet you'd get the best compression, so i'd say Parquet","timestamp":"1629985740.0"},{"comment_id":"431377","poster":"msh600","content":"Why does the GeographyRegionID comes after the dates? Can someone explain?","timestamp":"1629889680.0","upvote_count":"15","comments":[{"comment_id":"447754","poster":"Lrng15","timestamp":"1632072660.0","content":"\"GeographyRegionID\" should come first and then followed by \"Year\", \"Month\", \"Day\"","upvote_count":"17"},{"timestamp":"1631648640.0","content":"Official Databricks documentation says that the common partition column is date. I guess that might be the reason.\nhttps://docs.databricks.com/delta/best-practices.html#choose-the-right-partition-column","poster":"Podavenna","upvote_count":"4","comment_id":"444783","comments":[{"content":"I think It is about scanning cost. If you put dates at the end, you will have to scan all previous dates for each region before loading the daily load. This question is about a daily load. It is better to scan the previous dates only once before loading the daily data.","comments":[{"timestamp":"1682598960.0","comment_id":"882633","upvote_count":"1","poster":"mamahani","content":"regarding the path, in the question 36 such an option is not even provided in the answer area: https://www.examtopics.com/discussions/microsoft/view/67646-exam-dp-203-topic-1-question-36-discussion/ ;"}],"upvote_count":"6","timestamp":"1631818800.0","poster":"Marcus1612","comment_id":"446154"}]},{"poster":"fafage","upvote_count":"9","timestamp":"1633523700.0","content":"According to the description there will be different pipelines taking care of the daily load based on different GeographyRegionID, I would put the GeographyRegionID before the dates hierarchy to make a clear isolation for the individual pipelines.","comment_id":"458256"}]},{"timestamp":"1629879900.0","poster":"Amalbenrebai","upvote_count":"1","content":"For Box3 it is saveAsTable OR parquet ?","comment_id":"431274"}],"answer_description":"Box 1: .partitionBy -\nIncorrect Answers:\n✑ .format:\nMethod: format():\nArguments: \"parquet\", \"csv\", \"txt\", \"json\", \"jdbc\", \"orc\", \"avro\", etc.\n✑ .bucketBy:\nMethod: bucketBy()\nArguments: (numBuckets, col, col..., coln)\nThe number of buckets and names of columns to bucket by. Uses Hive's bucketing scheme on a filesystem.\nBox 2: (\"Year\", \"Month\", \"Day\",\"GeographyRegionID\")\nSpecify the columns on which to do the partition. Use the date columns followed by the GeographyRegionID column.\nBox 3: .saveAsTable(\"/DBTBL1\")\nMethod: saveAsTable()\nArgument: \"table_name\"\nThe table to save to.\nReference:\nhttps://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html https://docs.microsoft.com/en-us/azure/databricks/delta/delta-batch","timestamp":"2021-08-25 10:25:00","exam_id":67,"isMC":false,"question_text":"HOTSPOT -\nYou develop a dataset named DBTBL1 by using Azure Databricks.\nDBTBL1 contains the following columns:\n✑ SensorTypeID\n✑ GeographyRegionID\n✑ Year\n✑ Month\n✑ Day\n✑ Hour\n✑ Minute\n✑ Temperature\n✑ WindSpeed\n✑ Other\nYou need to store the data to support daily incremental load pipelines that vary for each GeographyRegionID. The solution must minimize storage costs.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0030700001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/60600-exam-dp-203-topic-3-question-14-discussion/","answers_community":[]},{"id":"1xmHmXqNmoLIU6IUYe3O","question_id":265,"discussion":[{"upvote_count":"21","timestamp":"1641465300.0","poster":"lukeonline","comment_id":"518151","content":"Selected Answer: AB\nA and B"},{"poster":"alexleonvalencia","comments":[{"comment_id":"502460","upvote_count":"6","poster":"VJPR","timestamp":"1639599840.0","content":"why not RBAC?","comments":[{"poster":"sensaint","content":"Assuming RBAC is already in place, predicate function for row-level security would be next step. However, it's not clearly stated in question which makes it confusing.","comment_id":"763700","upvote_count":"1","timestamp":"1672661580.0","comments":[{"poster":"[Removed]","timestamp":"1678664520.0","content":"That's why I went with AB instead because it wasn't mentioned. Therefore, we should assume that the system does not already have the RBAC already in place.","comment_id":"837473","upvote_count":"6"},{"timestamp":"1704191280.0","upvote_count":"1","poster":"dakku987","content":"see you can not add even row level security bcz you are saying some company will have access to some of its rows even that is not allowed \n\nAB","comment_id":"1111777"}]},{"timestamp":"1705016040.0","poster":"Tapaskaro","upvote_count":"1","comment_id":"1120290","content":"RBAC on storage, no impact on dedicated pool."}]}],"timestamp":"1639257540.0","comment_id":"499672","upvote_count":"18","content":"Selected Answer: AC\nRespuesta A/C"},{"timestamp":"1722529140.0","content":"Question is malformed. It does not indicate if all data resides in a single table or if each company has their own tables in the same database.","comment_id":"1259476","upvote_count":"4","poster":"7082935"},{"timestamp":"1719246420.0","poster":"slamcity","upvote_count":"1","content":"Selected Answer: AC\nboth needed for RLS","comment_id":"1236462"},{"comment_id":"1219610","timestamp":"1716820140.0","content":"Selected Answer: AC\nboth are needed for RLS","upvote_count":"1","poster":"slamcity"},{"upvote_count":"1","poster":"leenirs","timestamp":"1716694080.0","comment_id":"1218711","content":"Selected Answer: AC\nChatGPT 4o:\n\nTo ensure that users from each company can view only the data of their respective company in an Azure Synapse Analytics dedicated SQL pool, you should include the following objects in your solution:\n\nA. a security policy\nC. a predicate function\n\nSecurity Policy (A): A security policy in Azure Synapse Analytics is used to define the conditions under which access to data is granted. This can include row-level security (RLS) policies that control access to rows in a table based on the characteristics of the user executing a query.\n\nPredicate Function (C): A predicate function is used in conjunction with a security policy to enforce row-level security. The predicate function specifies the logic that determines whether a given row should be visible to a particular user. This function is often written as an inline table-valued function that checks user-specific attributes, such as their company affiliation, against the data in the table."},{"content":"A and C","poster":"Dusica","timestamp":"1714639800.0","upvote_count":"2","comment_id":"1205389"},{"poster":"Alongi","comment_id":"1197416","timestamp":"1713381900.0","upvote_count":"2","content":"Selected Answer: AC\nSec Policy & Predicate Function"},{"content":"Selected Answer: AC\nImplement RLS by using the CREATE SECURITY POLICY Transact-SQL statement, and predicates created as inline table-valued functions\n\ntherefore, answers are A and C\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=azure-sqldw-latest&preserve-view=true","poster":"mav2000","comment_id":"1158692","upvote_count":"2","timestamp":"1708863660.0"},{"upvote_count":"2","content":"Selected Answer: AB\nDespite their different purposes, security policies and custom RBAC roles share some common elements:\n\nBoth are designed to protect data from unauthorized access.\nBoth can be used to define permissions for users or groups of users.\nBoth can be managed by administrators.","comment_id":"1132891","poster":"Azure_2023","timestamp":"1706302980.0"},{"upvote_count":"2","timestamp":"1706202120.0","content":"AC - for sure","comment_id":"1131880","poster":"JIOAOI"},{"content":"Selected Answer: AB\nA and B","poster":"jsav1","upvote_count":"1","timestamp":"1704856140.0","comment_id":"1118101"},{"upvote_count":"2","comment_id":"1104915","poster":"Momoanwar","timestamp":"1703460720.0","content":"Selected Answer: AB\nChatgpt:\n\nIf only two responses must be selected from the given options, based on the question asked, the two most relevant objects to ensure that users can view only the data of their respective company would be:\n\nA. **A security policy**: This would define the rules and conditions for data access based on company affiliation.\n\nB. **A custom role-based access control (RBAC) role**: This would allow for the assignment of specific access rights depending on the user's company.\n\nEven though a predicate function could be used as part of a security policy implementation, it is typically a component of such a policy, rather than a standalone object. Options D and E are related to encryption and are not directly used to control data views based on the user's company.\n\nTherefore, the two most appropriate answers, according to the question, would be A and B."},{"poster":"MarkJoh","content":"Selected Answer: AC\nAnswer is A & C. Although as many have indicated, the steps are \n• Create the users or groups you want to isolate access.\n• Create the inline table-valued function that will filter the results based on the predicate defined.\n• Create a security policy for the table, assigning the function created above\n\nThe first step may look like \"objects\"/option B but option B says \"A custom role-based access control (RBAC) role.\nIn reality, you would want to create a domain table with companyId and RoleName and create one Role per companyId. (Or maybe a set of roles per companyId depending on what the requirements are). Then the predicate function would use the meta data driven companyIdRoleName table.","timestamp":"1702068840.0","upvote_count":"5","comment_id":"1091324"},{"timestamp":"1700376900.0","upvote_count":"2","comment_id":"1074487","content":"Based on this MS doc, A&C is the right answer\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=azure-sqldw-latest&preserve-view=true","poster":"Shanuramasubbu"},{"poster":"y154707","upvote_count":"1","comment_id":"1063021","content":"Question says: \"Which two objects should you include in the solution?\". It seems that answers A, B and C should be part of the solution, so any combination of the 3 should be ok in terms of a valid answer. If the question would asked for \"the sequence of the first 2 steps required to achieve the goal\" then the answer would be B => C => A.","timestamp":"1699196340.0"},{"comment_id":"1024651","upvote_count":"1","poster":"pperf","timestamp":"1696415700.0","content":"It's A & C"},{"timestamp":"1694439420.0","comment_id":"1004856","upvote_count":"2","poster":"EliteAllen","content":"Selected Answer: BC\nB: To define roles that have specific permissions to access certain data (company-specific).\nC: To implement a function that filters the data a user can access, based on their company."},{"content":"Selected Answer: AC\ncorrect","poster":"kkk5566","comment_id":"994010","timestamp":"1693394460.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1693394340.0","poster":"kkk5566","comment_id":"994008","comments":[],"content":"Selected Answer: AB\na and B"},{"timestamp":"1688670180.0","poster":"pavankr","upvote_count":"2","content":"RBAC is for to use \"internal\" company. So 100% wrong.","comment_id":"944946"},{"content":"Selected Answer: AC\nhttps://learn.microsoft.com/en-gb/training/modules/implement-compliance-controls-sensitive-data/5-implement-row-level-security\n\nCreate the users or groups you want to isolate access.\nCreate the inline table-valued function that will filter the results based on the predicate defined.\nCreate a security policy for the table, assigning the function created above.","poster":"auwia","comment_id":"934118","timestamp":"1687761480.0","upvote_count":"2"},{"content":"Option C, a predicate function, is not wrong. It can be a helpful tool for fine-grained control over data access. However, it is not strictly necessary for the solution described in the question. A security policy and a custom RBAC role can be used to achieve the desired outcome without a predicate function.\n\nHere is an example of how you could use a security policy and a custom RBAC role to control access to data in a Synapse Analytics dedicated SQL pool:\n\nCreate a security policy that defines who can access data, what data they can access, and how they can access it.\nCreate a custom RBAC role that gives users specific permissions to data in the pool.\nAssign the custom RBAC role to users based on their company name.\nWith this configuration in place, users will only be able to access data that they are authorized to access.\n\nThe use of a predicate function can be helpful in some scenarios, but it is not always necessary. In the case of the question, a security policy and a custom RBAC role are sufficient to achieve the desired outcome.","timestamp":"1686764460.0","upvote_count":"3","poster":"klayytech","comment_id":"923434"},{"upvote_count":"2","timestamp":"1682664900.0","comment_id":"883286","content":"I think A/C as per examples in docs:\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16#CodeExamples\ni dont think its RBAC; according to documentation Synapse RBAC is used to manage who can:\n\nPublish code artifacts and list or access published code artifacts,\nExecute code on Apaches Spark pools and Integration runtimes,\nAccess linked (data) services protected by credentials\nMonitor or cancel job execution, review job output, and execution logs.\"\nI do not see the direct link with limiting retrieved data here;\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-synapse-rbac","poster":"mamahani"},{"upvote_count":"2","comments":[{"content":"ChatGPT isn't always reliable","poster":"jz10","upvote_count":"4","timestamp":"1679938140.0","comment_id":"852325","comments":[{"timestamp":"1685203140.0","comments":[{"timestamp":"1694439300.0","comment_id":"1004855","upvote_count":"1","poster":"EliteAllen","content":"GPT-4 Chose BC, I asked are you sure his response:\nYes, I am confident with the choices.\n\nTo restrict data access at a granular level, where users from different companies can only view data related to their respective companies, implementing row-level security is a well-accepted strategy. Here's a bit more detail on why the selected options are pertinent:\n\nB. Custom Role-Based Access Control (RBAC) Role: Setting up custom roles can allow you to grant specific permissions for accessing certain data. By setting up roles based on companies, you can ensure that users are only able to access the data related to their company.\n\nC. Predicate Function: In the context of row-level security, a predicate function is used to filter the rows of the table that are visible to the user. You can define a function that filters rows based on the company attribute, ensuring that users can only access data from their own company."}],"comment_id":"908084","upvote_count":"2","poster":"janaki","content":"@jz10 you're correct. After ChatGPT answers any of your certification exam questions, you then type -- sure? ChatGPT will change it's answer...so 'Yes' ChatGPT is not reliable."}]}],"poster":"esaade","comment_id":"837729","content":"Selected Answer: BC\nAnswer: B and C\n\nTo ensure that users from each company can view only the data of their respective company in an Azure Synapse Analytics dedicated SQL pool, you can use custom role-based access control (RBAC) roles to define specific permissions for each company, and use predicate functions to apply row-level security (RLS) to restrict access based on company membership. By doing this, you can limit the scope of access to the appropriate company data.\n\nA security policy is a mechanism for implementing automatic security controls to enforce compliance requirements, which may not be directly related to company-specific data access.\n\nA column encryption key is used for encrypting sensitive data, but it does not necessarily restrict access based on company membership.\n\nAsymmetric keys are used for secure communication and authentication, but they do not directly relate to company-specific data access control.","timestamp":"1678693080.0"},{"content":"A, C\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16#CodeExamples","timestamp":"1678046220.0","upvote_count":"2","comment_id":"830272","poster":"AHUI"},{"comment_id":"820696","content":"Selected Answer: B\nThe answer is B","timestamp":"1677254640.0","poster":"haidebelognime","upvote_count":"1"},{"timestamp":"1673477400.0","poster":"yogiazaad","upvote_count":"1","content":"Given answer is correct.\nBelow from Microsoft documentation:\n\"A multi-tenant application can create a policy to enforce a logical separation of each tenant's data rows from every other tenant's rows. Efficiencies are achieved by the storage of data for many tenants in a single table. Each tenant can see only its data rows.\"\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16","comment_id":"772955"},{"poster":"Taou","timestamp":"1672355640.0","content":"Selected Answer: AC\nA and C must go together, so i think the right answer is AC","upvote_count":"2","comment_id":"761552"},{"comment_id":"736848","timestamp":"1670333220.0","upvote_count":"6","poster":"juamd","content":"According to Microsoft documentation: \nCREATE SECURITY POLICY SalesFilter\nADD FILTER PREDICATE Security.tvf_securitypredicate(SalesRep)\nON Sales.Orders\nWITH (STATE = ON);\nGO\nSo the answer are A and C"},{"content":"Selected Answer: AB\nGiven answer is correct.\nD & E are obviously wrong.\nC (in Row Level Security) is not necessary and may not be the right solution either. The best way to secure data is not to allow users to access the data at all. For example, we can store data in different databases or schemas and use RBAC to control user access. Row level security first gives users access to the data (in the table that contains all the data for all users) and then restrict data access to a particular part of the table. This is always less secure than not giving user access to the tables that do not contain any data the user should not have access to. Furthermore, Row Level Security may be breached by guessing work queries. I have done that before and I'm quite confident that I can breach any Row Level Security in SQL database but do not want to elaborate here.","upvote_count":"4","poster":"AzureJobsTillRetire","comment_id":"734654","comments":[{"timestamp":"1671516540.0","content":"Please disregard my previous comments. For purpose of the exam, the answer to the question is AC.","poster":"AzureJobsTillRetire","upvote_count":"1","comment_id":"750553"}],"timestamp":"1670096220.0"},{"content":"Selected Answer: AC\nCREATE SECURITY POLICY SalesFilter\nADD FILTER PREDICATE Security.tvf_securitypredicate(SalesRep)\nON Sales.Orders\nWITH (STATE = ON);\nGO\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=azure-sqldw-latest","poster":"dmitriypo","comment_id":"711369","upvote_count":"2","timestamp":"1667596440.0"},{"upvote_count":"5","content":"Selected Answer: AB\nfor those who select C : this function is to apply RLS on inserting and updating, but not selecting so other user can read the data.\n\nhttps://azure.microsoft.com/en-gb/blog/sql-database-row-level-security-block-predicates/#:~:text=Block%20predicates%20address%20a%20common,SQL%20Database%20(V12)%20server.","poster":"allagowf","comment_id":"692613","timestamp":"1665546660.0","comments":[{"content":"the link you provided talks about BLOCK predicates, the option in this question says \"filter predicates\". And in the article you linked it states \"Whereas filter predicates apply to read operations, block predicates apply to write operations\". Please don't give wrong answer/reference","comment_id":"718023","timestamp":"1668436500.0","upvote_count":"2","poster":"kl8585"}]},{"timestamp":"1662971700.0","upvote_count":"4","poster":"Phund","comment_id":"666745","content":"Selected Answer: AB\nscope is database in pool not table in database"},{"timestamp":"1660140120.0","comment_id":"645019","upvote_count":"2","poster":"Deeksha1234","content":"Selected Answer: AC\nImplement RLS by using the CREATE SECURITY POLICYTransact-SQL statement, and predicates created as inline table-valued functions.\n\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16#CodeExamples"},{"content":"Selected Answer: AC\nANSWER IS A AND C","poster":"HenryDevadar","comment_id":"643602","upvote_count":"1","timestamp":"1659849360.0"},{"comment_id":"624730","upvote_count":"2","timestamp":"1656515340.0","content":"a and c! C because RLS is implemented by the creation of a function on the column you want secure.","poster":"Remedios79"},{"upvote_count":"3","poster":"RanjitManuel","content":"Selected Answer: AC\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16#CodeExamples","timestamp":"1656208740.0","comment_id":"622312"},{"content":"A&B you can use role-based access control (RBAC) roles that are scoped to a resource group, a subscription, a storage account, or an individual queue or container.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-synapse-rbac-roles#synapse-rbac-actions-and-the-roles-that-permit-them\nThat is microsoft document, so no rubbish :)","timestamp":"1654767900.0","poster":"MvanG","comment_id":"613751","upvote_count":"1"},{"timestamp":"1652608320.0","upvote_count":"2","poster":"Mohamed63","content":"The correct answer is A/C. we can not use RBAC to restrict access on data. RBAC data reader role gives read access to the entire conaitner houses the data of two campnies you can read both of them.","comment_id":"602022"},{"comment_id":"591738","poster":"juanlu46","timestamp":"1650893040.0","upvote_count":"1","content":"Selected Answer: AC\nIt's a good chance for use row level security!\nYou need to create a function that filter the data; only filter predicate is supported by Synapse at the moment.\nThen you need to create a security policies that uses the function.\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver15"},{"content":"Selected Answer: AC\nThis is about row based security as it suggests that users should only be able to see their own data. Answer B is rubbish","comment_id":"581499","timestamp":"1649202900.0","poster":"AlCubeHead","upvote_count":"2"},{"poster":"AhmedDaffaie","upvote_count":"2","timestamp":"1648072560.0","content":"Why C? None explained why a function can perform RLS?","comment_id":"573929"},{"timestamp":"1647766620.0","content":"Selected Answer: AB\nA and B","comment_id":"571505","poster":"ranjsi01","upvote_count":"2"},{"upvote_count":"3","comments":[{"timestamp":"1647972120.0","content":"Why C?","comment_id":"573127","poster":"alex1491","upvote_count":"1"}],"comment_id":"536739","poster":"PallaviPatel","content":"Selected Answer: AC\ncorrect","timestamp":"1643616060.0"},{"poster":"Canary_2021","upvote_count":"1","comment_id":"519287","timestamp":"1641609780.0","content":"Selected Answer: AC\nAnswer A and C are correct."},{"upvote_count":"2","timestamp":"1641453000.0","content":"Row-Level Security enables you to use group membership or execution context to control access to rows in a database table. Implement RLS by using the CREATE SECURITY POLICY Transact-SQL statement, and predicates created as inline table-valued functions. hence the correct answer is A and C","poster":"PallaviPatel","comment_id":"518012"},{"content":"A and B: To a security principal or a managed identity for Azure resources, you can use role-based access control (RBAC) roles that are scoped to a resource group, a subscription, a storage account, or an individual queue or container","upvote_count":"5","timestamp":"1640910360.0","comment_id":"513788","poster":"SabaJamal2010AtGmail"}],"unix_timestamp":1639257540,"topic":"3","choices":{"A":"a security policy","C":"a predicate function","B":"a custom role-based access control (RBAC) role","D":"a column encryption key","E":"asymmetric keys"},"answer_images":[],"timestamp":"2021-12-11 22:19:00","question_text":"You are designing a security model for an Azure Synapse Analytics dedicated SQL pool that will support multiple companies.\nYou need to ensure that users from each company can view only the data of their respective company.\nWhich two objects should you include in the solution? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer":"AC","exam_id":67,"isMC":true,"answers_community":["AC (52%)","AB (43%)","4%"],"url":"https://www.examtopics.com/discussions/microsoft/view/67667-exam-dp-203-topic-3-question-15-discussion/","answer_description":"","question_images":[],"answer_ET":"AC"}],"exam":{"isBeta":false,"lastUpdated":"12 Apr 2025","name":"DP-203","isImplemented":true,"isMCOnly":false,"id":67,"numberOfQuestions":384,"provider":"Microsoft"},"currentPage":53},"__N_SSP":true}