{"pageProps":{"questions":[{"id":"cDBPwpxJL2VobWRpQdRo","answer_ET":"A","discussion":[{"comment_id":"366474","poster":"Alekx42","upvote_count":"3","timestamp":"1621951140.0","content":"I think \"no\" should be the answer. As said in the CosmosDB doc, \"The same query on the same data will always costs the same number of RUs on repeated executions\". If the query needs more RU than the ones availabile, the request is rate-limited and will have to be tried again. \nIn this case we have queries that are consistently slow but are not being rate-limite, which means that the number of RU already provisioned is enough. Increasing the RU only ensures that more queries can be executed at the same time, but does not improve the performance of the slow queries.\n\nSource: https://docs.microsoft.com/en-us/azure/cosmos-db/request-units"},{"comment_id":"361251","timestamp":"1621418340.0","content":"increase RU and lookupcollection are 2 possible answers. So;\nanswer is CORRECT","upvote_count":"4","poster":"memo43"},{"timestamp":"1621298340.0","upvote_count":"3","content":"The better solution should have index on ProductName","poster":"hoangton","comment_id":"359919"},{"timestamp":"1617359280.0","poster":"suman13","comments":[{"timestamp":"1618136640.0","poster":"Pairon","upvote_count":"11","comment_id":"333201","content":"I disagree. Having a look up collection is another correct option, but increasing RUs could be another way to reduce latency, IMHO."}],"content":"answer is wrong. we should use lookupcollection on ProductName.","comment_id":"326523","upvote_count":"2"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0037400001.png"],"isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have a container named Sales in an Azure Cosmos DB database. Sales has 120 GB of data. Each entry in Sales has the following structure.\n//IMG//\n\nThe partition key is set to the OrderId attribute.\nUsers report that when they perform queries that retrieve data by ProductName, the queries take longer than expected to complete.\nYou need to reduce the amount of time it takes to execute the problematic queries.\nSolution: You increase the Request Units (RUs) for the database.\nDoes this meet the goal?","answers_community":[],"exam_id":65,"timestamp":"2021-04-02 12:28:00","choices":{"B":"No","A":"Yes"},"unix_timestamp":1617359280,"answer_images":[],"topic":"4","answer":"A","url":"https://www.examtopics.com/discussions/microsoft/view/48782-exam-dp-200-topic-4-question-3-discussion/","answer_description":"To scale the provisioned throughput for your application, you can increase or decrease the number of RUs at any time.\nNote: The cost of all database operations is normalized by Azure Cosmos DB and is expressed by Request Units (or RUs, for short). You can think of RUs per second as the currency for throughput. RUs per second is a rate-based currency. It abstracts the system resources such as CPU, IOPS, and memory that are required to perform the database operations supported by Azure Cosmos DB.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/request-units","question_id":186},{"id":"lqOkb6lHiv33FAJ45T5s","isMC":false,"unix_timestamp":1618046880,"answer":"","exam_id":65,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0042200003.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/49780-exam-dp-200-topic-4-question-30-discussion/","discussion":[{"upvote_count":"27","timestamp":"1618046880.0","poster":"JaNieWiem","content":"There is 6 min between 44 and 50\nSo there shouldn't be any alert.","comment_id":"332422"},{"poster":"Wendy_DK","content":"I agree\nNo for all","comment_id":"337739","upvote_count":"6","timestamp":"1618683840.0"},{"comments":[{"content":"at 10:50 you are in a new window so no, no alerts at all","upvote_count":"1","comment_id":"494323","timestamp":"1638707280.0","poster":"ramelas"}],"poster":"bsa_2021","timestamp":"1624297260.0","content":"Every 5 minutes, it will check. Hence at exactly 10:50 it will check and (may be) the failure of the 2nd pipeline has already logged in by the time the check happens and hence the given answer is correct. Its my analysis and happy to be corrected.","upvote_count":"1","comment_id":"387331"},{"content":"No alerts at all. Not only because there is 6 min period, but also failed activity 3 in pipeline A will not cause pipeline to fails and activity 4 has COMPLETED condition, which means previous activity might either suceeed or fail and it still be triggered. Only last activities in the pipeline must have status success or skipped fot pipeline to succeed.","timestamp":"1622749200.0","poster":"maciejt","comment_id":"373850","upvote_count":"1"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0042100001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0042100002.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0042200001.png","https://www.examtopics.com/assets/media/exam-media/03872/0042200002.png"],"answer_ET":"","answers_community":[],"question_text":"HOTSPOT -\nYou have an Azure data factory that has two pipelines named PipelineA and PipelineB.\nPipelineA has four activities as shown in the following exhibit.\n//IMG//\n\nPipelineB has two activities as shown in the following exhibit.\n//IMG//\n\nYou create an alert for the data factory that uses Failed pipeline runs metrics for both pipelines and all failure types. The metric has the following settings:\n✑ Operator: Greater than\n✑ Aggregation type: Total\n✑ Threshold value: 2\n✑ Aggregation granularity (Period): 5 minutes\n✑ Frequency of evaluation: Every 5 minutes\nData Factory monitoring records the failures shown in the following table.\n//IMG//\n\nFor each of the following statements, select yes if the statement is true. Otherwise, select no.\nNOTE: Each correct answer selection is worth one point.\nHot Area:\n//IMG//","topic":"4","question_id":187,"timestamp":"2021-04-10 11:28:00","answer_description":"Box 1: No -\nOnly one failure at this point.\n\nBox 2: No -\nOnly two failures within 5 minutes.\n\nBox 3: Yes -\nMore than two (three) failures in 5 minutes\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/alerts-insights-configure-portal"},{"id":"56eoHPpeXb2LGSTeSaAg","timestamp":"2020-05-15 15:18:00","question_text":"You have an enterprise data warehouse in Azure Synapse Analytics named DW1 on a server named Server1.\nYou need to verify whether the size of the transaction log file for each distribution of DW1 is smaller than 160 GB.\nWhat should you do?","discussion":[{"comment_id":"102707","content":"A is correct\n-- Transaction log size\nSELECT\n instance_name as distribution_db,\n cntr_value*1.0/1048576 as log_file_size_used_GB,\n pdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor","timestamp":"1591311780.0","poster":"Abhitm","upvote_count":"30"},{"upvote_count":"1","content":"hi to all,\n\nit's A for sure.\nthe \"master\" stuff can put some confusion here but it's right.\n\nplease see https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/synapse-analytics/sql-data-warehouse/quickstart-scale-compute-tsql.md\n\nregards","comment_id":"241480","timestamp":"1607767740.0","poster":"dumpsm42"},{"comment_id":"228300","timestamp":"1606394280.0","poster":"syu31svc","upvote_count":"3","content":"A is 100% correct as per the link given"},{"poster":"VJ8","content":"Answer should be C - Reference: https://docs.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-database-files-transact-sql?view=sql-server-ver15 \n\nsys.database_files gives the log size","upvote_count":"2","comment_id":"101808","timestamp":"1591206840.0"},{"upvote_count":"1","content":"Not sure which answer is the correct one, as the selected answer include \"master database\". The dmv \"dm_pdw_nodes_os_performance_counters\" doesn't exist in the master database. The dmv \"sys.database_files\" only shows 1 log file (not for each distribution), hence my guess is that these answer are incorrectly formulated","poster":"MarvinS91","timestamp":"1589548680.0","comment_id":"89518"}],"isMC":true,"unix_timestamp":1589548680,"answer_images":[],"choices":{"A":"On the master database, execute a query against the sys.dm_pdw_nodes_os_performance_counters dynamic management view.","B":"From Azure Monitor in the Azure portal, execute a query against the logs of DW1.","D":"Execute a query against the logs of DW1 by using the Get-AzOperationalInsightsSearchResult PowerShell cmdlet.","C":"On DW1, execute a query against the sys.database_files dynamic management view."},"answer_description":"The following query returns the transaction log size on each distribution. If one of the log files is reaching 160 GB, you should consider scaling up your instance or limiting your transaction size.\n-- Transaction log size\n\nSELECT -\ninstance_name as distribution_db,\ncntr_value*1.0/1048576 as log_file_size_used_GB,\npdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\n\nWHERE -\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-manage-monitor","topic":"4","answer":"A","url":"https://www.examtopics.com/discussions/microsoft/view/20651-exam-dp-200-topic-4-question-31-discussion/","answers_community":[],"question_id":188,"question_images":[],"answer_ET":"A","exam_id":65},{"id":"QHe0GpGNVI3BAydtsDI0","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0042700001.png"],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0042600001.png"],"discussion":[{"poster":"rakadhar","upvote_count":"8","content":"The answer is correct . \nSee this link --> https://www.cathrinewilhelmsen.net/table-partitioning-in-sql-server/ \nFrom this link below excerpt :-\n\nPartition functions are created as either range left or range right, it is not possible to combine both in the same partition function. In a range left partition function, all boundary values are upper boundaries, they are the last values in the partitions. If you partition by year, you use December 31st. If you partition by month, you use January 31st, February 28th / 29th, March 31st, April 30th and so on. In a range right partition function, all boundary values are lower boundaries, they are the first values in the partitions. If you partition by year, you use January 1st. If you partition by month, you use January 1st, February 1st, March 1st, April 1st and so on:","comment_id":"336577","timestamp":"1618517460.0"},{"upvote_count":"2","content":"This question doesn't make any sense !","comment_id":"502818","poster":"satyamkishoresingh","timestamp":"1639649580.0"},{"timestamp":"1618154940.0","poster":"MsIrene","comment_id":"333397","upvote_count":"4","content":"The answer is correct. Should be RIGHT. Basically, it means that partition starts from the given value, i.e. from 20090101 to 20091231, the next one from 20100101 to 20101231 and so on."},{"upvote_count":"4","poster":"vaseva1","comment_id":"328583","content":"i think its should be LEFT","timestamp":"1617620520.0"}],"answer_description":"Box 1: RIGHT -\nUse right for dates.\n1- RIGHT means < or >=\n2- LEFT means <= and >.\nBox 2: 20090101, 201001010, 20110101, 20120101\nFour values are better than three or two.\nReference:\nhttps://medium.com/@selcukkilinc23/what-it-means-range-right-and-left-in-table-partitioning-2d654cb99ade","unix_timestamp":1617620520,"isMC":false,"question_id":189,"answer_ET":"","exam_id":65,"answer":"","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/49181-exam-dp-200-topic-4-question-33-discussion/","topic":"4","question_text":"HOTSPOT -\nYou have an Azure Cosmos DB database.\nYou need to use Azure Stream Analytics to check for uneven distributions of queries that can affect performance.\nWhich two settings should you configure? To answer, select the appropriate settings in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2021-04-05 13:02:00"},{"id":"DStAQVoFgVdxGrcgAhw4","question_id":190,"discussion":[{"comment_id":"368202","upvote_count":"2","content":"Why do you have to create a block blob? Shouldn't you schedule the lifecycle policy to run as as last step?","poster":"Alekx42","comments":[{"comment_id":"369149","timestamp":"1622260980.0","upvote_count":"1","content":"It is because life cycle policy is only applicable for block blob and append blobas of now. Also you first need to create the block blob --> select the life cycle management --> then apply he rules.","poster":"ajay9654"}],"timestamp":"1622138400.0"}],"answer_description":"Step 1: Create a block blob in a Blob storage account\nFirst create the block blob.\nAzure Blob storage lifecycle management offers a rich, rule-based policy for GPv2 and Blob storage accounts.\nStep 2: Use an Azure Resource Manager template that has a lifecycle management policy\nStep 3: Create a rule that has the rule actions of TierToCool and TierToArchive\nEach rule definition includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names.\nNote: You can add a Rule through Azure portal:\nSign in to the Azure portal.\n1. In the Azure portal, search for and select your storage account.\n2. Under Blob service, select Lifecycle Management to view or change your rules.\n3. Select the List View tab.\n4. Select Add a rule and name your rule on the Details form. You can also set the Rule scope, Blob type, and Blob subtype values.\n5. Select Base blobs to set the conditions for your rule. For example, blobs are moved to cool storage if they haven't been modified for 30 days.\n6. Etc.\nIncorrect Answers:\n✑ Schedule the lifecycle management policy to run:\nYou don't Schedule the lifecycle management policy to run. The platform runs the lifecycle policy once a day. Once you configure a policy, it can take up to 24 hours for some actions to run for the first time.\n✑ Create a rule filter:\nNo need for a rule filter. Rule filters limit rule actions to a subset of blobs within the storage account.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts","exam_id":65,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0043300001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/53675-exam-dp-200-topic-4-question-35-discussion/","answer":"","question_text":"DRAG DROP -\nYou are implementing an Azure Blob storage account for an application that has the following requirements:\n✑ Data created during the last 12 months must be readily accessible.\n✑ Blobs older than 24 months must use the lowest storage costs. This data will be accessed infrequently.\n✑ Data created 12 to 24 months ago will be accessed infrequently but must be readily accessible at the lowest storage costs.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0043200001.png"],"timestamp":"2021-05-27 20:00:00","answers_community":[],"unix_timestamp":1622138400,"topic":"4","answer_ET":"","isMC":false}],"exam":{"isImplemented":true,"numberOfQuestions":228,"isMCOnly":false,"provider":"Microsoft","name":"DP-200","id":65,"lastUpdated":"12 Apr 2025","isBeta":false},"currentPage":38},"__N_SSP":true}