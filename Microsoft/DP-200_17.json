{"pageProps":{"questions":[{"id":"WTJgCDHQzsicbLLBBU6M","topic":"2","isMC":true,"answer":"CE","exam_id":65,"timestamp":"2020-11-26 13:11:00","answer_ET":"CE","question_text":"You have an Azure Stream Analytics query. The query returns a result set that contains 10,000 distinct values for a column named clusterID.\nYou monitor the Stream Analytics job and discover high latency.\nYou need to reduce the latency.\nWhich two actions should you perform? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","discussion":[{"upvote_count":"14","timestamp":"1606392660.0","content":"C and E are correct\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/repartition\nBoth touch on optimization","comment_id":"228277","poster":"syu31svc"}],"answer_images":[],"unix_timestamp":1606392660,"question_id":81,"answers_community":[],"answer_description":"C: Scaling a Stream Analytics job takes advantage of partitions in the input or output. Partitioning lets you divide data into subsets based on a partition key. A process that consumes the data (such as a Streaming Analytics job) can consume and write different partitions in parallel, which increases throughput.\nE: Streaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. The higher the number of SUs, the more CPU and memory resources are allocated for your job. This capacity lets you focus on the query logic and abstracts the need to manage the hardware to run your\nStream Analytics job in a timely manner.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption","question_images":[],"choices":{"D":"Convert the query to a reference query.","B":"Add a temporal analytic function.","A":"Add a pass-through query.","C":"Scale out the query by using PARTITION BY.","E":"Increase the number of streaming units."},"url":"https://www.examtopics.com/discussions/microsoft/view/37833-exam-dp-200-topic-2-question-11-discussion/"},{"id":"gfO8hbqL7YX0KUPUyFKw","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0016100001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0016200001.jpg"],"isMC":false,"answer":"See the explanation below.","unix_timestamp":1620340320,"exam_id":65,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0015900001.jpg"],"topic":"2","discussion":[{"content":"Need to configure output section as well -- In the output select Azure cosmos db and select database and container or else need to create new ones","poster":"Sasidhar39","comment_id":"351441","timestamp":"1620340320.0","upvote_count":"8"}],"answer_description":"Step 1: Create a Stream Analytics job\n1. Sign in to the Azure portal.\n2. Select Create a resource in the upper left-hand corner of the Azure portal.\n3. Select Analytics > Stream Analytics job from the results list.\n4. Fill out the Stream Analytics job page.\n\n5. Check the Pin to dashboard box to place your job on your dashboard and then select Create.\n6. You should see a Deployment in progress... notification displayed in the top right of your browser window.\n\nStep 2: Configure job input -\n1. Navigate to your Stream Analytics job.\n2. Select Inputs > Add Stream input > Azure Blob storage\n\n3. In the Azure Blob storage setting choose: storage10277521. Leave other options to default values and select Save to save the settings.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal","question_text":"SIMULATION -\n//IMG//\n\nUse the following login credentials as needed:\n\nAzure Username: xxxxx -\n\nAzure Password: xxxxx -\nThe following information is for technical support purposes only:\n\nLab Instance: 10277521 -\nYou plan to generate large amounts of real-time data that will be copied to Azure Blob storage.\nYou plan to create reports that will read the data from an Azure Cosmos DB database.\nYou need to create an Azure Stream Analytics job that will input the data from a blob storage named storage10277521 to the Cosmos DB database.\nTo complete this task, sign in to the Azure portal.","answers_community":[],"answer_ET":"See the explanation below.","timestamp":"2021-05-07 00:32:00","url":"https://www.examtopics.com/discussions/microsoft/view/52025-exam-dp-200-topic-2-question-12-discussion/","question_id":82},{"id":"dpFztzhelPOAghblPWK7","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0017100001.jpg"],"answers_community":[],"answer_description":"Step 1: Create a new Azure Data Factory V2\n1. Go to the Azure portal.\n2. Select Create a resource on the left menu, select Analytics, and then select Data Factory.\n\n4. On the New data factory page, enter a name.\n5. For Subscription, select your Azure subscription in which you want to create the data factory.\n6. For Resource Group, use one of the following steps:\n✑ Select Use existing, and select an existing resource group from the list.\n✑ Select Create new, and enter the name of a resource group.\n7. For Version, select V2.\n8. For Location, select the location for the data factory.\n9. Select Create.\n10. After the creation is complete, you see the Data Factory page.\nStep 2: Create a schedule trigger for the Data Factory\n1. Select the Data Factory you created, and switch to the Edit tab.\n\n2. Click Trigger on the menu, and click New/Edit.\n\n3. In the Add Triggers page, click Choose trigger..., and click New.\n\n4. In the New Trigger page, do the following steps:\na. Confirm that Schedule is selected for Type.\nb. Specify the start datetime of the trigger for Start Date (UTC) to: 24:00:00 c. Specify Recurrence for the trigger. Select Every Hour, and enter 2 in the text box.\n\n5. In the New Trigger window, check the Activated option, and click Next.\n6. In the New Trigger page, review the warning message, and click Finish.\n7. Click Publish to publish changes to Data Factory. Until you publish changes to Data Factory, the trigger does not start triggering the pipeline runs.\n\nReferences:\nhttps://docs.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger","answer":"See the explanation below.","exam_id":65,"url":"https://www.examtopics.com/discussions/microsoft/view/55869-exam-dp-200-topic-2-question-14-discussion/","question_id":83,"unix_timestamp":1624399740,"question_text":"SIMULATION -\n//IMG//\n\nUse the following login credentials as needed:\n\nAzure Username: xxxxx -\n\nAzure Password: xxxxx -\nThe following information is for technical support purposes only:\n\nLab Instance: 10277521 -\nYou plan to create multiple pipelines in a new Azure Data Factory V2.\nYou need to create the data factory, and then create a scheduled trigger for the planned pipelines. The trigger must execute every two hours starting at 24:00:00.\nTo complete this task, sign in to the Azure portal.","discussion":[{"timestamp":"1626612480.0","upvote_count":"2","comment_id":"408954","content":"How appears this question type into the exam?, We need to do in the azure portal, or drag and drop box, or is in other way?. Help please","poster":"jamorey"},{"timestamp":"1624399740.0","upvote_count":"1","comment_id":"388322","content":"Correct.","poster":"dragos_dragos62000"}],"topic":"2","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0017300001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0017400003.png","https://www.examtopics.com/assets/media/exam-media/03872/0017500001.png","https://www.examtopics.com/assets/media/exam-media/03872/0017500002.png","https://www.examtopics.com/assets/media/exam-media/03872/0017600001.png","https://www.examtopics.com/assets/media/exam-media/03872/0017700001.png"],"answer_ET":"See the explanation below.","timestamp":"2021-06-23 00:09:00","isMC":false},{"id":"hrMb8T2RXgf6gKQBVp6F","exam_id":65,"url":"https://www.examtopics.com/discussions/microsoft/view/14282-exam-dp-200-topic-2-question-15-discussion/","question_text":"Each day, company plans to store hundreds of files in Azure Blob Storage and Azure Data Lake Storage. The company uses the parquet format.\nYou must develop a pipeline that meets the following requirements:\n✑ Process data every six hours\n✑ Offer interactive data analysis capabilities\n✑ Offer the ability to process data using solid-state drive (SSD) caching\n✑ Use Directed Acyclic Graph(DAG) processing mechanisms\n✑ Provide support for REST API calls to monitor processes\n✑ Provide native support for Python\n✑ Integrate with Microsoft Power BI\nYou need to select the appropriate data technology to implement the pipeline.\nWhich data technology should you implement?","discussion":[{"timestamp":"1584056280.0","upvote_count":"30","poster":"c1265","comment_id":"63284","content":"this really looks like spark, https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview"},{"timestamp":"1587470280.0","poster":"wyxh","content":"Spark clusters in HDInsight provide connectors for BI tools such as Power BI for data analytics.\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview\nStorm processes streams of data in real time, and in the question is stated the data must be Processed every six hours","upvote_count":"21","comment_id":"77446"},{"comment_id":"367472","content":"hadoop is dead. who cares","timestamp":"1622064480.0","poster":"GData23","upvote_count":"4"},{"upvote_count":"1","timestamp":"1619541180.0","content":"The answer is : \"E \"","comment_id":"344101","poster":"davita8"},{"poster":"Hassan_Mazhar_Khan","timestamp":"1618562160.0","comment_id":"336861","upvote_count":"1","content":"The answer is : \"E \""},{"upvote_count":"3","poster":"rjile","timestamp":"1614848280.0","content":"The answer is : \"E \"\nSpark clusters in HDInsight provide connectors for BI tools such as Power BI for data analytics.","comment_id":"303244"},{"comment_id":"286576","poster":"vidray","upvote_count":"2","content":"https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview\n\nAnswer is : HDInsight Spark cluster","timestamp":"1612842360.0"},{"upvote_count":"2","poster":"Shiva2","timestamp":"1611444780.0","content":"I think it must be spark too. I don't think there is a direct configuration from power bi to storm. You will need to process data in storm pass the processed data to SQL server, then visualize. Here there is no mention of SQL server , hence answer must be E. Spark.\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-use-bi-tools","comment_id":"274919"},{"timestamp":"1610972220.0","poster":"Prada","upvote_count":"1","comment_id":"270230","content":"It must be E. HDInsight Spark cluster due to Offer the ability to process data using solid-state drive (SSD) caching\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-improve-performance-iocache"},{"content":"I would say E as per link https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview:\nRead the parts on Caching on SSDs, The SparkContext connects to the Spark master and is responsible for converting an application to a directed graph (DAG) of individual tasks, REST APIs, Integration with BI Tools\nPython is mentioned as well","upvote_count":"4","poster":"syu31svc","timestamp":"1606305780.0","comment_id":"227519","comments":[{"upvote_count":"1","content":"hi to all,\nit's E mostly because of the power BI, the text says exactly \"integrate with power BI\" and in the microsoft documentatio for hd insight spark cluster => \"...Integration with BI Tools Spark clusters in HDInsight provide connectors for BI tools such as Power BI for data analytics....\", so for me it's E\n\nregards","comment_id":"238425","timestamp":"1607440680.0","poster":"dumpsm42"}]},{"upvote_count":"1","comment_id":"193432","content":"Answer is correct. Checkout this - https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview.\nPython can also be used to develop Storm components.\nCreate solutions in multiple languages: You can write Storm components in the language of your choice, such as Java, C#, and Python.","timestamp":"1601885040.0","poster":"Sagja"},{"content":"Storm does not provide Python (https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing) and is not used for 6hr batch processing (https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing).\nSpark supports all of the options. Hence, E is the correct answer.","upvote_count":"2","comment_id":"188912","timestamp":"1601282640.0","poster":"M0e"},{"timestamp":"1599934740.0","poster":"EYIT","comment_id":"178360","content":"E. HDInsight Spark cluster \nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing","upvote_count":"2"},{"content":"it should be Spark because of (DAG + PowerBI integration)","timestamp":"1596793560.0","poster":"Arsa","upvote_count":"3","comment_id":"152471"},{"timestamp":"1595335800.0","poster":"rmk4ever","comment_id":"140308","content":"Ans is HDInsight Spark cluster\nCaching on SSDs\nIntegration with BI Tools\nSpark master is responsible for converting an application to a directed graph (DAG) \nREST API-based Spark job server to remotely submit and monitor job\nfull reference: https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview","upvote_count":"7"},{"poster":"azrnovice","upvote_count":"12","comment_id":"101128","timestamp":"1591125120.0","content":"Question says need python native support. Azure Storm don't support Python. Check out this comparison chart:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"},{"timestamp":"1590236820.0","poster":"Luke97","upvote_count":"8","content":"I think HDInsight Apache Spark should be the correct answer.\n1. Offer interactive data analysis\n2. Offer caching \n3. DirectQuery (live connection) to PowerBI\nAnd Spark also use DAG too (https://docs.microsoft.com/en-au/azure/hdinsight/spark/apache-spark-streaming-exactly-once).","comment_id":"94329"},{"timestamp":"1589216580.0","comments":[{"content":"Other way round as azrnovice wrote: Spark enables Python, Storm does not. \nStorm is for real-time, with Spark one can run jobs\nSpark supports Power BI integration and Storm does not","comment_id":"140184","poster":"Treadmill","upvote_count":"1","timestamp":"1595324340.0"},{"upvote_count":"2","timestamp":"1596697920.0","poster":"pokus","comment_id":"151760","content":"it does"}],"poster":"runningman","comment_id":"87233","content":"i don't think Spark cluster enables Python, so Storm might be right.","upvote_count":"3"},{"upvote_count":"9","content":"storm is for real time analytics not 6hr batch jobs so answer is spark","comment_id":"81942","poster":"gunjan075","timestamp":"1588297740.0"},{"timestamp":"1585689420.0","comment_id":"69952","poster":"mclawson1966","upvote_count":"2","content":"I don't see that Storm integrates directly with Power BI - but it does Storm, so Storm is the correct answer."},{"upvote_count":"1","timestamp":"1585290540.0","comments":[{"upvote_count":"6","comment_id":"68705","content":"Because of PowerBI requirement, I would choose for Spart Cluster.","timestamp":"1585362540.0","poster":"zenomas"}],"poster":"zenomas","comment_id":"68515","content":"I think it should be Spark. If not why?"},{"upvote_count":"4","poster":"qeqewqe","content":"why not spark? DAG is thhere too","timestamp":"1583531640.0","comment_id":"60080"},{"poster":"Laredo","upvote_count":"4","timestamp":"1582052880.0","comment_id":"52213","content":"yes, I agree with this"},{"timestamp":"1581895500.0","poster":"avestabrzn","upvote_count":"2","comments":[{"poster":"PHaringsNL","upvote_count":"2","content":"The DAG is the convincing part why it should be Storm in this","comment_id":"52042","timestamp":"1582025520.0"}],"comment_id":"51411","content":"Does everybody agree with this?"}],"answer_ET":"B","answer_images":[],"question_images":[],"answers_community":[],"question_id":84,"choices":{"D":"HDInsight Apache Hadoop cluster using MapReduce","A":"Azure SQL Data Warehouse","C":"Azure Stream Analytics","B":"HDInsight Apache Storm cluster","E":"HDInsight Spark cluster"},"isMC":true,"timestamp":"2020-02-17 00:25:00","topic":"2","answer_description":"Storm runs topologies instead of the Apache Hadoop MapReduce jobs that you might be familiar with. Storm topologies are composed of multiple components that are arranged in a directed acyclic graph (DAG). Data flows between the components in the graph. Each component consumes one or more data streams, and can optionally emit one or more streams.\nPython can be used to develop Storm components.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview","unix_timestamp":1581895500,"answer":"B"},{"id":"HCoeMYxUZ1L4uzExOn0z","answers_community":[],"topic":"2","isMC":false,"question_id":85,"unix_timestamp":1618046520,"discussion":[{"poster":"unidigm","content":"Apache Sqoop, Apache Hive, Ambari Hive View","upvote_count":"4","comment_id":"367469","timestamp":"1622063400.0"},{"content":"For Process, I think it should be hive \nFor Download, the answer seems correct. But instead of 'Ambari Hive View', I think it should be 'Apache Hive View'","comment_id":"355545","timestamp":"1620827100.0","upvote_count":"1","poster":"Kratik"},{"timestamp":"1618562340.0","content":"For Process it should be 'Hive' as it provide full storage mechanism","comment_id":"336865","upvote_count":"1","poster":"Hassan_Mazhar_Khan"},{"content":"can't be \"hive\" for process task?","comment_id":"332418","upvote_count":"2","timestamp":"1618046520.0","poster":"tucho"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0018100001.png"],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0018000001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/49778-exam-dp-200-topic-2-question-16-discussion/","question_text":"HOTSPOT -\nA company is deploying a service-based data environment. You are developing a solution to process this data.\nThe solution must meet the following requirements:\n✑ Use an Azure HDInsight cluster for data ingestion from a relational database in a different cloud service\n✑ Use an Azure Data Lake Storage account to store processed data\n✑ Allow users to download processed data\nYou need to recommend technologies for the solution.\nWhich technologies should you use? To answer, select the appropriate options in the answer area.\nHot Area:\n//IMG//","timestamp":"2021-04-10 11:22:00","answer_ET":"","exam_id":65,"answer_description":"Box 1: Apache Sqoop -\nApache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.\nAzure HDInsight is a cloud distribution of the Hadoop components from the Hortonworks Data Platform (HDP).\nIncorrect Answers:\nDistCp (distributed copy) is a tool used for large inter/intra-cluster copying. It uses MapReduce to effect its distribution, error handling and recovery, and reporting.\nIt expands a list of files and directories into input to map tasks, each of which will copy a partition of the files specified in the source list. Its MapReduce pedigree has endowed it with some quirks in both its semantics and execution.\nRevoScaleR is a collection of proprietary functions in Machine Learning Server used for practicing data science at scale. For data scientists, RevoScaleR gives you data-related functions for import, transformation and manipulation, summarization, visualization, and analysis.\n\nBox 2: Apache Kafka -\nApache Kafka is a distributed streaming platform.\nA streaming platform has three key capabilities:\nPublish and subscribe to streams of records, similar to a message queue or enterprise messaging system.\nStore streams of records in a fault-tolerant durable way.\nProcess streams of records as they occur.\nKafka is generally used for two broad classes of applications:\nBuilding real-time streaming data pipelines that reliably get data between systems or applications\nBuilding real-time streaming applications that transform or react to the streams of data\n\nBox 3: Ambari Hive View -\nYou can run Hive queries by using Apache Ambari Hive View. The Hive View allows you to author, optimize, and run Hive queries from your web browser.\nReferences:\nhttps://sqoop.apache.org/\nhttps://kafka.apache.org/intro\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-use-hive-ambari-view","answer":""}],"exam":{"isBeta":false,"isMCOnly":false,"id":65,"name":"DP-200","isImplemented":true,"lastUpdated":"12 Apr 2025","numberOfQuestions":228,"provider":"Microsoft"},"currentPage":17},"__N_SSP":true}