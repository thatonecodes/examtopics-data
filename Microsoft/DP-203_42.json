{"pageProps":{"questions":[{"id":"3yr3zzc3sUqEmu5iyrkQ","question_id":206,"answer_description":"","answer":"B","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/68447-exam-dp-203-topic-2-question-51-discussion/","choices":{"C":"High Concurrency with Auto Termination","A":"Standard with Auto Termination","D":"Standard with Autoscaling","B":"High Concurrency with Autoscaling"},"unix_timestamp":1640225820,"timestamp":"2021-12-23 03:17:00","exam_id":67,"discussion":[{"content":"Selected Answer: B\nB is correct answer.\nHigh concurrency cluster cannot terminated, so C is wrong. \nStandard cluster cannot shared by multiple tasks, so A and D are wrong.","upvote_count":"19","poster":"Canary_2021","comments":[{"upvote_count":"17","poster":"HaBroNounen","comment_id":"513147","content":"\"High Concurrency clusters do not terminate automatically by default.\"\nbut u can change that default so your argument about C is incorrect..\nLink: https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-mode","timestamp":"1640852820.0"}],"timestamp":"1640225820.0","comment_id":"507503"},{"poster":"_lene_","comment_id":"864774","content":"Selected Answer: C\nThe cluster does auto-scaling by default. Auto-termination should be set up manually","timestamp":"1680965460.0","upvote_count":"5"},{"upvote_count":"1","poster":"DixonDavis","content":"I think its C. Firstly I couldn't find anything related to autoscaling at cluster level . The autoscaling is enabled by default at the pipeline. So I don't find out anything that mention how to set up auto scaling for High concurrency cluster so ruling out B.","timestamp":"1732228200.0","comment_id":"1316048"},{"comment_id":"1001517","upvote_count":"1","content":"Selected Answer: B\nB is correct","poster":"kkk5566","timestamp":"1694088660.0"},{"content":"Here is my take on this. Autoscaling addresses first requirement i-e latency and Auto terminate addresses third requirement i-e cost. Now we have to implement both of them manually, meaning none of them is by default. But in the question it says reduce cost without compromising other requirements. So if we go for auto-termination that means we are not autoscaling, compromising the first requirement i-e the latency requirement. therefore the given option is correct i-e B: High Concurrency with autoscaling","upvote_count":"4","poster":"UzairMir","comment_id":"951607","timestamp":"1689344520.0"},{"content":"Selected Answer: B\nAgree to B","comment_id":"897883","poster":"Reloadedvn","timestamp":"1684097160.0","upvote_count":"1"},{"comment_id":"642770","upvote_count":"1","content":"Selected Answer: B\nB is correct","poster":"Deeksha1234","timestamp":"1659679800.0"},{"poster":"jz10","timestamp":"1651822020.0","upvote_count":"4","content":"Selected Answer: B\nJust because auto termination is eligible for high concurrency clusters, doesn't mean we have to use it.\nA key requirement is to \"minimize query latency\", which makes autoscaling more favorable.\n\nRef: \"Workloads can run faster compared to a constant-sized under-provisioned cluster.\"\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-size-and-autoscaling","comment_id":"597609"},{"upvote_count":"4","poster":"Amsterliese","comment_id":"583193","content":"High Concurrency clusters can be configured with auto termination (I just checked). BUT: The questions says: reduce costs WITHOUT compromising the other requirements. So I would still go for autoscaling, since there is no answer option that offers both (autoscaling and auto termination)","timestamp":"1649490600.0"},{"content":"Answer should be B as per below \nThe key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies.Autoscaling clusters can reduce overall costs compared to a statically-sized cluster.","upvote_count":"4","timestamp":"1647708900.0","comment_id":"571169","poster":"sunithagsk"},{"upvote_count":"3","comment_id":"564013","poster":"alex1491","timestamp":"1646826900.0","content":"Selected Answer: C\ni try it and it´s possible to create a cluster with auto termination."},{"timestamp":"1645250760.0","comment_id":"550700","poster":"AngelJP","upvote_count":"2","content":"Selected Answer: C\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-mode\n- High Concurrency clusters do not terminate automatically by default.\n- A Standard cluster is recommended for a single user."},{"comment_id":"547409","timestamp":"1644875220.0","content":"I would say C. High Concurrency with Auto Termination.\nAlthough the default is no auto terminate we can still overwrite that setting.","poster":"danjelmt","upvote_count":"2"},{"comment_id":"545969","upvote_count":"2","timestamp":"1644680940.0","poster":"BK10","content":"B is correct answer.\nHigh concurrency cluster cannot AUTO terminated"},{"timestamp":"1641330900.0","content":"Auto terminate for high concurrency cluster is possible. But due to the 2nd point 'Maximize the number of users that can run queries on the cluster at the same time', I will go with option B. High Concurrency and Auto Scaling","comments":[{"comment_id":"569207","poster":"kamil_k","upvote_count":"3","timestamp":"1647451260.0","content":"Also to minimise query latency you don't want to have to wait for a cluster to spin up after it terminates"}],"comment_id":"516999","poster":"ANath","upvote_count":"5"},{"upvote_count":"1","timestamp":"1641101820.0","comment_id":"514845","poster":"venkatibm","content":"it's correct"}],"question_text":"You are designing an Azure Databricks cluster that runs user-defined local processes.\nYou need to recommend a cluster configuration that meets the following requirements:\n✑ Minimize query latency.\n✑ Maximize the number of users that can run queries on the cluster at the same time.\n✑ Reduce overall costs without compromising other requirements.\nWhich cluster type should you recommend?","answers_community":["B (72%)","C (28%)"],"answer_ET":"B","topic":"2","question_images":[]},{"id":"OMICECALDlVNSpreVbcP","answer_description":"Box 1: @trigger().startTime -\nstartTime: A date-time value. For basic schedules, the value of the startTime property applies to the first occurrence. For complex schedules, the trigger starts no sooner than the specified startTime value.\nBox 2: /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\nOne dataset per hour per deviceType.\n\nBox 3: Flatten hierarchy -\n- FlattenHierarchy: All files from the source folder are in the first level of the target folder. The target files have autogenerated names.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers https://docs.microsoft.com/en-us/azure/data-factory/connector-file-system","topic":"2","question_id":207,"exam_id":67,"timestamp":"2021-12-14 19:50:00","answer":"","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/67969-exam-dp-203-topic-2-question-52-discussion/","answers_community":[],"answer_ET":"","question_text":"HOTSPOT -\nYou are building an Azure Data Factory solution to process data received from Azure Event Hubs, and then ingested into an Azure Data Lake Storage Gen2 container.\nThe data will be ingested every five minutes from devices into JSON files. The files have the following naming pattern.\n/{deviceType}/in/{YYYY}/{MM}/{DD}/{HH}/{deviceID}_{YYYY}{MM}{DD}HH}{mm}.json\nYou need to prepare the data for batch data processing so that there is one dataset per hour per deviceType. The solution must minimize read times.\nHow should you configure the sink for the copy activity? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0023500001.png"],"discussion":[{"poster":"ItHYMeRIsh","comment_id":"501614","timestamp":"1639507800.0","comments":[{"upvote_count":"4","poster":"Bro111","comments":[{"timestamp":"1670233140.0","upvote_count":"6","content":"It is not an option. It says /{deviceID}/out/{YYYY}/{MM}/{DD}/{HH}.json","comment_id":"735811","poster":"sensaint"}],"content":"Why not /{deviceType}/out/{YYYY}/{MM}/{DD}/{HH}.json ?","timestamp":"1670009160.0","comment_id":"734022"}],"content":"The correct copy behavior is merge - not flatten hierarchy. \n\nThe question starts with a folder structure as the following:\n/{deviceType}/in/{YYYY}/{MM}/{DD}/{HH}/{deviceID}_{YYYY}{MM}{DD}HH}{mm}.json\n\nIt indicates there are multiple device ID JSON files per deviceType. Those need to be merged to get the target naming pattern - \"one file per device type per hour.\"\nThe target naming pattern is the following:\n/{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\n\nThe correct copy behavior is \"Merge\" because there are multiple files in the source folder that are merged into a single folder per device type per hour.","upvote_count":"106"},{"poster":"onyerleft","comment_id":"505793","upvote_count":"91","comments":[{"timestamp":"1656170040.0","content":"but, the solution must minimize read times, I think is @trigger().startTime","comment_id":"622166","upvote_count":"2","poster":"Davico93"}],"content":"1) @trigger().outputs.windowStartTime - this output is from a tumbling window trigger, and is required to identify the correct directory at the /{HH}/ level. Using windowStartTime will give the hour with complete data. The @trigger().startTime is for a schedule trigger, which corresponds to the hour for which data has not arrived yet. \n2) /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json is the naming pattern to achieve an hourly dataset for each device type.\n3) Multiple files for each device type will exist on the source side, since the naming pattern starts with {deviceID}... so the files must be merged in the sink to create a single file per device type.","timestamp":"1640055300.0"},{"content":"1. @trigger().startTime\n- the window trigger requires a window, this does not necessarally is the case in the question\n- the oders are using commas. Could be @pipeline().TriggerTime, bit it uses a dot.\n\n2. /yyyy/mm/dd/hh_deviceType.json\n- deviceID would not aggregate all devices by type\n- /deviceType.json would not split by hour\n- hh.json would not split by device type\n\n3. Merge\n- Dynamic content requires reading the content\n- Flatten would not merge data into one file","comment_id":"1287345","timestamp":"1726928100.0","poster":"renan_ineu","upvote_count":"1"},{"upvote_count":"10","comments":[{"upvote_count":"2","content":"Merge is a better answer.\nFlatten hierarchy does not reduce the number of files in the directory.\n https://www.linkedin.com/pulse/copy-behaviour-activity-adf-lokesh-sharma/","comment_id":"1137976","poster":"j888","timestamp":"1706827920.0"}],"timestamp":"1705596300.0","content":"Got this question on my exam on january 17, I answered these\n@trigger().StartTime\n /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json \nMerge files\nI passed :)","poster":"ELJORDAN23","comment_id":"1126065"},{"content":"The files must be MERGED > each hour, \nso on @trigger().outputs.windowStartTime => start time of the window\n\nThe author made 2/3 errors on this Q, grrr :)\n\n@trigger().outputs.windowStartTime: \nGives the start time of the current window n\n\n@trigger().StartTime: \nGives the start time of each trigger within that window n","poster":"blazy002","timestamp":"1703169540.0","comment_id":"1102615","upvote_count":"2"},{"upvote_count":"6","poster":"phydev","timestamp":"1698764280.0","comment_id":"1058965","content":"Was on my exam today (31.10.2023)."},{"comment_id":"1007700","timestamp":"1694698800.0","poster":"Chemmangat","content":"It's @trigger().outputs.windowStartTime\nRef : https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables\nUnder Tumbling Window Trigger","upvote_count":"1"},{"poster":"kkk5566","comment_id":"1001525","upvote_count":"1","timestamp":"1694089020.0","content":"1) @trigger().outputs.windowStartTime\n/{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\nMerge"},{"comment_id":"944658","poster":"pavankr","timestamp":"1688647800.0","content":"what exactly you want to \"FLATTEN\"??? You need to Merge files. period.","upvote_count":"2"},{"content":"1) @trigger().outputs.windowStartTime\n2) /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\n3) Merge","comment_id":"909816","poster":"rocky48","upvote_count":"9","timestamp":"1685412060.0"},{"poster":"rzeng","timestamp":"1666942800.0","upvote_count":"6","comment_id":"706241","content":"1. windowstarttime\n2. yyyy/mm/dd/hh_devicetype.json\n3. Merge"},{"content":"1) @trigger().outputs.windowStartTime\n2) /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json \n3) Merge\nagree with onyer","poster":"Deeksha1234","upvote_count":"3","comment_id":"642890","timestamp":"1659693240.0"},{"upvote_count":"10","content":"Of course it is a merge, can't believe the official provided answers are so wrong ... Who wrote that","poster":"Rafafouille76","comment_id":"553733","timestamp":"1645539540.0","comments":[{"content":"I know it's almost as bad as Microsoft documentation about Azure.. That's why we see so much confusion over so many questions","upvote_count":"4","timestamp":"1647451860.0","comment_id":"569218","poster":"kamil_k"}]},{"content":"Would you have to delay the tumbling processing by 60minutes to pick up data that hasn't arrived for that hour yet?","comment_id":"525868","timestamp":"1642431420.0","poster":"Jaws1990","upvote_count":"1"},{"poster":"Canary_2021","timestamp":"1641346200.0","content":"The batch job runs in Data Factory should use Tumbling window trigger, so system variable trigger().outputs.windowStartTime should be passed in as the parameter.","upvote_count":"3","comment_id":"517102"},{"poster":"jv2120","content":"data is generated every 5 min but output needs every 1 hour/device it, it needs to merge files to achieve this.","comment_id":"504243","upvote_count":"2","timestamp":"1639835400.0"},{"upvote_count":"2","timestamp":"1639831500.0","poster":"tony4fit","comment_id":"504218","comments":[{"content":"think logically what flatten and merge means and what is asked in the question","comment_id":"613140","upvote_count":"4","poster":"Aditya0891","timestamp":"1654674720.0"}],"content":"The answers are correct. Flatten Hierarchy. https://vmfocus.com/2019/01/09/using-azure-data-factory-to-copy-data-between-azure-file-shares-part-1/"}],"unix_timestamp":1639507800,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0023600001.png"]},{"id":"PPZ3yQ4jVwMcjmsXSe2r","timestamp":"2021-12-10 08:55:00","unix_timestamp":1639122900,"question_id":208,"question_text":"DRAG DROP -\nYou are designing an Azure Data Lake Storage Gen2 structure for telemetry data from 25 million devices distributed across seven key geographical regions. Each minute, the devices will send a JSON payload of metrics to Azure Event Hubs.\nYou need to recommend a folder structure for the data. The solution must meet the following requirements:\n✑ Data engineers from each region must be able to build their own pipelines for the data of their respective region only.\n✑ The data must be processed at least once every 15 minutes for inclusion in Azure Synapse Analytics serverless SQL pools.\nHow should you recommend completing the structure? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","answer":"","isMC":false,"topic":"2","exam_id":67,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0023800001.jpg"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0023700003.jpg"],"discussion":[{"comments":[{"upvote_count":"6","content":"ItHYMeRIsh you re a genius man","comment_id":"658597","timestamp":"1677862740.0","poster":"ML_Novice"},{"poster":"nmnm22","content":"thats such a cool explanation, i aspire to have the same critical thinking skills u have","comment_id":"865867","upvote_count":"4","timestamp":"1696892400.0"},{"upvote_count":"1","poster":"Deeksha1234","timestamp":"1675694640.0","comment_id":"643385","content":"Agree, correct answer"},{"content":"I'm geting ~500k folders for 1*12*30*24*60. I get your point that heirarchy would be a lot cleaner.","poster":"sdokmak","comment_id":"608237","upvote_count":"1","timestamp":"1669611720.0"},{"content":"Excellent explanation:D","comment_id":"534684","upvote_count":"1","poster":"alex623","timestamp":"1659004260.0"},{"content":"This is correct","timestamp":"1655773080.0","comment_id":"505794","poster":"onyerleft","upvote_count":"3"},{"poster":"DataEX","timestamp":"1691619600.0","content":"The correct structure answer will have 561.600 folders per year.","upvote_count":"1","comment_id":"803822"},{"comment_id":"536685","content":"Brilliantly explained thank you. correct answer.","timestamp":"1659239220.0","poster":"PallaviPatel","upvote_count":"2"},{"upvote_count":"8","comment_id":"931246","poster":"auwia","content":"In IoT workloads, there can be a great deal of data being ingested that spans across numerous products, devices, organizations, and customers. It's important to pre-plan the directory layout for organization, security, and efficient processing of the data for down-stream consumers. A general template to consider might be the following layout:\n\n{Region}/{SubjectMatter(s)}/{yyyy}/{mm}/{dd}/{hh}/\n\nFor example, landing telemetry for an airplane engine within the UK might look like the following structure:\n\nUK/Planes/BA1293/Engine1/2017/08/11/12/\nIn this example, by putting the date at the end of the directory structure, you can use ACLs to more easily secure regions and subject matters to specific users and groups. If you put the date structure at the beginning, it would be much more difficult to secure these regions and subject matters. For example, if you wanted to provide access only to UK data or certain planes, you'd need to apply a separate permission for numerous directories under every hour directory. This structure would also exponentially increase the number of directories as time went on.","timestamp":"1703317800.0","comments":[{"timestamp":"1703317860.0","upvote_count":"4","content":"https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#monitor-telemetry","comment_id":"931248","poster":"auwia"},{"comment_id":"1149706","poster":"moneytime","timestamp":"1723588860.0","content":"This explains it.I retire to best practice.Lol...Thanks","upvote_count":"1"}]}],"timestamp":"1655224800.0","upvote_count":"208","comment_id":"501606","poster":"ItHYMeRIsh","content":"The correct answer is \n{raw/regionID}/{YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}.json\n\n{raw/regionID} is the first level because raw is the container name for the raw data. RegionID follows it for ease of managing security.\n\n{YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}.json instead of {deviceID}/{YYYY}/{MM}/{DD}/{HH}/{mm}.json. The primary reason is that you want your namespace structure to have as few folders as high up and narrow those down as you get deeper into your structure.\n\nFor example, if you have 1 year worth of data and 25 million devices, using {YYYY}/{MM}/{DD}/{HH}/{mm}/ results in 2.1 million folders (1 year * 12 months * 30 days [estimate] * 24 hours * 60 minutes). If you start your folder structure with {deviceID}, you end up with 25 million folders - one for each device - before you even get to including the date in the hierarchy."},{"timestamp":"1654840500.0","poster":"gf2tw","comment_id":"498390","content":"raw/RegionId should be in the first box as raw is the name of your container. Furthermore, putting RegionId as one of the first foldernames allows easy partitioning and simpler RBAC for the Data Engineers.","comments":[{"timestamp":"1655210820.0","poster":"SAli12","comment_id":"501433","content":"Yes I agree, raw/regionId --> timestamp --> deviceId.json","upvote_count":"5"}],"upvote_count":"15"},{"poster":"Sathya_sree","timestamp":"1743091440.0","content":"Answer Area Position Value\nFirst value regionID\nSecond value raw\nThird value {YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}","comment_id":"1410955","upvote_count":"1"},{"timestamp":"1703316600.0","comment_id":"931233","content":"I'll follow best practice from Microsoft:\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#monitor-telemetry\n\nSo: /raw/regionid/deviceid/YYYY/MM/DD/HH\n(without minutes).","poster":"auwia","upvote_count":"5"},{"timestamp":"1701317040.0","upvote_count":"1","content":"The correct answer is\n{raw/regionID}/{YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}.json","comment_id":"909819","poster":"rocky48"},{"content":"I think that link will help us to find the correct answer:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices\n\nThe given example for a directory structure is: *{Region}/{SubjectMatter(s)}/{yyyy}/{mm}/{dd}/{hh}/*","upvote_count":"4","poster":"georgich87","comment_id":"578684","timestamp":"1664516340.0"},{"timestamp":"1663851660.0","poster":"wwdba","comment_id":"573033","content":"{raw/regionID}/{YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}.json","upvote_count":"2"},{"upvote_count":"3","timestamp":"1661184000.0","content":"IMHO {YYYY}/{MM}/{DD}/{HH}/{regionID/raw}/{deviceID}.json (given answer) is correct. Please pay attention that there is no minutes {mm} course it is not supported by Time format \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/blob-storage-azure-data-lake-gen2-output","poster":"staniopolis","comment_id":"553884"},{"timestamp":"1661168100.0","comment_id":"553706","upvote_count":"3","poster":"staniopolis","content":"{raw/regionID}/{YYYY}/{MM}/{DD}/{HH}/{deviceID}.json\n\nTime Format [optional]: if the time token is used in the prefix path, specify the time format in which your files are organized. Currently the only supported value is\nHH."},{"comments":[{"timestamp":"1658532720.0","upvote_count":"2","poster":"Canary_2021","content":"Still feel {raw/RegionID} / {YYYY/MM/DD/mm} /{DeviceID} is correct. Just have some questions after compare answers of question 54.","comment_id":"530189"}],"content":"Question 54: the correct answer of box 2 is {YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\nOne dataset per hour per deviceType.\n\nSo looks like regionid and deviceid should be put after {YYYY}/{MM}/{DD}/{HH}/{mm} .\n\n{YYYY}/{MM}/{DD}/{HH}/{mm}/{raw/regionID}/{deviceID}.json","timestamp":"1658531940.0","upvote_count":"1","poster":"Canary_2021","comment_id":"530183"},{"upvote_count":"2","content":"The Question says : Each minute, the devices will send a JSON payload. That means the data is demarcated by region and by minutes.\n{raw/RegionID} / {YYYY/MM/DD/mm} /{DeviceID}","timestamp":"1657255260.0","poster":"engrbrain","comment_id":"519349"},{"upvote_count":"2","comment_id":"513027","poster":"SabaJamal2010AtGmail","timestamp":"1656561900.0","content":"/{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}."},{"comments":[{"timestamp":"1703317740.0","content":"without minute info.","poster":"auwia","comment_id":"931244","upvote_count":"2"}],"timestamp":"1655196120.0","poster":"PA7","comment_id":"501293","upvote_count":"4","content":"raw/regionid - > DeviceId -> YYYY/MM/dd/HH-mm"},{"content":"{raw/regionID}/{deviceID}/{YYYY}/{MM}/{DD}/{HH}{mm} imo.","comment_id":"501185","comments":[{"comments":[{"comment_id":"949792","content":"IMO, with {mm}. \nOtherwise, every HH dir will have 25mil (device) * 60 (freq. of incoming files)","upvote_count":"1","poster":"tsmk","timestamp":"1705070880.0"}],"timestamp":"1703317680.0","poster":"auwia","comment_id":"931243","content":"without minute in my opinion","upvote_count":"2"}],"upvote_count":"4","timestamp":"1655185680.0","poster":"mr_corte"}],"answers_community":[],"answer_description":"Box 1: {raw/regionID}\nBox 2: {YYYY}/{MM}/{DD}/{HH}/{mm}\nBox 3: {deviceID}\nReference:\nhttps://github.com/paolosalvatori/StreamAnalyticsAzureDataLakeStore/blob/master/README.md","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/67521-exam-dp-203-topic-2-question-53-discussion/"},{"id":"cwTPwDYddJadRWqtEDLu","question_id":209,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0024100001.png","https://www.examtopics.com/assets/media/exam-media/04259/0024200001.jpg"],"exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/68068-exam-dp-203-topic-2-question-54-discussion/","timestamp":"2021-12-15 20:02:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0023900001.png","https://www.examtopics.com/assets/media/exam-media/04259/0024000001.png"],"answer_ET":"","answer_description":"Box 1: WHERE EventType='HeartBeat'\nBox 2: ,TumblingWindow(Second, 5)\nTumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals.\nThe following diagram illustrates a stream with a series of events and how they are mapped into 10-second tumbling windows.\n\nIncorrect Answers:\n,SessionWindow.. : Session windows group events that arrive at similar times, filtering out periods of time where there is no data.\nReference:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/session-window-azure-stream-analytics https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics","question_text":"HOTSPOT -\nYou are implementing an Azure Stream Analytics solution to process event data from devices.\nThe devices output events when there is a fault and emit a repeat of the event every five seconds until the fault is resolved. The devices output a heartbeat event every five seconds after a previous event if there are no faults present.\nA sample of the events is shown in the following table.\n//IMG//\n\nYou need to calculate the uptime between the faults.\nHow should you complete the Stream Analytics SQL query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"upvote_count":"96","content":"I think the right answers should be WHERE EventType='HeartBeat' and Session window. If we want to calculate the uptime between the faults, we must use session window for each device, we know that will be receiving events for each 5 seconds if there is no error, so when an error occurs (or if we reach the maximum size of the window) then a new event will not be received within the next 5 seconds and the window will close, calculating the uptime. However if We use Tumbling window, it´s not possible to calculate the uptime beyond 5 seconds","comment_id":"502432","comments":[{"poster":"Pian12345","upvote_count":"1","content":"totally agree with this","timestamp":"1731399060.0","comment_id":"1310484"},{"timestamp":"1645754940.0","content":"I concur!","upvote_count":"1","comment_id":"555694","poster":"ovokpus"},{"content":"Yes this sounds right","timestamp":"1640056680.0","upvote_count":"3","poster":"onyerleft","comment_id":"505799"},{"upvote_count":"2","content":"what happen if the event continues and the 50,000 second finishes? you cannot count that as a fault event","timestamp":"1656034440.0","comment_id":"621395","comments":[{"content":"Sorry, you are right @Fer079!","poster":"Davico93","upvote_count":"2","timestamp":"1656035940.0","comment_id":"621401"}],"poster":"Davico93"},{"comments":[{"upvote_count":"4","timestamp":"1643806500.0","comment_id":"538704","content":"when we are filtering out the faults then we expect to receive right events every 5 seconds, so if we don´t receive an event within 5 seconds then we know that there is a fault and the session windows will close (because it has been set to 5 seconds) calculating the time in this session window","comments":[{"content":"It actually says when a fault occurs the device will repeat the same message every 5 seconds so there actually is never a time where you do not receive a message every 5 seconds but you may receive a message within 5 seconds in case of an error.","comment_id":"601357","comments":[{"poster":"Aditya0891","comment_id":"616868","content":"yeah it will repeat the message but not with event type as heartbeat. So the where condition works here","timestamp":"1655312520.0","upvote_count":"1"}],"timestamp":"1652483220.0","upvote_count":"1","poster":"nefarious_smalls"}],"poster":"Fer079"},{"comment_id":"881190","content":"we are trying to calculate here the uptime i.e. when the device was up and running correctly, and exclude the failures; the session window will group 'hearbeat'events which happened within 5 seconds from each other; if any failure happens, the next event with hearbeat will be happening after 6 seconds (e.g. hearbeat 1 sec, failure 2 sec, heartbeat 7 sec) -> because they write in the question that it always is 5 seconds between events, unless failure happens (obviously a failure of a device is happening on no schedule, so it can happen as soon as you plug in your device to measure heartbeat);","timestamp":"1682491260.0","comments":[{"poster":"mamahani","upvote_count":"1","timestamp":"1682491260.0","content":"but after failure its emitting also signal only after 5 seconds, even if failure persists; so the next event will be minimum after 6 seconds from the previous one; with windows session of timeout of 5 seconds, the windows will just timeout; and as soon as devices restarts a new session will be counted , until next failure; if no failure occurs, the session will last almost 14 hrs (max duration set to 50000 seconds i.e. around 14hrs); so imo condition on eventype is correct, and it should be grouped by sessionwindow; here is some useful docs on the concepts https://www.passio-consulting.com/post/azure-stream-analytics-windowing-functions#:~:text=The%20hopping%20window%20is%20similar,information%20in%20another%20time%20frame.\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns","comment_id":"881191"}],"upvote_count":"1","poster":"mamahani"}],"upvote_count":"8","timestamp":"1642514940.0","poster":"romanzdk","content":"How it can be WHERE EventType='HearBeat'? If we filter out all the faults how can you calculate the time between the faults? I would go for BC","comment_id":"526688"},{"content":"This link is relevant here https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#session-windows","timestamp":"1676049120.0","comment_id":"804598","upvote_count":"1","poster":"yogiazaad"}],"timestamp":"1639594920.0","poster":"Fer079"},{"timestamp":"1640380740.0","comment_id":"508871","content":"My answer is:\nQuestion 1: B. Use LAG function as a filter to only filter out the events that switch from 'HeartBeat' to fault or witch from fault to 'HeartBeat'.\nQuestion 2: C. No matter if there is a fault, device always sends message every 5min. Calculate the uptime between the faults don't need any window here. Any duration > 5s should between fault line and heartbeat line should be part of items that need to count into to calculate duration.","comments":[{"comment_id":"535642","upvote_count":"8","content":"You cannot use the LAG function here because the \"partition by\" by deviceId is not included here, so the change between the status could be between different devices. This LAG function is evaluated before the \"group by\" clause of the query.\nIf you see the Microsoft documentation:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics\nIt says clearly that \"LAG isn't affected by predicates in the WHERE clause, join conditions in the JOIN clause, or grouping expressions in the GROUP BY clause of the current query because it's evaluated before those clauses.\"","poster":"Fer079","timestamp":"1643481540.0","comments":[{"content":"LAG does not require the PARTITION BY this is optional..","comment_id":"585908","poster":"ubaldo1002","upvote_count":"2","timestamp":"1649954340.0"},{"timestamp":"1682487960.0","comment_id":"881154","poster":"mamahani","upvote_count":"1","content":"you do not need partition by with LAG function; its an optional parameter; however in this scenario this is not the reason why we should not be using this function; with LAG we will receive in the query result only the \"transition\" events i.e. the device works correctly (eventype'heartbeat;) and then there is fault ('fault')-> we would receive only the record with \"faul\" (as its different then previous line event i.e. heartbeat; by this one record we will not know how long the device was operational correctly, because we dont have these records anymore; we need to have 'startting ' record for correctly operating device with heartbeat event , and this for every singe \"re-start' after the fault; LAG function would be good to calculate e.g. the increasing heartbeat by comparing the heartbeat of previous records with current one; but not in this user case;"}]}],"poster":"Canary_2021","upvote_count":"24"},{"comment_id":"1293627","upvote_count":"1","poster":"8ac3742","timestamp":"1728168540.0","content":"SELECT DeviceID,\n MIN(EventTime) AS StartTime,\n MAX(EventTime) AS EndTime,\n DATEDIFF(second, MIN(EventTime), MAX(EventTime)) AS duration_in_seconds\nFROM input TIMESTAMP BY EventTime\nWHERE EventType = 'HeartBeat'\nGROUP BY DeviceID, TumblingWindow(second, 5)\nHAVING DATEDIFF(second, MIN(EventTime), MAX(EventTime)) > 5"},{"poster":"renan_ineu","upvote_count":"1","timestamp":"1726984500.0","comment_id":"1287567","content":"1. You want \"between faults\" and between faults there are heartbeats. So EventType must be heartbeat.\n\n2. Usually I'd go with Session (check link), but this session timing is bugging me. Why 50k seconds for a window limit? I will probably have a global uptime, not uptimes between faults... So I'll go with tumbling.\n\nhttps://learn.microsoft.com/en-us/stream-analytics-query/session-window-azure-stream-analytics"},{"comment_id":"1271828","poster":"RG_123","upvote_count":"2","timestamp":"1724530740.0","content":"Chat GPT - EventType = 'Heartbeat', Session Window\n\nTo calculate the uptime between faults in the Azure Stream Analytics SQL query, you should complete the query by using the SessionWindow function, which groups events based on a period of inactivity, allowing you to calculate durations of uptime between faults.\n\nHere's how to complete the query:\n\nFilter the HeartBeat events: You should filter only for HeartBeat events because these indicate the system is running without faults.\nUse SessionWindow: This function groups events into sessions based on gaps of inactivity. In this scenario, a session is defined by the HeartBeat events, which will end when a fault occurs.\nCalculate the duration: The DATEDIFF function is used to calculate the time difference between the start and end of each session, giving you the uptime between faults."},{"upvote_count":"2","comment_id":"1001532","content":"WHERE EventType='HeartBeat' and Session window.","poster":"kkk5566","timestamp":"1694089260.0"},{"poster":"rocky48","content":"1. Where EventType = 'HeartBeat'\n2. SessionWindow","upvote_count":"4","timestamp":"1685412360.0","comment_id":"909821"},{"comment_id":"886798","timestamp":"1682980680.0","content":"1. Where EventType = 'HeartBeat'\n2. SessionWindow","poster":"SinSS","upvote_count":"1"},{"comments":[{"upvote_count":"1","comment_id":"652470","poster":"dom271219","timestamp":"1661578200.0","content":"Sorry ignore it"}],"timestamp":"1661577900.0","comment_id":"652467","poster":"dom271219","content":"The where clause must be EvenType != 'HearBeat' otherwise you're not counting the uptime between the fault","upvote_count":"1"},{"upvote_count":"1","timestamp":"1659793620.0","content":"Agree with Fer079 , EventType='HeartBeat' and Session window is correct","comment_id":"643409","poster":"Deeksha1234"},{"comment_id":"622335","timestamp":"1656215040.0","upvote_count":"1","content":"WHERE EventType='HeartBeat' is definitely correct as you would need to filter out other events to calculate the uptime.\nIf you look at the example in link https://docs.microsoft.com/en-us/stream-analytics-query/session-window-azure-stream-analytics \nit would be crystal clear that sessionwindow is the right answer and @iooj (a lot of thanks) has already tested it","poster":"uzairahm"},{"content":"tricky but instructive question.","upvote_count":"2","comment_id":"579326","timestamp":"1648789620.0","poster":"sparkchu"},{"comment_id":"528996","poster":"iooj","content":"I created a Stream Analytics job and tested all combinations and here is my answer.\nWith a tumbling window, you will never be able to accumulate the correct interval. \nThe session is suitable here, but if the session closes earlier (by timeout) than the event occurs, then it will also fail to accumulate. So please note that in the timeout should be 6, not 5. A working version: EventType='HeartBeat' and SessionWindow(second, 6, 50000). But... \nP.S. In the data example on the screenshot, the difference is generally indicated in minutes, in this situation, none of the answers will work, you will need to change seconds to minutes.","comments":[{"comment_id":"605955","timestamp":"1653299100.0","upvote_count":"1","content":"Thanks for testing it, but I think your conclusion is wrong.\nWe should calculate the difference (without any limitation). If you use SessionWindow with a timeout of 6 you limit this functionality. You get the right answer for the data in the table but what happens if you have a failure after >6 seconds?\nI think Canary_2021 is right -> B, C\n\nP.S. :-D didn't recognized it... but would say that this is a typo in the table.","poster":"MadEgg"}],"upvote_count":"13","timestamp":"1642751220.0"},{"upvote_count":"1","comment_id":"525929","content":"B and A?","timestamp":"1642435560.0","poster":"romanzdk"},{"poster":"engrbrain","content":"The answer is BC. Every T_SQL Group by Query that needs to calculate max based on certain criteria should use the HAVING function to group that criteria","timestamp":"1641624420.0","comment_id":"519352","upvote_count":"5"},{"timestamp":"1640093220.0","comment_id":"506132","upvote_count":"4","content":"For me the session window suits for the given scenario. Also no device ID has been considered in the given answer, which is essential for calculating the uptime period per device","poster":"MFR"}],"topic":"2","answer":"","isMC":false,"unix_timestamp":1639594920,"answers_community":[]},{"id":"e3Gfk9eZNAqxMvB3sIGM","discussion":[{"timestamp":"1703250900.0","comments":[{"timestamp":"1706521260.0","comment_id":"535359","upvote_count":"4","content":"We all hope man","poster":"anto69"},{"content":"Username suits youe wis! :D","poster":"CodingOwl","comment_id":"709084","upvote_count":"2","timestamp":"1730462340.0"}],"upvote_count":"50","poster":"bad_atitude","comment_id":"507101","content":"I wish you a DP203 as easy as this question folks"},{"upvote_count":"4","comment_id":"643410","poster":"Deeksha1234","content":"Selected Answer: A\nA is correct","timestamp":"1722952080.0"},{"content":"Selected Answer: A\nCorrect","timestamp":"1705507620.0","upvote_count":"3","comment_id":"525930","poster":"romanzdk"},{"poster":"leandrors","comment_id":"506576","upvote_count":"3","timestamp":"1703209500.0","content":"Selected Answer: A\nCorrect"},{"comment_id":"504737","content":"Selected Answer: A\nCorrect","timestamp":"1702976400.0","poster":"Will_KaiZuo","upvote_count":"4"}],"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/68281-exam-dp-203-topic-2-question-55-discussion/","exam_id":67,"question_id":210,"answer_ET":"A","timestamp":"2021-12-19 10:00:00","question_text":"You are creating a new notebook in Azure Databricks that will support R as the primary language but will also support Scala and SQL.\nWhich switch should you use to switch between languages?","unix_timestamp":1639904400,"answer_description":"","answers_community":["A (100%)"],"topic":"2","answer_images":[],"isMC":true,"choices":{"A":"%<language>","C":"\\\\[<language >]","D":"\\\\(<language >)","B":"@<Language >"},"answer":"A"}],"exam":{"lastUpdated":"12 Apr 2025","isMCOnly":false,"numberOfQuestions":384,"isBeta":false,"isImplemented":true,"provider":"Microsoft","name":"DP-203","id":67},"currentPage":42},"__N_SSP":true}