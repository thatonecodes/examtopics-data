{"pageProps":{"questions":[{"id":"9i3HAEpkTidhxe7Kafym","isMC":true,"discussion":[{"poster":"chaoxes","upvote_count":"15","comment_id":"247824","content":"C. Gremlin API\nWhen we talk about relationships and/or edges/nodes, Gremin API is the answer","timestamp":"1608365640.0"},{"content":"identify the relationships between users\nAnswer is C","comment_id":"235682","timestamp":"1607169300.0","upvote_count":"7","poster":"syu31svc"}],"answer_ET":"C","answer":"C","answer_images":[],"unix_timestamp":1607169300,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/38894-exam-dp-201-topic-1-question-19-discussion/","timestamp":"2020-12-05 12:55:00","exam_id":66,"topic":"1","question_id":11,"answer_description":"Gremlin features fast queries and traversals with the most widely adopted graph query standard.\nReference:\nhttps://docs.microsoft.com/th-th/azure/cosmos-db/graph-introduction?view=azurermps-5.7.0","choices":{"D":"Cassandra","A":"MongoDB","C":"Gremlin","B":"Table"},"answers_community":[],"question_text":"You are designing a data store that will store organizational information for a company. The data will be used to identify the relationships between users. The data will be stored in an Azure Cosmos DB database and will contain several million objects.\nYou need to recommend which API to use for the database. The API must minimize the complexity to query the user relationships. The solution must support fast traversals.\nWhich API should you recommend?"},{"id":"2Nw2R1ccx6hVI7UTJGaZ","timestamp":"2020-12-16 20:09:00","answer_images":[],"topic":"1","choices":{"B":"No","A":"Yes"},"isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are designing an HDInsight/Hadoop cluster solution that uses Azure Data Lake Gen1 Storage.\nThe solution requires POSIX permissions and enables diagnostics logging for auditing.\nYou need to recommend solutions that optimize storage.\nProposed Solution: Implement compaction jobs to combine small files into larger files.\nDoes the solution meet the goal?","url":"https://www.examtopics.com/discussions/microsoft/view/40084-exam-dp-201-topic-1-question-2-discussion/","answer_ET":"A","discussion":[{"comment_id":"245867","upvote_count":"7","timestamp":"1608145740.0","content":"Correct answer","poster":"chaoxes"},{"poster":"Deepu1987","comment_id":"295560","timestamp":"1613886360.0","content":"Somewhat similar to above qn \nhttps://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance#structure-your-data-set","upvote_count":"2"}],"unix_timestamp":1608145740,"question_id":12,"exam_id":66,"question_images":[],"answer":"A","answers_community":[],"answer_description":"Depending on what services and workloads are using the data, a good size to consider for files is 256 MB or greater. If the file sizes cannot be batched when landing in Data Lake Storage Gen1, you can have a separate compaction job that combines these files into larger ones.\nNote: POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files. As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1. Avoiding small file sizes can have multiple benefits, such as:\n✑ Lowering the authentication checks across multiple files\n✑ Reduced open file connections\n✑ Faster copying/replication\n✑ Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices"},{"id":"8axyO1eZEDA7McMmdhvo","timestamp":"2020-03-10 17:54:00","answers_community":[],"question_text":"HOTSPOT -\nYou are designing a new application that uses Azure Cosmos DB. The application will support a variety of data patterns including log records and social media relationships.\nYou need to recommend which Cosmos DB API to use for each data pattern. The solution must minimize resource utilization.\nWhich API should you recommend for each data pattern? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"Log records: SQL -\n\nSocial media mentions: Gremlin -\nYou can store the actual graph of followers using Azure Cosmos DB Gremlin API to create vertexes for each user and edges that maintain the \"A-follows-B\" relationships. With the Gremlin API, you can get the followers of a certain user and create more complex queries to suggest people in common. If you add to the graph the Content Categories that people like or enjoy, you can start weaving experiences that include smart content discovery, suggesting content that those people you follow like, or finding people that you might have much in common with.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/social-media-apps","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0006800001.jpg"],"topic":"1","answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0006700001.jpg"],"isMC":false,"question_id":13,"url":"https://www.examtopics.com/discussions/microsoft/view/16139-exam-dp-201-topic-1-question-20-discussion/","discussion":[{"timestamp":"1608365700.0","comment_id":"247825","content":"Log records: SQL (because logs are row oriented)\nSocial Media: Gremlin (because it uses relationships)","upvote_count":"27","poster":"chaoxes"},{"comment_id":"362550","content":"propose solution is correct.\n\nCassandra API is mostly used to handle high volume and real time data while the requirement is related to log records, SQL API is sufficient in this terms. Gremlin API is appropriate when we are talking about connection between entities.\n\nReference: https://acloudguru.com/blog/engineering/azure-cosmos-db-apis-use-cases-and-trade-offs","upvote_count":"4","timestamp":"1621553880.0","poster":"cadio30"},{"timestamp":"1583859240.0","content":"why SQL over Cassandra for log records?","upvote_count":"3","comment_id":"61837","comments":[{"timestamp":"1584235380.0","upvote_count":"46","comment_id":"64102","poster":"z8zhong","content":"log data are row-oriented so SQL handle them better, Cassandra are mainly for column-oriented data"},{"content":"It also states in the documentation that Cassandra is only recommend to migrate existing Cassandra databases to CosmosDB. In all other cases, the SQL Api is recommended.\n\nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/3-analyze-the-decision-criteria","upvote_count":"40","poster":"kempstonjoystick","comment_id":"70066","timestamp":"1585732380.0"}],"poster":"klasius"}],"unix_timestamp":1583859240,"answer":"","exam_id":66},{"id":"pBv1z9qVUvstOw9xA8pW","topic":"1","answer_description":"Azure Blob Storage containers is a general purpose object store for a wide variety of storage scenarios. Blobs are stored in containers, which are similar to folders.\nIncorrect Answers:\nC: Azure Data Lake Storage is an optimized storage for big data analytics workloads.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-storage","question_images":[],"answer":"B","question_id":14,"answer_ET":"B","timestamp":"2020-03-22 03:36:00","exam_id":66,"answers_community":[],"discussion":[{"comment_id":"66734","comments":[{"comment_id":"450464","content":"RBAC Security on Azure Blob can be scoped at the container level or above. With two containers (one for raw data and one for curated data) without folders, it would be possible to manage the security. BUT the current use case states that: \" datascientists need access to specifics folders in the \"raw\" folder. You cannot manage security at this level with Azure Blob. You have to use Azure Data Lake with RBAC/ACLs. The right answer is C","timestamp":"1632421500.0","poster":"Marcus1612","upvote_count":"1"},{"comment_id":"70067","comments":[{"comments":[{"comment_id":"78006","timestamp":"1587576960.0","comments":[{"upvote_count":"16","content":"However, strictly speaking, in BLOB storage, data not stored in folders, just the name of the blob will include the folder name. So if the requirement is to store in folder, it have to be ADLS","timestamp":"1587660600.0","comments":[{"upvote_count":"11","content":"Also, in blob, without RBAC you can only grant permission to the level of container.","poster":"Leonido","comment_id":"78751","timestamp":"1587660960.0"}],"poster":"Leonido","comment_id":"78741"}],"upvote_count":"3","poster":"Yuri1101","content":"Agree, especially it is only required to handle standardized data. There is no need to use ADLS."},{"poster":"lingjun","content":"When an Azure role is assigned to an Azure AD security principal, Azure grants access to those resources for that security principal. Access can be scoped to the level of the subscription, the resource group, the storage account, or an individual container or queue. An Azure AD security principal may be a user, a group, an application service principal, or a managed identity for Azure resources.\nhttps://docs.microsoft.com/de-de/azure/storage/common/storage-auth-aad","upvote_count":"2","comment_id":"217798","timestamp":"1605172860.0"}],"comment_id":"74483","poster":"MLCL","upvote_count":"10","content":"There is the notion of public anonymous access in blob storage as well as shared access signatures, and of course RBAC can be implemented through Azure AD for Blobs and Queues, so the security requirements can be met.\nCheck this doc : https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-rbac-portal","timestamp":"1586866740.0"}],"upvote_count":"13","timestamp":"1585732500.0","content":"I agree, Azure Data Lake Stroage includes ACLs which can be applied to folder structures, which Blob Storage does not. Therefore the security requirements mean the answer should be ADLS","poster":"kempstonjoystick"},{"comment_id":"385261","upvote_count":"2","timestamp":"1624080540.0","content":"there is, it is called container.","poster":"tes"}],"upvote_count":"112","poster":"Sam9999","content":"Shouldn't answer be C, there is no concept of folders and folder permissions in Azure storage.","timestamp":"1584844560.0"},{"upvote_count":"45","poster":"HeB","comment_id":"79976","content":"Answer should definitely be C, Azure Data Lake Storage Gen2.","timestamp":"1587924360.0"},{"poster":"tes","upvote_count":"3","content":"The given answer is wrong and it should be C. The answer given states container is same as folder but it is not. A folder can have sub folders and access can be given only to sub folder. Where as in containers there are no sub containers hence the answer is wrong. Folder however can be given access in ADLS Gen2 using ACL so when we have a straight forward answer, why go with assumtion that 'container is same as folder'","timestamp":"1624080780.0","comment_id":"385262"},{"upvote_count":"1","content":"Azure Data Lake Store Gen2 is a superset of Azure Blob storage capabilities. In the list below, some of the key differences between ADLS Gen2 and Blob storage are summarized.\n\n ADLS Gen2 supports ACL and POSIX permissions allowing for more granular access control compared to Blob storage.\nADLS Gen2 introduces a hierarchical namespace. This is a true file system, unlike Blob Storage which has a flat namespace. This capability has a significant impact on performance, especially in big data analytics scenarios.\nADLS Gen2 is an HDFS-compatible store. This means that Apache Hadoop services can use data stored in ADLS Gen2. Azure Blob storage is not Hadoop-compatible.","comment_id":"375059","timestamp":"1622894400.0","poster":"azurenav"},{"content":"ADLS is the appropriate solution here as it has ACL function.","poster":"cadio30","upvote_count":"1","comment_id":"372269","timestamp":"1622594040.0"},{"timestamp":"1622113200.0","poster":"Arjun16","content":"In Question they mentioned about flat files and columnar optimized files(Binary Files) and Containers are similar to folders, so Azure storage is Correct","comment_id":"367826","upvote_count":"1"},{"comment_id":"362564","poster":"cadio30","content":"The requirements leads to using ADLS gen 2 as it can manage the folder level using ACL","timestamp":"1621557240.0","upvote_count":"1"},{"poster":"davita8","content":"C. Azure Data Lake Storage Gen2","upvote_count":"2","comment_id":"344845","timestamp":"1619635620.0"},{"upvote_count":"2","comment_id":"334407","timestamp":"1618288800.0","poster":"rmk4ever","content":"Columnar optimized file for Raw, enriched and curated structure with Folder level access\nAns is ADLS\nref: \nhttps://www.dremio.com/data-lake/adls/\nhttps://medium.com/microsoftazure/building-your-data-lake-on-adls-gen2-3f196fc6b430"},{"poster":"Deepu1987","content":"The given answer is correct as when you check the below link\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-storage it's clearly mentioned that ADLS can be used with certain restrictions it can be accessed via az synapse using poly base feature. There are certain performance tuning guidelines but in qn it's asked it need to be easily accessed by data scientistists as per the conditions we can go with blob storage","comment_id":"294222","timestamp":"1613739120.0","upvote_count":"1"},{"comment_id":"284803","content":"I will choose Azure Datalake on the only fact that the question did ask for columnar optimized files and this is available in Datalake and not Azure storage account","poster":"AyeshJr","upvote_count":"2","timestamp":"1612614900.0"},{"comment_id":"254106","upvote_count":"2","content":"A folder can be created in a blob (e.g. via button \"Create folder\" in the portal) but such a folder is virtual. Using Azure Storage Explorer (presently v1.17.0), one can verify that an SAS can be created on a blob container, but not on a folder within a blob. \nStill, multiple containers could be created where each container maps to one group of users in the security requirements. This is not forbidden in the question. If each container has one or more folders, all requirements would still be met, making answer B a \"minimum viable answer\". \nHowever, I agree that answer C is the best and most flexible. Using Azure Storage Explorer, one can easily verify that the option \"Manage Access Control Lists\" is available on an individual folder.","timestamp":"1609165980.0","poster":"mohowzeh"},{"comment_id":"204610","content":"The given answer is clearly incorrect. All the points that are mentioned in the questions are hints to use ADLS Gen 2.","timestamp":"1603450260.0","poster":"M0e","upvote_count":"3"},{"comment_id":"197642","poster":"monumentalcrankiness","upvote_count":"3","content":"The answer also mentions that the files are supposed to be explored by Data Scientists in curated folder. ADLS Gen 2 hooked up with Databricks or Azure Synapse Analytics is a ready-made solution for this kind of exploration.","timestamp":"1602416700.0"},{"upvote_count":"2","poster":"monumentalcrankiness","comment_id":"197634","timestamp":"1602416400.0","content":"I think correct answer should be ADLS Gen 2."},{"content":"raw, curated folder, folder level access all characteristics of ADLS","timestamp":"1598717340.0","poster":"yilpiz","comment_id":"169377","upvote_count":"6"},{"poster":"Bob123456","upvote_count":"1","content":"I believe there is actually only a single layer of containers. You can virtually create a \"file-system\" like layered storage, but in reality everything will be in 1 layer, the container in which it is.\nSo Answer should be DATA LAKE","comment_id":"157467","timestamp":"1597335000.0"},{"comments":[{"timestamp":"1597047600.0","poster":"Yaswant","upvote_count":"6","content":"*B* is correct","comment_id":"154299"}],"upvote_count":"3","content":"Here we are talking about storage solutions and granting permissions to the solution. Data lake is mostly suited for analytical workloads and in blobs we have a concept of virtual folders and we can create access policies in storage explorer for blobs. So C is correct (Just an intution).","timestamp":"1596864600.0","comment_id":"152871","poster":"Yaswant"},{"timestamp":"1590666840.0","content":"It is ADLS","poster":"Abhitm","comment_id":"97504","upvote_count":"8"}],"question_text":"You need to recommend a storage solution to store flat files and columnar optimized files. The solution must meet the following requirements:\n✑ Store standardized data that data scientists will explore in a curated folder.\n✑ Ensure that applications cannot access the curated folder.\n✑ Store staged data for import to applications in a raw folder.\n✑ Provide data scientists with access to specific folders in the raw folder and all the content the curated folder.\nWhich storage solution should you recommend?","choices":{"C":"Azure Data Lake Storage Gen2","D":"Azure SQL Database","B":"Azure Blob storage","A":"Azure Synapse Analytics"},"isMC":true,"answer_images":[],"unix_timestamp":1584844560,"url":"https://www.examtopics.com/discussions/microsoft/view/17150-exam-dp-201-topic-1-question-21-discussion/"},{"id":"IquVJvHXkkQFUtvwzBZZ","topic":"1","answer_description":"Choose a partition key that has a wide range of values and access patterns that are evenly spread across logical partitions. This helps spread the data and the activity in your container across the set of logical partitions, so that resources for data storage and throughput can be distributed across the logical partitions.\nChoose a partition key that spreads the workload evenly across all partitions and evenly over time. Your choice of partition key should balance the need for efficient partition queries and transactions against the goal of distributing items across multiple partitions to achieve scalability.\nCandidates for partition keys might include properties that appear frequently as a filter in your queries. Queries can be efficiently routed by including the partition key in the filter predicate.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview#choose-partitionkey","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0007000001.png"],"question_id":15,"answer":"A","answer_ET":"A","timestamp":"2020-04-01 11:19:00","exam_id":66,"answers_community":[],"discussion":[{"content":"Given there are 100 million orders in a 24 hour period, and there are only three catgegories, is Item/Id not a better solution, otherwise the category will cause significant hotspots?","poster":"kempstonjoystick","timestamp":"1585732740.0","upvote_count":"67","comment_id":"70068","comments":[{"upvote_count":"7","poster":"Taddi10","content":"I think if the id was an integrer (inremental foe exemple ) it can be a good partition key but with this format i think category is the best choice","comment_id":"150396","timestamp":"1596541920.0"}]},{"comment_id":"83728","comments":[{"poster":"Treadmill","upvote_count":"2","comment_id":"153508","timestamp":"1596965220.0","content":"D correct: Source as above quoted https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview"}],"poster":"MamadouNiang","content":"2 paragraphs below the link given in microsoft docs, there is an interesting answer :\n\nUsing item ID as the partition key\n\nIf your container has a property that has a wide range of possible values, it is likely a great partition key choice. One possible example of such a property is the item ID. For small read-heavy containers or write-heavy containers of any size, the item ID is naturally a great choice for the partition key.\n\nThe item ID is a great partition key choice for the following reasons:\nThere are a wide range of possible values (one unique item ID per item).\nBecause there is a unique item ID per item, the item ID does a great job at evenly balancing RU consumption and data storage.\nYou can easily do efficient point reads since you'll always know an item's partition key if you know its item ID.","timestamp":"1588612440.0","upvote_count":"35"},{"content":"Answer should be \"item/id\". You can find almost similar example in below link.\nhttps://docs.microsoft.com/en-us/learn/modules/monitor-and-scale-cosmos-db/5-partition-\nlesson","timestamp":"1621275600.0","upvote_count":"5","comment_id":"359771","comments":[{"poster":"MMM777","comment_id":"375220","upvote_count":"1","timestamp":"1622902140.0","content":"This example is definitely VERY similar to the question and explains why several of the proposed values are not good choices, and also shows \"Item/Id\" to be a decent choice."}],"poster":"IAMKPR"},{"content":"D. Item/id is the answer","upvote_count":"6","poster":"davita8","timestamp":"1619635920.0","comment_id":"344847"},{"comments":[{"content":"the item/id is the correct solution, regarding to the explanation in the link that you posted, all the documents related to the item/id will store in same partition.","poster":"BobFar","timestamp":"1621464660.0","comment_id":"361701","upvote_count":"1"}],"poster":"Deepu1987","timestamp":"1613988720.0","upvote_count":"1","content":"Given solution is right where we choose the item/category. It's explained in detail in the below link https://medium.com/walmartglobaltech/deep-dive-azure-cosmos-partitions-and-partitionkey-14e898f371cd this concept is of major focus as question may not be exactly asked in exam we need to need to know the concept of physical & logical partitions pre-requisites & Partition key as well.","comment_id":"296522"},{"timestamp":"1613742660.0","poster":"TaherAli2020","content":"If you use the Item/Category property as a partition key, then it has a small cardinality. Even if the documents are evenly distributed across the collection, for large collections, any category might outgrow a single partition.\n\nIf the categories aren't evenly distributed across the documents in the collection, then the problem is even worse. The dominant category restricts the ability of Azure Cosmos DB to scale.\n\nItem/Category is not a good choice for the partition key.\nhttps://docs.microsoft.com/en-us/learn/modules/monitor-and-scale-cosmos-db/5-partition-lesson","comment_id":"294284","comments":[{"content":"perfect! the link provided clear states the strategy of optimizing partition.","comment_id":"372290","timestamp":"1622598060.0","upvote_count":"2","poster":"cadio30"},{"poster":"tejasjoshi","comment_id":"399534","timestamp":"1625530140.0","upvote_count":"2","content":"Superb ! Its crystal clear now. Partition should be on Item/id. Requesting all to go through above link."},{"comment_id":"299756","content":"Nice link. It is exactly the case from the ex.","poster":"sturcu","upvote_count":"1","timestamp":"1614346200.0"},{"timestamp":"1620189420.0","poster":"TkSQL","content":"this link is the answer to all the confusion here","upvote_count":"3","comment_id":"349928"}],"upvote_count":"14"},{"upvote_count":"4","content":"From https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview#choose-partitionkey:\n\"Have a high cardinality. In other words, the property should have a wide range of possible values.\"\nD is the answer","comment_id":"238319","poster":"syu31svc","timestamp":"1607436300.0"},{"upvote_count":"5","content":"item/id for sure.\nsee the section \"Propose partition key values for the collection\" at: \nhttps://docs.microsoft.com/en-us/learn/modules/monitor-and-scale-cosmos-db/5-partition-lesson","comment_id":"225226","timestamp":"1606079460.0","poster":"brcdbrcd"},{"upvote_count":"1","content":"Candidates for partition keys might include properties that appear frequently as a filter in your queries. Queries can be efficiently routed by including the partition key in the filter predicate.\nItem ID will not appear as a filter most likely","poster":"lingjun","timestamp":"1605174240.0","comment_id":"217810","comments":[{"timestamp":"1605174540.0","content":"For small read-heavy containers or write-heavy containers of any size, Item-ID is naturally good choice. In this case, we have balanced read/write workload","upvote_count":"1","comment_id":"217814","poster":"lingjun"}]},{"poster":"M0e","content":"Given the discussion here: https://docs.microsoft.com/en-us/learn/modules/monitor-and-scale-cosmos-db/5-partition-lesson, \"Item/id\" is the correct answer","timestamp":"1603451280.0","comment_id":"204619","upvote_count":"12"},{"poster":"monumentalcrankiness","comment_id":"204100","content":"I shall go with D. Item/Id\nItem/Category is out. It will only create 3 logical partitions, that also unevenly distributed. A logical distribution has a size cap of 20 GB. With 100 million orders per day, it won't be very hard to reach that limit quickly.\n \nOrderTime is out. 16:30 to 17:00 spike shall create a hotspot problem.\n \nItem/Currency is out. Only 1 value \"USD\" will result in everything cramming up one logical partition.\n \nOnly Item/id is left. So this is the answer.","timestamp":"1603352880.0","upvote_count":"8"},{"poster":"Shivam131","content":"your partition key should:\n\nBe a property that has a value which does not change. If a property is your partition key, you can't update that property's value.\nHave a high cardinality. In other words, the property should have a wide range of possible values.\nSpread request unit (RU) consumption and data storage evenly across all logical partitions. This ensures even RU consumption and storage distribution across your physical partitions.","comment_id":"187701","timestamp":"1601129100.0","upvote_count":"1"},{"timestamp":"1597180020.0","content":"https://docs.microsoft.com/en-us/azure/cosmos-db/partition-data#logical-partitions\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview\nhttps://www.examtopics.com/exams/microsoft/dp-201/view/6/\n\nD Item/ID\n\nCategory doesn’t distribute RU evenly across partitions. Low cardinality.","poster":"Ash666","upvote_count":"3","comment_id":"155860"},{"comment_id":"152874","timestamp":"1596865680.0","poster":"Yaswant","content":"Consider we have provisioned a throughput of 1200 request units and we know that throughput can be provisioned in cosmos db only at a container level or at a database level.\nIn our case we consider our online retailer to be Walkart. Now walkart has an account in cosmosdb and they have a document db with coresql api. Now walkart has created a container named orders in their cosmos account and provisioned 1200ru's.\nNow consider the case of choosing a partition key. Considering they have 1200 customer id-s and if they use id as partition key they will have their throughput spread across partitions which makes their unused throughput in vain as customers come buy and go and it makes a hotspot. Now if we choose product category as partition we'll be having a balanced throughput and read-write.","upvote_count":"1"},{"comment_id":"142727","upvote_count":"1","content":"these comments causing further confusing for new bees as it's not able to draw whats final correct answer.. I would go by Item/Category only... this combo may not give repeated values as item would be different in same category.. item/id might create super heavy number of partitions","poster":"krisspark","timestamp":"1595594340.0"},{"content":"In this case A is correct indeed. See the reference and be aware of the read/write balancing. The read is as important as the throuput. \n\nPartition keys for read-heavy containers\nFor most containers, the above criteria is all you need to consider when picking a partition key. For large read-heavy containers, however, you might want to choose a partition key that appears frequently as a filter in your queries. Queries can be efficiently routed to only the relevant physical partitions by including the partition key in the filter predicate.\nIf most of your workload's requests are queries and most of your queries have an equality filter on the same property, this property can be a good partition key choice. For example, if you frequently run a query that filters on UserID, then selecting UserID as the partition key would reduce the number of cross-partition queries","comment_id":"122870","poster":"LeonLeon","timestamp":"1593443100.0","comments":[{"upvote_count":"1","timestamp":"1594733220.0","comment_id":"134869","poster":"Sudipta3009","content":"Ur explanation is correct"}],"upvote_count":"7"},{"timestamp":"1591944780.0","upvote_count":"3","poster":"BHAWS","content":"Choose a partition key that has a wide range of values,so the data is evenly spread across logical partitioning. Hence I suggest the answer is item/category","comment_id":"108446"},{"timestamp":"1590220680.0","comment_id":"94216","content":"No doubt it is item/ID. refer to https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview#choose-partitionkey. it is clearly addressed Using item ID as the partition key. partition key here is not partition used in DW.","poster":"henry_x","upvote_count":"5"},{"content":"Here Item/id - id refers to the unique id of each document in cosmos not the item id.","timestamp":"1588999800.0","poster":"azurearch","upvote_count":"1","comments":[{"poster":"azurearch","timestamp":"1589611500.0","content":"agree with ceasarrr","comment_id":"89770","upvote_count":"2"}],"comment_id":"85927"},{"poster":"ceasarrr","content":"Item/Id is the correct answer. Item/Category is not balanced (15k,35k,6k)","upvote_count":"19","timestamp":"1587285600.0","comment_id":"76310"},{"content":"I would say that \"category\" is correct only if we infer that it is likely to be commonly used in queries. But the question specifically says to optimize for throughput, so I'd think \"item/id\" would be more correct.","poster":"zb99","comments":[{"comments":[{"comment_id":"79525","content":"Disagree. Id here stands for \"Order ID\", not \"Item Id\", so you'll end up with 100M partitions, which will lead to high read latency - because of inter partition data movement.","timestamp":"1587823440.0","comments":[{"upvote_count":"4","comment_id":"127583","content":"It doesn't stand for 'Order ID'. It says: Item/id","timestamp":"1594024800.0","poster":"Israel2"}],"upvote_count":"2","poster":"Leonido"}],"timestamp":"1587159840.0","upvote_count":"3","comment_id":"75869","content":"Agree, only Item/Id can distribute data evenly since the difference in the number of items is huge among those three categories.","poster":"Yuri1101"}],"comment_id":"74932","timestamp":"1586968440.0","upvote_count":"5"},{"comments":[{"comment_id":"79941","content":"You're probably confusing partition key unique values with logical partitions, high cardinality isn't a bad thing, actually, it's good. The distribution of the partitions is handled behind the curtains, so if you have 1000 distinct values of 10mb each, they may end up into a single partition, considering the max size of a partition is about 10gb. So the correct answer is item/id, otherwise the partitions wouldn't be balanced as requested.","comments":[{"timestamp":"1589406840.0","upvote_count":"4","poster":"Niteen","comment_id":"88508","content":"You're saying correctly - High cardinality isn't a bad thing. It's not like Tablespace/ Table level partitioning. To choose the correct key for not only write but read purpose also. So, here most of the data divided into category wise. Hence, \"Category\" is the correct answer."},{"poster":"Piiri565","content":"Choose a partition key that has a wide range of values and access patterns that are evenly spread across logical partitions,\nAccording to this line , its clearly saying pattern , and there is no pattern in the id, so no logical partitioning with the id,,\nSo the given answer is correct","upvote_count":"3","comment_id":"218580","timestamp":"1605281160.0"}],"poster":"Tombarc","timestamp":"1587917220.0","upvote_count":"25"}],"upvote_count":"10","timestamp":"1586867160.0","poster":"MLCL","comment_id":"74486","content":"The answer is correct, imagine if in 24h, every kind of item has been ordered at least once, you will end up with 100k logical partitons, is that a good choice for partitioning ? i think partitionning by category is more efficient since we will have a 1h intensive read/write period and dispatching items in 3 partitions is less time consuming that dispatching on thousands."},{"upvote_count":"3","timestamp":"1586174880.0","poster":"samok","content":"Agree with kempstonjoystick","comment_id":"71762"},{"content":"i agree.","poster":"Ard","timestamp":"1585827060.0","comment_id":"70403","upvote_count":"4"}],"question_text":"Your company is an online retailer that can have more than 100 million orders during a 24-hour period, 95 percent of which are placed between 16:30 and 17:00.\nAll the orders are in US dollars. The current product line contains the following three item categories:\n✑ Games with 15,123 items\n✑ Books with 35,312 items\n✑ Pens with 6,234 items\nYou are designing an Azure Cosmos DB data solution for a collection named Orders Collection. The following documents is a typical order in Orders Collection.\n//IMG//\n\nOrders Collection is expected to have a balanced read/write-intensive workload.\nWhich partition key provides the most efficient throughput?","choices":{"D":"Item/id","A":"Item/Category","B":"OrderTime","C":"Item/Currency"},"isMC":true,"answer_images":[],"unix_timestamp":1585732740,"url":"https://www.examtopics.com/discussions/microsoft/view/17749-exam-dp-201-topic-1-question-22-discussion/"}],"exam":{"provider":"Microsoft","isBeta":false,"lastUpdated":"12 Apr 2025","id":66,"numberOfQuestions":206,"isImplemented":true,"isMCOnly":false,"name":"DP-201"},"currentPage":3},"__N_SSP":true}