{"pageProps":{"questions":[{"id":"M9cCR2H6c6hvPrFpbyJQ","topic":"2","question_images":[],"isMC":true,"question_text":"You have an Azure Data Factory pipeline that performs an incremental load of source data to an Azure Data Lake Storage Gen2 account.\nData to be loaded is identified by a column named LastUpdatedDate in the source table.\nYou plan to execute the pipeline every four hours.\nYou need to ensure that the pipeline execution meets the following requirements:\n✑ Automatically retries the execution when the pipeline run fails due to concurrency or throttling limits.\n✑ Supports backfilling existing data in the table.\nWhich type of trigger should you use?","discussion":[{"content":"Selected Answer: D\nD is correct answer. \nhttps://www.sqlshack.com/how-to-schedule-azure-data-factory-pipeline-executions-using-triggers/\nAzure Data Factory pipeline executions using Triggers: \n• Schedule Trigger: The schedule trigger is used to execute the Azure Data Factory pipelines on a wall-clock schedule.\n• Tumbling Window Trigger: Can be used to process history data. Also can define Delay, Max concurrency, retry policy etc.\n• Event-Based Triggers : The event-based trigger executes the pipelines in response to a blob-related event, such as creating or deleting a blob file, in an Azure Blob Storage","poster":"Canary_2021","upvote_count":"27","timestamp":"1671933600.0","comment_id":"508929"},{"upvote_count":"1","content":"Selected Answer: D\nD is correct.","comment_id":"926972","poster":"steveo123","timestamp":"1718756340.0"},{"content":"Selected Answer: D\nCorrect. As soon as you see backfill, its tumbling.","timestamp":"1717217880.0","upvote_count":"1","poster":"Ankit_Az","comment_id":"911738"},{"poster":"Deeksha1234","timestamp":"1691331360.0","comment_id":"643425","content":"Selected Answer: D\nD is correct","upvote_count":"1"},{"upvote_count":"3","timestamp":"1690303140.0","content":"(D)\nTumbling window trigger\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#tumbling-window-trigger\n\nRetry capability:\nIs Supported. Failed pipeline runs have a default retry policy of 0, or a policy that's specified by the user in the trigger definition. Automatically retries when the pipeline runs fail due to concurrency/server/throttling limits (that is, status codes 400: User Error, 429: Too many requests, and 500: Internal Server error).","comment_id":"636828","poster":"Sriramiyer92"},{"upvote_count":"2","poster":"dev2dev","content":"Selected Answer: D\nD is correct. Tumbling Window has more advance options for setting retry and concurrency policies which schedule doesnt have.","comment_id":"531904","timestamp":"1674628020.0"}],"question_id":211,"answer_description":"","answer_images":[],"exam_id":67,"choices":{"B":"on-demand","A":"event","C":"schedule","D":"tumbling window"},"answer":"D","url":"https://www.examtopics.com/discussions/microsoft/view/68558-exam-dp-203-topic-2-question-56-discussion/","unix_timestamp":1640397600,"timestamp":"2021-12-25 03:00:00","answers_community":["D (100%)"],"answer_ET":"D"},{"id":"F2bPVU1qDNrGK0Hr2UbH","question_id":212,"answer_ET":"AC","unix_timestamp":1641243780,"isMC":true,"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/69388-exam-dp-203-topic-2-question-57-discussion/","answer_images":[],"answer_description":"","timestamp":"2022-01-03 22:03:00","question_text":"You are designing a solution that will copy Parquet files stored in an Azure Blob storage account to an Azure Data Lake Storage Gen2 account.\nThe data will be loaded daily to the data lake and will use a folder structure of {Year}/{Month}/{Day}/.\nYou need to design a daily Azure Data Factory data load to minimize the data transfer between the two accounts.\nWhich two configurations should you include in the design? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point","exam_id":67,"answers_community":["AC (76%)","AD (16%)","8%"],"question_images":[],"choices":{"D":"Delete the source files after they are copied.","A":"Specify a file naming pattern for the destination.","C":"Filter by the last modified date of the source files.","B":"Delete the files in the destination before loading the data."},"answer":"AC","discussion":[{"comment_id":"540591","content":"Selected Answer: AC\nAC is correct, there is no point about deletion in source and might be the case that the data should stay in source too.","upvote_count":"15","timestamp":"1644001200.0","poster":"Philipp"},{"comment_id":"594206","timestamp":"1651204980.0","content":"Selected Answer: AC\nI think the C option has impact in data transfer, B are incorrect, D is irrelevant for the question, and A is a complement of the task","upvote_count":"7","poster":"necktru"},{"upvote_count":"1","content":"C & D \nThe question requested a solution that will reduce data transfer between the two systems .\nThe solution is thesame as how can REUNDANCY or Multiple Copies of the data be avoided during copying.\nExplanation \n1.). Deleting the source files after they are copied will keep track of the where to start the next copy .\n2.)Filter by the last modified date of the source files; This also helps to keep track of where to resuming file movement from","poster":"moneytime","comment_id":"1149831","timestamp":"1707885300.0"},{"upvote_count":"2","timestamp":"1694089500.0","content":"Selected Answer: AC\nAC is correct","poster":"kkk5566","comment_id":"1001541"},{"poster":"Spinozabubble","upvote_count":"5","content":"A. Specify a file naming pattern for the destination:\nBy specifying a file naming pattern for the destination files in the Azure Data Lake Storage Gen2 account, you can ensure that the files are organized and stored in a structured manner. This can help with data management and subsequent processing.\n\nC. Filter by the last modified date of the source files:\nBy filtering the source files based on the last modified date, you can select only the files that have been modified on the current day. This reduces the amount of data transferred and improves the efficiency of the data load process.","timestamp":"1683393900.0","comment_id":"890906"},{"upvote_count":"5","content":"Selected Answer: AC\nshould be AC","comment_id":"643460","timestamp":"1659807120.0","poster":"Deeksha1234"},{"content":"I will go for AC\nWhy not D? Cause they are not mentionned some cost opitmisation","timestamp":"1646821680.0","upvote_count":"3","poster":"Boumisasound","comment_id":"563954"},{"content":"AD are correct ?","upvote_count":"1","timestamp":"1645578780.0","comment_id":"554113","poster":"boopathi"},{"comment_id":"539944","content":"The requirement is to minimize the data transfer.\nIf we delete the files in source then there is no need to filter for daily load. So answer C,D is incorrect. Beside, there is no requirement to for minimizing the cost.\nTo my point of view, AC is correct because, even though filter by the modified date will take long time for lot of files, it won't impact the transfer.","upvote_count":"4","timestamp":"1643919420.0","poster":"Istiaque"},{"comment_id":"531914","upvote_count":"6","content":"Selected Answer: AD\nNormally we move the files after being processed, so it has to be D.","poster":"dev2dev","comments":[{"content":"is A,D correct","timestamp":"1643836080.0","comment_id":"539198","upvote_count":"2","poster":"yo1233"}],"timestamp":"1643092740.0"},{"upvote_count":"2","timestamp":"1641697440.0","comment_id":"519875","poster":"rainbowyu","comments":[{"poster":"djblue","upvote_count":"4","comment_id":"550189","content":"Minimizing the process time is not part of the question. \"Minimizing the data transfer\", whatever that is - either time or amount.","timestamp":"1645183860.0"}],"content":"Shout it be A &D as the requirement is to minimize the process time. Will option C take longer compared to D?"},{"comment_id":"517109","poster":"Canary_2021","content":"Selected Answer: CD\nEither C or D can realize daily incremental load. Not sure why need to setup both of them.","timestamp":"1641347340.0","upvote_count":"3"},{"poster":"edba","comments":[{"comment_id":"774078","upvote_count":"4","poster":"Dusica","timestamp":"1673581860.0","comments":[{"timestamp":"1727994900.0","upvote_count":"1","comment_id":"1292951","content":"why? \"Minimizing the data transfer\" has nothing to do with A.","poster":"drosen"}],"content":"YOU CAN'T GO WITHOUT A"}],"upvote_count":"2","content":"should it be C, D?","comment_id":"516047","timestamp":"1641243780.0"}]},{"id":"J7CHj0iY5d5FjPHbVMvO","isMC":true,"topic":"2","answer":"C","discussion":[{"comment_id":"527695","upvote_count":"8","timestamp":"1658239740.0","poster":"ANath","content":"Correct Answer."},{"timestamp":"1723031160.0","comment_id":"1143358","content":"Selected Answer: C\nAppend","poster":"Alongi","upvote_count":"1"},{"comment_id":"1001543","timestamp":"1709821500.0","content":"Selected Answer: C\nAppend","upvote_count":"1","poster":"kkk5566"},{"content":"Selected Answer: C\nC is correct","comment_id":"926994","poster":"steveo123","timestamp":"1702955400.0","upvote_count":"1"},{"content":"C is correct","poster":"Jiaa","comment_id":"761433","timestamp":"1688061780.0","upvote_count":"2"},{"content":"Selected Answer: C\nC is correct","poster":"Daniko","upvote_count":"2","timestamp":"1682026020.0","comment_id":"700310"},{"comment_id":"643461","poster":"Deeksha1234","content":"C is correct","timestamp":"1675711980.0","upvote_count":"1"},{"upvote_count":"1","poster":"Remedios79","comment_id":"619340","content":"Append is correct","timestamp":"1671555120.0"},{"content":"agree with append","timestamp":"1656439800.0","upvote_count":"4","poster":"bad_atitude","comment_id":"511545"}],"answer_ET":"C","url":"https://www.examtopics.com/discussions/microsoft/view/68821-exam-dp-203-topic-2-question-58-discussion/","timestamp":"2021-12-28 21:10:00","answers_community":["C (100%)"],"unix_timestamp":1640722200,"answer_description":"","choices":{"B":"complete","A":"update","C":"append"},"question_text":"You plan to build a structured streaming solution in Azure Databricks. The solution will count new events in five-minute intervals and report only events that arrive during the interval. The output will be sent to a Delta Lake table.\nWhich output mode should you use?","answer_images":[],"question_id":213,"question_images":[],"exam_id":67},{"id":"IrVcWdHtx71j5cALLmWd","answer":"A","question_images":[],"isMC":true,"unix_timestamp":1640728980,"url":"https://www.examtopics.com/discussions/microsoft/view/68835-exam-dp-203-topic-2-question-59-discussion/","answer_description":"","topic":"2","question_id":214,"discussion":[{"poster":"corebit","content":"Selected Answer: A\n\"Data flows are available both in Azure Data Factory and Azure Synapse Pipelines\"\n\"Use the derived column transformation to generate new columns in your data flow or to modify existing fields.\"\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-derived-column","comment_id":"511609","upvote_count":"23","timestamp":"1672264980.0"},{"poster":"Canary_2021","upvote_count":"9","content":"Selected Answer: B\nDerived Column cannot get DateTime (created or lastmodified datetime) of the files. \nGet Metadata activity can retrieve the DateTime of the files.\nso answer should be B.","comment_id":"527693","comments":[{"upvote_count":"3","comment_id":"785541","content":"Cant we just use the current datetime when the data is loaded. It doesnt say that we need to get data from the files. Just datetime which is kind of confusing. I will say, use derived column","poster":"Jerrie86","timestamp":"1706023800.0"},{"upvote_count":"10","poster":"Canary_2021","content":"If it is a real time process and pipeline is triggered to load data to table1 when file drop to container immediately, the created datetime of the file is similar as the pipeline process datetime. In this way Derived Column works. \nThe question is not clear.","timestamp":"1674144840.0","comment_id":"527698"}],"timestamp":"1674144480.0"},{"content":"A. Use this transformation to add any new columns to existing data.","poster":"kkk5566","upvote_count":"2","timestamp":"1724903220.0","comment_id":"992755"},{"comment_id":"643606","upvote_count":"2","poster":"Deeksha1234","content":"correct","timestamp":"1691387340.0"},{"content":"Selected Answer: A\nCorrect","poster":"Anandtr","comment_id":"643446","upvote_count":"2","timestamp":"1691337540.0"},{"upvote_count":"1","content":"What is the DateTime measuring? The DML transaction time or a file property?\n\nIf the measurement gives respect to the DML transaction time, you can use this: https://docs.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#currentTimestamp","poster":"mkthoma3","comment_id":"619330","timestamp":"1687271040.0"}],"exam_id":67,"answers_community":["A (74%)","B (26%)"],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.\nYou have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of\nTable1.\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\nSolution: In an Azure Synapse Analytics pipeline, you use a data flow that contains a Derived Column transformation.\nDoes this meet the goal?","timestamp":"2021-12-28 23:03:00","answer_ET":"A","choices":{"B":"No","A":"Yes"},"answer_images":[]},{"id":"SzIs302hAcQCKsdkkrvF","question_id":215,"answer_ET":"A","unix_timestamp":1629724260,"isMC":true,"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/60393-exam-dp-203-topic-2-question-6-discussion/","answer_images":[],"answer_description":"","timestamp":"2021-08-23 15:11:00","question_text":"You have an Azure Data Factory instance that contains two pipelines named Pipeline1 and Pipeline2.\nPipeline1 has the activities shown in the following exhibit.\n//IMG//\n\nPipeline2 has the activities shown in the following exhibit.\n//IMG//\n\nYou execute Pipeline2, and Stored procedure1 in Pipeline1 fails.\nWhat is the status of the pipeline runs?","exam_id":67,"answers_community":["A (87%)","13%"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0015500001.jpg","https://www.examtopics.com/assets/media/exam-media/04259/0015500002.jpg"],"choices":{"B":"Pipeline1 and Pipeline2 failed.","A":"Pipeline1 and Pipeline2 succeeded.","C":"Pipeline1 succeeded and Pipeline2 failed.","D":"Pipeline1 failed and Pipeline2 succeeded."},"discussion":[{"poster":"SaferSephy","upvote_count":"51","comment_id":"441434","content":"Correct answer is A. The trick is the fact that pipeline 1 only has a Failure dependency between de activity's. In this situation this results in a Succeeded pipeline if the Stored procedure failed. \n\nIf also the success connection was linked to a follow up activity, and the SP would fail, the pipeline would be indeed marked as failed.\n\nSo A.","timestamp":"1631101740.0","comments":[{"poster":"BK10","upvote_count":"1","content":"well explained! A is right","timestamp":"1644780600.0","comment_id":"546664"},{"upvote_count":"5","comment_id":"908390","timestamp":"1685257380.0","content":"correct ,Execute pipeline explained here https://youtu.be/Jkz1dtLrBE4","poster":"Ram0202"}]},{"comment_id":"430980","timestamp":"1629833460.0","upvote_count":"28","poster":"echerish","content":"Pipeline 2 executes Pipeline 1 if success set variable. Since Pipeline 1 exists it's a success\nPipeline 1 Stored procedure fails. If fails set variable. Since the expected outcome is fail the job runs successfully and sets variable1. \n\nAt least that's how I understand it"},{"content":"Selected Answer: A\nA is correct","comment_id":"1247373","poster":"Danweo","upvote_count":"1","timestamp":"1720885980.0"},{"poster":"moneytime","timestamp":"1707450300.0","comment_id":"1145223","content":"I chose D\nThere are two paths defined in the question. \nFor pipeline1, upon failure \nFor pipeline2; Upon success. \nNow ,for pipeline1, to respond to the run of pipeline 2 simply showed that they were connected. \n\nThis is a case of Upon if else block condition.\n\n\nWhen previous activity (procedure)failed : node Upon Success is skipped and its parent node failed; overall pipeline1 failed\n\nWhen previous activity succeeds: node Upon Success succeeded and node Upon Failure is skipped (and its parent node succeeds); overall pipeline succeeds.\nHence ,pipeline 2 succeeds \n\nThe pipeline2 can be visualized as thus\nP2->P1","upvote_count":"1"},{"upvote_count":"1","comment_id":"999310","poster":"kkk5566","timestamp":"1693906740.0","content":"Selected Answer: A\nCorrect answer is A."},{"upvote_count":"3","poster":"azaspirant","comment_id":"983186","content":"Selected Answer: B\nHow do we know which is the default behaviour? Its not mentioned anywhere wether there is a success dependency or failure dependency","timestamp":"1692238980.0"},{"upvote_count":"1","comment_id":"974699","poster":"akhil5432","timestamp":"1691411940.0","content":"Selected Answer: A\nOPTION \"A\""},{"poster":"joponlu","upvote_count":"2","comment_id":"906263","content":"Selected Answer: A\nCorrect!!!","timestamp":"1684982160.0"},{"poster":"mamahani","comment_id":"895025","content":"Selected Answer: A\nA is correct answer","upvote_count":"2","timestamp":"1683806340.0"},{"poster":"Billybob0604","timestamp":"1669980900.0","upvote_count":"1","content":"So the default behaviour is Failed dependency ? If so the answer is A. But it doesn't say this anywhere in the question.","comment_id":"733682"},{"comment_id":"679434","upvote_count":"1","poster":"NoobTester","timestamp":"1664170200.0","content":"Answer is correct.\nThis article helped: https://www.sqlshack.com/dependencies-in-azure-data-factory/"},{"poster":"Deeksha1234","upvote_count":"1","comment_id":"639606","timestamp":"1659170940.0","content":"Selected Answer: A\nA is correct"},{"upvote_count":"1","poster":"SebK","timestamp":"1647978420.0","content":"Selected Answer: A\nCorrect","comment_id":"573197"},{"content":"Selected Answer: A\nA correct:\nPipeline 1 is in try catch sentence --> Success\nPipeline 2 --> Success\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#try-catch-block","poster":"AngelJP","upvote_count":"6","timestamp":"1647457860.0","comment_id":"569281","comments":[{"poster":"AvSUN","timestamp":"1693984320.0","content":"Thanks","comment_id":"1000325","upvote_count":"1"}]},{"timestamp":"1643455200.0","content":"Selected Answer: A\nA correct. I agree with SaferSephy's comments below.","poster":"PallaviPatel","comment_id":"535415","upvote_count":"3"},{"timestamp":"1642401180.0","poster":"dev2dev","content":"A is correct. Pipeline 1 is connected to Set variable to Failure node/event. Its like handling exceptions/errors in programming language. Without Failure node, it would be treated as failed.","comment_id":"525500","upvote_count":"2"},{"upvote_count":"1","poster":"VeroDon","timestamp":"1641148620.0","comment_id":"515138","content":"Selected Answer: A\nCorrect"},{"timestamp":"1640532060.0","comment_id":"509669","upvote_count":"1","poster":"JSSA","content":"Correct answer is A"},{"upvote_count":"1","comment_id":"496103","content":"Selected Answer: A\ncorrect","poster":"rashjan","timestamp":"1638888000.0"},{"upvote_count":"5","poster":"medsimus","timestamp":"1634201400.0","comment_id":"461924","content":"Correct answer , I tested it in synapse . the first activity failed but the pipeline succeded"},{"poster":"Oleczek","upvote_count":"4","timestamp":"1630412340.0","content":"Just checked it myself on Azure, answer A is correct.","comment_id":"436319"},{"upvote_count":"5","timestamp":"1630299360.0","poster":"wdeleersnyder","comment_id":"435082","content":"I'm not seeing this... what's not being called out is if Pipeline 2 has a dependency on Pipeline 1. It happens all the time where two pipelines run; one runs, the other fails.\n\nIt should be D in my opinion."},{"comment_id":"432868","poster":"gangstfear","content":"The answer must be B","upvote_count":"2","timestamp":"1630051380.0"},{"upvote_count":"1","content":"Can someone please explain why the answer is A?","comment_id":"429992","timestamp":"1629724260.0","comments":[{"comment_id":"525504","upvote_count":"1","content":"if you look at the green and red squares, they are called Success and Failure events, in psuedo code , pipeline can be read as \"On Error Set Variable\", where as pipeline 2 has \"On Sucess Set Variable\"","timestamp":"1642401300.0","poster":"dev2dev"},{"content":"Pipeline1 has the failure dependency","timestamp":"1629979800.0","upvote_count":"2","comment_id":"432274","poster":"Ayanchakrain"}],"poster":"JohnMasipa"}],"answer":"A"}],"exam":{"isBeta":false,"isImplemented":true,"lastUpdated":"12 Apr 2025","numberOfQuestions":384,"id":67,"isMCOnly":false,"provider":"Microsoft","name":"DP-203"},"currentPage":43},"__N_SSP":true}