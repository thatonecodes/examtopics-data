{"pageProps":{"questions":[{"id":"sVKL0c0Omux3L4HcuZ3x","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/63470-exam-dp-300-topic-1-question-19-discussion/","answer_ET":"","topic":"1","discussion":[{"upvote_count":"8","comment_id":"717255","poster":"Ciupaz","content":"Question not for DP-300 exam.","timestamp":"1731495840.0"},{"content":"answer is correct. \n\nbelow video illustrate that:\nhttps://youtu.be/-DjOsJsIlA4?t=1266","comment_id":"655975","poster":"Icyb3r","upvote_count":"4","timestamp":"1725178500.0"},{"poster":"Backy","timestamp":"1720827480.0","content":"The sequence is not complete\n\nWhat is the point of step #1 to create master key if there is no step #2 to create a credential?\n\nThe external data source requires a credential and not master key\nWithout the credential, the list does not make sense\n\nThe list Create a master key on the database\n\n\nCREATE DATABASE SCOPED CREDENTIAL AzureStorageCredential\nWITH IDENTITY = 'user', Secret = '<azure_storage_account_key>';","comment_id":"630726","upvote_count":"1"},{"upvote_count":"2","comment_id":"542899","timestamp":"1707375720.0","poster":"tesen_tolga","content":"This is a DP-203 question."},{"timestamp":"1696243500.0","content":"Is it correct?","upvote_count":"1","comment_id":"456013","poster":"o2091","comments":[{"timestamp":"1699007520.0","upvote_count":"3","content":"Why dont you find out for yourself, and let us know.","comment_id":"472018","poster":"jerkyflexoff"},{"content":"I think it is correct.","timestamp":"1697070480.0","comment_id":"460855","poster":"U_C","upvote_count":"1"},{"content":"its correct.","poster":"o2091","comment_id":"483750","timestamp":"1700612520.0","upvote_count":"4"}]}],"question_id":11,"exam_id":68,"timestamp":"2021-10-02 12:45:00","question_text":"DRAG DROP -\nYou are creating a managed data warehouse solution on Microsoft Azure.\nYou must use PolyBase to retrieve data from Azure Blob storage that resides in parquet format and load the data into a large table called FactSalesOrderDetails.\nYou need to configure Azure Synapse Analytics to receive the data.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","unix_timestamp":1633171500,"question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0005200001.png"],"answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0005300001.png"],"answer_description":"To query the data in your Hadoop data source, you must define an external table to use in Transact-SQL queries. The following steps describe how to configure the external table.\nStep 1: Create a master key on database.\n1. Create a master key on the database. The master key is required to encrypt the credential secret.\n(Create a database scoped credential for Azure blob storage.)\nStep 2: Create an external data source for Azure Blob storage.\n2. Create an external data source with CREATE EXTERNAL DATA SOURCE..\nStep 3: Create an external file format to map the parquet files.\n3. Create an external file format with CREATE EXTERNAL FILE FORMAT.\nStep 4. Create an external table FactSalesOrderDetails\n4. Create an external table pointing to data stored in Azure storage with CREATE EXTERNAL TABLE.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-azure-blob-storage","answer":""},{"id":"XI7qnm0bd4nfp1maBOM7","answers_community":[],"answer_description":"Step 1: Attach the SSISDB database\nStep 2: Turn on the TRUSTWORTHY property and the CLR property\nIf you are restoring the SSISDB database to an SQL Server instance where the SSISDB catalog was never created, enable common language runtime (clr)\nStep 3: Open the master key for the SSISDB database\nRestore the master key by this method if you have the original password that was used to create SSISDB. open master key decryption by password = 'LS1Setup!' --'Password used when creating SSISDB'\nAlter Master Key Add encryption by Service Master Key\nStep 4: Encrypt a copy of the master key by using the service master key\nReference:\nhttps://docs.microsoft.com/en-us/sql/integration-services/catalog/ssis-catalog","discussion":[{"poster":"bakamon","comment_id":"909298","comments":[{"comments":[{"content":"1. Attach the SSISDB database (E)\n2. Turn on the TRUSTWORTHY property and the CLR property (D)\n3. Open the master key for the SSISDB database (F)\n4. Encrypt a copy of the master key by using the service master key (C)\n\nFirst step would be attach the SSISDB database, because you can't turn on TRUSTWORTHY setting before database actually exists. Then, turn on CLR. Last, according to below link, opening up master key and encrypting it with SMK - should be the last steps:\n\nhttps://learn.microsoft.com/en-us/sql/integration-services/catalog/ssis-catalog?view=sql-server-ver16#to-restore-the-ssis-database","timestamp":"1733177040.0","poster":"voodoo_sh","comment_id":"1321119","upvote_count":"1"}],"upvote_count":"2","timestamp":"1732141380.0","comment_id":"1315541","content":"Answer is correct.","poster":"learnazureportal"}],"content":"1. Attach the SSISDB database (E)\n2. Open the master key for the SSISDB database (F)\n3. Encrypt a copy of the master key by using the service master key ©\n4. Turn on the TRUSTWORTHY property and the CLR property (D)\n\nturning on the TRUSTWORTHY property and the CLR property (D) should be done after opening the master key for the SSISDB database (F) and encrypting a copy of the master key by using the service master key ©. This is because the TRUSTWORTHY property and CLR property are related to security and encryption, so it’s important to ensure that the master key is properly encrypted before enabling these properties.","upvote_count":"6","timestamp":"1685357700.0"},{"timestamp":"1726520760.0","comment_id":"1284945","upvote_count":"2","poster":"nova24","content":"Isn't this the right order?\nTurn on the TRUSTWORTHY property and the CLR property\nAttach the SSISDB database\nOpen the master key for the SSISDB database\nEncrypt a copy of the master key by using the service master key"},{"comment_id":"644802","content":"I believe this question should refer to the SSISDB being restored to another SQL Server as part of the answer is \"Turn on trustworthy property and CLR property\"","upvote_count":"1","timestamp":"1660110420.0","poster":"DSilsbury"},{"timestamp":"1636357080.0","upvote_count":"4","content":"The correct url iss:\nhttps://docs.microsoft.com/en-us/sql/integration-services/catalog/ssis-catalog?view=sql-server-ver15#to-restore-the-ssis-database","comment_id":"474157","poster":"quermi"},{"upvote_count":"1","poster":"o2091","content":"What would be the correct order?","comment_id":"454166","timestamp":"1632922020.0"},{"content":"It means master database lost with relevant information also lost so recreate a new SSIS catalog?","timestamp":"1631082960.0","comment_id":"441261","upvote_count":"2","poster":"Aggie0702"},{"upvote_count":"3","content":"Odd question.","comment_id":"363342","timestamp":"1621647240.0","poster":"gills"},{"upvote_count":"4","comments":[{"comment_id":"312718","poster":"Raffer","timestamp":"1615928520.0","content":"It doesn't say deleted; it says lost. Could be corruption + lost backups.","upvote_count":"1"}],"content":"This is such a weird question, it doesn't address the problem at hand which is a failure causing the master table being deleted. Why would that even happen? I would want to figure that out","poster":"rtk513","timestamp":"1615291380.0","comment_id":"306357"},{"timestamp":"1613909820.0","content":"Attaching the database and enabling CLR can be done at any order.\nIn this MS documentation, they first enable CLR, then attach/restore the database:\nhttps://docs.microsoft.com/en-us/sql/integration-services/catalog/ssis-catalog?view=sql-server-ver15","poster":"Corbiz","upvote_count":"3","comment_id":"295747"},{"timestamp":"1612880460.0","upvote_count":"3","poster":"yuck","content":"Link is 404, this is a correct one https://docs.microsoft.com/en-us/sql/integration-services/catalog/ssis-catalog","comment_id":"286875"}],"answer_ET":"","timestamp":"2021-02-09 15:21:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0002300001.png"],"question_id":12,"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/44361-exam-dp-300-topic-1-question-2-discussion/","topic":"1","answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0002400001.png"],"isMC":false,"unix_timestamp":1612880460,"question_text":"DRAG DROP -\nYou have SQL Server 2019 on an Azure virtual machine that contains an SSISDB database.\nA recent failure causes the master database to be lost.\nYou discover that all Microsoft SQL Server integration Services (SSIS) packages fail to run on the virtual machine.\nWhich four actions should you perform in sequence to resolve the issue? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct.\nSelect and Place:\n//IMG//","exam_id":68},{"id":"BqwvgFB5S8MlcfQUBzGe","isMC":false,"answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0005500001.png","https://www.examtopics.com/assets/media/exam-media/04275/0005600001.png"],"discussion":[{"comment_id":"456014","content":"Is it correct?","timestamp":"1727865960.0","poster":"o2091","comments":[{"timestamp":"1730559780.0","content":"Yea it is, I checked the reference!","upvote_count":"9","poster":"jerkyflexoff","comment_id":"471689"}],"upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/microsoft/view/63471-exam-dp-300-topic-1-question-20-discussion/","answer_description":"Box 1: adf_publish -\nBy default, data factory generates the Resource Manager templates of the published factory and saves them into a branch called adf_publish. To configure a custom publish branch, add a publish_config.json file to the root folder in the collaboration branch. When publishing, ADF reads this file, looks for the field publishBranch, and saves all Resource Manager templates to the specified location. If the branch doesn't exist, data factory will automatically create it. And example of what this file looks like is below:\n{\n\"publishBranch\": \"factory/adf_publish\"\n}\nBox 2: /dwh_barchlet/ adf_publish/contososales\nRepositoryName: Your Azure Repos code repository name. Azure Repos projects contain Git repositories to manage your source code as your project grows. You can create a new repository or use an existing repository that's already in your project.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/source-control","answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0005600002.png"],"exam_id":68,"answer_ET":"","question_text":"HOTSPOT -\nYou configure version control for an Azure Data Factory instance as shown in the following exhibit.\n//IMG//\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2021-10-02 12:46:00","unix_timestamp":1633171560,"topic":"1","question_id":13,"answers_community":[]},{"id":"r6ltVjsqfc9oPcdrpenf","topic":"1","answers_community":["B (69%)","A (31%)"],"exam_id":68,"answer_description":"","unix_timestamp":1630065600,"question_images":[],"discussion":[{"comment_id":"1559659","poster":"sincerebb","timestamp":"1744305660.0","content":"Selected Answer: B\nThe answer is correct.","upvote_count":"1"},{"content":"• Append Mode: Outputs only the new rows added during each trigger (i.e., each 5-minute interval in this case). This is ideal when you're continuously adding new records to the destination (e.g., a Delta Lake table) without updating existing data.","timestamp":"1727186160.0","comment_id":"1288629","upvote_count":"2","poster":"bingomutant"},{"upvote_count":"1","comments":[{"timestamp":"1685057340.0","content":"Why would you use a data lake if you do not want to keep old data?","comment_id":"906937","poster":"MS_KoolaidMan","upvote_count":"1"}],"timestamp":"1662020940.0","poster":"Icyb3r","comment_id":"655990","content":"Selected Answer: A\nIn question not mentioned that we need to keep old data. so the correct answer should be A"},{"poster":"nehima","comment_id":"652112","content":"Selected Answer: B\nThe answer is append","upvote_count":"2","timestamp":"1661498580.0"},{"comments":[{"poster":"zafara55","comment_id":"949801","content":"This is correct. Only events that arrive during the 5 minutes interval are kept. A is correct.","timestamp":"1689166320.0","upvote_count":"1"}],"content":"Solution is 'Complete'. The Question stated - \"will count new events in five-minute intervals and report only events that arrive during the interval. - Says 'NEW' events and 'ONLY ARRIVE DURING THE 5min INTERVAL' - so it can't be append because that would just add new data to the old data.","comment_id":"634948","upvote_count":"3","timestamp":"1658456220.0","poster":"New_Azure_User"},{"content":"Selected Answer: A\nThe preceding example continuously updates a table that contains the aggregate number of events by customer.","poster":"eric0718","timestamp":"1651175640.0","comment_id":"594025","upvote_count":"2"},{"timestamp":"1647075480.0","content":"Selected Answer: A\nComplete - The question doesn't state we need to keep existing data, only that we need from the given interval.","upvote_count":"1","poster":"CaptainJameson","comment_id":"565998"},{"content":"Selected Answer: B\nAppend","timestamp":"1645396680.0","comment_id":"552338","upvote_count":"2","poster":"AlCubeHead"},{"content":"Selected Answer: B\nAppend is the answer","comment_id":"536639","upvote_count":"2","timestamp":"1643598660.0","poster":"VinayakBudapanahalli"},{"comment_id":"501227","upvote_count":"1","content":"I think append","poster":"sqljuanito","timestamp":"1639471440.0"},{"upvote_count":"2","comment_id":"496161","content":"complete - You can also use Structured Streaming to replace the entire table with every batch. One example use case is to compute a summary using aggregation\n\nhttps://docs.databricks.com/delta/delta-streaming.html","poster":"ramelas","timestamp":"1638892680.0"},{"comment_id":"494156","content":"https://docs.microsoft.com/en-us/azure/databricks/getting-started/spark/streaming\nAppend Mode: Only new rows appended in the result table since the last trigger are written to external storage. This is applicable only for the queries where existing rows in the Result Table are not expected to change\nThen must be complete. append is if the table is static, with few changes.","timestamp":"1638692640.0","poster":"quermi","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nit should be append","comment_id":"480832","poster":"Manmeets27","comments":[{"poster":"ramelas","comment_id":"496156","upvote_count":"1","timestamp":"1638892200.0","content":"why? it should be complete because you just want to report events that happen in those 5 five minute intervals, you dont want data from the window of 10 minutes ago neither 15 minutes ago. so you replace the entire table to achieve that and it is more straight forward"}],"timestamp":"1637250960.0"},{"timestamp":"1631685480.0","poster":"Aggie0702","upvote_count":"4","content":"The answer is correct.","comment_id":"445028"},{"comment_id":"437198","poster":"Dawn7","upvote_count":"1","timestamp":"1630504620.0","content":"Which is the correct answer? I think complete as the question do not ask to add new recrods."},{"timestamp":"1630328580.0","poster":"maple580122","upvote_count":"2","content":"compete mode is correct. It requires count but not add a new record.","comment_id":"435514"},{"comments":[{"content":"why? it should be complete because you just want to report events that happen in those 5 minute intervals, you dont want data from the window of 10 minutes ago neither 15 minutes ago. so you replace the entire table to achieve that and it is more straight forward","upvote_count":"1","comment_id":"496159","poster":"ramelas","timestamp":"1638892260.0"}],"comment_id":"433038","content":"Correc answer is B - Append","timestamp":"1630065600.0","poster":"Gesia","upvote_count":"1"}],"choices":{"A":"complete","B":"append","C":"update"},"answer_ET":"B","answer":"B","question_text":"You plan to build a structured streaming solution in Azure Databricks. The solution will count new events in five-minute intervals and report only events that arrive during the interval.\nThe output will be sent to a Delta Lake table.\nWhich output mode should you use?","isMC":true,"timestamp":"2021-08-27 14:00:00","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/60855-exam-dp-300-topic-1-question-21-discussion/","question_id":14},{"id":"OFz6RiHdziPQnHQorCf6","question_text":"HOTSPOT -\nYou are performing exploratory analysis of bus fare data in an Azure Data Lake Storage Gen2 account by using an Azure Synapse Analytics serverless SQL pool.\nYou execute the Transact-SQL query shown in the following exhibit.\n//IMG//\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\nHot Area:\n//IMG//","topic":"1","answer_description":"Box 1: CSV files that have file named beginning with \"tripdata_2020\"\n\nBox 2: a header -\nFIRSTROW = 'first_row'\nSpecifies the number of the first row to load. The default is 1 and indicates the first row in the specified data file. The row numbers are determined by counting the row terminators. FIRSTROW is 1-based.\nExample: Option firstrow is used to skip the first row in the CSV file that represents header in this case (firstrow=2). select top 10 * from openrowset( bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.csv', format = 'csv', parser_version = '2.0', firstrow = 2 ) as rows\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-single-csv-file","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/60331-exam-dp-300-topic-1-question-22-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0005800001.png","https://www.examtopics.com/assets/media/exam-media/04275/0005900001.png"],"question_id":15,"unix_timestamp":1629699180,"timestamp":"2021-08-23 08:13:00","exam_id":68,"answer":"","discussion":[{"content":"The provided answer is correct.","timestamp":"1695090120.0","upvote_count":"10","comment_id":"447354","poster":"learnazureportal"},{"content":"This is a DP-203 question.","comment_id":"542902","poster":"tesen_tolga","upvote_count":"10","timestamp":"1707375780.0"},{"poster":"Ciupaz","upvote_count":"1","comment_id":"693971","timestamp":"1728829200.0","comments":[{"content":"if it is a DP-203 question, is a question like this still on the DP-300 Exam?","poster":"gboy115","upvote_count":"1","comment_id":"698460","comments":[{"timestamp":"1732989060.0","content":"Azure Data Lake Storage questions are not present in the MeasureUp practice test, and also in the DP-300 documentation.","upvote_count":"1","poster":"Ciupaz","comment_id":"731817"}],"timestamp":"1729278240.0"}],"content":"Exam DP-203: Data Engineering on Microsoft Azure"},{"content":"The answer is correct","poster":"ajaykadu555","comment_id":"641191","upvote_count":"1","timestamp":"1722590880.0"},{"content":"I would go with header. In all examples I've seen from ms they use first_row=2 when there is a header row, which is not used.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-single-csv-file","poster":"CaptainJameson","comment_id":"493905","upvote_count":"1","timestamp":"1701716880.0"},{"upvote_count":"1","comment_id":"481186","content":"2da- a data, \nits ok?","timestamp":"1700348820.0","poster":"o2091"},{"content":"2nd one should be a data\nhttps://docs.microsoft.com/en-us/sql/t-sql/functions/openrowset-transact-sql?view=sql-server-ver15","comments":[{"poster":"pepix74","comment_id":"430573","content":"the query stated to start from 2nd row, so the first row should be header (and therefore discarded) and 2nd the data - just my opinion","upvote_count":"2","timestamp":"1692863400.0"}],"poster":"azure2022","timestamp":"1692771180.0","upvote_count":"1","comment_id":"429709"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0005900002.png"],"answer_ET":"","isMC":false}],"exam":{"provider":"Microsoft","isBeta":false,"name":"DP-300","numberOfQuestions":360,"id":68,"lastUpdated":"12 Apr 2025","isImplemented":true,"isMCOnly":false},"currentPage":3},"__N_SSP":true}