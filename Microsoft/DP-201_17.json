{"pageProps":{"questions":[{"id":"jQBYprwlj4cb8muyp3Zx","unix_timestamp":1597663500,"answer_ET":"A","question_images":[],"answer":"A","choices":{"B":"No","A":"Yes"},"timestamp":"2020-08-17 13:25:00","question_id":81,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/28837-exam-dp-201-topic-2-question-20-discussion/","answer_images":[],"isMC":true,"answer_description":"If you need to transform data in a way that is not supported by Data Factory, you can create a custom activity with your own data processing logic and use the activity in the pipeline. You can create a custom activity to run R scripts on your HDInsight cluster with R installed.\nReference:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script.\nDoes this meet the goal?","topic":"2","discussion":[{"timestamp":"1597663500.0","poster":"Nieswurz","comments":[{"poster":"Pairon","content":"I agree with you. The insert operation into the DWH should be come after the R script process.","timestamp":"1616507040.0","upvote_count":"2","comment_id":"318135"},{"timestamp":"1621920720.0","content":"Agree with the statement and as of this year, the Azure Synapse doesn't support R language unless it is executed against Azure SQL Manage Instance","comment_id":"366135","poster":"cadio30","upvote_count":"5"}],"comment_id":"159922","content":"The proposed solution seems to let the R function do the loading into Synapse. The answer then should be 'no', but more likely the description seems again to be incomplete.","upvote_count":"16"},{"comment_id":"381989","comments":[{"comment_id":"381992","content":"My bad it only applies to SQL Server 2016 & Azure SQL Managed Instance, Moderator please dont post this.","timestamp":"1623688140.0","comments":[{"comment_id":"391847","upvote_count":"1","poster":"tes","timestamp":"1624779360.0","content":"there is no moderator human"}],"upvote_count":"1","poster":"azurrematt123"}],"upvote_count":"2","timestamp":"1623687960.0","content":"Looks like executing R is possible(sp_execute_external_script), please review the link.\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-execute-external-script-transact-sql?view=sql-server-ver15","poster":"azurrematt123"},{"comment_id":"352922","upvote_count":"1","poster":"dbdev","comments":[{"poster":"dbdev","upvote_count":"1","content":"https://docs.microsoft.com/en-US/azure/data-factory/transform-data#custom-activity","comment_id":"370232","timestamp":"1622379000.0"},{"content":"yes R function can be used within the Stored procedure for SQL Server or Azure SQL Managed instance , however the statement states that the data is loaded using SQL Synapse which does not support R at this time","comment_id":"375469","poster":"KRV","timestamp":"1622929800.0","upvote_count":"1"}],"timestamp":"1620552360.0","content":"https://www.mssqltips.com/sqlservertip/6622/stored-procedure-in-sql-server-with-r-code/\nThe R function can be used inside stored procedure activity, so answer makes sense to me."},{"content":"The answer should be No. Because MS always supply options for users so it won't engage in one specific programming language such as R.","timestamp":"1615961400.0","poster":"I","upvote_count":"1","comment_id":"312973"},{"poster":"Madhumita88","content":"I am also not cleared with this answer","timestamp":"1615891500.0","comment_id":"312223","upvote_count":"1"},{"content":"I am still not clear as to what is the correct answer","timestamp":"1610250420.0","upvote_count":"1","comment_id":"263637","poster":"S3"},{"timestamp":"1607440740.0","comment_id":"238428","poster":"syu31svc","content":"Solution proposed is on data pipeline and orchestration so I would say yes","upvote_count":"1"},{"comment_id":"183435","timestamp":"1600663320.0","upvote_count":"2","content":"Should use a tumbling window trigger in ADF for incremental loading.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate","comments":[{"timestamp":"1604619000.0","upvote_count":"1","content":"You can do incremental load using schedule trigger. Does not have to be Tumbling window","comment_id":"213761","poster":"sandGrain"}],"poster":"AJMorgan591"},{"content":"can we run a stored procedure to execute the R script ?? I don't think so.","comments":[{"timestamp":"1608239040.0","content":"possible","upvote_count":"1","poster":"VMLearn","comment_id":"246873"},{"poster":"Bob123456","comment_id":"164027","timestamp":"1598145900.0","upvote_count":"6","comments":[{"comment_id":"216708","timestamp":"1605020940.0","upvote_count":"4","content":"That is for sql server and managed instances and the question is about Azure Synapse Analytics","poster":"pablocg"}],"content":"https://docs.microsoft.com/en-us/sql/machine-learning/tutorials/quickstart-r-create-script?view=sql-server-ver15\ni believe this answers the question . Answer should be 'yes'"}],"timestamp":"1598145600.0","upvote_count":"2","comment_id":"164024","poster":"Bob123456"},{"content":"The explanation of the answer contains R on an HDInsight-Cluster. This kind of solution is stated to be incorrect in another questions explanation - in favor of an Azure function.","comment_id":"159939","timestamp":"1597664880.0","poster":"Nieswurz","upvote_count":"2"}],"exam_id":66},{"id":"3cEX4wifjlSxjYIBTfB7","answer_images":[],"topic":"2","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse.\nDoes this meet the goal?","answer":"B","choices":{"A":"Yes","B":"No"},"discussion":[{"content":"But I can do with this too!","timestamp":"1599574140.0","poster":"avix","comment_id":"175912","comments":[{"comments":[{"poster":"Kalo","comment_id":"292864","upvote_count":"4","content":"with a mount in DBFS, we can ingest data from ADLS","timestamp":"1613592960.0"}],"timestamp":"1609760880.0","content":"it is possible, but first you need to ingest data from staging source","poster":"andreeavi","comment_id":"259326","upvote_count":"3"}],"upvote_count":"20"},{"content":"answer should be yes.","comment_id":"211513","upvote_count":"17","timestamp":"1604350080.0","poster":"Shrikant_Kulkarni"},{"poster":"cadio30","content":"This requirement is possible with the use R script in Azure Databricks job. Therefore, answer should be 'Yes'","upvote_count":"2","timestamp":"1621922400.0","comment_id":"366151"},{"poster":"mohowzeh","content":"A scheduled daily Databricks job does the trick. Data Factory isn't the only tool that can bring data from one place to another... Answer should be yes.","upvote_count":"4","comment_id":"267462","timestamp":"1610662140.0"},{"upvote_count":"3","content":"Who is gonna stop me from using Databricks. There seems to be no technical limitation in this approach","poster":"Psycho360","timestamp":"1607203440.0","comment_id":"236038"},{"poster":"Akva","upvote_count":"9","comment_id":"215684","content":"I think it should be YES.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse","timestamp":"1604900640.0"}],"answer_description":"You should use an Azure Data Factory, not an Azure Databricks job.\nReference:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data","exam_id":66,"answer_ET":"B","timestamp":"2020-09-08 16:09:00","isMC":true,"answers_community":[],"question_id":82,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/30870-exam-dp-201-topic-2-question-21-discussion/","unix_timestamp":1599574140},{"id":"ljAsOva7SptdVWAYdAhp","question_images":[],"isMC":true,"topic":"2","answers_community":[],"question_id":83,"unix_timestamp":1597665600,"answer_description":"Use a stored procedure, not an Azure Databricks notebook to invoke the R script.\nReference:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse.\nDoes this meet the goal?","answer":"B","timestamp":"2020-08-17 14:00:00","choices":{"B":"No","A":"Yes"},"answer_images":[],"discussion":[{"timestamp":"1597665600.0","upvote_count":"27","comments":[{"comment_id":"259328","content":"first step is to ingest data..","poster":"andreeavi","timestamp":"1609760940.0","upvote_count":"1"},{"content":"I think notebooks are only interactive. It should be a job cluster. Any opinions?","timestamp":"1617708600.0","upvote_count":"2","poster":"maynard13x8","comment_id":"329559"},{"content":"Now your comment is ambiguous. Do you mean correct answer provided in that case 'NO' is answer or the Solution provided is correct and it will do the Job, in this case 'Yes' will be the answer...","timestamp":"1629307860.0","comment_id":"426997","poster":"Bhagya123456","upvote_count":"4"}],"content":"This should be the correct answer.","poster":"Nieswurz","comment_id":"159950"},{"content":"Yes, this solution meets the goal. You can use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. This will allow you to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics on a daily basis.","comment_id":"906652","timestamp":"1685018040.0","poster":"bakamon","upvote_count":"1"},{"poster":"Ssv2030","content":"The answer should be NO because:\n1. we can't assume that the Azure Databricks notebook will execute/run the transform R script, it is not mentioned that Azure Databricks notebook will run the R script\n2. for incremental loads in ADF, I think a tumbling trigger should be used.\ncan someone pls confirm?","timestamp":"1631450640.0","upvote_count":"1","comment_id":"443464"},{"poster":"MMM777","timestamp":"1622986500.0","upvote_count":"4","comment_id":"376056","content":"Answer should be YES: ADF can trigger a Databricks notebook (not required to be user-driven):\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"},{"timestamp":"1621923540.0","comment_id":"366165","poster":"cadio30","content":"The answer is Yes. R script is executed in the azure databricks notebook and once the transformation is completed then the mount the Azure Synapse to load the data.\n\nReference: https://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse","upvote_count":"1"},{"comments":[{"timestamp":"1607632620.0","poster":"BungyTex","upvote_count":"1","content":"Don't have to, can just use a regular schedule no problem.","comment_id":"240434"}],"upvote_count":"2","content":"Should use a tumbling window trigger in ADF for incremental loading.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate","poster":"AJMorgan591","timestamp":"1600663380.0","comment_id":"183436"},{"content":"I'm surprised as I ran R in Azure Databrick","upvote_count":"2","timestamp":"1599574620.0","poster":"avix","comment_id":"175920"},{"timestamp":"1598201760.0","content":"The solution template mentioned by Bob123456 does not fit, as --- per description --- the R script is to be run when the data is still located in the data lake. After the R based transformation, the result is to be loaded to the DWH. This type of processing would need polybase for accessing the data lake, which is not mentioned here.","comments":[{"upvote_count":"2","content":"Databricks notebook can use mount to access data lake. Notebook is correct answer","timestamp":"1601586300.0","poster":"apandey","comment_id":"191214"}],"comment_id":"164549","upvote_count":"3","poster":"Nieswurz"},{"poster":"Bob123456","content":"this is incorrect \nhttps://docs.microsoft.com/en-us/sql/machine-learning/tutorials/quickstart-r-create-script?view=sql-server-ver15","comment_id":"164026","upvote_count":"1","timestamp":"1598145840.0"}],"answer_ET":"B","exam_id":66,"url":"https://www.examtopics.com/discussions/microsoft/view/28839-exam-dp-201-topic-2-question-22-discussion/"},{"id":"CZUPZMuTo9MkheB5AjGy","unix_timestamp":1624790400,"answer":"C","choices":{"B":"Azure Analysis Services using Microsoft Visual Studio","A":"Azure Data Factory instance using Azure Portal","C":"Azure Stream Analytics Edge application using Microsoft Visual Studio","D":"Azure Data Factory instance using Microsoft Visual Studio"},"question_images":[],"isMC":true,"discussion":[{"timestamp":"1628841900.0","upvote_count":"1","comment_id":"424139","content":"It's a duplicate Question","poster":"satyamkishoresingh"},{"timestamp":"1624790400.0","upvote_count":"1","comment_id":"391966","poster":"mosheshito","content":"Correct"}],"timestamp":"2021-06-27 12:40:00","question_text":"A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\nThe company must be able to monitor the devices in real-time.\nYou need to design the solution.\nWhat should you recommend?","answer_images":[],"topic":"2","answers_community":[],"answer_description":"Azure Stream Analytics (ASA) on IoT Edge empowers developers to deploy near-real-time analytical intelligence closer to IoT devices so that they can unlock the full value of device-generated data.\nYou can use Visual Studio plugin to create an ASA Edge job.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge","exam_id":66,"question_id":84,"answer_ET":"C","url":"https://www.examtopics.com/discussions/microsoft/view/56163-exam-dp-201-topic-2-question-23-discussion/"},{"id":"xn1B3fTyER2Mt617xBvg","exam_id":66,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/36509-exam-dp-201-topic-2-question-24-discussion/","discussion":[{"comment_id":"215534","poster":"NabilR","content":"Answer is correct. F should specify \"Input\"","timestamp":"1604870340.0","upvote_count":"25","comments":[{"poster":"Nik71","upvote_count":"2","comment_id":"319037","timestamp":"1616583000.0","content":"yep Input need to be partitioned not output"}]},{"content":"I have seen this question and answer before but I don't think it is correct as it specifically mentions optimize performance.\n\nIn Microsoft's documentation, it specifies partitioning input and output to leverage parallelization, so I think E and F should be the answer.\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization","poster":"pablocg","comments":[{"timestamp":"1624779780.0","content":"\"E and F should be the answer\"? What is the different between E and F when it is copy pasted?","upvote_count":"1","poster":"tes","comment_id":"391854"},{"comment_id":"338186","content":"Isn't the input already partitioned?","timestamp":"1618749720.0","poster":"anamaster","upvote_count":"1"}],"timestamp":"1605021720.0","upvote_count":"6","comment_id":"216715"},{"comment_id":"391698","poster":"Arusham","content":"This question came in my DP 200 exam\nI gave on 24th June 2021","timestamp":"1624761900.0","upvote_count":"2"},{"content":"The correct answer: B & F \n\nIn F: you can change the latest word ( output >>>> input )! \nA. Implement event ordering\nB. Scale the SU count for the job up\nC. Implement Azure Stream Analytics user-defined functions (UDF)\nD. Scale the SU count for the job down\nE. Implement query parallelization by partitioning the data output\nF. Implement query parallelization by partitioning the data input","comment_id":"373327","poster":"Qrm_1972","upvote_count":"5","timestamp":"1622703660.0"},{"timestamp":"1621924080.0","poster":"cadio30","upvote_count":"1","comment_id":"366171","content":"Answer are Scale up the Streaming Units then include partition on 'Input' (done in the query)"},{"content":"E and F are exactly same.","timestamp":"1615962060.0","upvote_count":"2","poster":"I","comment_id":"312979"},{"comments":[{"comment_id":"338185","content":"I had exactly the same question on dp-200","poster":"anamaster","upvote_count":"3","timestamp":"1618749660.0"}],"upvote_count":"2","content":"i saw this question in dp-200, but option E,F wasnt there either","poster":"rajat009","timestamp":"1608254040.0","comment_id":"246980"},{"comment_id":"246978","content":"DP-200 question not 201","upvote_count":"4","poster":"rajat009","timestamp":"1608253920.0"},{"content":"Scaling the SU count is correct\npartition the output not input\nso B is correct\nEither E or F is right since there is a typo of output twice","poster":"syu31svc","timestamp":"1607441280.0","comment_id":"238441","upvote_count":"2"}],"answers_community":[],"answer_description":"Scale out the query by allowing the system to process each input partition separately.\nF: A Stream Analytics job definition includes inputs, a query, and output. Inputs are where the job reads the data stream from.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization","question_id":85,"timestamp":"2020-11-08 22:19:00","question_images":[],"answer_ET":"BF","isMC":true,"topic":"2","answer":"BF","choices":{"D":"Scale the SU count for the job down","A":"Implement event ordering","F":"Implement query parallelization by partitioning the data output","E":"Implement query parallelization by partitioning the data output","C":"Implement Azure Stream Analytics user-defined functions (UDF)","B":"Scale the SU count for the job up"},"question_text":"A company has a real-time data analysis solution that is hosted on Microsoft Azure. The solution uses Azure Event Hub to ingest data and an Azure Stream\nAnalytics cloud job to analyze the data. The cloud job is configured to use 120 Streaming Units (SU).\nYou need to optimize performance for the Azure Stream Analytics job.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","unix_timestamp":1604870340}],"exam":{"numberOfQuestions":206,"isImplemented":true,"id":66,"provider":"Microsoft","name":"DP-201","lastUpdated":"12 Apr 2025","isBeta":false,"isMCOnly":false},"currentPage":17},"__N_SSP":true}