{"pageProps":{"questions":[{"id":"Q2LawtjKq9629Qstjuve","url":"https://www.examtopics.com/discussions/microsoft/view/15925-exam-dp-200-topic-10-question-1-discussion/","choices":{"D":"ExecRequests","A":"RequestSteps","B":"DmsWorkers","C":"SqlRequests"},"topic":"10","exam_id":65,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou need to implement diagnostic logging for Data Warehouse monitoring.\nWhich log should you use?","question_id":61,"discussion":[{"poster":"[Removed]","comment_id":"82563","comments":[{"upvote_count":"1","comment_id":"123744","content":"That's not the correct link/view for the SQL DW and the view is Available for the DW - it's called \"sys.dm_pdw_exec_requests\".\n\nHere you can find all the available logging options for the DW and a screenshot from Azure where it shows an option box with all options from the question part:\nhttps://azure.microsoft.com/en-us/blog/workload-insights-with-sql-data-warehouse-delivered-through-azure-monitor-diagnostic-logs-pass/\n\nThe question and what the view provides are different if we use: \"ExecRequests\" - because it has a cache hit column where the value is \"1\" or \"0\". So, as you can see no %.\n\nFor the \"sys.dm_pdw_sql_requests: SqlRequest\" - I don't see any cache hit columns, but it stores all the data whereas the \"ExecRequest: sys.dm_pdw_exec_requests\" stores Current or Recent requests.\n\nSo, if we don't pay attention on \"cache hit %\" from the question then it looks like the answer is: \"ExecRequest: sys.dm_pdw_exec_requests\".","poster":"Ikrom","comments":[{"upvote_count":"1","comment_id":"123781","poster":"Ikrom","timestamp":"1593558840.0","comments":[{"content":"Here is what I found most relevant to the requirement where we can find \n- \"Cache hit percentage\"\n- \"Cache used percentage\"\nalso the aggregation type is what we need: \"Avg, Min, Max\".\n\nMaybe the answer itself is not correct.\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity","poster":"Ikrom","timestamp":"1593558900.0","comments":[{"upvote_count":"2","content":"I'd say D. ExecRequests as the cache hit% is calculated in Azure Synapse based on cache hits (1) and misses (0) which are collected through sys.dm_pdw_exec_requests \n\nresult_cache_hit decimal Details whether a completed query used result set cache.\nApplies to: Azure SQL Data Warehouse 1 = Result set cache hit\n0 = Result set cache miss\nNegative values = Reasons why result set caching was not used. See remarks section for details.\n\n(cache hits / cache miss) * 100 where cache hits is the sum of all columnstore segments hits in the local SSD cache and cache miss is the columnstore segments misses in the local SSD cache summed across all nodes\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-exec-requests-transact-sql?view=aps-pdw-2016-au7","timestamp":"1595499180.0","poster":"Treadmill","comment_id":"141908"}],"comment_id":"123782","upvote_count":"1"}],"content":"Here is what I found most relevant to the requirement where we can find \"Cache hit percentage\" and \"Cache used percentage\" - also the aggregation type what we need: \"Avg, Min, Max\".\nMaybe the answer itself is not correct.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity"}],"timestamp":"1593551940.0"}],"timestamp":"1588413240.0","upvote_count":"16","content":"According to the docs, the ExecRequests is not available for Azure SQL Data Warehouse https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-requests-transact-sql\n\nTherefore, SQLRequests should be the correct answer to monitor Azure SQL Data Warehouse: \nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-sql-requests-transact-sql"},{"upvote_count":"7","poster":"Luke97","comment_id":"95616","comments":[{"comment_id":"101781","timestamp":"1591203780.0","content":"Correct answer is exec_requests - https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-exec-requests-transact-sql?view=aps-pdw-2016-au7","poster":"VJ8","upvote_count":"2"}],"timestamp":"1590433680.0","content":"result_cache_hit in sys.dm_pdw_exec_requests"},{"upvote_count":"2","content":"https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-exec-requests-transact-sql?view=aps-pdw-2016-au7\nSearch for cache and you'll get your answer as D","poster":"syu31svc","timestamp":"1606488720.0","comments":[{"poster":"dumpsm42","upvote_count":"3","content":"hi to all, \nyes: D\nthis link explains better and the microsoft supported links\nhttps://techcommunity.microsoft.com/t5/azure-synapse-analytics/azure-dw-result-set-caching/ba-p/688544\nregards","comment_id":"238131","timestamp":"1607423100.0"}],"comment_id":"229092"},{"upvote_count":"3","poster":"hart232","comment_id":"187766","timestamp":"1601135640.0","content":"Its Execrequests as SQLRequests dont have cache option."},{"content":"SQL Request give more detailed information while Exec Request provide only limited information about query execution.","poster":"PrashantSeth","comment_id":"76200","timestamp":"1587256500.0","upvote_count":"6"},{"upvote_count":"1","content":"As referred in the https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-exec-requests-transact-sql\nMy answer is incline to \"exec-requests\"\nexec_requests: After referring the Holds information about all requests currently or recently active in SQL Data Warehouse\nSQL_requests : Holds information about all SQL Server query distributions as part of a SQL step in the query.","timestamp":"1583892900.0","poster":"SP16","comment_id":"62080"},{"content":"Shouldn't that be Execute requests for cache monitoring?","poster":"z8zhong","comment_id":"61158","timestamp":"1583768400.0","upvote_count":"3"},{"content":"based on microsoft docs, i would assume \"ExecRequests\" as correct answer.\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-exec-requests-transact-sql","upvote_count":"5","poster":"snimz","comment_id":"61050","timestamp":"1583753580.0"}],"answer_ET":"C","unix_timestamp":1583753580,"isMC":true,"timestamp":"2020-03-09 12:33:00","answer":"C","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0046200005.png"],"question_images":[],"answer_description":"Scenario:\nThe Azure SQL Data Warehouse cache must be monitored when the database is being used.\n\nReferences:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-sql-requests-transact-sq"},{"id":"RdQkIGmmmxepHnxLjb0y","url":"https://www.examtopics.com/discussions/microsoft/view/36128-exam-dp-200-topic-10-question-2-discussion/","question_images":[],"answers_community":[],"answer_images":[],"timestamp":"2020-11-05 08:56:00","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou need setup monitoring for tiers 6 through 8.\nWhat should you configure?","answer_description":"Scenario:\nTiers 6 through 8 must have unexpected resource storage usage immediately reported to data engineers.\nTier 3 and Tier 6 through Tier 8 applications must use database density on the same server and Elastic pools in a cost-effective manner.","topic":"10","discussion":[{"timestamp":"1606521120.0","content":"Tiers 6 through 8 must have unexpected resource storage usage immediately reported to data engineers. -> E is the answer since Tier 6 through Tier 8 applications must use database density on the same server and Elastic pools in a cost-effective manner","poster":"syu31svc","upvote_count":"1","comment_id":"229293"},{"timestamp":"1604562960.0","upvote_count":"1","content":"There is a Data Space Used % metric available in Azure Monitor for Azure SQL Database and Elastic Pool which could be used for an alert. \nEven if the database is within an elastic pool as per requirements it might be more useful to monitor at the database level as there may be per database storage limits","poster":"big_data_au","comment_id":"213262"}],"isMC":true,"answer":"E","question_id":62,"exam_id":65,"answer_ET":"E","unix_timestamp":1604562960,"choices":{"C":"an alert rule to monitor CPU percentage in elastic pools that emails data engineers","A":"extended events for average storage percentage that emails data engineers","B":"an alert rule to monitor CPU percentage in databases that emails data engineers","E":"an alert rule to monitor storage percentage in elastic pools that emails data engineers","D":"an alert rule to monitor storage percentage in databases that emails data engineers"}},{"id":"GR2cFq3vEEhX0pmyjkaT","question_id":63,"discussion":[{"timestamp":"1589828520.0","upvote_count":"18","poster":"Luke97","comment_id":"91575","content":"Azure Data Factory provides the following performance optimization features:\n\n- Data Integration Units\n- Self-hosted integration runtime scalability\n- Parallel copy\n- Staged copy\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/copy-activity-performance#parallel-copy"},{"poster":"vrmei","upvote_count":"1","timestamp":"1623122520.0","comment_id":"377190","content":"Data Factory pipeline that runs from Cosmos DB to SQL Database\n\nconsider this to choose the correct answer"},{"upvote_count":"1","timestamp":"1622840580.0","content":"Is B the correct answer?","comment_id":"374584","poster":"niwe"}],"answer_images":[],"question_text":"You are monitoring the Data Factory pipeline that runs from Cosmos DB to SQL Database for Race Central.\nYou discover that the job takes 45 minutes to run.\nWhat should you do to improve the performance of the job?","timestamp":"2020-05-18 21:02:00","answer":"B","answers_community":[],"answer_description":"Performance tuning tips and optimization features. In some cases, when you run a copy activity in Azure Data Factory, you see a \"Performance tuning tips\" message on top of the copy activity monitoring, as shown in the following example. The message tells you the bottleneck that was identified for the given copy run.\nIt also guides you on what to change to boost copy throughput. The performance tuning tips currently provide suggestions like:\n✑ Use PolyBase when you copy data into Azure SQL Data Warehouse.\n✑ Increase Azure Cosmos DB Request Units or Azure SQL Database DTUs (Database Throughput Units) when the resource on the data store side is the bottleneck.\n✑ Remove the unnecessary staged copy.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/data-factory/copy-activity-performance","choices":{"A":"Decrease parallelism for the copy activities.","C":"Configure the copy activities to use staged copy.","B":"Increase that data integration units.","D":"Configure the copy activities to perform compression."},"question_images":[],"answer_ET":"B","isMC":true,"topic":"11","exam_id":65,"url":"https://www.examtopics.com/discussions/microsoft/view/20889-exam-dp-200-topic-11-question-1-discussion/","unix_timestamp":1589828520},{"id":"2cdtQKddjAUkA2YLWlxE","choices":{"B":"automatic tuning","C":"Query Store","A":"the sp_update_stats stored procedure","D":"the dbcc checkdb command"},"question_text":"What should you implement to optimize SQL Database for Race Central to meet the technical requirements?","question_id":64,"isMC":true,"topic":"11","answer_images":[],"answer_description":"Scenario: The query performance of Race Central must be stable, and the administrative time it takes to perform optimizations must be minimized. sp_updatestats updates query optimization statistics on a table or indexed view. By default, the query optimizer already updates statistics as necessary to improve the query plan; in some cases you can improve query performance by using UPDATE STATISTICS or the stored procedure sp_updatestats to update statistics more frequently than the default updates.\nIncorrect Answers:\nD: dbcc checkdchecks the logical and physical integrity of all the objects in the specified database\nReferences:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-updatestats-transact-sql?view=sql-server-ver15","answer_ET":"A","discussion":[{"comment_id":"23731","timestamp":"1574457660.0","poster":"STH","comments":[{"timestamp":"1600287480.0","content":"https://docs.microsoft.com/en-us/azure/azure-sql/database/automatic-tuning-overview","poster":"emmanuelcs","comment_id":"180571","upvote_count":"1"},{"content":"don't think automatic tuning is available for Azure Sql SWH (which is required)","comments":[{"upvote_count":"1","content":"Nevermind. The other application is in sql dwh, this one is sql db. Than I agree.","timestamp":"1610306520.0","comment_id":"264222","poster":"ThijsN"}],"timestamp":"1610306400.0","poster":"ThijsN","comment_id":"264221","upvote_count":"1"},{"timestamp":"1621428900.0","upvote_count":"1","poster":"memo43","comment_id":"361391","content":"i saw this question another dump and the answer was \"automatic tunning\" too"}],"content":"answer B : automatic tunning","upvote_count":"31"},{"comment_id":"61143","upvote_count":"15","timestamp":"1583765100.0","content":"I believe this is a typo and it meant sp_updatestats. In that case A is correct. \nThere is no sp_update procedure","poster":"samok"},{"timestamp":"1606478280.0","comments":[{"poster":"dumpsm42","comment_id":"239560","content":"agree.\nthe text says it all \"...The query performance of Race Central must be stable, and the administrative time it takes to perform optimizations must be minimized...\"\nto be stable means that we must have an automatic process and not a manual one when needed => B\n\nregards","timestamp":"1607553900.0","upvote_count":"3"}],"content":"Azure SQL Database includes database advisors that provide performance tuning recommendations for single and pooled databases. These recommendations are available in the Azure portal as well as by using PowerShell. You can also enable automatic tuning so that Azure SQL Database can automatically implement these tuning recommendations.\nB it is.","upvote_count":"2","comment_id":"228999","poster":"syu31svc"},{"timestamp":"1594813080.0","content":"The question leaves unclear, over what dimension the query time should be stable: 1) Same query over time, 2) same query with different parameters (\"carId\"), or 3) initial run or repeated queries (cache). Happy guessing the intention of the one who asks!","upvote_count":"1","comment_id":"135676","poster":"Nieswurz"},{"upvote_count":"2","comment_id":"87473","timestamp":"1589267700.0","content":"with automatic tuning , depending ot the workload, all the hints are deployed or in a waiting approval state. then manually you approve and deploy it. so the optimization is done with a minimal manual process. and only updating stats is not enough to optimize , you need to check indexes.","poster":"wyxh"},{"content":"sp_updatestats is not the same with sp_update, isn't it ?\nI think the correct answer here is: B: automatic tunning","poster":"cdume","upvote_count":"3","comment_id":"52046","timestamp":"1582026360.0"},{"upvote_count":"3","poster":"Abbas","content":"Automatic tuning is one option but here it is talking about the administrative time should be minimal, which means manual process and hence sp_updatestats.","comment_id":"29350","timestamp":"1576246020.0"}],"exam_id":65,"timestamp":"2019-11-22 22:21:00","answer":"A","url":"https://www.examtopics.com/discussions/microsoft/view/8914-exam-dp-200-topic-11-question-2-discussion/","question_images":[],"answers_community":[],"unix_timestamp":1574457660},{"id":"8r2VP301HJEOG0eQTyCC","topic":"11","discussion":[{"timestamp":"1596108900.0","content":"I think AE is correct. We are estimating RU/s. By calculating storage data divided by the number of requests, you know how much is average data size per request. Since 1RU is about 1KB item, you can estimate the average RU/s required.","poster":"extraego","upvote_count":"18","comment_id":"147354"},{"upvote_count":"8","comments":[{"timestamp":"1595446020.0","upvote_count":"3","comment_id":"141356","poster":"Treadmill","content":"E for sure, but I'm not sure of the D.\n\nThe option given is \"session consistency\" and actually the concept is called \"data consistency\" and \"session\" is just one of the data consistency levels.\n\nAs in link stated:\nData consistency: The strong and bounded staleness consistency levels consume approximately two times more RUs while performing read operations when compared to that of other relaxed consistency levels."},{"timestamp":"1612249560.0","content":"D is out of the question because Session consistency is just the configuration you set. You observer other things and may want to chaneg consitency level","comment_id":"281681","upvote_count":"1","poster":"dev2dev"}],"content":"I think DE is the correct one.\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/request-units?fbclid=IwAR2etKSJ0ZcHjnzbfJ6gdlu1_UHN8Sx4VfHn8MNdz4Qnxs0-KzJV4APyU24#request-unit-considerations","poster":"Zanzara","timestamp":"1589032200.0","comment_id":"86077"},{"comment_id":"241421","poster":"dumpsm42","timestamp":"1607759340.0","upvote_count":"3","content":"hi to all\n\nrequests and data+index.\nhttps://docs.microsoft.com/pt-pt/azure/cosmos-db/request-units\n\nregards"},{"upvote_count":"1","timestamp":"1601521200.0","poster":"hart232","content":"Factors that affect the cost of query operations include:\n\nThe number of query results\nThe number of predicates\nThe nature of the predicates\nThe number of user-defined functions\nThe size of the source data\nThe size of the result set\nProjections","comment_id":"190700"},{"content":"Look at info for basic capacity planner in this link: https://docs.microsoft.com/en-us/azure/cosmos-db/estimate-ru-with-capacity-planner. Throughput is the same as RU/s. The inputs include storage and # of requests.","timestamp":"1596570240.0","poster":"rjdask","comment_id":"150678","upvote_count":"4"},{"upvote_count":"4","comment_id":"91581","content":"Agree with Zanzara. Cosmos DB RU considerations are\n- Item Size\n- Item Indexing\n- Item property count\n- Indexed properties\n- Data consistency\n- Query patterns\n- Script usage","poster":"Luke97","timestamp":"1589829180.0"},{"upvote_count":"2","timestamp":"1589275020.0","poster":"pawhit","comment_id":"87522","content":"Not sure on the answer to this one but I would think F was one of them. You provision throughput for a container which is a measure of crud per second so looking at the average throughput makes sense to me. Can't see what benefit E gives you here - RUs are about activity taking place in a given second."},{"upvote_count":"1","poster":"Huepig","comment_id":"69773","content":"https://docs.microsoft.com/en-us/azure/data-factory/copy-activity-performance","timestamp":"1585643400.0"}],"question_text":"Which two metrics should you use to identify the appropriate RU/s for the telemetry data? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","unix_timestamp":1585643400,"answer_description":"Scenario: The telemetry data must be monitored for performance issues. You must adjust the Cosmos DB Request Units per second (RU/s) to maintain a performance SLA while minimizing the cost of the RU/s.\nWith Azure Cosmos DB, you pay for the throughput you provision and the storage you consume on an hourly basis.\nWhile you estimate the number of RUs per second to provision, consider the following factors:\nItem size: As the size of an item increases, the number of RUs consumed to read or write the item also increases.","exam_id":65,"url":"https://www.examtopics.com/discussions/microsoft/view/17713-exam-dp-200-topic-11-question-3-discussion/","choices":{"F":"Avg Throughput/s","A":"Number of requests","D":"Session consistency","C":"End to end observed read latency at the 99 th percentile","B":"Number of requests exceeded capacity","E":"Data + Index storage consumed"},"question_images":[],"timestamp":"2020-03-31 10:30:00","question_id":65,"isMC":true,"answer":"AE","answer_ET":"AE","answer_images":[],"answers_community":[]}],"exam":{"numberOfQuestions":228,"id":65,"isImplemented":true,"lastUpdated":"12 Apr 2025","provider":"Microsoft","isMCOnly":false,"isBeta":false,"name":"DP-200"},"currentPage":13},"__N_SSP":true}