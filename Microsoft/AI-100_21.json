{"pageProps":{"questions":[{"id":"cPZeLe4MVr8aD79z4EHR","answer":"B","answers_community":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure SQL database, an Azure Data Lake Storage Gen 2 account, and an API developed by using Azure Machine Learning Studio.\nYou need to ingest data once daily from the database, score each row by using the API, and write the data to the storage account.\nSolution: You create an Azure Data Factory pipeline that contains a Machine Learning Execute Pipeline activity.\nDoes this meet the goal?","url":"https://www.examtopics.com/discussions/microsoft/view/50476-exam-ai-100-topic-2-question-58-discussion/","exam_id":39,"unix_timestamp":1618844100,"question_images":[],"isMC":true,"topic":"2","timestamp":"2021-04-19 16:55:00","answer_description":"The Machine Learning Execute Pipeline activity enables batch prediction scenarios such as identifying possible loan defaults, determining sentiment, and analyzing customer behavior patterns.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service","choices":{"A":"Yes","B":"No"},"discussion":[{"upvote_count":"1","timestamp":"1687247940.0","comment_id":"928255","content":"B. No\n\nExplanation:\n\nThe given solution does not meet the goal. Azure Data Factory (ADF) does not provide a built-in activity called \"Machine Learning Execute Pipeline\" that can be used to directly invoke Azure Machine Learning Studio (classic) APIs.\n\nTo ingest data from the Azure SQL database, score each row using the Azure Machine Learning Studio API, and write the data to the Azure Data Lake Storage Gen2 account, you would need a different approach.","poster":"rveney"},{"poster":"mingchieh2","comments":[{"comment_id":"360274","poster":"YSBINW","upvote_count":"1","timestamp":"1621326900.0","comments":[{"comment_id":"362141","timestamp":"1621513500.0","upvote_count":"1","content":"This is a confusing one. These both can be used but the batch execute is the older way (MLS Classic) and has a specific API exposed as a web service. Whereas the execute pipeline is a reference to the experiment setup in the ADF UX. IMO is seems like question 54 is a Yes, and question 58 is a No. Thanks mingchieh2 for calling this out.","poster":"soren"}],"content":"What should be the right question ? This is tricky...Batch Execution Activity can invoke studio (classic) and make preditions....In this case, we are not talking about the classic version( I would say).\nIn other hands Machine Learning Execute Pipeline activity enables batch prediction also, but I didn't found anything specifying \"prediction on Studio\"....\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service\nhttps://docs.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-ml-batch-execution-activity"}],"timestamp":"1618844100.0","upvote_count":"4","content":"Question54 & 58, same series, solution only 1 word diff, need to take care \n\nYou create an Azure Data Factory pipeline that contains the Machine Learning Batch Execution activity\n\nYou create an Azure Data Factory pipeline that contains a Machine Learning Execute Pipeline activity\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning","comment_id":"338891"}],"question_id":101,"answer_images":[],"answer_ET":"B"},{"id":"FJvxlRaV2tUXKQSu6h0G","discussion":[{"timestamp":"1568700900.0","upvote_count":"18","content":"I think it is Databricks because of Scala and R support","poster":"Karl","comments":[{"content":"Although it could be a valid option, I found that Storm supports multi-language:\nhttps://storm.apache.org/about/multi-language.html\n\nHard to say what's the best answer...","timestamp":"1590825480.0","poster":"giusecozza","comment_id":"98710","upvote_count":"3"},{"comment_id":"266753","content":"Scala and R support is also there with HDinsight . Your logic does not holds.","upvote_count":"1","poster":"UpsetUser","timestamp":"1610602680.0","comments":[{"timestamp":"1626781020.0","content":"Not sure that HDinsight supports R (for Scala it does) : https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview","poster":"Messatsu","upvote_count":"1","comment_id":"410213"}]}],"comment_id":"11376"},{"content":"Storm supports C# & Java\nDatabricks supports C#/F#, Java, Python, R, Scala\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing","upvote_count":"6","comments":[{"comments":[{"comment_id":"150908","content":"Storm supports scala and other languages \nhttps://www.educba.com/apache-storm-vs-apache-spark/\nso HDInsight with Apache Storm is the correct answer","upvote_count":"2","poster":"LeoBritto","timestamp":"1596604260.0"}],"upvote_count":"3","content":"Databricks, as Storm do not support either scala or R \nhttps://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview","timestamp":"1593443340.0","comment_id":"122873","poster":"zemplenib"}],"comment_id":"83506","timestamp":"1588581600.0","poster":"abbam"},{"comment_id":"928499","content":"C. Azure HDInsight with Apache Storm\n\nExplanation:\nTo broker messages at scale from Kafka streams to Azure Storage, you can recommend using Azure HDInsight with Apache Storm.\n\nApache Storm is a distributed real-time stream processing system that can handle high-volume data streams. It provides fault-tolerant processing and can integrate well with Apache Kafka for data ingestion.","timestamp":"1687270500.0","poster":"rveney","upvote_count":"1"},{"comment_id":"360398","poster":"jdev","timestamp":"1621336140.0","content":"For real-time Kafka data streams - HD Insight with Apache Storm is best. The key context here is 'streams'.","comments":[{"comment_id":"381862","content":"Kafka as an input is not supported in Storm, but it is in Databricks. So I would say Databricks is the answer\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing","poster":"lollo1234","upvote_count":"1","comments":[{"timestamp":"1623674400.0","upvote_count":"1","comment_id":"381864","poster":"lollo1234","content":"You can also see that in the Sinks or Outputs, Azure Storage is supported in Databricks while it's not in Storm"}],"timestamp":"1623674280.0"}],"upvote_count":"1"},{"comment_id":"360396","content":"HD Insight with apache Strom is preferred - for processing real-time data streams.","poster":"jdev","upvote_count":"1","timestamp":"1621336020.0"},{"upvote_count":"2","comments":[{"content":"Disagree. There are just 3 lines in the question to make the distinction and you suggest we ignore the 1st line?\nAbout your 2nd comment on brokering messages, it means \"integration capabilities\". Having the capability to Input and/or Sink.\n\nBoth are Real-time stream processing techs to consume messages from queue etc.\n\nBoth Databricks (on Spark) and HDInsight with Storm can \"Sink\" Kafka while only Databricks has a direct input integration capability.\n\nHDInsight with Storm does not support Scala and R... I think this does need consideration.\n\nDatabricks is the answer IMO\n\nRead again in detail:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing","comment_id":"287794","poster":"Cornholioz","upvote_count":"1","timestamp":"1612983540.0","comments":[{"comment_id":"295231","upvote_count":"2","timestamp":"1613840340.0","content":"Going back on my inference. Databricks won't work here as it is a Spark-based analytics solution that allows you to create big data pipelines. Moreover, it runs on Spark clusters and not Hadoop clusters.\nAnother reliable & verified practice test states HDInsight as the solution. It discards ML Service but the option doesn't combine it with HDInsight.\nGiven this, I'll go with HDInsight + Storm","poster":"Cornholioz"}]}],"comment_id":"281453","poster":"SGZoom","content":"C is the correct Ans, because question does NOT mention utilization of your data team R and Scala expertise (believe they are meant to distract you). Question is to suggest technology to “broker” messages and Apache Storm is the right choice since it is meant for ingestion / streaming. DataBricks is an “Data Analytics” platform requiring ingestion thru Data factory / Kafka / Event Hub, etc.. so DataBricks is incorrect","timestamp":"1612215840.0"},{"content":"I think C and A both could be possible answers,, but since here big data is not mentioned, so we can eliminate A. So answer is correct : C","comment_id":"266751","timestamp":"1610602500.0","upvote_count":"1","poster":"UpsetUser"},{"poster":"vilas94","upvote_count":"1","timestamp":"1603015080.0","comment_id":"201944","content":"Answer is Azure Databricks which supports R and Scala , which is not supported by Apache Storm"},{"poster":"CeliaZhou","comment_id":"198305","timestamp":"1602496920.0","content":"I believe the answer is A: Azure Databricks, according to this article from Microsoft: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing\nApache Spark in Azure Databricks: C#/F#, Java, Python, R, Scala\nAzure Functions:C#, F#, Java, Node.js, Python\nHDInsight with Storm: C#, Java","upvote_count":"2"},{"comment_id":"185275","poster":"sayak17","upvote_count":"1","timestamp":"1600863960.0","content":"Apache Storm with Apache Kafka on HDInsight seems correct as per https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-storm-with-kafka\nAnd yes Apache Spark supports all languages mentioned in question https://spark.apache.org/\n\nHowever Azure Databricks can also be a answer because of:\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/kafka\nhttps://docs.databricks.com/spark/latest/sparkr/index.html\nhttps://docs.databricks.com/languages/scala.html\n\nSo not sure what to pick here"},{"comment_id":"178806","content":"1. Databricks can interface with Kafka. It supports R and Scala\nhttps://docs.databricks.com/spark/latest/structured-streaming/kafka.html\nhttps://databricks.com/r-programming\n\n\n2. Storm and Kafka works together. HD insights also supports R and Scala\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-storm-with-kafka\nCan't find any article which specifically says that storm supports both R and Scala.\n\n\n3. Ruling out Machine learning R server. This is primarily used if someone has need to do things on R\n\nSo my vote is for Databricks","upvote_count":"1","poster":"Nova077","timestamp":"1600013040.0"},{"timestamp":"1587579960.0","content":"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/kafka","poster":"princesskay","comment_id":"78025","upvote_count":"1"},{"timestamp":"1583761860.0","content":"Apache Spark in Azure Databricks\nhttps://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks","poster":"SepidehJah","upvote_count":"1","comment_id":"61116"},{"upvote_count":"1","timestamp":"1578440400.0","poster":"MK1977","content":"https://docs.microsoft.com/en-us/azure/hdinsight/kafka/apache-kafka-introduction","comment_id":"36521"},{"upvote_count":"4","content":"Option C,HDInsight with Apache Storm is the right answer as has been shown in the solution - correct","poster":"Swatishri","comment_id":"35964","timestamp":"1578305520.0"}],"exam_id":39,"url":"https://www.examtopics.com/discussions/microsoft/view/5288-exam-ai-100-topic-2-question-59-discussion/","question_images":[],"question_id":102,"answer_ET":"C","answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-streaming-at-scale-overview?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%\n2Fhdinsight%2Fhadoop%2FTOC.json&bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json","topic":"2","timestamp":"2019-09-17 08:15:00","answers_community":[],"unix_timestamp":1568700900,"choices":{"C":"Azure HDInsight with Apache Storm","A":"Azure Databricks","D":"Azure HDInsight with Microsoft Machine Learning Server","B":"Azure Functions"},"answer":"C","isMC":true,"question_text":"Your company has a data team of Scala and R experts.\nYou plan to ingest data from multiple Apache Kafka streams.\nYou need to recommend a processing technology to broker messages at scale from Kafka streams to Azure Storage.\nWhat should you recommend?","answer_images":[]},{"id":"3CSEotwgJ7sUm4IzW1L8","answer":"B","answers_community":[],"topic":"2","isMC":true,"choices":{"A":"Yes","B":"No"},"question_images":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an app named App1 that uses the Face API.\nApp1 contains several PersonGroup objects.\nYou discover that a PersonGroup object for an individual named Ben Smith cannot accept additional entries. The PersonGroup object for Ben Smith contains\n10,000 entries.\nYou need to ensure that additional entries can be added to the PersonGroup object for Ben Smith. The solution must ensure that Ben Smith can be identified by all the entries.\nSolution: You create a second PersonGroup object for Ben Smith.\nDoes this meet the goal?","answer_description":"Instead, use a LargePersonGroup. LargePersonGroup and LargeFaceList are collectively referred to as large-scale operations. LargePersonGroup can contain up to 1 million persons, each with a maximum of 248 faces. LargeFaceList can contain up to 1 million faces. The large-scale operations are similar to the conventional PersonGroup and FaceList but have some differences because of the new architecture.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/how-to-use-large-scale","discussion":[{"upvote_count":"1","comment_id":"930262","timestamp":"1687420080.0","content":"B. No\n\nTo ensure that additional entries can be added to the PersonGroup object for Ben Smith, you need to convert it to a LargePersonGroup object. A LargePersonGroup object can hold up to 1,000,000 entries\n\nCreating a second PersonGroup object for Ben Smith will not meet the goal because it will not allow you to add more entries to the original PersonGroup object for Ben Smith.","poster":"rveney"},{"content":"this was in the AI-100 exam i took today, May 31","timestamp":"1622444220.0","upvote_count":"1","poster":"berserkguts","comment_id":"370735"},{"poster":"fhqhfhqh","content":"This question was in the exam.","timestamp":"1621506480.0","comment_id":"362052","upvote_count":"1"},{"upvote_count":"1","content":"correct","comment_id":"315135","poster":"Nickname__for__discussions","timestamp":"1616186220.0"}],"exam_id":39,"answer_images":[],"answer_ET":"B","timestamp":"2021-03-19 21:37:00","question_id":103,"url":"https://www.examtopics.com/discussions/microsoft/view/47758-exam-ai-100-topic-2-question-6-discussion/","unix_timestamp":1616186220},{"id":"EYTbWnPZXKdO9QAFMW1E","answers_community":[],"unix_timestamp":1565368740,"choices":{"B":"Azure SQL Database","C":"Azure SQL Data Warehouse","A":"Azure Database for MySQL"},"timestamp":"2019-08-09 18:39:00","question_images":[],"topic":"2","answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/mysql/overview","url":"https://www.examtopics.com/discussions/microsoft/view/3403-exam-ai-100-topic-2-question-60-discussion/","answer_images":[],"discussion":[{"comment_id":"6402","upvote_count":"25","poster":"exam_taker5","content":"I believe this should be data warehouse. Since the experiment will only run once a month, querying power is not needed. Warehouse is also able to store much more data and would minimize cost for 200TB","timestamp":"1565368740.0"},{"content":"True. Moreover the provided link shows that the upper limit for SQL Database is 4 TB","comments":[{"upvote_count":"7","comment_id":"72227","timestamp":"1586285820.0","comments":[{"upvote_count":"2","timestamp":"1600865040.0","content":"SQL Database can go more than 200 TB. Check Gen5 here https://azure.microsoft.com/en-in/pricing/details/sql-database/single/","poster":"sayak17","comment_id":"185285"}],"poster":"Miles19","content":"SQL database can go up to 100TB on hyper-scale, but it's still not sufficient. I agree with the data warehouse."}],"poster":"Bharat","comment_id":"8446","timestamp":"1566867120.0","upvote_count":"7"},{"timestamp":"1687270680.0","comment_id":"928500","upvote_count":"1","content":"C. Azure SQL Data Warehouse\n\nExplanation:\nTo store more than 200 TB of relational tables and minimize compute costs for the AI application that will use Azure Machine Learning Studio experiment, the recommended data storage solution is Azure SQL Data Warehouse.\n\nAzure SQL Data Warehouse is a cloud-based, fully managed data warehouse service that provides high-performance analytics and scalability. It is designed to handle large volumes of data and allows you to scale compute resources independently of storage. This flexibility enables you to allocate the necessary compute resources for the monthly experiment while minimizing costs during idle periods.","poster":"rveney"},{"poster":"dijaa","upvote_count":"1","comment_id":"429984","content":"Difinetely warehouse","timestamp":"1629724020.0"},{"upvote_count":"1","content":"For all those in favor of 'C', please note that a Warehouse has low storage costs and high computational costs (OLAP).\nA database has high storage costs but comparatively lower computational costs. Here we must minimize compute costs.","poster":"Sonuand","timestamp":"1624934880.0","comment_id":"393424"},{"poster":"Wisenut","timestamp":"1612130340.0","content":"My pick would be C due to the following \n1) Storage limits\n2) Ability to pause DW computes when it is not used \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-compute-overview\nhttps://stackify.com/azure-sql-database-vs-warehouse/","comment_id":"280844","upvote_count":"4","comments":[{"comment_id":"287810","timestamp":"1612984980.0","content":"Agree. Although I wouldn't choose DWH over SQL DB for the storage limits but for the pause feature to reduce costs. And also because of the Massive Parallel Processing which is required for an AI ML project such as this one.\nThis is also why the question is relevant in an AI exam because it is important to pick a DWH instead of a DB even if it doesn't detail requirements about having datamarts or slice & dice of a DWH.","upvote_count":"1","poster":"Cornholioz"}]},{"timestamp":"1610770860.0","comment_id":"268558","upvote_count":"3","content":"Can someone tell me How this question is relevant to AI-100 exam,,, ??","poster":"UpsetUser","comments":[{"comment_id":"278694","content":"Bruhhh..... Just learn it though. We'll ask questions after we have gotten the certificates","poster":"Timeless_Faceless","upvote_count":"5","timestamp":"1611851700.0"},{"content":"AI-100/AI-102 focusses on becoming an Azure AI architect. By nature of what it has to offer, there will be questions where it will focus on AI storage and retrieval and hence database options will be one of them.","poster":"allanm","timestamp":"1622027460.0","comment_id":"367060","upvote_count":"1"}]},{"poster":"nepketo","comment_id":"208918","upvote_count":"6","timestamp":"1604012280.0","content":"I think it's Azure SQL Data Warehouse, which is now called Azure Synapse. Gen 1 has max size of 240 TB with an ability to grow up to 1 PB with clustered columnstore compression. Plus, with the pause and scale feature, compute costs can be greatly reduced which is what we want here.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-service-capacity-limits\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-best-practices"},{"comments":[{"upvote_count":"1","content":"The next thing they say is it should minimize compute costs. Now if you check the 40 core 204 GB option for sql database under gen 5 and the 32 core 320 GB option for \"database for mysql\" then it seems that \"database for mysql\" should be the answer","comments":[{"comment_id":"185295","comments":[{"upvote_count":"1","poster":"Wisenut","content":"But doesnt Azure DB for MySQL have a 16TB storage limit","comment_id":"280843","timestamp":"1612129560.0","comments":[{"poster":"Cornholioz","content":"Correct. I wouldn't choose MySQL here.\nhttps://docs.microsoft.com/en-us/azure/mysql/concepts-pricing-tiers","upvote_count":"2","comment_id":"287804","timestamp":"1612984560.0"}]}],"timestamp":"1600865580.0","content":"relevant links:\nhttps://azure.microsoft.com/en-in/pricing/details/sql-database/single/\nhttps://azure.microsoft.com/en-us/pricing/details/mysql/server/","poster":"sayak17","upvote_count":"1"}],"comment_id":"185293","poster":"sayak17","timestamp":"1600865580.0"},{"timestamp":"1602331800.0","upvote_count":"1","comment_id":"197309","content":"The answer could be \"A. Azure Database for MySQL\"","poster":"Johnnien"}],"upvote_count":"1","comment_id":"180682","content":"So it says the source data is 200TB. Are they not asking where to store the output data? SQL Database could be sufficient then?","poster":"Shay14","timestamp":"1600317420.0"},{"content":"I think SQL Data Warehouse is correct answer due to 200 TB size of data. It should be C.","comment_id":"179934","upvote_count":"1","comments":[{"poster":"sayak17","comment_id":"185283","timestamp":"1600864920.0","upvote_count":"1","content":"check Gen5 here. it can very well support it https://azure.microsoft.com/en-in/pricing/details/sql-database/single/"}],"poster":"htest2000","timestamp":"1600184760.0"},{"comment_id":"149486","upvote_count":"1","timestamp":"1596423960.0","content":"I just spent some time reading through several articles on this topic, and I'm certain that SQL DB cannot be the solution because of the amount of data is capped around 10 TB. It's cheaper initially but cannot support the premises of this question. The correct answer is Data Warehouse","poster":"avg15","comments":[{"upvote_count":"1","poster":"sayak17","timestamp":"1600865160.0","comment_id":"185287","content":"Check this https://azure.microsoft.com/en-in/pricing/details/sql-database/single/ It can go more than 200 TB. See under Gen5"}]},{"poster":"Atanu","content":"C is the right ans","comment_id":"77369","timestamp":"1587460800.0","upvote_count":"1"},{"timestamp":"1584935400.0","comment_id":"67173","content":"Max instance storage size (reserved) - 2 TB for 4 vCores (Gen5 only)\n- 8 TB for other sizes Gen4: 1 TB\nGen5:\n- 1 TB for 4, 8, 16 vCores\n- 2 TB for 24 vCores\n- 4 TB for 32, 40, 64, 80 vCores\nThese are the max instance for SQL database, the answer should be SQL data warehouse","poster":"putriafebriana","upvote_count":"2","comments":[{"content":"https://stackify.com/azure-sql-database-vs-warehouse/\nAzure SQL Database Azure SQL Data Warehouse\nData type Relational Relational","comment_id":"67174","timestamp":"1584935880.0","poster":"putriafebriana","upvote_count":"3"}]},{"comment_id":"36524","upvote_count":"1","content":"I believe keyword was 'relational', hence the Azure SQL DB response (and low cost).","comments":[{"comment_id":"37085","upvote_count":"4","timestamp":"1578579060.0","content":"Relational data type is relevant for both SQL DB and SQL Warehouse","poster":"Swatishri"}],"timestamp":"1578441060.0","poster":"MK1977"},{"content":"Correct answer should be Azure SQL Data Warehouse","poster":"SwatiMS","timestamp":"1576147140.0","upvote_count":"4","comments":[{"comments":[{"comment_id":"185284","timestamp":"1600864980.0","upvote_count":"1","poster":"sayak17","content":"why? Check Gen5 for sql database here. it can support more than 200TB https://azure.microsoft.com/en-in/pricing/details/sql-database/single/"}],"upvote_count":"5","comment_id":"34126","poster":"Stants","timestamp":"1577857140.0","content":"Only Azure SQL Data Warehouse can support 200TB DB https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-service-capacity-limits"}],"comment_id":"29011"}],"answer":"A","question_id":104,"isMC":true,"question_text":"You are designing an AI application that will use an azure Machine Learning Studio experiment.\nThe source data contains more than 200 TB of relational tables. The experiment will run once a month.\nYou need to identify a data storage solution for the application. The solution must minimize compute costs.\nWhich data storage solution should you identify?","answer_ET":"A","exam_id":39},{"id":"jVKmaP4VskxIvKm9kZk0","discussion":[{"poster":"Nickname__for__discussions","upvote_count":"6","timestamp":"1618314660.0","content":"https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-overview\nAzure Logic Apps is a cloud service that helps you schedule, automate, and orchestrate tasks, business processes, and workflows when you need to integrate apps, data, systems, and services across enterprises or organizations\n\nFrom here I would say that C is correct as \"near real-time\" in the question, and logic apps do have triggers for that. \nI am not sure that this \"near real-time\" condition can be achieved with ADF.","comment_id":"334636"},{"poster":"rveney","timestamp":"1687270860.0","comment_id":"928504","upvote_count":"1","content":"A. An Azure Data Factory pipeline that uses an SAP table connector and a Machine Learning Execute Pipeline activity.\n\nExplanation:\nTo develop a fraud detection API using Azure Machine Learning and identify potentially fraudulent transactions in the SAP production landscape in near real-time, the recommended workflow is to use an Azure Data Factory pipeline that utilizes an SAP table connector and a Machine Learning Execute Pipeline activity.\n\nAzure Data Factory is a cloud-based data integration service that allows you to create pipelines to orchestrate and automate data movement and data transformation. By using the SAP table connector, you can extract data from the SAP production landscape and pass it to the Machine Learning Execute Pipeline activity."}],"answers_community":[],"timestamp":"2021-04-13 13:51:00","answer":"A","answer_images":[],"unix_timestamp":1618314660,"question_images":[],"topic":"2","isMC":true,"answer_ET":"A","answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service","question_id":105,"url":"https://www.examtopics.com/discussions/microsoft/view/50017-exam-ai-100-topic-2-question-61-discussion/","exam_id":39,"choices":{"A":"an Azure Data Factory pipeline that uses an SAP table connector and a Machine Learning Execute Pipeline activity","C":"an Azure logic app triggered by an SAP message that calls the API and sends an email based on the results","D":"a scheduled Jupyter Notebook in Azure Databricks that connects to SAP HANA","B":"a Microsoft Excel workbook that imports SAP transactions and uses the Excel add-in for web services to score the transactions"},"question_text":"You have an SAP production landscape.\nYou plan to use Azure Machine Learning to develop a fraud detection API. The API will identify potentially fraudulent transactions in the SAP production landscape in near real time.\nYou need to recommend a workflow for the API.\nWhat should you recommend?"}],"exam":{"isBeta":false,"isImplemented":true,"numberOfQuestions":206,"name":"AI-100","lastUpdated":"12 Apr 2025","isMCOnly":false,"id":39,"provider":"Microsoft"},"currentPage":21},"__N_SSP":true}