{"pageProps":{"questions":[{"id":"EsbqfxIKzPAddvRA3Tgv","question_text":"A company uses Azure SQL Database to store sales transaction data. Field sales employees need an offline copy of the database that includes last year's sales on their laptops when there is no internet connection available.\nYou need to create the offline export copy.\nWhich three options can you use? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","question_id":86,"unix_timestamp":1573058460,"answer":"BCE","answer_images":[],"choices":{"E":"Export to a BACPAC file by using the SqlPackage utility","C":"Export to a BACPAC file by using the Azure portal","A":"Export to a BACPAC file by using Azure Cloud Shell, and save the file to an Azure storage account","B":"Export to a BACPAC file by using SQL Server Management Studio. Save the file to an Azure storage account","D":"Export to a BACPAC file by using Azure PowerShell and save the file locally"},"exam_id":65,"answer_description":"You can export to a BACPAC file using the Azure portal.\nYou can export to a BACPAC file using SQL Server Management Studio (SSMS). The newest versions of SQL Server Management Studio provide a wizard to export an Azure SQL database to a BACPAC file.\nYou can export to a BACPAC file using the SQLPackage utility.\nIncorrect Answers:\nD: You can export to a BACPAC file using PowerShell. Use the New-AzSqlDatabaseExport cmdlet to submit an export database request to the Azure SQL\nDatabase service. Depending on the size of your database, the export operation may take some time to complete. However, the file is not stored locally.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-export","answers_community":[],"question_images":[],"isMC":true,"timestamp":"2019-11-06 17:41:00","topic":"2","answer_ET":"BCE","discussion":[{"upvote_count":"31","content":"Shouldn't the solution be C, D and E ?\n\nAll tools are valid way to create the BACPAC file, but A and B store it online to Azure Storage, so it cannot be accessed offline","poster":"STH","comments":[{"content":"The question is not how to distribute the offline copy, but how to create the original export, which would then be distributed.","comment_id":"23094","comments":[{"timestamp":"1583517120.0","comment_id":"59978","poster":"goodzilla","upvote_count":"3","comments":[{"poster":"cowtown","comment_id":"126169","upvote_count":"3","timestamp":"1593870300.0","content":"When it's not available != no network access ever."}],"content":"agree with STH\n\"...includes last year's sales on their laptops when there is no internet connection available.\". If there is no internet connection, no way to connect to Azure BLOB storage"}],"poster":"Frederi","upvote_count":"4","timestamp":"1574266380.0"},{"upvote_count":"4","timestamp":"1618114200.0","content":"Without internet how you access azure portal?","poster":"AshCool","comment_id":"332961"},{"poster":"111222333","timestamp":"1620911580.0","comment_id":"356416","content":"You cannot really save a BACPAC file locally using PowerShell either - only to Azure Storage account.\n\nTo export with PowerShell, you must use command \"New-AzSqlDatabaseExport\", and you have to specify a parameter \"-StorageUri\" which specifies Blob location:\nhttps://docs.microsoft.com/en-us/powershell/module/az.sql/new-azsqldatabaseexport?view=azps-5.9.0\n\nSo, answer D also stores it online and hence is not correct.","upvote_count":"2"}],"comment_id":"19561","timestamp":"1573058460.0"},{"content":"Correct Answer : B , C and E","timestamp":"1578484320.0","poster":"cnuusd","upvote_count":"12","comment_id":"36682"},{"poster":"LordSnoek","upvote_count":"1","timestamp":"1623488820.0","comment_id":"380340","content":"its B,D,E - according to Whizlabs"},{"poster":"Hevz","upvote_count":"1","content":"Azure SQL Managed Instance does not currently support exporting a database to a BACPAC file using Azure PowerShell. To export a managed instance into a BACPAC file, use SQL Server Management Studio or SQLPackage.","timestamp":"1621604880.0","comment_id":"363069"},{"comment_id":"359166","poster":"hoangton","timestamp":"1621211700.0","upvote_count":"2","content":"C,D,E because A,B are save data to Azure storage account"},{"poster":"111222333","comment_id":"356386","comments":[{"comment_id":"356422","poster":"111222333","upvote_count":"1","content":"Just to elaborate a bit more on D, I think that Microsoft purposely wrote the incorrect statement \"and save the file locally\" so we can interpret it as an invalid statement. (Even though you can save it to Storage Account and download it locally like with Azure portal.) \nNotice how they do not emphasize where to save files in answers C and E, and how they emphasize Storage Account in answers A and B. That's why I think that the emphasis is on the part \"and save the file locally\".","timestamp":"1620912120.0"}],"timestamp":"1620909900.0","upvote_count":"4","content":"Correct answer is B, C, E.\n\nI came to this conclusion by eliminating incorrect ones.\n\nThere are 4 ways to export an SQL Database to a BACPAC file:\n1. Export using Azure Portal -> only to Storage Account\n2. Export using SQLPackage command-line utility -> to local destination\n3. Export using SQL Server Management Studio -> to local or to Storage Account\n4. Export using PowerShell -> only to Storage Account\n\nSo, answer A is incorrect because you cannot export a BACPAC file by using Cloud Shell.\nAnswer D is incorrect because you cannot save a BACPAC file locally using PowerShell, but only to Storage account.\n\nAnother thing to note is that I think it is assumed that people will download BACPAC file from Azure Storage Account right after it is saved there and have them locally when the Internet breaks."},{"timestamp":"1620712860.0","comment_id":"354361","content":"answer should be C, D & E.\nmoreover Option A & B are itself saying to save the file to Azure Storage Account and getting the file from Azure Storage Account is practically impossible with internet connection.","upvote_count":"2","poster":"Maky2365"},{"poster":"SorinXp","upvote_count":"1","content":"\"Field sales employees need an offline copy of the database that includes last yearג€™s sales on their laptops when there is no internet connection available.\" -> The BACPAC file contains only metadata, no data. How is it possible to include the last year sales?","comment_id":"354076","timestamp":"1620673740.0"},{"content":"CDE are the appropriate answers","comments":[{"comment_id":"354296","poster":"cadio30","upvote_count":"1","content":"B,C,D,E are ways to perform database export process though if the requirement is to have the file locally then the appropriate solution are C,D,E. Both C and D requires azure blob storage to store the copy and in option D, it was stated that it will save the file locally. The option E is straight forward the user indicates the target drive and folder before the export begins. As for option 'B' using it exports the data into the local drive but it was directed to azure storage account.","timestamp":"1620705840.0"}],"poster":"cadio30","upvote_count":"2","comment_id":"348168","timestamp":"1620004020.0"},{"poster":"dbdev","upvote_count":"1","content":"CDE:\nhttps://www.sqlshack.com/how-to-perform-azure-sql-database-import-export-operations-using-powershell/","timestamp":"1619872860.0","comment_id":"346952"},{"timestamp":"1618562520.0","upvote_count":"1","poster":"Hassan_Mazhar_Khan","comment_id":"336866","content":"It should be C,D,E"},{"content":"the key is here \n\"on their laptops when there is no internet connection available\"\n\nIf you don't have an internet connection, how do you access a blob storage?\nI think C, D and E","poster":"Jzerpa_ccs","comment_id":"306372","timestamp":"1615294080.0","upvote_count":"1"},{"poster":"ck1729","upvote_count":"2","content":"B, D, E. Azure Portal cannot be an option. please see link below.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/database-export","timestamp":"1615024200.0","comment_id":"304615"},{"timestamp":"1609959060.0","comment_id":"261227","upvote_count":"1","content":"I expect the issue with the PowerShell option is that while it can be used to export the BACPAC it is not exported to local storage. From the wording of that choice it seems to imply it is a direct export to local storage which is not possible.","poster":"JohnCrawford"},{"upvote_count":"1","poster":"KRV","comment_id":"259836","content":"Option B , D & E , since even though you can create a backup using the Azure Portal, the backup won’t be available locally.\n\n\nExport to a BACPAC file by using SQL Server Management Studio. Save the file to an Azure storage account\nExport to a BACPAC file by using Azure PowerShell and save the file locally\nExport to a BACPAC file by using the SqlPackage utility","timestamp":"1609808580.0"},{"poster":"syu31svc","upvote_count":"2","content":"I would go for BCE as the link provided supports it","timestamp":"1606306440.0","comment_id":"227537"},{"upvote_count":"1","content":"As far as I have searched\n- Export to a BACPAC file by using Azure Cloud Shell is not possible\n- Cannot save the BACPAC file locally via a straight forward Azure powershell command\nSo, the answers given are correct in the sense that they can produce a BACPAC file for consumption.","timestamp":"1601217720.0","comment_id":"188413","poster":"hart232"},{"comment_id":"188131","upvote_count":"1","poster":"sandGrain","timestamp":"1601184480.0","content":"4 possible answers B,C,D,E. But due to offline requirement I think it should be C,D,E. These 3 options do not mention anything where the file are stored. But A & B options specifys where the file is stored, which is az storage account and it is not accessable when offline."},{"timestamp":"1597960440.0","content":"Answer is CDE because ,AB option will store the data in storage account which can not be accessed offline","upvote_count":"3","poster":"Jatinmaya","comment_id":"162516"},{"timestamp":"1597831020.0","poster":"avix","content":"It's BDE . From Azure Portal how can you save it locally?","upvote_count":"1","comment_id":"161403","comments":[{"poster":"devpool","timestamp":"1612614360.0","upvote_count":"1","content":"Azure portal exports BACPC to a storage account. \nB also store file to a storage account.","comment_id":"284792"}]},{"comments":[{"timestamp":"1596639300.0","content":"Exporting a BACPAC of a database from Azure SQL Managed Instance using the Azure portal is not currently supported. Use SQL Server Management Studio or SQLPackage instead.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/database-export","comment_id":"151257","poster":"kilowd","upvote_count":"3"}],"comment_id":"135396","poster":"arkadipb","upvote_count":"1","timestamp":"1594782840.0","content":"B, D and E would be correct, as Azure portal saves the file into Azure storage, thus can't be accessed offline"},{"upvote_count":"2","content":"Can't be B because it doesn't meet the requirement of having an offline copy. the azure storage can only be accessed by the internet. has to be CDE","comment_id":"132490","poster":"HeleneB","timestamp":"1594514940.0"},{"comment_id":"123252","content":"B, C, E is correct.\nFor option B , we can save BACPAC file offline by SSMS and then its our choice to upload it on Storage account.","timestamp":"1593497880.0","comments":[{"comment_id":"127993","upvote_count":"1","poster":"MLCL","content":"A is also correct.","timestamp":"1594053840.0"}],"poster":"Shubham02","upvote_count":"1"},{"upvote_count":"2","comment_id":"106932","timestamp":"1591800840.0","content":"The correct answer is only B and E","poster":"Rohit77"},{"upvote_count":"5","comment_id":"102085","content":"B,D,E ?","poster":"tsobizack","timestamp":"1591240020.0"},{"upvote_count":"1","poster":"diulin","timestamp":"1591031100.0","comment_id":"100190","content":"The only unapropriate answer is A.\nIt well may be that there are 4 correct answers and we can choose B or D along with C and E."},{"comment_id":"94358","timestamp":"1590239700.0","upvote_count":"6","poster":"Luke97","content":"To be honest, I don't feel the answers are stated clear. Azure Portal also CAN'T export BACPAC to local. You need to export to your Azure Storage Account, then download to local drive. So, it is NOT much difference compare to Azure PowerShell. Only two options can truly save BACPAC file directly to local laptop.\n1. From local laptop SSMS and using Export Data Application wizard;\n2. Use SQLPackage utility (which is also local to you as same as SSMS).","comments":[{"upvote_count":"1","comment_id":"259834","poster":"KRV","content":"Luke even though your are absolutely right , since there are 3 answer choices to be selected , ruling out the Azure portal as you rightly mentioned the only other options which survives amongst the options provided is \n\n ○ Export to a BACPAC file by using SQL Server Management Studio. Save the file to an Azure storage account option D so I would say Options B , D & E\n ○ Export to a BACPAC file by using Azure PowerShell and save the file locally\nExport to a BACPAC file by using the SqlPackage utility","timestamp":"1609808460.0"}]},{"poster":"skiwi","comment_id":"75383","content":"Its C, D, E","timestamp":"1587056760.0","upvote_count":"2"},{"comment_id":"64686","content":"using powershell we cant store backup locally, hence C is not right","poster":"azurearch","timestamp":"1584359520.0","upvote_count":"3","comments":[{"upvote_count":"1","content":"i meant D","poster":"azurearch","timestamp":"1584359580.0","comment_id":"64687"}]},{"poster":"tes","timestamp":"1576988040.0","comment_id":"31647","content":"The answers are correct. D is partially true in that it can be created but not to local drive.\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-export","upvote_count":"5"},{"content":"https://docs.microsoft.com/en-us/azure/sql-database/sql-database-export","poster":"tes","timestamp":"1576987920.0","comment_id":"31646","upvote_count":"5"},{"comment_id":"25355","upvote_count":"1","poster":"epgd","content":"If the question is only about how to create the original export, why the option D. Export by Using Power Shell ,is incorrect?¿","timestamp":"1575103920.0"}],"url":"https://www.examtopics.com/discussions/microsoft/view/7753-exam-dp-200-topic-2-question-17-discussion/"},{"id":"aJ6eXQgNueHTXTdRHzfm","question_images":[],"answer_ET":"A","answers_community":[],"answer":"A","topic":"2","answer_images":[],"exam_id":65,"question_text":"Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.\nYou need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.\nSolution:\n1. Create an external data source pointing to the Azure Data Lake Gen 2 storage account\n2. Create an external file format and external table using the external data source\n3. Load the data using the CREATE TABLE AS SELECT statement\nDoes the solution meet the goal?","answer_description":"You need to create an external file format and external table using the external data source.\nYou load the data using the CREATE TABLE AS SELECT statement.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store","timestamp":"2021-12-30 14:55:00","question_id":87,"discussion":[{"poster":"Muishkin","content":"answer should be NO ..As it is from data source & fileformat","timestamp":"1651845420.0","comment_id":"597752","upvote_count":"2"},{"poster":"edba","comment_id":"513430","timestamp":"1640872500.0","upvote_count":"3","content":"should it be \"No\" as last step shall be Create external table ...with (location..,data_source=..,fileformat=..)?"}],"unix_timestamp":1640872500,"choices":{"A":"Yes","B":"No"},"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/69067-exam-dp-200-topic-2-question-18-discussion/"},{"id":"jgJlfUaZN8Vk7BPEoiHF","answer":"B","question_text":"Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.\nYou need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.\nSolution:\n1. Create a remote service binding pointing to the Azure Data Lake Gen 2 storage account\n2. Create an external file format and external table using the external data source\n3. Load the data using the CREATE TABLE AS SELECT statement\nDoes the solution meet the goal?","timestamp":"2021-05-23 16:37:00","isMC":true,"question_id":88,"choices":{"A":"Yes","B":"No"},"exam_id":65,"discussion":[{"comment_id":"431171","poster":"wxlf23","upvote_count":"1","content":"1. Create an external data source pointing to the Azure Data Lake Gen 2 storage account\nvs\n1. Create a remote service binding pointing to the Azure Data Lake Gen 2 storage account\n\nWhich is correct?","timestamp":"1629870600.0"},{"timestamp":"1628863860.0","poster":"Ankush1994","upvote_count":"2","content":"A\nAnswer is correct","comment_id":"424344"},{"timestamp":"1621780620.0","content":"This should be the correct answer","comment_id":"364642","poster":"niwe","upvote_count":"1"}],"answer_ET":"B","answers_community":[],"topic":"2","answer_description":"You need to create an external file format and external table from an external data source, instead from a remote service binding pointing.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store","question_images":[],"answer_images":[],"unix_timestamp":1621780620,"url":"https://www.examtopics.com/discussions/microsoft/view/53423-exam-dp-200-topic-2-question-19-discussion/"},{"id":"ZoU1cjPOVI563FNgo1zE","isMC":true,"choices":{"C":"Use Azure Data Factory UI with Blob storage linked service as a source","A":"Use the Copy Data tool with Blob storage linked service as the source","B":"Use Azure PowerShell with SQL Server linked service as a source","D":"Use the .NET Data Factory API with Blob storage linked service as the source"},"exam_id":65,"discussion":[{"poster":"brcdbrcd","timestamp":"1604590560.0","comment_id":"213525","upvote_count":"35","comments":[{"poster":"tankzzz","upvote_count":"5","comment_id":"340350","content":"This is the answer. 100%","timestamp":"1619013180.0"}],"content":"The answer is B. Use Azure PowerShell with SQL Server linked service as a source\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-powershell"},{"timestamp":"1599144060.0","comment_id":"172728","upvote_count":"7","content":"C says the linked service is Blob storage. The question says it coming from SQL Server. Why would you use Blob storage as the source?","poster":"BungyTex"},{"timestamp":"1620887580.0","upvote_count":"5","content":"Correct answer is B.\n\n1. Both *Azure Data Factory UI (Azure Portal)* and *Azure PowerShell* can be used to incrementally load data from multiple tables in SQL Server to a database in Azure SQL Database.\n2. Both *Azure Data Factory UI (Azure Portal)* and *Azure PowerShell* can use an SQL Server linked service as a source for this incremental copy.\n\nEvidence for these two statements:\n- *ADF GUI*: https://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal\n- *PowerShell*: https://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-powershell\n\nHence, use *Azure Data Factory UI (Azure Portal) with SQL Server linked service* or use *Azure PowerShell with SQL Server linked service*.\n\nSince *ADF with SQL Server linked service* is not an option among the answers (\"Blob storage linked service as a source\" in answer C is really not necessary), the correct answer is B: Use Azure PowerShell with SQL Server linked service as a source.","comment_id":"356075","poster":"111222333"},{"upvote_count":"2","comment_id":"351137","timestamp":"1620315960.0","poster":"Qrm_1972","content":"The correct answer is : 100% is C"},{"timestamp":"1619760720.0","poster":"cadio30","comments":[{"content":"Disregard this and the answer is 'B' as the other connection string relies on azure blob storage and the source is from on-prem SQL Server","comment_id":"349179","upvote_count":"1","poster":"cadio30","timestamp":"1620108900.0"}],"upvote_count":"1","content":"This should be ADF","comment_id":"345901"},{"poster":"Sai2609","comment_id":"341441","timestamp":"1619161980.0","upvote_count":"3","content":"The answer is B since the catch is the movement of data incrementally which can be done easily through powershell"},{"comment_id":"335306","poster":"Nevia","upvote_count":"1","content":"In my opinion the key word is \"pipeline\". ADF UI is the only one that creates a pipeline","comments":[{"comment_id":"360823","poster":"maciejt","content":"ADF is not good with incremental or delta loads. Besides all anwers except B refers to Blob as source and the source is on prem SQL, so it can only be B","upvote_count":"1","timestamp":"1621369020.0"}],"timestamp":"1618388880.0"},{"upvote_count":"2","timestamp":"1614826680.0","content":"looks like the question is based on a scenario presented in one of Microsoft help docs :\nhttps://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf\nthe scenario assuming the following:\nThe Scenario\nWe set up an ADF pipeline that composes two data migration activities. Together they move data on a daily basis between a SQL Server database and Azure SQL Database. The two activities are:\n\nCopy data from a SQL Server database to an Azure Blob Storage account\nCopy data from the Azure Blob Storage account to Azure SQL Database.","comment_id":"303029","poster":"felmasri"},{"timestamp":"1607763540.0","upvote_count":"1","comment_id":"241447","poster":"dumpsm42","content":"hi to all,\nthe text says \"...tool...\" so for me that leaves out B and D.\nanswer C seems right but the source is SQL Server onprem, not the blob storage !\nso for me it's A because its source is SQL onPrem and because of this link:\nhttps://docs.microsoft.com/pt-pt/azure/data-factory/tutorial-hybrid-copy-data-tool\n\nso it's A for me.\n\nregards","comments":[{"comment_id":"241449","timestamp":"1607763660.0","content":"Under New Linked Service, search for SQL Server, and then select Continue.\n\nIn the New Linked Service (SQL Server) dialog box, under Name, enter SqlServerLinkedService. Select +New under Connect via integration runtime. You must create a self-hosted integration runtime, download it to your machine, and register it with Data Factory. The self-hosted integration runtime copies data between your on-premises environment and the cloud.","upvote_count":"1","poster":"dumpsm42"}]},{"comment_id":"181084","comments":[{"comments":[{"poster":"big_data_au","comments":[{"content":"I confirm, ADF copy activity can draw from on prem, data flows need staging on cloud as they run on IR","comment_id":"360824","timestamp":"1621369140.0","upvote_count":"1","poster":"maciejt"}],"comment_id":"213279","timestamp":"1604564460.0","content":"Not if you are using a self hosted integration runtime - ADF can draw directly from on-prem SQL","upvote_count":"4"}],"poster":"poomazuretest","content":"ADF need staging area on cloud using Blob","comment_id":"184982","timestamp":"1600833600.0","upvote_count":"3"}],"content":"Can someone please explain how blob storage comes into picture in this scenario? The data transformation is being carried out using ADF, and the data moves between on premise SQL Server instance and Azure SQL DB.","upvote_count":"3","poster":"anarvekar","timestamp":"1600359360.0"}],"answers_community":[],"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/30498-exam-dp-200-topic-2-question-2-discussion/","answer":"C","question_images":[],"question_id":89,"question_text":"You develop data engineering solutions for a company.\nYou must integrate the company's on-premises Microsoft SQL Server data with Microsoft Azure SQL Database. Data must be transformed incrementally.\nYou need to implement the data integration solution.\nWhich tool should you use to configure a pipeline to copy data?","topic":"2","timestamp":"2020-09-03 16:41:00","answer_description":"The Integration Runtime is a customer managed data integration infrastructure used by Azure Data Factory to provide data integration capabilities across different network environments.\nA linked service defines the information needed for Azure Data Factory to connect to a data resource. We have three resources in this scenario for which linked services are needed:\n✑ On-premises SQL Server\n✑ Azure Blob Storage\n✑ Azure SQL database\nNote: Azure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data. The key concept in the ADF model is pipeline. A pipeline is a logical grouping of Activities, each of which defines the actions to perform on the data contained in\nDatasets. Linked services are used to define the information needed for Data Factory to connect to the data resources.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf","answer_ET":"C","unix_timestamp":1599144060},{"id":"MZwLHtc3x2VUJg1GHvKy","answer_description":"Aparch Spark is an open-source, parallel-processing framework that supports in-memory processing to boost the performance of big-data analysis applications.\nHDInsight is a managed Hadoop service. Use it deploy and manage Hadoop clusters in Azure. For batch processing, you can use Spark, Hive, Hive LLAP,\nMapReduce.\nLanguages: R, Python, Java, Scala, SQL\nYou can create an HDInsight Spark cluster using an Azure Resource Manager template. The template can be found in GitHub.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing","url":"https://www.examtopics.com/discussions/microsoft/view/55159-exam-dp-200-topic-2-question-21-discussion/","choices":{"C":"HDInsight Hadoop Cluster","B":"Azure Stream Analytics","F":"HDInsight Storm Cluster","E":"HDInsight Kafka Cluster","D":"Azure SQL Data Warehouse","A":"HDInsight Spark Cluster"},"answer_ET":"A","isMC":true,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0018500005.png"],"unix_timestamp":1623504360,"exam_id":65,"question_id":90,"timestamp":"2021-06-12 15:26:00","discussion":[{"comment_id":"380484","timestamp":"1623504360.0","poster":"hoangton","content":"Answer is CORECT. key word is in-memory data processing","upvote_count":"4"}],"answers_community":[],"answer":"A","answer_images":[],"question_text":"You need to develop a pipeline for processing data. The pipeline must meet the following requirements:\n✑ Scale up and down resources for cost reduction\n✑ Use an in-memory data processing engine to speed up ETL and machine learning operations.\n✑ Use streaming capabilities\n✑ Provide the ability to code in SQL, Python, Scala, and R\nIntegrate workspace collaboration with Git\n//IMG//\n\nWhat should you use?","topic":"2"}],"exam":{"provider":"Microsoft","id":65,"isImplemented":true,"numberOfQuestions":228,"name":"DP-200","isMCOnly":false,"lastUpdated":"12 Apr 2025","isBeta":false},"currentPage":18},"__N_SSP":true}