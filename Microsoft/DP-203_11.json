{"pageProps":{"questions":[{"id":"aM7VwHIEfEyACyEo8J8f","answer":"B","exam_id":67,"answer_ET":"B","topic":"1","question_id":51,"isMC":true,"unix_timestamp":1620619740,"answer_images":[],"discussion":[{"content":"From the documentation the answer is clear enough. B is the right answer. \nWhen choosing a distribution column, select a distribution column that: \"Is not a date column. All data for the same date lands in the same distribution. If several users are all filtering on the same date, then only 1 of the 60 distributions do all the processing work.\"","upvote_count":"65","comment_id":"421491","timestamp":"1628400900.0","comments":[{"poster":"YipingRuan","content":"To minimize data movement, select a distribution column that:\n\nIs used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses.\n\n\"PurchaseKey\" is not used in the group by","timestamp":"1634446920.0","comment_id":"463414","upvote_count":"8","comments":[{"upvote_count":"3","comment_id":"710280","content":"A distribution column should have high cardinality to ensure even distribution over nodes.","timestamp":"1667455140.0","poster":"cem_kalender"}]},{"poster":"YipingRuan","content":"Consider using the round-robin distribution for your table in the following scenarios:\n\nWhen getting started as a simple starting point since it is the default\nIf there is no obvious joining key\nIf there is no good candidate column for hash distributing the table\nIf the table does not share a common join key with other tables\nIf the join is less significant than other joins in the query","timestamp":"1634446800.0","comment_id":"463412","upvote_count":"7"}],"poster":"AugustineUba"},{"comment_id":"421203","upvote_count":"21","poster":"waterbender19","timestamp":"1628341920.0","content":"I think the answer should be D for that specific query. If you look at the datatypes, DateKey is an INT datatype not a DATE datatype.","comments":[{"poster":"waterbender19","timestamp":"1628350020.0","comments":[{"content":"But the DateKey is used in the WHERE clause.","comments":[{"content":"I agree, date key is int, and besides, even if it was a date, when you query a couple days then 1 million rows per distribution is not that much. So what if you are going to use only a couple distributions to do the job? Isn't it still faster than using all distributions to process all of the records to get the required date range?","comment_id":"567557","upvote_count":"1","poster":"kamil_k","timestamp":"1647250980.0"}],"timestamp":"1641434940.0","comment_id":"517909","poster":"Lucky_me","upvote_count":"4"}],"content":"and thet statement that Fact table will be added 1 million rows daily means that each datekey value has an equal amount of rows associated with that value.","upvote_count":"5","comment_id":"421268"},{"content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute this link says date filed , NOT a date Data type. B is correct","upvote_count":"7","comment_id":"439053","poster":"AnandEMani","timestamp":"1630750920.0"},{"timestamp":"1647251160.0","upvote_count":"2","content":"n.b. if we look at the example query itself the date range is 31 days so we will use 31 distributions out of 60, and only process ~31 million records","comment_id":"567560","poster":"kamil_k"}]},{"timestamp":"1742549100.0","content":"Selected Answer: D\nThe Query is filtering particular date range,. so fast processing hash distribution is applied in date column.","comment_id":"1401487","poster":"Ammy_b","upvote_count":"1"},{"poster":"jay0809","content":"Selected Answer: D\nD, this avoids scanning irrelevant data which helps other query operations which triggers data movement to move less data as it's filtered\n\nB does not make sense at all coz everytime this will be a unique value","upvote_count":"1","timestamp":"1742474100.0","comment_id":"1401021"},{"comment_id":"1356928","poster":"IMadnan","upvote_count":"1","timestamp":"1739636700.0","content":"Selected Answer: D\nHash distribution is generally preferred for large fact tables. Among the hash distribution options, hash-distributed on DateKey seems most beneficial because the query has a WHERE clause filtering on DateKey. Distributing on DateKey will help to localize data access for the given query pattern, minimizing data movement and potentially improving query performance.\n\nTherefore, Option D: hash-distributed on DateKey is the most likely correct answer."},{"comment_id":"1354633","timestamp":"1739218020.0","poster":"dippip123","upvote_count":"1","content":"Selected Answer: B\nIn addition to not using a Date column, the azure documentation also says:\n\n\nTo minimize data movement, select a distribution column or set of columns that:\nIs not used in WHERE clauses. When a query's WHERE clause and the table's distribution columns are on the same column, the query could encounter high data skew, leading to processing load falling on only few distributions. This impacts query performance, ideally many distributions share the processing load."},{"content":"Selected Answer: D\nd is correct","timestamp":"1739135700.0","comment_id":"1354131","upvote_count":"1","poster":"samirarian"},{"upvote_count":"3","timestamp":"1737233820.0","content":"Selected Answer: B\nHi Community, it's B. We are not talking about partitions. We are talking about distribution and we know that its a poor choice to choose a low cardinality column like datekey for a distribution.","comment_id":"1342739","poster":"JustImperius"},{"content":"Selected Answer: D\nIn the Where clause, DateKey is the one which can affect to query.\nPartition should be with DateKey","timestamp":"1734435300.0","upvote_count":"1","poster":"nockda","comment_id":"1327870"},{"comment_id":"1325214","upvote_count":"1","timestamp":"1733947680.0","content":"Selected Answer: D\nHash-distributed on DateKey\nCharacteristics:\nRows are distributed based on the hash of DateKey, which is also the column used in the query filter (WHERE DateKey >= ... AND DateKey <= ...).\nThis ensures that rows satisfying the query predicate are grouped on fewer nodes, minimizing data movement and speeding up the query.\nSuitability:\nPerfect for the FactPurchase table in this scenario because:\nThe query filters on DateKey.\nFiltering and aggregation on DateKey avoids significant data movement across nodes","poster":"Imtiaz_alum"},{"comment_id":"1325204","upvote_count":"1","poster":"nockda","content":"Selected Answer: D\nAnswer should be D. \nPurchaseKey is not related with this query.","timestamp":"1733946240.0"},{"timestamp":"1733413560.0","comment_id":"1322420","upvote_count":"1","content":"Selected Answer: B\nEven if Datekey is int, it is used in \"WHERE\" clause so it is not suitable as well.\n\n1: \"Is used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses. When two large fact tables have frequent joins, query performance improves when you distribute both tables on one of the join columns. When a table is not used in joins, consider distributing the table on a column or column set that is frequently in the GROUP BY clause.\"\n\n2: \"Is not used in WHERE clauses. When a query's WHERE clause and the table's distribution columns are on the same column, the query could encounter high data skew, leading to processing load falling on only few distributions. This impacts query performance, ideally many distributions share the processing load.\"","poster":"Daniel627"},{"poster":"f7c717f","upvote_count":"1","content":"Selected Answer: D\nThe answer is \"D\" since the question is referring to \"three years of data\" so they are asking for date partitioning","comment_id":"1321047","timestamp":"1733160060.0"},{"poster":"moize","upvote_count":"1","content":"Selected Answer: D\nD. Distribué par hachage sur DateKey\n\nCette distribution optimise les requêtes qui filtrent par DateKey en minimisant les mouvements de données entre les nœuds.","comment_id":"1320931","timestamp":"1733142060.0"},{"timestamp":"1732770660.0","poster":"EmnCours","comment_id":"1319026","content":"Selected Answer: D\nCorrect Answer: D","upvote_count":"1"},{"poster":"nockda","comment_id":"1317690","upvote_count":"1","timestamp":"1732559220.0","content":"Selected Answer: D\nIt should be D."},{"poster":"nockda","upvote_count":"1","comment_id":"1313676","timestamp":"1731864240.0","content":"Selected Answer: D\nThe answer should be D. because this query is searched by datekey"},{"content":"To optimize parallel processing in Azure Synapse Analytics, it's important to select a distribution column or set of columns that:\n\nHas many unique values.\nDoes not have many nulls.\nIs not a date column.\nPartitioning by DateKey does not meet these criteria. Date columns typically have limited unique values, can contain many nulls, and, by nature, are date fields. Given these constraints, DateKey is not suitable for distribution.\n\nFor fact tables, which usually exceed 2 GB in size, a hash distribution is recommended. In this scenario, using PurchaseKey as the distribution key is ideal as it aligns with all the specified criteria: it has many unique values, is unlikely to have nulls, and is not a date column. This approach ensures more efficient data distribution and query performance.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","timestamp":"1724484000.0","comment_id":"1271563","upvote_count":"1","poster":"renan_ineu"},{"comment_id":"1252723","upvote_count":"1","poster":"evangelist","content":"Selected Answer: D\nThe real approach in actual project is limiting the dateKey:\n DistributionKey AS HASHBYTES('MD5', CONCAT(YEAR(CAST(DateKey AS DATE)), FORMAT(CAST(DateKey AS DATE), 'MM')))\n)\nWITH\n(\n DISTRIBUTION = HASH(DistributionKey),\n CLUSTERED COLUMNSTORE INDEX\n);","timestamp":"1721597700.0"},{"timestamp":"1720928340.0","comment_id":"1247598","upvote_count":"1","content":"Selected Answer: B\ntotal votes on B:85 on D; 23","poster":"evangelist"},{"upvote_count":"1","comment_id":"1247597","poster":"evangelist","content":"Selected Answer: D\nAnswer could be only D","timestamp":"1720928220.0"},{"poster":"Danweo","upvote_count":"1","content":"Selected Answer: B\nYou don't want to hash on the Date column generally, definitely not when its being included in the Where clause, PurchaseKey is the only acceptable option given as the table is too large for roundrobin. I want to know if those other columns in the group by could possibly be used also?","timestamp":"1720261500.0","comment_id":"1243311"},{"content":"Question 20 and 39 Is same","comments":[{"content":"is not the same: \"D. hash-distributed on IsOrderFinalized\"","poster":"practia","comment_id":"1254980","timestamp":"1721909340.0","upvote_count":"1"}],"poster":"kitesh1994","comment_id":"1198971","upvote_count":"2","timestamp":"1713593040.0"},{"timestamp":"1709624400.0","poster":"AKTommy","comment_id":"1166284","content":"Selected Answer: B\nB is my correct answer","upvote_count":"2"},{"upvote_count":"1","poster":"pawades","timestamp":"1708158900.0","comment_id":"1152448","content":"Explain me something - if you use Purchasekey as a hash distribution, and then want to do a partition, which column will you use for partition, we mostly date column for partition, but if we use date column then during query execution where you want to query data for let's say Jan month, wouldn't the query will need data from multiple nodes? eventually slowing the results? isn't it easy to keep data on a single node get faster results. Am I missing anything here?"},{"upvote_count":"1","comments":[{"poster":"lola_mary5","comment_id":"1145790","upvote_count":"1","timestamp":"1707512100.0","content":"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choose-a-distribution-column-with-data-that-distributes-evenly"}],"poster":"moneytime","timestamp":"1706622360.0","content":"For me.\nI chose D.\nThe reason is that the\" datekey\" is of tyoe \"integer\" not \" Date\" .This qualifies it to be used as a non-auto-incremental surrogate key for the fact table.","comment_id":"1135845"},{"upvote_count":"3","comment_id":"1103944","timestamp":"1703323380.0","content":"Selected Answer: B\nSomething not immediately clear to me was that distributing and partitioning are different, hence I was confused that one should not distribute over date columns. \n\nBottom line is, do not distribute over date columns but you can partition over them. In this question they specifically ask about distribution method. Query optimization for large tables directly points to hashing.","poster":"jongert"},{"comment_id":"1082899","timestamp":"1701201300.0","upvote_count":"4","content":"You want to distribute by productKey and partition by date. Then all distributions will be looked at in parallel and then, within each distribution, only the desired partitions will be looked at. Thereby, the query is fully scaled out and the quickest it can be.","poster":"MarkJoh"},{"poster":"AlejandroU","content":"B) the chosen distribution column should not be used in WHERE clauses; thus, we can discard DateKey (even though it is not a Date data type) to minimize data movement. The chosen distribution column must have many unique values; thus we potentially have 2 candidates: PurchaseKey or PurchaseOrderID; however, the chosen one should have no NULLS or only a few, making PurchaseKey the ideal in order to distribute evenly.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","timestamp":"1697878740.0","comment_id":"1049370","upvote_count":"4"},{"poster":"Vanq69","comment_id":"1024126","content":"Selected Answer: D\nIs there any \"official\" answer to this?\n A. Replicated: Replicated tables have copies of the entire table on each distribution. While this option can eliminate data movement, it may not be the most efficient choice for very large tables with frequent updates.\n B. Hash-Distributed on PurchaseKey: Hash distribution on \"PurchaseKey\" may lead to data skew if \"PurchaseKey\" doesn't have a wide range of unique values. Additionally, it doesn't align with the primary filtering condition on \"DateKey.\"\n C. Round-Robin: Round-robin distribution ensures even data distribution, but it doesn't take advantage of data locality for specific types of queries.\n D. Hash-Distributed on DateKey: Distributing on \"DateKey\" aligns with your primary filtering condition, but it's a date column. This could lead to clustering by date, especially if many users filter on the same date.\n\nNone of the answers seem to fit. D could be the best guess but it's a date column.","upvote_count":"2","timestamp":"1696356060.0"},{"comment_id":"997616","poster":"kkk5566","timestamp":"1693745160.0","upvote_count":"1","content":"Selected Answer: B\nB is correct"},{"poster":"gozdek","timestamp":"1688033280.0","content":"Selected Answer: C\nB is total nonsense if PurchaseKey has a unique value for every row it would end up distributing it evenly so same as round-robin. Distributing by date would slow down the query because in a situation presented in the question only 31 out of 60 distributions would be used. So in my opinion C is the correct answer.","comment_id":"937879","upvote_count":"1"},{"content":"Selected Answer: B\nB is the correct Answer","poster":"SHENOOOO","comment_id":"796127","timestamp":"1675348560.0","upvote_count":"2"},{"upvote_count":"1","content":"To me the answer should be D. \nA query on the table that has a WHERE clause filtering on column A will perform partition elimination and scan one partition. That same query may run faster in scenario 2 as there are fewer rows to scan in a partition. A query that has a WHERE clause filtering on column B will scan all partitions. The query may run faster in scenario 1 than in scenario 2 as there are fewer partitions to scan.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/partitions/partitioned-tables-and-indexes?view=sql-server-ver16\n\nWanted to hear from the experts here.","comment_id":"783777","poster":"DindaS","timestamp":"1674339240.0"},{"upvote_count":"2","comment_id":"781099","timestamp":"1674130320.0","content":"Selected Answer: B\nB is the obvious answer. Hash is optimized for higher analytical performance while Round-robin is optimized for higher loading speed.","poster":"shakes103"},{"content":"B is right","poster":"Deeksha1234","upvote_count":"1","comment_id":"636811","timestamp":"1658764920.0"},{"timestamp":"1656178500.0","poster":"vlad888","comment_id":"622212","content":"Anyone who even one time run similar query in Synapse and look into execution plan understand that PurchaseKey doesn't help: there will be shuffle move dms operation! I suppose all these qyestions has mistake here. Because only column from GROUP BY clause will help. Or round_robin (although it will has almost the same cost as PurchaseKey if last one evenly distributed)","upvote_count":"1"},{"timestamp":"1647641280.0","comment_id":"570771","poster":"Ramkrish39","content":"Agree B is the right answer","upvote_count":"1"},{"timestamp":"1643356740.0","upvote_count":"2","comment_id":"534522","poster":"PallaviPatel","content":"Selected Answer: C\nI will go with round robin. \n''Consider using the round-robin distribution for your table in the following scenarios:\n\nWhen getting started as a simple starting point since it is the default\nIf there is no obvious joining key\nIf there is no good candidate column for hash distributing the table\nIf the table does not share a common join key with other tables\nIf the join is less significant than other joins in the query"},{"poster":"yovi","upvote_count":"1","content":"Anyone, when you finish an exam, do they give you the correct answers in the end?","comment_id":"510699","comments":[{"poster":"dev2dev","content":"those finished exam will not know the answer. because answers are not reveled","upvote_count":"1","timestamp":"1641908520.0","comment_id":"521563"}],"timestamp":"1640651760.0"},{"poster":"Mahesh_mm","content":"B is correct ans","upvote_count":"1","comment_id":"510421","timestamp":"1640618340.0"},{"comment_id":"509477","content":"Selected Answer: B\nIt's correct","timestamp":"1640506080.0","poster":"danish456","upvote_count":"2"},{"poster":"trietnv","timestamp":"1640165880.0","content":"Selected Answer: B\n1. choose distribution b/c \"joining a round-robin table usually requires reshuffling the rows, which is a performance hit\"\n2. Choose PurchaseKey b/c \"not used in WHERE\"\nrefer:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\nand\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","comment_id":"506890","upvote_count":"3"},{"upvote_count":"1","comment_id":"499543","content":"Selected Answer: B\nB is correct","poster":"Aslam208","timestamp":"1639242240.0"},{"comment_id":"483113","timestamp":"1637483280.0","content":"Selected Answer: B\nIts cleary a hash on purchasekey column","poster":"Hervedoux","upvote_count":"3"},{"content":"Took the exam today, this question came out. \nAns: B","poster":"ohana","timestamp":"1635024900.0","comment_id":"466752","upvote_count":"5"},{"timestamp":"1631048220.0","upvote_count":"5","comment_id":"441097","poster":"Marcus1612","content":"To optimize the MPP, data have to be distributed evenly. Datekey is not a good candidate because the data will be distributed evenly one day per 60 days. In practice, if many users query the fact table to retreive the data about the week before, only 7 nodes will process the queries instead of 60. According to microsoft documentation:\"To balance the parallel processing, select a distribution column that .. Is not a date column. All data for the same date lands in the same distribution. If several users are all filtering on the same date, then only 1 of the 60 distributions do all the processing work.","comments":[{"timestamp":"1631048340.0","comment_id":"441098","upvote_count":"2","poster":"Marcus1612","content":"the good answer is B"}]},{"content":"The reference given in the answer is precise: Choose a distribution column with data that a) distributes evenly b) has many unique values c) does not have NULLs or few NULLs and d) IS NOT A DATE COLUMN... definitely the best choice for the Hash distribution is on the Identity column.","comment_id":"415574","timestamp":"1627401300.0","poster":"andimohr","upvote_count":"4"},{"comment_id":"402466","upvote_count":"1","comments":[{"comment_id":"402468","poster":"noone_a","timestamp":"1625813460.0","content":"edit, this is incorrect as it will have 1 million records added daily for 3 years, putting it over 2GB","upvote_count":"4"}],"timestamp":"1625813340.0","content":"although its a fact table, replicated is the correct distribution in this case.\nEach row is 141 bytes in size x 1000000 records = 135Mb total size\n\nMicrosoft recommend replicated distribution for anything under 2GB.\n\nWe have no further information regarding table growth so this answer is based only on the info provided.","poster":"noone_a"},{"comment_id":"392948","content":"Yes - do not use date column - there is such recomendation in synapse docs. But here we have range search - potensiallu several nodes will be used.","timestamp":"1624885320.0","poster":"vlad888","upvote_count":"1"},{"timestamp":"1624885200.0","poster":"vlad888","content":"Actually it is clear that it should be hash distributed. BUT Product key brings no benefit for this query - doesn't participated in it at all. So - DateKey. Although it is unusual for Synapse","upvote_count":"4","comment_id":"392942"},{"upvote_count":"2","comment_id":"386767","timestamp":"1624250520.0","content":"I don't think there is enough information to decide this. Also we can not decide it by just looking at one query. Only considering this query and if we assume no other dimensions are connected to this fact table, good answer would be D.","poster":"savin"},{"comments":[{"upvote_count":"1","comment_id":"392938","content":"Avoiding partition - compute node to be precise - is least desirable thing - it is mpp system. 60 nodes performs work faster then 5.","timestamp":"1624884840.0","poster":"vlad888"}],"upvote_count":"1","comment_id":"378333","content":"My answer goes with D...\nIn most cases data is partitioned on a date column that is closely tied to the order in which the data is loaded into the SQL pool. Partitioning improves query performance. A query that applies a filter to partitioned data can limit the scan to only the qualifying partitions thereby improving performance dramatically as filtering can avoid a full table scan and only scan a smaller subset of data. It also seems, the data partitioned on date will get distributed uniformly across the nodes thereby avoiding a partition to be hot partition.","timestamp":"1623247380.0","poster":"ChandrashekharDeshpande"},{"content":"Agree to B","comment_id":"367399","upvote_count":"3","timestamp":"1622053860.0","poster":"bc5468521"},{"comments":[{"timestamp":"1622107380.0","content":"The question is about this exact query. To minimize the time for this query you should distribute the work. But - if we do hash distribution on date column this will utilize at most 30 distributions. Round robin would be a good choice if this is really the only query we run, but we probably want to join with other tables on the primary key. So hash distribution on the primary key might be better choice. If we assume uniform primary key distribution, hashing on the PK will have the effect of round robin. - hence B is the correct answer.","poster":"baobabko","comments":[{"poster":"DrC","comment_id":"368651","timestamp":"1622197080.0","content":"Also: 1 million rows of data added daily and will contain three years of data.\nIt will have over a billion rows when loaded.\nThat will put it over the 2GB recommendation for hash-distributed.\n\nConsider using a hash-distributed table when:\n* The table size on disk is more than 2 GB.\n* The table has frequent insert, update, and delete operations.","upvote_count":"1","comments":[{"content":"Only round robin will use all 60 partitions. There is no join Key.","poster":"lsdudi","timestamp":"1626408000.0","comment_id":"407569","upvote_count":"1"}]}],"upvote_count":"7","comment_id":"367749"}],"upvote_count":"2","content":"Round robin looks to be the best fit","comment_id":"354081","timestamp":"1620673980.0","poster":"Ritab"},{"content":"\"Not D: Do not use a date column. . All data for the same date lands in the same distribution. If several users are all filtering on the same date, then only 1 of the 60 distributions do all the processing work.\" ???\nthe same implies for ProductKey, now forgiven query we may need to check every record for the date, so checking all 60 distribution ???","comment_id":"353439","upvote_count":"2","poster":"Pradip_valens","comments":[{"poster":"freerider","upvote_count":"3","content":"According to the reference there are multiple things that makes it inappropiate to use the date column:\nIs not used in WHERE clauses. This could narrow the query to not run on all the distributions.\nIs not a date column. WHERE clauses often filter by date. When this happens, all the processing could run on only a few distributions.\n\nReplicated is unlikely to be correct since it's to much data (a million rows per day for the last 3 years).\n\nThey also use the product key in the reference example.","timestamp":"1620730620.0","comment_id":"354624"},{"comments":[{"poster":"yolap31172","timestamp":"1633975920.0","comment_id":"460751","content":"I don't think we should assume anything that's not provided (like there will be queries with joins). Round robin will 100% work to minimize time for this query. Hash distribution on PurchaseKey *should* work... unless we are unlucky and hashing results in some skew.","upvote_count":"1"}],"comment_id":"367747","timestamp":"1622107320.0","upvote_count":"2","content":"The question is about this exact query. To minimize the time for this query you should distribute the work. But - if we do hash distribution on date column this will utilize at most 30 distributions. Round robin would be a good choice if this is really the only query we run, but we probably want to join with other tables on the primary key. So hash distribution on the primary key might be better choice. If we assume uniform primary key distribution, hashing on the PK will have the effect of round robin.","poster":"baobabko"}],"timestamp":"1620619740.0"}],"question_text":"You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns.\n//IMG//\n\nFactPurchase will have 1 million rows of data added daily and will contain three years of data.\nTransact-SQL queries similar to the following query will be executed daily.\n\nSELECT -\nSupplierKey, StockItemKey, COUNT(*)\n\nFROM FactPurchase -\n\nWHERE DateKey >= 20210101 -\n\nAND DateKey <= 20210131 -\nGROUP By SupplierKey, StockItemKey\nWhich table distribution will minimize query times?","url":"https://www.examtopics.com/discussions/microsoft/view/52251-exam-dp-203-topic-1-question-39-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0008200001.png"],"answers_community":["B (58%)","D (36%)","7%"],"choices":{"A":"replicated","C":"round-robin","B":"hash-distributed on PurchaseKey","D":"hash-distributed on DateKey"},"timestamp":"2021-05-10 06:09:00","answer_description":""},{"id":"xe4KRXR5oPwQrntJW3pc","topic":"1","discussion":[{"upvote_count":"124","comments":[{"upvote_count":"5","comment_id":"1212635","timestamp":"1715907600.0","poster":"anthony854","content":"Serverless SQL pool can recursively traverse folders if you specify /** at the end of path. The following query will read all files from all folders and subfolders located in the csv/taxi folder.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files"},{"comment_id":"463542","timestamp":"1634475600.0","upvote_count":"41","poster":"captainpike","content":"I tested and proove you right, the answer is B. Remind the question is referring to serverless SQL and not dedicated SQL pool. \"Unlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any subfolder.\""}],"content":"I believe the answer should be B. \nIn case of a serverless pool a wildcard should be added to the location.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table","comment_id":"347458","poster":"Chillem1900","timestamp":"1726814520.0"},{"upvote_count":"31","content":"\"Serverless SQL pool can recursively traverse folders only if you specify /** at the end of path.\"\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files","poster":"alain2","comments":[{"poster":"Preben","timestamp":"1623081000.0","comment_id":"376887","upvote_count":"22","content":"When you are quoting from Microsoft documentation, do not ADD in words to the sentence. 'Only' is not used."}],"comment_id":"360135","timestamp":"1726814520.0"},{"upvote_count":"1","poster":"technoguy","comments":[{"content":"sorry it should be B","timestamp":"1743017760.0","upvote_count":"1","comment_id":"1410535","poster":"technoguy"}],"content":"Selected Answer: D\nbecause we have not given wildcard expression","timestamp":"1743017700.0","comment_id":"1410534"},{"content":"Selected Answer: C\nyou do not need to use LOCATION='/topfolder/*' to read files in subfolders when using external tables in serverless SQL pools.\nYou use wildcards like '/topfolder/*' when querying directly using OPENROWSET","poster":"ayn1","upvote_count":"1","comment_id":"1401413","timestamp":"1742519640.0"},{"comment_id":"1400597","upvote_count":"1","content":"Selected Answer: B\nCorrect answer is B","timestamp":"1742403120.0","poster":"ngabonzic"},{"content":"Selected Answer: C\nIf you specify LOCATION to be a folder, a PolyBase query that selects from the external table will retrieve files from the folder and all of its subfolders. Just like Hadoop, PolyBase doesn't return hidden folders. It also doesn't return files for which the file name begins with an underline (_) or a period (.).","poster":"AMJB","upvote_count":"1","comment_id":"1395593","timestamp":"1741955940.0"},{"content":"Selected Answer: B\nIf you specify LOCATION to be a folder, a PolyBase query that selects from the external table will retrieve files from the folder and all of its subfolders. Just like Hadoop, PolyBase doesn't return hidden folders. It also doesn't return files for which the file name begins with an underline (_) or a period (.).","poster":"AMJB","timestamp":"1741955580.0","comment_id":"1395590","upvote_count":"1"},{"upvote_count":"1","timestamp":"1739118480.0","poster":"Pey1nkh","content":"Selected Answer: B\nWhen you query ExtTable with LOCATION='/topfolder/', only the files directly inside /topfolder/ will be returned.","comment_id":"1354005"},{"upvote_count":"1","timestamp":"1736902560.0","poster":"krishna1303","comment_id":"1340580","content":"Selected Answer: B\nthe answer should be B"},{"content":"Selected Answer: B\nThe answer is A, the answer C corresponds to LOCATION='/topfolder/**'","poster":"EmnCours","upvote_count":"1","comment_id":"1317924","timestamp":"1732599900.0"},{"upvote_count":"1","poster":"seranvijay","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files#traverse-folders-recursively","timestamp":"1731032760.0","comment_id":"1308624"},{"comment_id":"460937","timestamp":"1727075580.0","content":"Correct answer , I test it and it works :\n\nCREATE EXTERNAL TABLE ext2 ([User] varchar(40), Feature varchar(40), Event varchar(40) , Time varchar(40))\n WITH (\n LOCATION = '/topfolder',\n DATA_SOURCE = ds2,\n FILE_FORMAT = csvFormat\n )\nGO\nSELECT COUNT(*) as cnt FROM ext2\nGO","comments":[{"content":"This is for serverless SQL. What you showed is for dedicated SQL, behaviour is different as explained here: \"Unlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any subfolder.\"","poster":"captainpike","upvote_count":"2","comment_id":"463545","timestamp":"1634475660.0"}],"upvote_count":"2","poster":"medsimus"},{"poster":"poundmanluffy","content":"Selected Answer: B\nOption is definitely \"B\"\n\nBelow is the documentation given on MS Docs:\n\nRecursive data for external tables\n\nUnlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any sub-folder.","comment_id":"576165","timestamp":"1727075580.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"633678","timestamp":"1727075580.0","content":"(Seems from this document that Answer is \"C\")In SQL Server, the CREATE EXTERNAL TABLE statement creates the path and folder if it doesn't already exist. You can then use INSERT INTO to export data from a local SQL Server table to the external data source. For more information, see PolyBase Queries.\n\nIf you specify LOCATION to be a folder, a PolyBase query that selects from the external table will retrieve files from the folder and all of its subfolders. Just like Hadoop, PolyBase doesn't return hidden folders. It also doesn't return files for which the file name begins with an underline (_) or a period (.).\n\nIn this example, if LOCATION='/webdata/', a PolyBase query will return rows from mydata.txt and mydata2.txt. It won't return mydata3.txt because it's a file in a hidden folder. And it won't return _hidden.txt because it's a hidden file.\n\nRecursive data for external tables\n\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver15&tabs=dedicated","poster":"Raza12"},{"content":"Selected Answer: B\n\"native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any sub-folder.\"\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table","upvote_count":"2","comment_id":"722604","timestamp":"1727075520.0","poster":"rohitbinnani"},{"content":"Selected Answer: B\nI believe that the right answer is B, because from the root directory, there will be listed two files and two folders","poster":"lesardinha","upvote_count":"1","comment_id":"1241603","timestamp":"1720032840.0"},{"comment_id":"1238975","timestamp":"1719623100.0","content":"Selected Answer: B\nCREATE EXTERNAL TABLE ExtTable (\n EmployeeId INT,\n EmployeeName VARCHAR(100),\n EmployeeStartDate DATE\n)\nWITH (\n LOCATION = '/topfolder/**',\n DATA_SOURCE = your_data_source,\n FILE_FORMAT = your_file_format\n);","poster":"evangelist","upvote_count":"1"},{"timestamp":"1719622920.0","comment_id":"1238974","content":"Selected Answer: C\nchoice is C, I dont understand why such a simple question trigger such many discussions","upvote_count":"2","poster":"evangelist"},{"poster":"Nadine_nm","content":"I think the answer should be B, because there are only two possibilites that would allow to return all the subfolders : \n- A hadoop external table\n- The query contains LOCATION='/webdata/**\nif this is a native external tables, they don't return subfolders","upvote_count":"1","comment_id":"1237991","timestamp":"1719478020.0"},{"timestamp":"1718489700.0","comment_id":"1231132","poster":"KeiNek","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated#location--folder_or_filepath-1","upvote_count":"3"},{"timestamp":"1717477560.0","comment_id":"1223916","poster":"Sirishagiri1","content":"Selected Answer: B\nIt is B","upvote_count":"1"},{"poster":"gplusplus","upvote_count":"1","comment_id":"1181259","timestamp":"1711243800.0","content":"Selected Answer: B\nNot recursive wo **"},{"upvote_count":"5","comment_id":"1165288","timestamp":"1709526000.0","poster":"AKTommy","content":"I have created this case in my Datalake & Synapse Severless SQL pool and run sql as below:\nselect top 10 *\nfrom openrowset(\n bulk 'topfolder/',\n data_source = 'Test',\n format = 'csv',\n parser_version = '2.0',\n firstrow = 2\n ) \nwith (\n EmployeeId int\n) as rows\n\nThe answer for this one is B (only 2 files returned)"},{"timestamp":"1706942700.0","upvote_count":"1","content":"Selected Answer: B\nAnswer should be B.","comment_id":"1139024","poster":"rocky48"},{"comment_id":"1113857","poster":"bomafrique","timestamp":"1704384780.0","content":"Answer B is correct for me too.","upvote_count":"1"},{"poster":"sdg2844","timestamp":"1704235380.0","content":"Selected Answer: B\nAgree it should be B. The question is a little off, because they don't specify whether using or not using a wildcard to do so. Assuming by default then, no wildcard is used, only those top-level files will be returned.","upvote_count":"2","comment_id":"1112332"},{"comment_id":"1101684","poster":"lisa710","timestamp":"1703086200.0","content":"answer c is correct","upvote_count":"1"},{"upvote_count":"1","poster":"blnak32","timestamp":"1701372180.0","comment_id":"1084709","content":"Selected Answer: B\nStrongly B: (solid reason with reference )\n1. This query uses Serverless Pool and it is only available for native External Table\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table\n2. \"native external tables don't return subfolders unless you specify /** at the end of path\"\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&tabs=serverless#location--folder_or_filepath-1"},{"content":"The answer is B based on the below doc\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver16&tabs=dedicated","poster":"Shanuramasubbu","upvote_count":"1","comment_id":"1076784","timestamp":"1700605500.0"},{"comments":[{"timestamp":"1718025960.0","poster":"yespmedleon","upvote_count":"1","comment_id":"1227887","content":"Based on the link you provide, it shows an exact copy of the image in the question. \n\nIt clearly states:\n\nUnlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any sub-folder."}],"upvote_count":"1","comment_id":"1063905","poster":"y154707","timestamp":"1699279560.0","content":"Question says \"You create an external table named ExtTable that has LOCATION='/topfolder/'. \"\n\nBased on this link: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&tabs=serverless#location--folder_or_filepath-1\n\n\"If you specify LOCATION to be a folder, a PolyBase query that selects from the external table will retrieve files from the folder and all of its subfolders. Just like Hadoop, PolyBase doesn't return hidden folders. It also doesn't return files for which the file name begins with an underline (_) or a period (.).\"\n\nSo based on this, the answer is correct. When created, the ExtTable get data from files on the topfolder and all of its subfolders, thus when queried it would return the data from all the files."},{"timestamp":"1694493660.0","upvote_count":"1","poster":"74gjd_37","content":"Selected Answer: B\nSince there were no wildcards (/**) no nested folders are used.","comment_id":"1005368"},{"timestamp":"1694157420.0","poster":"[Removed]","upvote_count":"1","content":"Selected Answer: A\nNative external table","comment_id":"1002233"},{"upvote_count":"1","content":"Selected Answer: B\ncorrect","poster":"kkk5566","comment_id":"993100","timestamp":"1693313880.0"},{"comment_id":"980687","timestamp":"1692008940.0","content":"the correct answer is B","upvote_count":"1","poster":"lfss"},{"comment_id":"972202","poster":"akhil5432","upvote_count":"1","timestamp":"1691155980.0","content":"Selected Answer: B\nMOST SUITED ans ib option B"},{"timestamp":"1686521040.0","content":"Selected Answer: B\nB should be the answer","comment_id":"921008","poster":"Deeksha1234","upvote_count":"1"},{"timestamp":"1686413400.0","comment_id":"920167","poster":"VikkiC","upvote_count":"2","content":"This documentation confirmed the answer B is correct.\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&tabs=serverless#location--folder_or_filepath-1"},{"comment_id":"905525","timestamp":"1684907940.0","content":"When you query ExtTable by using an Azure Synapse Analytics serverless SQL pool, only File1.csv and File4.csv will be returned [B].\n\nWhen you create an external table with LOCATION=‘/topfolder/’, only the files that are directly under the specified folder will be returned. In this case, only File1.csv and File4.csv are directly under the /topfolder/ directory and will be returned when querying ExtTable.","poster":"bakamon","upvote_count":"1"},{"upvote_count":"1","content":"The answer is B","timestamp":"1682394900.0","poster":"rocky48","comment_id":"879885"},{"timestamp":"1681198260.0","poster":"mamahani","content":"have the question been rephrased or sth? because as of 11/04/2023 it does not show which query has been used to actually retrieve the data; if the query contained the wildcard, then all the files would be retrieved; and if it did not contain, only two of them; for that reason should i receive this question in exact this form on the exam, i will go for C, because knowing how to write the query, I would be able to retrieve all the files;","comment_id":"867056","upvote_count":"5"},{"timestamp":"1679052300.0","poster":"esaade","upvote_count":"1","content":"Selected Answer: B\nIn this case, the LOCATION parameter is set to '/topfolder/', so only files located directly in that folder will be included in the results. Therefore, the query will only return File1.csv and File4.csv. The correct answer is B.","comment_id":"841928"},{"poster":"cashew","upvote_count":"1","comment_id":"824573","timestamp":"1677574680.0","content":"so it's either B or C, depending on which doc. ref. you look at.\nAnswer is B based on: [synapse-analytics ref](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table)\nAnswer is C based on: [t-sql ref](https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&tabs=dedicated#location--folder_or_filepath-1)\n\nfrom testing out practically in azure synapse, i find that the behaviour follows answer B as per synapse-analytics ref. \n\nthe explicit requirement to have <path>/** for recursiveness is similar to the linux terminal interface where you go \"ls -r\" or \"rm -r\", having /** for recursive behaviour is better(less ambiguous) and more aligned with standards i suppose."},{"poster":"bubby248","upvote_count":"1","comment_id":"802223","timestamp":"1675872780.0","content":"B, as there is no /**"},{"content":"Selected Answer: C\nIf you specify LOCATION to be a folder, a PolyBase query that selects from the external table will retrieve files from the folder and all of its subfolders. So it will return all.\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated","comment_id":"801050","timestamp":"1675783080.0","upvote_count":"4","poster":"INDEAVR","comments":[{"content":"Polybase is used in dedicated SQL pool; the question is about the serveless SQL pool;","upvote_count":"3","timestamp":"1681198020.0","comment_id":"867049","poster":"mamahani"}]},{"poster":"Venub28","content":"B is the right answer","timestamp":"1673713260.0","upvote_count":"2","comment_id":"775705"},{"timestamp":"1672814760.0","comment_id":"765321","content":"Answer is B","upvote_count":"2","poster":"Gargipati"},{"upvote_count":"2","content":"Selected Answer: B\nWithout the wildcard this wont return file 2 and file 3 so b is the answer","comment_id":"760192","timestamp":"1672251540.0","poster":"MarkTek"},{"poster":"RV123","content":"Selected Answer: B\nB is correct","upvote_count":"2","comment_id":"749601","timestamp":"1671436560.0"},{"poster":"vigilante89","comment_id":"746793","content":"Serverless SQL pool can recursively traverse folders if you specify /** at the end of path.\n\nLocation = '/topfolder/' will show only the files within THIS folder.\nLocation = '/topfolder/**' will show all the files within this folder along with all files within sub-folders.","upvote_count":"3","timestamp":"1671164040.0"},{"poster":"brzhanyu","timestamp":"1669820280.0","upvote_count":"1","comment_id":"731636","content":"Selected Answer: B\nB is correct"},{"poster":"gerrie1979","comment_id":"701517","content":"I tested the solution out in Azure Synapse with all the necessary steps connected to the Data Lake Gen2 storage account and answer B is correct, the subdirectories are not taken into account.","timestamp":"1666440120.0","upvote_count":"1"},{"poster":"Jawidkaderi","comment_id":"682208","content":"I kind of agree with captainpike. However, I have my reason also:\nI am reading the questions as such:\nthe table is created inside the /topfolder/\nwhich means the new table is in the same line as: File1.csv, /folder1/, /folder2/, File4.csv\nSo, I think [A] is the correct answer. File2.csv and File3.csv","timestamp":"1664408700.0","upvote_count":"2"},{"poster":"smsme323","upvote_count":"1","timestamp":"1663830120.0","comment_id":"675814","comments":[{"content":"correct, only with location='/topfolder/**' it will have access to all the subfolders","poster":"gerrie1979","timestamp":"1667058240.0","comment_id":"707297","upvote_count":"1"}],"content":"Selected Answer: B\nneed wildcard to access all files."},{"content":"Selected Answer: C\nGiven answer C is Correct !!","poster":"anks84","comment_id":"662833","timestamp":"1662579300.0","upvote_count":"2"},{"content":"Selected Answer: C\nI agree with @temacc, C","timestamp":"1661423760.0","upvote_count":"2","poster":"yyyhhh","comment_id":"651743"},{"upvote_count":"2","timestamp":"1660118700.0","poster":"examtopicscap","content":"Selected Answer: B\ncorrect B","comment_id":"644854"},{"content":"Unlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any sub-folder.","upvote_count":"3","timestamp":"1659592200.0","poster":"Raza12","comment_id":"642183"},{"timestamp":"1658769480.0","comment_id":"636849","upvote_count":"1","content":"B is the correct. Same example in: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=serverless","poster":"Janisys"},{"comment_id":"621529","timestamp":"1656061500.0","upvote_count":"2","content":"Answer : B In serverless SQL pools must be specified /** at the end of the location path. In Dedicated pool the folders are alwasy scanned recursively.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","poster":"Nishikag"},{"poster":"temacc","upvote_count":"5","timestamp":"1655817360.0","comment_id":"619825","content":"Selected Answer: C\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver15&tabs=dedicated#location--folder_or_filepath"},{"content":"C is correct as mentionned in the official documentation which showcase a similar example : https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver15&tabs=dedicated#location--folder_or_filepath","poster":"amiral404","upvote_count":"4","timestamp":"1653301920.0","comment_id":"605991"},{"comment_id":"600054","poster":"Dothy","timestamp":"1652265840.0","content":"I believe the answer should be B.","upvote_count":"1"},{"timestamp":"1651730400.0","comment_id":"597140","poster":"carloalbe","upvote_count":"1","content":"In this example, if LOCATION='/webdata/', a PolyBase query will return rows from mydata.txt and mydata2.txt. It won't return mydata3.txt because it's a file in a hidden folder. And it won't return _hidden.txt because it's a hidden file. https://docs.microsoft.com/en-us/sql/t-sql/statements/media/aps-polybase-folder-traversal.png?view=sql-server-ver15b\n\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver15&tabs=dedicated"},{"poster":"BJPJowee","comment_id":"593530","upvote_count":"1","timestamp":"1651121580.0","content":"the answer is correct. C see the link https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver15&tabs=dedicated"},{"upvote_count":"1","content":"Ans is definitely B","timestamp":"1651041720.0","poster":"MS_Nikhil","comment_id":"592915"},{"comment_id":"569154","poster":"Ozren","content":"Selected Answer: B\nThis is not a recursive pattern like '.../**'. So the answer is B, not C.","timestamp":"1647445500.0","upvote_count":"2"},{"comments":[{"upvote_count":"3","poster":"kamil_k","comment_id":"565650","timestamp":"1727075580.0","content":"ok I've done the test:\n1. created gen 2 storage acct\n2. created azure synapse workspace\n3. created container myfilesystem, subfolder topfolder and another subfolder topfolder under that\n4. created two csv files and dropped one per folder i.e. one in topfolder and the other in topfolder/topfolder\n5. executed the following code: \n\nDROP EXTERNAL DATA SOURCE test;\nCREATE EXTERNAL DATA SOURCE test\nWITH\n ( \n LOCATION = 'https://[storage-account-name].blob.core.windows.net/myfilesystem' \n )\n\nCREATE EXTERNAL FILE FORMAT test \nWITH ( \n FORMAT_TYPE = DELIMITEDTEXT, \n FORMAT_OPTIONS ( \n FIELD_TERMINATOR = ',',\n FIRST_ROW = 2\n )\n); \n\nCREATE EXTERNAL TABLE test\n(id int, value int)\nWITH ( \n LOCATION='/topfolder/', \n DATA_SOURCE = test, \n FILE_FORMAT = test\n);\n\nSELECT * FROM test;\n\nThe result were only records from File1.csv which was located in the first \"topfolder\".","comments":[{"poster":"kamil_k","content":"in other words, the answer C is incorrect. I forgot to mention I used the built-in serverless SQL Pool","comment_id":"565652","timestamp":"1647016440.0","upvote_count":"2"}]},{"poster":"islamarfh","comment_id":"577248","content":"this is the from the document tell that B is indeed correct \nIn this example, if LOCATION='/webdata/', a PolyBase query will return rows from mydata.txt and mydata2.txt. It won't return mydata3.txt because it's a file in a hidden folder. And it won't return _hidden.txt because it's a hidden file.","timestamp":"1648529340.0","upvote_count":"1"}],"poster":"kamil_k","comment_id":"565302","content":"this one is tricky, I found information here which would suggest answer C is indeed correct:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver15&tabs=dedicated#arguments-2","timestamp":"1646983380.0","upvote_count":"1"},{"timestamp":"1646463300.0","upvote_count":"2","content":"Selected Answer: B\nI believe the answer should be B.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table","poster":"RalphLiang","comment_id":"561279"},{"upvote_count":"2","comment_id":"555945","content":"Selected Answer: B\nTested. Ans: B","timestamp":"1645791360.0","poster":"KosteK"},{"content":"Answer is C\nIf you specify LOCATION to be a folder, a PolyBase query that selects from the external table will retrieve files from the folder and all of its subfolders. \nRefer https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver15&tabs=dedicated","poster":"toms100","timestamp":"1644377580.0","comment_id":"543511","upvote_count":"3"},{"timestamp":"1643099340.0","poster":"PallaviPatel","content":"Selected Answer: B\nB is correct answer.","comment_id":"531977","upvote_count":"2"},{"poster":"Sandip4u","upvote_count":"2","comment_id":"519992","timestamp":"1641711780.0","content":"The answer is B , In case of a serverless pool a wildcard should be added to the location , otherwise this will not fetch the files from child folders"},{"content":"Selected Answer: B\nas we need ** o pick all files","upvote_count":"2","timestamp":"1641249360.0","comment_id":"516108","poster":"bharatnhkh10"},{"upvote_count":"1","comment_id":"514864","poster":"dev2dev","timestamp":"1641106200.0","content":"C is Wrong, B is correct because the files under subfolders need /** wild card. See Traverse folders recursively.\nhttps://functions-app-temp.azurewebsites.net/api/HttpTriggeredFunction?code=KV8oVoFcIsm8uNTzI7pvxUq6goH6UPfBUYwNj8lcrhe7YrMQ5G8bng=="},{"upvote_count":"1","poster":"SabaJamal2010AtGmail","comment_id":"511746","timestamp":"1640748000.0","content":"File1.csv and File4.csv only is only returned because LOCATION='/topfolder/' To return all files specify LOCATION='/topfolder/**’"},{"poster":"Mahesh_mm","upvote_count":"1","comment_id":"509610","content":"Location should be 'topfolder/**' to access subfolder. so answer is B","timestamp":"1640524920.0"},{"poster":"m2shines","comment_id":"501435","timestamp":"1639493280.0","content":"Selected Answer: B\nAnswer is B","upvote_count":"1"},{"timestamp":"1639059780.0","comment_id":"497789","poster":"ploer","content":"Selected Answer: B\nAS ** ist missing no files in subfolders will be used in serverless pool","upvote_count":"1"},{"poster":"Huepig","comment_id":"495057","upvote_count":"2","timestamp":"1638787860.0","content":"Selected Answer: B\nThe query used in the question will only read all the files in the folder.\nTo query the files in the folder recursively, the query must specify '/**'\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files#read-all-files-from-specific-folder"},{"timestamp":"1637656860.0","content":"B is the correct answer. According to Azure online document,\nUnlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any sub-folder.","upvote_count":"3","comment_id":"484835","poster":"PortlandFighters"},{"content":"B is the correct answer.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset","poster":"Lucky_me","timestamp":"1635426660.0","upvote_count":"1","comment_id":"469271"},{"timestamp":"1633416360.0","comments":[{"comments":[{"timestamp":"1634548800.0","poster":"sravanvenu","upvote_count":"1","comment_id":"464002","content":"yes, worked for me"}],"timestamp":"1634475720.0","poster":"captainpike","comment_id":"463546","content":"correct but I couldn't make /** to work. could you?","upvote_count":"1"}],"upvote_count":"1","poster":"sravanvenu","content":"Option B is correct. Subfolders are traversed if ** is included after folder path\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files#traverse-folders-recursively","comment_id":"457544"},{"timestamp":"1626866580.0","upvote_count":"4","content":"The answer is B","poster":"elimey","comment_id":"410835"},{"content":"Answer is B. \nC can be the answer only if there are wildcards in the path \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files","comments":[{"timestamp":"1633374360.0","upvote_count":"1","poster":"estrelle2008","content":"So, look in the answer, between parentheses \"provide a path to a folder or a pattern\n(using wildcards)\" \nThus C (using wildcard **), I guess?","comment_id":"457319"}],"timestamp":"1626463200.0","upvote_count":"3","poster":"AKC11","comment_id":"408074"},{"comment_id":"393761","content":"Answer should be B. Please fix in the exam question.","poster":"InvisibleShadow","upvote_count":"4","timestamp":"1624969080.0"},{"upvote_count":"6","timestamp":"1622121780.0","comment_id":"367966","content":"Go for B","poster":"bc5468521"},{"poster":"wfrf92","timestamp":"1620674100.0","upvote_count":"7","content":"Unlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any sub-folder.\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","comment_id":"354082"}],"answer_ET":"B","answer_description":"","answers_community":["B (72%)","C (25%)","1%"],"unix_timestamp":1619938980,"url":"https://www.examtopics.com/discussions/microsoft/view/51494-exam-dp-203-topic-1-question-4-discussion/","isMC":true,"answer":"B","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0002000001.png"],"answer_images":[],"timestamp":"2021-05-02 09:03:00","question_id":52,"exam_id":67,"choices":{"C":"File1.csv, File2.csv, File3.csv, and File4.csv","A":"File2.csv and File3.csv only","B":"File1.csv and File4.csv only","D":"File1.csv only"},"question_text":"You have files and folders in Azure Data Lake Storage Gen2 for an Azure Synapse workspace as shown in the following exhibit.\n//IMG//\n\nYou create an external table named ExtTable that has LOCATION='/topfolder/'.\nWhen you query ExtTable by using an Azure Synapse Analytics serverless SQL pool, which files are returned?"},{"id":"SKVXDLRkShM3f7rQtLDT","url":"https://www.examtopics.com/discussions/microsoft/view/67468-exam-dp-203-topic-1-question-40-discussion/","question_text":"You are implementing a batch dataset in the Parquet format.\nData files will be produced be using Azure Data Factory and stored in Azure Data Lake Storage Gen2. The files will be consumed by an Azure Synapse Analytics serverless SQL pool.\nYou need to minimize storage costs for the solution.\nWhat should you do?","answers_community":["A (68%)","C (23%)","9%"],"topic":"1","answer_description":"","unix_timestamp":1639090500,"question_images":[],"answer_ET":"A","discussion":[{"comment_id":"501585","timestamp":"1639504860.0","upvote_count":"75","poster":"m2shines","comments":[{"timestamp":"1710497940.0","poster":"Homer23","upvote_count":"2","comment_id":"1174205","content":"I found this comparison of compression methods, which explained that A should not be the answer.\nhttps://www.linkedin.com/pulse/comparison-compression-methods-parquet-file-format-saurav-mohapatra/\n\"BROTLI : This is a relatively new codec which offers very high compression ratio , but with lower compression and decompression speeds. This codec is useful when storage space is a major constraint. This technique also offers parallel processing that other methods don't.\""},{"comment_id":"521781","timestamp":"1641937800.0","poster":"assU2","comments":[{"upvote_count":"2","poster":"jongert","content":"Very confused at first, after thinking about it and rereading this is what I found:\nIt says we are implementing the batch process in parquet format, so we should think about a situation where we write the file and specify snappy compression as an argument explicitly. \n\nThe phrasing is very confusing I have to say, but if you argue from a 'query externally' perspective, then B and C would yield the same benefit. Therefore, A makes the most sense and connects best with the question.","timestamp":"1703325060.0","comment_id":"1103955"}],"content":"Isn't snappy a default compressionCodec for parquet in azure?\nhttps://docs.microsoft.com/en-us/azure/data-factory/format-parquet","upvote_count":"24"}],"content":"Answer should be A, because this talks about minimizing storage costs, not querying costs"},{"timestamp":"1639242720.0","poster":"Aslam208","upvote_count":"23","comments":[{"poster":"Massy","timestamp":"1651240260.0","content":"in serverless sql pool you don't create a copy of the data, so how could be cost effective?","upvote_count":"2","comment_id":"594501","comments":[{"poster":"Bro111","timestamp":"1669556460.0","content":"Don't forget that there is Transaction cost part of storage cost, so taking a subset of columns will lower transaction cost consequently storage cost.","upvote_count":"1","comment_id":"728258"}]},{"upvote_count":"5","timestamp":"1653229740.0","comment_id":"605562","content":"This is not correct. \n1. External tables are are not saved in the database. (This is why they're external)\n2. You're assuming that the SQL Serverless pools have a local storage. They don't -- > https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool","comments":[{"upvote_count":"2","poster":"Aditya0891","comments":[{"comment_id":"618077","content":"check this https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas. Answer C is correct","upvote_count":"4","poster":"Aditya0891","timestamp":"1655525280.0"}],"timestamp":"1655524680.0","comment_id":"618071","content":"well there is a possibility to create an external table and load only the required columns using openrowset in serverless sql pool to a different container in ADLS. Remember serverless sql pool does support cetas with openrowset but dedicated pool doesn't support loading data using openrowset. So basically the solution could be load the required columns using cetas using openrowset to a differnet container and delete the source data from previous container after loading the filtered data to a different container in ADLS"}],"poster":"RehanRajput"}],"content":"C is the correct answer, as an external table with a subset of columns with parquet files would be cost-effective.","comment_id":"499551"},{"upvote_count":"1","comment_id":"1411671","content":"Selected Answer: C\nC is correct answer","poster":"PrasadMP","timestamp":"1743244800.0"},{"timestamp":"1739636880.0","content":"Selected Answer: A\nParquet is a columnar storage format that inherently provides compression benefits. Applying Snappy compression on top of Parquet's internal compression will further reduce the storage footprint of the files in Azure Data Lake Storage Gen2. Snappy is a well-suited compression codec for analytical workloads, offering a good balance between compression ratio and decompression speed, which is important for Azure Synapse Analytics serverless SQL pool to efficiently query the data. Options B, C, and D do not directly address minimizing storage costs for the Parquet files themselves. Option B is about query access, Option C is about query efficiency but not storage, and Option D is counterproductive to storage cost minimization.","poster":"IMadnan","upvote_count":"1","comment_id":"1356930"},{"poster":"moize","timestamp":"1733142720.0","content":"Selected Answer: A\nA. Utilisez la compression Snappy pour les fichiers\n\nCette approche permet de réduire la taille des fichiers Parquet, ce qui minimise les coûts de stockage dans Azure Data Lake Storage Gen2 tout en restant compatible avec Azure Synapse Analytics.","upvote_count":"1","comment_id":"1320939"},{"poster":"EmnCours","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs-legacy","upvote_count":"1","comment_id":"1319027","timestamp":"1732771080.0"},{"upvote_count":"1","comment_id":"1277085","content":"Selected Answer: A\nUsing Snappy compression (Option A) is specifically designed to reduce the size of Parquet files, thereby directly minimizing storage costs.","poster":"a85becd","timestamp":"1725326220.0"},{"poster":"Danweo","upvote_count":"1","comment_id":"1243317","content":"Selected Answer: C\nThe question is confusing but I believe it is C, because you can use CETAS to store this external table in Gen2 (this is the storage solution), from there you will query it using serverless SQL pool.","timestamp":"1720262220.0"},{"content":"A, B and C they are all acceptable, D is just stupid\nBut pay attention to \"You need to minimize storage costs for the solution\" that means snappy parquet compresson - A is correct","timestamp":"1714468380.0","upvote_count":"2","comment_id":"1204442","poster":"Dusica"},{"timestamp":"1713157980.0","upvote_count":"3","comment_id":"1195815","content":"Selected Answer: A\nUse Snappy compression for the files is the only answer, which is about minimizing cost of storage. While one is using serverless SQL pool, the external tables are available, which are the only metadata...","poster":"dgerok"},{"timestamp":"1711951440.0","content":"Using Snappy compression for the Parquet files helps minimize storage costs while still maintaining good compression efficiency. Snappy is a compression library that offers a good balance between compression ratio and processing speed. By compressing the data using Snappy, you can significantly reduce the amount of storage required for your dataset.\n\nOption B, using OPENROWSET to query the Parquet files, doesn't directly impact storage costs. It's a method for querying data but doesn't address storage optimization.\n\nOption C, creating an external table with a subset of columns, may help reduce query costs by minimizing the amount of data that needs to be processed during queries. However, it doesn't directly address storage costs.\n\nOption D, storing all data as strings in the Parquet files, would likely increase storage costs rather than minimize them. Storing data as strings without appropriate compression would result in larger file sizes compared to using efficient compression algorithms like Snappy.","comment_id":"1187232","poster":"Elanche","upvote_count":"6"},{"comment_id":"1186092","upvote_count":"2","content":"A. Use Snappy compression for the files.","poster":"ankeshpatel2112","timestamp":"1711802880.0"},{"timestamp":"1709167680.0","upvote_count":"3","poster":"Zen9nez","content":"The answer is C - Parquet has default SNAPPY compression which cannot be overwritten so why would I apply SNAPPY again?","comment_id":"1162102"},{"comment_id":"1157966","poster":"s_unsworth","timestamp":"1708782360.0","content":"Selected Answer: A\nFurther information required for this question. There isn't enough information to go off as to what is being asked. The initial question is in regards to storage which would result in using the snappy compression answer. If you are asking about querying the data then this should be clearly defined in the question. If someone was to create a User Story with regards to this (As a Manager I want to store data in the data lake at the reduced cost) then you wouldn't be providing them with an External table. You would give them information on storage.","upvote_count":"2"},{"timestamp":"1704904860.0","content":"Selected Answer: A\nSnappy compression can reduce the size of Parquet files by up to 70%. This can save you a significant amount of money on storage costs.","upvote_count":"1","comment_id":"1118839","poster":"Joanna0"},{"upvote_count":"2","comment_id":"1002301","poster":"[Removed]","content":"Selected Answer: A\nSnappy","timestamp":"1694163900.0"},{"timestamp":"1693745400.0","content":"Selected Answer: A\nusing compression","upvote_count":"2","comment_id":"997621","poster":"kkk5566"},{"upvote_count":"1","timestamp":"1692868320.0","comment_id":"989021","content":"To minimize storage costs for the solution, you should use Snappy compression for the files. Snappy is a fast and efficient data compression and decompression library that can be used to compress Parquet files. This will help reduce the size of the data files and minimize storage costs in Azure Data Lake Storage Gen2. So, the correct answer is A. Use Snappy compression for the files","poster":"kkk5566"},{"poster":"andjurovicela","upvote_count":"1","content":"When presented with only the options of column pruning (variant of this is C) and compression (example of it would be snappy), ChatGPT choses C.","timestamp":"1690277940.0","comment_id":"962556"},{"comment_id":"937012","upvote_count":"10","content":"Selected Answer: A\nI would go by exclusion:\nA. Use Snappy compression for the files. ---> nothing against this!\nB. Use OPENROWSET to query the Parquet files. --> doing this I just get a preview of the parquet files\nC. Create an external table that contains a subset of columns from the Parquet files. --> no body asked for a subset\nD. Store all data as string in the Parquet --> nobody asked that","timestamp":"1687977840.0","comments":[{"content":"Storing data as a string would also make the file size bigger","timestamp":"1690295280.0","poster":"semauni","comment_id":"962813","upvote_count":"1"}],"poster":"auwia"},{"poster":"VittalManikonda","comment_id":"932546","upvote_count":"4","content":"As per chat gpt , answer is A","timestamp":"1687610340.0"},{"timestamp":"1687551240.0","poster":"vctrhugo","comment_id":"931921","upvote_count":"3","content":"Selected Answer: A\nSnappy compression is a popular and efficient compression algorithm for Parquet files. It provides a good balance between compression ratio and query performance. By compressing the Parquet files using Snappy, you can significantly reduce the storage footprint, leading to lower storage costs.\n\nC is not effective for minimizing storage costs: While creating an external table with a subset of columns can help reduce storage costs, it doesn't specifically address the Parquet format or compression. This option is more related to data modeling and selecting specific columns for query performance rather than minimizing storage costs."},{"comment_id":"885252","upvote_count":"14","content":"Selected Answer: A\nA. Use Snappy compression for the files.\n\nThe most effective way to minimize storage costs for the solution is to use compression. Parquet files can be compressed using a variety of codecs, including Snappy, which provides a good balance between compression ratio and query performance. By compressing the data, the file size is reduced, which results in lower storage costs.\n\nOption B, using OPENROWSET to query the Parquet files, is a method to query the data, but it does not address storage costs.\n\nOption C, creating an external table that contains a subset of columns from the Parquet files, is a useful optimization to reduce the amount of data scanned and therefore reduce query cost, but it does not address storage costs.\n\nOption D, storing all data as strings in the Parquet files, is not a good approach because it would result in larger file sizes and potentially slower query performance due to the need to convert the data back to its original format.\n\nAnswer is A","timestamp":"1682860920.0","poster":"rocky48"},{"timestamp":"1681714380.0","comment_id":"872424","poster":"mamahani","upvote_count":"3","content":"Selected Answer: C\ni belive the answer is correct; i dont think it should be A, see ms docs \"(from the last link: However, when writing to a Parquet file, the service (i.e. data factory) chooses SNAPPY, which is the default for Parquet format. Currently, there is no option to override this behavior.”)\nthis means data factory will already use snappy compression, and we cannot do anything about it, we cant change that; so how can this be a correct answer? if we are forced to use snappy, then we dont choose to apply snappy; i believe external table will be here the answer","comments":[{"comment_id":"872433","upvote_count":"1","content":"also see the question 41;","timestamp":"1681715040.0","poster":"mamahani"}]},{"content":"Selected Answer: C\nAnswer should be C","upvote_count":"1","timestamp":"1679478840.0","comment_id":"846905","poster":"Hisayuki"},{"content":"Selected Answer: A\nA. Use Snappy compression for the files.\n\nThe Snappy compression algorithm provides a good balance between compression ratio and decompression speed, making it an efficient choice for Parquet files. By compressing the files, you can significantly reduce the storage costs associated with storing large volumes of data. Additionally, Snappy compression is supported by both Azure Data Lake Storage Gen2 and Azure Synapse Analytics, making it an ideal solution for this scenario.\n\nOption B is not a valid solution as OPENROWSET cannot directly query the Parquet files stored in Azure Data Lake Storage Gen2.\n\nOption C may be a possible solution, but it requires additional configuration steps, such as creating external data sources, credentials, and schema mapping.\n\nOption D is not a valid solution because storing all data as string in Parquet files will increase the storage requirements and may impact performance during data retrieval.","comment_id":"839003","timestamp":"1678809240.0","poster":"esaade","upvote_count":"6"},{"content":"Selected Answer: A\noption A is correct because que is about storage not about quering","timestamp":"1675937940.0","comment_id":"803070","poster":"DAYENKAR","upvote_count":"1"},{"poster":"jhargett1","comment_id":"792159","timestamp":"1675034940.0","content":"A. Use Snappy compression for the files.\n\nSnappy is a high-performance data compression algorithm that is specifically designed for use with Parquet files. It is an open-source algorithm that is known to provide a good balance between compression ratio and processing speed. By using Snappy compression, you can reduce the overall storage costs for the data files, as the compressed files will take up less storage space.\n\nB. Using OPENROWSET to query the Parquet files is a way to query external data from a SQL Server instance, but it does not minimize storage costs.\n\nC. Creating an external table that contains a subset of columns from the Parquet files is a way to optimize the performance of queries, but it does not minimize storage costs.\n\nD. Storing all data as string in the Parquet files will increase the storage size and hence it will increase the storage costs, not minimize it.","upvote_count":"7"},{"comments":[{"timestamp":"1674663840.0","comment_id":"787887","content":"When you query the external table, the SQL pool retrieves the data from the Data Lake Gen2 storage account and processes it in memory, it doesn't store a copy of the data in the SQL pool, so it doesn't require extra storage.","poster":"Lestrang","upvote_count":"1"}],"timestamp":"1674663780.0","comment_id":"787884","upvote_count":"3","poster":"Lestrang","content":"I do not have a certain solution but..\nA. using snappy\nit is default, so unless it was another format and changing to snappy. otherwise it is snappy by default, so using it again will not reduce costs because it is already snappy\n\ncreating an external table doesn't have to do with querying costs.\ncreating a standard table has its own storage costs attached.\nbut an external table serves as a virtual table and allows you to query the data in the external data source using standard SQL. The external table serves as a virtual table and allows you to query the data in the external data source using standard SQL.\n\nSo this is reducing storage compared to creating a standard table in the dataset. \n\nI am more inclined to c."},{"upvote_count":"1","content":"Selected Answer: A\nThe answer is A. Consider the compression codec to use when writing to Parquet files. When reading from Parquet files, Data Factories automatically determine the compression codec based on the file metadata.\nSupported types are \"none\", \"gzip\", \"snappy\" (default), and \"lzo\".\n\nAlso you must consider the question is asking for storage cost and not operational (querying included) cost.\nhttps://learn.microsoft.com/en-us/azure/data-factory/format-parquet#:~:text=The%20compression%20codec%20to%20use%20when%20writing%20to%20Parquet%20files.%20When%20reading%20from%20Parquet%20files%2C%20Data%20Factories%20automatically%20determine%20the%20compression%20codec%20based%20on%20the%20file%20metadata.%0ASupported%20types%20are%20%22none%22%2C%20%22gzip%22%2C%20%22snappy%22%20(default)%2C%20and%20%22lzo%22.","comment_id":"781111","timestamp":"1674130860.0","poster":"shakes103"},{"upvote_count":"1","timestamp":"1673574960.0","comment_id":"774004","poster":"nicky87654","content":"Selected Answer: C\nCOORECT ---->C"},{"timestamp":"1673184420.0","poster":"niravjoshi00711","comment_id":"769454","upvote_count":"1","content":"Selected Answer: C\nit will be C"},{"content":"The correct answer is:\n\nA: Use Snappy compression for the files.","upvote_count":"1","timestamp":"1672164360.0","poster":"akk_1289","comment_id":"758895"},{"timestamp":"1671472980.0","poster":"tembal","comment_id":"750112","content":"A good option to minimize storage costs for the solution would be to use Snappy compression for the files. Snappy is a fast, open-source compressor that is well-suited for compressing large data files, such as those produced by Azure Data Factory. Snappy compression can help to significantly reduce the size of the files, which can in turn help to reduce storage costs.","upvote_count":"1"},{"poster":"Jay_98_11","comment_id":"738887","upvote_count":"1","content":"Selected Answer: C\nShould be C","timestamp":"1670495280.0"},{"timestamp":"1668857640.0","poster":"OldSchool","comment_id":"721964","content":"Selected Answer: B\nAnswer is B. No additional storage cost.\nSELECT *\nFROM OPENROWSET(BULK '/folder/*.parquet',\n DATA_SOURCE='storage', --> Root URL is in LOCATION of DATA SOURCE\n FORMAT = 'PARQUET') AS [file]","upvote_count":"1"},{"comments":[{"poster":"kl8585","timestamp":"1668158820.0","comment_id":"715907","upvote_count":"1","content":"*A , sorry"}],"timestamp":"1668158820.0","upvote_count":"1","comment_id":"715906","content":"Selected Answer: A\nC because is the only one talking about STORAGE costs. The other options are effective for QUERY costs.","poster":"kl8585"},{"timestamp":"1668098280.0","poster":"dmov","content":"I believe specifically for storage costs it should be A","comment_id":"715390","upvote_count":"1"},{"comment_id":"712550","timestamp":"1667758500.0","content":"A seems correct","upvote_count":"1","poster":"sumanthss"},{"upvote_count":"2","comment_id":"711193","timestamp":"1667570100.0","poster":"Igor85","content":"only A has something to do with storage. B and C are simply the ways how serverless pool reads the files from ADLS"},{"poster":"ted0809","upvote_count":"1","content":"Serverless task.. answer is B no doubt.\nno storage cost.","comment_id":"707360","timestamp":"1667064780.0"},{"timestamp":"1666073160.0","comment_id":"697956","content":"The explanation is strong enough to support answer C. I think A should be the correct answer. If it is C, explain it. Question is about minimizing the cost not about better performance.","upvote_count":"1","poster":"AdarshKumarKhare"},{"comment_id":"646867","timestamp":"1660495440.0","content":"Selected Answer: A\nA seems to be correct","upvote_count":"2","poster":"Deeksha1234"},{"upvote_count":"1","comment_id":"628790","timestamp":"1657288020.0","content":"Selected Answer: C\nExternal tables are used to read data from files or write data to files in Azure Storage. With Synapse SQL, you can use external tables to read external data using dedicated SQL pool or serverless SQL pool.","poster":"dsp17"},{"poster":"sdokmak","timestamp":"1653434460.0","comment_id":"606946","content":"Selected Answer: B\nI agree with Canary2021","upvote_count":"2"},{"comments":[{"upvote_count":"2","poster":"uzairahm","comment_id":"622048","timestamp":"1656145500.0","content":"Exactly external tables are external and kind of queried as views (just for understanding) so no extra storage costs incurred"}],"content":"Selected Answer: C\nnot A - The default compression for a parquet file is SNAPPY. Even in Python as well.\nC - because an external table that contains a subset of columns from the Parquet files will not need re-saving them in databases and that would save storage costs.","comment_id":"583670","poster":"rohitbinnani","upvote_count":"10","timestamp":"1649592240.0"},{"comments":[{"upvote_count":"1","poster":"Machinery","comment_id":"656452","timestamp":"1662049800.0","content":"Answer seems a bit misleading. While it's true that Snappy can increase file size, it should generally decrease it. The file size at rest is not usually larger. I do think the answer is A is not correct."}],"content":"The answer is NOT A. Snappy compression offers fast compression, but file size at rest is larger which will translate into higher storage cost. The answer is C where an external table with requisite columns is made available which will reduce the amount of storage","poster":"DingDongSingSong","comment_id":"575321","upvote_count":"5","timestamp":"1648257420.0"},{"poster":"cotillion","comment_id":"541712","content":"Selected Answer: A\nOnly A has sth to do with the storage","upvote_count":"2","timestamp":"1644151560.0"},{"upvote_count":"1","comment_id":"534532","timestamp":"1643358180.0","poster":"PallaviPatel","content":"A looks to be correct."},{"upvote_count":"3","poster":"dev2dev","comments":[{"upvote_count":"2","poster":"Ramkrish39","timestamp":"1647641400.0","comments":[{"poster":"OldSchool","timestamp":"1670169780.0","upvote_count":"1","comment_id":"735222","content":"Nope.\nselect top 10 *\nfrom openrowset(\n bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.parquet',\n format = 'parquet') as rows"}],"content":"OPENROWSET is for JSON files","comment_id":"570772"}],"content":"Since this is a batch process, and we can delete files once loaded and this can't be avoid initial/temporary storage cost of any form for loading data, including most optimized parquet format with compression option. So best approach would be to store only required columns which can save storage. However, we can always use OPENROWSET if we are not interested to persist data. Yeah, like someone said, this is shitty question with shitty options.","timestamp":"1642046700.0","comment_id":"522620"},{"poster":"bhushanhegde","content":"As per the documentation, A is the correct answer\nhttps://docs.microsoft.com/en-us/azure/data-factory/format-parquet#dataset-properties","timestamp":"1641912900.0","upvote_count":"1","comments":[{"comment_id":"622047","poster":"uzairahm","timestamp":"1656145440.0","upvote_count":"1","content":"snappy is the default codec as per hte link you shared and when parquet is selected it is already use to further reduce cost external tables could be used and that would not incur any more storage costs"}],"comment_id":"521593"},{"comments":[{"timestamp":"1661188680.0","upvote_count":"1","comment_id":"650371","content":"You're missing the sentence below that saying: \"When reading from Parquet files, you can specify only the columns you want to read and skip the rest.\"","poster":"joyoses663"}],"timestamp":"1641482640.0","comment_id":"518315","poster":"Jaws1990","content":"Selected Answer: A\ncreating an external table with fewer columns than the file has no effect on the file itself and will actually fail so in no way helps with storage costs.\nSee MS documentation \"The column definitions, including the data types and number of columns, must match the data in the external files. If there's a mismatch, the file rows will be rejected when querying the actual data.\"","upvote_count":"6"},{"poster":"Canary_2021","upvote_count":"5","comments":[{"comment_id":"538974","content":"But this has nothing to do with storage costs. Only some bytes in the data dictionary are added and you are not even charged for this.","poster":"ploer","upvote_count":"1","comments":[],"timestamp":"1643821380.0"}],"content":"Selected Answer: B\nIn order to query data from external table, need to creat these 3 items. Feel that they all cost some storage. \nCREATE EXTERNAL DATA SOURCE\nCREATE EXTERNAL FILE FORMAT\nCREATE EXTERNAL TABLE\n\nIf using open row set, don’t need to creat any thing, so l select B.","comment_id":"514338","timestamp":"1641008400.0"},{"comments":[{"upvote_count":"3","content":"That is correct. Looks like whoever put it here, didnt remember it clearly.","poster":"Jerrylolu","timestamp":"1639704360.0","comment_id":"503285"},{"content":"Like many others...","timestamp":"1641937980.0","upvote_count":"5","poster":"assU2","comment_id":"521784"}],"comment_id":"502538","timestamp":"1639613820.0","upvote_count":"12","content":"This question is garbage.","poster":"TestMitch"},{"poster":"vijju23","upvote_count":"7","timestamp":"1639090500.0","comment_id":"498104","content":"Answer is B. which is best as per storage cost. reason we are querying parquet file when need using OPENROWSET."}],"choices":{"B":"Use OPENROWSET to query the Parquet files.","D":"Store all data as string in the Parquet files.","C":"Create an external table that contains a subset of columns from the Parquet files.","A":"Use Snappy compression for the files."},"question_id":53,"answer":"A","answer_images":[],"exam_id":67,"timestamp":"2021-12-09 23:55:00","isMC":true},{"id":"jSwoklhjQPaRde06I3vF","answer_description":"Step 1: Create an external data source\nYou can create external tables in Synapse SQL pools via the following steps:\n1. CREATE EXTERNAL DATA SOURCE to reference an external Azure storage and specify the credential that should be used to access the storage.\n2. CREATE EXTERNAL FILE FORMAT to describe format of CSV or Parquet files.\n3. CREATE EXTERNAL TABLE on top of the files placed on the data source with the same file format.\nStep 2: Create an external file format object\nCreating an external file format is a prerequisite for creating an external table.\nStep 3: Create an external table\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0008500001.jpg"],"unix_timestamp":1639127220,"timestamp":"2021-12-10 10:07:00","discussion":[{"poster":"avijitd","content":"Looks correct answer","timestamp":"1654903440.0","comment_id":"499032","upvote_count":"41"},{"comment_id":"931924","upvote_count":"21","poster":"vctrhugo","timestamp":"1703369760.0","content":"1. CREATE EXTERNAL DATA SOURCE to reference an external Azure storage and specify the credential that should be used to access the storage.\n2. CREATE EXTERNAL FILE FORMAT to describe format of CSV or Parquet files.\n3. CREATE EXTERNAL TABLE on top of the files placed on the data source with the same file format."},{"content":"The answer is correct","timestamp":"1727422020.0","comment_id":"1183967","poster":"dgerok","upvote_count":"1"},{"poster":"kkk5566","upvote_count":"1","timestamp":"1709477400.0","content":"source, format ,external","comment_id":"997622"},{"timestamp":"1677140520.0","poster":"CrazyHorse","upvote_count":"6","comment_id":"650623","content":"If I run CREATE EXTERNAL FILE FORMAT before CREATE EXTERNAL DATA SOURCE, will it change something?","comments":[{"poster":"almachg","comments":[{"comment_id":"853116","poster":"aemilka","upvote_count":"5","content":"\"More than one order of answer choices is correct\", so it seems to me that CREATE EXTERNAL FILE FORMAT before CREATE EXTERNAL DATA SOURCE should be accepted too.","timestamp":"1695897840.0"}],"content":"Can anyone who knows answer this, would like to know too. Asked chatgpt and it says there's no functional difference. However, it is generally considered a best practice to create the external file format before the external data source.","upvote_count":"3","timestamp":"1694757720.0","comment_id":"839650"},{"comment_id":"880374","upvote_count":"4","content":"It doesn't really matter in which order you create a file format or a data source. I have done it in different orders before.","timestamp":"1698237360.0","poster":"zekescookies"}]},{"timestamp":"1674671220.0","poster":"Deeksha1234","upvote_count":"1","comment_id":"636821","content":"given answer is correct"},{"comment_id":"632957","content":"why creating a query that creates a table is not correct?","upvote_count":"1","comments":[{"timestamp":"1675508400.0","content":"CeTAS also exports the query results to Blob storage or Data LAke. Its not a requirement in this question :) https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas","upvote_count":"3","comment_id":"642270","poster":"VeroDon"}],"timestamp":"1674042360.0","poster":"Dicer"},{"poster":"SKN0865","upvote_count":"5","content":"Correct:\nSee Microsoft docs:\nYou can create external tables in Synapse SQL pools via the following steps:\n\n1) CREATE EXTERNAL DATA SOURCE to reference an external Azure storage and specify the credential that should be used to access the storage.\n2) CREATE EXTERNAL FILE FORMAT to describe format of CSV or Parquet files.\n3) CREATE EXTERNAL TABLE on top of the files placed on the data source with the same file format.","comment_id":"624611","timestamp":"1672322400.0"},{"upvote_count":"2","comment_id":"607924","content":"correct","timestamp":"1669534620.0","poster":"Rrk07"},{"timestamp":"1667986680.0","upvote_count":"1","poster":"SandipSingha","content":"correct","comment_id":"598926"},{"timestamp":"1663462140.0","upvote_count":"1","poster":"lotuspetall","comment_id":"570181","content":"correct"},{"poster":"PallaviPatel","timestamp":"1658989500.0","comment_id":"534533","upvote_count":"2","content":"correct"},{"timestamp":"1656921840.0","content":"Correct","comment_id":"516439","upvote_count":"1","poster":"ANath"},{"timestamp":"1654844820.0","comment_id":"498458","content":"Correct","upvote_count":"1","poster":"gf2tw"}],"isMC":false,"question_text":"DRAG DROP -\nYou need to build a solution to ensure that users can query specific files in an Azure Data Lake Storage Gen2 account from an Azure Synapse Analytics serverless SQL pool.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\nSelect and Place:\n//IMG//","answer_ET":"","exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/67532-exam-dp-203-topic-1-question-41-discussion/","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0008400001.jpg"],"question_id":54,"answer":"","topic":"1"},{"id":"Zi09FvO9A9j2lD3MUEfN","answers_community":["CE (100%)"],"answer_description":"","question_id":55,"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/67533-exam-dp-203-topic-1-question-42-discussion/","answer_images":[],"answer_ET":"CE","answer":"CE","topic":"1","unix_timestamp":1639127340,"exam_id":67,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0008500003.png"],"question_text":"You are designing a data mart for the human resources (HR) department at your company. The data mart will contain employee information and employee transactions.\nFrom a source system, you have a flat extract that has the following fields:\n✑ EmployeeID\n\nFirstName -\n//IMG//\n\n✑ LastName\n✑ Recipient\n✑ GrossAmount\n✑ TransactionID\n✑ GovernmentID\n✑ NetAmountPaid\n✑ TransactionDate\nYou need to design a star schema data model in an Azure Synapse Analytics dedicated SQL pool for the data mart.\nWhich two tables should you create? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","choices":{"E":"a fact table for Transaction","B":"a dimension table for EmployeeTransaction","A":"a dimension table for Transaction","D":"a fact table for Employee","C":"a dimension table for Employee"},"discussion":[{"poster":"avijitd","upvote_count":"24","content":"Correct Answer . Emp info as Dimension & trans table as fact","comment_id":"499035","timestamp":"1639186080.0"},{"comment_id":"1321109","timestamp":"1733175180.0","content":"Selected Answer: CE\nTable de fait et table de dimension.","poster":"moize","upvote_count":"1"},{"comment_id":"1319030","timestamp":"1732771440.0","content":"Selected Answer: CE\nCorrect Answer: CE","poster":"EmnCours","upvote_count":"1"},{"comment_id":"1316966","content":"Selected Answer: CE\nCurrect answqer - CE","timestamp":"1732436580.0","poster":"mavdplas","upvote_count":"1"},{"upvote_count":"4","comment_id":"1027793","timestamp":"1696750140.0","content":"Selected Answer: CE\nCorrect. A dimension is like a dictionary of information, therefore will hold the customer data such as name, address, date of birth, whatever. The fact table contains the facts. Usually numbers, usually the data that we are getting from a system from which we will create metrics later on. Therefore for transactions.","poster":"ellala"},{"timestamp":"1694069220.0","poster":"hassexat","comment_id":"1001240","upvote_count":"1","content":"Selected Answer: CE\nCorrect answers"},{"content":"Selected Answer: CE\nCE is correct","timestamp":"1693745460.0","poster":"kkk5566","comment_id":"997623","upvote_count":"1"},{"content":"Selected Answer: CE\nCorrect Answers","timestamp":"1675350360.0","poster":"SHENOOOO","upvote_count":"3","comment_id":"796154"},{"content":"Selected Answer: CE\nCorrect answer","timestamp":"1668158940.0","poster":"kl8585","comment_id":"715909","upvote_count":"3"},{"comment_id":"636823","content":"correct answer - CE","upvote_count":"2","poster":"Deeksha1234","timestamp":"1658766660.0"},{"timestamp":"1655642760.0","poster":"Remedios79","comment_id":"618689","upvote_count":"2","content":"correct"},{"content":"Selected Answer: CE\nCorrect","poster":"hm358","timestamp":"1654944900.0","comment_id":"614948","upvote_count":"3"},{"content":"correct","timestamp":"1652082000.0","poster":"SandipSingha","comment_id":"598928","upvote_count":"1"},{"comment_id":"596022","timestamp":"1651492500.0","poster":"tg2707","upvote_count":"1","comments":[{"upvote_count":"3","comments":[{"poster":"gabrysr1997","comment_id":"630407","timestamp":"1657616220.0","content":"If we would know we wouldn't be here.","upvote_count":"13"}],"content":"do you even know what is fact or dim table? If you know you wouldn't be asking this question","timestamp":"1654575300.0","poster":"Aditya0891","comment_id":"612569"}],"content":"why not fact table for employee and dim table for transactions"},{"content":"CE is correct","comment_id":"587201","timestamp":"1650196800.0","poster":"Egocentric","upvote_count":"1"},{"poster":"NewTuanAnh","comment_id":"584070","content":"Selected Answer: CE\nCE is the correct answer","timestamp":"1649657640.0","upvote_count":"2"},{"timestamp":"1647895620.0","upvote_count":"2","poster":"SebK","content":"Selected Answer: CE\nCE is correct","comment_id":"572505"},{"timestamp":"1645502820.0","comment_id":"553432","content":"Selected Answer: CE\nDimension for employee and fact for transactions.","upvote_count":"1","poster":"surya610"},{"timestamp":"1643358600.0","content":"Selected Answer: CE\ncorrect","comment_id":"534535","upvote_count":"1","poster":"PallaviPatel"},{"timestamp":"1639127340.0","poster":"gf2tw","comment_id":"498461","upvote_count":"2","content":"Correct"}],"timestamp":"2021-12-10 10:09:00"}],"exam":{"name":"DP-203","id":67,"lastUpdated":"12 Apr 2025","isImplemented":true,"numberOfQuestions":384,"isBeta":false,"provider":"Microsoft","isMCOnly":false},"currentPage":11},"__N_SSP":true}