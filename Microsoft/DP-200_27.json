{"pageProps":{"questions":[{"id":"QkkA8EQ7qDb1v1TsPije","url":"https://www.examtopics.com/discussions/microsoft/view/49477-exam-dp-200-topic-2-question-6-discussion/","answer_ET":"","answer_description":"Box 1: Azure Event Hubs -\nThis portion of a streaming architecture is often referred to as stream buffering. Options include Azure Event Hubs, Azure IoT Hub, and Kafka.\nIncorrect Answers: Not HDInsight Kafka\nAzure Functions need a trigger defined in order to run. There is a limited set of supported trigger types, and Kafka is not one of them.\n\nBox 2: Azure Stream Analytics -\nAzure Stream Analytics provides a managed stream processing service based on perpetually running SQL queries that operate on unbounded streams.\nYou can also use open source Apache streaming technologies like Storm and Spark Streaming in an HDInsight cluster.\n\nBox 3: Azure Synapse Analytics -\nAzure Synapse Analytics provides a managed service for large-scale, cloud-based data warehousing. HDInsight supports Interactive Hive, HBase, and Spark\nSQL, which can also be used to serve data for analysis.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/","isMC":false,"timestamp":"2021-04-07 11:33:00","exam_id":65,"answer":"","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0015200001.png"],"topic":"2","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0015300001.png"],"discussion":[{"timestamp":"1617795120.0","comment_id":"330298","poster":"samkslee","upvote_count":"33","content":"The analytical data store should be Azure Cosmos DB, as the question said \"use a document store\"."},{"poster":"ak08","upvote_count":"13","timestamp":"1617787980.0","content":"Analytical data store should be Cosmos DB, since, Synapse analytics is not a document store.","comment_id":"330216"},{"timestamp":"1622733600.0","content":"Agree. Analytical store should be Cosmos DB","upvote_count":"1","comment_id":"373691","poster":"josegv"},{"poster":"Saravjeet","content":"it should be cosmos DB","comment_id":"360324","upvote_count":"1","timestamp":"1621331100.0"},{"upvote_count":"2","content":"Same sentiment with the Analytics data store should be Azure Cosmos DB","poster":"cadio30","comment_id":"345914","timestamp":"1619761980.0"},{"comment_id":"343538","upvote_count":"2","content":"Agree with samkslee & ak08 - Synapse is not a document store, Cosmos DB is.","timestamp":"1619464500.0","poster":"Internet_User"}],"question_id":131,"question_text":"HOTSPOT -\nYou are designing a new Lambda architecture on Microsoft Azure.\nThe real-time processing layer must meet the following requirements:\nIngestion:\n✑ Receive millions of events per second\n✑ Act as a fully managed Platform-as-a-Service (PaaS) solution\n✑ Integrate with Azure Functions\nStream processing:\n✑ Process on a per-job basis\n✑ Provide seamless connectivity with Azure services\n✑ Use a SQL-based query language\nAnalytical data store:\n✑ Act as a managed service\n✑ Use a document store\n✑ Provide data encryption at rest\nYou need to identify the correct technologies to build the Lambda architecture using minimal effort. Which technologies should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","unix_timestamp":1617787980},{"id":"nyekRy2h3FNahVRKY4VR","answer_ET":"B","url":"https://www.examtopics.com/discussions/microsoft/view/52892-exam-dp-200-topic-2-question-60-discussion/","choices":{"A":"Yes","B":"No"},"answer_description":"You load the data using the CREATE TABLE AS SELECT statement.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store","isMC":true,"timestamp":"2021-05-16 17:48:00","exam_id":65,"answer":"B","answers_community":[],"question_images":[],"topic":"2","answer_images":[],"discussion":[{"content":"Doen't it meet the goal","timestamp":"1621368780.0","upvote_count":"1","comment_id":"360817","poster":"maciejt"},{"timestamp":"1621180080.0","comment_id":"358867","comments":[{"comment_id":"387504","timestamp":"1624322940.0","poster":"lgtiza","upvote_count":"2","content":"I think it's not correct unless the table (not the external table) was created already, which means you need a CREATE TABLE ... AS SELECT.. statement. Since it doesn't say the table exists you need to create it."}],"upvote_count":"3","content":"answer is CORRECT","poster":"memo43"}],"question_id":132,"question_text":"Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.\nYou need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.\nSolution:\n1. Create an external data source pointing to the Azure storage account\n2. Create an external file format and external table using the external data source\n3. Load the data using the INSERT`¦SELECT statement\nDoes the solution meet the goal?","unix_timestamp":1621180080},{"id":"CIWEQEfPSg4edx0Qz8mv","timestamp":"2021-06-30 16:14:00","question_text":"HOTSPOT -\nYou are building an Azure Stream Analytics job that queries reference data from a product catalog file. The file is updated daily.\nThe reference data input details for the file are shown in the Input exhibit.\n//IMG//\n\nThe storage account container view is shown in the Refdata exhibit.\n//IMG//\n\nYou need to configure the Stream Analytics job to pick up the new reference data.\nWhat should you configure? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","exam_id":65,"discussion":[{"content":"Correct","timestamp":"1625062440.0","upvote_count":"2","comment_id":"394817","poster":"Marcello83"}],"answer_description":"Box 1: {date}/product.csv -\nIn the 2nd exhibit we see: Location: refdata / 2020-03-20\nNote: Path Pattern: This is a required property that is used to locate your blobs within the specified container. Within the path, you may choose to specify one or more instances of the following 2 variables:\n{date}, {time}\nExample 1: products/{date}/{time}/product-list.csv\nExample 2: products/{date}/product-list.csv\n\nExample 3: product-list.csv -\n\nBox 2: YYYY-MM-DD -\nNote: Date Format [optional]: If you have used {date} within the Path Pattern that you specified, then you can select the date format in which your blobs are organized from the drop-down of supported formats.\nExample: YYYY/MM/DD, MM/DD/YYYY, etc.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data","url":"https://www.examtopics.com/discussions/microsoft/view/56382-exam-dp-200-topic-2-question-61-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0026300001.png","https://www.examtopics.com/assets/media/exam-media/03872/0026400001.png","https://www.examtopics.com/assets/media/exam-media/03872/0026500001.png"],"answers_community":[],"unix_timestamp":1625062440,"isMC":false,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0026600001.png"],"answer":"","topic":"2","question_id":133,"answer_ET":""},{"id":"RsRUlPz6Tb4HRHaF32s1","topic":"2","exam_id":65,"answers_community":[],"timestamp":"2020-03-27 07:23:00","discussion":[{"upvote_count":"12","comment_id":"68514","content":"Same as my thoughts. BDF","timestamp":"1585290180.0","poster":"zenomas"},{"content":"Shouldn't be EventGrid because that is for subscribing to Azure events, ie. New blob in a container.","poster":"cowtown","timestamp":"1593791040.0","comment_id":"125665","upvote_count":"5","comments":[{"comment_id":"349193","upvote_count":"1","content":"This should clear up the differences.\n\nReference: https://www.cognizantsoftvision.com/blog/azure-event-grid-vs-event-hubs/","poster":"cadio30","timestamp":"1620110400.0"}]},{"content":"My ans is BDF","upvote_count":"2","poster":"Ambujinee","timestamp":"1621824660.0","comment_id":"365146"},{"content":"Azure Event Hubs is a more suitable solution when we need a service that can receive and process millions of events per second and provide low-latency event processing. It can handle data from concurrent sources and route it to a variety of stream-processing infrastructure and analytics services, as I have already mentioned. Azure Event Hubs are used more for telemetry scenarios. \n\nOn the other hand, Azure Event Grid is ideal for reactive scenarios, like when an item has been shipped or an item has been added or updated on storage. We have to take into account also its native integrations with Functions, Logic Apps and Webhooks. Moreover, Event Grid is cheaper than Event Hubs and more suitable when we don’t have to deal with big data.\n\nSo I'll go with CDF","timestamp":"1620617880.0","upvote_count":"1","poster":"kaigalmane","comment_id":"353429"}],"question_id":134,"isMC":true,"unix_timestamp":1585290180,"answer":"BDF","url":"https://www.examtopics.com/discussions/microsoft/view/17539-exam-dp-200-topic-2-question-7-discussion/","answer_description":"You can use Azure Logic apps to send tweets to an event hub and then use a Stream Analytics job to read from event hub and send them to PowerBI.\nReferences:\nhttps://community.powerbi.com/t5/Integrations-with-Files-and/Twitter-streaming-analytics-step-by-step/td-p/9594","question_text":"You develop data engineering solutions for a company.\nYou need to ingest and visualize real-time Twitter data by using Microsoft Azure.\nWhich three technologies should you use? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer_images":[],"choices":{"F":"Event Hub instance","E":"Event Grid subscription","C":"Azure Stream Analytics Job that queries Twitter data from an Event Grid","B":"Azure Stream Analytics Job that queries Twitter data from an Event Hub","D":"Logic App that sends Twitter posts which have target keywords to Azure","A":"Event Grid topic"},"answer_ET":"BDF","question_images":[]},{"id":"74yS8TxannuhYd7wTVGe","choices":{"B":"No","A":"Yes"},"question_images":[],"answers_community":["A (100%)"],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\n✑ A workload for data engineers who will use Python and SQL\n✑ A workload for jobs that will run notebooks that use Python, Scala, and SQL\n✑ A workload that data scientists will use to perform ad hoc analysis in Scala and R\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\n✑ The data engineers must share a cluster.\n✑ The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\n✑ All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\nYou need to create the Databricks clusters for the workloads.\nSolution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.\nDoes this meet the goal?","url":"https://www.examtopics.com/discussions/microsoft/view/49150-exam-dp-200-topic-2-question-8-discussion/","discussion":[{"timestamp":"1617608760.0","content":"A workload for jobs that will run notebooks that use Python, Scala, and SQL --> so Standard clusters because of Scala (High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.)","upvote_count":"13","poster":"vaseva1","comment_id":"328469","comments":[{"comment_id":"356438","content":"Correct. Create New Cluster UI for Jobs allows either Standard or Single Node. It does list high concurrency as an option\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/create","upvote_count":"1","timestamp":"1620912960.0","poster":"Chiranjib"}]},{"timestamp":"1619595000.0","comment_id":"344472","poster":"sharma21","upvote_count":"7","content":"A is correct"},{"comment_id":"485258","timestamp":"1637689140.0","content":"Selected Answer: A\nStandard for jobs and high concurrency for darta scientists and data engineers","upvote_count":"1","poster":"FredNo"},{"poster":"elimey","comment_id":"413251","upvote_count":"1","timestamp":"1627139460.0","content":"High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.\n\nmeans job can not be high concurrency\n\n\nso, ANSWER IS A"},{"upvote_count":"3","timestamp":"1620698820.0","content":"appropriate answer is A","poster":"cadio30","comment_id":"354258"},{"upvote_count":"5","timestamp":"1618882620.0","content":"Correct answer is A","poster":"Wendy_DK","comment_id":"339256"},{"poster":"Prabhakaran94","content":"Correct answer is:\nYes","comment_id":"338807","timestamp":"1618831740.0","upvote_count":"4"}],"answer_images":[],"answer":"A","answer_ET":"B","unix_timestamp":1617608760,"topic":"2","timestamp":"2021-04-05 09:46:00","question_id":135,"isMC":true,"exam_id":65,"answer_description":"We would need a High Concurrency cluster for the jobs.\nNote:\nStandard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.\nA high concurrency cluster is a managed cloud resource. The key benefits of high concurrency clusters are that they provide Apache Spark-native fine-grained sharing for maximum resource utilization and minimum query latencies.\nReferences:\nhttps://docs.azuredatabricks.net/clusters/configure.html"}],"exam":{"provider":"Microsoft","numberOfQuestions":228,"isImplemented":true,"isMCOnly":false,"isBeta":false,"name":"DP-200","lastUpdated":"12 Apr 2025","id":65},"currentPage":27},"__N_SSP":true}