{"pageProps":{"questions":[{"id":"qQQ6eqkDdMztylSHdJaf","answer_description":"Scenario: ADatum identifies the following requirements for the Health Interface application:\nSupport a more scalable batch processing solution in Azure.\nReduce the amount of time it takes to add data from new hospitals to Health Interface.\nData Factory integrates with the Azure Cosmos DB bulk executor library to provide the best performance when you write to Azure Cosmos DB.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db\nDesign data processing solutions","unix_timestamp":1584355740,"question_text":"What should you recommend as a batch processing solution for Health Interface?","topic":"25","question_id":136,"discussion":[{"upvote_count":"64","comments":[{"content":"it's actually ADF as per their explanation, they marked it wrong. Bricks would also do I guess, there's little that ADF can do that databricks can't, if anything.","timestamp":"1618319040.0","upvote_count":"4","comment_id":"334694","comments":[{"timestamp":"1618319160.0","upvote_count":"1","poster":"maciejt","content":"ok, ADF can use copy data from on-premise source, spark, which is used by ADF data fows and data bricks can't do that","comment_id":"334695"}],"poster":"maciejt"}],"content":"How come batch processing using Azure Stream analytics why not Azure data bricks? seems like wrong answer this should be D.","timestamp":"1584355740.0","poster":"Amitkhanna","comment_id":"64658"},{"content":"Technology choices for batch processing are\n1. Azure Synapse Analytics\n2. Azure HDInsight\n3. Azure Data Lake Analytics\n4. Azure Databricks\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing","upvote_count":"34","timestamp":"1586456580.0","comments":[{"upvote_count":"2","comment_id":"304844","poster":"rajneesharora","content":"ADF has Data Flows, why is ADF not listed as part of Batch Processing? Secondly, changing the Units, will scale ADF as well... Sending data from On-Premise cant be done via DataBricks, DataBricks can act on it once data is in Azure, ADF seems to be the option","timestamp":"1615066860.0"}],"poster":"Luke97","comment_id":"72721"},{"comment_id":"479003","poster":"massnonn","timestamp":"1637010360.0","upvote_count":"1","content":"for batch processing is databricks"},{"timestamp":"1622040840.0","content":"Why is it that no body is choosing Azure stream analytics as the input of the processing solution is messages generated by the website.","upvote_count":"2","comment_id":"367227","poster":"PowerBIRangerGuru"},{"poster":"Qrm_1972","comment_id":"365554","content":"Correct Answer: B\nExplanation/Reference:\nExplanation:\nScenario: ADatum identifies the following requirements for the Health Interface application:\nSupport a more scalable batch processing solution in Azure.\nReduce the amount of time it takes to add data from new hospitals to Health Interface.\nData Factory integrates with the Azure Cosmos DB bulk executor library to provide the best performance when you write to Azure Cosmos DB.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db","timestamp":"1621857000.0","upvote_count":"2"},{"content":"I would choose ADF.\nhttps://devblogs.microsoft.com/cosmosdb/migrating-relational-data-into-cosmos-db-using-azure-data-factory-and-azure-databricks/","upvote_count":"1","comment_id":"358424","poster":"dbdev","timestamp":"1621153320.0"},{"comment_id":"345680","timestamp":"1619729100.0","upvote_count":"1","poster":"davita8","content":"D. Azure Databricks"},{"comment_id":"334703","timestamp":"1618319640.0","upvote_count":"1","poster":"maciejt","content":"Not sure if databricks can access on prem data source. If yes, then no question D.\nIf not, then you have to use ADF copy data activity to opy from on prem to staging. But as different hospitals have different data formats then you have to transform it to common format. ADF can use mappng data flow or call databricks notebook to do that (but only from staged data already in Azure). dataflow unfortunately is not auto scalable, you have to redefine how many cores you want to use, so I would call databricks notebook from ADF after copy data in ADF. Cosest anwer seems C - ADF."},{"upvote_count":"1","timestamp":"1615385880.0","comment_id":"307227","poster":"AlexD332","content":"https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing\nit seems Databricks"},{"upvote_count":"2","content":"Don't go by word \"batch\". read this: \nHealth Interface -\nADatum has a critical application named Health Interface that receives hospital messages related to patient care and status updates. So stream analytics seems to be correct.","poster":"AmolRajmane","timestamp":"1613657340.0","comment_id":"293429"},{"poster":"mohowzeh","content":"The more reactions I read, the more confused I get. My 2 cents: in this case, the hospitals send the data in batch. This means not message-by-message, but a file containing several messages or records. Most of the discussion here looks at \"batch processing\", which is another story to do with analysing big data stored in files. To me, batch processing is not the correct context of this case. What we need is to ingest files coming from the hospital from time to time. Azure Data Factory seems right to me. The answer's comment also seems to point to this solution, so the answer itself might be a typo.","timestamp":"1610637300.0","comment_id":"267090","upvote_count":"3"},{"content":"Can I use ADF only for solution of both Health Insights and Health Interface?","timestamp":"1609038120.0","upvote_count":"1","poster":"Johnnien","comment_id":"253043"},{"content":"Which product would provide the best performance?","comment_id":"253011","poster":"Johnnien","timestamp":"1609033320.0","upvote_count":"2"},{"upvote_count":"1","poster":"BungyTex","timestamp":"1607551800.0","content":"It has B showing as the answer, but then the description underneath implies C where it talks about data Factory and Cosmos DB. Data Factory is scalable.","comment_id":"239542"},{"comments":[{"poster":"syu31svc","timestamp":"1607908380.0","comment_id":"243118","upvote_count":"1","content":"Disregard this; Databricks for batch processing"}],"upvote_count":"1","poster":"syu31svc","comment_id":"238065","content":"\"Minimize the number of services required to perform data processing, development, scheduling, monitoring, and the operationalizing of pipelines.\"\nI would pick Data Factory as the answer","timestamp":"1607417340.0"},{"timestamp":"1604554080.0","content":"The answer should be D: Databricks. Purely because of Scalability factor. ADF can be used but Databricks is better when it comes to scaling.","comments":[{"timestamp":"1618319280.0","comment_id":"334697","poster":"maciejt","content":"ADF can call databricks notebook in its pipeline","upvote_count":"1"}],"poster":"sandGrain","comment_id":"213213","upvote_count":"2"},{"comment_id":"211410","upvote_count":"2","content":"They mentioned, health interface application received data in batches (group of messages as batch from existing c# application). If ADF is answer how solution is expecting to receive data (http source / json files on blob store?) with varying schema and perform bulk insert into cosmodb? It has to be ADB receiving messages / batches on stream and ingesting them into cosmodb.","poster":"Shrikant_Kulkarni","timestamp":"1604336940.0"},{"comment_id":"190589","content":"Azure Databricks","timestamp":"1601499660.0","upvote_count":"2","poster":"groy"},{"poster":"Vj57","timestamp":"1600082280.0","upvote_count":"1","content":"The question says \"More scalable batch processing\" so if you refer the link only 'Azure Databricks' is scalable from the list. So this should be the answer\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing","comment_id":"179232"},{"content":"With ADF you can add a notebook from ADB. With ADF you can do batch processing and moreover both ADF and ADB has underlying architecture of apache spark. Performance wise both are almost same...the requirement can be achieved by both but ADF is less in terms of coding compared to ADB","timestamp":"1599330600.0","upvote_count":"2","comment_id":"174123","poster":"avix"},{"upvote_count":"2","comment_id":"166913","content":"Adf vs Bricks which would be ideal? as its batch I feel its should be databricks","timestamp":"1598460060.0","poster":"Porus"},{"timestamp":"1596716940.0","content":"I think the answer should be ADF. Eventhough it is not a batch processing solution per sè, if you have a look on the documentation link it also refers to ADF. \n\nHowever, Databricks would also sound plausible here in my opinion since it is the only \"real\" designated batch processing solution. \n\nI would stick with ADF but also think Databricks would be plausible. \n\nAzure Stream Analytics just does not make any sense at all here","comment_id":"151949","poster":"MLurgi","upvote_count":"1"},{"comment_id":"148962","content":"most of the questions and discussions in DP-201 are so confusing.. not sure which answer is correct unless having subject knowledge","timestamp":"1596353940.0","poster":"krisspark","upvote_count":"18"},{"upvote_count":"2","content":"if input is cosmos DB, it should be data factory. as Azure stream analysis only support event hub, IOT hub and Blog storage as input. And the provided explain also mentioned a link: https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db that use data factory to connect to cosmos db","poster":"envy","comment_id":"133770","timestamp":"1594629540.0"},{"content":"Databricks is more for big data analytics. In this case batch processing is needed to load data into Cosmos DB. So ADF makes more sense.","poster":"SidN","timestamp":"1592966640.0","upvote_count":"1","comment_id":"117992"},{"content":"According to the given info, for the Health interface, cosmos DB is appropriate storage solutions. it's been stated the messages are sent in batches, in that case, Stream analytics is the best bet here as it can stream messages directly to Cosmos DB Sink. The given answer is right","timestamp":"1592833500.0","upvote_count":"1","comment_id":"116450","poster":"Abhilvs"},{"timestamp":"1592399820.0","comment_id":"112433","content":"ADF makes more sense here as the requirement is to load the data from branches to target DB (Most likely Cosmos DB). Databrcks is more for bigdata analytics processing.","poster":"SidN","upvote_count":"1"},{"upvote_count":"1","content":"clearly Azure Databricks\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing","comment_id":"109229","timestamp":"1592031180.0","poster":"hecaci8196"},{"comment_id":"104812","poster":"remz","upvote_count":"1","content":"DataBricks dont support C#, Analytics is Correct","comments":[{"comment_id":"125986","upvote_count":"3","content":"The C# is deprecated and will be removed.","poster":"MLCL","timestamp":"1593846600.0"}],"timestamp":"1591559640.0"},{"poster":"Carmina","content":"I also agree that it should be Databricks","upvote_count":"2","comment_id":"87800","timestamp":"1589301540.0"},{"content":"Stream Analytics is a streaming solution, not a batch processing solution. Data factory is an orchestration solution with data copy capabilities. Have no idea what the Azure Cycle thingy is. So Databricks is the only solution here qualified as Batch processing solution.","upvote_count":"11","comment_id":"82600","timestamp":"1588421940.0","poster":"Leonido"},{"comment_id":"82240","timestamp":"1588348560.0","poster":"runningman","upvote_count":"4","content":"the comment below the 'answer' suggests the answer should be ADF, not the highlighted answer 'B'. But ADF is not really a batch processing solution, per the MS docs (as Luke97 clearly references)."},{"poster":"Luke97","timestamp":"1588236420.0","content":"It require \"Support a more scalable batch processing solution in Azure\". So Databricks is the only auto-autoscaling option. (https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing)","upvote_count":"9","comment_id":"81597"},{"comments":[{"comment_id":"82237","poster":"runningman","timestamp":"1588348440.0","upvote_count":"7","content":"i don't think 'reduce the time it takes to add data' means make it real time... it just means speed it up! (the data load was getting slower, per the case study.) I think the answer should be databricks."}],"content":"\"Reduce the amount of time it takes to add data\" = Real-Time that means the answer is Azure Stream Analytics, so the answer is correct B","timestamp":"1587708180.0","upvote_count":"3","comment_id":"78970","poster":"Loai"},{"comment_id":"74087","timestamp":"1586783700.0","comments":[{"content":"If you check this link \"https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing\", ADF is not an option answer is D(Azure data bricks).","timestamp":"1599893280.0","poster":"Kashan_Ali","upvote_count":"3","comment_id":"178085"}],"upvote_count":"17","content":"ADF should be used for batch processing. Ans should be C","poster":"bansal_vikrant"},{"timestamp":"1584487560.0","upvote_count":"13","poster":"Shailbakshi","comment_id":"65368","content":"should be D"}],"question_images":[],"choices":{"D":"Azure Databricks","B":"Azure Stream Analytics","C":"Azure Data Factory","A":"Azure CycleCloud"},"answers_community":[],"isMC":true,"answer_images":[],"answer_ET":"B","timestamp":"2020-03-16 11:49:00","exam_id":66,"answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/16734-exam-dp-201-topic-25-question-1-discussion/"},{"id":"tUqNIw4JetPiweDqKhqy","answers_community":[],"topic":"3","question_text":"You are planning a big data solution in Azure.\nYou need to recommend a technology that meets the following requirements:\n✑ Be optimized for batch processing.\n✑ Support autoscaling.\n✑ Support per-cluster scaling.\nWhich technology should you recommend?","unix_timestamp":1622013240,"answer_description":"Azure Databricks is an Apache Spark-based analytics platform. Azure Databricks supports autoscaling and manages the Spark cluster for you.\nIncorrect Answers:\nA, B:","question_id":137,"url":"https://www.examtopics.com/discussions/microsoft/view/53590-exam-dp-201-topic-3-question-1-discussion/","discussion":[{"timestamp":"1709210280.0","content":"Based on current capabilities, both B and D are correct. probably this question might asked in multiple choice questions.\nhttps://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing","upvote_count":"1","poster":"thapasuman","comment_id":"1162534"},{"timestamp":"1629355680.0","poster":"Bhagya123456","comment_id":"427274","upvote_count":"1","content":"The answer provided is 100% Correct. It is Databricks."},{"poster":"YuvrajSingh","comment_id":"393762","timestamp":"1624969140.0","content":"Answer B \nhttps://azure.microsoft.com/en-in/blog/drive-higher-utilization-of-azure-hdinsight-clusters-with-autoscale/","upvote_count":"1"},{"comment_id":"392265","content":"When the question was released it was not possible on HDinsight hence the exam is being shut down! correct answer ADB","upvote_count":"2","poster":"tes","timestamp":"1624815180.0"},{"poster":"kasseler85","timestamp":"1623838860.0","comment_id":"383288","upvote_count":"2","content":"Should be B:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-autoscale-clusters"},{"upvote_count":"4","comment_id":"368146","poster":"Wendy_DK","timestamp":"1622134440.0","content":"Correct Answer is D"},{"comments":[{"poster":"Akhi_244","timestamp":"1623207240.0","content":"Auto scaling is not possible in HDInsight with spark","upvote_count":"4","comment_id":"377929","comments":[{"comment_id":"434960","content":"HDInsight Autoscale is supported in Spark and Hadoop (Hive) clusters as a generally available feature. \nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-autoscale-clusters","poster":"brazil_guy","upvote_count":"1","timestamp":"1630281120.0"}]},{"poster":"ZodiaC","comment_id":"387976","upvote_count":"3","timestamp":"1624366260.0","content":"NO AUTOSCALING READ DESCRIPTION FIRST! 1000% NOT B!"}],"comment_id":"366905","upvote_count":"3","content":"It should be B: HDInsight with spark . Key word id Big data processing","poster":"Rabindra","timestamp":"1622013240.0"}],"question_images":[],"isMC":true,"answer_ET":"D","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0025000001.png"],"choices":{"D":"Azure Databricks","A":"Azure Synapse Analytics","C":"Azure Analysis Services","B":"Azure HDInsight with Spark"},"answer":"D","exam_id":66,"timestamp":"2021-05-26 09:14:00"},{"id":"yg5Qf2fftyPERYOOglAD","unix_timestamp":1623764700,"url":"https://www.examtopics.com/discussions/microsoft/view/55374-exam-dp-201-topic-3-question-10-discussion/","topic":"3","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0026100005.png"],"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0026100004.png"],"answer":"","question_id":138,"isMC":false,"answer_description":"Box 1: Azure Active Directory (Azure AD)\nOn Azure, managed identities eliminate the need for developers having to manage credentials by providing an identity for the Azure resource in Azure AD and using it to obtain Azure Active Directory (Azure AD) tokens.\n\nBox 2: a managed identity -\nA data factory can be associated with a managed identity for Azure resources, which represents this specific data factory. You can directly use this managed identity for Data Lake Storage Gen2 authentication, similar to using your own service principal. It allows this designated factory to access and copy data to or from your Data Lake Storage Gen2.\nNote: The Azure Data Lake Storage Gen2 connector supports the following authentication types.\n✑ Account key authentication\n✑ Service principal authentication\n✑ Managed identities for Azure resources authentication\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage","discussion":[{"upvote_count":"1","comment_id":"604163","timestamp":"1653000060.0","poster":"nefarious_smalls","content":"Correct!!!!"},{"timestamp":"1623764700.0","poster":"ZodiaC","content":"100% Correct","comment_id":"382695","upvote_count":"4"}],"answer_ET":"","answers_community":[],"exam_id":66,"timestamp":"2021-06-15 15:45:00","question_text":"HOTSPOT -\nYou have an Azure subscription that contains an Azure Data Lake Storage account. The storage account contains a data lake named DataLake1.\nYou plan to use an Azure data factory to ingest data from a folder in DataLake1, transform the data, and land the data in another folder.\nYou need to ensure that the data factory can read and write data from any folder in the DataLake1 file system. The solution must meet the following requirements:\n✑ Minimize the risk of unauthorized user access.\n✑ Use the principle of least privilege.\n✑ Minimize maintenance effort.\nHow should you configure access to the storage account for the data factory? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point\nHot Area:\n//IMG//"},{"id":"860xCCyoJKqCPW5iqkhJ","topic":"3","exam_id":66,"unix_timestamp":1623561060,"question_images":[],"choices":{"A":"a managed identity","D":"an account key","C":"a user-assigned managed identity","B":"a storage access signature (SAS)"},"isMC":true,"answer_description":"Azure Stream Analyticsג€‰supportsג€‰Managed Identity authenticationג€‰for both Azure Event Hubsג€‰input and output.\nNote: First, you create a managed identity for your Azure Stream Analytics job.ג€‰\n1. In theג€‰Azure portal, open your Azure Stream Analytics job.ג€‰\n2. Fromג€‰theג€‰leftג€‰navigationג€‰menu, selectג€‰Managed Identityג€‰located underג€‰Configure. Then, check the box next toג€‰Useג€‰System-assigned Managed Identityג€‰and selectג€‰Save.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/event-hubs-managed-identity","timestamp":"2021-06-13 07:11:00","discussion":[{"content":"Correct Answer!!","comment_id":"604160","upvote_count":"1","poster":"nefarious_smalls","timestamp":"1652999880.0"},{"comment_id":"380854","content":"Correct answer","timestamp":"1623561060.0","poster":"Hardik08","upvote_count":"4"}],"answers_community":[],"answer":"A","question_id":139,"answer_images":[],"question_text":"You are designing a real-time stream processing solution in Azure Stream Analytics. The solution must read data from a blob container in an Azure Storage account via a service endpoint.\nYou need to recommend an authentication mechanism for the solution.\nWhat should you recommend?","answer_ET":"A","url":"https://www.examtopics.com/discussions/microsoft/view/55217-exam-dp-201-topic-3-question-11-discussion/"},{"id":"z1jpdKD1MtHthUzNH93C","answers_community":[],"choices":{"A":"Azure conditional access policies","B":"Azure Active Directory (Azure AD) Privileged Identity Management (PIM)","D":"Azure Active Directory (Azure AD) Identity Protection","C":"Azure Key Vault secrets"},"discussion":[{"content":"https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/overview","comment_id":"317187","upvote_count":"7","timestamp":"1616419440.0","poster":"H_S"},{"upvote_count":"5","timestamp":"1619713620.0","content":"A. Azure conditional access policies","poster":"davita8","comment_id":"345535"}],"question_text":"You are designing security for administrative access to Azure Synapse Analytics.\nYou need to recommend a solution to ensure that administrators use two-factor authentication when accessing the data warehouse from Microsoft SQL Server\nManagement Studio (SSMS).\nWhat should you include in the recommendation?","question_id":140,"answer_ET":"A","url":"https://www.examtopics.com/discussions/microsoft/view/47944-exam-dp-201-topic-3-question-12-discussion/","question_images":[],"topic":"3","exam_id":66,"unix_timestamp":1616419440,"answer_images":[],"timestamp":"2021-03-22 14:24:00","answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-conditional-access","answer":"A","isMC":true}],"exam":{"numberOfQuestions":206,"lastUpdated":"12 Apr 2025","id":66,"isMCOnly":false,"isImplemented":true,"provider":"Microsoft","isBeta":false,"name":"DP-201"},"currentPage":28},"__N_SSP":true}