{"pageProps":{"questions":[{"id":"AumsitcAgofXBU0QOdCC","url":"https://www.examtopics.com/discussions/microsoft/view/46919-exam-az-204-topic-3-question-8-discussion/","answers_community":["A (88%)","13%"],"isMC":true,"question_id":231,"timestamp":"2021-03-13 19:18:00","exam_id":48,"choices":{"B":"SubscriptionClient","D":"CloudQueueClient","A":"QueueClient","C":"TopicClient"},"topic":"3","answer":"A","question_images":[],"answer_ET":"A","unix_timestamp":1615659480,"question_text":"You develop Azure solutions.\nA .NET application needs to receive a message each time an Azure virtual machine finishes processing data. The messages must NOT persist after being processed by the receiving application.\nYou need to implement the .NET object that will receive the messages.\nWhich object should you use?","discussion":[{"upvote_count":"92","content":"Correct Answer: A\n\nAzure.Storage.Queues.QueueClient: .NET v12\nAzure.Storage.Queues.CloudQueueClient: .NET v11 (Legacy)\n\n\nSo, the question is really about what kind of queue message tool you should use. And the key word here is that \"message must NOT persist after being processed\". \n\nAzure.Storage.Queues.QueueClient supports \"At-Most-Once\" deliver mode, while Azure.Storage.Queues.CloudQueueClient doesn't.\n\n\nReference:\n\nhttps://docs.microsoft.com/en-us/dotnet/api/azure.storage.queues.queueclient?view=azure-dotnet\n\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.queue.cloudqueueclient?view=azure-dotnet-legacy","comment_id":"364568","comments":[{"upvote_count":"1","timestamp":"1735993140.0","poster":"Ody","comment_id":"1336375","content":"Copilot says: Neither `Azure.Storage.Queues.QueueClient` nor `Azure.Storage.Queues.CloudQueueClient` supports the **At-Most-Once** delivery mode. \n\nBoth of these clients operate on an **At-Least-Once** delivery model, meaning a message might be delivered more than once if it is not explicitly deleted after being processed.\n\nIf you need **At-Most-Once** delivery, Azure Service Bus with the **ReceiveAndDelete** mode is a better option"},{"upvote_count":"4","poster":"guchao2000","content":"It's QueueClient of Service Bus.","comment_id":"782314","timestamp":"1674223620.0"},{"content":"It seems the CloudQueueClient is a legacy class and is used for creation of QueueClient instances.\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.queue.cloudqueueclient.getqueuereference","poster":"MiraA","timestamp":"1633341060.0","comment_id":"457007","upvote_count":"2"},{"comment_id":"1119712","poster":"SSR999","content":"but when you do receive message on storage queue it wont delete message automatically","upvote_count":"1","timestamp":"1704976140.0"}],"timestamp":"1621776120.0","poster":"mlantonis"},{"timestamp":"1621533960.0","comments":[{"timestamp":"1691709120.0","upvote_count":"1","comment_id":"978135","content":"I agree. Plus that Azure.Storage.Queues.QueueClient AFAIK is more appropriate for data intensive scenarios.","poster":"basquiat"}],"poster":"Spooky7","content":"First of all - question is not precise as we don't know which QueueClient they are asking about. There are two options:\n- Microsoft.AzureService.Bus.QueueClient?\n- Azure.Storage.Queues.QueueClient?\n\nI would say it is about Microsoft.AzureService.Bus.QueueClient as the difference between Azure.Storage.Queues.CloudQueueClient (v12) is just a legacy version of the Azure.Storage.Queues.QueueClient (v11)\nSo the question is really about what kind of queue message tool you should use. And the key word here is that \"message must NOT persist after being processed\". So correct answer would be Microsoft.AzureService.Bus.QueueClient (A) as it supports \"At-Most-Once\" deliver mode while Azure.Storage.Queues.CloudQueueClient doesn't.","upvote_count":"19","comment_id":"362412"},{"timestamp":"1737962340.0","comment_id":"1347276","poster":"wafa_chaari","content":"Selected Answer: A\nQueueClient allows us to manipulate a queue and its msgs. we can add, update delete msgs in a queue. we can also receive msgs from a queue which can then be processed and deleted.","upvote_count":"1"},{"content":"Selected Answer: A\nQueueClient is the answer","comment_id":"1307261","poster":"Vichu_1607","timestamp":"1730796540.0","upvote_count":"1"},{"poster":"8ac3742","upvote_count":"1","content":"The answer is \"QueueClient\":The CloudQueueClient from Azure Storage Queue requires manual deletion of messages, while the QueueClient from Azure Service Bus can use the ReceiveAndDelete mode to automatically delete messages after processing.","comment_id":"1261093","timestamp":"1722869940.0"},{"comment_id":"1168122","content":"Selected Answer: B\nSubscriptionClient Given the requirement for messages to not persist after being processed and the implied need for a publish/subscribe model","timestamp":"1709823900.0","upvote_count":"2","poster":"ShoaibAnwar"},{"timestamp":"1705659480.0","upvote_count":"2","poster":"raymond_abcd","content":"Selected Answer: A\nThink it the servicebus namespace because it is messaging not the storagequeue namespace. The QueueClient is deprecated for Servicebus. It is not in the answers but it should be ServiceBusClient. So in this case I would go for the depricated object answer A","comment_id":"1126598"},{"timestamp":"1696155780.0","poster":"RikinPatel","upvote_count":"2","comment_id":"1022177","content":"Selected Answer: A\n@Admin Please correct answer.\nAs per MS Doc and ChatGPT correct ans is A so please correct ans and its explanation"},{"poster":"alexein74","upvote_count":"1","timestamp":"1674496260.0","content":"Selected Answer: A\nA. QueueClient","comment_id":"785676"},{"timestamp":"1669829340.0","comment_id":"731804","content":"Selected Answer: A\nA. QueueClient","poster":"OPT_001122","upvote_count":"1"},{"comment_id":"688318","timestamp":"1665119880.0","upvote_count":"7","content":"Microsoft creates all these random naming convention showing how disorganized they are in individual islands and they dare to ask questions along this fault lines.","poster":"gmishra88"},{"comments":[{"content":"QueueClient: This is used for Azure Service Bus Queues, which follows a one-to-one messaging pattern, meaning only one receiver can process a message. However, messages in a queue persist until they are explicitly deleted or expire, which does not fit the requirement.","poster":"ns4098","timestamp":"1726659240.0","comment_id":"1285692","upvote_count":"1"}],"poster":"Perplex","upvote_count":"3","content":"Selected Answer: A\nA is correct, the new API calls it QueueClient. See also most upvoted answer.","comment_id":"641576","timestamp":"1659500580.0"},{"poster":"xRiot007","comment_id":"618150","timestamp":"1655542260.0","upvote_count":"1","content":"A - version 12\nD - \"legacy\" version 11"},{"upvote_count":"1","content":"What about SeviceBus QueueClient in the old API?\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.servicebus.queueclient?view=azure-dotnet","comment_id":"557342","timestamp":"1645968180.0","poster":"Bartimaeus"},{"content":"Selected Answer: A\nA is the correct answer","timestamp":"1645181520.0","comment_id":"550171","poster":"idroj","upvote_count":"3"},{"content":"So the explanation for why A is wrong is incorrect? It states you cannot access the VM using A","timestamp":"1644184320.0","upvote_count":"1","poster":"Ccastan1","comment_id":"542054"},{"poster":"prabhjot","comment_id":"366163","upvote_count":"1","content":"yes A is correct","timestamp":"1621923480.0"},{"upvote_count":"2","timestamp":"1620999900.0","comment_id":"357253","content":"A. QueueClient","poster":"glam"},{"comment_id":"309902","content":"Why not A?","timestamp":"1615659480.0","comments":[{"timestamp":"1616428500.0","comment_id":"317309","content":"See the answer information:\nA queue allows processing of a message by a single consumer. Need a CloudQueueClient to access the Azure VM.","upvote_count":"3","poster":"Shion2009","comments":[{"upvote_count":"2","timestamp":"1655029320.0","poster":"ivan0590","comment_id":"615259","content":"I really don't know why you need to access the Azure VM, it makes no sense. Whatever the VM does, in the end, it add a message to a queue in Azure. And you don't need to access the VM to access the queue. This explanation is very awkward.\nAlso, is it true that CloudQueueClient can access a VM? That sounds really strange, it goes beyond the purpose of a \"queue client\". It's like using a fork as a screwdriver...\nThe only difference between CloudQueueClient and QueueClient I've found so far is that CloudQueueClient is the legacy version of the client (v11) and QueueClient is the newest version (v12).\nSo, I would choose A."}]},{"comments":[{"content":"You're right.\nnew version --> .NET v12 --> QueueClient\nold version --> .NET v11 --> CloudQueueClient\nLink --> https://docs.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-queues?tabs=dotnet#create-the-queue-storage-client\nTherefore, the answer is QueueClient.","poster":"vladans","timestamp":"1616779860.0","upvote_count":"7","comment_id":"321323"}],"poster":"MrZoom","upvote_count":"18","comment_id":"313057","content":"Agreed, A is the new-style API, and D is the old-style API, so IMHO, A is better.\n\nLink (A): https://docs.microsoft.com/en-us/dotnet/api/azure.storage.queues.queueclient?view=azure-dotnet\nLink (D): https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.queue.cloudqueueclient?view=azure-dotnet-legacy","timestamp":"1615971300.0"}],"poster":"aperez1979","upvote_count":"4"}],"answer_images":[],"answer_description":""},{"id":"zAMh8IC8TeO1GHjYcV0p","question_id":232,"exam_id":48,"answer_description":"","topic":"3","timestamp":"2021-03-14 01:21:00","url":"https://www.examtopics.com/discussions/microsoft/view/46962-exam-az-204-topic-3-question-9-discussion/","isMC":false,"question_text":"DRAG DROP -\nYou are maintaining an existing application that uses an Azure Blob GPv1 Premium storage account. Data older than three months is rarely used.\nData newer than three months must be available immediately. Data older than a year must be saved but does not need to be available immediately.\nYou need to configure the account to support a lifecycle management rule that moves blob data to archive storage for data not modified in the last year.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","discussion":[{"upvote_count":"124","content":"Since we already have a premium P1 account with gpv1. Why not:\n- Upgrade the existing one to GPv2\n- Create a new GPV2 standard account with default access level to cool\n- And then copy archive data to the GPV2 and delete the data from original storage account. \nThat makes sense to me.","timestamp":"1618225500.0","poster":"sien","comment_id":"333877","comments":[{"content":"Is there any requirement in question, which says set default access tier to COOL?\nPlease clarify.","comment_id":"378319","poster":"jay158","timestamp":"1623245580.0","comments":[{"content":"I have 2 ideas:\n1. \n- One HOT for newer than 3 months\n- One COOL for older than 3 months and Archive data.\n2.\n- One HOT for non-archived data. Can be accessed immediately. Because they just said \"Older than 3 months data are rarely accessed\" but didn't tell us anything about can it be accessed immediately.\n- One COOL for archived data. Archived tier can just be set at blob level. \"Only the hot and cool access tiers can be set at the account level. The archive access tier can only be set at the blob level\" https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers","timestamp":"1624539180.0","comment_id":"389606","poster":"lighting","upvote_count":"3"}],"upvote_count":"2"},{"poster":"WillPassExam","upvote_count":"2","comments":[{"comment_id":"338900","poster":"sien","content":"I would say so.","upvote_count":"1","timestamp":"1618844700.0"},{"poster":"sien","timestamp":"1618844760.0","upvote_count":"4","comment_id":"338902","content":"Also look at this blog:\nhttps://www.apptio.com/blog/essential-guide-azure-blob-storage-pricing/\nOnly GPv2 and Blob storage accounts support tiering. If you are using GPv1, and you want to leverage tiering, convert your account to GPv2 through the Azure portal."}],"comment_id":"337267","timestamp":"1618616940.0","content":"does this mean at the end, we have 2 GPv2 storage accounts, one access level is cool (for archive data) and the other is hot?"},{"poster":"ferut","content":"My choice as well","comment_id":"360066","timestamp":"1621315080.0","upvote_count":"1"},{"comment_id":"417008","upvote_count":"1","content":"I agree. But can the first step be the last?\n- Create a new GPV2 standard account with default access level to cool\n- And then copy archive data to the GPV2 and delete the data from original storage account.\n- Upgrade the existing one to GPv2","timestamp":"1627586340.0","poster":"Chked"},{"content":"Agreed with these steps. I am not sure why we need to upgrade current P1 to GPv2. Maybe there is no lifecycle policy for legacy type account or triggers that will enable newly added blobs enqueue for future deletion.\n\nI am pretty sure we need another account. Premium doesn't have hot or cool tier. We must always make a new account and copy data.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview\n\n\" Data stored in a premium block blob storage account cannot be tiered to Hot, Cool, or Archive using Set Blob Tier or using Azure Blob Storage lifecycle management. To move data, you must synchronously copy blobs from the block blob storage account to the Hot tier in a different account using the Put Block From URL API or a version of AzCopy that supports this API. The Put Block From URL API synchronously copies data on the server, meaning the call completes only once all the data is moved from the original server location to the destination location.\"","timestamp":"1651404300.0","upvote_count":"1","comment_id":"595567","poster":"vavra"},{"upvote_count":"2","timestamp":"1636794000.0","content":"Both provided answer and the answer in the fist comment looks correct, I have decided to go with provided solution.\n1. Upgrade the existing one to GPv2\n2. copy the data to be achieved to a standard GPv2 storage account and then delete the data from the original solution (assuming that storage account with achieve tier we want to copy to already exists.\nWhy would create new account with cool tier when we want it to store archived data, true that achieve tier is not available by default, but then we will need extra step to set to to achieve from cool)\n3. Change the storage account access tier from hot to cool. (Question does not mention how frequent we access data in 1st 3 months, but duration seems a bit more (3 months) and changing it to cool will save cost. \nWe will have two storage account-\n1. cool storage account for newer data - to access data \n2. Achieved storage account for older copied data- (to be changed to archive)","comments":[{"poster":"unilldreams","comment_id":"477362","content":"please ignore last line - (to be changed to archive)","timestamp":"1636794540.0","upvote_count":"1"},{"poster":"john4p","content":"Changing the \"storage account access tier from hot to cool\" is wrong. You'll end up with two cool accounts, but you need a hot one because newer data \"must be available immediately\".","comments":[{"comment_id":"618207","poster":"ivan0590","content":"I agree with you. \n\nWhen the text says \"Change the storage account access...\" is not talking about the new GPv2 account, it's talking about the original GPv1 account (later migrated to GPv2).\nOtherwise, the text would be something like \"Change the new GPv2 storage account access...\".\nTherefore, it doesn't make sense to have to have the two storage accounts in the cool tier. So, the copy option is the one that's wrong.\n\nNow, regarding the order of the 3 questions, I think it should be:\n1. Upgrade the storage account to GPv2.\n2. Create a new GPv2 standard account with default access level to cool.\n3. Copy the data to be archived to a Standard GPv2 storage account and then delete the data from the original storage account.\n\nAnd probably step 1 and step 2 are interchangeable, since I don't think that doing one thing before the other would change anything. The important thing is to copy the data after configuring both storage accounts.","upvote_count":"1","timestamp":"1655549820.0"}],"upvote_count":"1","comment_id":"495978","timestamp":"1638880380.0"}],"poster":"unilldreams","comment_id":"477358"},{"comments":[{"timestamp":"1631693820.0","poster":"MiraA","upvote_count":"5","comment_id":"445081","content":"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers\n\"Only the hot and cool access tiers can be set at the account level. The archive access tier can only be set at the blob level.\"\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-upgrade\n\"If an account access tier is not specified on upgrade, it will be upgraded to hot by default.\"\n\nhttps://docs.microsoft.com/en-us/dotnet/api/azure.storage.blobs.specialized.blobbatch.setblobaccesstier\nFunction BlobBatch.SetBlobAccessTier"}],"content":"My explanation...\n\nThe \"archive\" tier cannot be set at the account level, it can only be set at the blob level.\nSo we need two storage accounts only - one \"hot\" to store new data and one \"cold\" to store both older and archive data. Setting the \"archive\" tier is matter of blob copy/move operation within the application code.\n\n1. Upgrade the storage account to GPv2 (it becomes \"hot\" by default)\n2. Create a new GPv2 Standard account and set its default access tier level to \"cool\" (creates empty storage for older and archive data)\n3. Copy the data to be archived to a Standard GPv2 storage account and the delete the data from the original storage account (individual blobs get marked as \"archive\" during move operation, the newly create GPv2 is mentioned as the target)\n\nNow we have two storage accounts:\nThe first \"hot\" with new data only.\nThe second \"cold\" with archive (and older) data only.","timestamp":"1631693820.0","upvote_count":"32","comment_id":"445080","poster":"MiraA"},{"content":"Why we need to Upgrade the existing one to GPv2 as we are creating a new one?","upvote_count":"14","comments":[{"poster":"warchoon","content":"To have additional account for the archive data?","timestamp":"1677235260.0","upvote_count":"2","comment_id":"820365"}],"poster":"Ram0202","timestamp":"1627477440.0","comment_id":"416206"}]},{"poster":"mlantonis","comments":[{"comment_id":"618476","upvote_count":"4","content":"Agreed: 1. upgrade storage account 1 from GPv1 -> v2 (hot), 2. create storage account 2 GPv2 (cool) 3. copy data from account 1 to account 2. End result: account 1 (hot), account 2(cool and archive) both accounts can set lifecycle policy","poster":"tosm","timestamp":"1655602440.0"},{"poster":"mlantonis","upvote_count":"2","timestamp":"1622375640.0","content":"Reference:\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-upgrade?tabs=azure-portal","comment_id":"370153"},{"comments":[{"comment_id":"462469","timestamp":"1634284980.0","poster":"ensa","content":"every time with good explanation but step 3 needed because why not transfer the old data that needed to new one and delete the old one for saving cost","upvote_count":"1"}],"poster":"mlantonis","content":"Although Step 3 seems unusual and not necessary.","timestamp":"1622375700.0","upvote_count":"4","comment_id":"370156"}],"timestamp":"1621777620.0","comment_id":"364594","content":"Step 1: Upgrade the storage account to GPv2\nObject storage data tiering between hot, cool, and archive is supported in Blob Storage and General Purpose v2 (GPv2) accounts. General Purpose v1 (GPv1) accounts don't support tiering. You can easily convert your existing GPv1 or Blob Storage accounts to GPv2 accounts through the Azure portal.\n\nStep 2: Create a new GPV2 standard account with default access level to cool\n\nStep 3: Copy the data to be archived to a Standard GPv2 storage account and then delete the data from the original storage account","upvote_count":"23"},{"poster":"8ac3742","timestamp":"1722870720.0","content":"In real practice, I just keep one account: upgrade the account to GPV2 which supports tier, change the account tier to cool since the data in the account is infrequently used, and achieve the blob which is not modified for more than 1 year.","comment_id":"1261097","upvote_count":"1"},{"poster":"Christian_garcia_martin","timestamp":"1722829020.0","upvote_count":"1","content":"Upgrade , Create and Copy","comment_id":"1260907"},{"upvote_count":"2","comment_id":"1076290","poster":"11_NickName_11","content":"Why to upgrade the storage account to GPv2?","timestamp":"1700570460.0"},{"content":"The requirement is just: You need to configure the account to SUPPORT a lifecycle management rule.\n\nSo you only need \"Upgrade the existing one to GPv2\"\nNo more steps from the list are required\n\nThen you could configure the lifecycle management rule.\nOnce you apply the rule, the files tier will be changed automatically","timestamp":"1699952760.0","poster":"AlbertoBT","upvote_count":"2","comment_id":"1070176"},{"upvote_count":"1","content":"Azure Blob storage lifecycle management offers a rich, rule-based policy for General Purpose v2 and Blob storage accounts.","poster":"narenazure","comment_id":"872074","timestamp":"1681671960.0"},{"upvote_count":"3","poster":"deepak_26","content":"Create , change , copy\n\nData stored in a premium block blob storage account cannot be tiered to Hot, Cool, or Archive using Set Blob Tier or using Azure Blob Storage lifecycle management. To move data, you must synchronously copy blobs from the block blob storage account to the Hot tier in a different account using the Put Block From URL API or a version of AzCopy that supports this API.","timestamp":"1679972400.0","comment_id":"852713"},{"poster":"motekim","content":"Just for reference: GPv2 supports Hot, Cool, and Archive tiers","comment_id":"847005","upvote_count":"2","timestamp":"1679485140.0"},{"poster":"dimsok","upvote_count":"2","timestamp":"1675954800.0","content":"Upgrade-Create-Copy","comment_id":"803353"},{"poster":"JamieS","timestamp":"1675077060.0","comment_id":"792670","upvote_count":"2","content":"Seems like we don't have the entire world view, and that there's already an existing standard v2 account elsewhere. If that's the case we're starting with v1 storage where the data currently is and we also have a default v2 storage that exists.\n\nWe upgrade the current v1 to v2 so we can access lifetime management\nwe copy the data to be archived to the standard v2 (that already existed, and by default has hot tier)\nthen we set the tier to be cool (where the archive data was just copied to).\n\nI think the current answer is correct."},{"content":"The answer doesn't make any sense to me.\nin the 2nd step, it says copy the data from old storage account to new one, it means there are 2 storage accounts, why you need to upgrade the existing?","comment_id":"716239","timestamp":"1668187800.0","poster":"micro9000","upvote_count":"2"},{"upvote_count":"1","comment_id":"702068","poster":"Akiu","content":"Upgrade to GPv2\nCopy data\nChange tier to cool\nYou can't create the second account in a cool tier because of this:\nData stored in a premium block blob storage account cannot be tiered to hot, cool, or archive using Set Blob Tier or using Azure Blob Storage lifecycle management. To move data, you must synchronously copy blobs from the block blob storage account to the hot tier in a different account using ...\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#blob-lifecycle-management","timestamp":"1666520160.0"},{"poster":"Azziet","timestamp":"1656446700.0","comment_id":"624214","upvote_count":"7","content":"Upgrade\nCreate\nCopy"},{"timestamp":"1649433900.0","comment_id":"582953","upvote_count":"3","poster":"Evo_Morales","content":"Agree with voting/answer, but the question itself seems flawed. Need to learn not to read anything else into the questions and not add steps/requirements that yes, would make sense."},{"upvote_count":"5","content":"Got it in exam 03/22","timestamp":"1646859420.0","comment_id":"564334","poster":"petitbilly"},{"content":"for me not have sense - Upgrade the existing one to GPv2\nSo:\n-create ..\n-change..\n-copy","comment_id":"540891","poster":"massnonn","timestamp":"1644058320.0","upvote_count":"1"},{"poster":"edengoforit","upvote_count":"1","content":"Azure offers three types of storage accounts: General Purpose v2 (GPv2), General Purpose v1 (GPv1), and a dedicated object storage service—Blob Storage. GPv1 and GPv2 support the same storage types, but GPv2 supports Hot, Cool, and Archive tiers. Adopt GPv2 or Blob Storage to leverage tiering.\n\nGPv2 offers up a variety of storage options (blob, files, queues, tables, and disks), performance tiers, and replication options not available with dedicated blob storage account types. Azure uses GPv2 to roll out new product enhancements (e.g., new redundancy options). GPv2 is the preferred storage account type if you want early access to new services. Azure storage account options are:\n\nGPv2: Basic storage account type for blobs, files, queues, and tables. Recommended for most scenarios using Azure Storage.\nGPv1: Legacy account type for blobs, files, queues, and tables. Use GPv2 accounts instead when possible.","timestamp":"1642202160.0","comment_id":"523805"},{"upvote_count":"6","comment_id":"518661","content":"Confusing here\nWhat's the right answer??","timestamp":"1641509700.0","poster":"Yazhu"},{"comment_id":"423704","poster":"ning","upvote_count":"9","content":"The goal should be\n1. 0-90 days old --> hot\n2. 91-365 days old --> cool\n3. 366 or more --> archive\n\nBut I cannot see anything satisfy that","timestamp":"1628781720.0","comments":[{"content":"The given answer is correct. I can only assume the second storage is in GPv2 Archived but wasn't been advised in the question.\nIt makes sense when you put this together.\n\n1)So when you upgrade from GPV1 to Gpv2, it is defaulting to hot.\n2)Take advantage of the hot tier pricing by moving your archive data to another Gpv2 storage that wasn't been advised in the question\n3)No change this storage to cool tier to satisfy the requirement","comment_id":"430640","poster":"j888","timestamp":"1629796920.0","upvote_count":"4"}]},{"poster":"lms001","content":"since the solution is insisting on having three processes,,\n1. Create a new gpv2 account\n2. Copy archive data to the new gpv2 account and delete the data from the original account\n3. Change the access tier from hot to cool","upvote_count":"8","comment_id":"414483","timestamp":"1627284600.0"},{"comments":[{"content":"Ignore the above. I believe below step is more appropriate:\n1) upgrade from GPV1 to GPV2, so the default will be in hot tier\n2) Take the advantage of pricing by moving the data, so copy the archive data to another Gpv2\n3) Now last step is to change the status from the current Hot tier to Cool.\n\nI think this is more sensible..","poster":"j888","timestamp":"1628586480.0","comment_id":"422586","upvote_count":"7"}],"comment_id":"402284","poster":"j888","upvote_count":"1","timestamp":"1625785500.0","content":"\"Copy the data to be archived to standard GPv2 storage account and then delete the data from original storage account\" - Clearly this indicating a second storage account was created- New storage GPv2.\n\nI would choose to create a new storage gpv2. Then change from hot to cool and lastly move the data across as a more appropriate choice. If they are enforcing 3 answers\n\nThe answer doesn't make sense and if you want to enforce choices then the above is more appropriate."},{"poster":"SlavMar","content":"Why upgrade - if v1 works always as HOT than we don't need to touch it. Just creat3 v2, copy data from v1 and set v2 to cool - 2 steps","comment_id":"393201","upvote_count":"3","timestamp":"1624907400.0"},{"comments":[{"timestamp":"1625669460.0","upvote_count":"2","content":"Once upgraded, there is still one storage. Where do you copy archive data to in step 2?","comment_id":"401000","poster":"gunz123"}],"poster":"[Removed]","content":"The answer is correct.\n1) you upgrade your existing one.\n2) you copy the archive data to a new gpv2 account.\n3) now you can change the acess level from hot to cold.","timestamp":"1624250460.0","upvote_count":"1","comment_id":"386765"},{"comments":[{"upvote_count":"2","content":"Because of this part: \"Data older than three months is rarely used\" so it can be in cool storage.","poster":"timwarp","timestamp":"1623630540.0","comments":[{"upvote_count":"3","content":"yes but you still needs HOT for younger files...","timestamp":"1631079900.0","comment_id":"441237","poster":"zolty13"}],"comment_id":"381418"}],"timestamp":"1623245280.0","content":"I think answer is just one action\n- Upgrade the existing one to GPv2\nWhy everyone trying to set default access level to COOL? this is not a requirement","poster":"jay158","upvote_count":"2","comment_id":"378312"},{"content":"Shouldn't the question be about which TWO actions you need to perform? And then:\n- upgrade account\n- change tier to cool (however it can be done in 1 step during upgrading)\n\nAny answer with 3 actions doesn't make sense. For instance what is the point of upgrading storage account and after creating new one? You can have default cool tier and rule which will move some old blobs to archive tier. There is no need to have 2 separate storage accounts.","timestamp":"1621535880.0","upvote_count":"14","comment_id":"362431","comments":[{"upvote_count":"4","content":"I don't even see the reason why to change the default tier, so only the upgrade of the given answers would be right? New data should be hot (which is default) and you need rules to put older data to cool or archive. To me there is also no need for two accounts and also not for changing the default tier. What part am I missing?","poster":"Peter304403","timestamp":"1623060660.0","comment_id":"376677"}],"poster":"Spooky7"},{"poster":"r1999","upvote_count":"5","content":"My guess as to why the answer is correct:\n1) Upgrade the account to v2 because of the archive feature\n2) Copy the data, to a new storage account (while it is only implicitly suggested that this step also involves creating a new storage account).\n3) Set the access level to cool AFTER having copied the data. Read/write data from/to a storage account that is set to 'cool' is expensive, thus best not to set the account to 'cool' while still having to write a lot of data during the first initial archive action.\n\n(This based on my interpretation of the pieces of the answers provided)","comment_id":"357712","timestamp":"1621069860.0"},{"poster":"glam","content":"1) Upgrade existing GPv1 to GPv2\n2) Create new one Standard GPv2\n3) Copy data to Standard GPv2","upvote_count":"4","comment_id":"357255","timestamp":"1621000020.0","comments":[{"content":"glam, this is the first time I've disagreed with you! Thanks for all the simple, solid answers you contributed up to this point. You've been very helpful. Actually, I think you have this one correct as well, but you didn't give the full answer on step 2. \"Create new one Standard GPv2, and set default to cool\", right?","upvote_count":"3","poster":"JoeInOregon","timestamp":"1621984200.0","comment_id":"366720"}]},{"upvote_count":"1","poster":"kimalto452","content":"how we can archive data with gpv2 cool access? that doesn't have any sense...","timestamp":"1620926220.0","comment_id":"356556"},{"poster":"eobo","upvote_count":"1","comments":[{"poster":"Deputy_Cartman","comment_id":"332819","upvote_count":"1","timestamp":"1618088700.0","content":"Because it's cheaper per GB of data stored than Hot. It has higher writing and \"interaction\" costs than Hot, but if you're storing lots of data that is infrequently accessed or modified, as of the time of my post, the first 50 TB of data stored is $0.0184 per GB in Hot Tier and $0.01 per GB for Cool. If you're storing TB of data, that difference adds up REALLY quickly."}],"timestamp":"1617422760.0","content":"Why should we change the tier from hot to cool?","comment_id":"327153"},{"comments":[{"content":"Azure Blob storage lifecycle management is only available for GPv2 and Blob storage accounts","comment_id":"327575","timestamp":"1617478140.0","upvote_count":"4","poster":"koumki"}],"timestamp":"1617396780.0","comment_id":"326998","content":"Why would someone want to upgrade to GPv2 and then not utilize its features?","upvote_count":"1","poster":"jokergester"},{"content":"Maybe like this:\n1) Create a new GPv2 Standard account…\n2) Change the storage account access tier from hot to cool\n3) Copy the data to be archived to a Standard GPv2 storage account and then delete...","timestamp":"1616781660.0","comments":[{"comment_id":"486902","poster":"altafpatel1984","timestamp":"1637868900.0","upvote_count":"1","content":"So there will be 2 cool storage accounts. 1 & 2. So your answer is wrong at very first sight. Please don't misguide viewers here, they come to this portal to pass examination, not for timepass."},{"timestamp":"1616941680.0","poster":"XYZ2","content":"I think it could be\n1) Upgrade existing GPv1 to GPv2\n2) Create new one Standard GPv2\n3) Copy data to Standard GPv2","upvote_count":"16","comment_id":"322698"},{"comment_id":"322694","poster":"XYZ2","upvote_count":"2","content":"Looks good, but on your step 1 GPv2 created with cool tier already, in this case second step is pointless","timestamp":"1616941500.0"}],"upvote_count":"4","comment_id":"321342","poster":"vladans"},{"content":"the answer is correct","comments":[{"poster":"vb3d","content":"How can it be correct?\nIt talks about copying the data to a standard v2 account. If we started with v1 account and upgraded to v2, why copy? It is the same account.\nI would assume the first step is create a v2 account, then copy the data to it","comment_id":"319125","upvote_count":"2","timestamp":"1616591580.0","comments":[{"timestamp":"1618617720.0","comment_id":"337268","content":"I *guess*, at the end, there should be two GPv2, one is cool for data older than 3 months, and the other is hot, which is the original one holding data newer than 3 months.","poster":"WillPassExam","upvote_count":"1"}]}],"poster":"Marusyk","upvote_count":"4","timestamp":"1615681260.0","comment_id":"310104"}],"answer_images":["https://img.examtopics.com/az-204/image592.png"],"unix_timestamp":1615681260,"answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04273/0022400001.jpg"],"answers_community":[],"answer_ET":""},{"id":"OIewcdy4TGGQgBqQvQNU","question_id":233,"answer_description":"","answer_ET":"ABD","isMC":true,"answer_images":[],"discussion":[{"timestamp":"1700376420.0","poster":"OPT_001122","content":"Selected Answer: ABD\na b and d","upvote_count":"15","comment_id":"721811"},{"content":"Selected Answer: ABD\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-overview#prerequisites-for-point-in-time-restore\naccording to the above reference Point-in-time restore requires that the following Azure Storage features be enabled before you can enable point-in-time restore:\n\nSoft delete\nChange feed\nBlob versioning","poster":"AQMA","upvote_count":"9","timestamp":"1710195360.0","comment_id":"836575"},{"comment_id":"1088509","upvote_count":"1","timestamp":"1733404860.0","poster":"Weam","content":"Correct\nMicrosoft recommends that after you enable blob versioning, you also update your application to stop taking snapshots of block blobs. If versioning is enabled for your storage account, all block blob updates and deletions are captured and preserved by versions. Taking snapshots does not offer any additional protections to your block blob data if blob versioning is enabled, and may increase costs and application complexity."},{"poster":"juanckar","upvote_count":"6","content":"This was on the exam (July 2023). Went with highly voted. Scored 917","comment_id":"944409","timestamp":"1720251120.0"},{"comment_id":"703246","upvote_count":"4","content":"In portal\n\"Select Turn on point-in-time restore. When you select this option, soft delete for blobs, versioning, and change feed are also enabled\"\nSo it needs the three options to do point in time restore. You probably can't clear them, once Turn on point-in-time is chosen.","poster":"coffecold","timestamp":"1698169560.0"},{"timestamp":"1696773180.0","content":"\"Before you enable and configure point-in-time restore, enable its prerequisites for the storage account: soft delete, change feed, and blob versioning\"","comment_id":"689372","upvote_count":"2","poster":"gmishra88"},{"poster":"AbdulMannan","comment_id":"683525","timestamp":"1696074840.0","content":"Got this question on 30-Sep-2022 exam.\nAnswer is correct. Passed with 870 score.","comments":[{"timestamp":"1696156560.0","comment_id":"684142","upvote_count":"3","poster":"Mousavi","content":"I will take exam on 5 October. Any suggestion?"}],"upvote_count":"4"},{"content":"correct","upvote_count":"7","timestamp":"1682830560.0","comment_id":"594851","poster":"andrvelich"}],"question_text":"You need to implement a solution to resolve the retail store location data issue.\nWhich three Azure Blob features should you enable? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer":"ABD","question_images":[],"timestamp":"2022-04-30 06:56:00","choices":{"B":"Change feed","D":"Versioning","F":"Immutability","E":"Object replication","A":"Soft delete","C":"Snapshots"},"unix_timestamp":1651294560,"answers_community":["ABD (100%)"],"exam_id":48,"topic":"30","url":"https://www.examtopics.com/discussions/microsoft/view/74953-exam-az-204-topic-30-question-1-discussion/"},{"id":"zdbxIVAHKMdKxG66vmGy","choices":{"B":"Azure Event Hub","C":"Azure Service Bus topic","A":"Azure Storage queue","D":"Azure Event Grid topic"},"answers_community":["B (67%)","A (33%)"],"answer_description":"","discussion":[{"timestamp":"1591725120.0","comment_id":"106173","content":"Correct: \"Information regarding agreements is used by multiple divisions within Contoso, Ltd. User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\"","comments":[{"upvote_count":"1","poster":"GCMan","comment_id":"207834","timestamp":"1603895040.0","content":"I think Azure Event Hub is correct but I see arguments of a couple other options."},{"comment_id":"228441","content":"As @coolest said: B is correct, so the answer is correct. Note: \"You can use Event Capture to store the agreements into Azure blob storage for long term storage\".","poster":"Juanlu","timestamp":"1606404180.0","upvote_count":"1","comments":[{"comment_id":"703259","timestamp":"1666634640.0","poster":"coffecold","content":"\"If you need to store data for more than 7 days, a feature of Azure Event Hubs called Capture is the preferred solution for longer-term storage. When configuring Capture, there are two locations where this information can be stored: Azure Blob Storage or Azure Data Lake Store account\" \nSo Event capture just uses a storage solution as well.","upvote_count":"2"}]},{"comment_id":"758037","upvote_count":"2","content":"On My Exam:2022-12-26\nMy Choice:\nB:Azure Event Hub","timestamp":"1672104480.0","poster":"KingChuang"}],"poster":"perry230","upvote_count":"34"},{"poster":"coolest","upvote_count":"15","comment_id":"152123","timestamp":"1596738960.0","content":"B - Azure Event Hub is Correct.\n\nIf you are looking at millions of agreements per hour, you need to use a data ingestion service like the Azure Event Hub. You can use Event Capture to store the agreements into Azure blob storage for long term storage."},{"poster":"overhill","timestamp":"1732382160.0","upvote_count":"1","content":"Millions of agreements per hour will quickly drain the capacity of the queue or even Service Bus. and Event Grid is not capable of handling such a volume. \nHence Event Hubs is the only option with Capture to Blob Storage.\nThis is a great question","comment_id":"1316747"},{"content":"Selected Answer: A\nA. Azure Storage Queue\n\nThis option ensures that the high volume of user agreements is managed effectively while providing the durability and availability required for the data. Azure Event Hub, while powerful, is more complex and tailored for scenarios involving real-time event streaming and telemetry, which is not the primary focus here.","poster":"giuliohome","comment_id":"1238815","upvote_count":"2","timestamp":"1719596100.0"},{"poster":"Archana_G","content":"On My Exam:2024-05-11\nMy Choice:\nB:Azure Event Hub","comment_id":"1209810","upvote_count":"1","timestamp":"1715434980.0"},{"timestamp":"1697727240.0","upvote_count":"2","comment_id":"1048045","content":"Selected Answer: B\nShould be Azure Blob Storage, if was a choice...\nAzure Blob Storage is a highly scalable object storage service that can store any type of data. It is a good option for storing user agreements for long periods of time because it is cost-effective and durable.","poster":"Ciupaz"},{"upvote_count":"1","comment_id":"1025399","content":"On my exam 2023-10 before the Update of the Exam","poster":"ENGs","timestamp":"1696494060.0"},{"timestamp":"1690613640.0","poster":"BaoNguyen2411","content":"Got this question on 29/06/2023","upvote_count":"1","comment_id":"966182"},{"poster":"OPT_001122","upvote_count":"2","content":"Selected Answer: B\nAzure Event Hub","comment_id":"721813","timestamp":"1668840540.0"},{"comments":[{"poster":"coffecold","timestamp":"1666634880.0","comment_id":"703262","upvote_count":"1","content":"Storing is storing, it is not the 7-days data that is kept in retention in the event hub. Completed is if the data is finally stored in a storage solution. So I go for option A"}],"comment_id":"689385","content":"Another brilliant question from Microsoft. The wording is very vague. Where to store after it is completed. After it is completed, store it in Archive till the retention period. But because that is not in the answer, and I guess Microsoft does not potentially understand the meaning of \"complete\" properly, and guessing that word is added just to add some: entertainment: I choose event-hub \nBut you should not use any of these options for storing information. But yes, you can use events capture with storage blob, but that is a lot of thinking","poster":"gmishra88","upvote_count":"9","timestamp":"1665237660.0"},{"content":"Got it on 03/2022!","comment_id":"566507","upvote_count":"2","timestamp":"1647141960.0","poster":"meoukg"},{"comment_id":"520793","upvote_count":"1","timestamp":"1641811500.0","poster":"aruni_mishra","content":"Azure Event Hubs enables you to automatically capture the streaming data in Event Hubs in an Azure Blob storage or Azure Data Lake Storage Gen 1 or Gen 2 account of your choice, with the added flexibility of specifying a time or size interval. \nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview\nAns: Azure Event Hub"},{"poster":"altafpatel1984","content":"Azure Storage Queue can process 2000 messages per second. i.e. 72 million messages per hour. Since message is to be processed here, it cannot be event and hence Storage Queue will be used to store data and hence answer is A - Storage Queue\n\nhttps://docs.microsoft.com/en-us/learn/modules/communicate-between-apps-with-azure-queue-storage/2-create-the-azure-storage-infrastructure","comment_id":"482793","comments":[{"upvote_count":"1","timestamp":"1645892400.0","poster":"Bartimaeus","content":"It's 7.2 million, but it's only target throughput - max is 20_000/s = 72 million","comment_id":"556772"}],"timestamp":"1637435460.0","upvote_count":"3"},{"comment_id":"302271","content":"Answer: B","poster":"paru123456789","upvote_count":"3","timestamp":"1614718980.0"},{"timestamp":"1612901700.0","poster":"cbn","upvote_count":"5","comment_id":"287115","comments":[{"timestamp":"1612901760.0","poster":"cbn","upvote_count":"2","comment_id":"287116","content":"Sorry, I mean B (Event Hub)"}],"content":"\"Information regarding agreements is used by multiple divisions within Contoso, Ltd.\"\n- This needs multiple subscribers. Storage queue cannot be used for this.\n\n\"The volume of agreements is expected to be in the millions per hour.\"\n- This leads to choice for Event Hub / Service bus topic\n\n\"When a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version\"\n- This is not about agreements, however an Event Hub supports this scenario as well.\n\nI will go with C (Event Hub)"},{"content":"Ok, yeah, so we need to handle millions. So Storage Queue or Event Hub.\nBut then, do we need messages or events?\nIt says we need to store (maybe temporarily, until processed), and events do not contain content to be stored. So I'd say Storage Queue. Right?","poster":"mvr","timestamp":"1611821520.0","upvote_count":"5","comment_id":"278380"},{"timestamp":"1611330300.0","poster":"matejka","comment_id":"273829","content":"Azure Event Hub is correct as it is able to handle milions of events per second. But the questions is very confusing as it states \"to store...\" and Event Hub is not designed to persistent store data. It stores the events and should route those to eg. storage account or so.","upvote_count":"2"},{"upvote_count":"1","content":"should be B","poster":"khoant","comment_id":"263101","timestamp":"1610182860.0"},{"poster":"Cornholioz","comment_id":"252921","timestamp":"1609018380.0","content":"I still don't know for sure. The scenario gives a few facts and the question asks to store the agreements. Couldn't find a strong argument to say Event Hub is better than Queue in this case. I can eliminate Service Bus Topic and Event Grid Topic, because of the traffic numbers. But between the other two, it's hard.\nEven after all the research, I'm only making a guess here: Event Hub","upvote_count":"1"},{"upvote_count":"1","content":"Who talk about store? The case says: \"When a user submits content, they must agree to a user agreement\", so ALWAYS a user submits content, they MUST agree.\n\"Event Hubs Standard tier currently supports a maximum retention period of seven days.\" so it's more than enough for an approval process","comment_id":"241457","poster":"luppittegui","timestamp":"1607765160.0"},{"upvote_count":"1","poster":"aroravibhu","comment_id":"231467","content":"Shouldnt't be eventhub as question is stressing on \"store the agreement after processing\". We can't store messages in event hub, should be Azure Service Bus topic","timestamp":"1606783560.0"},{"comment_id":"192400","poster":"AidanT","upvote_count":"11","content":"I wouldn't store it in any of those options!","timestamp":"1601737620.0"},{"comment_id":"183570","poster":"viji3281","content":"the selected answer is correct","upvote_count":"2","timestamp":"1600681200.0"},{"content":"Why not A?\n\"Azure Queue Storage is a service for storing large numbers of messages. You access messages from anywhere in the world via authenticated calls using HTTP or HTTPS. A queue message can be up to 64 KB in size. A queue may contain millions of messages, up to the total capacity limit of a storage account. \"\nhttps://docs.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction#:~:text=Azure%20Queue%20Storage%20is%20a,limit%20of%20a%20storage%20account.","poster":"rrongcheng","comment_id":"145038","upvote_count":"5","comments":[{"comment_id":"300346","poster":"pieronegri","upvote_count":"1","timestamp":"1614441960.0","content":"it says \"Information regarding agreements is used by multiple divisions within Contoso, Ltd.\" \nIt means pub sub pattern, which is not gracefully supported by storage queue."}],"timestamp":"1595859540.0"},{"comments":[{"timestamp":"1598227980.0","content":"wrong. storage queue can also be infinite. https://docs.microsoft.com/en-us/rest/api/storageservices/put-message#uri-parameters","comment_id":"164766","poster":"quokka","upvote_count":"2"}],"poster":"eladt","comment_id":"131196","content":"Should be \"C. Azure Service Bus topic\" - its the only one with max\\unlimited retention period.\nAll the others keeps the data for temp period.","timestamp":"1594363320.0","upvote_count":"1"},{"upvote_count":"5","comments":[{"poster":"xofowi5140","comments":[{"content":"And it has a maximum size limit that is above 80 GB and can store data indefinitely. The default TTL is only a default setting that can be changed. \nTherefore, I am thinking it's a Storage Queue. but it's a convoluted question.","comment_id":"147960","timestamp":"1596190860.0","upvote_count":"2","poster":"bob2Be"}],"comment_id":"130475","content":"Storage queue has scalability target of 2000 messages per second, which corresponds to 7.2 million per hour.","timestamp":"1594289160.0","upvote_count":"4"}],"comment_id":"119084","poster":"gagar500","timestamp":"1593059160.0","content":"The volume of agreements is expected to be in the millions per hour.\n- so it should be Event Hubs"},{"content":"Regarding retention, you can capture the data using Event Hubs Capture, so it is still a better choice than other options\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-faq#what-is-the-maximum-retention-period-for-events","poster":"Deli_SK","comment_id":"118392","upvote_count":"4","timestamp":"1593000120.0"},{"content":"\"User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\"\n\n\"Event Hubs Standard tier currently supports a maximum retention period of seven days. Event hubs aren't intended as a permanent data store. Retention periods greater than 24 hours are intended for scenarios in which it's convenient to replay an event stream into the same systems;\"\nThen definately not Event Hub.\n\nSo other options are:\n- Event grid stores small events (<64kB, 1MB in preview) and it's meant to send them away to other service.\n- Azure Service Bus topic (not queue) - 14 days retention in basic tier, unlimited in higher tiers. Should handle high volume of agreements. \n- Azure Storage queue - 7 days TTL\n\nI would go with Service bus topic.","poster":"nazzzu","timestamp":"1592677440.0","upvote_count":"8","comments":[{"comment_id":"117213","upvote_count":"1","poster":"bijolianabhi","content":"What is the correct answer? I believe, it should be Azure Service Bus Topic.","timestamp":"1592899500.0"},{"poster":"Cornholioz","content":"Where does it say User Agreements/responses need to be \"retained\" and for <this> long? It only says it must not be lost and be available to all parties regardless of uptime. Are we interpreting \"most not be lost\" as must be stored for a long duration? Or does it mean it should be available to be consumed by all parties?","comments":[{"timestamp":"1611594240.0","content":"Also they talk about when a new version of the service is released it must re-eval the last 7 days worth of data, Event HUBs has the ability to reply data and it's retention is 7 days","upvote_count":"1","comment_id":"276153","poster":"AfroYeti"}],"timestamp":"1606612860.0","comment_id":"229990","upvote_count":"2"}],"comment_id":"114940"}],"url":"https://www.examtopics.com/discussions/microsoft/view/22677-exam-az-204-topic-31-question-1-discussion/","question_images":[],"answer_ET":"B","question_id":234,"answer_images":[],"question_text":"You need to store the user agreements.\nWhere should you store the agreement after it is completed?","answer":"B","unix_timestamp":1591725120,"topic":"31","isMC":true,"timestamp":"2020-06-09 19:52:00","exam_id":48},{"id":"kztd9BJVMIiAEVZ40iHJ","question_id":235,"answer_description":"","answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04273/0017100001.jpg"],"topic":"31","unix_timestamp":1617881880,"timestamp":"2021-04-08 13:38:00","exam_id":48,"answer_images":["https://img.examtopics.com/az-204/image632.png"],"discussion":[{"content":"1- Queue Trigre[]\n2- Blob[]","poster":"MohmmadFayez","timestamp":"1626933840.0","comments":[{"timestamp":"1638608400.0","upvote_count":"1","poster":"Lucario95","content":"This is correct according to the data type of \"content\" for the first binding and \"output\" for the second.","comment_id":"493601"},{"content":"On My Exam:2022-12-26\nMy Answer:\nQuere Trigger\nBlob","comment_id":"758042","poster":"KingChuang","timestamp":"1672104540.0","upvote_count":"2"},{"poster":"warchoon","timestamp":"1681171200.0","comment_id":"866702","upvote_count":"1","content":"agree\nThis is the only one reason to transform a string to a stream in the binding function"},{"comment_id":"458650","timestamp":"1633600560.0","poster":"MiraA","content":"The \"content\" parameter is of type \"string\" so it must be QueueTrigger.\nFor example BlobTrigger uses Stream type, CosmosDBTrigger uses IReadOnlyList<> type, it seems the Table Storage has no trigger binding.\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings?tabs=csharp#supported-bindings\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue-trigger\nhttps://docs.microsoft.com/cs-cz/azure/azure-functions/functions-bindings-storage-blob-trigger\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-cosmosdb-v2-trigger","comments":[{"upvote_count":"1","comment_id":"525078","poster":"PhilLI","content":"Agree with Azure Queue Storage trigger indeed (see example on https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue-trigger?tabs=csharp#example)","timestamp":"1642350840.0"},{"content":"BlobTrigge can also use string parameter\n\n[FunctionName(\"BlobTriggerCSharp\")] \npublic static void Run([BlobTrigger(\"samples-workitems/{name}\")] Stream myBlob, string name, ILogger log)\n{\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp","timestamp":"1642601760.0","upvote_count":"4","poster":"Mev4953","comment_id":"527562"}],"upvote_count":"17"}],"upvote_count":"45","comment_id":"411397"},{"upvote_count":"20","poster":"gmishra88","comment_id":"689418","comments":[{"timestamp":"1732382880.0","content":"LOOOOL True... Whatever","comment_id":"1316756","upvote_count":"1","poster":"overhill"}],"content":"Another CSI Miami question from Microsoft.\nYes, detectives, the content has to be a Stream if it is Blob trigger. \nMicrosoft (don't be evil) did not give any information on how the function is triggered, but stereotypically put the red-herring in the description: \" website shows the content from azure storage blob\". Of course that does not mean the function is triggered from it. Next my only clue is the type of this parameter. \nMicrosoft says .Net knowledge is required and any SDK is enough. But how will a javascript person know this that it is Stream and not the content given as String?","timestamp":"1665239160.0"},{"timestamp":"1712250900.0","poster":"oskx2","content":"I think it should be BlobTrigger/Blob because the output it has the Stream data type and from where the binding expression will come ({name}) if it's not specified in the trigger. Also, blob trigger also receives the content as string.\nAnyways, this is an awful question. Shame on Microsoft","comment_id":"1189467","upvote_count":"1"},{"content":"Requirement/question is unclear. Think the idea is that the function reads the uploaded data by the user. So it should be triggered by the Blob trigger. After the check it updates the content in the blob storage. So blob storage is the output parameter.","timestamp":"1707472860.0","comment_id":"1145377","poster":"raymond_abcd","upvote_count":"1"},{"poster":"LSandro","timestamp":"1699447620.0","upvote_count":"1","comment_id":"1065628","content":"BlobTrigger + Blob\n\nSource: https://learn.microsoft.com/en-us/answers/questions/1186952/how-to-get-a-blobtrigger-with-a-stream-in-azure-fu"},{"timestamp":"1690878480.0","poster":"Dianahu","comment_id":"968887","upvote_count":"1","content":"https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-input?tabs=python-v2%2Cin-process&pivots=programming-language-csharp\n[FunctionName(\"BlobInput\")]\npublic static void Run(\n [QueueTrigger(\"myqueue-items\")] string myQueueItem,\n [Blob(\"samples-workitems/{queueTrigger}\", FileAccess.Read)] Stream myBlob,\n ILogger log)\n{\n log.LogInformation($\"BlobInput processed blob\\n Name:{myQueueItem} \\n Size: {myBlob.Length} bytes\");\n}"},{"upvote_count":"3","timestamp":"1687257000.0","comment_id":"928347","poster":"JH81","content":"Terrible requirements that leave us questioning the actual implementation. But I have to agree that QueueTrigger is the first answer because the requirements state \"accepts user generated content as a string\". That can only be the QueueTrigger. Blob for the output is not in question."},{"timestamp":"1662068040.0","poster":"le129","upvote_count":"1","comment_id":"656690","content":"Blob trigger seems correct. trigger binding can be string if the content is small.\nBinding to string, or Byte[] is only recommended when the blob size is small. This is recommended because the entire blob contents are loaded into memory. For most blobs, use a Stream or CloudBlockBlob type. For more information, see Concurrency and memory usage.\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=in-process%2Cextensionv5&pivots=programming-language-csharp"},{"timestamp":"1644104340.0","content":"the question said accept user upload content as a string. \nand user upload to blob to trigger the content check. \nso blobtrigger and blob are correct.","upvote_count":"3","comment_id":"541346","poster":"shawnz"},{"upvote_count":"3","poster":"leonidn","timestamp":"1643445480.0","content":"\"content\" is of type string. Then QueueTrigger is the only what that is applicable. Agree with \"blob\".","comment_id":"535319"},{"content":"It is really annoying the way the question is worded. Where in the specs is written the source and the target of the Function ?!? It is true that we can infer it from the possible answers. But why dont ask then \"which are valid input / output\" for the function ?","poster":"gfiorini","comments":[{"poster":"gmishra88","timestamp":"1665238140.0","content":"It is Microsoft","upvote_count":"3","comment_id":"689397"}],"upvote_count":"8","comment_id":"482772","timestamp":"1637433060.0"},{"upvote_count":"3","content":"The question has nothing to do with the architecture or design. It's actually asking if you can recognize valid trigger and output bindings. The only valid bindings are Queue Trigger (string) and Blob output (File.Write supports writing to a stream).","comment_id":"470416","poster":"phvogel","timestamp":"1635633240.0"},{"content":"I believe this is blobtrigger --> user upload contents into blob storage ...\nNeed send to a service ... So I am think send to a queue for processing ...\nJust from the given info ...","poster":"ning","upvote_count":"2","comment_id":"433839","timestamp":"1630152240.0","comments":[{"content":"Depends on your interpretation for what is final step for azure function, whether to send the contents to the service, or remove the contents from the blob storage ... The requirements are not clear ...","poster":"ning","timestamp":"1630152480.0","comment_id":"433845","upvote_count":"2"},{"comment_id":"514613","content":"Blob trigger recieves stream, Service Bus Queue trigger recieves string, therefore the first option is not blob trigger but a queue trigger","poster":"mcanic","upvote_count":"2","timestamp":"1641060480.0"}]},{"timestamp":"1628210160.0","comment_id":"420523","comments":[{"timestamp":"1628210520.0","poster":"j888","upvote_count":"1","comment_id":"420525","content":"I was wrong.\nThe service bus trigger would be \npublic static void Run([ServiceBusTrigger(\"CustomerQueue\"...."}],"content":"This is tricky, storage will be my general choice, however this statement \"Messages are sent to contentuploadservice\" leading me to believe this is a service bus.","upvote_count":"1","poster":"j888"},{"upvote_count":"6","comments":[{"content":"Well, based what information was given it seems that entire design looks little bit different. User post content and it is saved in BlobStorage and available immediately. After that check is done and if content is invalid then it is replaced. So given answer is correct.","comment_id":"376138","poster":"Spooky7","timestamp":"1622992140.0","upvote_count":"5"},{"comment_id":"866701","timestamp":"1681170960.0","content":"So why did you transform stream to string and then transform it to stream again?","poster":"warchoon","upvote_count":"1"}],"poster":"SuperPetey","comment_id":"349967","timestamp":"1620193440.0","content":"I believe the answer is QueueTrigger and Queue. This is an architectural design issue -- the entire reason to do user content auditing is to prohibit bad content from being stored and used. Therefore, use a queue to store these messages until they are verified. No need to store content in a blob until then -- Azure Storage Queue perfect solution."},{"timestamp":"1618344720.0","comment_id":"334911","content":"Correct.\nRef: https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-output?tabs=csharp\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-input?tabs=csharp","upvote_count":"14","poster":"clarionprogrammer"},{"poster":"kwaazaar","upvote_count":"6","comment_id":"331133","timestamp":"1617881880.0","content":"Correct"}],"question_text":"HOTSPOT -\nYou need to implement the bindings for the CheckUserContent function.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answers_community":[],"isMC":false,"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/49599-exam-az-204-topic-31-question-2-discussion/"}],"exam":{"name":"AZ-204","lastUpdated":"12 Apr 2025","isImplemented":true,"provider":"Microsoft","isMCOnly":false,"numberOfQuestions":452,"isBeta":false,"id":48},"currentPage":47},"__N_SSP":true}