{"pageProps":{"questions":[{"id":"rDpIw1Dxs12spFsN5MqZ","answer_ET":"B","question_images":[],"unix_timestamp":1670659680,"exam_id":70,"answer_images":[],"question_text":"This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nContoso, Ltd. is a company that sells enriched financial data to a variety of external customers.\nContoso has a main office in Los Angeles and two branch offices in New York and Seattle.\n\nExisting Environment -\n\nData Infrastructure -\nContoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.\nThe data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.\nContoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.\nUsers frequently query the Synapse Analytics workspace by using Transact-SQL.\n\nUser Problems -\nContoso identifies the following user issues:\nSome users indicate that the visuals in Power BI reports are slow to render when making filter selections.\nUsers indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.\nUsers indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.\n\nPlanned Changes -\nContoso plans to implement the following changes:\nInto the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.\nBuild a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.\nProvide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.\nImplement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.\nDeploy prebuilt datasets to Power BI to simplify the query experience of the customers.\nProvide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.\nYou need to identify the root cause of the data refresh issue.\nWhat should you use?","timestamp":"2022-12-10 09:08:00","topic":"1","answers_community":["B (80%)","C (20%)"],"discussion":[{"timestamp":"1674115920.0","upvote_count":"7","poster":"Az301301X","content":"Selected Answer: B\nQuery Diagnostics will do the trick. \"...you can use it to understand what sort of queries you're emitting, what slowdowns you might run into during authoring refresh...\" https://learn.microsoft.com/en-us/power-query/query-diagnostics. \nSo, B.","comment_id":"780879"},{"upvote_count":"6","comment_id":"743204","poster":"nbagchi","content":"Correct,\nQuery Diagnostics helps in understanding what Power Query is doing at authoring and at refresh time in Power BI Desktop.","timestamp":"1670871480.0"},{"poster":"Deloro","upvote_count":"1","comment_id":"1007190","timestamp":"1694667420.0","content":"i would also say B since its data refresh, \nperformance analyzer is time of the visuals to refresh (and update)"},{"poster":"gerryboy","timestamp":"1692895920.0","upvote_count":"1","content":"Selected Answer: B\nwill give all stats in Query diagnostics.","comment_id":"989342"},{"comment_id":"963971","upvote_count":"1","content":"Selected Answer: B\nData refresh is the equivalent of data load, which is more upfront than loading visuals. Hence we need to use Query Diagnostics (answer B)","poster":"hoss29","timestamp":"1690389000.0"},{"content":"Selected Answer: B\nB is correct answer. \nThe question is asking why does the refresh fail not why does the report take a long time to load. \nThe PA will analyse where the bottleneck is. \nPower Query Editor will tell you why the refresh is failing.","comment_id":"922728","upvote_count":"3","timestamp":"1686714360.0","poster":"Eltooth"},{"poster":"Tranter","timestamp":"1686203160.0","upvote_count":"3","comment_id":"917853","content":"When you encounter slow rendering specifically during filter selections in Power BI reports, the Performance Analyzer tool is more suitable for identifying the root causes and optimizing performance. The Performance Analyzer helps you analyze the rendering time of visuals and components in your report, allowing you to pinpoint the specific areas that are causing the slowdown during filter selections. By using the Performance Analyzer, you can focus on optimizing those visuals or components to improve rendering performance.\n\nOn the other hand, Query Diagnostics in Power Query Editor is designed to analyze and optimize the performance of data loading and transformations within the Power Query Editor. It is more relevant when you want to troubleshoot and optimize the data loading process, rather than the rendering performance during filter selections.\n\nso in the context of slow rendering during filter selections, the Performance Analyzer in Power BI Desktop is the recommended tool to use for identifying the root causes and optimizing performance.","comments":[{"comment_id":"994981","content":"Question mentioned data refresh issues so your explanation for Query Diagnostics in Power Query Editor suits","poster":"Qordata","timestamp":"1693475220.0","upvote_count":"1"}]},{"comment_id":"870403","upvote_count":"1","poster":"DarioReymago","timestamp":"1681497060.0","content":"Selected Answer: C\nI prefere C, B doesnt show info about interactions and metrics"},{"content":"Selected Answer: C\nMonitor report performance in Power BI Desktop using the Performance Analyzer. Monitoring will help you learn where the bottlenecks are, and how you can improve report performance.\n\nMonitoring performance is relevant in the following situations:\n\nYour Import data model refresh is slow.\nYour DirectQuery or Live Connection reports are slow.\nYour model calculations are slow.","upvote_count":"2","comment_id":"801855","poster":"laureli","timestamp":"1675849980.0","comments":[{"poster":"laureli","timestamp":"1675850160.0","upvote_count":"2","content":"https://learn.microsoft.com/en-us/power-bi/guidance/monitor-report-performance","comment_id":"801858"},{"upvote_count":"1","timestamp":"1684931220.0","comment_id":"905879","content":"C cannot be used to monitor Premium per user activities or capacity so will go with B","poster":"Tranter"}]},{"timestamp":"1673240940.0","comments":[{"comment_id":"795274","content":"'It is asking for the 'root cause of the data refresh issue.' Not the visualsâ€¦ so answer is not Performance Analyzer.","poster":"KoryMills","upvote_count":"1","timestamp":"1675267080.0"}],"content":"Should be C - Performance Analyzer\nhttps://learn.microsoft.com/en-us/power-bi/guidance/monitor-report-performance","poster":"abain89","upvote_count":"4","comment_id":"770038"},{"poster":"JasonVu","comment_id":"740773","content":"Got this on 12/08/2022","timestamp":"1670659680.0","upvote_count":"2"}],"choices":{"C":"Performance analyzer in Power BI Desktop","A":"the Usage Metrics Report in powerbi.com","B":"Query Diagnostics in Power Query Editor"},"url":"https://www.examtopics.com/discussions/microsoft/view/90896-exam-dp-500-topic-1-question-1-discussion/","question_id":1,"answer":"B","answer_description":"","isMC":true},{"id":"dO0G7gc6H8T4FDgYfNSc","isMC":true,"answer_ET":"A","answer":"A","choices":{"A":"Grant the sales managers the Build permission for the existing Power BI datasets.","C":"Create a deployment pipeline and grant the sales managers access to the pipeline.","D":"Create a PBIT file and distribute the file to the sales managers.","B":"Grant the sales managers admin access to the existing Power BI workspace."},"question_text":"This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\n\nExisting environment -\nLitware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.\n\nAzure Resources -\nLitware has the following Azure resources:\nAn Azure Synapse Analytics workspace named synapseworkspace1\nAn Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1\nA Synapse Analytics dedicated SQL pool named SQLDW\n\nDedicated SQL Pool -\nSQLDW contains a dimensional model that contains the following tables.\n\n//IMG//\n\nSQLDW contains the following additional tables.\n//IMG//\n\n\nSQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].\nThe sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.\n\nPower BI -\nLitware has a new Power BI tenant that contains an empty workspace named Sales Analytics.\nAll users have Power BI Premium per user licenses.\nIT data analytics are workspace administrators. The IT data analysts will create datasets and reports.\nA single imported dataset will be created to support the companyâ€™s sales analytics goals. The dataset will be refreshed every 30 minutes.\n\nRequirements -\n\nAnalytics Goals -\nLitware identifies the following analytics goals:\nProvide historical reporting of sales by product and channel over time.\nAllow sales managers to perform ad hoc sales reporting with minimal effort.\nPerform market basket analysis to understand which products are commonly purchased in the same transaction.\nIdentify which customers should receive promotional emails based on their likelihood of purchasing promoted products.\nLitware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.\n\nSecurity Requirements -\nLitware identifies the following security requirements for the analytics environment:\nAll the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.\nCustomer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.\nSales associates must be prevented from seeing the CustomerEmail column in Power BI reports.\nSales managers must be prevented from modifying reports created by other users.\nDevelopment Process Requirements\nLitware identifies the following development process requirements:\nSQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.\nPower BI content must be deployed to test and production by using deployment pipelines.\nAll SQL scripts must be stored in Azure Repos.\nThe IT data analysts prefer to build Power BI reports in Synapse Studio.\nYou need to configure the Sales Analytics workspace to meet the ad hoc reporting requirements.\nWhat should you do?","unix_timestamp":1670873940,"timestamp":"2022-12-12 20:39:00","exam_id":70,"answer_images":[],"answers_community":["A (71%)","D (25%)","4%"],"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/91274-exam-dp-500-topic-1-question-10-discussion/","question_images":["https://img.examtopics.com/dp-500/image8.png","https://img.examtopics.com/dp-500/image9.png"],"discussion":[{"upvote_count":"12","content":"A is Correct, Template file will require data loading and will lead to separate Power BI dataset and reports. To minmize the effort, should give users build access to existing dataset.","comments":[{"timestamp":"1672648500.0","comment_id":"763592","content":"Totally agree","poster":"cherious","upvote_count":"3"}],"timestamp":"1671085980.0","poster":"Bob5379","comment_id":"745769"},{"timestamp":"1673552220.0","comment_id":"773836","upvote_count":"8","poster":"Saffar","content":"Selected Answer: A\nI think it should be \"A\". Build permission for the existing Power BI datasets. This will reduce the effort for managers, more secure than template which will lead to different dataset/refresh cycle, etc. They can create their own reports from same dataset and have nothing to do with allowing them to change the dataset or any other reports."},{"upvote_count":"1","content":"ChatGPT says A","comment_id":"1054617","timestamp":"1698326880.0","poster":"TheSwedishGuy"},{"comment_id":"984951","timestamp":"1692420960.0","content":"Selected Answer: A\nI took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was \"Grant the sales managers the Build permission for the existing Power BI datasets.\"","upvote_count":"3","poster":"SamuComqi"},{"timestamp":"1688576760.0","content":"Selected Answer: A\nI think A is the correct one and the next step is azure synapse is https://techcommunity.microsoft.com/t5/educator-developer-blog/how-to-connect-azure-synapse-to-power-bi-for-data-visualization/ba-p/3614555","poster":"MohsenSic","comment_id":"943975","upvote_count":"1"},{"poster":"PrudenceK","timestamp":"1688161200.0","comment_id":"939406","content":"Selected Answer: C\nBy creating a deployment pipeline, you can establish a controlled and standardized process for promoting Power BI content from the development environment to the test and production workspaces. Granting the sales managers access to the pipeline allows them to deploy their own ad hoc reports while adhering to the development process requirements. This ensures that the reports go through the proper testing and production stages.","upvote_count":"1"},{"upvote_count":"2","timestamp":"1684329720.0","comment_id":"900180","poster":"asyia","content":"Correct Answer :\nC. Create a deployment pipeline and grant the sales managers access to the pipeline.\n\nExplanation:\nCreating a deployment pipeline is a recommended approach to deploy Power BI content to different environments, such as the test and production workspaces. By setting up a deployment pipeline, you can ensure a standardized and automated process for deploying Power BI reports and dashboards. Granting sales managers access to the pipeline allows them to initiate and manage the deployment of Power BI content themselves, enabling ad hoc reporting with minimal effort. This empowers sales managers to access the necessary reports and perform ad hoc analysis as per their requirements, without needing direct access to the development or production workspaces."},{"timestamp":"1681658580.0","comment_id":"871871","comments":[{"poster":"DarioReymago","upvote_count":"2","timestamp":"1681658640.0","comment_id":"871872","content":"Sorry answer B"}],"content":"Selected Answer: A\nI will set Admin role to Manager and readers for the rest to avoid other edit or modify reports","poster":"DarioReymago","upvote_count":"1"},{"upvote_count":"1","timestamp":"1681529880.0","comment_id":"870589","content":"Selected Answer: A\nI agree that A is the correct answer","poster":"GHill1982"},{"content":"Selected Answer: A\nThe requirement is regarding to the \"workspace\" so, it clearly the \"A\". Building permission allow them to create reports without affecting the previous one and you could avoid additional effort. \nLink : https://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-build-permissions","upvote_count":"3","poster":"solref","comment_id":"855247","timestamp":"1680148260.0"},{"upvote_count":"2","timestamp":"1677592920.0","comment_id":"824873","content":"I would choose A","poster":"Review66"},{"comment_id":"765362","timestamp":"1672819380.0","comments":[{"comment_id":"800600","upvote_count":"2","content":"1. Using the Build feature, users would still be creating their own separate reports and individual reports will be unaffected.\n2. Business requirements says a single imported dataset will be used and refreshed every 30 minutes.\n3. For ad-hoc reporting, it is not recommended to distribute pbit files among all a long lisy of users.\n\nHence, Build permissions is the most feasible answer I can think of.","timestamp":"1675749720.0","poster":"cookiemonster42"}],"poster":"Az301301X","content":"Selected Answer: D\nIt says: \"Sales managers must be prevented from modifying reports created by other users.\" So, a PBIT (...a template) would do the trick. So, D.","upvote_count":"6"},{"timestamp":"1671191820.0","comment_id":"747132","upvote_count":"3","poster":"you5uf","content":"Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.\nSales managers must be prevented from modifying reports created by other users.\nThe feasibility Says PBIT to comply with 1st point"},{"poster":"nbagchi","comment_id":"743268","content":"Correct\nCheck this link for more info: https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-templates","timestamp":"1670873940.0","upvote_count":"4"}],"question_id":2,"answer_description":""},{"id":"YwLy8msaDVTexCVnWUDC","url":"https://www.examtopics.com/discussions/microsoft/view/92977-exam-dp-500-topic-1-question-100-discussion/","choices":{"C":"all the workspace members of any workspace that uses the dataset","B":"the workspace admins of any workspace that uses the dataset","A":"the Power BI admins","D":"any users that accessed a report that uses the dataset within the last 30 days"},"answers_community":["B (100%)"],"topic":"1","answer_description":"","isMC":true,"question_images":[],"timestamp":"2022-12-27 14:43:00","discussion":[{"upvote_count":"8","poster":"jeroen12345","timestamp":"1672148580.0","comment_id":"758569","content":"Selected Answer: B\nIt will be send to the contact list of the workspace. Which by default contains the workspace admins (but can be modified). B"},{"upvote_count":"5","comment_id":"762556","timestamp":"1672474980.0","poster":"cherious","content":"Selected Answer: B\nAssuming that contact list has not modified for the workspaces then the answer is B, because by default the contact list only includes workspace admins.\n \nWhen you click on Notify Contacts, the window pops up and shows that \"An email notification will be sent to all the contacts for all impacted workspaces, including workspaces you don't have access to.\" So if other users are added in the contact list, then they will also receive email"},{"upvote_count":"1","timestamp":"1692422760.0","content":"Selected Answer: B\nI took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- the workspace admins of any workspace that uses the dataset","poster":"SamuComqi","comment_id":"984991"},{"comment_id":"869389","poster":"DarioReymago","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/service-dataset-impact-analysis#notify-contacts\n\nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/service-create-the-new-workspaces#create-a-contact-list","upvote_count":"2","timestamp":"1681388340.0"},{"timestamp":"1673672340.0","upvote_count":"4","content":"it's B\nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/service-create-the-new-workspaces#create-a-contact-list","comment_id":"775093","poster":"Saffar"}],"question_id":3,"answer_ET":"B","answer_images":[],"answer":"B","unix_timestamp":1672148580,"exam_id":70,"question_text":"You plan to modify a Power BI dataset.\nYou open the Impact analysis panel for the dataset and select Notify contacts.\nWhich contacts will be notified when you use the Notify contacts feature?"},{"id":"RovxuH8X7U4vdyeKi5uY","isMC":true,"question_images":[],"answer_description":"","unix_timestamp":1674372660,"question_text":"You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.\nYou need to present the data distribution statistics from a DataFrame in a tabular view.\nWhich method should you invoke on the DataFrame?","topic":"1","choices":{"C":"explain","A":"rollup","D":"describe","B":"cov"},"question_id":4,"answer_images":[],"answers_community":["D (100%)"],"answer_ET":"D","answer":"D","discussion":[{"poster":"Eltooth","timestamp":"1687776180.0","comment_id":"934312","content":"Selected Answer: D\nD is correct answer.","upvote_count":"1"},{"comment_id":"869847","upvote_count":"2","poster":"DarioReymago","timestamp":"1681430940.0","content":"Selected Answer: D\nCorrect answer is D"},{"timestamp":"1674372660.0","upvote_count":"4","comment_id":"784034","poster":"per_ing","content":"Selected Answer: D\nDuplicate of 62 and 103. Correct answer is D"}],"exam_id":70,"url":"https://www.examtopics.com/discussions/microsoft/view/96447-exam-dp-500-topic-1-question-101-discussion/","timestamp":"2023-01-22 08:31:00"},{"id":"O6tLtzqf5RgyVFBaY2rP","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are using an Azure Synapse Analytics serverless SQL pool to query a collection of Apache Parquet files by using automatic schema inference. The files contain more than 40 million rows of UTF-8-encoded business names, survey names, and participant counts. The database is configured to use the default collation.\nThe queries use OPENROWSET and infer the schema shown in the following table.\n//IMG//\n\nYou need to recommend changes to the queries to reduce I/O reads and tempdb usage.\nSolution: You recommend using OPENROWSET WITH to explicitly define the collation for businessName and surveyName as Latin1_General_100_BIN2_UTF8.\nDoes this meet the goal?","isMC":true,"discussion":[{"timestamp":"1697172960.0","poster":"Qordata","upvote_count":"2","comment_id":"1042295","content":"Selected Answer: A\nThe comments are very descriptive and suggest answer as A"},{"upvote_count":"1","timestamp":"1696035000.0","content":"Selected Answer: A\nI sitck with AAAAAAA","poster":"sheilawu","comment_id":"1021178"},{"upvote_count":"2","comment_id":"1005285","timestamp":"1694480820.0","poster":"fireofsea","content":"Selected Answer: A\nMake sure that you are explicilty specifying some UTF-8 collation (for example Latin1_General_100_BIN2_UTF8) for all string columns in WITH clause or set some UTF-8 collation at database level. \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files"},{"poster":"fireofsea","upvote_count":"1","comment_id":"995730","timestamp":"1693547520.0","content":"Selected Answer: A\nhttps://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/always-use-utf-8-collations-to-read-utf-8-text-in-serverless-sql/ba-p/1883633","comments":[{"comments":[{"upvote_count":"3","poster":"hoss29","content":"please disregard as here we are not changing the database collation. Answer should be YES. From the copilot: \nReducing I/O Reads\nWhen you query Parquet files in Azure Synapse Analytics, the service can push down certain filter predicates to the storage layer. This means that only the relevant row groups are read from the Parquet files, which can significantly reduce I/O reads. However, this predicate pushdown is only possible if the collation of the column in the Parquet file matches the collation of the column in SQL Server. By explicitly setting the collation to Latin1_General_100_BIN2_UTF8, you ensure that the collations match, enabling predicate pushdown and reducing I/O reads.\n\nReducing tempdb Usage\nWhen you query data with a different collation than the serverâ€™s default collation, SQL Server needs to perform a collation conversion. This conversion happens in memory and can increase tempdb usage. By setting the collation at the column level to match the dataâ€™s actual collation, you avoid these conversions and reduce tempdb usage.","timestamp":"1696347840.0","comment_id":"1024037"}],"comment_id":"1024025","upvote_count":"1","poster":"hoss29","timestamp":"1696346040.0","content":"But according to that same link, the database has to be dropped beforehand. Last paragraph says \" NOTE: If you have existing tables that used default database collation, changing default database collation would not be applied on them. You would need to drop and re-create external tables so they can pickup new default.\" So i think the answer is NO"}]},{"comment_id":"939476","content":"Selected Answer: B\nDefining the collation for the columns as Latin1_General_100_BIN2_UTF8 will not directly reduce I/O reads and tempdb usage. Collation determines the sorting and comparison rules for character data and does not have a direct impact on I/O or tempdb usage.\n\nTo reduce I/O reads and tempdb usage, you should consider the following approaches:\n\n Implement appropriate indexing on the columns used in the queries to improve query performance and reduce the need for extensive I/O reads.\n Optimize the query logic by using efficient filters and aggregations to minimize the amount of data read and processed.\n Consider partitioning the data based on relevant criteria, such as date or another logical partitioning key, to improve query performance and reduce I/O reads.\n Utilize appropriate compression techniques for the Parquet files to reduce their size, leading to reduced I/O reads.","poster":"PrudenceK","timestamp":"1688173260.0","upvote_count":"1"},{"timestamp":"1682437080.0","poster":"sgodd_0298","upvote_count":"2","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-data-storage","comment_id":"880588"},{"content":"Selected Answer: A\na is correct \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files","upvote_count":"3","timestamp":"1681432380.0","poster":"DarioReymago","comment_id":"869859"},{"content":"Selected Answer: A\nThe Latin1_General_100_BIN2_UTF8 collation has additional performance optimization that works only for parquet and Cosmos DB.\nLINK: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files \n\nCollations: The default inheritance can be overridden by explicitly stating a different collation for a character-based data type.","comment_id":"854096","poster":"solref","timestamp":"1680072780.0","upvote_count":"4"},{"upvote_count":"4","content":"Selected Answer: A\nData in a Parquet file is organized in row groups. Serverless SQL pool skips row groups based on the specified predicate in the WHERE clause, which reduces IO. The result is increased query performance.\nIf we want to filter by businessName and/or by survey name then is recommended to use the Latin1_General_100_BIN2_UTF8 collation\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool#use-proper-collation-to-utilize-predicate-pushdown-for-character-columns","timestamp":"1676356860.0","comment_id":"808151","poster":"Fer079"},{"timestamp":"1674014400.0","content":"Selected Answer: B\nB is the correct one.","comment_id":"779596","upvote_count":"3","poster":"louisaok"},{"timestamp":"1673478960.0","upvote_count":"3","content":"using \"OPENROWSET WITH to explicitly define the collation for businessName and surveyName as Latin1_General_100_BIN2_UTF8\", will allow you to read the string properly, but I think it has nothing to do with decreasing the I/O, so most likely it's NO","comment_id":"772965","poster":"Saffar"},{"timestamp":"1673140560.0","content":"Selected Answer: B\nNo,\n\nSolution: You recommend defining a data source and view for the Parquet files. You recommend updating the query to use the view.","upvote_count":"4","poster":"louisaok","comment_id":"769008"},{"content":"Selected Answer: B\nYou don't need to use the OPENROWSET WITH clause when reading Parquet files\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files","comment_id":"762248","comments":[],"poster":"Maazi","upvote_count":"4","timestamp":"1672427400.0"}],"answer_ET":"A","exam_id":70,"answer":"A","topic":"1","answer_images":[],"question_images":["https://img.examtopics.com/dp-500/image130.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/93312-exam-dp-500-topic-1-question-102-discussion/","question_id":5,"answers_community":["A (61%)","B (39%)"],"choices":{"A":"Yes","B":"No"},"answer_description":"","timestamp":"2022-12-30 20:10:00","unix_timestamp":1672427400}],"exam":{"isMCOnly":false,"numberOfQuestions":183,"provider":"Microsoft","isImplemented":true,"id":70,"isBeta":false,"name":"DP-500","lastUpdated":"12 Apr 2025"},"currentPage":1},"__N_SSP":true}