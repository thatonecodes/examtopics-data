{"pageProps":{"questions":[{"id":"OPS5gsvawAE5OFwTJ57u","answer_ET":"C","question_images":[],"timestamp":"2022-12-10 12:08:00","isMC":true,"answer":"C","exam_id":70,"question_id":121,"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/90915-exam-dp-500-topic-1-question-42-discussion/","answer_images":[],"answer_description":"","question_text":"You are building a Power BI dataset that will use two data sources.\nThe dataset has a query that uses a web data source. The web data source uses anonymous authentication.\nYou need to ensure that the query can be used by all the other queries in the dataset.\nWhich privacy level should you select for the data source?","choices":{"D":"Private","A":"None","C":"Public","B":"Organizational"},"unix_timestamp":1670670480,"discussion":[{"content":"Selected Answer: C\nAnswer is correct.\nIf the report has only one data source, there is no need to change this privacy level (None is the default) because there is no risk of exposing data between various sources. However, as soon as a second data source is created in the report, Power BI asks for the privacy level to be determined for both data sources because it cannot be kept to None now. So choosing it to Public is the correct option.","comment_id":"744120","poster":"nbagchi","timestamp":"1670941380.0","upvote_count":"11"},{"timestamp":"1672262040.0","poster":"Maazi","comment_id":"760345","upvote_count":"5","content":"Selected Answer: C\nCheck this out Ref: https://learn.microsoft.com/en-us/power-bi/enterprise/desktop-privacy-levels"},{"poster":"Alborz","timestamp":"1690811760.0","content":"Selected Answer: D\nOption C: Public is not the correct privacy level to select in this scenario. Choosing the \"Public\" privacy level would allow the data source query to be shared with other users, making the data accessible to them. However, using the \"Public\" privacy level for a web data source with anonymous authentication can potentially expose sensitive information to unauthorized users.\nIn this case, selecting the \"Private\" privacy level is the appropriate choice because it allows the data source query to access the web data source with anonymous authentication without any privacy restrictions, ensuring that other queries in the dataset can use the data returned by this web data source query.","upvote_count":"1","comment_id":"968167"},{"comment_id":"910892","poster":"Eltooth","content":"Selected Answer: C\nC is correct answer.","upvote_count":"1","timestamp":"1685513220.0"},{"upvote_count":"2","comments":[{"content":"it cannot be None because this level stops being a viable option as soon as the PBI desktop report has more than one source which is explicitly emphasized in the question (probably exactly to avoid people answering with None)","poster":"stfglv","comment_id":"774214","timestamp":"1673596380.0","upvote_count":"1"}],"timestamp":"1670670480.0","content":"correct answer is A.\nTo ensure that a query that uses a web data source with anonymous authentication can be used by all the other queries in a Power BI dataset, you should select the None privacy level for the data source. The None privacy level allows all the queries in the dataset to access the data from the web data source, regardless of whether the data source uses anonymous authentication.","poster":"AT96","comment_id":"740895"}],"answers_community":["C (94%)","6%"]},{"id":"5VAiYkr23ex1KZT6yPTo","question_text":"DRAG DROP -\nYou manage a Power BI dataset that queries a fact table named SalesDetails. SalesDetails contains three date columns named OrderDate, CreatedOnDate, and ModifiedDate.\nYou need to implement an incremental refresh of SalesDetails. The solution must ensure that OrderDate starts on or after the beginning of the prior year.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\n//IMG//","question_id":122,"answer_description":"","unix_timestamp":1670941980,"exam_id":70,"answer":"","topic":"1","answer_images":["https://img.examtopics.com/dp-500/image47.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/91438-exam-dp-500-topic-1-question-43-discussion/","timestamp":"2022-12-13 15:33:00","answers_community":[],"question_images":["https://img.examtopics.com/dp-500/image46.png"],"discussion":[{"comments":[{"content":"small adjustment: 1 year will be archived and 6 months will be incrementally refreshed","comment_id":"1101811","poster":"MaryemSB","upvote_count":"1","timestamp":"1703095200.0"}],"upvote_count":"1","timestamp":"1701853020.0","poster":"MaryemSB","content":"it's required to exclude data before start of prior year, so, best practice is to filter it out first.\nif we are in June 2023, then all data we have is 1 year and 6 months. \ndata will be split into 1 year of incremental refresh and 6 months will be archived. we can't use the option \"configure IR to archive data 2 years before refresh date\" otherwise, IR will have no sense!\nsolution is :\n1. filter prior year\n2. create parameters\n3. create custom filter for date\n4. configure incremental refresh ONE YEAR BEFORE REFRESH DATE","comment_id":"1089119"},{"content":"create\nfilter based on start and end\nrefresh 2 years, start of previous needs to be included","comment_id":"1007322","upvote_count":"1","poster":"Deloro","timestamp":"1694676120.0"},{"poster":"Plb2","upvote_count":"4","comments":[{"poster":"hoss29","timestamp":"1692196620.0","content":"I think the answer provided is correct, because we have to consider a refresh date that occurs in june 2023 for example, which means we need to refresh from Jan 2022 onwards, and that cannot be done by \"configure incr refresh to incrementally refresh (instead of archive) data since 1 year before refresh date\" . After the first 2 steps, we need to first archive the last 2 years and then filter OrderDate to the start of the prior year.","upvote_count":"1","comment_id":"982678"}],"content":"I feel there's an error in the answers.\nIt should be (create parameters, filter orderdata on parameters and then)\nconfigure incr refresh to archive data 2 years before refresh date\nconfigure incr refresh to incrementally refresh (instead of archive) data since 1 year before refresh date","comment_id":"933841","timestamp":"1687720380.0"},{"timestamp":"1679830080.0","content":"https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-configure\nis ok","comments":[{"comments":[{"comments":[{"upvote_count":"1","poster":"Deloro","content":"2 years, so you always include start of prior year ?","timestamp":"1695117540.0","comment_id":"1011174"}],"comment_id":"871951","upvote_count":"1","timestamp":"1681663740.0","poster":"fdsdfgxcvbdsfhshfg","content":"yeah it's weird, but if it has to be 4 steps..."}],"content":"I mean, I not sure why archive 2years before. I prefere 1 year before","timestamp":"1679830500.0","comment_id":"850949","upvote_count":"2","poster":"DarioReymago"}],"upvote_count":"1","poster":"DarioReymago","comment_id":"850939"},{"poster":"stfglv","content":"I don't understand why would we need four steps. I would say that the first 3 steps should be 1. create RangeStart and RangeEnd parameters 2. filter the OrderDate column using a customer filter based on these wo parameters 3. configure an incremental refresh to archive data before the last two years. But after that you should arguably publish the report and do a manual refresh as explained here https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-overview. So what is this 4th step supposed to do?","comments":[{"poster":"stfglv","comment_id":"785356","upvote_count":"3","content":"Just realized that filtering OrderDate to start of previous year is also needed (but does not have to be the 4th step, can even be the first thing) gives us the subset of data that is relevant since the question more/less states that nothing before that matters. Also this type of filtering determines that the incremental policy should not affect the last two years but only the previous year.","timestamp":"1674478800.0"},{"poster":"fdsdfgxcvbdsfhshfg","upvote_count":"3","timestamp":"1681663680.0","comment_id":"871948","content":"Let's suppose we're in 2023 and the solution requires us to have OrderDate starting on or after the beginning of the prior year (so 2022 onward):\n1. Create RangeStart RangeEnd - obviously\n2. Add an applied step to filter our dates between these two params - obviously\n3. Create an incremental refresh to archive data that starts ONE YEAR BEFORE THE REFRESH DATE.\nNow what that third step does is all we need for the solution - it creates a yearly partition for the entire 2022, and the quarterly/monthly partitions in the current 2023 year. We don't need any additional filtering.\n\nThe same can be achieved in 4 steps, with last 2 years being archived (2021, 2022) and then by filtering OrderDate to the start of the prior year.\n\nSo two approaches are possible in here, but we should select the 4-steps one.\n\nhttps://youtu.be/Kui_1G6kQIQ?t=466\n\nCorrect me If I'm wrong"}],"timestamp":"1673597460.0","comment_id":"774231","upvote_count":"2"},{"content":"Correct","comment_id":"744135","poster":"nbagchi","upvote_count":"4","timestamp":"1670941980.0"}],"answer_ET":"","isMC":false},{"id":"Lgz5CEKshdD368zQgJK6","answer_description":"","isMC":false,"exam_id":70,"question_id":123,"url":"https://www.examtopics.com/discussions/microsoft/view/93992-exam-dp-500-topic-1-question-44-discussion/","question_text":"DRAG DROP -\nYou plan to create a Power BI report that will use an OData feed as the data source. You will retrieve all the entities from two different collections by using the same service root.\nThe OData feed is still in development. The location of the feed will change once development is complete.\nThe report will be published before the OData feed development is complete.\nYou need to minimize development effort to change the data source once the location changes.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n//IMG//","timestamp":"2023-01-05 10:00:00","topic":"1","unix_timestamp":1672909200,"question_images":["https://img.examtopics.com/dp-500/image48.png"],"answer":"","discussion":[{"upvote_count":"1","content":"correct","comment_id":"1089164","poster":"MaryemSB","timestamp":"1701856320.0"},{"comment_id":"951489","comments":[{"upvote_count":"3","content":"Changed my mind, this parameter should replace first part of the URL","comment_id":"957203","timestamp":"1689833940.0","poster":"konitoki"}],"content":"I would consider populating LAST part of the query with parameter as we are going to use the same service root, and as per link service root is the first part of the query\nhttps://learn.microsoft.com/en-us/odata/concepts/url-components","upvote_count":"1","poster":"konitoki","timestamp":"1689335640.0"},{"upvote_count":"4","content":"Create\nGet data\nDuplicate","comment_id":"934274","timestamp":"1687773840.0","poster":"Eltooth"},{"comment_id":"850958","poster":"DarioReymago","content":"is ok, the last step (duplicate) is because it needed a 2do collection","timestamp":"1679831160.0","upvote_count":"2"},{"poster":"taza31","upvote_count":"1","content":"Correct","timestamp":"1674571200.0","comment_id":"786613"},{"content":"I don't see why the last step would be duplicating the query from the Advanced Editor so I would go with so I would go with changing the resource path as the last step","comment_id":"767450","comments":[{"upvote_count":"2","content":"You need two collections, that's why you need to duplicate the query and change the resource path in the URL to the second collection once you've duplicated the query. The answer is correct.","comment_id":"802927","timestamp":"1675927500.0","poster":"ThariCD"}],"timestamp":"1672996860.0","poster":"ivanb94","upvote_count":"4"},{"comment_id":"766417","timestamp":"1672909200.0","content":"Please confirm the answer","upvote_count":"1","poster":"AshwinN1992"}],"answer_ET":"","answer_images":["https://img.examtopics.com/dp-500/image49.png"],"answers_community":[]},{"id":"Fu07f6LZIKCvyvFk6cxh","answer_description":"","exam_id":70,"answer":"","question_text":"DRAG DROP -\nYou have an Azure Synapse Analytics serverless SQL pool.\nYou need to return a list of files and the number of rows in each file.\nHow should you complete the Transact-SQL statement? To answer, drag the appropriate values to the targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\n//IMG//","timestamp":"2022-12-13 15:57:00","answer_ET":"","answers_community":[],"unix_timestamp":1670943420,"url":"https://www.examtopics.com/discussions/microsoft/view/91443-exam-dp-500-topic-1-question-45-discussion/","isMC":false,"question_images":["https://img.examtopics.com/dp-500/image50.png"],"answer_images":["https://img.examtopics.com/dp-500/image51.png"],"question_id":124,"discussion":[{"comment_id":"792704","upvote_count":"10","poster":"bad_atitude","timestamp":"1675079580.0","content":"Count Big\nOpenRowSet"},{"upvote_count":"1","content":"COUNT_BIG\nOPENROWSET","poster":"MaryemSB","comment_id":"1089354","timestamp":"1701868200.0"},{"timestamp":"1692421860.0","comment_id":"984969","upvote_count":"2","content":"I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- COUNT_BIG\n- OPENROWSET","poster":"SamuComqi"},{"timestamp":"1687773900.0","upvote_count":"1","poster":"Eltooth","comment_id":"934276","content":"Count_Big\nOpenRowSet"},{"poster":"DarioReymago","content":"COUNT_BIG use *, \nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/count-big-transact-sql?view=sql-server-ver16\nand OpenRowSet","comment_id":"852637","upvote_count":"3","timestamp":"1679966640.0"},{"poster":"Madiba_kaka","comment_id":"783828","upvote_count":"3","comments":[{"comment_id":"1123909","timestamp":"1705385940.0","content":"If this was true, group by would not have been used.","upvote_count":"1","poster":"reemprive"}],"content":"Since parquet file can be very big, the APPROX_COUNT_DISTINCT makes sense because it's designed for use in big data scenarios. Also, APPROX_COUNT_DISTINCT requires less memory than an exhaustive COUNT DISTINCT operation. With very big file an approximate row count will suffice as opposed to an exact count. https://learn.microsoft.com/en-us/sql/t-sql/functions/approx-count-distinct-transact-sql?view=sql-server-ver16","timestamp":"1674347280.0"},{"timestamp":"1672861320.0","poster":"moreinva43","comments":[{"comments":[{"timestamp":"1673444400.0","upvote_count":"3","poster":"Ngol","content":"Your comment makes sense from the links provided","comment_id":"772584"}],"upvote_count":"8","poster":"ivanb94","timestamp":"1672997160.0","content":"Not only that but the count_big actually counts rows (check here https://learn.microsoft.com/en-us/sql/t-sql/functions/count-big-transact-sql?view=sql-server-ver16) which is what the Q asks us to do instead of counting non-null values that can repeat across multiple rows (check here https://learn.microsoft.com/en-us/sql/t-sql/functions/approx-count-distinct-transact-sql?view=sql-server-ver16) making approx_count_distinct the wrong answer. Not to mention the approximation element of it all that has no place in the given question.","comment_id":"767453"}],"upvote_count":"4","content":"I can find no example of using approx_count_distinct with an asterisk, while I know it can be used with count_big.","comment_id":"766030"},{"upvote_count":"3","content":"Correct","poster":"nbagchi","timestamp":"1670943420.0","comment_id":"744162"}],"topic":"1"},{"id":"Ght9V0DIi3AGQ0dFRRQR","question_text":"HOTSPOT -\nYou have an Azure Synapse Analytics serverless SQL pool and an Azure Data Lake Storage Gen2 account.\nYou need to query all the files in the ‘csv/taxi/’ folder and all its subfolders. All the files are in CSV format and have a header row.\nHow should you complete the query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n//IMG//","topic":"1","question_images":["https://img.examtopics.com/dp-500/image52.png"],"isMC":false,"answer_images":["https://img.examtopics.com/dp-500/image53.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/90933-exam-dp-500-topic-1-question-46-discussion/","unix_timestamp":1670681640,"exam_id":70,"question_id":125,"discussion":[{"content":"it says: query all folders and subfolders, hence /** .\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files#traverse-folders-recursively","poster":"Az301301X","comment_id":"743732","timestamp":"1670918400.0","upvote_count":"18"},{"upvote_count":"15","content":"Correct answer is\nBULK 'csv/taxi/**' : although 'csv/taxi/*.csv' also works (tested it), the file extension is not specified. It can be the case that the files are .txt files but still in CSV format.\nand FIRSTROW=2: you need to skip the headers (tested it). FIRSTROW=1 is the default value and will return the data including the headers.","poster":"eekman","comments":[{"timestamp":"1679967000.0","comment_id":"852644","upvote_count":"1","poster":"DarioReymago","content":"thats correct"}],"comment_id":"788507","timestamp":"1674720300.0"},{"upvote_count":"4","comment_id":"934278","timestamp":"1687774020.0","poster":"Eltooth","content":"BULK 'csv/taxi/**'\nFIRSTROW=2"},{"comment_id":"787171","comments":[{"comments":[{"upvote_count":"1","poster":"cookiemonster42","comment_id":"794499","content":"Sorry again, no HEADER = TRUE","timestamp":"1675189680.0"}],"comment_id":"794497","content":"Sorry pals, I'd like to change my opinion, there's no FIRST ROW = TRUE in the code, so the FIRSTROW = 2 in this particular case","upvote_count":"1","poster":"cookiemonster42","timestamp":"1675189620.0"}],"timestamp":"1674611340.0","poster":"cookiemonster42","content":"1) https://mydatalake.blob.core.windows.net/data/files/**: All files in the files folder, and recursively its subfolders.\n2) FIRSTROW=1 The FIRSTROW attribute isn't intended to skip column headers. Skipping headers isn't supported by the BULK INSERT statement. If you choose to skip rows, the SQL Server Database Engine looks only at the field terminators, and doesn't validate the data in the fields of skipped rows.\n\nhttps://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql?view=sql-server-ver16","upvote_count":"3"},{"comment_id":"762933","comments":[{"poster":"ivanb94","comment_id":"767461","timestamp":"1672997580.0","upvote_count":"3","content":"How can it be correct when the given solution does not have /** at the end? You probably meant to say that the solution with /** is correct. I agree, obviously."}],"poster":"MrXBasit","timestamp":"1672519440.0","upvote_count":"2","content":"Serverless SQL pool can recursively traverse folders if you specify /** at the end of path. The following query will read all files from all folders and subfolders located in the csv/taxi folder.\n\nThe answer is correct."},{"content":"The answer is correct:\nhttps://mydatalake.blob.core.windows.net/data/files/file*.csv: All .csv files in the files folder with names that start with \"file\".\n\nSource: https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files\n\nAnd it should be FIRSTROW = 2, because it has headers. \nIn the example (follow the link below), you can see that when the file has now header then FIRSTROW = 1.\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#read-specific-columns-from-csv-file\nIn Source","upvote_count":"2","comment_id":"761813","poster":"cherious","timestamp":"1672392780.0"},{"poster":"nbagchi","timestamp":"1670943480.0","comments":[{"comment_id":"746949","poster":"ABHI2023","upvote_count":"5","timestamp":"1671178440.0","content":"FIRSTROW=2 , as first row is header , Reference : https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files?ns-enrollment-type=learningpath&ns-enrollment-id=learn.wwl.model-query-explore-data-for-azure-synapse"},{"timestamp":"1671180720.0","comment_id":"746980","upvote_count":"8","poster":"Az301301X","content":"FIRSTROW=2, remember the \"headers\"."}],"content":"Correct answer is \nBULK 'csv/taxi/**' and FIRSTROW=1","upvote_count":"3","comment_id":"744164"},{"poster":"AT96","upvote_count":"1","timestamp":"1670681640.0","comment_id":"741035","content":"Correct answer is BULK 'adl://<your_adls_account_name>.dfs.core.windows.net/csv/taxi/',"}],"answer_description":"","answer_ET":"","answer":"","timestamp":"2022-12-10 15:14:00","answers_community":[]}],"exam":{"isImplemented":true,"isBeta":false,"isMCOnly":false,"id":70,"name":"DP-500","numberOfQuestions":183,"lastUpdated":"12 Apr 2025","provider":"Microsoft"},"currentPage":25},"__N_SSP":true}