{"pageProps":{"questions":[{"id":"VbuHoW0nkEKbfphBcx9y","isMC":true,"question_id":21,"answer_images":[],"exam_id":72,"question_images":[],"answers_community":["C (100%)"],"discussion":[{"content":"Selected Answer: C\nOptimise!","timestamp":"1740409860.0","comment_id":"1361072","poster":"benni_ale","upvote_count":"1"},{"content":"Selected Answer: C\nIn Delta Lake, the OPTIMIZE command is used to consolidate small Parquet files into larger ones. This improves query performance by reducing the overhead of managing many small files.\n\nVACUUM command is used to clean up and remove files that are no longer needed (e.g., old versions of data files, deleted files)","timestamp":"1739750160.0","comment_id":"1357488","poster":"henryphchan","upvote_count":"2"},{"content":"Selected Answer: C\nOptimize solves the \"small file problem\" by consolidating multiple small files into larger parquet files","upvote_count":"2","comment_id":"1352855","timestamp":"1738907760.0","poster":"prabhjot"},{"comment_id":"1349050","poster":"Tuki93","content":"Selected Answer: C\nOptimize: Consolidates multiple small Parquet files into large file. Big Data processing engines, and all Fabric engines, benefit from having larger files sizes. Having files of size above 128 MB, and optimally close to 1 GB, improves compression and data distribution, across the cluster nodes. It reduces the need to scan numerous small files for efficient read operations. It's a general best practice to run optimization strategies after loading large tables.","timestamp":"1738240140.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1346219","poster":"doctordodge","content":"Selected Answer: C\nLink: https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance","timestamp":"1737748920.0"},{"upvote_count":"4","timestamp":"1735577820.0","comment_id":"1334265","content":"Selected Answer: C\nTo consolidate the underlying Parquet files in Table1, you should run the OPTIMIZE command.","poster":"GHill1982"}],"choices":{"D":"CACHE","B":"BROADCAST","C":"OPTIMIZE","A":"VACUUM"},"unix_timestamp":1735577820,"topic":"1","answer_ET":"C","answer":"C","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/153656-exam-dp-700-topic-1-question-28-discussion/","question_text":"You have a Fabric workspace that contains a lakehouse and a notebook named Notebook1. Notebook1 reads data into a DataFrame from a table named Table1 and applies transformation logic. The data from the DataFrame is then written to a new Delta table named Table2 by using a merge operation.\nYou need to consolidate the underlying Parquet files in Table1.\nWhich command should you run?","timestamp":"2024-12-30 17:57:00"},{"id":"TzpIgapShtuQYUhEC0MG","answers_community":["G (100%)"],"question_images":[],"question_id":22,"topic":"1","answer_ET":"G","answer_description":"","isMC":true,"exam_id":72,"choices":{"G":"Location","B":"Capacity","E":"Item type","C":"Activity name","D":"Submitter","F":"Job type","A":"Start time"},"url":"https://www.examtopics.com/discussions/microsoft/view/153657-exam-dp-700-topic-1-question-29-discussion/","timestamp":"2024-12-30 17:57:00","question_text":"You have five Fabric workspaces.\nYou are monitoring the execution of items by using Monitoring hub.\nYou need to identify in which workspace a specific item runs.\nWhich column should you view in Monitoring hub?","answer":"G","answer_images":[],"unix_timestamp":1735577820,"discussion":[{"upvote_count":"6","content":"Selected Answer: G\nCorrect. The Location shows the Workspace.\n\nhttps://learn.microsoft.com/en-us/training/modules/monitor-fabric-items/3-use-monitor-hub","poster":"GHill1982","comment_id":"1334266","timestamp":"1735577820.0"},{"timestamp":"1743920700.0","poster":"Besmarlyn","comment_id":"1558176","upvote_count":"1","content":"Selected Answer: G\nAnswer is correct"},{"poster":"benni_ale","timestamp":"1740409980.0","content":"Selected Answer: G\nLocation","upvote_count":"1","comment_id":"1361073"},{"timestamp":"1739750280.0","content":"Selected Answer: G\nIn Monitoring Hub, Location = Workspace","upvote_count":"2","poster":"henryphchan","comment_id":"1357491"},{"comment_id":"1349076","poster":"Tuki93","content":"Selected Answer: G\nProvided answer \"Location\" is correct","timestamp":"1738243920.0","upvote_count":"1"}]},{"id":"axEfcdIFweFTI1HURHn1","answers_community":["B (100%)"],"question_id":23,"answer_images":[],"question_text":"You have a Fabric workspace that contains a warehouse named Warehouse1.\nYou have an on-premises Microsoft SQL Server database named Database1 that is accessed by using an on-premises data gateway.\nYou need to copy data from Database1 to Warehouse1.\nWhich item should you use?","url":"https://www.examtopics.com/discussions/microsoft/view/152696-exam-dp-700-topic-1-question-3-discussion/","answer":"B","isMC":true,"choices":{"C":"a KQL queryset","B":"a data pipeline","D":"a notebook","A":"a Dataflow Gen1 dataflow"},"question_images":[],"topic":"1","answer_description":"","unix_timestamp":1733671980,"timestamp":"2024-12-08 16:33:00","answer_ET":"B","exam_id":72,"discussion":[{"poster":"robertlavigne","content":"Selected Answer: B\nB. \nNotebooks can’t go through a gateway, KQL isn’t appropriate for a sql db. Gen1 dataflows are deprecated. Data pipelines work through a datagateway and are fast","comment_id":"1345082","upvote_count":"8","timestamp":"1737605400.0"},{"content":"Selected Answer: B\nIt says, you need to copy data, dont specify if we need to do some transformations. In this case, a data pipeline with an activity \"copy data\" is enough and the best choice in terms of efficency and minimal effort.\n\nCheck this link:\n\nhttps://learn.microsoft.com/en-us/fabric/fundamentals/decision-guide-pipeline-dataflow-spark","poster":"CMDev","comment_id":"1410350","upvote_count":"1","timestamp":"1742992800.0"},{"comments":[{"comment_id":"1399288","poster":"arunmewada","upvote_count":"2","timestamp":"1742142120.0","content":"answer is A"}],"content":"Selected Answer: B\nI attempted to use a **data pipeline** to load data directly from **on-premises** to the **warehouse**, but since the warehouse does not support external sources, I first loaded the data into a **Lakehouse** and then moved it to the **warehouse**.","upvote_count":"1","timestamp":"1742141760.0","poster":"arunmewada","comment_id":"1399286"},{"timestamp":"1737038340.0","content":"Selected Answer: B\nA data pipeline, since it has the \"Copy data\" activity, the most efficient way to copy data into fabric when there are no transformations to be done (if there are a Dataflow Gen 2 might be better, depends on the amount of data to be copied).","upvote_count":"2","poster":"mmanrik","comment_id":"1341727"},{"poster":"QAZdbarhate12345678","timestamp":"1734244920.0","content":"Selected Answer: B\n• In this case, a data pipeline can be used to transfer data from Database1 (the SQL Server database) to Warehouse1 (in the Fabric workspace).","comment_id":"1326727","upvote_count":"3"},{"upvote_count":"2","timestamp":"1734009840.0","content":"Selected Answer: B\nDataflow Gen 1 is deprecated.","comment_id":"1325625","poster":"Tuki93"}]},{"id":"NeXb28KjYsY0RnHl13Xi","question_text":"You have a Fabric workspace that contains a warehouse named DW1. DW1 is loaded by using a notebook named Notebook1.\nYou need to identify which version of Delta was used when Notebook1 was executed.\nWhat should you use?","answer":"D","question_images":[],"answer_description":"","question_id":24,"discussion":[{"content":"Selected Answer: D\nYou can see this in the Monitor. On the Details section for a Notebook there is Runtime information which gives the version i.e. Runtime 1.3 (Spark 3.5, Delta 3.2)","comment_id":"1334900","poster":"GHill1982","upvote_count":"10","timestamp":"1735663980.0"},{"timestamp":"1740410100.0","upvote_count":"1","poster":"benni_ale","comment_id":"1361075","content":"Selected Answer: D\nMonitor"},{"poster":"Tuki93","comment_id":"1349093","upvote_count":"1","content":"Selected Answer: D\nWe can find this in Monitor. example: Runtime information\nRuntime 1.2 (Spark 3.4, Delta 2.4)","timestamp":"1738245540.0"}],"answer_images":[],"answer_ET":"D","isMC":true,"unix_timestamp":1735663980,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/153719-exam-dp-700-topic-1-question-30-discussion/","topic":"1","exam_id":72,"timestamp":"2024-12-31 17:53:00","choices":{"D":"Fabric Monitor","A":"Real-Time hub","E":"the Microsoft Fabric Capacity Metrics app","B":"OneLake data hub","C":"the Admin monitoring workspace"}},{"id":"sVxX2VM6kG6sygIa6s43","discussion":[{"content":"Alter table dbo.dimcustomer\nAdd constraint PK_dimcustomer primary key nonclustered (customerkey) \nnot enforced \n\nPRIMARY KEY is only supported when NONCLUSTERED and NOT ENFORCED are both used.\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/table-constraints","timestamp":"1737300000.0","poster":"amli123","upvote_count":"19","comments":[{"comment_id":"1359697","upvote_count":"1","timestamp":"1740128460.0","content":"ALTER TABLE dbo.DimCustomer\nADD CONSTRAINT PK_DimCustomer PRIMARY KEY CLUSTERED (CustomerKey)\nENFORCED","poster":"ChenFu"}],"comment_id":"1343057"},{"upvote_count":"3","timestamp":"1740574260.0","poster":"Othman2110","comment_id":"1361917","content":"Primary Key\nNon Clustered\nNot Enforced"},{"poster":"fassil","upvote_count":"2","comment_id":"1361529","timestamp":"1740507960.0","content":"Alter table dbo.dimcustomer\nAdd constraint PK_dimcustomer primary key nonclustered (customerkey)\nnot enforced\n\nCreate a Microsoft Fabric Warehouse table with a primary key:\nCREATE TABLE PrimaryKeyTable (c1 INT NOT NULL, c2 INT);\nALTER TABLE PrimaryKeyTable ADD CONSTRAINT PK_PrimaryKeyTable PRIMARY KEY NONCLUSTERED (c1) NOT ENFORCED;"},{"upvote_count":"4","poster":"Tuki93","comment_id":"1349099","timestamp":"1738246260.0","content":"Provided answer is WRONG.\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/table-constraints\nSQL analytics endpoint and Warehouse in Microsoft Fabric support these table constraints:\n\nPRIMARY KEY is only supported when NONCLUSTERED and NOT ENFORCED are both used.\n\nFOREIGN KEY is only supported when NOT ENFORCED is used.\n\nUNIQUE constraint is only supported when NONCLUSTERED and NOT ENFORCED are both used.\n\nSQL analytics endpoint and Warehouse don't support default constraints at this time."}],"answer_images":["https://img.examtopics.com/dp-700/image24.png"],"unix_timestamp":1737300000,"question_id":25,"answers_community":[],"answer_description":"","timestamp":"2025-01-19 16:20:00","question_images":["https://img.examtopics.com/dp-700/image22.png","https://img.examtopics.com/dp-700/image23.png"],"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/154903-exam-dp-700-topic-1-question-31-discussion/","answer":"","isMC":false,"question_text":"DRAG DROP -\nYou have a Fabric workspace that contains a warehouse named Warehouse1.\nIn Warehouse1, you create a table named DimCustomer by running the following statement.\n//IMG//\n\nYou need to set the Customerkey column as a primary key of the DimCustomer table.\nWhich three code segments should you run in sequence? To answer, move the appropriate code segments from the list of code segments to the answer area and arrange them in the correct order.\n//IMG//","exam_id":72,"answer_ET":""}],"exam":{"isBeta":false,"id":72,"provider":"Microsoft","isImplemented":true,"numberOfQuestions":97,"name":"DP-700","isMCOnly":false,"lastUpdated":"12 Apr 2025"},"currentPage":5},"__N_SSP":true}