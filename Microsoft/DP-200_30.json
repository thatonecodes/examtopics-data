{"pageProps":{"questions":[{"id":"jfY5KQnOFXfvulMVkXDy","url":"https://www.examtopics.com/discussions/microsoft/view/26009-exam-dp-200-topic-3-question-14-discussion/","answer_ET":"B","unix_timestamp":1595031540,"discussion":[{"content":"Random number\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview","comment_id":"140183","timestamp":"1595324160.0","upvote_count":"19","poster":"Shiven"},{"comment_id":"337371","timestamp":"1618638840.0","upvote_count":"6","content":"Since is asked what masking \"format\" to select (not masking function), number is right.","poster":"Nevia"},{"poster":"syu31svc","content":"It's random number to be exact but let's just go with B","timestamp":"1606394760.0","comment_id":"228317","upvote_count":"3"},{"timestamp":"1601141700.0","comment_id":"187828","poster":"hart232","content":"There is no masking option called Number. We only have random number. \nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"273983","content":"that's why it is confusing","poster":"BRW","timestamp":"1611345300.0"}]},{"upvote_count":"3","comment_id":"146449","content":"Actually options are incorrect. There should be masking name as Random, email, default etc.","poster":"avix","timestamp":"1596021660.0","comments":[{"content":"Number is Correct ...goto https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-configure-portal","poster":"kilowd","comment_id":"152312","timestamp":"1596773640.0","upvote_count":"5"}]},{"comments":[{"timestamp":"1595303160.0","upvote_count":"5","comment_id":"140015","content":"Default will change it to 0","poster":"singhadi003"},{"comment_id":"140290","upvote_count":"3","timestamp":"1595333040.0","poster":"VMLearn","content":"default gives you static value not random one"}],"content":"C = default","comment_id":"137528","upvote_count":"2","poster":"cmihai","timestamp":"1595031540.0"}],"exam_id":65,"answers_community":[],"answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-dynamic-data-masking-get-started-portal","question_text":"Your company manages a payroll application for its customers worldwide. The application uses an Azure SQL database named DB1. The database contains a table named Employee and an identity column named EmployeeId.\nA customer requests the EmployeeId be treated as sensitive data.\nWhenever a user queries EmployeeId, you need to return a random value between 1 and 10 instead of the EmployeeId value.\nWhich masking format should you use?","choices":{"A":"string","C":"default","B":"number"},"isMC":true,"question_images":[],"timestamp":"2020-07-18 02:19:00","question_id":146,"answer":"B","answer_images":[],"topic":"3"},{"id":"znLyyI7IKaubLy2jZUta","url":"https://www.examtopics.com/discussions/microsoft/view/49811-exam-dp-200-topic-3-question-16-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0032100001.png"],"question_text":"DRAG DROP -\nYou have an Azure data factory.\nYou need to ensure that pipeline-run data is retained for 120 days. The solution must ensure that you can query the data by using the Kusto query language.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\nSelect and Place:\n//IMG//","answer":"","answer_description":"Step 1: Create an Azure Storage account that has a lifecycle policy\nTo automate common data management tasks, Microsoft created a solution based on Azure Data Factory. The service, Data Lifecycle Management, makes frequently accessed data available and archives or purges other data according to retention policies. Teams across the company use the service to reduce storage costs, improve app performance, and comply with data retention policies.\nStep 2: Create a Log Analytics workspace that has Data Retention set to 120 days.\nData Factory stores pipeline-run data for only 45 days. Use Azure Monitor if you want to keep that data for a longer time. With Monitor, you can route diagnostic logs for analysis to multiple different targets, such as a Storage Account: Save your diagnostic logs to a storage account for auditing or manual inspection. You can use the diagnostic settings to specify the retention time in days.\nStep 3: From Azure Portal, add a diagnostic setting.\nStep 4: Send the data to a log Analytics workspace,\nEvent Hub: A pipeline that transfers events from services to Azure Data Explorer.\nKeeping Azure Data Factory metrics and pipeline-run data.\nConfigure diagnostic settings and workspace.\nCreate or add diagnostic settings for your data factory.\n1. In the portal, go to Monitor. Select Settings > Diagnostic settings.\n2. Select the data factory for which you want to set a diagnostic setting.\n3. If no settings exist on the selected data factory, you're prompted to create a setting. Select Turn on diagnostics.\n4. Give your setting a name, select Send to Log Analytics, and then select a workspace from Log Analytics Workspace.\n5. Select Save.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor","question_id":147,"isMC":false,"answers_community":[],"answer_ET":"","timestamp":"2021-04-10 16:23:00","unix_timestamp":1618064580,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0032200001.png"],"discussion":[{"comments":[{"poster":"Maddaa","content":"Agree!","comment_id":"344166","timestamp":"1619548500.0","upvote_count":"2"},{"content":"100% agree","comment_id":"333404","upvote_count":"5","timestamp":"1618155540.0","poster":"Kenai"},{"upvote_count":"2","poster":"cadio30","content":"Support this solution","comment_id":"351708","timestamp":"1620368100.0"}],"comment_id":"332610","upvote_count":"42","poster":"MsIrene","content":"Storage account is not needed here. \n\nI would say the order would be like that: \nStep 1: Create a Log Analytics workspace that has Data Retention set to 120 days.\nStep 2: From Azure Portal, add a diagnostic setting.\nStep 3: Select the PipelineRuns Category\nStep 4: Send the data to a Log Analytics workspace.","timestamp":"1618064580.0"},{"timestamp":"1619478540.0","poster":"Wendy_DK","upvote_count":"5","comment_id":"343623","content":"Step 1: Create a Log Analytics workspace that has Data Retention set to 120 days.\nStep 2: From Azure Portal, add a diagnostic setting.\nStep 3: Select the PipelineRuns Category\nStep 4: Send the data to a Log Analytics workspace."},{"poster":"mric","content":"According to the linked article, it's: first Storage Account, then Event Hub, and finally Log Analytics.\nSo I would say:\n1- Create an Azure Storage Account with a lifecycle policy\n2- Stream to an Azure Event Hub\n3- Create a Log Analytics workspace that has a Data Retention set to 120 days\n4- Send the data to a Log Analytics Workspace\nSource: https://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#keeping-azure-data-factory-metrics-and-pipeline-run-data","upvote_count":"1","timestamp":"1624605060.0","comment_id":"390206"},{"poster":"Wendy_DK","comment_id":"339644","upvote_count":"1","content":"I agree.\nStep 1: Create a Log Analytics workspace that has Data Retention set to 120 days.\nStep 2: From Azure Portal, add a diagnostic setting.\nStep 3: Select the PipelineRuns Category\nStep 4: Send the data to a Log Analytics workspace.","timestamp":"1618929540.0"}],"exam_id":65,"topic":"3"},{"id":"OonN7OxwWU7sZmxzCVgr","discussion":[{"comments":[{"comment_id":"355193","upvote_count":"1","timestamp":"1620788460.0","content":"Reference: https://dataedo.com/kb/tools/ssms/gdpr-in-sql-server-and-azure-sql-database","poster":"cadio30"}],"content":"Azure SQL Server > Azure SQLDB > Security > Data Discovery & Classifications > Classification Tab > Add Classification","upvote_count":"3","poster":"cadio30","timestamp":"1620260820.0","comment_id":"350632"}],"isMC":false,"topic":"3","exam_id":65,"question_id":148,"question_text":"SIMULATION -\n//IMG//\n\nUse the following login credentials as needed:\n\nAzure Username: xxxxx -\n\nAzure Password: xxxxx -\nThe following information is for technical support purposes only:\n\nLab Instance: 10277521 -\nYou need to classify the following information as Confidential:\n✑ Database: db3\n✑ Schema: SalesLT\n✑ Table: Customer\n\nColumn: Phone Information -\n//IMG//\n\n✑ Type: Contact Info\nTo complete this task, sign in to the Azure portal.","answer_ET":"See the explanation below.","unix_timestamp":1620260820,"answers_community":[],"answer_description":"1. In Azure Portal, locate and select database db3.\n2. Select Security and Advance Data Security, and Click Enable advanced Data Security Protection\n\n3. Click the Data Discovery & Classification card.\n4. Click on Add classification in the top menu of the window.\n\n5. In the context window that opens, select the schema > table > column that you want to classify, and the information type and sensitivity label. Then click on the blue Add classification button at the bottom of the context window.\n\nSelect/enter the following -\n✑ Schema: SalesLT\n✑ Table: Customer\n✑ Column: Phone Information\n\nInformation type: Contact Info -\n\n\n6. To complete your classification and persistently label (tag) the database columns with the new classification metadata, click on Save in the top menu of the window.\n\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-data-discovery-and-classification","answer":"See the explanation below.","timestamp":"2021-05-06 02:27:00","url":"https://www.examtopics.com/discussions/microsoft/view/51940-exam-dp-200-topic-3-question-17-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0032400001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0032400005.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0032500002.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0032600001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0032600005.png","https://www.examtopics.com/assets/media/exam-media/03872/0032700001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0032700002.jpg"]},{"id":"nkMfx3tusb36r0osyF6h","question_images":[],"discussion":[{"comment_id":"361158","timestamp":"1621411200.0","poster":"memo43","upvote_count":"4","content":"answer is CORRECT\nAlways Encryption --->>Protects data from SQL administrators and admins\n--->>Data is encrypted/decrypted on the client side\n\nit is not mentioned in this question"}],"timestamp":"2021-05-19 10:00:00","answer":"A","isMC":true,"choices":{"C":"Always Encrypted for all columns","D":"Advanced Data Security for this database","B":"Secure transfer required","A":"Transparent Data Encryption (TDE)"},"answer_images":[],"answer_description":"Azure SQL Database currently supports encryption at rest for Microsoft-managed service side and client-side encryption scenarios.\n✑ Support for server encryption is currently provided through the SQL feature called Transparent Data Encryption.\n✑ Client-side encryption of Azure SQL Database data is supported through the Always Encrypted feature.\nReference:\nhttps://docs.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest","unix_timestamp":1621411200,"exam_id":65,"question_id":149,"answers_community":[],"topic":"3","question_text":"You have a data warehouse in Azure Synapse Analytics.\nYou need to ensure that the data in the data warehouse is encrypted at rest.\nWhat should you enable?","url":"https://www.examtopics.com/discussions/microsoft/view/53096-exam-dp-200-topic-3-question-18-discussion/","answer_ET":"A"},{"id":"EySn2hQ4EQm0mmmYyls2","unix_timestamp":1623544380,"answers_community":[],"exam_id":65,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0032900001.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0032900002.jpg"],"topic":"3","question_id":150,"isMC":false,"timestamp":"2021-06-13 02:33:00","url":"https://www.examtopics.com/discussions/microsoft/view/55208-exam-dp-200-topic-3-question-19-discussion/","question_text":"DRAG DROP -\nYou manage the Microsoft Azure Databricks environment for a company. You must be able to access a private Azure Blob Storage account. Data must be available to all Azure Databricks workspaces. You need to provide the data access.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","answer_description":"Step 1: Create a secret scope -\nStep 2: Add secrets to the scope\nNote: dbutils.secrets.get(scope = \"<scope-name>\", key = \"<key-name>\") gets the key that has been stored as a secret in a secret scope.\nStep 3: Mount the Azure Blob Storage container\nYou can mount a Blob Storage container or a folder inside a container through Databricks File System - DBFS. The mount is a pointer to a Blob Storage container, so the data is never synced locally.\nNote: To mount a Blob Storage container or a folder inside a container, use the following command:\n\nPython -\ndbutils.fs.mount(\nsource = \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net\", mount_point = \"/mnt/<mount-name>\", extra_configs = {\"<conf-key>\":dbutils.secrets.get(scope = \"<scope-name>\", key = \"<key-name>\")}) where: dbutils.secrets.get(scope = \"<scope-name>\", key = \"<key-name>\") gets the key that has been stored as a secret in a secret scope.\nReferences:\nhttps://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html","answer_ET":"","answer":"","discussion":[{"timestamp":"1624440060.0","comment_id":"388650","content":"I think it should be:\nUSE BLOB STORAGE KEY\nCREATE A SECRET SCOPE\nMOUNT THE AZURE BLOB STORAGE CONTAINER","upvote_count":"1","comments":[{"poster":"Rosh4yuh","content":"It should be a storage account key. given answer is correct","comment_id":"415621","timestamp":"1627405920.0","upvote_count":"1"}],"poster":"emski"},{"poster":"hoangton","content":"It is correct answer! \nMount storage account at the last step\nhttps://docs.microsoft.com/en-us/azure/databricks/scenarios/store-secrets-azure-key-vault","timestamp":"1623544380.0","comment_id":"380744","upvote_count":"1"}]}],"exam":{"isBeta":false,"numberOfQuestions":228,"isMCOnly":false,"lastUpdated":"12 Apr 2025","provider":"Microsoft","name":"DP-200","isImplemented":true,"id":65},"currentPage":30},"__N_SSP":true}