{"pageProps":{"questions":[{"id":"zE9AcSVzxTClSKHgNs1t","answer_images":[],"answers_community":["A (70%)","B (17%)","13%"],"choices":{"C":"storage event","D":"custom event","A":"schedule","B":"tumbling"},"question_id":111,"unix_timestamp":1691405340,"answer_ET":"A","exam_id":67,"isMC":true,"topic":"1","timestamp":"2023-08-07 12:49:00","question_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/microsoft/view/117526-exam-dp-203-topic-1-question-93-discussion/","question_text":"You have an Azure Data Factory pipeline named pipeline1.\n\nYou need to execute pipeline1 at 2 AM every day. The solution must ensure that if the trigger for pipeline1 stops, the next pipeline execution will occur at 2 AM, following a restart of the trigger.\n\nWhich type of trigger should you create?","answer_description":"","discussion":[{"comments":[{"upvote_count":"4","poster":"p_mks","timestamp":"1693833900.0","comment_id":"998584","content":"I don't see 'Retry' as a requirement, they mentioned only about the next pipeline execution. 'A' seems to be more appropriate."}],"upvote_count":"10","comment_id":"989793","content":"Answer is Tumbling : \nLink : https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison\n\"Retry capability Supported. Failed pipeline runs have a default retry policy of 0, or a policy that's specified by the user in the trigger definition. Automatically retries when the pipeline runs fail due to concurrency/server/throttling limits (that is, status codes 400: User Error, 429: Too many requests, and 500: Internal Server error)\"\n\nRetry capability is not supported on Schedule trigger","timestamp":"1692949560.0","poster":"yassine70"},{"comment_id":"1321326","upvote_count":"1","content":"Selected Answer: B\nTumbling because \"following a restart of the trigger.\"\nSchedule does not support restart capability\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison","timestamp":"1733220780.0","poster":"17lan"},{"poster":"EmnCours","content":"Selected Answer: B\nB. tumbling","upvote_count":"1","timestamp":"1732920540.0","comment_id":"1319957"},{"comment_id":"1299683","upvote_count":"1","timestamp":"1729256700.0","content":"Selected Answer: B\nA tumbling window trigger runs at a specified interval, and if the trigger stops, it maintains its schedule when restarted. This ensures the pipeline executes exactly at 2 AM daily, even after a trigger restart. Tumbling windows are useful for consistent execution without overlap or gaps, which makes them ideal for daily fixed scheduling like in this scenario.","poster":"Harteko"},{"content":"A tumbling window trigger runs at a specified interval, and if the trigger stops, it maintains its schedule when restarted. This ensures the pipeline executes exactly at 2 AM daily, even after a trigger restart. Tumbling windows are useful for consistent execution without overlap or gaps, which makes them ideal for daily fixed scheduling like in your scenario.","comment_id":"1299682","upvote_count":"1","timestamp":"1729256640.0","poster":"Harteko"},{"poster":"KarlGardnerDataEngineering","upvote_count":"2","content":"This question is not clear enough. \"If a pipeline stops\" can mean many different things.","comment_id":"1214632","timestamp":"1716249660.0"},{"poster":"Dusica","timestamp":"1713961200.0","comment_id":"1201341","content":"Schedule at 1 , repeat once, delay 60 minutes","upvote_count":"1"},{"upvote_count":"1","comment_id":"1186567","timestamp":"1711857420.0","content":"Selected Answer: A\nI'll go for A based on the use cases provided Microsoft Data Platform MVP:\nhttps://www.cathrinewilhelmsen.net/triggers-azure-data-factory/","poster":"lcss27"},{"comment_id":"1152180","timestamp":"1708109640.0","content":"Selected Answer: B\nYou set up a tumbling window trigger with a window size of 1 hour and a start time of 2 AM.\nThe trigger fires automatically at 2 AM every hour.\nIf the trigger encounters an outage and misses a window (e.g., cannot execute at 2 AM), it automatically resumes at the beginning of the next window (i.e., at 3 AM).\nThis ensures that the pipeline executes at 2 AM after the restart, effectively catching up for the missed execution.","poster":"Azure_2023","upvote_count":"1"},{"poster":"Khadija10","content":"Selected Answer: A\nin this scenario, the requirement is to execute the pipeline at a specific time every day (2 AM) and ensure that if the trigger stops, the next pipeline execution occurs at 2 AM following a restart of the trigger. A tumbling window trigger might not guarantee that the pipeline always runs at exactly 2 AM daily, especially if the trigger stops and restarts at different times.\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#tumbling-window-trigger\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-trigger-with-json","comment_id":"1141952","upvote_count":"1","timestamp":"1707208620.0"},{"content":"The answer is B\nTumbling window trigger is more specific .The pipeline is trigger every 24hrs which is how a tumbling window funtion works.\nScheduler is general .\nOne can say that a schedule trigger is called Tumbling trigger when it works at non overlapping equal intervals of time just as in the pipeline case study","upvote_count":"1","comment_id":"1138937","poster":"moneytime","timestamp":"1706924220.0"},{"content":"A\nTumbling widow trigger is more specific. \nScheduler is general .\nI can say this Schedule trigger is called Tumbling trigger..","timestamp":"1706923800.0","comment_id":"1138934","upvote_count":"1","poster":"moneytime"},{"poster":"jongert","upvote_count":"3","comment_id":"1106710","content":"Selected Answer: A\nThe tumbling window trigger run waits for the triggered pipeline run to finish. Its run state reflects the state of the triggered pipeline run. For example, if a triggered pipeline run is cancelled, the corresponding tumbling window trigger run is marked cancelled. This is different from the \"fire and forget\" behavior of the schedule trigger, which is marked successful as long as a pipeline run started.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison","timestamp":"1703672160.0"},{"poster":"mav2000","upvote_count":"1","content":"I believe it's tumbling, It doesn't say that if the trigger fails from the get-go, but if it stops, and in schedule, the state of a pipe is successful if the pipeline ran at first\n\n\"The tumbling window trigger run waits for the triggered pipeline run to finish. Its run state reflects the state of the triggered pipeline run. For example, if a triggered pipeline run is cancelled, the corresponding tumbling window trigger run is marked cancelled. This is different from the \"fire and forget\" behavior of the schedule trigger, which is marked successful as long as a pipeline run started.\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison","comment_id":"1099701","timestamp":"1702903560.0"},{"content":"Selected Answer: D\nLooks to me like Tumbling","comment_id":"1067365","upvote_count":"3","poster":"jsdsadfsad","timestamp":"1699632300.0"},{"comment_id":"1028860","upvote_count":"4","poster":"madhubanti123","content":"Selected Answer: A\nSchedule triggers allow you to schedule pipelines to run at specific times and intervals. They are also idempotent, which means that if a pipeline execution fails due to a trigger failure, the next pipeline execution will still occur at the scheduled time.","timestamp":"1696852560.0"},{"comment_id":"1025014","content":"\"following a restart of the trigger\" sounds a lot like a \"retry\" and only tumbling would offer a retry not schedule.","upvote_count":"1","timestamp":"1696439160.0","poster":"Vanq69"},{"poster":"ruggerofreddi","comment_id":"1015146","content":"I would say tumbling with a max concurrency = 1. in this way if the first run doesn't stop the second one will not start","upvote_count":"4","timestamp":"1695490860.0"},{"upvote_count":"2","comment_id":"1002328","timestamp":"1694166540.0","poster":"[Removed]","content":"Selected Answer: A\nSchedule"},{"timestamp":"1693906080.0","upvote_count":"1","content":"Tumbling","comment_id":"999302","poster":"hassexat"},{"content":"Selected Answer: A\nA is correct","poster":"kkk5566","comment_id":"998420","upvote_count":"1","timestamp":"1693819680.0"},{"comment_id":"980913","upvote_count":"3","content":"Tumbling","poster":"Biswada","timestamp":"1692027720.0"},{"timestamp":"1691949120.0","content":"Sounds like a tumbling trigger with self-dependency: \"if the trigger for pipeline1 stops, the next pipeline execution will occur at 2 AM, following a restart of the trigger\", implicating that the trigger does not start if the previous run of the trigger is not yet completed.\n\nBy using a tumbling trigger with self-dependency, one can let a tigger only start if a previous run of the same trigger has completed. To achieve that maxConcurrency has to be set to '1'. \n\nRef: https://learn.microsoft.com/en-us/azure/data-factory/tumbling-window-trigger-dependency","comment_id":"980177","upvote_count":"2","poster":"Matt2000"},{"upvote_count":"4","comment_id":"974600","timestamp":"1691405340.0","content":"Selected Answer: A\nSchedule","poster":"akhil5432"}]},{"id":"4Qm1RdcXaLin5Xjl0CK9","url":"https://www.examtopics.com/discussions/microsoft/view/117659-exam-dp-203-topic-1-question-94-discussion/","question_images":["https://img.examtopics.com/dp-203/image332.png","https://img.examtopics.com/dp-203/image333.png"],"answer_ET":"","answer":"","topic":"1","answers_community":[],"question_text":"HOTSPOT\n-\n\nYou have an Azure data factory named adf1 that contains a pipeline named ExecProduct. ExecProduct contains a data flow named Product.\n\nThe Product data flow contains the following transformations:\n\n1. WeeklyData: A source that points to a CSV file in an Azure Data Lake Storage Gen2 account with 20 columns\n2. ProductColumns: A select transformation that selects from WeeklyData six columns named ProductID, ProductDescr, ProductSubCategory, ProductCategory, ProductStatus, and ProductLastUpdated\n3. ProductRows: An aggregate transformation\n4. ProductList: A sink that outputs data to an Azure Synapse Analytics dedicated SQL pool\n\nThe Aggregate settings for ProductRows are configured as shown in the following exhibit.\n\n//IMG//\n\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","discussion":[{"comment_id":"978184","timestamp":"1691717580.0","upvote_count":"26","poster":"[Removed]","content":"Yes , no, yes - https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate"},{"comment_id":"976546","poster":"[Removed]","timestamp":"1691579700.0","upvote_count":"5","content":"How can one know that the answer to the third question is either yes or no? In my understanding, you'd have to assume that there is no duplicates in the source table, i.e. there are no rows that share the same values in all columns except for productid"},{"upvote_count":"1","poster":"aeab260","timestamp":"1739590140.0","content":"Y,N,Y. Almost exactly same solution on Microsoft https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate#removing-duplicate-rows","comment_id":"1356705"},{"content":"No, No, Yes.","upvote_count":"2","comment_id":"1254780","poster":"Aurangzaib","timestamp":"1721890200.0"},{"timestamp":"1714553940.0","content":"Yes, No, Yes","poster":"Dusica","comment_id":"1204963","upvote_count":"1"},{"upvote_count":"2","content":"Yes, no, yes. You can take a look on https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate","comment_id":"1183181","poster":"Alongi","timestamp":"1711446780.0"},{"comments":[{"poster":"lola_mary5","content":"I also think it's yes, yes, yes. We're grouping by ProductID and select the first/max ProductDescr per ProductID, like so:\nselect ProductID, first/max(ProductDescr) as FirstProductDescr\nfrom ProductColumns\ngroup by ProductID","timestamp":"1708172520.0","comment_id":"1152535","upvote_count":"2"}],"timestamp":"1708069020.0","content":"How come we might have multiple product description for unique product id if we group by product id ? It must be yes yes yes","upvote_count":"4","poster":"mariano2","comment_id":"1151799"},{"poster":"Bharatkumarc","comment_id":"1147017","upvote_count":"1","timestamp":"1707633480.0","content":"What if product id is duplicate i.e. for different combinations of the other columns we have the same product ID.. in that case, we will have multiple records for same product id even after aggregation and so answer to third Q would be No"},{"content":"Yes / No / Yes","poster":"Charley92","comment_id":"1138972","upvote_count":"1","timestamp":"1706928000.0"},{"comment_id":"1135793","timestamp":"1706619300.0","poster":"Anto____","upvote_count":"2","content":"YES. NO. YES"},{"content":"No - Input Schema is same as output schema, since there is no derived column to hold the aggregate value ( Function \"first\" -returns the fist row among duplicate grouping key ie thge first unique row Product Id in this example ). Input Schema will be same as Output Schema as the aggregator in this case is only used for removing duplicate rows by Product Id\nNo - Product Description is not a group by column, hence there is no guarantee it will be unique.\nYes - Product Id is the grouping Key hence the out will only have 1 unique row per Product Id","comments":[{"timestamp":"1713962100.0","comments":[{"upvote_count":"1","comment_id":"1204676","timestamp":"1714495800.0","content":"What about the column qualifier !=ProductID ? I think that makes it 5 in the return","poster":"7f668ce"}],"poster":"Dusica","content":"Your explanation is correct for the first but you misscounted the columns - there are 6; Y N Y","comment_id":"1201361","upvote_count":"1"}],"poster":"be8a152","comment_id":"1129050","timestamp":"1705957920.0","upvote_count":"2"},{"poster":"jongert","upvote_count":"1","comment_id":"1106718","content":"1 - Yes, group by productId and then column pattern match the others results in 1 + 5 = 6 columns\n2 - No, for aggregation using a group by clause we get one row for each unique value that we group by.\n3 - Yes, the opposite compared to (2), since we are actually grouping by productId now.","timestamp":"1703673120.0"},{"upvote_count":"2","content":"1- Yes \n2- No \n3- Yes \nwe are not creating new column, using aggregate function for deduplication\nhttps://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate","timestamp":"1701941100.0","poster":"hcq31818","comment_id":"1090111"},{"comment_id":"1081690","upvote_count":"1","content":"The syntax mentions name!= which means \"not equal to\" ProductID, so my answer would be: yes, yes, no","timestamp":"1701098100.0","poster":"Tincox","comments":[{"timestamp":"1702813740.0","content":"Upon second glance my assumption was incorrect; the \"first\" pattern applies to all rows who's name is not ProductID. ProductID itself is the grouping column, so will return one output row for each unique value. ProductDescr., however, is part of the other columns so here it's the combination of these columns that has to be unique (first row of this combination of values is returned, the rest is dropped). The ProductDescr. column itself can generate more rows per unique value. So answer should be Yes, No, Yes.","comment_id":"1098857","upvote_count":"2","poster":"Tincox"}]},{"content":"1. No There will be 7 in output (6 are at input)\n2. No We haven't tested anything by ProductDescr\n3. Yes We have grouped by ProductID and added new column","poster":"OldSchool","comments":[{"poster":"Vanq69","upvote_count":"3","content":"Did we add a new column tho? $$ -> first($$) would just remove duplicate rows in ProductID and only keep the first value that is encountered. So it should be yes, no, yes.\nhttps://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate","timestamp":"1696440600.0","comment_id":"1025031"}],"comment_id":"1014027","upvote_count":"4","timestamp":"1695381120.0"},{"upvote_count":"4","content":"yes, no, yes","timestamp":"1693819740.0","poster":"kkk5566","comment_id":"998422"},{"comment_id":"990561","upvote_count":"3","content":"Yes , no, yes","timestamp":"1693030740.0","poster":"kkk5566"},{"upvote_count":"4","poster":"mmoayed","comment_id":"984363","content":"I have notice that some answers might be wrong. What does this mean? who is confirming the correct answers ?","timestamp":"1692353580.0"},{"upvote_count":"2","poster":"mmoayed","content":"yes, no, yes","timestamp":"1692353460.0","comment_id":"984362"},{"poster":"MSExpert","content":"Yes No Yes","upvote_count":"2","timestamp":"1692136620.0","comment_id":"982015"},{"timestamp":"1692085740.0","poster":"DataEngDP","comment_id":"981399","upvote_count":"3","content":"yes, no, yes"},{"comments":[{"comments":[{"poster":"prshntdxt7","timestamp":"1706427180.0","comment_id":"1133901","upvote_count":"1","content":"that explains....thanks.\nthere would be 6 columns only."}],"content":"The irs no new aggregated measurement. You're grouping by ProductID, so that makes one column. All the other columns that are not 'ProductID' are grouped by choosing their first row. So you will obtain the same number of columns, like a SELECT DISTINCT.","upvote_count":"7","timestamp":"1692246660.0","comment_id":"983216","poster":"Elxaxe"}],"timestamp":"1691809920.0","upvote_count":"1","comment_id":"979087","content":"6 columns from product + the aggregated measurement = 7 columsn in total, so answer is NO?","poster":"ClydeZ"}],"timestamp":"2023-08-09 13:15:00","question_id":112,"answer_description":"","isMC":false,"answer_images":["https://img.examtopics.com/dp-203/image334.png"],"exam_id":67,"unix_timestamp":1691579700},{"id":"cgESyfEns4iZyEonnusr","answer":"B","isMC":true,"answers_community":["B (100%)"],"unix_timestamp":1691623800,"url":"https://www.examtopics.com/discussions/microsoft/view/117702-exam-dp-203-topic-1-question-95-discussion/","exam_id":67,"timestamp":"2023-08-10 01:30:00","answer_ET":"B","answer_images":[],"question_text":"You manage an enterprise data warehouse in Azure Synapse Analytics.\n\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\n\nYou need to monitor resource utilization to determine the source of the performance issues.\n\nWhich metric should you monitor?","topic":"1","question_id":113,"discussion":[{"content":"Correct","timestamp":"1707528600.0","upvote_count":"6","poster":"MSExpert","comment_id":"977093"},{"content":"Selected Answer: B\nGot this question on my exam on january 17, answer B is correct.","poster":"ELJORDAN23","upvote_count":"1","comment_id":"1126051","timestamp":"1721313120.0"},{"timestamp":"1713305280.0","poster":"ndangalasi","upvote_count":"3","comment_id":"1045314","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache"},{"content":"B. Cache hit percentage should be correct since it only affects common used queries, which should be saved and loaded from cache.","comment_id":"1025035","poster":"Vanq69","timestamp":"1712252100.0","upvote_count":"3"},{"upvote_count":"2","timestamp":"1709551800.0","poster":"kkk5566","content":"Selected Answer: B\nB is correct","comment_id":"998423"}],"answer_description":"","choices":{"D":"Data IO percentage","A":"DWU limit","B":"Cache hit percentage","C":"Local tempdb percentage"},"question_images":[]},{"id":"Pw1hQ81FnrCo3u7yk9Wh","topic":"1","timestamp":"2023-08-02 14:25:00","answer_ET":"","discussion":[{"comment_id":"970147","timestamp":"1722601500.0","content":"correct!\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#data-source","upvote_count":"15","poster":"g2000"},{"timestamp":"1725442260.0","comment_id":"998425","content":"SELECT *\nFROM OPENROWSET(BULK 'http://<storage account>.dfs.core.windows.net/container/folder/*.parquet',\n FORMAT = 'PARQUET') AS [file]","upvote_count":"4","poster":"kkk5566"},{"poster":"pramod4lk","timestamp":"1722757560.0","upvote_count":"4","comment_id":"971805","content":"Correct, Serverless SQL pool uses BULK."}],"question_text":"HOTSPOT\n-\n\nYou have an Azure Synapse Analytics serverless SQL pool.\n\nYou have an Apache Parquet file that contains 10 columns.\n\nYou need to query data from the file. The solution must return only two columns.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","question_id":114,"answer_images":["https://img.examtopics.com/dp-203/image336.png"],"unix_timestamp":1690979100,"url":"https://www.examtopics.com/discussions/microsoft/view/117112-exam-dp-203-topic-1-question-96-discussion/","exam_id":67,"answer_description":"","answers_community":[],"question_images":["https://img.examtopics.com/dp-203/image335.png"],"isMC":false,"answer":""},{"id":"5BVgAkvibraWIAcm2jd8","topic":"1","timestamp":"2023-11-06 07:55:00","answer_ET":"D","discussion":[{"comment_id":"1263394","content":"Selected Answer: D\nfrom this link : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\nPartition elimination is available only in the partitioned tables created on Parquet or CSV formats that are synchronized from Apache Spark pools. You might create external tables on Parquet partitioned folders, but the partitioning columns are inaccessible and ignored, while the partition elimination won't be applied. Don't create external tables on Delta Lake folders because they aren't supported. Use Delta partitioned views if you need to query partitioned Delta Lake data.","timestamp":"1723279980.0","poster":"fahfouhi94","upvote_count":"3"},{"comment_id":"1204971","timestamp":"1714555260.0","upvote_count":"1","content":"D is correct\nthe keyword here is: \"Delta Lake table named SparkTable1\"\nhere is how partitioned view works:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-views#partitioned-views\nAnd this link shows that dedicated pool does not talk to delta table:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","poster":"Dusica"},{"timestamp":"1713066960.0","poster":"mahmoud_salah30","upvote_count":"1","content":"Partition elimination is available only in the partitioned tables created on Parquet or CSV formats that are synchronized from Apache Spark pools.\"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\n\"","comment_id":"1195257"},{"comment_id":"1183194","poster":"Alongi","content":"Selected Answer: A\nOnly partitioned Table can support partition elimination, so the answer should be A","timestamp":"1711448100.0","upvote_count":"4"},{"content":"The folder partition elimination is available in the native external tables that are synchronized from the Synapse Spark pools. If you have partitioned data set and you would like to leverage the partition elimination with the external tables that you create, use the partitioned views instead of the external tables.","upvote_count":"1","comment_id":"1155469","poster":"mariano2","timestamp":"1708515420.0"},{"poster":"moneytime","timestamp":"1707013980.0","upvote_count":"4","comment_id":"1139729","content":"D is correct. \nThe solution is achieved by storing the delta lake table in sql spool ,so that transact-SQL query can be used on it..The choice of sql pool to use is determined by the format of the data .which is Delta.This format (Delta) is only supported by severless sql pool,hence it will be chosen for the job.Yes,it is important to state thay it can use the transct sql query through the openrowset function.\nThe next case in the solution is the partitioning part.\nAs a precaution from azure documentation ,external tables in severless sql pool with delta format should not be partitioned .If a partition must be used for query optimization,then use , a partition view .This view also contain partition folders or rather support partition folder elimination which is need for query optimization"},{"upvote_count":"2","comments":[{"comment_id":"1101936","content":"Quote from link provided suggesting D is correct\n“use the partitioned views instead of the external tables.”","upvote_count":"1","poster":"ExamDestroyer69","timestamp":"1703105820.0"}],"timestamp":"1700909040.0","content":"Everything is mentioned here:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop","comment_id":"1079932","poster":"arihant_jain"},{"timestamp":"1699253700.0","comment_id":"1063560","poster":"metiii","upvote_count":"4","content":"Selected Answer: D\nD is correct.\n\"The OPENROWSET function is not supported in dedicated SQL pools in Azure Synapse.\" so it eliminates A,B and C.\nRef: https://learn.microsoft.com/en-us/sql/t-sql/functions/openrowset-transact-sql?view=sql-server-ver16\nOnly the partitioned view in the serverless sql pool is correct since \"External tables in serverless SQL pools do not support partitioning on Delta Lake format. Use Delta partitioned views instead of tables if you have partitioned Delta Lake data sets.\"\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables#delta-tables-on-partitioned-folders"}],"question_id":115,"question_text":"You have an Azure Synapse Analytics workspace that contains an Apache Spark pool named SparkPool1. SparkPool1 contains a Delta Lake table named SparkTable1.\n\nYou need to recommend a solution that supports Transact-SQL queries against the data referenced by SparkTable1. The solution must ensure that the queries can use partition elimination.\n\nWhat should you include in the recommendation?","answer_images":[],"unix_timestamp":1699253700,"url":"https://www.examtopics.com/discussions/microsoft/view/125496-exam-dp-203-topic-1-question-97-discussion/","exam_id":67,"answer_description":"","choices":{"D":"a partitioned view in a serverless SQL pool","A":"a partitioned table in a dedicated SQL pool","B":"a partitioned view in a dedicated SQL pool","C":"a partitioned index in a dedicated SQL pool"},"answers_community":["D (64%)","A (36%)"],"question_images":[],"isMC":true,"answer":"D"}],"exam":{"isImplemented":true,"name":"DP-203","numberOfQuestions":384,"isMCOnly":false,"id":67,"provider":"Microsoft","lastUpdated":"12 Apr 2025","isBeta":false},"currentPage":23},"__N_SSP":true}