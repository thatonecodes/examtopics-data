{"pageProps":{"questions":[{"id":"lWtJkM0WOmmNA7dwMrnh","isMC":true,"question_text":"You are creating a new Azure Machine Learning pipeline using the designer.\nThe pipeline must train a model using data in a comma-separated values (CSV) file that is published on a website. You have not created a dataset for this file.\nYou need to ingest the data from the CSV file into the designer pipeline using the minimal administrative effort.\nWhich module should you add to the pipeline in Designer?","answer_ET":"C","choices":{"C":"Import Data","B":"Enter Data Manually","D":"Dataset","A":"Convert to CSV"},"answer":"C","exam_id":64,"answer_images":[],"topic":"3","timestamp":"2020-06-25 10:57:00","question_images":[],"question_id":306,"discussion":[{"timestamp":"1624795920.0","content":"Import data using the Import Data module\nThe Import Data module skips registering your dataset in Azure Machine Learning and imports data directly from a datastore or HTTP URL. So Answer is C","upvote_count":"43","poster":"sachinrkp","comment_id":"121202"},{"content":"Answer is D\nBy creating a dataset, you create a reference to the data source location, along with a copy of its metadata. Because the data remains in its existing location, you incur no extra storage cost, hence minimise administrative effort as required in the question. For the data to be accessible by Azure Machine Learning, datasets must be created from paths in Azure datastores or public web URLs.\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets","timestamp":"1624960980.0","upvote_count":"14","poster":"amelia","comments":[{"timestamp":"1651750860.0","content":"The question is asking \"which module should you...\" Dataset is not a designer module so even though you could use it, the question is not about that. Ans is C.","comment_id":"350248","upvote_count":"16","poster":"chaudha4"}],"comment_id":"122649"},{"upvote_count":"3","poster":"fhlos","timestamp":"1719567420.0","comment_id":"936432","content":"Selected Answer: C\nC - ChatGPT\nThe \"Import Data\" module in the Azure Machine Learning designer allows you to import data from various sources, including web URLs. It provides a simple way to ingest data from a CSV file published on a website without the need to create a dataset beforehand."},{"content":"C. Import Data\nThe Dataset module is used to create a dataset from existing data sources, such as Azure Blob Storage, Azure Data Lake Storage, or Azure SQL Database. Since the data is already in a CSV file, the Import Data module is the best option for ingesting the data into the pipeline with minimal administrative effort.","poster":"phdykd","upvote_count":"2","comment_id":"810091","timestamp":"1708044360.0"},{"upvote_count":"1","poster":"mamau","content":"C. Import Data for minimal admin effort\nUsing the Import Data component in Azure Machine Learning Studio requires minimal administrative effort compared to using Azure Machine Learning datasets. The Import Data component allows you to import data into your pipeline by specifying the URL of the file, so you can easily access and work with the data without the need to manually download and upload the file. This makes it a quick and efficient way to get your data into the pipeline with minimal administrative effort. On the other hand, creating an Azure Machine Learning dataset requires more steps, such as uploading the data to a storage account and creating the dataset in the Azure Machine Learning Studio, which can take more time and effort.\nhttps://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-designer-import-data","comment_id":"807056","timestamp":"1707799740.0"},{"timestamp":"1701983040.0","poster":"michaelmorar","comment_id":"738340","content":"Selected Answer: C\nImport data is correct","upvote_count":"1"},{"comment_id":"662138","timestamp":"1694070240.0","content":"maybe this question is a bit outdated. Through a Dataset object is currently possible to read data from HTTP URL's, so in my opinion D is the correct answer.\n\nhttps://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py","poster":"giusecozza","upvote_count":"3"},{"timestamp":"1670398500.0","upvote_count":"5","content":"Selected Answer: C\nAnswer is C","comment_id":"495735","poster":"dija123"},{"upvote_count":"2","comment_id":"446680","content":"I see only this kind of comment by the user across many question. I suspect this is just spamming.. even the user display name is suspect","timestamp":"1663428780.0","poster":"prasad06"},{"timestamp":"1658757780.0","upvote_count":"5","content":"C. Enter Data Manually (to minimize administrative cost)","comment_id":"413981","poster":"Tejoo"},{"upvote_count":"4","timestamp":"1657541160.0","content":"On exam 2021/7/10","poster":"ljljljlj","comment_id":"403919"},{"poster":"trickerk","upvote_count":"6","timestamp":"1657180800.0","comment_id":"400633","content":"Correct answer: C. \nModules for data input and output on designer:\n- Enter Data Manually\n- Export Data\n- Import Data\nhttps://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/module-reference"},{"upvote_count":"2","comment_id":"399232","content":"Answer D is correct. Please refer the below link which states add dataset Module for import data. https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-train-score","comments":[{"content":"Wrong\nModules for data input and output on designer:\n- Enter Data Manually\n- Export Data\n- Import Data\nhttps://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/module-reference","comment_id":"421155","poster":"trickerk","timestamp":"1659869220.0","upvote_count":"1"}],"timestamp":"1657032060.0","poster":"Kapil1803"},{"content":"https://docs.microsoft.com/en-us/azure/machine-learning/how-to-designer-import-data\nThere are two possible ways to extract data\n1. Register Data - to \"Dataset\" segment \n2. Using \"Import Data\" \n\nSince question is already ask about extracting data via website and user didn't register dataset beforehand \nC is should be correct answer.","comment_id":"383172","upvote_count":"4","timestamp":"1655363580.0","poster":"SnowCheetah"},{"poster":"rishi_ram","comment_id":"373848","upvote_count":"4","timestamp":"1654285020.0","content":"Answer is C:\nImport Data module - Use the Import Data module to directly access data from online datasources.\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-designer-import-data\nhttps://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/import-data"},{"content":"Dataset is not even a module that you can use in designer. Import Data is correct answer.","upvote_count":"10","comment_id":"321541","timestamp":"1648341960.0","poster":"dev2dev"},{"upvote_count":"4","comment_id":"313824","content":"explication for the question is not even for the designer. \nthe answer is 'C'","timestamp":"1647579960.0","poster":"kty"},{"upvote_count":"2","comment_id":"313135","timestamp":"1647512400.0","content":"\"Which module should you add to the pipeline in Designer?\"\nThere is no module available in designer called \"Dataset\" so its not even a valid option.","poster":"dev2dev"},{"timestamp":"1645573260.0","upvote_count":"7","content":"Answer is C.\nDirectly from the designer module:\nLoad data from web URLs or from various cloud-based storage in Azure, such as Azure SQL database, Azure blob storage..\nhttps://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/import-data","poster":"treadst0ne","comment_id":"297022"},{"comment_id":"287925","content":"Answer is C 100% sure.","poster":"ZeeshanNawaz","upvote_count":"5","timestamp":"1644537060.0"},{"content":"Azure Machine Learning datasets - Register datasets in Azure Machine Learning to enable advanced features that help you manage your data. BUT in pipeline section it also says:The Dataset object points to data that lives in or is accessible from a datastore or at a Web\nURL\n\nImport Data module - Use the Import Data module to directly access data from online datasources.\n\nThis question is really ambiguous","comment_id":"275147","timestamp":"1643016540.0","upvote_count":"1","poster":"hachascloud"},{"content":"answer is C\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-designer-import-data","comment_id":"260517","timestamp":"1641411300.0","upvote_count":"5","poster":"aziti"},{"upvote_count":"3","poster":"zihao","timestamp":"1641344460.0","comment_id":"259835","content":"At the newest version, the answer is B. \nImport data can only work for the data stored in the datastore"},{"comment_id":"176588","upvote_count":"8","content":"Answer is C. D is not possible...the text says \"You have not created a dataset for this file\"","comments":[{"content":"agreed.","poster":"alphmzla","comment_id":"257186","timestamp":"1641086760.0","upvote_count":"3"}],"poster":"Alberto_Lugo","timestamp":"1631198760.0"},{"poster":"podval","comment_id":"134051","content":"I assume creating a Dataset does not minimize administrative efforts. \nImport is easy.","upvote_count":"5","timestamp":"1626190980.0"},{"upvote_count":"8","content":"Answer is C","timestamp":"1624611420.0","comment_id":"119262","poster":"sreemenon23"}],"answer_description":"","unix_timestamp":1593075420,"url":"https://www.examtopics.com/discussions/microsoft/view/24014-exam-dp-100-topic-3-question-45-discussion/","answers_community":["C (100%)"]},{"id":"Lnuiiqnj8rZE1MlMqpPm","isMC":false,"exam_id":64,"answer_images":[],"question_id":307,"answer_ET":"E","url":"https://www.examtopics.com/discussions/microsoft/view/54036-exam-dp-100-topic-3-question-46-discussion/","unix_timestamp":1622526180,"timestamp":"2021-06-01 07:43:00","discussion":[{"comment_id":"421160","upvote_count":"14","content":"Given answer is correct cause \"data_folder\" already has 'train' path.\nTake a look at:\ndata_ref = ml_data.path('train').as_download(path_on_compute='train_data')","timestamp":"1644238500.0","poster":"trickerk"},{"timestamp":"1638344580.0","upvote_count":"6","comment_id":"371592","content":"How about answer B as question says data is in 'train' folder ?","comments":[{"content":"I think the script will run inside the folder, so the absolute path will be returned, so doesn't need to describe the folder's name.","upvote_count":"1","timestamp":"1641551940.0","comment_id":"400677","poster":"trickerk"}],"poster":"rishi_ram"},{"poster":"NullVoider_0","content":"The correct code segment to load the training data is B. This is because you have defined the data reference as data_ref = ml_data.path('train').as_download(path_on_compute='train_data'), which means that the data will be downloaded to the train_data folder on the compute target. Therefore, you need to use os.path.join(data_folder, 'train', 'data.csv') to read the CSV file from the train folder in the ml-data datastore.","comment_id":"1101534","upvote_count":"1","timestamp":"1718881920.0"},{"timestamp":"1703763660.0","poster":"fhlos","comments":[{"comments":[{"poster":"Lion007","upvote_count":"2","timestamp":"1719741540.0","content":"Option B assumes that the data.csv file is still inside the directory 'train' within the data_folder. This is WRONG since the path is already constructed in the data_ref without the need to repeat the 'train' directory, as the data_ref is already pointing to the correct location of the data.csv file.","comment_id":"1109711"}],"content":"The Correct answer is as given: E\nWe just need to join the data_folder with the CSV file data.csv\ndata = pd.read_csv(os.path.join(data_folder, 'data.csv'))\n\nLet me explain why B is WRONG:\nThe DataReference is configured to download the data from a datastore to a local directory named 'train_data' on the compute target:\ndata_ref = ml_data.path('train').as_download(path_on_compute='train_data') \n\nIn this context, the data_ref object will download the contents of the 'train' folder from the Azure Blob Storage to a local directory called 'train_data' on the compute target. The script_params in the Estimator object then passes this data_ref as an argument for --data-folder:\nscript_params={'--data-folder': data_ref}, \n\nGiven this setup, the training script (optin E) expects the --data-folder argument to specify the path to the directory where the data.csv file is located. It then reads this CSV file into a pandas DataFrame.","poster":"Lion007","upvote_count":"2","timestamp":"1719741540.0","comment_id":"1109710"}],"comment_id":"936438","content":"B - ChatGPT\nThe correct code segment to load the training data in this scenario is:\n\nB.\nimport os\nimport argparse\nimport pandas as pd\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder')\nargs = parser.parse_args()\ndata_folder = args.data_folder\ndata = pd.read_csv(os.path.join(data_folder, 'train', 'data.csv'))\n\nExplanation:\nThe code segment B properly handles the command-line argument parsing using the argparse module and retrieves the data_folder argument. It then uses os.path.join to construct the correct path to the training data file data.csv within the specified data_folder.\n\nThe other code segments (A, C, D, E) either have syntax errors, incorrect path references, or incorrect argument parsing, which would lead to issues when trying to load the training data.\n\nTherefore, the correct code segment is B.","upvote_count":"2"},{"poster":"Andrea2","upvote_count":"4","comment_id":"613647","timestamp":"1670573040.0","content":"I think answer E is correct. Data_ref contains the reference to data, that has been downloaded on the compute at the path train_data. For this reason you can simply add data.csv to load data."},{"content":"Seems different from #43 above (using input)","upvote_count":"1","timestamp":"1643120100.0","comment_id":"413885","poster":"YipingRuan"},{"upvote_count":"2","comment_id":"392433","poster":"MohsenSic","timestamp":"1640653020.0","content":"@rishi_ram, I guess it is wrong as we will end up with two ''train\"s in the path, one from data_folder and one from \"train\""}],"topic":"3","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0021700001.png","https://www.examtopics.com/assets/media/exam-media/04274/0021800001.png","https://www.examtopics.com/assets/media/exam-media/04274/0021800002.png","https://www.examtopics.com/assets/media/exam-media/04274/0021800003.png","https://www.examtopics.com/assets/media/exam-media/04274/0021800004.png","https://www.examtopics.com/assets/media/exam-media/04274/0021800005.png"],"answers_community":[],"answer":"E","answer_description":"Example:\ndata_folder = args.data_folder\n# Load Train and Test data\ntrain_data = pd.read_csv(os.path.join(data_folder, 'data.csv'))\nReference:\nhttps://www.element61.be/en/resource/azure-machine-learning-services-complete-toolbox-ai","question_text":"You define a datastore named ml-data for an Azure Storage blob container. In the container, you have a folder named train that contains a file named data.csv.\nYou plan to use the file to train a model by using the Azure Machine Learning SDK.\nYou plan to train the model by using the Azure Machine Learning SDK to run an experiment on local compute.\nYou define a DataReference object by running the following code:\n//IMG//\n\nYou need to load the training data.\nWhich code segment should you use?\nA.\n//IMG//\n\nB.\n//IMG//\n\nC.\n//IMG//\n\nD.\n//IMG//\n\nE.\n//IMG//"},{"id":"keRmb5B3bJjabjgu5pDv","question_id":308,"answer_ET":"A","url":"https://www.examtopics.com/discussions/microsoft/view/47487-exam-dp-100-topic-3-question-47-discussion/","answer":"A","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0022000001.png","https://www.examtopics.com/assets/media/exam-media/04274/0022000002.png","https://www.examtopics.com/assets/media/exam-media/04274/0022000003.png"],"exam_id":64,"timestamp":"2021-03-17 11:34:00","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou create an Azure Machine Learning service datastore in a workspace. The datastore contains the following files:\n✑ /data/2018/Q1.csv\n✑ /data/2018/Q2.csv\n✑ /data/2018/Q3.csv\n✑ /data/2018/Q4.csv\n✑ /data/2019/Q1.csv\nAll files store data in the following format:\nid,f1,f2,I\n1,1,2,0\n2,1,1,1\n3,2,1,0\n4,2,2,1\nYou run the following code:\n//IMG//\n\nYou need to create a dataset named training_data and load the data from all files into a single data frame by using the following code:\n//IMG//\n\nSolution: Run the following code:\n//IMG//\n\nDoes the solution meet the goal?","unix_timestamp":1615977240,"choices":{"A":"Yes","B":"No"},"answer_description":"","answers_community":["A (91%)","9%"],"topic":"3","discussion":[{"upvote_count":"15","timestamp":"1650865320.0","comment_id":"342354","poster":"Haet","content":"The Answer is clearly no"},{"content":"Its seems the question has been changed/updated since some of these comments.","poster":"reddragondms","comment_id":"679514","timestamp":"1695714060.0","upvote_count":"11"},{"poster":"james2033","upvote_count":"1","timestamp":"1729297380.0","comment_id":"1047380","content":"This question is out-of-date, obsoleted. Should be\n\nfrom azure.ai.ml import ...\n\nnot \n\nfrom azureml.core import Dataset\n\nReference: https://github.com/Azure/azure-sdk-for-python/tree/azure-ai-ml_1.11.1/sdk/ml/azure-ai-ml#authenticate-the-client"},{"content":"Selected Answer: A\nIt meets the requirements. See example below from Microsoft: \n\n # create tabular dataset from all csv files in the directory\n tabular_dataset_3 = Dataset.Tabular.from_delimited_files(path=(datastore,'weather/**/*.csv'))\n\n # create tabular dataset from multiple paths\n data_paths = [(datastore, 'weather/2018/11.csv'), (datastore, 'weather/2018/12.csv')]\n tabular_dataset_4 = Dataset.Tabular.from_delimited_files(path=data_paths)\n\nLink: https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#azureml-data-dataset-factory-tabulardatasetfactory-from-delimited-files\n\nSaM","poster":"PI_Team","upvote_count":"1","comment_id":"968970","timestamp":"1722509640.0"},{"timestamp":"1719567960.0","comment_id":"936444","upvote_count":"1","poster":"fhlos","content":"Selected Answer: B\nNo, the solution does not meet the goal. The code provided to create the dataset and load the data into a single DataFrame is incorrect.\n\nTo create a dataset named training_data and load the data from all files into a single DataFrame, you need to modify the code as follows:\n\npython\nCopy code\nfrom azureml.core import Dataset\n\npaths = [(data_store, 'data/2018/*.csv'), (data_store, 'data/2019/*.csv')]\ntraining_data = Dataset.Tabular.from_delimited_files(paths)\ndata_frame = training_data.to_pandas_dataframe()\nExplanation:\n\nThe paths variable is updated to specify the paths of all files to be included in the dataset. In this case, it includes all CSV files in the /data/2018 and /data/2019 directories.\nThe Dataset.Tabular.from_delimited_files() method is used to create the dataset training_data by providing the paths variable.\nThe to_pandas_dataframe() method is called on the training_data dataset to load the data from all files into a single pandas DataFrame.\nBy making these changes, the code will create the desired dataset and load the data from all files into a single DataFrame."},{"upvote_count":"1","timestamp":"1718511240.0","poster":"abhishekm94","content":"Correct answer is Yes\nLink :: https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py&viewFallbackFrom=azure-ml-pyandhttps%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fpython%2Fapi%2Fazureml-core%2Fazureml.data.tabulardataset%3Fview%3Dazure-ml-py","comment_id":"924836"},{"content":"Selected Answer: A\nQuestion updated as of Jan 2023.... based on\nhttps://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py\nand\nhttps://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py\n\nAnswer seems to be A - YES","poster":"centurion2020","upvote_count":"9","timestamp":"1707501420.0","comment_id":"803555"},{"poster":"sultanmr123","comment_id":"727811","comments":[{"timestamp":"1702120020.0","content":"Why B?\nThe Dataset is Tabular, and there is no need for two file paths.","upvote_count":"1","poster":"casiopa","comment_id":"740018"}],"upvote_count":"1","content":"yes Answer is B","timestamp":"1701031560.0"},{"comment_id":"662400","timestamp":"1694085660.0","content":"Answer is correct","upvote_count":"2","poster":"ai_lover"},{"comment_id":"413526","upvote_count":"1","content":"web_path ='https://dprepdata.blob.core.windows.net/demo/Titanic.csv'\ntitanic_ds = Dataset.Tabular.from_delimited_files(path=web_path, set_column_types={'Survived': DataType.to_bool()})\n\n# preview the first 3 rows of titanic_ds\ntitanic_ds.take(3).to_pandas_dataframe()\n\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#set-data-schema","poster":"YipingRuan","timestamp":"1658708880.0"},{"poster":"brendal89","content":"I think the answer might be 'yes'.\n\nsee this similar example for parquet files:\ndatastore_path = [(dstore, dset_name + '/*/*/data.parquet')]\ndataset = Dataset.Tabular.from_parquet_files(path=datastore_path, partition_format = dset_name + '/{partition_time:yyyy/MM}/data.parquet')\n\nthe partition_format argument appears optional.\nreference: https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb","comments":[{"poster":"l2azure","upvote_count":"13","content":"Answer is 'No'.\nYou must create a pandas dataframe which is only possible from a Dataset.Tabular object.\nIn this case (see last line) the dataframe cannot be made since it is a Dataset.File object.","comment_id":"331880","timestamp":"1649500200.0","comments":[{"poster":"Arend78","comment_id":"743862","timestamp":"1702461720.0","upvote_count":"3","content":"I think they changed the question. The code now ends with a Tabular Dataset, that indeed can be user as input to as_pandas_dataframe() \nI think the answer is now \"Yes\""},{"poster":"l2azure","content":"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py","timestamp":"1649500380.0","comment_id":"331881","upvote_count":"2"}]}],"upvote_count":"3","timestamp":"1649419860.0","comment_id":"331157"},{"content":"answer is yes.","timestamp":"1647513240.0","poster":"dev2dev","comments":[{"content":"no. its not calling the correct function it should be \"from_delimited_files\" instead of \"from_files\"","poster":"dev2dev","upvote_count":"9","timestamp":"1647513660.0","comment_id":"313146"}],"comment_id":"313143","upvote_count":"1"}],"answer_images":[],"isMC":true},{"id":"2RaPFNjJqA08xm9xgeXi","choices":{"B":"No","A":"Yes"},"answer_images":[],"timestamp":"2021-07-07 11:04:00","question_id":309,"isMC":true,"discussion":[{"content":"Based on the comments, I think that at some stage, the solution/image for this question(48) was swapped with the solution/image of the previous question(47), leading to confusion for new readers.\nIf the solution has \"Dataset.File.from_files(paths)\", then the answer is B, No\nIf the solution has \"Dataset.Tabular.from_delimited_files(paths)\", then the answer is A, Yes\nReference: \nhttps://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py\nhttps://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset(class)?view=azure-ml-py","timestamp":"1705238340.0","upvote_count":"15","comment_id":"775436","poster":"[Removed]"},{"content":"Yes. It works","poster":"slash_nyk","comment_id":"402354","upvote_count":"8","timestamp":"1657331640.0"},{"poster":"james2033","content":"This question is out-of-date, obsoleted. Should be\n\nfrom azure.ai.ml import ...\n\nnot \n\nfrom azureml.core import Dataset\n\nReference: https://github.com/Azure/azure-sdk-for-python/tree/azure-ai-ml_1.11.1/sdk/ml/azure-ai-ml#authenticate-the-client","comment_id":"1047383","timestamp":"1729297380.0","upvote_count":"1"},{"timestamp":"1719568200.0","comment_id":"936449","upvote_count":"1","poster":"fhlos","content":"Selected Answer: B\nNo - ChatGPT\nNo, the solution does not meet the goal. The code provided to create the dataset and load the data into a single DataFrame is incorrect.\n\nTo create a dataset named training_data and load the data from all files into a single DataFrame, you need to modify the code as follows:\n\nfrom azureml.core import Dataset\n\npaths = [(data_store, 'data/2018/*.csv'), (data_store, 'data/2019/*.csv')]\ntraining_data = Dataset.Tabular.from_delimited_files(paths)\ndata_frame = training_data.to_pandas_dataframe()\nExplanation:\n\nThe paths variable is updated to specify the paths of all files to be included in the dataset. In this case, it includes all CSV files in the /data/2018 and /data/2019 directories.\nThe Dataset.Tabular.from_delimited_files() method is used to create the dataset training_data by providing the paths variable.\nThe to_pandas_dataframe() method is called on the training_data dataset to load the data from all files into a single pandas DataFrame.\nBy making these changes, the code will create the desired dataset and load the data from all files into a single DataFrame."},{"comment_id":"924834","upvote_count":"1","poster":"abhishekm94","timestamp":"1718511240.0","content":"As per documentation, the correct answer is Yes. \nlink:: https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py&viewFallbackFrom=azure-ml-pyandhttps%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fpython%2Fapi%2Fazureml-core%2Fazureml.data.tabulardataset%3Fview%3Dazure-ml-py"},{"poster":"Crusader2k13","content":"It is clearly No and the answer is correct! \n\nYou can't create a pandas dataframe from Dataset.File.from_files(), only from a Tabular dataset! \n\nSee: \nhttps://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py\n\nFileDataset has no to_pandas_dataframe() method. See:\nhttps://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py","timestamp":"1703023260.0","comment_id":"750279","upvote_count":"1"},{"comment_id":"461861","poster":"nick234987","timestamp":"1665731400.0","upvote_count":"2","content":"it should be YES. Check this link: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#from-delimited-files-path--validate-true--include-path-false--infer-column-types-true--set-column-types-none--separator------header-true--partition-format-none--support-multi-line-false--empty-as-string-false--encoding--utf8--"},{"timestamp":"1664431500.0","poster":"skrjha20","content":"It should be Yes\n# create tabular dataset from all csv files in the directory\n tabular_dataset_3 = Dataset.Tabular.from_delimited_files(path=(datastore,'weather/**/*.csv'))","comment_id":"453869","upvote_count":"3"},{"upvote_count":"5","timestamp":"1661598840.0","content":"Tried in aml. It works...","comment_id":"433017","poster":"Marcello83"},{"content":"# create tabular dataset from all csv files in the directory\n tabular_dataset_3 = Dataset.Tabular.from_delimited_files(path=(datastore,'weather/**/*.csv'))\n\nhttps://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py","comment_id":"413188","upvote_count":"4","timestamp":"1658666640.0","poster":"YipingRuan"},{"poster":"trickerk","comment_id":"400697","timestamp":"1657184640.0","content":"Answer should be Yes.","upvote_count":"4"}],"topic":"3","question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0022100006.png","https://www.examtopics.com/assets/media/exam-media/04274/0022100007.png","https://www.examtopics.com/assets/media/exam-media/04274/0022100008.png"],"answer_ET":"B","exam_id":64,"answer":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou create an Azure Machine Learning service datastore in a workspace. The datastore contains the following files:\n✑ /data/2018/Q1.csv\n✑ /data/2018/Q2.csv\n✑ /data/2018/Q3.csv\n✑ /data/2018/Q4.csv\n✑ /data/2019/Q1.csv\nAll files store data in the following format:\nid,f1,f2,I\n1,1,2,0\n2,1,1,1\n3,2,1,0\n4,2,2,1\nYou run the following code:\n//IMG//\n\nYou need to create a dataset named training_data and load the data from all files into a single data frame by using the following code:\n//IMG//\n\nSolution: Run the following code:\n//IMG//\n\nDoes the solution meet the goal?","answer_description":"Use two file paths.\nUse Dataset.Tabular_from_delimeted, instead of Dataset.File.from_files as the data isn't cleansed.\nNote:\nA FileDataset references single or multiple files in your datastores or public URLs. If your data is already cleansed, and ready to use in training experiments, you can download or mount the files to your compute as a FileDataset object.\nA TabularDataset represents data in a tabular format by parsing the provided file or list of files. This provides you with the ability to materialize the data into a pandas or Spark DataFrame so you can work with familiar data preparation and training libraries without having to leave your notebook. You can create a\nTabularDataset object from .csv, .tsv, .parquet, .jsonl files, and from SQL query results.\nReference:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets","url":"https://www.examtopics.com/discussions/microsoft/view/57350-exam-dp-100-topic-3-question-48-discussion/","answers_community":["B (100%)"],"unix_timestamp":1625648640},{"id":"4l8DYxPpb8J8K4JXOUWB","choices":{"B":"No","A":"Yes"},"discussion":[{"content":"The correct answer should be Yes. I have tested that the code works.","poster":"PakE","comment_id":"322383","timestamp":"1648443900.0","upvote_count":"18"},{"content":"I think the answer might be 'yes'.\n\nsee this similar example for parquet files:\ndatastore_path = [(dstore, dset_name + '/*/*/data.parquet')]\ndataset = Dataset.Tabular.from_parquet_files(path=datastore_path, partition_format = dset_name + '/{partition_time:yyyy/MM}/data.parquet')\n\nthe partition_format argument appears optional.\nreference: https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb","upvote_count":"11","poster":"brendal89","comment_id":"331159","timestamp":"1649420100.0"},{"poster":"james2033","content":"This question is out-of-date, obsoleted. Should be\n\nfrom azure.ai.ml import ...\n\nnot \n\nfrom azureml.core import Dataset\n\nReference: https://github.com/Azure/azure-sdk-for-python/tree/azure-ai-ml_1.11.1/sdk/ml/azure-ai-ml#authenticate-the-client","timestamp":"1729297440.0","upvote_count":"1","comment_id":"1047384"},{"content":"Selected Answer: A\nYES - Chat GPT\nYes, the solution meets the goal. The provided code correctly creates a dataset named training_data and loads the data from all files into a single DataFrame.\n\nfrom azureml.core import Dataset\n\npaths = [(data_store, 'data/2018/*.csv'), (data_store, 'data/2019/*.csv')]\ntraining_data = Dataset.Tabular.from_delimited_files(paths)\ndata_frame = training_data.to_pandas_dataframe()\nExplanation:\n\nThe code registers the Azure Blob container datastore named data_store in the workspace.\nThe paths variable is defined to specify the paths of all files to be included in the dataset. It includes all CSV files in the /data/2018 and /data/2019 directories.\nThe Dataset.Tabular.from_delimited_files() method is used to create the dataset training_data by providing the paths variable.\nThe to_pandas_dataframe() method is called on the training_data dataset to load the data from all files into a single pandas DataFrame.\nBy executing this code, the dataset training_data will be created, and the data from all files will be loaded into a single DataFrame for further processing.","upvote_count":"1","comment_id":"936453","timestamp":"1719568380.0","poster":"fhlos"},{"comment_id":"617981","upvote_count":"2","content":"On exam 18-06-22","timestamp":"1687045380.0","poster":"therealola"},{"poster":"azurelearner666","timestamp":"1681446300.0","upvote_count":"1","content":"Selected Answer: A\nA. Yes.\n\nfrom azureml.core import Dataset\n\n# Get the default datastore\ndefault_ds = ws.get_default_datastore()\n\n#Create a tabular dataset from the path on the datastore (this may take a short while)\ntab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n\n# Display the first 20 rows as a Pandas dataframe\ntab_data_set.take(20).to_pandas_dataframe()","comment_id":"585527"},{"upvote_count":"1","poster":"Thornehead","timestamp":"1679604660.0","comment_id":"573894","content":"Read the question again. The answer is not yes because it has missing values in it. The data has to be processed first then it should be put in for the training."},{"upvote_count":"5","timestamp":"1665731700.0","content":"It is yes, no doubt","poster":"nick234987","comment_id":"461862"},{"content":"Hi All. The code works but answer should be yes. Pay attention to the output. Print out the the dataset and you will notice the difference in soruce","comment_id":"390839","timestamp":"1656206700.0","upvote_count":"1","poster":"slash_nyk"},{"timestamp":"1655576520.0","comment_id":"384993","content":"The answer is Yes. \n\nfrom azureml.core import Dataset\n\n# Get the default datastore\ndefault_ds = ws.get_default_datastore()\n\n#Create a tabular dataset from the path on the datastore (this may take a short while)\ntab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n\n# Display the first 20 rows as a Pandas dataframe\ntab_data_set.take(20).to_pandas_dataframe()","poster":"surfing","upvote_count":"3"},{"poster":"rsamant","timestamp":"1654015140.0","content":"answer is yes. tested","comment_id":"371221","upvote_count":"5"},{"timestamp":"1652718300.0","upvote_count":"2","poster":"scipio","content":"I think the problem is the * for the directory. Something like this:\npaths = [(data_store, 'data/2018/*.csv'),(data_store, 'data/2019/*.csv')]\nit would be the correct way","comments":[{"timestamp":"1655955240.0","comment_id":"388461","poster":"treadst0ne","content":"I had the same concern, but after testing it, it is possible to create a Tabular dataset passing \"parent_folder/*/*.csv\" as a path.\nSo yes, answer should be A.","upvote_count":"2"}],"comment_id":"358910"},{"upvote_count":"1","content":"from azureml.core import Workspace, Datastore, Dataset\n\ndatastore_name = 'your datastore name'\n\n# get existing workspace\nworkspace = Workspace.from_config()\n \n# retrieve an existing datastore in the workspace by name\ndatastore = Datastore.get(workspace, datastore_name)\n\n# create a TabularDataset from 3 file paths in datastore\ndatastore_paths = [(datastore, 'weather/2018/11.csv'),\n (datastore, 'weather/2018/12.csv'),\n (datastore, 'weather/2019/*.csv')]\n\nweather_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)","poster":"ali25","comment_id":"324972","timestamp":"1648722360.0"}],"answers_community":["A (100%)"],"unix_timestamp":1616907900,"question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0022300001.png","https://www.examtopics.com/assets/media/exam-media/04274/0022300002.png","https://www.examtopics.com/assets/media/exam-media/04274/0022300003.png"],"answer_description":"Use two file paths.\nUse Dataset.Tabular_from_delimeted as the data isn't cleansed.\nNote:\nA TabularDataset represents data in a tabular format by parsing the provided file or list of files. This provides you with the ability to materialize the data into a pandas or Spark DataFrame so you can work with familiar data preparation and training libraries without having to leave your notebook. You can create a\nTabularDataset object from .csv, .tsv, .parquet, .jsonl files, and from SQL query results.\nReference:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou create an Azure Machine Learning service datastore in a workspace. The datastore contains the following files:\n✑ /data/2018/Q1.csv\n✑ /data/2018/Q2.csv\n✑ /data/2018/Q3.csv\n✑ /data/2018/Q4.csv\n✑ /data/2019/Q1.csv\nAll files store data in the following format:\nid,f1,f2,I\n1,1,2,0\n2,1,1,1\n3,2,1,0\n4,2,2,1\nYou run the following code:\n//IMG//\n\nYou need to create a dataset named training_data and load the data from all files into a single data frame by using the following code:\n//IMG//\n\nSolution: Run the following code:\n//IMG//\n\nDoes the solution meet the goal?","answer":"A","question_id":310,"isMC":true,"exam_id":64,"url":"https://www.examtopics.com/discussions/microsoft/view/48339-exam-dp-100-topic-3-question-49-discussion/","timestamp":"2021-03-28 07:05:00","answer_ET":"A","topic":"3","answer_images":[]}],"exam":{"provider":"Microsoft","isBeta":false,"numberOfQuestions":512,"lastUpdated":"12 Apr 2025","isMCOnly":false,"isImplemented":true,"id":64,"name":"DP-100"},"currentPage":62},"__N_SSP":true}