{"pageProps":{"questions":[{"id":"AxcgLdrSf8m1sSwRW6DG","answer":"","isMC":false,"topic":"2","timestamp":"2021-04-28 14:27:00","exam_id":64,"question_id":101,"question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0008900001.png","https://www.examtopics.com/assets/media/exam-media/04274/0009000001.png"],"unix_timestamp":1619612820,"discussion":[{"comments":[{"timestamp":"1686724500.0","poster":"Edriv","comment_id":"744880","upvote_count":"2","content":"Correct"}],"upvote_count":"64","timestamp":"1635737460.0","comment_id":"346518","poster":"iuolu","content":"The correct answer: No No Yes No"},{"timestamp":"1635424020.0","comment_id":"344633","content":"second answer should be no, as the here not specified parameter min_node_count defaults to 'none' - see https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute(class)?view=azure-ml-py","comments":[{"comment_id":"583700","poster":"azurelearner666","content":"Exactly. Also technically it defaults to zero, so only the cluster is created and the nodes are created/added whenever they are needed.\nBut response is No, No, Yes, No","timestamp":"1665407460.0","upvote_count":"2"},{"comment_id":"345075","upvote_count":"1","content":"Have you looked at other questions?\nAre the answers to other questions correct?","poster":"scott_klein_12345","timestamp":"1635485400.0"}],"upvote_count":"13","poster":"reini"},{"comment_id":"1221513","poster":"Ejire","upvote_count":"1","content":"Answer is No NoYes No.\nIf you need voucher for your DP and AI exams contact me on+2348139103938","timestamp":"1732970880.0"},{"poster":"kel_dp_100","timestamp":"1715718060.0","upvote_count":"1","content":"the second should be Yes, from GPT\nYes, your understanding is correct. The wait_for_completion method, when called on a compute target, will wait until the specified compute target (in this case, the 'aml-cluster') has all its nodes provisioned and ready for use. The parameter show_output=True will also display detailed output while waiting, so you can monitor the progress.\n\nSo, in your specific code, it will not return until the 'aml-cluster' has all four nodes provisioned and ready for use.","comment_id":"1070893"},{"poster":"MarinaMijailovic","comment_id":"894596","content":"NO NO YES NO\n\nNO - If it doesn't exist an exception is raised, then a new compute target is created. It doesn't delete and replace an existing one.\nNO - The wait_for_completion() method will return when the compute target is in the 'Succeeded' or 'Failed' provisioning state. It doesn't specifically wait for all four nodes to become active.\nYES - The compute target is being created with 'lowpriority' VMs, which can be preempted if Azure needs the capacity.\nNO - This code snippet does not include any code to delete the compute target after the training experiment completes.","timestamp":"1699687920.0","upvote_count":"5"},{"content":"Exactly this question was on exam 07/March/2023","poster":"Yuriy_Ch","comment_id":"832860","upvote_count":"2","timestamp":"1694164140.0"},{"content":"Seen on the exam 20Feb2023","timestamp":"1692555000.0","upvote_count":"2","comment_id":"815788","poster":"jpalaci22"},{"timestamp":"1665390000.0","poster":"turtle666","content":"training_compute created from ComputeTarget class, and wait_for_completion() not have min_node option, different from AmlCompute Class\nhttps://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computetarget?view=azure-ml-py#azureml-core-compute-computetarget-wait-for-completion","comment_id":"583622","upvote_count":"1"},{"timestamp":"1664311560.0","upvote_count":"3","content":"No, Yes, Yes and No.\n\nRead the question and coding carefully.","comment_id":"576430","poster":"Thornehead"},{"upvote_count":"2","timestamp":"1663483140.0","poster":"kkkk_jjjj","comment_id":"570369","content":"on exam 18/03/2022"},{"timestamp":"1662749520.0","comment_id":"564324","content":"On march-9-2022","poster":"TheYazan","upvote_count":"1"},{"poster":"JoshuaXu","upvote_count":"1","comment_id":"473634","timestamp":"1651865880.0","content":"on exam 6 Nov 2021"},{"timestamp":"1650440460.0","poster":"hargur","upvote_count":"1","content":"on 19Oct2021","comment_id":"464981"},{"content":"on 17/7/2021","timestamp":"1642423860.0","poster":"Rosh4yuh","upvote_count":"3","comment_id":"408390"},{"upvote_count":"4","content":"On exam 2021/7/10","poster":"ljljljlj","comment_id":"403890","timestamp":"1641909120.0"},{"timestamp":"1640508240.0","content":"min_node_count | default value: None\nMinimum number of nodes to wait for before considering provisioning to be complete. This doesn't have to equal the minimum number of nodes that the compute was provisioned with, however it should not be greater than that.\n\nhttps://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute(class)?view=azure-ml-py#wait-for-completion-show-output-false--min-node-count-none--timeout-in-minutes-25--is-delete-operation-false-","poster":"Meg04","comment_id":"390988","upvote_count":"1"},{"upvote_count":"4","content":"yes agree with other correct answer is No No Yes No","timestamp":"1640376180.0","comment_id":"389810","poster":"azurecert2021"},{"timestamp":"1637144400.0","poster":"rishi_ram","comment_id":"359380","content":"wait_for_completion \nWait for the AmlCompute cluster to finish provisioning.\n\nThis can be configured to wait for a minimum number of nodes, and to timeout after a set period of time.\nHence Second Answer should be NO","upvote_count":"3"},{"upvote_count":"8","content":"Box 2 is defnitely NO!!! Max Nodes = 4, its not min nodes.","timestamp":"1635573420.0","poster":"levm39","comment_id":"345917"}],"answer_ET":"","question_text":"HOTSPOT -\nYou are preparing to use the Azure ML SDK to run an experiment and need to create compute. You run the following code:\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answers_community":[],"answer_images":["https://img.examtopics.com/dp-100/image599.png"],"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/51107-exam-dp-100-topic-2-question-29-discussion/"},{"id":"y5Zjfb3T93hIL42KWDiD","question_text":"DRAG DROP -\nYou are building an intelligent solution using machine learning models.\nThe environment must support the following requirements:\n✑ Data scientists must build notebooks in a cloud environment\n✑ Data scientists must use automatic feature engineering and model building in machine learning pipelines.\n✑ Notebooks must be deployed to retrain using Spark instances with dynamic worker allocation.\n✑ Notebooks must be exportable to be version controlled locally.\nYou need to create the environment.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","timestamp":"2020-09-15 03:27:00","question_id":102,"answer_ET":"","discussion":[{"upvote_count":"50","comments":[{"comment_id":"467271","upvote_count":"3","poster":"spaceykacey","timestamp":"1635142740.0","content":"incase anyone still has doubts about this, refer: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-databricks-automl-environment"},{"upvote_count":"3","timestamp":"1621530120.0","comment_id":"362372","content":"dont see the option for install ML SDK for python for databricks","comments":[{"poster":"prashantjoge","upvote_count":"3","timestamp":"1621533180.0","content":"you can add ML lib using a script action when you create the HDinsight service. databricks-rg-azdatabrickspt-fkd2ogyzogbag","comment_id":"362403"}],"poster":"prashantjoge"},{"content":"Notebooks must be deployed to retrain using Spark instances with dynamic worker allocation- This condition won't be satisfied with Jupyter","timestamp":"1617893940.0","poster":"bruce","upvote_count":"5","comment_id":"331292"}],"comment_id":"323770","timestamp":"1617046740.0","poster":"Dasist","content":"Should be: Create Azure Databricks cluster -> Install Azure ML SDK for Python -> Create and exec Jupyter notebook using AutoML -> Export Jupyter to local env. That because you need auto feature engineering provided by autoML"},{"timestamp":"1661223900.0","comment_id":"650542","upvote_count":"13","poster":"ajay_1233456","content":"1. Create Azure Databricks cluster\n2. Install Azure ML SDK for Python\n3. Create and exec Jupyter notebook using AutoML\n4. Export Jupyter to local env"},{"comment_id":"1233694","comments":[{"poster":"larimalarima","comment_id":"1236334","timestamp":"1719232860.0","upvote_count":"2","content":"I think it's most accurate"}],"poster":"OdaNabunaga","timestamp":"1718895240.0","upvote_count":"3","content":"1. Create an Azure Databricks cluster\n2. Install the Azure Machine Learning SDK for Python on the cluster\n3. Create and execute a Jupyter notebook by using automated machine learning (AutoML) on the cluster\n4. When the cluster is ready and has processed the notebook, export your Jupyter notebook to a local environment"},{"upvote_count":"2","poster":"PI_Team","comment_id":"947874","timestamp":"1688975340.0","content":"Create an Azure Databricks cluster to provide a cloud environment for data scientists to build their notebooks.\n\nInstall the Azure ML SDK for Python on the cluster to enable data scientists to use automatic feature engineering and model building in machine learning pipelines.\n\nCreate and execute the Zeppelin notebooks on the cluster to build and train machine learning models using Spark instances with dynamic worker allocation.\n\nWhen the cluster is ready, export Zeppelin notebooks to a local environment to enable version control of the notebooks locally.\n\nSaM"},{"timestamp":"1675303500.0","comment_id":"795681","content":"Here is the most accurate sequence of actions for creating the desired environment:\nCreate an Azure Databricks cluster\nInstall Microsoft Machine Learning for Apache Spark on the cluster\nCreate and execute Jupyter notebooks using AutoML on the cluster\nWhen the cluster is ready and has processed the notebook, export your Jupyter notebook to a local environment for version control.\nThis sequence of actions will allow you to take advantage of the Azure Databricks platform for cloud-based data processing, and the Microsoft Machine Learning for Apache Spark library for automating feature engineering and model building in your Jupyter notebooks. Additionally, exporting the notebooks to a local environment will allow you to version control them and collaborate with other team members.","upvote_count":"7","poster":"phdykd"},{"poster":"shubhangi2612","comment_id":"781200","content":"https://industry40.co.in/azure-hdinsight-and-azure-databricks/","upvote_count":"3","timestamp":"1674136920.0"},{"poster":"ning","content":"Totally agree \n\n1. Create Azure Databricks cluster \n2. Install Azure ML SDK for Python \n3. Create and exec Jupyter notebook using AutoML \n4. Export Jupyter to local env","upvote_count":"4","timestamp":"1652260800.0","comment_id":"600004"},{"upvote_count":"3","timestamp":"1648661520.0","comment_id":"578436","poster":"DingDongSingSong","content":"Reference this link: https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml\n\nAnswer is as per DASIST noted:\n1. Create Azure Databricks cluster\n2. Install Azure ML SDK for Python\n3. Create and exec Jupyter notebook using AutoML\n4. Export Jupyter to local env\n\nAnother link that supports this rationale is :https://industry40.co.in/azure-hdinsight-and-azure-databricks/. It clearly outlines why for Spark based environments, Databricks is a better option than HDInsight"},{"timestamp":"1638935100.0","poster":"ajayjha123","content":"Should be: Create Azure Databricks cluster -> Install Azure ML SDK for Python -> Create and exec Jupyter notebook using AutoML -> Export Jupyter to local env. That because you need auto feature engineering provided by autoML","upvote_count":"3","comment_id":"496506"},{"content":"I am still confused on the right answer.","timestamp":"1633909140.0","upvote_count":"2","poster":"[Removed]","comment_id":"460258"},{"upvote_count":"4","content":"agree: Create Azure Databricks cluster ->\nCreate and exec Jupyter notebook using AutoML ->\nInstall Azure ML SDK for Python ->\nExport Jupyter to local env","comment_id":"449190","timestamp":"1632263220.0","poster":"RyanTsai"},{"poster":"dija123","timestamp":"1629283920.0","comment_id":"426796","upvote_count":"3","content":"Create Azure Databricks cluster -> \nCreate and exec Jupyter notebook using AutoML ->\nInstall Azure ML SDK for Python -> \n Export Jupyter to local env"},{"content":"If anyone wants all questions ping me 9403778084","comment_id":"398412","poster":"Akki0120","comments":[{"content":"Can you please share the email","comment_id":"437344","upvote_count":"1","timestamp":"1630515360.0","poster":"Lutendo"}],"timestamp":"1625407560.0","upvote_count":"3"},{"comment_id":"294597","timestamp":"1613768040.0","poster":"tamoor","upvote_count":"1","content":"you can use only azure hdinsights because of condition you can use only apache-spark\nfor data bricks, you must use Hadoop."},{"content":"I believe Data Bricks is capable, but if you choose that as first step, there is no further actions can be chosen, which all around Zeppelin, but Data Bricks doesn't support Zeppelin.","comments":[{"comment_id":"279675","poster":"Srivathsan","upvote_count":"1","content":"https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect#:~:text=Databricks%20Connect%20allows%20you%20to,applications%20to%20Azure%20Databricks%20clusters.\n\nFrom the above link, it is seen that Databricks can support Zepplin.","timestamp":"1611969780.0"},{"poster":"prashantjoge","content":"data bricks is a an analytics platform. it does not support feature engineering","comment_id":"362376","upvote_count":"1","timestamp":"1621530540.0"}],"comment_id":"242014","timestamp":"1607806740.0","upvote_count":"3","poster":"dzzz"},{"comments":[{"poster":"HkIsCrazY","comment_id":"284597","comments":[{"content":"the given answer is correct","poster":"prashantjoge","upvote_count":"3","comment_id":"362404","timestamp":"1621533300.0"}],"timestamp":"1612593000.0","upvote_count":"3","content":"No, HDinsight also provides all the autoML and auto feature engineering features\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-run-machine-learning-automl"}],"timestamp":"1606502580.0","content":"Azure data bricks meets all the requirements. HDInsight does not.\nExample: automatic feature engineering is included with autoML. HDinsight does not include this feature.\n\nHDinsight: https://docs.microsoft.com/en-us/azure/hdinsight/\n\nAzure Databricks: https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/automl-hyperparam-tuning/","poster":"valkyrieShadow","upvote_count":"3","comment_id":"229192"},{"comment_id":"206018","content":"Why not jupyter note book?","timestamp":"1603688460.0","comments":[{"comment_id":"331829","comments":[{"content":"No, it supports too.","upvote_count":"1","comment_id":"567439","poster":"zehraoneexam","timestamp":"1647239520.0"}],"poster":"LakeSky","upvote_count":"1","content":"Maybe because jupyter notebook don't provide intepreter for Spark like Zeppelin?\nhttps://medium.com/ankitakumar140494/a-comprehensive-comparison-between-jupyter-notebook-and-apache-zeppelin-911501981bfb","timestamp":"1617955140.0"}],"upvote_count":"1","poster":"Karen_12321"},{"comment_id":"179595","comments":[{"poster":"sayak17","comment_id":"179603","timestamp":"1600134060.0","upvote_count":"1","content":"Why would both spark mllib and mmlspark be required? I feel first option will be databricks"}],"poster":"sayak17","timestamp":"1600133220.0","upvote_count":"2","content":"first option can be Azure Databricks also right? It can also do all of the steps after that."}],"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/31300-exam-dp-100-topic-2-question-3-discussion/","answer_images":["https://img.examtopics.com/dp-100/image596.png"],"exam_id":64,"unix_timestamp":1600133220,"question_images":["https://www.examtopics.com/assets/media/exam-media/04274/0005300001.png"],"answer":"","isMC":false,"answer_description":"","answers_community":[]},{"id":"i0UaPnKINRwWA9uSJKaw","question_images":[],"topic":"2","answer":"A","discussion":[{"upvote_count":"29","content":"I think Answer is 'A'\n\nIf you select the Quantiles binning mode, use the Quantile normalization option to determine how values are normalized prior to sorting into quantiles. Note that normalizing values transforms the values, but does not affect the final number of bins. For an example, see Effects of Different Normalization Methods.\n\nThe following normalization types are supported:\n\nPercent: Values are normalized within the range [0,100]\n\nPQuantile: Values are normalized within the range [0,1]\n\nQuantileIndex: Values are normalized within the range [1,number of bins]","timestamp":"1616001840.0","poster":"kty","comment_id":"313474"},{"poster":"l2azure","comment_id":"331856","timestamp":"1617961080.0","content":"Answer is correct, only the binning mode 'Entropy MDL' takes into account the target column.\nSee documentation:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/group-data-into-bins","comments":[{"comment_id":"922147","timestamp":"1686652800.0","content":"\"You need to normalize values to produce an output column into bins to predict a target column.\" =/= we need to take into consideration the target column when binning","upvote_count":"1","poster":"lcgcastro96"},{"poster":"azurelearner666","timestamp":"1649596380.0","content":"Wrong, it is A. and the proper way to normalize the values is with Quantile normalization\n\"If you select the Quantiles binning mode, use the Quantile normalization option to determine how values are normalized prior to sorting into quantiles. Note that normalizing values transforms the values, but does not affect the final number of bins\"\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/group-data-into-bins\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/group-data-into-bins#bkmk_Effects","comment_id":"583701","upvote_count":"3"}],"upvote_count":"13"},{"poster":"ashvinn","comment_id":"1259296","timestamp":"1722505740.0","upvote_count":"1","content":"A is to go"},{"timestamp":"1702384860.0","content":"Selected Answer: A\nThe Quantiles normalization module in Azure ML Studio normalizes values based on quantiles into a specified number of bins. The QuantileIndex normalization module further converts those binned quantiles into indices values.\n\nTogether these two normalization modules can take an input column and transform it into quantile indices, binning the original values.\n\nThis quantile binning allows the feature to then be used for targets prediction, meeting that part of the stated goal as well.","upvote_count":"2","poster":"NullVoider_0","comment_id":"1094490"},{"upvote_count":"2","content":"Selected Answer: B\nThe answer is B. No.\n\nQuantiles normalization is a technique that is used to normalize values by dividing the range of values into equal-sized intervals, or quantiles. QuantileIndex normalization is a technique that is used to normalize values by assigning each value to a quantile based on its rank.\n\nIn this case, the goal is to normalize values to produce an output column into bins to predict a target column. However, the solution of applying a Quantiles normalization with a QuantileIndex normalization will not achieve this goal. Instead, you should use a normalization technique that is specifically designed for binning, such as Equal Width Binning or Equal Frequency Binning.","comment_id":"960303","poster":"Ahmed_Gehad","timestamp":"1690105560.0"},{"poster":"phdykd","upvote_count":"2","timestamp":"1679163540.0","content":"B. No.\nQuantiles normalization is a normalization technique that transforms the values in a dataset to have a specified distribution, usually a normal distribution. It is not used to produce an output column into bins.\n\nQuantile Index normalization is a technique that transforms a dataset so that its values lie between 0 and 1, using the minimum and maximum values in the dataset. It is also not used to produce an output column into bins.\n\nTo produce an output column into bins, one could use the Discretize module in Azure Machine Learning Studio.","comment_id":"843050"},{"poster":"DingDongSingSong","upvote_count":"4","content":"The question is poorly worded:\nYou need to normalize values to produce an output column into bins to predict a target column.\n\nWhy would you bin an output (target) column when you're projecting a target column? That just doesn't make sense.","comment_id":"579148","timestamp":"1648756980.0"},{"timestamp":"1638379320.0","poster":"dija123","upvote_count":"2","content":"Selected Answer: A\nI go with A","comment_id":"491837"},{"upvote_count":"3","comment_id":"461853","poster":"v06ayxop1","content":"Entropy MDL is not yet available in new Azure ML but only in Azure ML classic.\nhttps://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/group-data-into-bins#how-to-configure-group-data-into-bins","timestamp":"1634193900.0"},{"comment_id":"445052","poster":"jitsblitz","content":"\"You need to normalize values to produce an output column into bins to predict a target column.\"\nOnly Quantiles binning normalizes. You are not asked to use target column for binning. Dont get confused with just the mention of target column. All feature selection is to predict target column only. Entropy MDL uses Target column to determine number of bins.","upvote_count":"8","timestamp":"1631688960.0"},{"upvote_count":"1","poster":"Askme101","timestamp":"1608988020.0","comment_id":"252620","content":"Yes answer is correct as 'a' as this requires a target column"}],"exam_id":64,"question_id":103,"isMC":true,"choices":{"A":"Yes","B":"No"},"answers_community":["A (67%)","B (33%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/40807-exam-dp-100-topic-2-question-30-discussion/","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are a data scientist using Azure Machine Learning Studio.\nYou need to normalize values to produce an output column into bins to predict a target column.\nSolution: Apply a Quantiles normalization with a QuantileIndex normalization.\nDoes the solution meet the goal?","answer_images":[],"answer_ET":"A","answer_description":"","unix_timestamp":1608988020,"timestamp":"2020-12-26 14:07:00"},{"id":"PMzYkNp6Y1yBlHd266ag","discussion":[{"upvote_count":"7","comment_id":"583706","poster":"azurelearner666","timestamp":"1665408540.0","content":"Response is correct, B.\nThe \"Scale and Reduce sampling mode\" will not be able to compensate for the class imbalance, so B (no) is the right Answer.\n\nThe proper response should be SMOTE. \nMore info on https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/smote"},{"content":"Correct af","timestamp":"1640346660.0","upvote_count":"6","poster":"Tehseen","comment_id":"389408"},{"poster":"windy610","content":"should use SMOTE","upvote_count":"1","timestamp":"1718435640.0","comment_id":"1097154"},{"poster":"nick234987","comment_id":"461429","content":"The correct answer is B","upvote_count":"2","timestamp":"1649836860.0"}],"answer_images":[],"timestamp":"2021-06-24 11:51:00","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are creating a new experiment in Azure Machine Learning Studio.\nOne class has a much smaller number of observations than the other classes in the training set.\nYou need to select an appropriate data sampling strategy to compensate for the class imbalance.\nSolution: You use the Scale and Reduce sampling mode.\nDoes the solution meet the goal?","isMC":true,"question_id":104,"answers_community":[],"answer_ET":"B","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/55967-exam-dp-100-topic-2-question-31-discussion/","answer_description":"Instead use the Synthetic Minority Oversampling Technique (SMOTE) sampling mode.\nNote: SMOTE is used to increase the number of underepresented cases in a dataset used for machine learning. SMOTE is a better way of increasing the number of rare cases than simply duplicating existing cases.\nIncorrect Answers:\nCommon data tasks for the Scale and Reduce sampling mode include clipping, binning, and normalizing numerical values.\nReference:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/smote https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/data-transformation-scale-and-reduce","answer":"B","unix_timestamp":1624528260,"topic":"2","choices":{"A":"Yes","B":"No"},"exam_id":64},{"id":"rcnOQm0Wxrk5xdiMTDpS","answer_images":[],"question_images":[],"exam_id":64,"answer_description":"","discussion":[{"timestamp":"1649934360.0","comments":[{"timestamp":"1696944720.0","poster":"azurelearner666","comment_id":"583709","content":"Fully agree, sometimes the questions are a bit \"off\" (and that's polite...)","upvote_count":"2"},{"poster":"kty","content":":))))))","timestamp":"1663429920.0","upvote_count":"1","comment_id":"313489"},{"poster":"Arend78","upvote_count":"2","comment_id":"738768","content":"Indeed, very confusing. Thanks for mentioning this!","timestamp":"1717828080.0"}],"poster":"lucazav","comment_id":"199729","content":"Summarize Data module output also has the following columns: P0.5, P1, P5, P95, P99, P99.5\nProbably the person in charge of these tests was not very prepared and did not even check Microsoft's docs, so he confused \"percentile\" values with \"p-\" values. By the way, \"p-values\" of what!?","upvote_count":"32"},{"poster":"phdykd","comment_id":"796698","upvote_count":"11","content":"C. Execute Python Script\nE. Summarize Data\n\nIn Azure Machine Learning Studio, you can use the \"Summarize Data\" module to generate a statistical summary that includes unique count and basic statistics such as mean, standard deviation, minimum, maximum, and quartiles for each feature column. To calculate p-value, you would need to use a \"Execute Python Script\" module and write a custom code to perform the hypothesis test and compute the p-value.\nThe \"Export Count Table\" module in Azure Machine Learning Studio only provides a count of the number of instances of each unique value in a single feature column, it doesn't provide statistical summary information like the mean, standard deviation, minimum, maximum, quartiles, or p-value. To get the p-value and a more comprehensive statistical summary, you would need to use the \"Execute Python Script\" module and write custom code as described in my previous answer.","timestamp":"1722650100.0"},{"timestamp":"1731310920.0","content":"Selected Answer: CE\nC. Execute Python Script\nE. Summarize Data\n\nThe \"Execute Python Script\" module allows you to run your own Python script, which can be used to calculate any statistic, including unique counts and p-values.\n\nThe \"Summarize Data\" module in Azure Machine Learning Studio provides a statistical summary of the input dataset, including count, mean, mode, standard deviation, minimum, and maximum.\n\nAnd like somebody mentioned, it should be percentile and not p-value.","comment_id":"894602","poster":"MarinaMijailovic","upvote_count":"3"},{"poster":"Edriv","timestamp":"1718293140.0","upvote_count":"1","comment_id":"744321","content":"Options AD"},{"upvote_count":"4","comment_id":"313492","poster":"kty","content":"Answer is just 'C'","timestamp":"1663430040.0"},{"poster":"saurabhk1","content":"Actually, the question is not clear about which type of p-value, it wants to calculate. \nOnly module that can do it is \"Execute a python script\"","comment_id":"303357","upvote_count":"4","timestamp":"1662292380.0"},{"upvote_count":"3","timestamp":"1656242100.0","poster":"Askme101","comment_id":"252624","content":"it should say percentile values"},{"content":"Isn't \"Execute a python script\" actually the only option to calculate the p-value?","timestamp":"1641195180.0","poster":"modschegiebsch","comment_id":"125376","upvote_count":"7"}],"choices":{"D":"Convert to Indicator Values","A":"Computer Linear Correlation","E":"Summarize Data","B":"Export Count Table","C":"Execute Python Script"},"question_text":"You are analyzing a dataset by using Azure Machine Learning Studio.\nYou need to generate a statistical summary that contains the p-value and the unique count for each feature column.\nWhich two modules can you use? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","url":"https://www.examtopics.com/discussions/microsoft/view/24662-exam-dp-100-topic-2-question-32-discussion/","question_id":105,"unix_timestamp":1593754380,"isMC":true,"answers_community":["CE (100%)"],"topic":"2","answer":"CE","answer_ET":"CE","timestamp":"2020-07-03 07:33:00"}],"exam":{"provider":"Microsoft","numberOfQuestions":512,"isBeta":false,"isImplemented":true,"id":64,"isMCOnly":false,"name":"DP-100","lastUpdated":"12 Apr 2025"},"currentPage":21},"__N_SSP":true}