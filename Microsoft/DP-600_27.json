{"pageProps":{"questions":[{"id":"ZU9iJhtc33Gq7e80MEa1","answer_ET":"B","answer_description":"","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have a Fabric tenant that contains a new semantic model in OneLake.\nYou use a Fabric notebook to read the data into a Spark DataFrame.\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.\nSolution: You use the following PySpark expression:\ndf.show()\nDoes this meet the goal?","timestamp":"2024-02-17 21:44:00","question_id":131,"exam_id":71,"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/134076-exam-dp-600-topic-1-question-55-discussion/","discussion":[{"upvote_count":"3","content":"Selected Answer: B\nCorrect methods: Use df.describe().show() for basic statistics and df.agg() with appropriate functions (min, max, mean, stddev) for detailed statistics.","comment_id":"1220294","timestamp":"1732807980.0","poster":"282b85d"},{"poster":"stilferx","comment_id":"1209141","content":"Selected Answer: B\nIMHO, NOOOOO\n\ndf.show() - shows the data in the dataframe","timestamp":"1731208920.0","upvote_count":"2"},{"upvote_count":"2","timestamp":"1726928160.0","content":"Selected Answer: B\nUse describe","poster":"a_51","comment_id":"1179394"},{"comment_id":"1160837","poster":"XiltroX","content":"Selected Answer: B\ndf.summary() is the only right answer.","upvote_count":"1","timestamp":"1724770620.0"},{"content":"Selected Answer: B\nThe correct syntax is df.describe().\n\nSources:\n* describe --> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html\n* show --> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html","upvote_count":"2","timestamp":"1723963380.0","poster":"SamuComqi","comment_id":"1153122","comments":[{"poster":"SamuComqi","comment_id":"1153124","timestamp":"1723963500.0","content":"Also df.summary() is a valid solution.\n\nSource ---> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.summary.html","upvote_count":"1"}]},{"poster":"Momoanwar","timestamp":"1723920240.0","comment_id":"1152809","content":"Selected Answer: B\nNo show is to display data","upvote_count":"1"}],"choices":{"B":"No","A":"Yes"},"answer":"B","question_images":[],"answer_images":[],"isMC":true,"topic":"1","unix_timestamp":1708202640},{"id":"ChfyUR1AylkvSFa91OnX","choices":{"B":"No","A":"Yes"},"unix_timestamp":1708202640,"answer":"A","url":"https://www.examtopics.com/discussions/microsoft/view/134077-exam-dp-600-topic-1-question-56-discussion/","topic":"1","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have a Fabric tenant that contains a new semantic model in OneLake.\nYou use a Fabric notebook to read the data into a Spark DataFrame.\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.\nSolution: You use the following PySpark expression:\ndf.summary()\nDoes this meet the goal?","question_images":[],"answer_images":[],"answer_ET":"A","answer_description":"","discussion":[{"upvote_count":"9","timestamp":"1715304300.0","content":"IMHO, A\n\nExample:\ndf1 = spark.createDataFrame([(1, 10), (2, 10), (2, 15)], schema = ['fruit_id', 'amount'])\ndf1.summary()\n\nsummary fruit_id amount\ncount 3 3\nmean 1.6666666666666667 11.666666666666666\nstddev 0.5773502691896257 2.886751345948129\nmin 1 10\n25% 1 10\n50% 2 10\n75% 2 15\nmax 2 15","comment_id":"1209142","poster":"stilferx"},{"comment_id":"1559502","poster":"Lotusss","timestamp":"1744268700.0","content":"Selected Answer: B\ndf.summary() alone wont cut it. \ndf.summary().show() is correct. So anwser is B","upvote_count":"1"},{"comment_id":"1354687","timestamp":"1739227800.0","content":"Selected Answer: B\nThe correct PySpark expression to calculate min, max, mean, and standard deviation for both numeric and STRING columns is:\ndf.describe()","upvote_count":"1","poster":"b01d700"},{"upvote_count":"1","poster":"slu239","timestamp":"1735407240.0","comment_id":"1333073","content":"Selected Answer: B\nNot meet the goal because it has to be df.summary().show()"},{"comment_id":"1326089","poster":"2fe10ed","content":"Selected Answer: A\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.summary.html#pyspark.sql.DataFrame.summary","timestamp":"1734087900.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"1258411","poster":"Pegooli","comments":[{"upvote_count":"1","comment_id":"1277361","content":"so the questions doesn't make sense if you are asked to calculate things that aren't defined","poster":"gover07","timestamp":"1725350760.0"}],"content":"Selected Answer: B\nUsing df.summary() in PySpark will provide summary statistics, including min, max, mean, and standard deviation for all numeric columns. However, it will not provide these statistics for string columns since summary statistics like min, max, mean, and standard deviation are not applicable to string data.","timestamp":"1722373680.0"},{"timestamp":"1720828140.0","content":"Selected Answer: A\nCorrect","poster":"6d1de25","comment_id":"1247058","upvote_count":"1"},{"timestamp":"1720460640.0","content":"Selected Answer: A\nIn pandas, use df.describe() for summary statistics of numeric columns.\nIn PySpark, use df.summary() for summary statistics of both numeric and string columns in a distributed computing environment.","upvote_count":"3","comment_id":"1244497","poster":"7d97b62"},{"timestamp":"1716903360.0","content":"Selected Answer: B\nwhile df.summary() does provide valuable information for numeric columns, it does not fully meet the goal of evaluating both string and numeric columns with the required statistical measures. Use df.summary() and df.agg() to cover numeric columns, and additional custom aggregations for string columns.","upvote_count":"4","comment_id":"1220297","poster":"282b85d"},{"poster":"XiltroX","timestamp":"1709052960.0","comment_id":"1160835","upvote_count":"1","content":"df.summary() is the only option where you can get MIX, MAX and AVG"},{"comment_id":"1153126","poster":"SamuComqi","timestamp":"1708246020.0","upvote_count":"4","content":"Selected Answer: A\nAlso df.describe() is a valid solution.\n\nSources:\n* summary --> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.summary.html\n* describe --> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html"},{"upvote_count":"2","comment_id":"1152811","timestamp":"1708202640.0","poster":"Momoanwar","content":"Selected Answer: A\nCorrect"}],"answers_community":["A (55%)","B (45%)"],"exam_id":71,"timestamp":"2024-02-17 21:44:00","question_id":132,"isMC":true},{"id":"8XSGhsRmR0HdoNscP2Zo","url":"https://www.examtopics.com/discussions/microsoft/view/134079-exam-dp-600-topic-1-question-57-discussion/","choices":{"B":"No","A":"Yes"},"answer_description":"","isMC":true,"timestamp":"2024-02-17 22:07:00","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table named Customer.\nWhen you query Customer, you discover that the query is slow to execute. You suspect that maintenance was NOT performed on the table.\nYou need to identify whether maintenance tasks were performed on Customer.\nSolution: You run the following Spark SQL statement:\n\nDESCRIBE HISTORY customer -\nDoes this meet the goal?","exam_id":71,"unix_timestamp":1708204020,"answer_ET":"A","answer_images":[],"question_images":[],"discussion":[{"comment_id":"1293806","content":"OPTIMIZE and VACUUM activities in the last 30 days are stored in the history. So, yes, you can see using this query whether maintenance took place in the last 30 days. https://learn.microsoft.com/en-us/azure/databricks/delta/history","poster":"Martin_Nbg","upvote_count":"3","timestamp":"1728206820.0"},{"comment_id":"1220301","content":"Selected Answer: A\nYes, running DESCRIBE HISTORY customer meets the goal of identifying whether maintenance tasks were performed on the Delta table.","poster":"282b85d","upvote_count":"3","timestamp":"1716903540.0"},{"timestamp":"1715304720.0","poster":"stilferx","content":"Selected Answer: A\nIMHO, A\n\nLink: https://learn.microsoft.com/en-us/azure/databricks/delta/history","comment_id":"1209148","upvote_count":"4"},{"timestamp":"1711038240.0","poster":"a_51","upvote_count":"2","comment_id":"1179411","content":"Selected Answer: A\nA correct.\ndisplay(spark.sql('describe history customer'))"},{"content":"Selected Answer: A\nRight answer. Describe HISTORY","timestamp":"1709053080.0","comment_id":"1160839","upvote_count":"2","poster":"XiltroX"},{"upvote_count":"2","poster":"Momoanwar","timestamp":"1708204020.0","content":"Selected Answer: A\nCorrect","comment_id":"1152819"}],"topic":"1","answers_community":["A (100%)"],"question_id":133,"answer":"A"},{"id":"fCFovsrVCjWpEf50LSDJ","isMC":true,"choices":{"A":"Yes","B":"No"},"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/134080-exam-dp-600-topic-1-question-58-discussion/","discussion":[{"comment_id":"1220302","poster":"282b85d","content":"Selected Answer: B\nNo, running REFRESH TABLE customer does not meet the goal of identifying whether maintenance tasks were performed on the Delta table. This Spark SQL command is used to refresh the metadata of a table. It ensures that the latest schema and data are available for queries but does not give any historical information about maintenance operations.","timestamp":"1732808460.0","upvote_count":"6"},{"comment_id":"1209149","upvote_count":"1","content":"Selected Answer: B\nIMHO, NOOO","timestamp":"1731209520.0","poster":"stilferx"},{"timestamp":"1730105820.0","comment_id":"1203416","upvote_count":"1","poster":"dp600","content":"Selected Answer: B\ncorrect"},{"poster":"Momoanwar","upvote_count":"1","content":"Selected Answer: B\nCorrect","timestamp":"1723921800.0","comment_id":"1152820"}],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table named Customer.\nWhen you query Customer, you discover that the query is slow to execute. You suspect that maintenance was NOT performed on the table.\nYou need to identify whether maintenance tasks were performed on Customer.\nSolution: You run the following Spark SQL statement:\n\nREFRESH TABLE customer -\nDoes this meet the goal?","question_images":[],"exam_id":71,"timestamp":"2024-02-17 22:10:00","topic":"1","unix_timestamp":1708204200,"question_id":134,"answers_community":["B (100%)"],"answer":"B","answer_ET":"B"},{"id":"SjoXy9r9KnlcUcpFIdAP","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/134083-exam-dp-600-topic-1-question-59-discussion/","isMC":true,"exam_id":71,"answer_description":"","topic":"1","question_images":[],"timestamp":"2024-02-17 22:16:00","answer":"B","answer_ET":"B","discussion":[{"poster":"282b85d","content":"Selected Answer: B\nThe EXPLAIN statement in Spark SQL is used to display the execution plan of a query. This plan shows how Spark will execute the query, including details about the operations and stages involved. While it is useful for understanding and optimizing query performance, it does not provide historical information about maintenance tasks like optimization, compaction, or vacuuming performed on the table.","comment_id":"1220304","timestamp":"1732808580.0","upvote_count":"7"},{"timestamp":"1731209580.0","upvote_count":"1","content":"Selected Answer: B\nIMHO, NOOOO","poster":"stilferx","comment_id":"1209150"},{"comment_id":"1166773","upvote_count":"2","timestamp":"1725564960.0","poster":"wellingtonluis","content":"Selected Answer: B\nGood luck !!!"},{"comment_id":"1163149","content":"Selected Answer: B\nCorrect answer is DESCRIBE HISTORY customer. \nGood luck everyone.","upvote_count":"2","poster":"XiltroX","timestamp":"1725146220.0"},{"poster":"earlqq","upvote_count":"1","content":"Correct","timestamp":"1724523660.0","comment_id":"1158137"},{"upvote_count":"2","content":"Selected Answer: B\nGiven answer is correct. Explain is for query execution plan","timestamp":"1723922160.0","poster":"Momoanwar","comment_id":"1152826"}],"unix_timestamp":1708204560,"choices":{"A":"Yes","B":"No"},"question_id":135,"answer_images":[],"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table named Customer.\nWhen you query Customer, you discover that the query is slow to execute. You suspect that maintenance was NOT performed on the table.\nYou need to identify whether maintenance tasks were performed on Customer.\nSolution: You run the following Spark SQL statement:\n\nEXPLAIN TABLE customer -\nDoes this meet the goal?"}],"exam":{"id":71,"lastUpdated":"12 Apr 2025","isBeta":false,"isMCOnly":false,"isImplemented":true,"name":"DP-600","numberOfQuestions":179,"provider":"Microsoft"},"currentPage":27},"__N_SSP":true}