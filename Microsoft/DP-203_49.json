{"pageProps":{"questions":[{"id":"TWnSPmY4wlmiS3d8vU9D","answer_ET":"C","question_images":[],"exam_id":67,"timestamp":"2022-09-06 22:30:00","answer":"C","choices":{"D":"a remote service binding","B":"an external library","C":"database scoped credentials","A":"an external resource pool"},"answer_description":"","discussion":[{"upvote_count":"5","poster":"anks84","timestamp":"1694032200.0","comment_id":"661640","content":"Selected Answer: C\nCorrect Answer !"},{"poster":"kkk5566","upvote_count":"2","content":"Selected Answer: C\ncorrect","timestamp":"1724997840.0","comment_id":"993744"},{"content":"Selected Answer: C\nCorrect","upvote_count":"2","timestamp":"1717300740.0","comment_id":"912513","poster":"Ankit_Az"},{"timestamp":"1706691540.0","poster":"GodfreyMbizo","upvote_count":"2","content":"correct answer","comment_id":"793874","comments":[{"content":"database scoped credentials first","poster":"GodfreyMbizo","upvote_count":"3","timestamp":"1706691540.0","comment_id":"793875"}]}],"answers_community":["C (100%)"],"unix_timestamp":1662496200,"question_text":"You have an Azure Synapse Analytics serverless SQL pool named Pool1 and an Azure Data Lake Storage Gen2 account named storage1. The\nAllowBlobPublicAccess property is disabled for storage1.\nYou need to create an external data source that can be used by Azure Active Directory (Azure AD) users to access storage from Pool1.\nWhat should you create first?","question_id":241,"url":"https://www.examtopics.com/discussions/microsoft/view/80722-exam-dp-203-topic-2-question-83-discussion/","topic":"2","answer_images":[],"isMC":true},{"id":"CcK50pqYJG7py3XpT5NC","answers_community":["A (100%)"],"answer":"A","topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/79113-exam-dp-203-topic-2-question-84-discussion/","timestamp":"2022-09-01 19:32:00","answer_ET":"A","question_images":[],"answer_images":[],"question_id":242,"answer_description":"","isMC":true,"exam_id":67,"question_text":"You have an Azure Data Factory pipeline named Pipeline1. Pipeline1 contains a copy activity that sends data to an Azure Data Lake Storage Gen2 account.\nPipeline1 is executed by a schedule trigger.\nYou change the copy activity sink to a new storage account and merge the changes into the collaboration branch.\nAfter Pipeline1 executes, you discover that data is NOT copied to the new storage account.\nYou need to ensure that the data is copied to the new storage account.\nWhat should you do?","unix_timestamp":1662053520,"choices":{"D":"Configure the change feed of the new storage account.","A":"Publish from the collaboration branch.","C":"Modify the schedule trigger.","B":"Create a pull request."},"discussion":[{"comment_id":"1243779","content":"Selected Answer: A\nA is correct","timestamp":"1720347780.0","poster":"evangelist","upvote_count":"1"},{"comment_id":"1241305","poster":"learnwell","upvote_count":"1","timestamp":"1719998760.0","content":"Selected Answer: A\nThe \"schedule trigger\" always runs in live mode, hence it will take the storage account name which is available in the live mode. Now on the other hand, when the changes in storage account name is done and merged to collaboration branch(by raising a PULL REQUEST) , it is still not yet available in ADF live mode which we need to do manually by publishing from the collaboration branch.","comments":[{"upvote_count":"1","poster":"learnwell","timestamp":"1719999300.0","comment_id":"1241307","content":"So basically the data did not get copied to new storage account because in live mode the trigger still holds the old storage account name and most certainly the data got copied to the old storage account when the scheduled trigger ran. So we need to publish \"from\" Collaboration branch \"to\" ADF live mode so that the updated storage account name gets available in the ADF live mode for the scheduled trigger to pick the NEW storage account name as parameter when it triggers next time."}]},{"upvote_count":"1","poster":"werfragt","comment_id":"1168253","comments":[{"content":"because you merged the pipeline to the collaboration branch, but since you didn't publish the changes, the pipeline didn't copy the data to the new storage account. Think of publish as deploying to live","upvote_count":"6","poster":"mav2000","comment_id":"1170229","timestamp":"1710065640.0"}],"content":"I don't get it. It is saying: \n\"After Pipeline1 executes, you discover that data is NOT copied to the new storage account.\nYou need to ensure that the data is copied to the new storage account.\"\nSo this means, the pipeline does not work. How on earth would it make sense to publish a pipeline which is not working?","timestamp":"1709835240.0"},{"comment_id":"993745","poster":"kkk5566","content":"Selected Answer: A\ncorrect","upvote_count":"2","timestamp":"1693375680.0"},{"poster":"ludaka","timestamp":"1687179000.0","upvote_count":"2","content":"Selected Answer: A\nCorrect answer.","comment_id":"927494"},{"comment_id":"895239","upvote_count":"1","timestamp":"1683824100.0","comments":[{"poster":"auwia","timestamp":"1687511400.0","upvote_count":"3","content":"I guessed the same, but in adf publish step is manually and not automatic, so when the question says code is merged, you can automatically assume that pull request was done and also approved by reviewers. Therefore the correct answer for me it's A.","comment_id":"931398"}],"content":"I selected B, pull request","poster":"kim32"},{"content":"I had heard \"publish to\", never heard of \"publish from\". So confused.","poster":"Xinyuehong","timestamp":"1665782820.0","comment_id":"695019","upvote_count":"2","comments":[{"timestamp":"1669417380.0","comment_id":"727127","content":"probably it was meant to publish from collaboration branch to adf_publish branch","upvote_count":"2","poster":"Igor85"}]},{"poster":"debarun","comments":[{"content":"Because the pull request is already implicit in the statement as it is said to be merged into the collaborating branch:\n\"You change the copy activity sink to a new storage account and MERGE the CHANGES INTO the COLLABORATION BRANCH.\"","poster":"DataEX","comment_id":"657521","upvote_count":"12","timestamp":"1662128940.0"}],"content":"Why not B ?","timestamp":"1662053520.0","comment_id":"656504","upvote_count":"1"}]},{"id":"ZwJQUz5QWRValw7YXtVI","discussion":[{"poster":"Azurre","content":"Correct Answer: D \n Offset of \"-01:00:00\" indicates to start the next trigger instance only after the previous trigger instance completes, and size of \"01:00:00\" indicates to wait for 1 hour after the previous trigger instance completes before starting the next one.","timestamp":"1710572040.0","upvote_count":"8","comment_id":"840604"},{"poster":"Okea","comment_id":"762657","timestamp":"1704028500.0","upvote_count":"6","content":"Answer: D\noffset \nOffset of the dependency trigger. Provide a value in time span format and both negative and positive offsets are allowed. This property is mandatory if the trigger is depending on itself and in all other cases it is optional. Self-dependency should always be a negative offset. If no value specified, the window is the same as the trigger itself.\n\nsize \nSize of the dependency tumbling window. Provide a positive timespan value. This property is optional.\nhttps://learn.microsoft.com/en-us/azure/data-factory/tumbling-window-trigger-dependency"},{"poster":"kkk5566","upvote_count":"1","comment_id":"993747","content":"Selected Answer: D\nD is correct","timestamp":"1724998320.0"},{"upvote_count":"4","timestamp":"1718488620.0","content":"Selected Answer: D\nOffset: \"-01:00:00\"\nSize: \"01:00:00\"\n\nThis configuration ensures that the trigger waits for the completion of the previous execution before starting a new one. The offset of \"-01:00:00\" indicates that the trigger should start one hour before the current time, and the size of \"01:00:00\" indicates that the trigger should have a duration of one hour.\n\nTherefore, the correct option is:\n\nD. offset: \"-01:00:00\" size: \"01:00:00\"","poster":"vctrhugo","comment_id":"924606"},{"upvote_count":"5","timestamp":"1694356740.0","content":"Selected Answer: D\n\"dependsOn\": [\n {\n \"type\": \"SelfDependencyTumblingWindowTriggerReference\",\n \"size\": \"01:00:00\",\n \"offset\": \"-01:00:00\"\n }","comment_id":"665481","poster":"dom271219"}],"unix_timestamp":1662820740,"answer":"D","answers_community":["D (100%)"],"answer_images":[],"answer_description":"","isMC":true,"choices":{"D":"offset: \"-01:00:00\" size: \"01:00:00\"","C":"offset: \"01:00:00\" size: \"01:00:00\"","A":"offset: \"-00:01:00\" size: \"00:01:00\"","B":"offset: \"01:00:00\" size: \"-01:00:00\""},"answer_ET":"D","question_text":"You have an Azure Data Factory pipeline named pipeline1 that is invoked by a tumbling window trigger named Trigger1. Trigger1 has a recurrence of 60 minutes.\nYou need to ensure that pipeline1 will execute only if the previous execution completes successfully.\nHow should you configure the self-dependency for Trigger1?","question_images":[],"timestamp":"2022-09-10 16:39:00","topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/81557-exam-dp-203-topic-2-question-85-discussion/","exam_id":67,"question_id":243},{"id":"4w4QAReXVo4mnkcql9vW","answer_description":"Box 1: A Get Metadata activity -\nDynamically size data flow compute at runtime\nThe Core Count and Compute Type properties can be set dynamically to adjust to the size of your incoming source data at runtime. Use pipeline activities like\nLookup or Get Metadata in order to find the size of the source dataset data. Then, use Add Dynamic Content in the Data Flow activity properties.\n\nBox 2: Dynamic content -\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/control-flow-execute-data-flow-activity","question_id":244,"timestamp":"2022-09-07 01:11:00","answers_community":[],"answer_ET":"","isMC":false,"exam_id":67,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0027400002.jpg"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0027400001.jpg"],"discussion":[{"upvote_count":"15","content":"Correct :\nUse pipeline activities like Lookup or Get Metadata in order to find the size of the source dataset data. Then, use Add Dynamic Content in the Data Flow activity properties. You can choose small, medium, or large compute sizes. Optionally, pick \"Custom\" and configure the compute types and number of cores manually.","poster":"dom271219","timestamp":"1662821580.0","comment_id":"665485"},{"content":"Looks correct. Checked in the doc.","timestamp":"1667509140.0","comment_id":"710789","poster":"dmitriypo","upvote_count":"6"},{"upvote_count":"2","comment_id":"1255309","timestamp":"1721953740.0","content":"Copilot\nSent by Copilot:\nTo optimize the number of cores used by Dataflow1 in your Azure Synapse Analytics pipeline, you should configure the core count dynamically based on the size of the incoming source data. Here are the steps to achieve this:\n\nUse a Lookup or Get Metadata activity in your pipeline to determine the size of the source dataset in storage1.\nAdd Dynamic Content to the Data Flow activity properties to set the core count based on the size of the data","poster":"iceberge"},{"upvote_count":"1","timestamp":"1707489780.0","content":"Correct","poster":"Alongi","comment_id":"1145559"},{"content":"Get Metadata & Dynamic Content","comment_id":"993753","timestamp":"1693376280.0","upvote_count":"1","poster":"kkk5566"},{"upvote_count":"3","content":"Looks Correct !!","poster":"anks84","comment_id":"661703","timestamp":"1662505860.0"}],"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/80744-exam-dp-203-topic-2-question-86-discussion/","unix_timestamp":1662505860,"answer":"","question_text":"HOTSPOT -\nYou have an Azure Synapse Analytics pipeline named Pipeline1 that contains a data flow activity named Dataflow1.\nPipeline1 retrieves files from an Azure Data Lake Storage Gen 2 account named storage1.\nDataflow1 uses the AutoResolveIntegrationRuntime integration runtime configured with a core count of 128.\nYou need to optimize the number of cores used by Dataflow1 to accommodate the size of the files in storage1.\nWhat should you configure? To answer, select the appropriate options in the answer area.\nHot Area:\n//IMG//"},{"id":"01Gmr2nLQWxp0Y2QRIE0","question_id":245,"answer_ET":"A","choices":{"B":"No","A":"Yes"},"timestamp":"2021-05-16 22:51:00","unix_timestamp":1621198260,"question_images":[],"exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/52912-exam-dp-203-topic-2-question-87-discussion/","answer_images":[],"discussion":[{"timestamp":"1630314300.0","poster":"Amalbenrebai","comment_id":"435295","content":"- data engineers: high concurrency cluster\n- jobs: Standard cluster\n- data scientists: Standard cluster","comments":[{"upvote_count":"1","poster":"Egocentric","timestamp":"1650197100.0","comment_id":"587204","content":"agreed"},{"comments":[{"poster":"Aditya0891","content":"single node cluster and standard cluster are different. In single node cluster you only have 1 node which act as driver and worker node while in standard cluster you can have separate driver and worker node and for jobs you can use standard or high concurrency cluster as well. So the requirements are satisfied here","comment_id":"612649","upvote_count":"1","timestamp":"1654591500.0"}],"content":"Tell me one thing: is this answer 9jobs) based on the text:\n\"A Single Node cluster has no workers and runs Spark jobs on the driver node.\n\nIn contrast, a Standard cluster requires at least one Spark worker node in addition to the driver node to execute Spark jobs.\"?\nI dont understand the connection between worker noodes and the requirements given in the question about jobs workspace.","upvote_count":"1","comment_id":"446636","poster":"Julius7000","timestamp":"1631886000.0"},{"poster":"supriyako","timestamp":"1663571460.0","upvote_count":"2","comment_id":"673017","content":"Correct. Because jobs could be for Scala notebook, which is supported by Standard cluster mode"},{"content":"The issue is the jobs are going to be ran by multiple users i.e. engineers and scientists? So it needs to be hugi concurrency cluster?","upvote_count":"1","timestamp":"1683273900.0","poster":"gogosgh","comment_id":"889885","comments":[{"upvote_count":"2","poster":"auwia","timestamp":"1687511880.0","content":"If you enable high concurrency then all scale scripts doesn't works, so scientists will stop to work). Standard cluster is scalable, will support all jobs and users! ;-)","comment_id":"931407"}]}],"upvote_count":"91"},{"comment_id":"435849","content":"The answer must be A!","poster":"gangstfear","upvote_count":"34","timestamp":"1630366140.0"},{"timestamp":"1721152980.0","poster":"Danweo","upvote_count":"1","content":"Selected Answer: A\nA is correct","comment_id":"1249141"},{"timestamp":"1704097020.0","poster":"dakku987","comment_id":"1111027","upvote_count":"2","content":"Selected Answer: B\nwe need HC cluster for data engineer,data scientist,jobs"},{"poster":"kkk5566","comment_id":"993757","upvote_count":"1","timestamp":"1693376760.0","content":"Selected Answer: A\ncorrect\n- data engineers: high concurrency cluster\n- jobs: Standard cluster\n- data scientists: Standard cluster"},{"upvote_count":"4","content":"Selected Answer: A\nHigh concurrence doesn't support scala.","timestamp":"1687512420.0","comment_id":"931412","poster":"auwia"},{"upvote_count":"1","comment_id":"931367","content":"Selected Answer: A\nTrue, correct.","timestamp":"1687508640.0","poster":"auwia"},{"content":"Selected Answer: A\nSCALA = STANDARD","upvote_count":"5","poster":"Ast999","timestamp":"1677858480.0","comment_id":"828106"},{"poster":"allagowf","timestamp":"1667016600.0","comment_id":"706940","content":"Selected Answer: A\ndata scientists and Job --> Scala --> Standard cluster .","upvote_count":"3"},{"poster":"greenlever","upvote_count":"1","timestamp":"1665708600.0","comment_id":"694385","content":"Selected Answer: A\nCorrect"},{"timestamp":"1662506160.0","poster":"anks84","upvote_count":"1","content":"Selected Answer: A\nWe would need a Standard cluster for the jobs to support Scala. High-concurrecny cluster does not support Scala.\nHence, the Answer is A !","comment_id":"661710"},{"poster":"Deeksha1234","timestamp":"1659162000.0","content":"Selected Answer: B\nthe answer should be No","comments":[{"content":"sorry 'A' should be correct","poster":"Deeksha1234","upvote_count":"3","comment_id":"647019","timestamp":"1660537440.0"}],"comment_id":"639510","upvote_count":"1"},{"comments":[{"upvote_count":"2","comment_id":"1007234","content":"High Concurrency Cluster does not support Scala.","comments":[{"comment_id":"1154534","upvote_count":"2","timestamp":"1708413720.0","poster":"AccountHatz","content":"The Shared access mode clusters aka the former High Concurrency cluster do now support Scala , they do not support R (https://learn.microsoft.com/en-us/azure/databricks/archive/compute/cluster-ui-preview)","comments":[{"timestamp":"1708556220.0","poster":"mav2000","upvote_count":"1","comment_id":"1155931","content":"Exactly! before, the cluster should've been Standard, because it wasn't able to support Scala, but now that it can, the best cluster is High concurrency from all the people executing jobs there."}]}],"timestamp":"1694669820.0","poster":"wanchihh"}],"comment_id":"623388","timestamp":"1656345660.0","poster":"sethuramansp","upvote_count":"2","content":"The answer should be \"NO\" as per the given statement \"The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\" since the Job cluster is standard it will not allow data scientists and engineers to collectively deploy their Notebooks in standard cluster as it requires High Concurrency Cluster"},{"timestamp":"1648350000.0","comment_id":"575920","poster":"Eyepatch993","content":"Selected Answer: B\nStandard clusters do not have fault tolerance. Both the data scientist and data engineers will be using the job cluster for processing their notebooks, so if a standard cluster is chosen and a fault occurs in the notebook of any one user, there is a chance that other notebooks might also fail. Due to this a high concurrency cluster is recommended for running jobs.","comments":[{"comment_id":"599626","content":"It may not be a best practice, but the question asked is: does the solution meet the stated requirements, and it does..","timestamp":"1652191440.0","poster":"Boompiee","upvote_count":"1"},{"upvote_count":"2","timestamp":"1654935780.0","content":"Read the question properly. it states that each data scientist will have a standard cluster and a separate standard cluster for running jobs. So there is no question of fault due to other users. The answer is A","comment_id":"614893","poster":"Aditya0891"}],"upvote_count":"4"},{"content":"As per Link: https://docs.azuredatabricks.net/clusters/configure.html\nStandard and Single Node clusters terminate automatically after 120 minutes by default. --> Data Scientists\nHigh Concurrency clusters do not terminate automatically by default.\nA Standard cluster is recommended for a single user. --> Standard for Data Scientists & High Concurrency for Data Engineers\nStandard clusters can run workloads developed in any language: Python, SQL, R, and Scala.\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. --> Jobs needs Standard","timestamp":"1646942520.0","upvote_count":"6","comment_id":"565016","poster":"Hanse"},{"comment_id":"554973","content":"Selected Answer: A\nYes it seems to be!","poster":"ovokpus","upvote_count":"2","timestamp":"1645662000.0"},{"timestamp":"1643363640.0","comment_id":"534583","content":"Selected Answer: A\ncorrect","poster":"PallaviPatel","upvote_count":"2"},{"comment_id":"532649","content":"Selected Answer: A\nData Engineers - High Concurrency cluster as it provides for sharing . Also caters for SQl,Python and R.\nData Scientist - Standard Clusters which automatically terminates after 120 minutes and caters for Scala,SQl,Python and R.\nJOBS- Standard Cluster","poster":"kilowd","timestamp":"1643177400.0","upvote_count":"2"},{"comment_id":"532319","poster":"let_88","timestamp":"1643133840.0","content":"As per the doc in Microsoft the High Concurrency cluster doesn't support Scala. \nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-mode","upvote_count":"7"},{"upvote_count":"2","timestamp":"1642310340.0","content":"Selected Answer: A\nThe answer must be A!","comment_id":"524638","poster":"tesen_tolga"},{"comment_id":"512703","timestamp":"1640810880.0","poster":"SabaJamal2010AtGmail","upvote_count":"2","content":"The solution does not meet the requirement because: \"High Concurrency clusters work only for SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala."},{"content":"Selected Answer: A\nData scientists and jobs use Scala so they need standard cluster","comment_id":"484327","timestamp":"1637595060.0","upvote_count":"9","poster":"FredNo"},{"content":"Answer is A.","poster":"Aslam208","comment_id":"471021","timestamp":"1635738480.0","upvote_count":"4"},{"content":"Shouldn't the answer be A, as ll the requirements are met:\nData Scientist - Standard\nData Engineer - High Concurrnecy\nJobs - Standard","poster":"gangstfear","comments":[{"timestamp":"1630393920.0","upvote_count":"6","poster":"satyamkishoresingh","comment_id":"436090","content":"Yes , Given solution is correct."}],"comment_id":"433611","upvote_count":"13","timestamp":"1630132740.0"},{"poster":"echerish","timestamp":"1630098420.0","content":"Question 23 and 24 seems to have been swapped. They Key is \n\nData Scientist - Standard\nData Engineer - High Concurrnecy\nJobs - Standard","comment_id":"433341","upvote_count":"8"},{"comments":[{"upvote_count":"1","content":"Above discussion is confusing until you reach this comment and google yourself.\nhttps://community.databricks.com/s/question/0D53f00001GHVi0CAH/why-doesnt-high-concurrency-cluster-support-scala\n\nQuestion states Scala is being used by Jobs. Usually we go with High concurrency cluster for Jobs but in this case Scala doesn't support High Concurrency Cluster So we are left with Standard.","poster":"mada_fakas1","timestamp":"1658636520.0","comment_id":"635871"}],"upvote_count":"9","comment_id":"431432","content":"Answer A\nScala is not supported in High Concurrency cluster --> Jobs & Data scientists --> Standard\nData engineers --> High concurrence","timestamp":"1629893820.0","poster":"MoDar"},{"comment_id":"383685","poster":"damaldon","content":"Answer: B\n-Data scientist should have their own cluster and should terminate after 120 mins - STANDARD\n-Cluster for Jobs should support scala - STANDARD\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure","upvote_count":"6","timestamp":"1623874380.0"},{"timestamp":"1622820060.0","upvote_count":"3","comment_id":"374452","poster":"Sunnyb","content":"B is the correct answer\nLink below:\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"},{"poster":"alain2","comments":[{"upvote_count":"4","timestamp":"1632378240.0","content":"since it is data engineers who does not use scala, so they can use High concurrency cluster","comment_id":"449951","poster":"gemealex"}],"timestamp":"1621337640.0","comment_id":"360418","upvote_count":"10","content":"B because: \"High Concurrency clusters work only for SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.\""},{"upvote_count":"9","timestamp":"1621198260.0","content":"Correct answer is B.\n\nJobs use Scala which is not supported in High Concurreny cluster.","poster":"111222333","comments":[{"poster":"siline","timestamp":"1640553060.0","comment_id":"509835","upvote_count":"2","content":"so they shoud use standard cluster which is correct in the given solution"}],"comment_id":"359042"}],"answer":"A","answer_description":"","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\n✑ A workload for data engineers who will use Python and SQL.\n✑ A workload for jobs that will run notebooks that use Python, Scala, and SQL.\n✑ A workload that data scientists will use to perform ad hoc analysis in Scala and R.\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\n✑ The data engineers must share a cluster.\n✑ The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\n✑ All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\nYou need to create the Databricks clusters for the workloads.\nSolution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.\nDoes this meet the goal?","isMC":true,"answers_community":["A (83%)","B (17%)"],"topic":"2"}],"exam":{"isImplemented":true,"isMCOnly":false,"id":67,"provider":"Microsoft","lastUpdated":"12 Apr 2025","numberOfQuestions":384,"name":"DP-203","isBeta":false},"currentPage":49},"__N_SSP":true}