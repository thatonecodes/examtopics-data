{"pageProps":{"questions":[{"id":"UM99i8a12AUJtRijPHU1","answer_ET":"","question_id":151,"unix_timestamp":1705153320,"url":"https://www.examtopics.com/discussions/microsoft/view/131086-exam-dp-203-topic-2-question-128-discussion/","answer_images":["https://img.examtopics.com/dp-203/image376.png"],"answers_community":[],"isMC":false,"discussion":[{"content":"correct: https://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16","poster":"GermanGerman","upvote_count":"6","timestamp":"1705153320.0","comment_id":"1121692"},{"poster":"evangelist","timestamp":"1721016780.0","content":"correct","upvote_count":"1","comment_id":"1248070"},{"content":"Correct","timestamp":"1712230140.0","poster":"Persius92","upvote_count":"1","comment_id":"1189267"},{"comment_id":"1121752","content":"Correct","upvote_count":"1","poster":"[Removed]","timestamp":"1705156920.0"}],"timestamp":"2024-01-13 14:42:00","question_images":["https://img.examtopics.com/dp-203/image375.png"],"answer":"","topic":"2","exam_id":67,"answer_description":"","question_text":"HOTSPOT\n-\n\nYou have an Azure Synapse Analytics dedicated SQL pool named Pool1 that contains an external table named Sales. Sales contains sales data. Each row in Sales contain data on a single sale, including the name of the salesperson.\n\nYou need to implement row-level security (RLS). The solution must ensure that the salespeople can access only their respective sales.\n\nWhat should you do? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//"},{"id":"mghVpIc2COPO2VLJSblA","answer_ET":"B","question_id":152,"unix_timestamp":1706188260,"url":"https://www.examtopics.com/discussions/microsoft/view/132133-exam-dp-203-topic-2-question-129-discussion/","answer_images":[],"answers_community":["B (65%)","D (35%)"],"isMC":true,"choices":{"D":"Frequency: Hour -\nInterval: 2","A":"Frequency: Month -\nInterval: 1","C":"Frequency: Minute -\nInterval: 60","B":"Frequency: Day -\nInterval: 1"},"discussion":[{"comment_id":"1136700","upvote_count":"6","poster":"Saranya22","content":"Selected Answer: D\nThe frequency of Hour with an interval of 2 would run the pipeline every 2 hours.","timestamp":"1706698860.0"},{"content":"Selected Answer: B\nThe frequency is a Day and interval 1, since every day should execute 4x. Within a day 4 times are fixed.","comment_id":"1131686","comments":[{"poster":"Gikan","content":"Every 1 Day(s)\nExecutes at these times\nHours 10,12, 14, 16\nMinutes 15\nhttps://learn.microsoft.com/en-us/azure/data-factory/media/how-to-create-schedule-trigger/advanced.png\nsource: \nhttps://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory","upvote_count":"1","timestamp":"1706863440.0","comment_id":"1138322"},{"timestamp":"1707192000.0","upvote_count":"1","comment_id":"1141737","content":"Agreed it should be daily.","poster":"j888"}],"poster":"erz","timestamp":"1706188260.0","upvote_count":"5"},{"timestamp":"1726669620.0","poster":"renan_ineu","upvote_count":"1","content":"Selected Answer: B\nFrequency is 1 day (every day).\nAfter choosing the frequency (recurrence), set the hours and the minutes.\nNo need to create one for each hour, because the setting accepts several hours and minutes.\n\nIn Linux, this would be a cron setting:\n15 10,12,14,16 * * *\n\nThe fields, separated by a space are:\nminute hour day-of-month month week-day\n\nThis means that the task will run:\nevery minute 15\non every hours 10, 12, 14 and 16\non every day of the month\non every month\nno matter which day-of-week it falls\n\nIt is possible to read that the frequency is \"every 2h, every day, every week, every month, and every year\", but for the Data Factory configuration, the recurrence is \"day\". Attention here: ADF calls it recurrence, not frequency. Looks the same, but the ogres that write these questions may decide to whatever they want.\n\nOther Linux cron doing the same, to be more clear:\n15 10-16/2 * * *\n15 10-16/2 */1 */1 *","comment_id":"1285783"},{"poster":"Alongi","content":"Selected Answer: B\nI found this question on my exam 30/04/2024, and I put B. I passed the exam with a high score, but I'm not sure if the answer is correct.","timestamp":"1714819140.0","upvote_count":"1","comment_id":"1206453"},{"comment_id":"1164714","upvote_count":"2","content":"Selected Answer: B\nCorrect me if I am wrong guys, but if option B is correct does that mean you have to create 4 separate triggers with the Frequency: Day and Interval: 1?\n\nIf that is the case then I am more inclined to B, since D will still run even after 4:15PM. If we want it to run only for the 4 times specified above then B is a more accurate answer.","timestamp":"1709463660.0","poster":"Delphin_8150"},{"comment_id":"1154315","poster":"Alongi","timestamp":"1708380240.0","upvote_count":"2","content":"Selected Answer: B\nInterval Day. You have to schedule 4 trigger but daily"}],"question_images":[],"timestamp":"2024-01-25 14:11:00","answer":"B","topic":"2","exam_id":67,"answer_description":"","question_text":"You have an Azure Data Factory pipeline named P1.\n\nYou need to schedule P1 to run at 10:15 AM, 12:15 PM, 2:15 PM, and 4:15 PM every day.\n\nWhich frequency and interval should you configure for the scheduled trigger?"},{"id":"ZOtjnZhpdnHsrO1TxzFI","answers_community":["B (64%)","C (34%)","2%"],"question_id":153,"exam_id":67,"choices":{"A":"Azure Synapse Analytics","C":"Azure Stream Analytics","D":"Azure SQL Database","B":"Azure Databricks"},"question_text":"You are designing a statistical analysis solution that will use custom proprietary Python functions on near real-time data from Azure Event Hubs.\nYou need to recommend which Azure service to use to perform the statistical analysis. The solution must minimize latency.\nWhat should you recommend?","isMC":true,"unix_timestamp":1629790860,"discussion":[{"comments":[{"timestamp":"1644301560.0","comments":[{"comment_id":"616744","content":"It's mentioned that \"python runs on real time data from event hubs not on event hubs\". Also event hub is to gather that data and after that it is analyzed by either databricks stream analytics. And since stream analytics doesn't support python so the answer is databricks","upvote_count":"2","timestamp":"1655295120.0","poster":"Aditya0891","comments":[{"timestamp":"1686336120.0","upvote_count":"2","content":"therefore i agree wih ASA","comments":[{"timestamp":"1686336240.0","content":"python can run Event Hubs libraries real time, it doesn't have to be supported by ASA, it just needs to send data to analytics service","comments":[{"content":"@RoyP654, the question asks which service to perform the statistical analysis (e.g. execute the python) suggesting that the python has not/will not be ran in events hubs","timestamp":"1702984260.0","upvote_count":"1","poster":"ExamDestroyer69","comment_id":"1100551"}],"comment_id":"919566","upvote_count":"1","poster":"RoyP654"}],"poster":"RoyP654","comment_id":"919564"}]}],"upvote_count":"2","poster":"anto69","comment_id":"542887","content":"But Python runs on Event Hubs why the other service does should support Python too?"}],"comment_id":"447296","content":"My answer will be B\nStream Analytics supports \"extending SQL language with JavaScript and C# user-defined functions (UDFs)\". There is no mention of Python support; hence Stream Analytics is not correct.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction\n\nAzure Databricks supports near real-time data from Azure Event Hubs. And includes support for R, SQL, Python, Scala, and Java. So I will go for option B.","upvote_count":"87","poster":"kolakone","timestamp":"1632003780.0"},{"comments":[{"timestamp":"1686336060.0","content":"the question does not ask which service can run Python, it's asking where to send the data for analytics since Python can run with Event Hubs libraries","upvote_count":"1","poster":"RoyP654","comment_id":"919560"}],"comment_id":"542888","poster":"anto69","timestamp":"1644301740.0","content":"I'm sure it's Stream Analytics cause Event Hubs already supports Python (https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-python-get-started-send). We don't need the other service to support it. We just need to lower costs. Hence ASA is the correct solution","upvote_count":"16"},{"content":"Selected Answer: B\nShould be B.\nSee the link below. Under the heading \"When to use other technologies\", it mentions \"Azure Stream Analytics supports user-defined functions (UDF) or user-defined aggregates (UDA) in JavaScript for cloud jobs and C# for IoT Edge jobs. C# user-defined deserializers are also supported. If you want to implement a deserializer, a UDF, or a UDA in other languages, such as Java or Python, you can use Spark Structured Streaming. You can also run the Event Hubs EventProcessorHost on your own virtual machines to do arbitrary streaming processing.\"\nAs the question mentions user-defined-function (UDF) in Python, ASA seems not support UDF in Python. Should use Spark Structured Streaming, which in this case here is Azure Databricks.\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/streaming-technologies","poster":"fbc11c2","comment_id":"1281387","timestamp":"1725948960.0","upvote_count":"1"},{"poster":"evangelist","content":"C is wrong, stream analytics is a SQL based analytics","upvote_count":"1","comment_id":"1247748","timestamp":"1720958100.0"},{"poster":"e56bb91","upvote_count":"1","content":"Selected Answer: B\nChatGPT 4o\nAzure Databricks is well-suited for real-time data processing and analytics. It provides a collaborative environment for working with Apache Spark, which is ideal for performing complex statistical analyses and machine learning tasks in real-time.","comment_id":"1244395","timestamp":"1720449540.0"},{"comment_id":"1220973","timestamp":"1716988740.0","content":"Selected Answer: B\nIt says we are using python after it is sent to event hubs: \"custom proprietary Python functions on near real-time data FROM Azure Event Hubs\". Yes, we can send events to Event Hub with python but it says that we are running statistical analysis AFTER we send it to Event Hub. Therefore, my answer is Databricks","upvote_count":"1","poster":"KarlGardnerDataEngineering"},{"comment_id":"1220830","timestamp":"1716979860.0","upvote_count":"2","poster":"eb36a01","content":"Azure Stream Analytics: Azure Stream Analytics is designed for real-time data processing and can directly ingest data from Azure Event Hubs. However, it has limited support for custom Python functions. It is more suitable for simple real-time analytics and transformations rather than complex statistical analysis with custom code.\ncorrect answer: Azure Databricks, we have custom python function"},{"content":"B is correct","timestamp":"1714011060.0","poster":"Dusica","upvote_count":"1","comment_id":"1201695"},{"comment_id":"1183933","upvote_count":"1","timestamp":"1711526160.0","poster":"poesklap","content":"Selected Answer: B\nAzure Databricks provides a fast and scalable Apache Spark-based analytics platform that supports Python, among other programming languages. It allows you to perform near real-time data processing and analysis efficiently, making it ideal for scenarios where low latency is a priority. Additionally, it offers seamless integration with Azure Event Hubs, enabling you to ingest data in real-time and apply custom Python functions for statistical analysis."},{"content":"B. Azure Databricks\n\nAzure Databricks provides a fully managed Apache Spark-based analytics platform that is well-suited for processing and analyzing real-time streaming data. It offers native integration with Azure Event Hubs, allowing you to ingest data in real-time and apply custom Python functions for statistical analysis with minimal latency. Additionally, Databricks provides scalable compute resources, optimized processing capabilities, and support for various programming languages, making it an ideal choice for near real-time data analysis scenarios.","timestamp":"1709707200.0","poster":"Elanche","upvote_count":"1","comment_id":"1166956"},{"poster":"moneytime","content":"C is correct.\nAt near realtime ,the window functions in azure stream analytics can be employed in compute some statical values (e.g count,maximum,min,avg. etc) of the data streaming from the even hub.","timestamp":"1707596760.0","comment_id":"1146621","upvote_count":"1"},{"upvote_count":"1","timestamp":"1707051300.0","poster":"Azure_2023","comment_id":"1140099","content":"Selected Answer: B\nCorrected!!!\n\n FROM Azure Event Hubs, not ON\n\nAzure Databricks"},{"timestamp":"1707051240.0","upvote_count":"1","content":"Selected Answer: D\nFROM Azure Event Hubs, not ON","poster":"Azure_2023","comment_id":"1140097"},{"content":"Selected Answer: C\nchatGPT explains - \nAzure Stream Analytics is designed for real-time data stream processing and analytics. It can ingest data from various sources, including Azure Event Hubs, and allows you to run near real-time analytics using a SQL-like language. With Stream Analytics, you can easily apply custom Python functions using user-defined functions (UDFs) and achieve low-latency processing.\n\nAzure Synapse Analytics and Azure Databricks are powerful analytics services, but they are more suitable for complex analytics and big data processing rather than near real-time, low-latency scenarios.\n\nAzure SQL Database is a relational database service and is not specifically designed for real-time stream processing.\n\nTherefore, in this case, Azure Stream Analytics is the recommended choice for minimizing latency in statistical analysis on near real-time data from Azure Event Hubs.","timestamp":"1706446980.0","poster":"prshntdxt7","upvote_count":"3","comment_id":"1134084"},{"poster":"sdg2844","content":"Selected Answer: C\nSimply, they always want the Stream Analytics answer. It's the most straightforward.","upvote_count":"3","timestamp":"1705429200.0","comment_id":"1124415"},{"comment_id":"1115385","content":"Azure Databricks","timestamp":"1704567660.0","poster":"maxCarter","upvote_count":"1"},{"timestamp":"1694148060.0","poster":"HSZ","upvote_count":"3","content":"Selected Answer: C\nFrom ChatGPT, To minimize latency for statistical analysis on near real-time data from Azure Event Hubs, I recommend using Azure Stream Analytics (Option C). Azure Stream Analytics is designed for real-time data processing and can ingest and analyze data from Event Hubs with low latency, making it a suitable choice for this scenario.","comments":[{"poster":"mav2000","upvote_count":"1","content":"Also from ChatGPT (GPT4) lol:\n\nFor processing near real-time data with custom proprietary Python functions and minimizing latency, the best service would be:\n\nB. Azure Databricks\n\nHere’s why:\n\nAzure Databricks is an Apache Spark-based analytics service that integrates smoothly with Azure services such as Azure Event Hubs. It supports real-time streaming data processing and can execute custom Python code, which is necessary for your custom statistical analysis functions. Databricks is designed to handle large-scale data processing and analytics with low latency, making it suitable for near real-time scenarios.\nThe other services have their uses but may not be the optimal choice for this particular scenario","comment_id":"1100738","timestamp":"1702998420.0"}],"comment_id":"1002111"},{"poster":"kkk5566","upvote_count":"1","content":"Selected Answer: B\nDatabriks supports Python.","comment_id":"999474","timestamp":"1693916220.0"},{"poster":"kkk5566","upvote_count":"1","content":"https://learn.microsoft.com/en-us/sql/t-sql/statements/create-partition-function-transact-sql?view=sql-server-ver16","comment_id":"991784","timestamp":"1693196100.0","comments":[{"comment_id":"991786","timestamp":"1693196520.0","content":"RIGHT :\nt<20100101, 20100101<=t<20110101, 20110101<=t<20120101\n20120101<=t\nLEFT\nt<=20100101, 20100101<t<=20110101,20110101<t<=20120101,\nt>20120101","upvote_count":"1","poster":"kkk5566","comments":[{"upvote_count":"1","content":"post wrong quiz","comment_id":"999328","poster":"kkk5566","timestamp":"1693907400.0"}]}]},{"content":"C is the answer","poster":"Abdullah77","comment_id":"987189","timestamp":"1692689640.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1690308660.0","content":"In this question, it's mentioned that Python will be used to produce the statistical solution (necessarily By Event Hubs), and the needed solution is the one which will follow and process the Statistical Solution (already processed by Python via Event Hubs). So in my opinion the answer is correct, I go for ASA too.","poster":"Zak_Zakaria","comment_id":"962990"},{"upvote_count":"1","timestamp":"1690279140.0","poster":"andjurovicela","comment_id":"962570","content":"ChatGPT would choose Databricks as well :)","comments":[{"content":"Try again!","timestamp":"1698669540.0","poster":"phydev","upvote_count":"2","comment_id":"1057664"}]},{"upvote_count":"2","poster":"yaberjorge","content":"Azure Stream Analytics\nAccording Chatgpt","comment_id":"948082","timestamp":"1688991480.0"},{"upvote_count":"2","comment_id":"936620","timestamp":"1687954020.0","poster":"auwia","content":"Selected Answer: B\nDatabriks supports Python."},{"upvote_count":"4","poster":"vctrhugo","content":"Selected Answer: B\nAzure Stream Analytics only supports programmability in SQL and JavaScript, while Apache Spark in Azure Databricks supports C#/F#, Java, Python, R, Scala.\n\nhttps://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing#general-capabilities","timestamp":"1686561720.0","comment_id":"921284"},{"upvote_count":"5","comment_id":"906381","poster":"TestingCRM","content":"Azure Databricks. See https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing#general-capabilities. Python is not supported in Azure Stream Analytics according to this documentation.","timestamp":"1684990800.0"},{"comments":[{"poster":"auwia","content":"Near real time","timestamp":"1687377180.0","upvote_count":"1","comment_id":"929871"}],"timestamp":"1684959120.0","upvote_count":"1","comment_id":"906137","content":"Selected Answer: C\nIts stream analytucs because it's asking for real-time analytics in azure","poster":"janaki"},{"upvote_count":"1","timestamp":"1684823520.0","comment_id":"904628","poster":"explorerhp","content":"It should be C - Azure Stream Analytics. Don't reason other comments."},{"content":"Selected Answer: B\nAzure Databricks","comment_id":"901059","timestamp":"1684404960.0","poster":"rocky48","upvote_count":"2"},{"content":"Selected Answer: B\nStream Analytics only supports a limited set of built-in functions and cannot execute custom Python code directly. Therefore, it may not be the best choice for this scenario where custom proprietary Python functions are required.","comment_id":"890943","upvote_count":"2","poster":"dksks","timestamp":"1683398280.0"},{"timestamp":"1681559880.0","upvote_count":"1","poster":"frankanalysis","content":"Both Azure Stream Analytics and Databricks support python, but Azure Stream is lower latency than Databricks. Thus option C.","comments":[{"poster":"frankanalysis","comment_id":"870896","content":"https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-databricks-comparison","timestamp":"1681559940.0","upvote_count":"1"},{"content":"azure stream analytics DOES NOT support python; azure stream analytics offers SQL to do the querying https://learn.microsoft.com/en-us/stream-analytics-query/stream-analytics-query-language-reference#see-also","comment_id":"876459","timestamp":"1682076840.0","upvote_count":"1","poster":"mamahani"}],"comment_id":"870894"},{"content":"Selected Answer: C\nC. Azure Stream Analytics would be the best recommendation for performing statistical analysis on near real-time data from Azure Event Hubs with low latency. Azure Stream Analytics is a fully-managed service that provides real-time data processing and analysis with low latency, high reliability, and scalable throughput.","comment_id":"840670","timestamp":"1678953840.0","upvote_count":"1","poster":"esaade"},{"poster":"esaade","content":"Given that the scenario involves real-time data from Azure Event Hubs and custom proprietary Python functions, the best Azure service to perform statistical analysis with minimal latency would be Azure Databricks.","comment_id":"831656","timestamp":"1678176780.0","upvote_count":"1"},{"content":"Azure Stream Analytics is a fully managed serverless offering that allows you to perform real-time analytics and complex event processing on data from various sources, including Azure Event Hubs. It can also run custom code in Python and supports popular machine learning libraries such as TensorFlow, Scikit-learn, and PyTorch.\n\nAzure Synapse Analytics and Azure SQL Database are designed for storing and querying data, but they are not optimized for real-time data processing and analysis. Azure Databricks is a powerful data engineering and data science tool, but it may be too heavyweight for simple statistical analysis tasks, and it incurs higher latency than Azure Stream Analytics.","poster":"ClaudioDB","comment_id":"830958","timestamp":"1678117020.0","upvote_count":"2"},{"poster":"rohanb1986","upvote_count":"2","comment_id":"757360","timestamp":"1672052400.0","content":"Selected Answer: B\nYou are designing a statistical analysis solution that will use custom proprietary Python functions on near real-time data from Azure Event Hubs.\nYou need to recommend which Azure service to use to perform the statistical analysis.\n\n\nIt asks us -Recommend which Azure service to use to perform the statistical analysis.\nThe Azure service should be able to perform the statistical analysis WHICH will use custom proprietary Python functions.\nSo, it should be Databricks not Stream Analytics"},{"upvote_count":"1","timestamp":"1665366060.0","poster":"Aslam208","comment_id":"690639","content":"Correct answer is B, as Azure Stream Analytics does not support Python"},{"upvote_count":"1","poster":"yyyhhh","timestamp":"1661501400.0","content":"Selected Answer: C\nI agree with VeroDon","comment_id":"652121"},{"timestamp":"1661174280.0","poster":"snice","comment_id":"650293","upvote_count":"1","content":"Azure Stream Analytics does NOT support python."},{"poster":"Deeksha1234","timestamp":"1659201120.0","content":"Should be B","comment_id":"639759","upvote_count":"2"},{"comment_id":"618810","timestamp":"1655665380.0","upvote_count":"2","content":"No discussion needed:\n\nAzure Stream Analytics supports the following four function types:\nJavaScript user-defined functions\nJavaScript user-defined aggregates\nC# user-defined functions (using Visual Studio)\nAzure Machine Learning\n\nYou wont build a statistical analysis solution on eventhubs...","poster":"flaviodiasps"},{"poster":"HebaN","content":"Selected Answer: B\nAnswer is B","comment_id":"612293","timestamp":"1654510800.0","upvote_count":"1"},{"timestamp":"1654158300.0","upvote_count":"2","comment_id":"610497","content":"I would go for the stream analytics service (ASA). The python functions are already developed, ready to be used in ASA.","poster":"jtu363","comments":[{"content":"look at the sentence properly - \"python runs on real time data from event hubs not on event hubs\". Both are different things","upvote_count":"1","comment_id":"616746","poster":"Aditya0891","timestamp":"1655295180.0"}]},{"poster":"Egocentric","timestamp":"1650218280.0","content":"really confused with this question","upvote_count":"3","comment_id":"587334"},{"upvote_count":"2","content":"Selected Answer: B\nCorrect answer. I agree with Kolakone.","timestamp":"1643462940.0","comment_id":"535480","poster":"PallaviPatel"},{"comment_id":"532536","poster":"Drummer","content":"This is the Microsoft Azure Stream Analytics Management Client Library. This package has been tested with Python 2.7, 3.5, 3.6, 3.7 and 3.8. For a more complete view of Azure libraries, see the azure sdk python release.\nhttps://docs.microsoft.com/en-us/python/api/overview/azure/mgmt-streamanalytics-readme?view=azure-python-preview","timestamp":"1643160840.0","upvote_count":"2"},{"poster":"joeljohnrm","comment_id":"520435","content":"Selected Answer: B\nThe data comes from the Event Hub, it is not processed by the Event Hub, so \"proprietary\" Python requires databricks.","timestamp":"1641758880.0","upvote_count":"1"},{"comments":[{"poster":"VeroDon","comment_id":"515767","timestamp":"1641216360.0","upvote_count":"1","content":"the question is really confusin, tho"}],"content":"Selected Answer: C\nCorrect. eventHub is able to reveive and send events using Python\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-python-get-started-send\nStream Analytics will read the data available from EventHub as usual. See point 4. from this link\n\"Select Explore on the Enable real-time insights from events tile\"\nhttps://docs.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics\nI think the problem might be that most understood that stream analytics would run python\nhttps://docs.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics","timestamp":"1641216300.0","poster":"VeroDon","comment_id":"515764","upvote_count":"5"},{"timestamp":"1641172320.0","upvote_count":"3","content":"Selected Answer: B\n\"proprietary\" python requires databricks.","comment_id":"515336","poster":"DrTaz"},{"poster":"Canary_2021","timestamp":"1639965120.0","upvote_count":"2","comment_id":"505178","content":"Selected Answer: B\nUnfortunately, Azure Stream Analytics doesn't support queries from a python script. Queries in Azure Stream Analytics are expressed in a SQL-like query language. So the answer should be B.\nhttps://stackoverflow.com/questions/58097539/execute-azure-steaming-analytics-queries-from-a-python-script"},{"timestamp":"1639527240.0","upvote_count":"2","comment_id":"501732","poster":"m2shines","content":"B, Azure Databricks"},{"poster":"avijitd","content":"Selected Answer: B\nAzure Data brick","timestamp":"1639194240.0","upvote_count":"2","comment_id":"499076"},{"timestamp":"1638931500.0","upvote_count":"1","comment_id":"496486","poster":"vijju23","content":"Answer is B"},{"content":"Selected Answer: B\nB because as already mentioned Python is not supported in Stream Analytics but with Azure Databricks it works.","comment_id":"495258","upvote_count":"3","poster":"rashjan","timestamp":"1638805980.0"},{"content":"Answer is B so python function not supported in-stream analytics :)","timestamp":"1638700200.0","comment_id":"494229","upvote_count":"1","poster":"proserv"},{"upvote_count":"1","poster":"sp63931","timestamp":"1636673940.0","comment_id":"476561","content":"should be b"},{"timestamp":"1629974580.0","comment_id":"432174","content":"should be B","poster":"SaferSephy","upvote_count":"4"},{"content":"The answer Should be B as Azure Stream Analytics doesn't support Python","upvote_count":"4","timestamp":"1629833760.0","comment_id":"430984","poster":"echerish"},{"content":"I think answer should be Azure Databricks as ASA does not support python queries.","poster":"Nilay95","upvote_count":"2","comment_id":"430648","timestamp":"1629797640.0"},{"upvote_count":"2","timestamp":"1629790860.0","comment_id":"430562","content":"Doesn't Azure Stream Analytics on support its native query language and Java. This question explicitly states python.","poster":"J2T2"}],"answer_description":"","timestamp":"2021-08-24 09:41:00","answer_ET":"B","url":"https://www.examtopics.com/discussions/microsoft/view/60470-exam-dp-203-topic-2-question-13-discussion/","question_images":[],"topic":"2","answer":"B","answer_images":[]},{"id":"f1D3rVPyc5gL5mC5JNLp","question_id":154,"timestamp":"2024-06-05 18:49:00","isMC":true,"topic":"2","answer_description":"","answer_images":[],"question_images":[],"answer_ET":"D","choices":{"C":"Append Variable","B":"Get Metadata","D":"Lookup","A":"Stored Procedure"},"url":"https://www.examtopics.com/discussions/microsoft/view/141945-exam-dp-203-topic-2-question-130-discussion/","answer":"D","discussion":[{"timestamp":"1726670460.0","poster":"renan_ineu","comment_id":"1285817","upvote_count":"1","content":"Selected Answer: D\n\"Lookup activity reads and returns the content of a configuration file or table. It also returns the result of executing a query or stored procedure\" - https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#:~:text=returns%20the%20result%20of%20executing%20a%20query%20or%20stored%20procedure\n\n\"When the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet\" - https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure#:~:text=stored%20procedure%20activity%2C-,use%20lookup%20acitivty,-and%20Script%20activity"},{"content":"Selected Answer: D\nWhen the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet.","upvote_count":"1","timestamp":"1720169220.0","comment_id":"1242633","poster":"fahfouhi94"},{"poster":"Alongi","comment_id":"1229417","timestamp":"1718217720.0","upvote_count":"2","content":"Selected Answer: D\nThe Stored Procedure Activity does not allow to return an output, so Lookup is the correct one. Refer to: https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"},{"content":"Selected Answer: D\nAlso ChatGPT:\nFor your specific requirement:\n\nRetrieve the Number of Sales Invoices for the Current Date:\nIf you simply need to call a stored procedure that returns the count and you want to use this count directly in your pipeline (e.g., for further processing or branching logic), the Lookup activity is preferable.\nIf the stored procedure's primary purpose is to perform operations (e.g., insert/update/delete) and the output is not directly needed within the pipeline, the Stored Procedure activity is more appropriate.\nGiven that your stored procedure returns the number of sales invoices and does not require input parameters, and assuming you want to use this count directly in the pipeline, the Lookup activity would be a good fit.","poster":"JamieMcD","comment_id":"1225295","timestamp":"1717663920.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nChatgpt:\nThe Stored Procedure activity in Azure Data Factory is specifically designed to execute SQL stored procedures. It is the most suitable activity when you need to run a stored procedure and handle the results or output of that procedure.\nThe other options do not fit the requirements:\n\nB. Get Metadata is used to retrieve metadata information from data stores, not to execute stored procedures.\nC. Append Variable is used to append a value to an existing variable, not for executing stored procedures.\nD. Lookup is used to retrieve a dataset from a data store but is not typically used to execute stored procedures that return a single value without parameters.","comment_id":"1224856","poster":"tadenet","timestamp":"1717606140.0"}],"unix_timestamp":1717606140,"exam_id":67,"question_text":"You are creating an Azure Data Factory pipeline.\n\nYou need to add an activity to the pipeline. The activity must execute a Transact-SQL stored procedure that has the following characteristics:\n\n• Returns the number of sales invoices for a current date\n• Does NOT require input parameters\n\nWhich type on activity should you use?","answers_community":["D (83%)","A (17%)"]},{"id":"CbaEtVr8BUK1JBZ4FbGt","discussion":[{"content":"Correct","poster":"e56bb91","comment_id":"1241327","upvote_count":"2","timestamp":"1720002120.0"},{"comment_id":"1237350","comments":[{"upvote_count":"1","content":"I mean stopped state just to be clear, we do this all the time for testing purposes.","poster":"Sr18","timestamp":"1719393840.0","comment_id":"1237352"}],"content":"Strange: trigger is in disabled state then How it will fire :P","timestamp":"1719393720.0","poster":"Sr18","upvote_count":"2"},{"comment_id":"1224867","poster":"tadenet","content":"Correct!","timestamp":"1717606680.0","upvote_count":"2"}],"topic":"2","exam_id":67,"question_text":"HOTSPOT\n-\n\nYou have an Azure Synapse Analytics workspace that contains three pipelines and three triggers named Trigger1, Trigger2, and Trigger3.\n\nTrigger3 has the following definition.\n\n//IMG//\n\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","timestamp":"2024-06-05 18:58:00","answer_images":["https://img.examtopics.com/dp-203/image387.png"],"question_images":["https://img.examtopics.com/dp-203/image385.png","https://img.examtopics.com/dp-203/image386.png"],"answer_description":"","unix_timestamp":1717606680,"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/141946-exam-dp-203-topic-2-question-131-discussion/","answers_community":[],"question_id":155,"isMC":false,"answer_ET":""}],"exam":{"isImplemented":true,"numberOfQuestions":384,"id":67,"isMCOnly":false,"provider":"Microsoft","name":"DP-203","lastUpdated":"12 Apr 2025","isBeta":false},"currentPage":31},"__N_SSP":true}