{"pageProps":{"questions":[{"id":"YId0oaHenEms1gJaiKAN","unix_timestamp":1680540240,"timestamp":"2023-04-03 18:44:00","answer_description":"","question_images":["https://img.examtopics.com/dp-203/image279.png"],"question_id":121,"answer_ET":"","topic":"2","discussion":[{"upvote_count":"53","comment_id":"860150","timestamp":"1680540240.0","poster":"ababatunde_hs","content":"Time partitioning is correct as the fastest way to load only new files, but requires that the timeslice information be part of the file or folder name (https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview)\n\nHowever, Parquet is the correct file format since it's a columnar format"},{"upvote_count":"9","content":"Time partitioning and parquet","comment_id":"993828","timestamp":"1693382820.0","poster":"kkk5566"},{"timestamp":"1730198040.0","poster":"suranga4","upvote_count":"1","comment_id":"1304399","content":"Answer should be Timeslicing and Parquet"},{"upvote_count":"1","content":"Parquet is the answer to the second question. You nee to take only 5 colums out of 62: with CSV you'd have to explore the file row-wise sequentially... Parquet is more efficient, since seleciton proceeds column-wise.","timestamp":"1711962420.0","poster":"MBRSDG","comment_id":"1187331","comments":[{"comment_id":"1187333","upvote_count":"2","content":"Just a notice. This answer should be proven by a benchmark. Currently I didn't find any benchmark comparing the performances of the two files. Logically speaking, I'd expect Parquet to be a lot more efficient, but it should have to be measured in practise.","timestamp":"1711962540.0","poster":"MBRSDG"}]},{"upvote_count":"2","content":"You need to minimize how long it takes to perform the incremental loads. With Parquet, which is a columnar format, it is way faster to select a few columns than csv.","poster":"vctrhugo","comment_id":"925675","timestamp":"1686962400.0"},{"timestamp":"1685345820.0","upvote_count":"1","content":"we can do incremental load just with deltatable for a parquet file which supported by datarbricks or synapse spark and here he didn't give details so I think it will be CSV","comment_id":"909144","poster":"vegeta379"},{"content":"I think the requirement is to select specific columns, hence CSV?","poster":"pavankr","comment_id":"906107","timestamp":"1684954560.0","upvote_count":"1"},{"content":"it supposed to be Parquet instead of CSV","upvote_count":"5","timestamp":"1684289400.0","comment_id":"899653","poster":"verisdev"}],"exam_id":67,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/104997-exam-dp-203-topic-2-question-100-discussion/","isMC":false,"answer_images":["https://img.examtopics.com/dp-203/image280.png"],"answer":"","question_text":"HOTSPOT\n-\n\nYou have an Azure Blob storage account that contains a folder. The folder contains 120,000 files. Each file contains 62 columns.\n\nEach day, 1,500 new files are added to the folder.\n\nYou plan to incrementally load five data columns from each new file into an Azure Synapse Analytics workspace.\n\nYou need to minimize how long it takes to perform the incremental loads.\n\nWhat should you use to store the files and in which format? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//"},{"id":"QjsRJtYvEJK7y89BZyDI","url":"https://www.examtopics.com/discussions/microsoft/view/108691-exam-dp-203-topic-2-question-101-discussion/","unix_timestamp":1683467520,"answer_ET":"","answer_images":["https://img.examtopics.com/dp-203/image297.png"],"discussion":[{"comment_id":"891434","poster":"[Removed]","upvote_count":"33","content":"Given answer is wrong. It should be BEGIN TRAN as SQL pool in Azure Synapse Analytics does not support distributed transaction.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-develop-transactions\n\n\"Limitations\n\nSQL pool does have a few other restrictions that relate to transactions.\n\nThey are as follows:\n\n No distributed transactions\n No nested transactions permitted\n No save points allowed\n No named transactions\n No marked transactions\n No support for DDL such as CREATE TABLE inside a user-defined transaction\n\"\nDistributed Transactions are only allowed in SQL Server and Azure SQL Managed Instance:\n\nhttps://learn.microsoft.com/de-de/sql/t-sql/language-elements/begin-distributed-transaction-transact-sql?view=sql-server-ver16","timestamp":"1683467520.0"},{"timestamp":"1685647920.0","poster":"janaki","comment_id":"912356","content":"Its BEGIN TRAN\nthen ROLLBACK TRAN","upvote_count":"12"},{"content":"The answers they provide here are a joke. The soltuion is Begin Tran not Begin Distributed Transaction.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-transactions","upvote_count":"1","comment_id":"1305837","timestamp":"1730468280.0","poster":"17c5d1e"},{"poster":"e56bb91","comment_id":"1244886","timestamp":"1720523760.0","content":"ChatGPT 4o code:\nBEGIN TRANSACTION;\n\nBEGIN TRY\n -- Insert data from staging table to target table\n INSERT INTO target_table (column1, column2, ...)\n SELECT column1, column2, ...\n FROM staging_table;\n \n -- Commit the transaction if no error occurs\n COMMIT TRANSACTION;\nEND TRY\nBEGIN CATCH\n -- Rollback the transaction if an error occurs\n ROLLBACK TRANSACTION;\n\n -- Optionally, you can log the error or re-throw it\n -- SELECT ERROR_MESSAGE() AS ErrorMessage;\n -- THROW;\nEND CATCH;","upvote_count":"1"},{"comment_id":"1147961","upvote_count":"1","timestamp":"1707732600.0","poster":"Alongi","content":"BEGIN TRAN\nROLLBACK TRAN"},{"poster":"be8a152","comment_id":"1137143","upvote_count":"1","content":"BEGIN TRAN & ROLLBACK TRAN","timestamp":"1706744400.0"},{"comment_id":"1107245","upvote_count":"1","poster":"[Removed]","timestamp":"1703716560.0","content":"It should be BEGIN TRAN\nand ROLLBACK TRAN"},{"upvote_count":"1","timestamp":"1693383000.0","comment_id":"993834","poster":"kkk5566","content":"BEGIN TRAN\nROLLBACK TRAN"}],"question_images":["https://img.examtopics.com/dp-203/image296.png"],"question_id":122,"isMC":false,"topic":"2","question_text":"DRAG DROP\n-\n\nYou are batch loading a table in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to load data from a staging table to the target table. The solution must ensure that if an error occurs while loading the data to the target table, all the inserts in that batch are undone.\n\nHow should you complete the Transact-SQL code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","exam_id":67,"answers_community":[],"answer_description":"","answer":"","timestamp":"2023-05-07 15:52:00"},{"id":"wM8CfOCEo2PhI2qtmzF1","answer":"","exam_id":67,"answer_description":"","answer_images":["https://img.examtopics.com/dp-203/image299.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/108724-exam-dp-203-topic-2-question-102-discussion/","question_images":["https://img.examtopics.com/dp-203/image298.png"],"question_text":"HOTSPOT\n-\n\nYou have two Azure SQL databases named DB1 and DB2.\n\nDB1 contains a table named Table1. Table1 contains a timestamp column named LastModifiedOn. LastModifiedOn contains the timestamp of the most recent update for each individual row.\n\nDB2 contains a table named Watermark. Watermark contains a single timestamp column named WatermarkValue.\n\nYou plan to create an Azure Data Factory pipeline that will incrementally upload into Azure Blob Storage all the rows in Table1 for which the LastModifiedOn column contains a timestamp newer than the most recent value of the WatermarkValue column in Watermark.\n\nYou need to identify which activities to include in the pipeline. The solution must meet the following requirements:\n\n• Minimize the effort to author the pipeline.\n• Ensure that the number of data integration units allocated to the upload operation can be controlled.\n\nWhat should you identify? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct answer is worth one point.\n\n//IMG//","question_id":123,"timestamp":"2023-05-08 09:42:00","topic":"2","discussion":[{"upvote_count":"15","timestamp":"1685626440.0","content":"Correct. The example is here\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal","poster":"DarKru","comment_id":"912151"},{"poster":"[Removed]","upvote_count":"11","content":"Seems correct to me","timestamp":"1683531720.0","comment_id":"891891"},{"upvote_count":"3","comment_id":"1239877","content":"Create a pipeline with the following workflow:\n\nThe pipeline in this solution has the following activities:\n\nCreate two Lookup activities. Use the first Lookup activity to retrieve the last watermark value. Use the second Lookup activity to retrieve the new watermark value. These watermark values are passed to the Copy activity.\nCreate a Copy activity that copies rows from the source data store with the value of the watermark column greater than the old watermark value and less than the new watermark value. Then, it copies the delta data from the source data store to Blob storage as a new file.\nCreate a StoredProcedure activity that updates the watermark value for the pipeline that runs next time.","timestamp":"1719785460.0","poster":"CezarioAbrantesPP"},{"timestamp":"1714819080.0","comment_id":"1206452","comments":[{"poster":"e56bb91","timestamp":"1719911760.0","content":"you will get higher score if you choose Lookup and Copy","comment_id":"1240655","upvote_count":"3"}],"poster":"Alongi","upvote_count":"2","content":"I found this question on my exam 30/04/2024, and I put \n- LOOKUP\n- DATAFLOW\n I passed the exam with a high score, but I'm not sure if the answer is correct."},{"poster":"kkk5566","upvote_count":"1","comment_id":"993838","timestamp":"1693383300.0","content":"lookup & copy activity"},{"comment_id":"893261","comments":[{"timestamp":"1687522440.0","comment_id":"931565","poster":"auwia","content":"The Filter activity in Azure Data Factory is used to filter an array of objects from a previous activity's output (typically from a Lookup activity). It cannot directly query a database or compare a value from a database (watermark in this case) against data in another database.","upvote_count":"5"}],"timestamp":"1683647160.0","poster":"haythemsi","upvote_count":"3","content":"Filter not lookup, because we have to \"Minimize the effort to author the pipeline\" and we have only the LastModifiedOn column as information, we are not sure for lookup."}],"answers_community":[],"unix_timestamp":1683531720,"isMC":false,"answer_ET":""},{"id":"cdbvk5wIx8aCfO04HMMR","question_text":"HOTSPOT\n-\n\nYou have an Azure Synapse serverless SQL pool.\n\nYou need to read JSON documents from a file by using the OPENROWSET function.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","discussion":[{"comment_id":"889920","upvote_count":"32","timestamp":"1683278400.0","content":"Correct. It's weird but best way to open a json is as a csv and with 0x0b for fieldterminator and fieldquote.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files","poster":"Yemeral"},{"comment_id":"1235985","upvote_count":"6","timestamp":"1719175020.0","content":"GPT would fail this exam and still explain the incorrect answers like a PRO","poster":"c430fb7"},{"content":"It's correct","upvote_count":"1","poster":"Alongi","comment_id":"1147971","timestamp":"1707733440.0"},{"poster":"dakku987","timestamp":"1704979320.0","upvote_count":"6","content":"I got this question today in exam 11-jan-2024","comment_id":"1119800"},{"comment_id":"993839","timestamp":"1693383420.0","poster":"kkk5566","content":"Correct","upvote_count":"4"}],"answers_community":[],"unix_timestamp":1683278400,"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/108533-exam-dp-203-topic-2-question-103-discussion/","timestamp":"2023-05-05 11:20:00","topic":"2","answer_description":"","question_id":124,"question_images":["https://img.examtopics.com/dp-203/image300.png"],"answer_ET":"","answer_images":["https://img.examtopics.com/dp-203/image301.png"],"exam_id":67,"isMC":false},{"id":"tN30Imui9VpQ4iSEsBGG","answer_images":[],"answers_community":["CD (90%)","10%"],"url":"https://www.examtopics.com/discussions/microsoft/view/108594-exam-dp-203-topic-2-question-104-discussion/","answer":"CD","discussion":[{"poster":"akk_1289","comment_id":"890803","timestamp":"1683384600.0","upvote_count":"25","comments":[{"upvote_count":"3","poster":"henryphchan","content":"agree with you","timestamp":"1683657540.0","comment_id":"893361","comments":[{"poster":"kkk5566","content":"agree with you","timestamp":"1693383540.0","upvote_count":"1","comment_id":"993841"}]},{"timestamp":"1695717420.0","poster":"abhijit1990","upvote_count":"2","content":"absolutely right","comment_id":"1017556"}],"content":"C. the ability to save without publishing\nD. the ability to save pipelines that have validation issues\nWhen you integrate Data Factory and GitHub, you can save your pipelines to a GitHub repository without publishing them to Azure. This allows you to work on your pipelines in a development environment and then publish them to Azure when you are ready.\n\nYou can also save pipelines that have validation issues. This is because GitHub does not validate your pipelines when you save them. This allows you to work on your pipelines and fix the validation issues before you publish them to Azure."},{"content":"B definitely; 4 times faster\nLeaning towards C although D is not incorrect either","poster":"Dusica","comment_id":"1205336","comments":[{"upvote_count":"2","poster":"Sr18","content":"No brainer its C & D.","timestamp":"1718752140.0","comment_id":"1232635"}],"timestamp":"1714633620.0","upvote_count":"1"},{"poster":"Delphin_8150","comment_id":"1164134","content":"Selected Answer: CD\nAnswer is CD hands down, no contest,","upvote_count":"2","timestamp":"1709386800.0"},{"comment_id":"1147979","timestamp":"1707733680.0","upvote_count":"1","poster":"Alongi","content":"Selected Answer: CD\nCD with no doubts"},{"comment_id":"1137145","poster":"be8a152","timestamp":"1706744520.0","content":"Correct \nC. the ability to save without publishing\nD. the ability to save pipelines that have validation issues","upvote_count":"1"},{"upvote_count":"3","timestamp":"1686963120.0","content":"Selected Answer: CD\nC. The ability to save without publishing: Integrating Data Factory with GitHub allows you to save changes to your pipelines without immediately publishing them. This provides flexibility in terms of saving work-in-progress or experimental changes without impacting the production pipelines.\n\nD. The ability to save pipelines that have validation issues: With GitHub integration, you can save pipelines that have validation issues. This is useful when you want to save your work-in-progress changes or modifications to a pipeline, even if it doesn't currently pass validation. You can continue to work on resolving the validation issues without losing your progress.","comment_id":"925678","poster":"vctrhugo"},{"upvote_count":"3","comment_id":"894245","timestamp":"1683741540.0","content":"Correct","poster":"aemilka"},{"comment_id":"893336","upvote_count":"3","content":"Selected Answer: CD\nCorrect","timestamp":"1683655320.0","poster":"haythemsi"},{"upvote_count":"1","timestamp":"1683328560.0","comment_id":"890370","comments":[],"content":"Selected Answer: BC\nI think B and C","poster":"Aninina"}],"topic":"2","unix_timestamp":1683328560,"question_text":"You use Azure Data Factory to create data pipelines.\n\nYou are evaluating whether to integrate Data Factory and GitHub for source and version control.\n\nWhat are two advantages of the integration? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.","exam_id":67,"timestamp":"2023-05-06 01:16:00","isMC":true,"question_images":[],"choices":{"C":"the ability to save without publishing","A":"additional triggers","D":"the ability to save pipelines that have validation issues","B":"lower pipeline execution times"},"answer_description":"","answer_ET":"CD","question_id":125}],"exam":{"isImplemented":true,"isMCOnly":false,"numberOfQuestions":384,"lastUpdated":"12 Apr 2025","provider":"Microsoft","isBeta":false,"name":"DP-203","id":67},"currentPage":25},"__N_SSP":true}