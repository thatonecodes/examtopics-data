{"pageProps":{"questions":[{"id":"50lkfyTgdGHVsRc6JBBb","answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0047600001.png"],"isMC":false,"answer_description":"Box 1: memory-optimized table -\nIn-Memory OLTP can provide great performance benefits for transaction processing, data ingestion, and transient data scenarios.\n\nBox 2: materialized view -\nTo support efficient querying, a common solution is to generate, in advance, a view that materializes the data in a format suited to the required results set. The\nMaterialized View pattern describes generating prepopulated views of data in environments where the source data isn't in a suitable format for querying, where generating a suitable query is difficult, or where query performance is poor due to the nature of the data or the data store.\nThese materialized views, which only contain data required by a query, allow applications to quickly obtain the information they need. In addition to joining tables or combining data entities, materialized views can include the current values of calculated columns or data items, the results of combining values or executing transformations on the data items, and values specified as part of the query. A materialized view can even be optimized for just a single query.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/architecture/patterns/materialized-view","question_text":"DRAG DROP -\nA company builds an application to allow developers to share and compare code. The conversations, code snippets, and links shared by people in the application are stored in a Microsoft Azure SQL Database instance. The application allows for searches of historical conversations and code snippets.\nWhen users share code snippets, the code snippet is compared against previously share code snippets by using a combination of Transact-SQL functions including SUBSTRING, FIRST_VALUE, and SQRT. If a match is found, a link to the match is added to the conversation.\nCustomers report the following issues:\n✑ Delays occur during live conversations\n✑ A delay occurs before matching links appear after code snippets are added to conversations\nYou need to resolve the performance issues.\nWhich technologies should you use? To answer, drag the appropriate technologies to the correct issues. Each technology may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","answer_ET":"","exam_id":65,"url":"https://www.examtopics.com/discussions/microsoft/view/56076-exam-dp-200-topic-5-question-4-discussion/","unix_timestamp":1624638720,"timestamp":"2021-06-25 18:32:00","question_id":211,"topic":"5","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0047600002.png"],"discussion":[{"upvote_count":"1","comment_id":"390643","content":"I really don't see how a materialized view is going to help with comparing text with substrings. What would even be in the materialized view? I'd just use memory-optimized table twice.","comments":[{"content":"I think it should be memory-optimized table for conversations (because this optimizes transactions and insertions in general), and columnstore index (clustered columnstore index in particular) which improves analytics and reporting in general. Matching links qualifies as analytics.","timestamp":"1624930380.0","upvote_count":"6","poster":"lgtiza","comment_id":"393397"}],"timestamp":"1624638720.0","poster":"hello_there_"}]},{"id":"D9xUJLhK4hg2tj4OkqzQ","answer_description":"Hash-distributed tables improve query performance on large fact tables.\nColumnstore indexes can achieve up to 100x better performance on analytics and data warehousing workloads and up to 10x better data compression than traditional rowstore indexes.\nIncorrect Answers:\nD, E: Round-robin tables are useful for improving loading speed.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-distribute https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-query-performance","unix_timestamp":1624441500,"timestamp":"2021-06-23 11:45:00","isMC":true,"discussion":[{"upvote_count":"3","content":"Clustered indexes may outperform clustered columnstore tables when a single row needs to be quickly retrieved. For queries where a single or very few row lookup is required to perform with extreme speed, consider a clustered index or nonclustered secondary index. The answer could be B as it is sales key-based rows retrieval.","timestamp":"1624441500.0","poster":"Hinzzz","comments":[{"comment_id":"390649","content":"I agree under the assumption that the table is only used for row retrieval by primary key. It isn't explicitly said that this is the only use though. If this really is the only use of the table, it would make more sense to put it in a CosmosDB with table API. Still, I think that B is the answer they want to hear. Otherwise, why would they have added that queries will use row retrieval by primary key?","timestamp":"1624639080.0","poster":"hello_there_","upvote_count":"1"}],"comment_id":"388678"}],"answer":"A","answers_community":[],"question_id":212,"answer_images":[],"topic":"5","url":"https://www.examtopics.com/discussions/microsoft/view/55900-exam-dp-200-topic-5-question-5-discussion/","choices":{"A":"hash distributed table with clustered ColumnStore index","B":"hash distributed table with clustered index","E":"round robin distributed table with clustered ColumnStore index","C":"heap table with distribution replicate","D":"round robin distributed table with clustered index"},"question_text":"You implement an enterprise data warehouse in Azure Synapse Analytics.\nYou have a large fact table that is 10 terabytes (TB) in size.\nIncoming queries use the primary key Sale Key column to retrieve data as displayed in the following table:\n//IMG//\n\nYou need to distribute the large fact table across multiple nodes to optimize performance of the table.\nWhich technology should you use?","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0047700001.jpg"],"exam_id":65,"answer_ET":"A"},{"id":"T9NcZYNt3oijc4WuvEHx","url":"https://www.examtopics.com/discussions/microsoft/view/11676-exam-dp-200-topic-5-question-8-discussion/","choices":{"A":"Implement event ordering","B":"Scale the SU count for the job up","D":"Scale the SU count for the job down","C":"Implement Azure Stream Analytics user-defined functions (UDF)","F":"Implement query parallelization by partitioning the data input","E":"Implement query parallelization by partitioning the data output"},"question_id":213,"isMC":true,"timestamp":"2020-01-09 19:49:00","unix_timestamp":1578595740,"question_text":"A company has a real-time data analysis solution that is hosted on Microsoft Azure. The solution uses Azure Event Hub to ingest data and an Azure Stream\nAnalytics cloud job to analyze the data. The cloud job is configured to use 120 Streaming Units (SU).\nYou need to optimize performance for the Azure Stream Analytics job.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer_ET":"BF","exam_id":65,"topic":"5","question_images":[],"answer_images":[],"answers_community":[],"answer":"BF","discussion":[{"upvote_count":"17","comment_id":"188331","poster":"Cassielovedata","content":"https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization\nThe link above from Azure has the following statement:\nThis article shows you how to take advantage of parallelization in Azure Stream Analytics. You learn how to scale Stream Analytics jobs by configuring input partitions and tuning the analytics query definition. \nI think it implies that once you partition the input, the output will be partitioned according to your query. There is no way that you can directly partition the output. Besides, the table of contents on the left shows that SU is one way to optimize the Stream Analysis. Thus, the answer should be B and F","timestamp":"1601206980.0"},{"upvote_count":"10","comment_id":"44207","content":"I think the correct answer are\n E. Implement query parallelization by partitioning the data output\n F. Implement query parallelization by partitioning the data input\nBecause increasing number of streaming units for a job might not reduce SU% Utilization if your query is not fully parallel.\nAnd I think 120 RU should be enought if we consider that 6 RU is the full capacity of a single computing node.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption","comments":[{"timestamp":"1581505500.0","comments":[{"poster":"dumpsm42","comments":[{"content":"Partitions in inputs and outputs\nPartitioning lets you divide data into subsets based on a partition key. If your input (for example Event Hubs) is partitioned by a key, it is highly recommended to specify this partition key when adding input to your Stream Analytics job. Scaling a Stream Analytics job takes advantage of partitions in the input and output. A Stream Analytics job can consume and write different partitions in parallel, which increases throughput.","timestamp":"1607727840.0","upvote_count":"1","comment_id":"241260","poster":"dumpsm42"}],"timestamp":"1607557500.0","upvote_count":"1","content":"hmmm i agree with your first assumption, 120 RU seems for me already a large number, i think microsoft wants us to really choose E and F; even if from azure event hub the data comes with partitions, as stated in the links we must explicity set the partition by for the analytics job so.... hm.... E and F. Sorry.","comment_id":"239595"}],"poster":"epgd","content":"But the question only about \"Azure Event Hub to ingest data and an Azure Stream\nAnalytics cloud job to analyze the data\".\nSo B. Scale the SU count for the job up\n F. Implement query parallelization by partitioning the data input","comment_id":"49454","upvote_count":"31"}],"poster":"epgd","timestamp":"1580298540.0"},{"comment_id":"356320","timestamp":"1620905460.0","poster":"Qrm_1972","content":"I have seen this question and the answer in other sites , the correct answer 100% is : BF","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"1298225","timestamp":"1728994800.0","poster":"drosen","content":"Increasing the number of streaming units (SU) is not exactly an optimization, but rather an improvement by adding resources. Focusing on optimizing with current resources is more accurate.\n\nIn that case, the best options would be:\n\nImplement query parallelization by partitioning the data input: Distribute the initial workload, improving performance without the need to add additional resources.\n\nImplement query parallelization by partitioning the data output: Ensure that data processing and delivery are also distributed efficiently, optimizing the entire workflow."}]},{"comment_id":"306214","poster":"akram786","content":"E and F as answer for optimization","timestamp":"1615275960.0","upvote_count":"2"},{"content":"Streaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. The higher the number of SUs, the more CPU and memory resources are allocated for your job.\nScaling up does increase performance but is not a good way to optimize jobs. I would say E and F as the answer","timestamp":"1606387380.0","comment_id":"228235","upvote_count":"2","poster":"syu31svc","comments":[{"poster":"syu31svc","comment_id":"230301","content":"Changing to B and E since input is already partitioned","upvote_count":"1","timestamp":"1606653360.0"}]},{"poster":"Andrexx","timestamp":"1603980120.0","content":"In my opinion the answer is correct. The cloud job is already configured to use 120 Streaming Units (SU), so if we partition the input, we must scale the SU up to support the extra SU load that will be generated by this partitioning. Makes sense?","upvote_count":"8","comment_id":"208612"},{"poster":"ADHDBA","upvote_count":"2","timestamp":"1578595740.0","comment_id":"37192","comments":[{"poster":"damew26089","content":"If it improves the performance of the jobs of the SQL Server, sure?","comments":[{"timestamp":"1592199600.0","comment_id":"110571","upvote_count":"2","poster":"diulin","content":"I agree with ADHDBA, the question is about optimizing and not improving performance.\nE and F.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization says:\n\n \" If your input (for example Event Hubs) is partitioned by a key, it is highly recommended to specify this partition key when adding input to your Stream Analytics job. Scaling a Stream Analytics job takes advantage of partitions in the input and output. A Stream Analytics job can consume and write different partitions in parallel, which increases throughput.\""}],"timestamp":"1590476400.0","upvote_count":"4","comment_id":"95831"}],"content":"How is adding extra steaming units optimizing performance ? Is adding RAM on a SQL Server optimizing performance ?"}],"answer_description":"Scale out the query by allowing the system to process each input partition separately.\nF: A Stream Analytics job definition includes inputs, a query, and output. Inputs are where the job reads the data stream from.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization"},{"id":"3ila9OoXpUX9g6A84ZyF","question_text":"You have to create a new single database instance of Microsoft Azure SQL database. You must ensure that client connections are accepted via a workstation.\nThe workstation will use SQL Server Management Studio to connect to the database instance.\nWhich of the following Powershell commands would you execute to create and configure the database? (Choose three.)","choices":{"C":"New-AzureRmSqlServer","E":"New-AzureRmSqlDatabase","B":"New-AzureRmSqlServerFirewallRule","D":"New-AzureRmSqlServerVirtualNetworkRule","A":"New-AzureRmSqlElasticPool"},"answers_community":[],"question_images":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0048300001.png"],"unix_timestamp":1630259820,"topic":"6","timestamp":"2021-08-29 19:57:00","question_id":214,"answer_ET":"BCE","answer_description":"The Microsoft documentation clearly gives the steps to create and configure the database. Please note the below snippet shows the new powershell commands, but you can also use the older Azure PowerShell commands.\n\nSince this is clearly given in the documentation, all other options are incorrect.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/scripts/sql-database-create-and-configure-database-powershell","url":"https://www.examtopics.com/discussions/microsoft/view/60970-exam-dp-200-topic-6-question-1-discussion/","exam_id":65,"isMC":true,"answer":"BCE","discussion":[{"comment_id":"434783","timestamp":"1630259820.0","content":"correct","poster":"jaykumarjkd99","upvote_count":"1"}]},{"id":"EUOnqVXFi7wA7hMinvP0","topic":"6","answers_community":[],"answer_description":"Yes, this will give you a good idea on the load on the Azure HDInsight cluster.\nThe Microsoft documentation mentions the following:\n\nMonitor cluster load -\nHadoop clusters can deliver the most optimal performance when the load on cluster is evenly distributed across all the nodes. This enables the processing tasks to run without being constrained by RAM, CPU, or disk resources on individual nodes.\nTo get a high-level look at the nodes of your cluster and their loading, sign in to the Ambari Web UI, then select the Hosts tab. Your hosts are listed by their fully qualified domain names. Each host's operating status is shown by a colored health indicator:\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-key-scenarios-to-monitor","isMC":true,"answer":"A","url":"https://www.examtopics.com/discussions/microsoft/view/57639-exam-dp-200-topic-6-question-108-discussion/","exam_id":65,"timestamp":"2021-07-11 16:51:00","unix_timestamp":1626015060,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0064900001.png"],"question_images":[],"answer_ET":"A","question_id":215,"question_text":"You have to deploy resources on Azure HDInsight for a batch processing job. The batch processing must run daily and must scale to minimize costs. You also be able to monitor cluster performance.\nYou need to decide on a tool that will monitor the clusters and provide information on suggestions on how to scale.\nYou decide on monitoring the cluster load by using the Ambari Web UI.\nWould this fulfill the requirement?","discussion":[{"comment_id":"404063","poster":"Avinash75","timestamp":"1626015060.0","content":"Answer should be B . Ambari web ui provides monitoring but is not able to provides insights on scaling","upvote_count":"1"}],"choices":{"A":"Yes","B":"No"}}],"exam":{"id":65,"isMCOnly":false,"provider":"Microsoft","numberOfQuestions":228,"isImplemented":true,"name":"DP-200","lastUpdated":"12 Apr 2025","isBeta":false},"currentPage":43},"__N_SSP":true}