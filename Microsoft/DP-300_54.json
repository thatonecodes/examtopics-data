{"pageProps":{"questions":[{"id":"patS15B8KnDLzYjQNOEA","answer_description":"","question_images":[],"answer_images":[],"unix_timestamp":1730521620,"url":"https://www.examtopics.com/discussions/microsoft/view/150638-exam-dp-300-topic-5-question-34-discussion/","answer":"D","isMC":true,"timestamp":"2024-11-02 05:27:00","exam_id":68,"answers_community":["D (100%)"],"question_text":"You have an Azure subscription that contains a SQL Server on Azure Virtual Machines instance named SQLVM1. SQLVM1 has the following configurations:\n\n• Automated patching is enabled.\n• The SQL Server IaaS Agent extension is installed.\n• The Microsoft SQL Server instance on SQLVM1 is managed by using the Azure portal.\n\nYou need to automate the deployment of cumulative updates to SQLVM1 by using Azure Update Manager. The solution must ensure that the SQL Server instance on SQLVM1 can be managed by using the Azure portal.\n\nWhat should you do first on SQLVM1?","answer_ET":"C","question_id":266,"discussion":[{"timestamp":"1737640980.0","comment_id":"1345433","upvote_count":"1","poster":"voodoo_sh","content":"Selected Answer: D\nD - Disable automated patching in SQL Iaas Agent extension"},{"poster":"voodoo_sh","content":"D - Set Automated patching to disable\n\nTo automate the deployment of cumulative updates to SQL Server using Azure Update Manager, while ensuring that SQLVM1 can still be managed via the Azure portal, you should first disable Automated Patching. This is necessary because using both Automated Patching and Azure Update Manager simultaneously can lead to conflicts and unexpected behavior","timestamp":"1731951300.0","comment_id":"1314105","upvote_count":"2"},{"upvote_count":"1","comment_id":"1306054","poster":"2f5c7cd","content":"Selected Answer: D\nhttps://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/automated-patching?view=azuresql\n\nChoose only one option to automatically update your VM as overlapping tools may lead to failed updates.","timestamp":"1730521620.0"}],"topic":"5","choices":{"D":"Set Automated patching to Disable.","A":"Install the Azure Monitor Agent.","B":"Uninstall the SQL Server IaaS Agent extension.","C":"Install the Log Analytics agent."}},{"id":"DCYi3jpunTcw16xaDGkh","choices":{"C":"a run group ID","D":"a user property","B":"a resource tag","E":"a correlation ID","A":"an annotation"},"question_id":267,"question_text":"You have an Azure Data Factory that contains 10 pipelines.\nYou need to label each pipeline with its main purpose of either ingest, transform, or load. The labels must be available for grouping and filtering when using the monitoring experience in Data Factory.\nWhat should you add to each pipeline?","answer":"A","answer_ET":"A","discussion":[{"poster":"Ciupaz","timestamp":"1731089520.0","upvote_count":"3","comment_id":"714028","content":"Azure Data Factory is out of DP-300 exam."},{"comment_id":"579503","content":"DP-203","poster":"cusman","upvote_count":"4","timestamp":"1711976040.0"},{"comment_id":"492744","content":"Selected Answer: A\nhttps://www.techtalkcorner.com/monitor-azure-data-factory-annotations/","poster":"stdevops","timestamp":"1701544440.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"475709","content":"its ok?","poster":"o2091","timestamp":"1699641240.0"}],"answer_images":[],"answer_description":"Azure Data Factory annotations help you easily filter different Azure Data Factory objects based on a tag. You can define tags so you can see their performance or find errors faster.\nReference:\nhttps://www.techtalkcorner.com/monitor-azure-data-factory-annotations/","topic":"5","unix_timestamp":1636569240,"exam_id":68,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/65797-exam-dp-300-topic-5-question-4-discussion/","isMC":true,"answers_community":["A (100%)"],"timestamp":"2021-11-10 19:34:00"},{"id":"yr0t6SovGgEzgorhKuq2","answer_images":["https://www.examtopics.com/assets/media/exam-media/04275/0023700002.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/65798-exam-dp-300-topic-5-question-5-discussion/","unix_timestamp":1636569240,"exam_id":68,"answers_community":[],"answer_description":"Box 1: No -\nJust one failure within the 5-minute interval.\n\nBox 2: No -\nJust two failures within the 5-minute interval.\n\nBox 3: No -\nJust two failures within the 5-minute interval.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-metric-overview","topic":"5","question_images":["https://www.examtopics.com/assets/media/exam-media/04275/0023600001.jpg","https://www.examtopics.com/assets/media/exam-media/04275/0023600002.jpg","https://www.examtopics.com/assets/media/exam-media/04275/0023600008.png","https://www.examtopics.com/assets/media/exam-media/04275/0023700001.png"],"isMC":false,"question_text":"HOTSPOT -\nYou have an Azure data factory that has two pipelines named PipelineA and PipelineB.\nPipelineA has four activities as shown in the following exhibit.\n//IMG//\n\nPipelineB has two activities as shown in the following exhibit.\n//IMG//\n\nYou create an alert for the data factory that uses Failed pipeline runs metrics for both pipelines and all failure types. The metric has the following settings:\n✑ Operator: Greater than\n✑ Aggregation type: Total\n✑ Threshold value: 2\n✑ Aggregation granularity (Period): 5 minutes\n✑ Frequency of evaluation: Every 5 minutes\nData Factory monitoring records the failures shown in the following table.\n//IMG//\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_id":268,"answer_ET":"","discussion":[{"comment_id":"696146","poster":"Ciupaz","content":"Not related to DP-300 exam","timestamp":"1697449020.0","upvote_count":"5"},{"comment_id":"953119","content":"bouncer for me..","upvote_count":"1","timestamp":"1721113380.0","poster":"Surjit24"},{"timestamp":"1715747400.0","content":"The answer No-No-No is correct.\nEvery 5 minutes period you need at least 3 errors in order to trigger the alert.","upvote_count":"1","poster":"Robrol","comment_id":"898031"},{"poster":"alexatl","content":"same here, do not understand the explanation","timestamp":"1706291880.0","comment_id":"788983","upvote_count":"2"},{"content":"explanation does not make sense","comment_id":"773635","timestamp":"1705072740.0","upvote_count":"1","poster":"alexatl"},{"comment_id":"579507","poster":"cusman","content":"DP-203","timestamp":"1680353700.0","upvote_count":"3"},{"content":"its ok?","timestamp":"1668105240.0","upvote_count":"1","poster":"o2091","comment_id":"475710"}],"answer":"","timestamp":"2021-11-10 19:34:00"},{"id":"tA5V6lPrU1D8uAPHdQ3A","timestamp":"2021-11-10 19:34:00","answer_images":[],"answers_community":[],"topic":"5","isMC":true,"exam_id":68,"answer_ET":"B","url":"https://www.examtopics.com/discussions/microsoft/view/65799-exam-dp-300-topic-5-question-6-discussion/","question_images":[],"answer":"B","answer_description":"Correct solution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse.\nReference:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data","discussion":[{"upvote_count":"6","poster":"cusman","content":"DP-203","timestamp":"1648817700.0","comment_id":"579511"},{"timestamp":"1729781580.0","content":"it seems that mapping data flow are unable to execute R script... like Azure databricks notebook do:\nIn Azure Databricks, notebooks are the primary tool for creating data science and machine learning workflows and collaborating with colleagues. Databricks notebooks provide real-time coauthoring in multiple languages, automatic versioning, and built-in data visualizations.","upvote_count":"1","comment_id":"1302505","poster":"MVFGrant"},{"upvote_count":"3","comment_id":"696147","poster":"Ciupaz","timestamp":"1665913080.0","content":"Not related to DP-300 exam."},{"comment_id":"475711","upvote_count":"1","poster":"o2091","content":"looks correct","timestamp":"1636569240.0"}],"unix_timestamp":1636569240,"question_id":269,"choices":{"B":"No","A":"Yes"},"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse.\nDoes this meet the goal?"},{"id":"vS4xJPfuT0qedN7p1UT0","answer_ET":"B","timestamp":"2021-11-10 19:27:00","choices":{"A":"Yes","B":"No"},"exam_id":68,"unix_timestamp":1636568820,"topic":"5","answer_description":"Must use an Azure Data Factory, not an Azure Databricks job.\nCorrect solution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse.\nReference:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/65792-exam-dp-300-topic-5-question-7-discussion/","answer_images":[],"discussion":[{"upvote_count":"1","timestamp":"1729782120.0","poster":"MVFGrant","content":"Answer is NO\nAzure Data Factory is the best solution to simple easy data migration , even you can use Databricks notebooks for some script code migrations, when the work is more complex , you should use Databricks , but not for simple easy problems , is Overkill and a waste of resources and money\nis true , you can do it using","comment_id":"1302506"},{"content":"Selected Answer: A\nAnswer should be yes.","comment_id":"1253124","upvote_count":"1","poster":"scottytohotty","timestamp":"1721657040.0"},{"comments":[{"content":"DP-203, Question! \nBUT Ans is Yes Databricks can do that and very easily.","poster":"Sr18","comment_id":"1239278","upvote_count":"1","timestamp":"1719666900.0"}],"poster":"U_C","comment_id":"863977","content":"Yes, this solution meets the goal. By scheduling an Azure Databricks job that executes an R notebook, you can transform the data from the staging zone in your Azure Data Lake Storage account. Then, by inserting the data into the data warehouse in Azure Synapse Analytics, you can complete the daily process of ingesting incremental data. So, the answer is A. Yes.","timestamp":"1680877380.0","upvote_count":"3"},{"upvote_count":"2","timestamp":"1677403860.0","comment_id":"822199","poster":"KIET2131","content":"A. Yes, this solution meets the goal of ingesting incremental data from the staging zone, transforming the data by executing an R script, and inserting the transformed data into a data warehouse in Azure Synapse Analytics using Azure Databricks. The scheduled Azure Databricks job can be used to execute the R notebook and insert the transformed data into the data warehouse."},{"content":"Not related to DP-300 exam.","comment_id":"696148","upvote_count":"2","poster":"Ciupaz","timestamp":"1665913080.0"},{"content":"DP-203","timestamp":"1648817760.0","poster":"cusman","comment_id":"579514","upvote_count":"4"},{"timestamp":"1636568820.0","content":"looks good, what do you think?","comment_id":"475700","upvote_count":"1","poster":"o2091"}],"answers_community":["A (100%)"],"isMC":true,"question_id":270,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse.\nDoes this meet the goal?","answer":"A"}],"exam":{"lastUpdated":"12 Apr 2025","name":"DP-300","isImplemented":true,"id":68,"isMCOnly":false,"isBeta":false,"provider":"Microsoft","numberOfQuestions":360},"currentPage":54},"__N_SSP":true}