{"pageProps":{"questions":[{"id":"PeTEJP6OvrNyOHuQHBao","question_id":71,"unix_timestamp":1735381800,"answer_images":[],"question_images":[],"choices":{"B":"No","A":"Yes"},"url":"https://www.examtopics.com/discussions/microsoft/view/153566-exam-dp-420-topic-3-question-18-discussion/","answer":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have a container named container1 in an Azure Cosmos DB for NoSQL account.\n\nYou need to make the contents of container1 available as reference data for an Azure Stream Analytics job.\n\nSolution: You create an Azure function to copy data to another Azure Cosmos DB for NoSQL container.\n\nDoes this meet the goal?","timestamp":"2024-12-28 11:30:00","answers_community":["B (100%)"],"topic":"3","exam_id":69,"answer_description":"","discussion":[{"content":"Selected Answer: B\nCreating an Azure Function to copy data to another Azure Cosmos DB for NoSQL container does not meet the stated goal. To make the contents of container1 available as reference data for an Azure Stream Analytics job, you should directly connect the Azure Stream Analytics job to the Azure Cosmos DB container using its input capabilities. Azure Stream Analytics supports Azure Cosmos DB as a direct input source, so copying data to another container is unnecessary and does not align with the requirement.","timestamp":"1741623660.0","poster":"Tuopikson","upvote_count":"1","comment_id":"1383803"},{"upvote_count":"1","content":"Selected Answer: B\nNo - correct","timestamp":"1735381800.0","comment_id":"1332905","poster":"3709334"}],"answer_ET":"B","isMC":true},{"id":"GtLUWN8uDKal1LzfgzNG","timestamp":"2022-07-15 15:56:00","answer_ET":"B","question_id":72,"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/77532-exam-dp-420-topic-3-question-2-discussion/","answers_community":["B (73%)","D (27%)"],"answer":"B","answer_images":[],"choices":{"D":"Configure the report to query a new aggregate container. Populate the aggregates by using SQL queries that run daily.","A":"Configure the report to query orders by using a SQL query.","C":"Configure the report to query orders by using a SQL query through a dedicated gateway.","B":"Configure the report to query a new aggregate container. Populate the aggregates by using the change feed."},"isMC":true,"question_text":"The following is a sample of a document in orders.\n//IMG//\n\nThe orders container uses customerId as the partition key.\nYou need to provide a report of the total items ordered per month by item type. The solution must meet the following requirements:\n✑ Ensure that the report can run as quickly as possible.\n✑ Minimize the consumption of request units (RUs).\nWhat should you do?","exam_id":69,"topic":"3","question_images":["https://www.examtopics.com/assets/media/exam-media/04276/0007300001.png"],"unix_timestamp":1657893360,"discussion":[{"comment_id":"631784","content":"Selected Answer: B\nAfter processing items in the change feed, you can build a materialized view and persist aggregated values back in Azure Cosmos DB. If you're using Azure Cosmos DB to build a game, you can, for example, use change feed to implement real-time leaderboards based on scores from completed games.\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql/change-feed-design-patterns#high-availability","poster":"grada","upvote_count":"10","timestamp":"1673798160.0"},{"comment_id":"1343038","upvote_count":"1","poster":"YellowSky002","content":"Selected Answer: B\nFor most scenarios, using the change feed to populate aggregates in Cosmos DB is the more RU-efficient approach. It allows you to process changes incrementally, update aggregates in real-time, and avoid the overhead of full scans. However, consider your specific data volume, change frequency, and aggregate complexity to choose the most appropriate strategy.","timestamp":"1737297720.0"},{"poster":"Blubb1860","timestamp":"1719643380.0","content":"Selected Answer: B\nB is correct, no additional SQL queries necessary","upvote_count":"1","comment_id":"1108496"},{"content":"Selected Answer: D\nThe answer is D","poster":"comoon","comment_id":"991295","upvote_count":"1","timestamp":"1709027760.0"},{"timestamp":"1702328340.0","upvote_count":"2","poster":"azuredemo2022three","content":"Selected Answer: B","comment_id":"920923"},{"comment_id":"882629","content":"Selected Answer: B\nOption D suggests populating the aggregates by using SQL queries that run daily. While this may reduce the RU consumption during querying, it may not necessarily minimize RU consumption overall. Additionally, this approach may result in stale data since the aggregates are only updated once a day.\n\nThe best approach to minimize RU consumption and ensure the report runs as quickly as possible is to use the change feed to populate the aggregates in real-time, as suggested in option B. This way, the aggregates are always up-to-date, and the report can be generated quickly and with minimal RU consumption.","upvote_count":"3","poster":"[Removed]","timestamp":"1698409680.0"},{"upvote_count":"2","poster":"BOT_123","comment_id":"790390","timestamp":"1690524180.0","content":"Selected Answer: B\nB Is Correct"},{"timestamp":"1680109260.0","poster":"TimSss","content":"Selected Answer: D\nI would go with D, we need to minimize RU, so we run the job daily which should be fine for these types of reports (using monthly data). Using the change feed costs much more RU.","upvote_count":"4","comment_id":"682886"},{"upvote_count":"2","timestamp":"1677058620.0","poster":"remz","content":"Selected Answer: B\nB Is Correct","comment_id":"650151"},{"content":"Selected Answer: D\nI would go with D, as we need to reduce RU. An additional container writes costs RU and the change-feed will be updated every time a new item appears.","comment_id":"643056","poster":"Gall","upvote_count":"2","timestamp":"1675627500.0"}]},{"id":"wMGzfC74OkUwuWy7Qpjj","isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have a container named container1 in an Azure Cosmos DB Core (SQL) API account.\n\nYou need to make the contents of container1 available as reference data for an Azure Stream Analytics job.\n\nSolution: You create an Azure function that uses Azure Cosmos DB for NoSQL change feed as a trigger and Azure event hub as the output.\n\nDoes this meet the goal?","topic":"3","discussion":[{"upvote_count":"1","timestamp":"1741984980.0","comment_id":"1395718","poster":"matejka","content":"Selected Answer: A\nA is correct"},{"content":"No, this solution does not meet the goal. \n\nTo use Azure Cosmos DB as reference data for an **Azure Stream Analytics job**, you would typically use Cosmos DB as a direct input or reference data source for the Stream Analytics job. Azure Stream Analytics supports Cosmos DB Core (SQL) API directly as a reference data source.\n\nThe provided solution involves using **Azure Functions** with Cosmos DB change feed as a trigger and outputting to **Azure Event Hub**, which is more suitable for real-time event-driven scenarios, but it doesn't directly provide Cosmos DB data as reference data to Stream Analytics.\n\nTo meet the goal, you should configure Cosmos DB as a reference data source in the Stream Analytics job instead of using the change feed and event hub combination.","timestamp":"1727235240.0","upvote_count":"1","poster":"8a6a5d3","comment_id":"1288864"},{"timestamp":"1715067360.0","poster":"[Removed]","content":"Selected Answer: B\nThis is a series of questions with multiple scenarios. Correct options are:\n1. You create an Azure Synapse pipeline that uses Azure Cosmos DB Core (SQL) API as the input and Azure Blob Storage as the output.\n2. You create an Azure Data Factory pipeline that uses Azure Cosmos DB Core (SQL) API as the input and Azure Blob Storage as the output.\nAll other options are incorrect.","upvote_count":"2","comment_id":"1207772"}],"answer_images":[],"timestamp":"2024-05-07 09:36:00","question_images":[],"exam_id":69,"unix_timestamp":1715067360,"url":"https://www.examtopics.com/discussions/microsoft/view/140119-exam-dp-420-topic-3-question-20-discussion/","answer":"B","answers_community":["B (67%)","A (33%)"],"choices":{"A":"Yes","B":"No"},"answer_ET":"A","question_id":73,"answer_description":""},{"id":"HjxRa3A3Sa8ZKAxgnh2J","timestamp":"2024-12-28 15:00:00","url":"https://www.examtopics.com/discussions/microsoft/view/153570-exam-dp-420-topic-3-question-21-discussion/","choices":{"A":"Yes","B":"No"},"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have a container named container1 in an Azure Cosmos DB Core (SQL) API account.\n\nYou need to make the contents of container1 available as reference data for an Azure Stream Analytics job.\n\nSolution: You create an Azure Synapse pipeline that uses Azure Cosmos DB for NoSQL as the input and Azure Blob Storage as the output.\n\nDoes this meet the goal?","discussion":[{"poster":"3709334","upvote_count":"1","timestamp":"1735394400.0","comment_id":"1332985","content":"Selected Answer: A\nAzure synapse pipeline with Azure cosmos DB as input and blob storage as output\nAzure data factory pipeline with Azure cosmos DB as input and blob storage as output"}],"exam_id":69,"topic":"3","answer_ET":"B","answer_description":"","answer_images":[],"isMC":true,"question_id":74,"answer":"A","question_images":[],"unix_timestamp":1735394400,"answers_community":["A (100%)"]},{"id":"plz66G8N2tbK7aqoTFfr","answer_images":[],"answers_community":["A (100%)"],"answer":"A","discussion":[{"upvote_count":"1","timestamp":"1739728440.0","content":"Selected Answer: A\nSynapse and Factory with blob storage as output","poster":"szkielet","comment_id":"1357321"},{"content":"Selected Answer: A\nAzure synapse pipeline with Azure cosmos DB as input and blob storage as output\nAzure data factory pipeline with Azure cosmos DB as input and blob storage as output","poster":"3709334","timestamp":"1735394400.0","comment_id":"1332987","upvote_count":"2"}],"timestamp":"2024-12-28 15:00:00","question_images":[],"choices":{"A":"Yes","B":"No"},"answer_description":"","exam_id":69,"url":"https://www.examtopics.com/discussions/microsoft/view/153571-exam-dp-420-topic-3-question-22-discussion/","unix_timestamp":1735394400,"answer_ET":"B","question_id":75,"isMC":true,"topic":"3","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have a container named container1 in an Azure Cosmos DB Core (SQL) API account.\n\nYou need to make the contents of container1 available as reference data for an Azure Stream Analytics job.\n\nSolution: You create an Azure Data Factory pipeline that uses Azure Cosmos DB for NoSQL as the input and Azure Blob Storage as the output.\n\nDoes this meet the goal?"}],"exam":{"isImplemented":true,"id":69,"provider":"Microsoft","numberOfQuestions":147,"isMCOnly":false,"name":"DP-420","lastUpdated":"12 Apr 2025","isBeta":false},"currentPage":15},"__N_SSP":true}