{"pageProps":{"questions":[{"id":"DRvM1txk3iBQ5X7Lit7f","question_images":[],"question_id":96,"exam_id":40,"url":"https://www.examtopics.com/discussions/microsoft/view/91092-exam-ai-102-topic-2-question-20-discussion/","isMC":true,"answers_community":["B (100%)"],"answer_ET":"B","choices":{"B":"Host the Computer Vision endpoint in a container on an on-premises server.","D":"Build an Azure web app to query the Computer Vision endpoint.","C":"Host an exported Open Neural Network Exchange (ONNX) model on an on-premises server.","A":"Build an on-premises web app to query the Computer Vision endpoint."},"topic":"2","unix_timestamp":1670806260,"timestamp":"2022-12-12 01:51:00","question_text":"You need to build a solution that will use optical character recognition (OCR) to scan sensitive documents by using the Computer Vision API. The solution must\nNOT be deployed to the public cloud.\nWhat should you do?","answer_images":[],"answer":"B","answer_description":"","discussion":[{"poster":"syupwsh","timestamp":"1739164020.0","content":"Selected Answer: B\nHost the Computer Vision endpoint in a container on an on-premises server is correct because this approach allows you to run the Computer Vision API locally without needing to deploy your solution to the public cloud. This setup ensures that sensitive documents are processed within your local environment, meeting the requirement to avoid the public cloud.\n\nAnswer is B","comment_id":"1354241","upvote_count":"1"},{"content":"Selected Answer: B\nI say this answer is B. Please hurry up and transport the meat.","comment_id":"1235172","poster":"HaraTadahisa","timestamp":"1719036900.0","upvote_count":"3"},{"timestamp":"1716906000.0","poster":"taiwan_is_not_china","upvote_count":"3","content":"Selected Answer: B\nB is the correct answer.","comment_id":"1220335"},{"timestamp":"1716213960.0","poster":"takaimomoGcup","comment_id":"1214381","content":"Selected Answer: B\nB is right answer.","upvote_count":"3"},{"upvote_count":"1","content":"Selected Answer: B\nmodel is hosted on-premise but the billing information has to be provided to the on-premise container to report the usage","poster":"evangelist","timestamp":"1708123080.0","comment_id":"1152261"},{"content":"Selected Answer: B\ncorrect answer\nhttps://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-container-support","upvote_count":"3","comment_id":"1062001","timestamp":"1699093260.0","poster":"rdemontis"},{"comment_id":"936559","timestamp":"1687951560.0","upvote_count":"4","poster":"zellck","content":"Selected Answer: B\nB is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/deploy-computer-vision-on-premises\nOne option to manage your Computer Vision containers on-premises is to use Kubernetes and Helm. Using Kubernetes and Helm to define a Computer Vision container image, we'll create a Kubernetes package. This package will be deployed to a Kubernetes cluster on-premises."},{"poster":"RAN_L","timestamp":"1678877580.0","content":"Selected Answer: B\nB. Host the Computer Vision endpoint in a container on an on-premises server.\n\nSince the solution should not be deployed to the public cloud, option B is the correct answer. By hosting the Computer Vision endpoint in a container on an on-premises server, the solution can still leverage the capabilities of the Computer Vision API while keeping the processing and data within the on-premises environment. Option A and D both involve using a web app, which would likely require hosting in the public cloud. Option C involves hosting an exported ONNX model, which may not have the same capabilities as the Computer Vision API.","upvote_count":"2","comment_id":"839794"},{"upvote_count":"2","comment_id":"742258","content":"Selected Answer: B\nAnswer is correct.","timestamp":"1670806260.0","poster":"HotDurian"}]},{"id":"5F54ROmfScX6mUGNOKeZ","answer":"B","exam_id":40,"question_id":97,"answers_community":["B (95%)","5%"],"choices":{"B":"optical character recognition (OCR)","C":"key phrase extraction","D":"document extraction","A":"image analysis"},"question_text":"You have an Azure Cognitive Search solution and a collection of handwritten letters stored as JPEG files.\n\nYou plan to index the collection. The solution must ensure that queries can be performed on the contents of the letters.\n\nYou need to create an indexer that has a skillset.\n\nWhich skill should you include?","isMC":true,"timestamp":"2023-03-15 11:57:00","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/102684-exam-ai-102-topic-2-question-21-discussion/","discussion":[{"comment_id":"839798","content":"Selected Answer: B\nTo ensure that queries can be performed on the contents of the letters, the skill that should be included in the indexer is optical character recognition (OCR).\n\nOption B, optical character recognition (OCR), is a technology that can recognize text within an image and convert it into machine-readable text. This skill will enable the search engine to read the handwritten letters and convert them into searchable text that can be indexed by Azure Cognitive Search.\n\nOption A, image analysis, is a useful skill for analyzing images to extract metadata, but it does not directly enable text recognition.\n\nOption C, key phrase extraction, extracts important phrases and concepts from text, but it requires the text to be already recognized and extracted by OCR or other text extraction techniques.\n\nOption D, document extraction, is a skill that extracts specific pieces of information from documents, but it does not address the challenge of recognizing and extracting text from handwritten letters.","poster":"RAN_L","timestamp":"1678877820.0","upvote_count":"14"},{"comment_id":"1354243","upvote_count":"1","timestamp":"1739164440.0","poster":"syupwsh","content":"Selected Answer: B\nOCR is specifically designed to extract text from images, including handwritten text from JPEG files. This will allow the contents of the handwritten letters to be indexed and made searchable.\n\nAnswer is B"},{"timestamp":"1727341920.0","comment_id":"1289367","poster":"fawzi008","upvote_count":"1","content":"Selected Answer: A\nImage Analysis includes OCR capabilities inside. \nhttps://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-image-analysis?tabs=4-0#analyze-image","comments":[{"timestamp":"1732907340.0","comment_id":"1319881","poster":"Alan_CA","upvote_count":"1","content":"your link refers to computer vision, not AI search"}]},{"timestamp":"1719036900.0","upvote_count":"1","poster":"HaraTadahisa","content":"Selected Answer: B\nI say this answer is B. Please hurry up and transport the meat.","comment_id":"1235171"},{"comment_id":"1220334","upvote_count":"1","poster":"taiwan_is_not_china","content":"Selected Answer: B\nOCR is the correct answer.","timestamp":"1716905940.0"},{"timestamp":"1716213660.0","content":"Why OCR?","comment_id":"1214375","poster":"takaimomoGcup","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1717457760.0","poster":"ks321","content":"Handwritten is the keyword","comment_id":"1223830"}]},{"upvote_count":"2","poster":"evangelist","content":"Selected Answer: B\nOCR to extract the text and then create an indexer on the text extracted","timestamp":"1708123140.0","comment_id":"1152262"},{"upvote_count":"1","content":"Selected Answer: B\nprovided answer is correct. OCR to scan handwritten documents","poster":"rdemontis","timestamp":"1699093560.0","comment_id":"1062004"},{"timestamp":"1687951380.0","poster":"zellck","upvote_count":"1","content":"Selected Answer: B\nB is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-ocr\nThe Optical character recognition (OCR) skill recognizes printed and handwritten text in image files.","comment_id":"936554"},{"content":"Therefore, option B, optical character recognition (OCR), is the most suitable skill to include in the indexer for indexing the contents of handwritten letters and making them searchable.","upvote_count":"1","comment_id":"935089","timestamp":"1687850160.0","poster":"ExamPage"}],"answer_description":"","answer_ET":"B","unix_timestamp":1678877820,"answer_images":[],"topic":"2"},{"id":"pXuYh0rkOhAEmlouUoKX","answer_ET":"","unix_timestamp":1679058300,"exam_id":40,"topic":"2","question_text":"HOTSPOT -\n\nYou have a library that contains thousands of images.\n\nYou need to tag the images as photographs, drawings, or clipart.\n\nWhich service endpoint and response property should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","question_id":98,"answer_description":"","isMC":false,"answer":"","answer_images":["https://img.examtopics.com/ai-102/image213.png"],"discussion":[{"comment_id":"841990","comments":[{"poster":"TJ001","comment_id":"1214175","upvote_count":"1","content":"out of box option this is the best bet if not custom vision and train the model for object detection - more work","timestamp":"1716189960.0"},{"poster":"mmaguero","upvote_count":"2","comments":[{"upvote_count":"1","poster":"PeteColag","timestamp":"1717201140.0","comment_id":"1222392","content":"This link no longer works"}],"comment_id":"910902","timestamp":"1685514060.0","content":"Agree, see json example at: https://westcentralus.dev.cognitive.microsoft.com/docs/services/computer-vision-v3-2/operations/56f91f2e778daf14a499f21b"},{"content":"agree and thanks for posting the related documentation","poster":"rdemontis","upvote_count":"1","timestamp":"1699093860.0","comment_id":"1062005"}],"timestamp":"1679058300.0","poster":"jimbojambo","content":"I think that the answers are wrong. They should be:\n1 - Computer Vision analyze image\n2 - imageType\n\nAccording to https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-detecting-image-types Computer Vision can analyze the content type of images, indicating whether an image is clip art or a line drawing","upvote_count":"55"},{"comment_id":"939652","comments":[{"poster":"zellck","comment_id":"946059","upvote_count":"6","content":"Gotten this in Jul 2023 exam.","timestamp":"1688774520.0"}],"poster":"zellck","upvote_count":"15","timestamp":"1688192880.0","content":"1. Computer Vision analyze images\n2. imageType\n\nhttps://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-detecting-image-types\nWith the Analyze Image API, Computer Vision can analyze the content type of images, indicating whether an image is clip art or a line drawing."},{"content":"Service Endpoint:\na. Computer Vision analyze images\n\nThis endpoint provides comprehensive analysis of images, including categorization.\n\nProperty:\nc. imageType\n\nThis property can be used to identify whether an image is a photograph, drawing, or clipart.\n\nSummary:\nService endpoint: Computer Vision analyze images\nProperty: imageType\n\nBy using the Computer Vision analyze images endpoint with the imageType property, we can effectively tag the images as photographs, drawings, or clipart.","poster":"gyaansastra","comment_id":"1361764","upvote_count":"1","timestamp":"1740549720.0"},{"content":"Computer Vision analyze images is CORRECT because this endpoint provides a variety of features for analyzing images, including categorization into types such as photographs, drawings, or clipart. It is well-suited for tagging large libraries of images with general descriptive tags.\n\nimageType is CORRECT because this property provides information about the type of image, such as whether it is a photograph, drawing, or clipart. This property is specifically designed to categorize the visual style or format of the image.","poster":"syupwsh","upvote_count":"1","timestamp":"1739340600.0","comment_id":"1355483"},{"content":"first is - Computer Vision analyze images\n2nd - imageType\n\ninfo here - https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-detecting-image-types","comment_id":"1285083","timestamp":"1726559940.0","poster":"mrg998","upvote_count":"1"},{"timestamp":"1721064360.0","comment_id":"1248476","poster":"krzkrzkra","upvote_count":"1","content":"1. Computer Vision analyze images\n2. imageType"},{"poster":"NagaoShingo","upvote_count":"1","comment_id":"1225485","timestamp":"1717677240.0","content":"1. Computer Vision analyze images\n2. imageType"},{"timestamp":"1716905820.0","content":"The following aligned answer is correct.\n1. Computer Vision analyze images\n2. imageType","poster":"taiwan_is_not_china","upvote_count":"1","comment_id":"1220331"},{"comment_id":"1218292","timestamp":"1716638520.0","content":"1. Computer Vision analyze images\n2. imageType","upvote_count":"1","poster":"nanaw770"},{"upvote_count":"1","poster":"takaimomoGcup","content":"Service endpoint should be \"Computer Vision analyze images\". Property should be \"imageType\".","comment_id":"1214379","timestamp":"1716213840.0"},{"content":"Appeared on Oct/29/2023.","poster":"trashbox","comment_id":"1056562","upvote_count":"5","timestamp":"1698552420.0"},{"upvote_count":"2","timestamp":"1696912800.0","content":"I would say the answers are correct. Image type can only indicate whether an image is clip art or a line drawing. It can’t tell you if it’s a photograph or not - you can’t just assume that if the image isn’t a clip art or a line drawing will automatically be categorized as a photograph. It’s a very sloppy solution IMO. Besides you have thousands of images and it’s a good reason to create your own model.","poster":"sl_mslconsulting","comment_id":"1039152","comments":[{"content":"The example at https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/concept-detecting-image-types\n\nimplies that if the \"imageType\": {\n \"clipArtType\": 0,\n \"lineDrawingType\": 0\n },\nthen we have an image.","comment_id":"1222401","poster":"PeteColag","upvote_count":"1","timestamp":"1717202460.0"}]},{"content":"To tag images as photographs, drawings, or clipart, you should use the following service endpoint and response property:\n\nService endpoint: Computer Vision image classification\nProperty: imageType\n\nThe Computer Vision image classification endpoint allows you to classify images into different categories, and the imageType property specifically provides information about the type of image, such as whether it is a photograph, drawing, or clipart.","comment_id":"924986","upvote_count":"5","timestamp":"1686904320.0","poster":"HarshSharma786"},{"timestamp":"1683553020.0","content":"ChatGPT:\nYou can use the Microsoft Azure Computer Vision API to tag the images as photographs, drawings, or clipart.\n\nYou can call the \"Describe Image\" API endpoint and use the \"imageType\" property of the response to determine if the image is a photograph, a drawing, or clipart. The \"imageType\" property can have the following values:\n\n\"Clipart\": Indicates that the image is a clipart.\n\"LineDrawing\": Indicates that the image is a line drawing.\n\"Photograph\": Indicates that the image is a photograph.\nYou can send an HTTP POST request to the API endpoint with the image file as the request body and specify the \"imageType\" in the \"visualFeatures\" parameter. The API will return a JSON response containing the \"imageType\" property along with other properties such as \"tags\", \"description\", and \"categories\".","comment_id":"892206","upvote_count":"2","poster":"ulloo"}],"answers_community":[],"timestamp":"2023-03-17 14:05:00","question_images":["https://img.examtopics.com/ai-102/image3.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/102936-exam-ai-102-topic-2-question-22-discussion/"},{"id":"YTfOQmLl8kzvTJd4dxOh","unix_timestamp":1678792620,"timestamp":"2023-03-14 12:17:00","question_images":[],"exam_id":40,"url":"https://www.examtopics.com/discussions/microsoft/view/102569-exam-ai-102-topic-2-question-23-discussion/","answers_community":["B (85%)","A (15%)"],"answer":"B","question_id":99,"isMC":true,"answer_ET":"B","answer_images":[],"discussion":[{"comment_id":"936552","content":"Selected Answer: B\nB is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/how-to/use-headpose#detect-head-gestures\nYou can detect head gestures like nodding and head shaking by tracking HeadPose changes in real time. You can use this feature as a custom liveness detector.\n\nLiveness detection is the task of determining that a subject is a real person and not an image or video representation. A head gesture detector could serve as one way to help verify liveness, especially as opposed to an image representation of a person.","comments":[{"comment_id":"1068484","content":"thanks for explanation","poster":"rdemontis","upvote_count":"2","timestamp":"1699789740.0"}],"poster":"zellck","timestamp":"1687951320.0","upvote_count":"19"},{"content":"Selected Answer: B\nValidating that subjects are real people can be done by detecting natural head movements. The HeadPose attribute provides the orientation of the head, and checking for natural changes in this attribute over time can help determine if the subject is a real person or a static image\n\nB is correct","comment_id":"1354248","poster":"syupwsh","timestamp":"1739164620.0","upvote_count":"1"},{"content":"Selected Answer: B\nBecause the Liveness Detection attribute is not in the list, HeadPose is the right answer","upvote_count":"1","timestamp":"1732907940.0","poster":"Alan_CA","comment_id":"1319884"},{"comment_id":"1235170","upvote_count":"1","timestamp":"1719036840.0","content":"Selected Answer: B\nI say this answer is B. Please hurry up and transport the meat.","poster":"HaraTadahisa"},{"content":"Selected Answer: B\nFaceAttributes.HeadPose is used this solution.","poster":"takaimomoGcup","upvote_count":"2","timestamp":"1716213720.0","comment_id":"1214376"},{"poster":"1668f51","comment_id":"1194050","timestamp":"1712879400.0","upvote_count":"3","content":"Selected Answer: A\n\"You need to use the Face service to validate that the subjects of the videos are real people.\" Never think too much into these questions. A can detect if it's real or not. Not asking anything else. It's A."},{"timestamp":"1696960140.0","comment_id":"1039698","content":"Selected Answer: B\nIt can’t be A. If you only try to detect the faces without tracking their position over time, the system can be easily fooled in this very specific scenario.","upvote_count":"1","poster":"sl_mslconsulting"},{"upvote_count":"3","poster":"msdfqwerfewf","content":"Selected Answer: A\nOption A is more appropriate for validating the presence of real people in the live video. By calling the face detection API and retrieving the face rectangle using the FaceRectangle attribute, you can detect and locate faces within the video frames. This helps in confirming the presence of actual human faces in the captured video.\n\nOption B, on the other hand, suggests repeatedly calling the face detection API and checking for changes to the FaceAttributes.HeadPose attribute. While head pose analysis can provide information about the orientation of detected faces, it may not be the most reliable approach for validating the authenticity of the subjects as real people. Checking for changes in head pose alone may not be sufficient to differentiate between real people and other forms of visual representations.","comment_id":"936167","timestamp":"1687930260.0"},{"poster":"ziggy1117","upvote_count":"2","content":"Selected Answer: B\nB: https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/how-to/use-headpose#detect-head-gestures","timestamp":"1686487620.0","comment_id":"920678"},{"upvote_count":"1","content":"B: https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/how-to/use-headpose#detect-head-gestures","comment_id":"919732","poster":"ziggy1117","timestamp":"1686362100.0"},{"poster":"Rob77","upvote_count":"3","comment_id":"890432","content":"B https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/how-to/use-headpose#detect-head-gestures\n\"Liveness detection is the task of determining that a subject is a real person and not an image or video representation\"","timestamp":"1683342720.0"},{"comment_id":"872308","upvote_count":"3","timestamp":"1681703400.0","poster":"Mike19D","content":"Selected Answer: B\nThe Answer is B. A could be a still picture"},{"poster":"marti_tremblay000","comment_id":"838775","upvote_count":"2","content":"Selected Answer: B\nThe answer is B\nDetect head gestures\nYou can detect head gestures like nodding and head shaking by tracking HeadPose changes in real time. You can use this feature as a custom liveness detector.\nReference https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/how-to/use-headpose","timestamp":"1678792620.0"}],"topic":"2","question_text":"You have an app that captures live video of exam candidates.\n\nYou need to use the Face service to validate that the subjects of the videos are real people.\n\nWhat should you do?","choices":{"B":"Call the face detection API repeatedly and check for changes to the FaceAttributes.HeadPose attribute.","A":"Call the face detection API and retrieve the face rectangle by using the FaceRectangle attribute.","C":"Call the face detection API and use the FaceLandmarks attribute to calculate the distance between pupils.","D":"Call the face detection API repeatedly and check for changes to the FaceAttributes.Accessories attribute."},"answer_description":""},{"id":"9OJ0PP1Wab2RwHEqZRzM","question_id":100,"isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/102686-exam-ai-102-topic-2-question-24-discussion/","answer_images":["https://img.examtopics.com/ai-102/image8.png"],"answer":"","exam_id":40,"unix_timestamp":1678880160,"answers_community":[],"answer_ET":"","question_text":"HOTSPOT\n-\n\nYou make an API request and receive the results shown in the following exhibits.\n\n//IMG//\n\n\n//IMG//\n\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","timestamp":"2023-03-15 12:36:00","question_images":["https://img.examtopics.com/ai-102/image5.png","https://img.examtopics.com/ai-102/image6.png","https://img.examtopics.com/ai-102/image7.png"],"topic":"2","answer_description":"","discussion":[{"upvote_count":"15","poster":"zellck","comment_id":"939646","content":"1. detects\n2. 797, 201\n\nhttps://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-face-detection#face-rectangle\nEach detected face corresponds to a faceRectangle field in the response. This is a set of pixel coordinates for the left, top, width, and height of the detected face. Using these coordinates, you can get the location and size of the face. In the API response, faces are listed in size order from largest to smallest.\n\nhttps://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-face-detection#attributes\n- QualityForRecognition\nThe overall image quality regarding whether the image being used in the detection is of sufficient quality to attempt face recognition on. The value is an informal rating of low, medium, or high. Only \"high\" quality images are recommended for person enrollment, and quality at or above \"medium\" is recommended for identification scenarios.","timestamp":"1704097020.0"},{"poster":"[Removed]","comments":[{"content":"exactly. Answer is correct","poster":"rdemontis","comment_id":"1062009","timestamp":"1714812000.0","upvote_count":"1"}],"comment_id":"1004780","timestamp":"1710167340.0","upvote_count":"7","content":"To answer the first question see the endpoint .../face/v1.0/detect?....\nTo answer the second question see the first object from the response \"faceRectangle\": {\"TOP\":201, \"LEFT:\"797....}"},{"timestamp":"1739340900.0","content":"detects is CORRECT because the response includes a list of detected faces along with their respective face IDs, face rectangles, and quality for recognition attributes. This matches the expected output of the Face Detection API, which is used to identify and locate human faces in an image.\n\n797, 201 is CORRECT because the face at this position has a qualityForRecognition attribute value of \"high,\" which indicates that it is suitable for use in person enrollment.","upvote_count":"1","comment_id":"1355484","poster":"syupwsh"},{"timestamp":"1732810740.0","poster":"taiwan_is_not_china","upvote_count":"2","content":"By feeling paizuri, you can see that the answers are ‘detect’ and ‘797,201’.","comment_id":"1220333"},{"poster":"taiwan_is_not_china","comment_id":"1220332","timestamp":"1732810680.0","content":"By feeling the paisley, you can see that the answers are ‘detect’ and ‘797,201’.","upvote_count":"1"},{"upvote_count":"2","content":"memorize. detects and 797, 201.","timestamp":"1732118340.0","poster":"takaimomoGcup","comment_id":"1214373"},{"content":"Final Answer:\n1. detects\n2. 797, 201","poster":"varinder82","timestamp":"1727316900.0","upvote_count":"1","comment_id":"1182997"},{"timestamp":"1694770560.0","upvote_count":"4","content":"The API detects faces.\n\nA face that can be used in person enrollment is at position 797, 201 within the photo.\n\nThis question provides information about an API request made to a face detection service. The request is sent to the endpoint \"https://facetesting.cognitiveservices.azure.com/face/v1.0/detect\" with the content of an image in the JSON format. The response from the API includes an array of detected faces, each with a unique faceId, faceRectangle, and faceAttributes.\n\nThe first statement asks what the API does with faces. The correct answer is \"detects\" because the endpoint used in the request is \"/detect,\" which implies that the API is used for face detection.\n\nThe second statement asks about the position of a face that can be used for person enrollment. The face's position is specified in the \"faceRectangle\" field of the JSON response. The correct answer is \"118, 754\" because that is the \"left\" and \"top\" position of the face rectangle for the fourth face in the response, which has a high enough quality for recognition to be used in person enrollment.","comment_id":"839835","comments":[{"upvote_count":"1","comments":[{"upvote_count":"1","poster":"Mattt","comment_id":"1366317","timestamp":"1741362000.0","content":"No, it isn't.\nIt's the high quality"}],"timestamp":"1697215440.0","poster":"uira","content":"\"118, 754\" has low quality, isn't it?","comment_id":"869619"}],"poster":"RAN_L"}]}],"exam":{"name":"AI-102","isBeta":false,"id":40,"numberOfQuestions":329,"provider":"Microsoft","isMCOnly":false,"lastUpdated":"12 Apr 2025","isImplemented":true},"currentPage":20},"__N_SSP":true}