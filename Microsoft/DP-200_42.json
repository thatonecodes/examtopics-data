{"pageProps":{"questions":[{"id":"O4Tq3FLqdSwuWUzFNod9","answer":"See the explanation below.","answer_ET":"See the explanation below.","answers_community":[],"question_id":206,"discussion":[{"upvote_count":"1","content":"I don't agree with the explanation. According to this \nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview\n\"...When using Azure AD Authentication, failed logins records will not appear in the SQL audit log. To view failed login audit records, you need to visit the Azure Active Directory portal, which logs details of these events...\"\nYou can log failed login events from AAD portal.","comment_id":"359302","timestamp":"1621231200.0","poster":"morales4dev","comments":[{"comment_id":"366499","timestamp":"1621952580.0","upvote_count":"2","content":"Unfortunately it is not mentioned if Azure AD authentication is used to login. Maybe user are just using the standard username and password created in the SQL database. Moreover, the lab asks to do something, and if they were using Azure AD authentication there would be nothing to do. So I think that the provided answer (enable auditing) is correct.","poster":"Alekx42"}]}],"timestamp":"2021-05-17 08:00:00","url":"https://www.examtopics.com/discussions/microsoft/view/52927-exam-dp-200-topic-4-question-6-discussion/","exam_id":65,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0038100001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0038200001.jpg"],"isMC":false,"answer_description":"Set up auditing for your database\nThe following section describes the configuration of auditing using the Azure portal.\n1. Go to the Azure portal.\n2. Navigate to Auditing under the Security heading in your SQL database db1/server pane\n\n3. If you prefer to enable auditing on the database level, switch Auditing to ON.\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-auditing","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0038000001.jpg"],"topic":"4","question_text":"SIMULATION -\nUse the following login credentials as needed:\n\nAzure Username: xxxxx -\n\nAzure Password: xxxxx -\nThe following information is for technical support purposes only:\n\nLab Instance: 10543936 -\n//IMG//\n\nYour company's security policy states that administrators must be able to review a list of the failed logins to an Azure SQL database named db1 during the previous 30 days.\nYou need to modify your Azure environment to meet the security policy requirements.\nTo complete this task, sign in to the Azure portal.","unix_timestamp":1621231200},{"id":"zmVfUhNr4G7FGiEtdB1b","timestamp":"2020-09-07 02:31:00","choices":{"D":"Backlogged Input Events","E":"Function Events","A":"Watermark Delay","C":"Out of order Events","B":"Late Input Events"},"url":"https://www.examtopics.com/discussions/microsoft/view/30732-exam-dp-200-topic-4-question-8-discussion/","answer":"BD","answer_description":"B: Late Input Events: events that arrived later than the configured late arrival tolerance window.\nNote: While comparing utilization over a period of time, use event rate metrics. InputEvents and OutputEvents metrics show how many events were read and processed.\nD: In job diagram, there is a per partition backlog event metric for each input. If the backlog event metric keeps increasing, it's also an indicator that the system resource is constrained (either because of output sink throttling, or high CPU).\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-scale-jobs","isMC":true,"discussion":[{"timestamp":"1599438660.0","upvote_count":"36","poster":"AJMorgan591","comments":[{"upvote_count":"1","content":"I agree. According to the docs, Arrival time is set on the source, so unrelated to the performance of ASA. It will be better to monitor Watermark Delay that indicates \"the delay of the streaming data processing job\"","timestamp":"1601213400.0","poster":"pablocg","comment_id":"188372"},{"upvote_count":"4","poster":"dzzz","comment_id":"240869","timestamp":"1607685060.0","content":"Your first link well supports A, D. \n\"If resource utilization is consistently over 80%, the watermark delay is rising, and the number of backlogged events is rising, consider increasing streaming units.\""}],"content":"The correct answer is A, D.\nB, C, E are unrelated to resource constraints.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-monitoring\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-time-handling","comment_id":"174864"},{"upvote_count":"2","content":"the same question is in dp300 and the correct answer is A and D","poster":"ramelas","comment_id":"492635","timestamp":"1638459540.0"},{"upvote_count":"1","timestamp":"1624094520.0","content":"It's A & D.","poster":"nav9","comment_id":"385375"},{"content":"https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption","timestamp":"1623576300.0","upvote_count":"1","poster":"vak2021","comment_id":"380989"},{"timestamp":"1619605380.0","upvote_count":"1","poster":"Maddaa","content":"Agree with A & D","comment_id":"344550"},{"timestamp":"1619481780.0","upvote_count":"1","comment_id":"343648","poster":"Wendy_DK","content":"it should be AD"},{"poster":"lky17","content":"AD should be selected\nIdentifying Bottlenecks\n\nUse the Metrics pane in your Azure Stream Analytics job to identify bottlenecks in your pipeline. Review Input/Output Events for throughput and \"Watermark Delay\" or Backlogged Events to see if the job is keeping up with the input rate. For Event Hub metrics, look for Throttled Requests and adjust the Threshold Units accordingly. For Cosmos DB metrics, review Max consumed RU/s per partition key range under Throughput to ensure your partition key ranges are uniformly consumed. For Azure SQL DB, monitor Log IO and CPU.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization#identifying-bottlenecks","timestamp":"1602684240.0","comment_id":"199852","upvote_count":"4"},{"comment_id":"190956","poster":"M0e","upvote_count":"3","content":"AD should be selected","timestamp":"1601555520.0"}],"exam_id":65,"topic":"4","question_images":[],"answer_ET":"BD","question_id":207,"unix_timestamp":1599438660,"answers_community":[],"answer_images":[],"question_text":"You have an Azure Stream Analytics job.\nYou need to ensure that the job has enough streaming units provisioned.\nYou configure monitoring of the SU% Utilization metric.\nWhich two additional metrics should you monitor? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point."},{"id":"UNqiXdHDN37gk7rQXI1S","discussion":[{"content":"https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance\nThe link provided clearly states that the optimization methods for cluster are only for I/O intensive job. However, the jobs for the question is like CPU intensive job and Memory intensive because the size of files is not large. Then the slow speed of processing comes from the more times of processing for each web servers files because HDInsight and Azure Data Lake Analytics have a per-file overhead. If you store your data as many small files, this can negatively affect performance.\nThen come back to the question, only the options that adjust the data structures can be the solutions. Thus, A and C will be the answer.","upvote_count":"18","timestamp":"1601255460.0","comment_id":"188749","poster":"Cassielovedata"},{"timestamp":"1615276200.0","upvote_count":"2","comment_id":"306221","poster":"akram786","content":"A and C are correct"},{"content":"My 2 cents: C and D\n\nWhy C? Storing files in a separate folder increases concurrency. \nWhy not A? When read literally, A advocates the creation of one single file that holds all the log information of the 250 servers at the same time. Since one file cannot be stored in more than one folder, this contradicts C. \n\nThe question also mentions \"performs analysis\". Performance optimisation of the analysis is a valid part of the answer. Since no information is given on any tools, a valid assumption is that it will be performed within the HD Insight realm. \nWhy not B? This parameter is used for memory problems, so it is not relevant for performance optimisation (https://dzone.com/articles/configuring-memory-for-mapreduce-running-on-yarn)\nWhy not E? If anything, this parameter should be reduced, not increased (https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-hive)\nD is correct. Scaling out worker nodes is mentioned on https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-optimize-hive-query","comments":[{"comment_id":"262613","poster":"Ab5381","content":"A does not contradicts C. A talks about combining all 250 files in a day into 1. C talks about ensuring you keep this 1 combined file in a folder. Next day combined file should be in another folder and should not be in previous day folder.","upvote_count":"4","timestamp":"1610113740.0"}],"comment_id":"249362","timestamp":"1608558120.0","poster":"mohowzeh","upvote_count":"3"},{"upvote_count":"4","content":"From the link provided:\nIn general, we recommend that your system have some sort of process to aggregate small files into larger ones for use by downstream applications. -> This supports A\nAgain, the choice you make with the folder and file organization should optimize for the larger file sizes and a reasonable number of files in each folder. -> This supports C","comment_id":"228249","poster":"syu31svc","timestamp":"1606389720.0"},{"poster":"RajatNaik","timestamp":"1597262100.0","upvote_count":"3","comment_id":"156717","content":"Answer should be C & E"},{"timestamp":"1595915580.0","upvote_count":"3","comment_id":"145487","content":"i don't seem this should be the right answer","poster":"JPaul"}],"timestamp":"2020-07-28 07:53:00","unix_timestamp":1595915580,"url":"https://www.examtopics.com/discussions/microsoft/view/26829-exam-dp-200-topic-5-question-1-discussion/","answer_ET":"AC","isMC":true,"answers_community":[],"answer_description":"A: Typically, analytics engines such as HDInsight and Azure Data Lake Analytics have a per-file overhead. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256MB to 100GB in size). Some engines and applications might have trouble efficiently processing files that are greater than 100GB in size.\nC: For Hive workloads, partition pruning of time-series data can help some queries read only a subset of the data which improves performance.\nThose pipelines that ingest time-series data, often place their files with a very structured naming for files and folders. Below is a very common example we see for data that is structured by date:\n\\DataSet\\YYYY\\MM\\DD\\datafile_YYYY_MM_DD.tsv\nNotice that the datetime information appears both as folders and in the filename.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance","choices":{"B":"Increase the value of the mapreduce.map.memory parameter","A":"Combine the daily log files for all servers into one file","D":"Increase the number of worker nodes","E":"Increase the value of the hive.tez.container.size parameter","C":"Move the log files into folders so that each day's logs are in their own folder"},"question_images":[],"answer_images":[],"exam_id":65,"question_text":"You manage a process that performs analysis of daily web traffic logs on an HDInsight cluster. Each of the 250 web servers generates approximately 10 megabytes (MB) of log data each day. All log data is stored in a single folder in Microsoft Azure Data Lake Storage Gen 2.\nYou need to improve the performance of the process.\nWhich two changes should you make? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","question_id":208,"topic":"5","answer":"AC"},{"id":"9Ld3JlP0Y7DQTA8zzepN","timestamp":"2020-12-17 13:57:00","unix_timestamp":1608209820,"question_text":"You have a SQL pool in Azure Synapse.\nA user reports that queries against the pool take longer than expected to complete.\nYou need to add monitoring to the underlying storage to help diagnose the issue.\nWhich two metrics should you monitor? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","isMC":true,"question_id":209,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/40199-exam-dp-200-topic-5-question-2-discussion/","discussion":[{"content":"\"To determine whether to scale up or down, consider all factors which can be impacted by DWU such as concurrency, memory, tempdb, and adaptive cache capacity. \" with this in mind, can number of query be a possible correct answer as well? \n\n\nRef : https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity","comments":[{"upvote_count":"1","content":"Maybe, but the number of active queries alone does not help much if you can't differentiate between the queries that are more resource-intesive and the ones that do not require much DWU.","timestamp":"1622284740.0","poster":"Alekx42","comment_id":"369365"}],"upvote_count":"2","timestamp":"1610245920.0","poster":"cjh1912","comment_id":"263610"},{"timestamp":"1608209820.0","upvote_count":"2","poster":"akash0680","content":"correct","comment_id":"246523"}],"answer_images":[],"answer_ET":"AE","choices":{"D":"Active queries","C":"Snapshot Storage Size","E":"Cache hit percentage","B":"DWU Limit","A":"Cache used percentage"},"answer_description":"A: Cache used is the sum of all bytes in the local SSD cache across all nodes and cache capacity is the sum of the storage capacity of the local SSD cache across all nodes.\nE: Cache hits is the sum of all columnstore segments hits in the local SSD cache and cache miss is the columnstore segments misses in the local SSD cache summed across all nodes\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity","question_images":[],"topic":"5","exam_id":65,"answer":"AE"},{"id":"wzOi3kcRwiBDibc5LIqD","unix_timestamp":1614023940,"answer_description":"Cluster-scoped Init Scripts: Init scripts are shell scripts that run during the startup of each cluster node before the Spark driver or worker JVM starts. Databricks customers use init scripts for various purposes such as installing custom libraries, launching background processes, or applying enterprise security policies.\nLogs for Cluster-scoped init scripts are now more consistent with Cluster Log Delivery and can be found in the same root folder as driver and executor logs for the cluster.\nReference:\nhttps://databricks.com/blog/2018/08/30/introducing-cluster-scoped-init-scripts.html","isMC":true,"timestamp":"2021-02-22 20:59:00","url":"https://www.examtopics.com/discussions/microsoft/view/45448-exam-dp-200-topic-5-question-3-discussion/","answers_community":[],"answer_images":[],"answer_ET":"C","question_images":[],"topic":"5","discussion":[{"content":"C is the correct Answer. \nAn init script is a shell script that runs during startup of each cluster node before the Apache Spark driver or worker JVM starts.\nInit script start and finish events are captured in cluster event logs. Details are captured in cluster logs.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/init-scripts","comment_id":"301928","comments":[{"content":"according to the link you provided it should be D","comment_id":"316447","poster":"anamaster","timestamp":"1616341440.0","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"461765","timestamp":"1634171580.0","poster":"kimalto452","content":"Every time a cluster launches, it writes a log to the init script log folder."}]}],"poster":"arpit_dataguy","upvote_count":"6","timestamp":"1614683640.0"},{"timestamp":"1651461720.0","comment_id":"595844","upvote_count":"1","poster":"Send2","content":"See https://www.examtopics.com/discussions/microsoft/view/56279-exam-dp-203-topic-4-question-7-discussion/"},{"timestamp":"1622285400.0","content":"\"C\" is correct. The cluster event log only capture the start and finish of the init script, but they do not provide details on the libraries installed by the init scripts. Even if it provided this information, it would still make more sense to look directly at the cluster init script logs.","comment_id":"369372","poster":"Alekx42","upvote_count":"4"},{"content":"\"D\" is correct.\n\"Init script start and finish events are captured in cluster event logs. \"\nhttps://docs.databricks.com/clusters/init-scripts.html","poster":"ewaf","comment_id":"313145","comments":[],"timestamp":"1615977540.0","upvote_count":"3"},{"comment_id":"296933","poster":"zyta","upvote_count":"2","content":"it should be \"D\" - cluster logs:\nInit script start and finish events are captured in cluster event logs. Details are captured in cluster logs\nhttps://docs.databricks.com/clusters/init-scripts.html (section logging)","timestamp":"1614023940.0","comments":[]}],"question_id":210,"answer":"C","question_text":"You create an Azure Databricks cluster and specify an additional library to install.\nWhen you attempt to load the library to a notebook, the library is not found.\nYou need to identify the cause of the issue.\nWhat should you review?","exam_id":65,"choices":{"B":"notebook logs","D":"cluster event logs","C":"global init scripts logs","A":"workspace logs"}}],"exam":{"isImplemented":true,"numberOfQuestions":228,"provider":"Microsoft","lastUpdated":"12 Apr 2025","isMCOnly":false,"isBeta":false,"name":"DP-200","id":65},"currentPage":42},"__N_SSP":true}