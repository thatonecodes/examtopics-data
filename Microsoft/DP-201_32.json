{"pageProps":{"questions":[{"id":"tRp0tSqqtCBcZYpkZRqm","answer":"A","isMC":true,"exam_id":66,"url":"https://www.examtopics.com/discussions/microsoft/view/18787-exam-dp-201-topic-3-question-27-discussion/","topic":"3","answers_community":[],"answer_ET":"A","question_id":156,"discussion":[{"timestamp":"1607339340.0","content":"https://docs.microsoft.com/en-us/sql/relational-databases/security/contained-database-users-making-your-database-portable?view=sql-server-ver15:\n\"Use contained database users to authenticate SQL Server and SQL Database connections at the database level\"\nA is correct","comment_id":"237241","upvote_count":"7","poster":"syu31svc"},{"comment_id":"449123","poster":"muni53","upvote_count":"1","content":"indeed contained db user needed","timestamp":"1632251400.0"},{"comment_id":"77083","upvote_count":"1","content":"Should be C since we already have an Azure AD group.","comments":[{"upvote_count":"36","comments":[{"comment_id":"285240","timestamp":"1612670760.0","content":"This is correct... Please see below:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview\n\nStatement: \"Azure AD authentication uses contained database users to authenticate identities at the database level.\"","poster":"rajneesharora","upvote_count":"3"}],"comment_id":"79403","content":"You cannot grant access to database access using RBAC, it must be on the database level, so the correct answer is \"contained user access\".","poster":"Tombarc","timestamp":"1587799440.0"}],"poster":"Yuri1101","timestamp":"1587408240.0"}],"timestamp":"2020-04-20 20:44:00","unix_timestamp":1587408240,"answer_images":[],"choices":{"A":"a contained database user","B":"a SQL login","D":"a shared access signature (SAS)","C":"an RBAC role"},"question_text":"You are designing the security for an Azure SQL database.\nYou have an Azure Active Directory (Azure AD) group named Group1.\nYou need to recommend a solution to provide Group1 with read access to the database only.\nWhat should you include in the recommendation?","answer_description":"Create a User for a security group\nA best practice for managing your database is to use Windows security groups to manage user access. That way you can simply manage the customer at the\nSecurity Group level in Active Directory granting appropriate permissions. To add a security group to SQL Data Warehouse, you use the Display Name of the security group as the principal in the CREATE USER statement.\nCREATE USER [<Security Group Display Name>] FROM EXTERNAL PROVIDER WITH DEFAULT_SCHEMA = [<schema>];\nIn our AD instance, we have a security group called Sales Team with an alias of salesteam@company.com. To add this security group to SQL Data Warehouse you simply run the following statement:\nCREATE USER [Sales Team] FROM EXTERNAL PROVIDER WITH DEFAULT_SCHEMA = [sales];\nReference:\nhttps://blogs.msdn.microsoft.com/sqldw/2017/07/28/adding-ad-users-and-security-groups-to-azure-sql-data-warehouse/","question_images":[]},{"id":"AlfXxkXEEMH2CmldBpBX","question_text":"HOTSPOT -\nYou use Azure Data Lake Storage Gen2 to store data that data scientists and data engineers will query by using Azure Databricks interactive notebooks. The folders in Data Lake Storage will be secured, and users will have access only to the folders that relate to the projects on which they work.\nYou need to recommend which authentication methods to use for Databricks and Data Lake Storage to provide the users with the appropriate access. The solution must minimize administrative effort and development effort.\nWhich authentication method should you recommend for each Azure service? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","timestamp":"2020-04-14 15:05:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0028100001.jpg"],"answer_description":"Databricks: Personal access tokens\nTo authenticate and access Databricks REST APIs, you use personal access tokens. Tokens are similar to passwords; you should treat them with care. Tokens expire and can be revoked.\nData Lake Storage: Azure Active Directory\nAzure Data Lake Storage Gen1 uses Azure Active Directory for authentication.\nReferences:\nhttps://docs.azuredatabricks.net/dev-tools/api/latest/authentication.html https://docs.microsoft.com/en-us/azure/data-lake-store/data-lakes-store-authentication-using-azure-active-directory","unix_timestamp":1586869500,"exam_id":66,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0028000001.jpg"],"answer":"","isMC":false,"question_id":157,"topic":"3","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/18418-exam-dp-201-topic-3-question-28-discussion/","answer_ET":"","discussion":[{"timestamp":"1591555200.0","comment_id":"104773","upvote_count":"30","content":"Answer is Correct\nhttps://docs.databricks.com/dev-tools/api/latest/authentication.html\nhttps://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2\nhttps://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2#adls2-aad-credentials","poster":"remz"},{"comment_id":"132024","upvote_count":"22","timestamp":"1594464780.0","content":"To minimize the admin effort the best option would be a creating a High Concurrency cluster enable AD credential passthrough, using RBAC to assign contributor role to the AD users (data engineers and data analyst) to the databricks workspace, apply ACLs to the specific folders for the AD users. Active Directory authentication perfectly works for both.","poster":"dip17"},{"poster":"muni53","content":"both shud be AD. ADB auto authenticates with AD","timestamp":"1632251460.0","comment_id":"449124","upvote_count":"1"},{"timestamp":"1615343580.0","poster":"Needium","content":"This seems a lot like Azure Active Directory in both boxes for me. First of all, I am authenticating to the Databrick UI itself to create and run notebooks and not the REST API. I would rather use the standard AAD to access Databricks and use the same AAD credentials to access ADLS Gen 2 to for the files. Of course, I will be implementing ACL to restrict access tothe folders each user should be able to access only on AAD.\n\nRef: https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough","comment_id":"306775","upvote_count":"4"},{"content":"This seems a lot like Azure Active Directory in both boxes for me. First of all, I am authenticating to the Databrick UI itself to create and run notebooks and not the REST API. I would rather use the standard AAD to access Databricks and use the same AAD credentials to access ADLS Gen 2 to for the files. Of course, I will be implementing ACL to restrict access tothe folders each user should be able to access only on AAD.\n\nRef: https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough","comment_id":"304958","timestamp":"1615097040.0","upvote_count":"1","poster":"Needium"},{"comment_id":"255812","content":"All the answers above are wrong. The correct answers are:\n\n1.Databricks: Azure Active Directory\n2.Data Lake Storage: Azure Active Directory\n\n1. There is no mention of connecting with the Databricks API, instead the descriptions days that users will connect to ADLS using Interactive Notebooks. For that they will have to log in to Databricks itself, which will be done with their AD accounts.\n\n2. Shared access signature or shared access keys do not use ACLs, but RBAC, and are applied on container or storage account level, NOT on directory or file level. I quote from https://docs.microsoft.com/nl-nl/azure/storage/blobs/data-lake-storage-access-control\n\n\n\"ACLs apply only to security principals in the same tenant, and they don't apply to users who use Shared Key or shared access signature (SAS) token authentication. That's because no identity is associated with the caller and therefore security principal permission-based authorization cannot be performed.\"","poster":"lastname","timestamp":"1609352700.0","comments":[{"comment_id":"274877","timestamp":"1611436380.0","upvote_count":"5","content":"1.Databricks: Azure Active Directory (minimize administrative effort)\n2.Data Lake Storage: Azure Active Directory","poster":"zarga"}],"upvote_count":"13"},{"upvote_count":"1","comment_id":"237251","poster":"syu31svc","content":"I would say the answer is correct\nhttps://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/authentication:\n\"To authenticate to and access Databricks REST APIs, you can use Azure Databricks personal access tokens or Azure Active Directory (Azure AD) tokens.\"\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control:\n\"Always use Azure AD security groups as the assigned principal in an ACL entry\"","comments":[{"upvote_count":"3","content":"I see no mention of an API, it's logging in to Databricks and then querying ADLS.","comment_id":"255795","poster":"lastname","timestamp":"1609351200.0"}],"timestamp":"1607339880.0"},{"content":"Where in the question does it talk about accessing the REST API? Personal access tokens are used to access the Databricks REST API. For interactive notebooks, AAD is the way to authenticate the users!","comments":[{"content":"Indeed.","comment_id":"255793","timestamp":"1609351140.0","poster":"lastname","upvote_count":"1"}],"poster":"M0e","timestamp":"1603556580.0","comment_id":"205211","upvote_count":"7"},{"comment_id":"151400","content":"Databricks - Azure Key Vault https://docs.microsoft.com/en-us/azure/databricks/security/secrets/example-secret-workflow\n\nADLS Gen 2 - AAD","poster":"Ash666","upvote_count":"4","timestamp":"1596658380.0"},{"poster":"azurearch","comment_id":"86911","content":"AAD cant do folder level permissions in ADLS, it needs ACL to do that.","timestamp":"1589167440.0","upvote_count":"6","comments":[{"timestamp":"1590931380.0","content":"Re: ADLS, the question concerns authentication only not authorisation, so AAD for authentication and then RBAC roles for authorisation.","comment_id":"99307","upvote_count":"6","poster":"pawhit"}]},{"timestamp":"1588354200.0","comment_id":"82273","upvote_count":"3","content":"It looks like a bad wording in the question. The requirements are not to secure the Notebook, but only the storage access, so What I do in those cases - define access using KeyVault (so user of that notebook won't see the credentials) and secure ADLS2 with Service Identity in AAD - that allows granular authorization and project scope.","poster":"Leonido"},{"upvote_count":"6","content":"I think for ADLS Gen2, it should use SAS rather than AAD (RBAC). Shared Key is not quite suitable as it make user effectively gains 'super-user' access, meaning full access to all operations on all resources, including setting owner and changing ACLs.","poster":"Luke97","comments":[{"timestamp":"1588052580.0","content":"I concur, I also think that AAD should be the authentication method for databricks since personal access token is used to access databricks REST API instead of interactive notebook.","poster":"HCL1991","comment_id":"80624","upvote_count":"4"},{"upvote_count":"4","timestamp":"1588353840.0","content":"Won't work - ADLS can only have account level SAS, and you need at least container wise","comment_id":"82272","poster":"Leonido"}],"timestamp":"1586869500.0","comment_id":"74505"}]},{"id":"LbyuNOfXNlKJVHjygqXm","question_text":"You store data in a data warehouse in Azure Synapse Analytics.\nYou need to design a solution to ensure that the data warehouse and the most current data is available within one hour of a datacenter failure.\nWhich three actions should you include in the design? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer":"BDE","url":"https://www.examtopics.com/discussions/microsoft/view/19045-exam-dp-201-topic-3-question-29-discussion/","choices":{"D":"Each day, create Azure Firewall rules that allow access to the restored data warehouse.","C":"If a failure occurs, modify the Azure Firewall rules of the data warehouse.","A":"Each day, restore the data warehouse from a geo-redundant backup to an available Azure region.","B":"If a failure occurs, update the connection strings to point to the recovered data warehouse.","E":"Each day, restore the data warehouse from a user-defined restore point to an available Azure region."},"exam_id":66,"isMC":true,"topic":"3","unix_timestamp":1587800220,"timestamp":"2020-04-25 09:37:00","answer_images":[],"answers_community":[],"question_images":[],"discussion":[{"upvote_count":"34","poster":"azurearch","comment_id":"86916","content":"It can be BCE instead of BDE Why to create firewall rule each day if we have one hour time to restore after failure","timestamp":"1589168340.0","comments":[{"upvote_count":"2","content":"Actually C and D are a bit confusing to me. probably they are badly worded or it is just me. i would actually be looking out for an option like \nD. If a failure occurs, create Azure Firewall rules that allow access to the restored data warehouse.\nWhy should I always create a firewall rule to open up the restored database pre-emptively? I believe it is safer to only create the firewall in the event of failures.\n\nOtherwise, an option C that explicitly qualifies the datawarehouse as restored would still have sounded more correct to me than the two options available in the question\nC. If a failure occurs, modify the Azure Firewall rules of the restored data warehouse.","comment_id":"304960","timestamp":"1615097460.0","poster":"Needium"},{"content":"C is a trick question to cause confusion, why would you update firewall rules on the existing data warehouse after it fails, existing being implied since they didn't mention restored. It's D because you're updating the restored database.","timestamp":"1590490920.0","poster":"MG23","comment_id":"95963","upvote_count":"15"}]},{"upvote_count":"11","comments":[{"upvote_count":"2","timestamp":"1607236200.0","comment_id":"236211","poster":"syu31svc","content":"I agree with BDE as the answer based on the link you shared"}],"poster":"proca","timestamp":"1595045340.0","comment_id":"137621","content":"In my opinion the correct answer is B,D,E. \nUpdate connection strings -> \n\"Because your recovered database resides in a different server, you need to update your application’s connection string to point to that server.\" \nConfigure firewall rules -> \nYou need to make sure that the firewall rules configured on server and on the database match those that were configured on the primary server and primary database.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/disaster-recovery-guidance?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json#configure-your-database-after-recovery"},{"poster":"ebenajay","content":"Answer given is correct.","timestamp":"1609728960.0","upvote_count":"2","comment_id":"259078"},{"upvote_count":"4","comment_id":"219384","poster":"ssgexam","timestamp":"1605393780.0","content":"Shouldn't it be A, B & D?"},{"comment_id":"205222","poster":"M0e","timestamp":"1603557420.0","upvote_count":"3","content":"Does ADW requires DAILY manual steps to provide 1 hour of RTO?? Realy??"},{"upvote_count":"2","timestamp":"1601557440.0","content":"what all the suggested answers are all manual operations?? And why user defined restore point is what the most current data we want to restore??","poster":"chanbull","comment_id":"190990"},{"poster":"runningman","timestamp":"1588331820.0","comment_id":"82142","upvote_count":"1","content":"but why suggest E if you say that a user defined rollback point is hard to establish accurately? I think A, then D in case there is an issue, then B to direct to working region."},{"poster":"Tombarc","content":"In my opinion the correct answer is A,D,E. Azure synapse takes a snapshot roughly every 4 hours and 1 geo-snapshot every day to a paired region . The question talks about datacenter failure, so unless you can predict when the failure will occur to take a user-defined restore point, you should rely on the automated geo-snapshot.","upvote_count":"6","comment_id":"79406","timestamp":"1587800220.0"}],"question_id":158,"answer_ET":"BDE","answer_description":"E: You can create a user-defined restore point and restore from the newly created restore point to a new data warehouse in a different region.\nNote: A data warehouse snapshot creates a restore point you can leverage to recover or copy your data warehouse to a previous state.\nA data warehouse restore is a new data warehouse that is created from a restore point of an existing or deleted data warehouse. On average within the same region, restore rates typically take around 20 minutes.\nIncorrect Answers:\nA: SQL Data Warehouse performs a geo-backup once per day to a paired data center. The RPO for a geo-restore is 24 hours. You can restore the geo-backup to a server in any other region where SQL Data Warehouse is supported. A geo-backup ensures you can restore data warehouse in case you cannot access the restore points in your primary region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/backup-and-restore"},{"id":"8X6SvNfbgOYrlcqhHsgh","exam_id":66,"url":"https://www.examtopics.com/discussions/microsoft/view/29023-exam-dp-201-topic-3-question-3-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0025200001.jpg"],"timestamp":"2020-08-19 07:35:00","discussion":[{"timestamp":"1597815300.0","comments":[{"upvote_count":"1","poster":"Ikrom","comments":[{"content":"Use randomized encryption, for data such as confidential investigation comments, which are not grouped with other records and are not used to join tables.","upvote_count":"2","comment_id":"168532","poster":"zglat","timestamp":"1598625540.0"}],"timestamp":"1598148660.0","comment_id":"164044","content":"and the same options to choose?"},{"comment_id":"382688","content":"Its sads Grouping data and Non grouping data","timestamp":"1623763380.0","poster":"ZodiaC","upvote_count":"2"}],"poster":"extraego","upvote_count":"30","content":"This was in my exam. The options were \"Grouping data\" and \"Non-grouping data\" instead of \"Searchable data\" and \"Non-searchable data\".","comment_id":"161259"},{"poster":"Vijaya","timestamp":"1599640680.0","content":"Use deterministic encryption for columns that will be used as search or grouping parameters. For example, a government ID number. Use randomized encryption for data such as confidential investigation comments, which aren't grouped with other records and aren't used to join tables.","comment_id":"176375","upvote_count":"14"},{"comment_id":"394259","content":"This was in my exam also..thanks guys","timestamp":"1625008320.0","poster":"teedap123","comments":[{"poster":"Vaishu05","content":"So the answers are correct right?","upvote_count":"1","timestamp":"1625065800.0","comment_id":"394864"}],"upvote_count":"3"}],"answer":"","topic":"3","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0025300001.jpg"],"answers_community":[],"question_text":"HOTSPOT -\nA company plans to use Azure SQL Database to support a line of business application. The application will manage sensitive employee data.\nThe solution must meet the following requirements:\n✑ Encryption must be performed by the application.\n✑ Only the client application must have access keys for encrypting and decrypting data.\n✑ Data must never appear as plain text in the database.\n✑ The strongest possible encryption method must be used.\n✑ Grouping must be possible on selected data.\nWhat should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_id":159,"answer_description":"Box 1: Always Encrypted with deterministic encryption\nDeterministic encryption always generates the same encrypted value for any given plain text value. Using deterministic encryption allows point lookups, equality joins, grouping and indexing on encrypted columns. However, it may also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column, especially if there is a small set of possible encrypted values, such as True/False, or North/South/East/West region.\nDeterministic encryption must use a column collation with a binary2 sort order for character columns.\nBox 2: Always Encrypted with Randomized encryption\n✑ Randomized encryption uses a method that encrypts data in a less predictable manner. Randomized encryption is more secure, but prevents searching, grouping, indexing, and joining on encrypted columns.\nNote: With Always Encrypted the Database Engine never operates on plaintext data stored in encrypted columns, but it still supports some queries on encrypted data, depending on the encryption type for the column. Always Encrypted supports two types of encryption: randomized encryption and deterministic encryption.\nUse deterministic encryption for columns that will be used as search or grouping parameters, for example a government ID number. Use randomized encryption, for data such as confidential investigation comments, which are not grouped with other records and are not used to join tables.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine","isMC":false,"unix_timestamp":1597815300,"answer_ET":""},{"id":"V3QcX5I7xkhqzC7qJVSZ","discussion":[{"upvote_count":"16","content":"Both boxes should be Managed Identity:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-identity","comment_id":"353747","timestamp":"1620653280.0","poster":"dbdev"},{"upvote_count":"5","comment_id":"369076","timestamp":"1622246460.0","poster":"Wendy_DK","content":"After carefully read the reference and the requirement(Development and maintenance effort must be minimized) in the question.\nBoth boxes should be Managed Identity. Here is the reason:\nBy using Managed Identity, it takes two steps\nBy using service principal, it needs three steps\nRef. https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse"},{"timestamp":"1637244540.0","content":"both Managed indentity, the moc describe that blob storage supported managed identity","poster":"massnonn","upvote_count":"1","comment_id":"480768"},{"comment_id":"362563","upvote_count":"2","poster":"Wendy_DK","content":"Azure data factory can use Managed Identity and service principal both as authentication.\nquestion here is :You are designing an Azure data factory that will copy data from Azure Blob storage to a data warehouse in Azure Synapse Analytics.\nAnswer here is correct.\nIf some one pick up Managed Identity on the second box, it should be correct answer too","timestamp":"1621557120.0"}],"answer":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0028400001.jpg"],"answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0028300003.jpg"],"question_text":"HOTSPOT -\nYou are designing an Azure data factory that will copy data from Azure Blob storage to a data warehouse in Azure Synapse Analytics.\nYou need to recommend an authentication mechanism that meet the following requirements:\n✑ Identities must be validated by using Azure Active Directory (Azure AD).\n✑ Development and maintenance effort must be minimized.\nWhich authentication mechanism should you recommend for each service? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_id":160,"topic":"3","exam_id":66,"timestamp":"2021-05-10 15:28:00","answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-msi https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse","answers_community":[],"unix_timestamp":1620653280,"url":"https://www.examtopics.com/discussions/microsoft/view/52291-exam-dp-201-topic-3-question-30-discussion/","isMC":false}],"exam":{"isMCOnly":false,"numberOfQuestions":206,"isImplemented":true,"name":"DP-201","id":66,"lastUpdated":"12 Apr 2025","isBeta":false,"provider":"Microsoft"},"currentPage":32},"__N_SSP":true}