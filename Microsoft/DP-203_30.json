{"pageProps":{"questions":[{"id":"kmNxcmHzaoSFfQ3ICgn5","answer":"A","answer_images":[],"timestamp":"2024-01-09 15:25:00","question_images":[],"answer_description":"","question_text":"You have an on-premises Linux server that contains a database named DB1.\n\nYou have an Azure subscription that contains an Azure data factory named ADF1 and an Azure Data Lake Storage account named ADLS1.\n\nYou need to create a pipeline in ADF1 that will copy data from DB1 to ADLS1.\n\nWhich type of integration runtime should you use to read the data from DB1?","unix_timestamp":1704810300,"choices":{"A":"self-hosted integration runtime","B":"Azure integration runtime","C":"Azure-SQL Server Integration Services (SSIS)"},"url":"https://www.examtopics.com/discussions/microsoft/view/130706-exam-dp-203-topic-2-question-123-discussion/","isMC":true,"discussion":[{"poster":"e56bb91","comment_id":"1240697","upvote_count":"5","timestamp":"1719917640.0","content":"ChatGPT confirmed self-hosted integration runtime"},{"content":"Selected Answer: A\nA. self-hosted integration runtime","poster":"bacms","timestamp":"1732071420.0","comment_id":"1315018","upvote_count":"2"},{"poster":"7082935","content":"It's kind of a toss up here, but the question does specify to \"create a Data Factory pipeline to copy data\". ONLY if it requested to \"Create a pipeline with an Execute SSIS Package activity\" would it be better to choose SSIS runtime.","comment_id":"1259152","timestamp":"1722478740.0","upvote_count":"1"},{"timestamp":"1719245940.0","comment_id":"1236456","content":"Selected Answer: A\nthe self-hosted integration runtime cannot be installed directly on a Linux server, you can set it up on a Windows machine within your network to facilitate data movement between an on-premises Linux database and Azure services.","upvote_count":"2","poster":"tadenet"},{"content":"A. Self Hosted integration run times will work but SHIR need to be installed on a windows machine in same network of Linux with supported DB driver. Its not easy but still very much possible.","upvote_count":"1","poster":"Sr18","comments":[{"upvote_count":"1","poster":"Sr18","timestamp":"1719006360.0","comment_id":"1235003","content":"After thinking, I think it will be better to use SSIS."}],"timestamp":"1719006060.0","comment_id":"1234997"},{"upvote_count":"2","content":"Selected Answer: A\nAlthough the self-hosted IR runs on Windows, it can connect to databases hosted on Linux servers. This is possible because the self-hosted IR uses database connectors and drivers that support remote connections to databases on different operating systems.","comment_id":"1221248","timestamp":"1717014720.0","poster":"Swayansu"},{"comment_id":"1171093","timestamp":"1710170340.0","content":"Selected Answer: C\nSSIS is correct, albeit a bit overkill, because self hosted can only run on windows, this makes it the best option","upvote_count":"1","poster":"mav2000"},{"timestamp":"1708811940.0","upvote_count":"2","poster":"mav2000","content":"Selected Answer: C\nThis was a hard question, but indeed apparently Self-hosted integration runtime only supports windows, and Azure is for cloud platforms, so the answer is the SSIS, though it's a bit overkill because SSIS is meant for transformations","comment_id":"1158178"},{"timestamp":"1705791240.0","comment_id":"1127539","poster":"Azure_2023","content":"Selected Answer: C\nAzure-SQL Server Integration Services (SSIS) is currently supported for connecting to on-premises Linux servers. The other two options aren't.","upvote_count":"1"},{"content":"Selected Answer: C\nC\nAs self-hosted integration runtime is only supported in Windows OS, Azure IR only supports withing Azure cloud platforms","upvote_count":"1","timestamp":"1705351260.0","comment_id":"1123661","poster":"1d22eec","comments":[{"upvote_count":"1","content":"No, Self-hosted Integration Runtime does not support installation on Linux directly but it can connect to databases hosted on Linux servers.","poster":"Pey1nkh","timestamp":"1740403020.0","comment_id":"1361019"}]},{"content":"Correct","upvote_count":"2","comment_id":"1117563","timestamp":"1704810300.0","poster":"JJFortunato"}],"answer_ET":"A","question_id":146,"topic":"2","answers_community":["A (55%)","C (45%)"],"exam_id":67},{"id":"ZlBAIeC1nsl89G9gUKkc","exam_id":67,"question_text":"DRAG DROP\n-\n\nYou have an Azure Data Lake Storage account named account1.\n\nYou use an Azure Synapse Analytics serverless SQL pool to access sales data stored in account1.\n\nYou need to create a bar chart that displays sales by product. The solution must minimize development effort.\n\nIn which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.\n\n//IMG//","discussion":[{"comment_id":"1199146","poster":"Alongi","timestamp":"1729427940.0","upvote_count":"3","content":"It's Correct guys"},{"upvote_count":"3","timestamp":"1724479800.0","comment_id":"1157719","content":"correct!","poster":"efeee333"},{"upvote_count":"4","comment_id":"1121200","timestamp":"1720827780.0","content":"correct","poster":"ccd5321"}],"answer_description":"","unix_timestamp":1705110180,"question_id":147,"answer_images":["https://img.examtopics.com/dp-203/image367.png"],"answer":"","answers_community":[],"isMC":false,"timestamp":"2024-01-13 02:43:00","url":"https://www.examtopics.com/discussions/microsoft/view/130995-exam-dp-203-topic-2-question-124-discussion/","question_images":["https://img.examtopics.com/dp-203/image366.png"],"topic":"2","answer_ET":""},{"id":"jtlQOI3eYXrWRREeaDDO","url":"https://www.examtopics.com/discussions/microsoft/view/131719-exam-dp-203-topic-2-question-125-discussion/","exam_id":67,"timestamp":"2024-01-21 10:01:00","topic":"2","question_images":["https://img.examtopics.com/dp-203/image368.png"],"question_id":148,"answer_ET":"","answer_description":"","discussion":[{"poster":"Pey1nkh","content":"given answer is correct","timestamp":"1740425940.0","upvote_count":"1","comment_id":"1361164"},{"upvote_count":"2","comment_id":"1162586","poster":"JordenJJ","content":"Kind of a weird way to ask this. The answer doesn't state it's a backup, it's solely a copy. You can't really infer from the details that it should be paused after copying.","timestamp":"1724930340.0"},{"timestamp":"1721545260.0","content":"Correct.\n\nIn case you're looking for a Long-Term Backup (LTR) concept:\n\nCreate a new user-defined restore point, or you can use one of the automatically generated restore points.\nRestore from the newly created restore point to a new data warehouse.\nAfter you have restored, you have the dedicated SQL pool online. Pause it indefinitely to save compute costs. The paused database incurs storage charges at the Azure Synapse storage rate.\nIf you need an active copy of the restored data warehouse, you can resume, which should take only a few minutes.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/backup-and-restore?context=%2Fazure%2Fsynapse-analytics%2Fcontext%2Fcontext","upvote_count":"2","comment_id":"1127715","poster":"Azure_2023"}],"answers_community":[],"answer":"","answer_images":["https://img.examtopics.com/dp-203/image369.png"],"question_text":"DRAG DROP\n-\n\nYou have an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to create a copy of the data warehouse and make the copy available for 28 days. The solution must minimize costs.\n\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n\n//IMG//","unix_timestamp":1705827660,"isMC":false},{"id":"Y1lelkAUbMfAELKTBlmf","topic":"2","answers_community":[],"answer_images":["https://img.examtopics.com/dp-203/image371.png"],"unix_timestamp":1705052280,"question_text":"HOTSPOT\n-\n\nYou have an Azure Synapse Analytics workspace that contains an Apache Spark pool named Pool1.\n\nYou need to read data from a CSV file and write the data to a Delta table by using Pool1.\n\nHow should you complete the PySpark code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_description":"","question_id":149,"exam_id":67,"timestamp":"2024-01-12 10:38:00","answer":"","answer_ET":"","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/130933-exam-dp-203-topic-2-question-126-discussion/","discussion":[{"content":"deltaTable = DeltaTable.forPath(spark, \"/path/to/table\")","timestamp":"1741468020.0","poster":"9370d83","comment_id":"1366686","upvote_count":"1"},{"poster":"renan_ineu","timestamp":"1726722000.0","comment_id":"1286147","upvote_count":"4","content":"The answers are correct.\n\nBesides the link from @GermanGerman, check the databricks syntax here:\nhttps://docs.databricks.com/en/delta/tutorial.html\n\nThe page has only \"forName\". For the question, the similar \"forPath\" is the choice."},{"timestamp":"1705152420.0","comment_id":"1121679","upvote_count":"4","content":"correct. https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/03-create-delta-tables","poster":"GermanGerman"},{"content":"df.cache() ,convertToDelta()","comments":[{"content":"why, explain?","timestamp":"1714153140.0","poster":"swathi_rs","upvote_count":"2","comment_id":"1202730"}],"comment_id":"1120654","poster":"redprint","timestamp":"1705052280.0","upvote_count":"2"}],"question_images":["https://img.examtopics.com/dp-203/image370.png"]},{"id":"ysSDXizAEm81lnxcek5A","answer_images":["https://img.examtopics.com/dp-203/image374.png"],"answer":"","topic":"2","question_text":"HOTSPOT\n-\n\nYou have an Azure Data Lake Storage account that contains one CSV file per hour for January 1, 2020, through January 31, 2023. The files are partitioned by using the following folder structure.\n\n//IMG//\n\n\nYou need to query the files by using an Azure Synapse Analytics serverless SQL pool. The solution must return the row count of each file created during the last three months of 2022.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\n//IMG//","exam_id":67,"discussion":[{"upvote_count":"6","comment_id":"1116988","content":"correct: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-specific-files","poster":"Lewiasskick","timestamp":"1704745980.0"},{"content":"wrong! \nr.filepath() : returns full path ( /year=2022/month=10/day=01/file1.csv )\nr.filepath(1) : returns year (2022)\nr.filepath(2) : returns month which we need.","timestamp":"1740425280.0","poster":"Pey1nkh","upvote_count":"2","comment_id":"1361157"},{"comment_id":"1259618","content":"Correct Answer\nMy reason: from the script provided, particularly from here\n \"From OPENROWSET ( BULK 'csv/system1/2022/*/*.csv',\nThe search on the file path uses a wildcard that begins after the year 2022 (i.e. the asterisks) and the first parameter is the month after 2022. \nHence, r.filepath(1) is correct.","upvote_count":"1","poster":"WayBacK","timestamp":"1722561300.0"},{"poster":"ageorgieva","upvote_count":"1","timestamp":"1719033720.0","content":"ChatGPT:\nGiven the typical folder structure in your case, which is csv/system1/{year}/{month}/{filename}.csv, you need to access specific segments of this path to filter data by month:\n\nr.filepath(0) would return the first segment after the initial folder, likely csv.\nr.filepath(1) would then return system1.\nr.filepath(2) refers to the next segment, which, according to your folder structure, represents the year (2022).\nSo answer is r.filepath(2).","comment_id":"1235122"},{"content":"Correct answer","timestamp":"1709844960.0","comment_id":"1168354","upvote_count":"2","poster":"MohamedBI12"},{"comment_id":"1150182","timestamp":"1707912480.0","poster":"mghf61","content":"r.filepath(2) rerurn year (2022)","upvote_count":"1"},{"poster":"Azure_2023","content":"Correct. The term \"first wildcard\" refers to the asterisk that represents the month in the file path placeholder. For this specific example.","comment_id":"1127732","timestamp":"1705829760.0","upvote_count":"3"},{"comments":[{"timestamp":"1706862660.0","content":"Just to be clear:\nThe right answer is filepath(1), because In your example the 2. parameter (star) is the month, but in the answer the year is fixed and the month is the first parameter.","poster":"Gikan","comment_id":"1138309","upvote_count":"4"}],"comment_id":"1126035","content":"Answers are correct:\nAs per next example, parameter (1) refers to year, and paremeter (2) to month.\n\nSELECT\n r.filepath() AS filepath\n ,r.filepath(1) AS [year]\n ,r.filepath(2) AS [month]\n ,COUNT_BIG(*) AS [rows]\nFROM OPENROWSET(\n BULK 'csv/taxi/yellow_tripdata_*-*.csv',\n DATA_SOURCE = 'SqlOnDemandDemo',\n FORMAT = 'CSV',\n PARSER_VERSION = '2.0', \n FIRSTROW = 2\n )\nWITH (\n vendor_id INT\n) AS [r]\nWHERE\n r.filepath(1) IN ('2017')\n AND r.filepath(2) IN ('10', '11', '12')\nGROUP BY\n r.filepath()\n ,r.filepath(1)\n ,r.filepath(2)\nORDER BY\n filepath;","poster":"[Removed]","timestamp":"1705594260.0","upvote_count":"4"},{"timestamp":"1705409220.0","poster":"zodraz","upvote_count":"3","comment_id":"1124205","content":"Second one is r.filepath(2) asper:\nWhen called without parameter, returns the full file path that a row originates from.\nWhen called with parameter, it returns part of path that matches the wildcard on position specified in the parameter. For example, parameter value 1 would return part of path that matches the first wildcard."}],"timestamp":"2024-01-08 21:33:00","answer_ET":"","answer_description":"","question_id":150,"url":"https://www.examtopics.com/discussions/microsoft/view/130652-exam-dp-203-topic-2-question-127-discussion/","isMC":false,"unix_timestamp":1704745980,"question_images":["https://img.examtopics.com/dp-203/image372.png","https://img.examtopics.com/dp-203/image373.png"],"answers_community":[]}],"exam":{"isImplemented":true,"lastUpdated":"12 Apr 2025","isMCOnly":false,"id":67,"isBeta":false,"numberOfQuestions":384,"name":"DP-203","provider":"Microsoft"},"currentPage":30},"__N_SSP":true}