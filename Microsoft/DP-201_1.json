{"pageProps":{"questions":[{"id":"i8KZI6s7ufbkZt03yN5F","question_id":1,"discussion":[{"upvote_count":"17","timestamp":"1605196560.0","content":"POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files. As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1.\naccording to this docs resource, I think the given answer is correct","comment_id":"218001","poster":"Piiri565"},{"comment_id":"389230","upvote_count":"5","content":"We can ignore questions where we see GEN1 as it is out of scope now.","timestamp":"1624511520.0","poster":"arpit_dataguy"},{"upvote_count":"1","poster":"Ambujinee","comment_id":"387961","timestamp":"1624365240.0","content":"File size is accepted within 256MB to 2GB"},{"comment_id":"360941","timestamp":"1621389600.0","content":"Referencing the provided link the minimum acceptable file size is 256MB whereas the propose solution started at 250MB. I would say the answer is 'NO'\n\nReference: https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices","upvote_count":"2","poster":"cadio30","comments":[{"content":"That makes not really sense for the this question","poster":"ZodiaC","upvote_count":"1","timestamp":"1623596340.0","comment_id":"381169"},{"comment_id":"381479","upvote_count":"1","timestamp":"1623638220.0","poster":"baobabko","content":"250 MB vs 256 MB gives less than 3% waste in worst-case. So it is acceptable. Answer should be YES"}]},{"timestamp":"1619070300.0","content":"So is this a trap question? as the guidance is 256MB and they are saying larger than 250MB... a small difference but below we recommended size","poster":"SplMonk","upvote_count":"1","comment_id":"340800"},{"content":"The given solution is correct \nTypically, analytics engines such as HDInsight and Azure Data Lake Analytics have a per-file overhead. If you store your data as many small files, this can negatively affect performance.\npls refer this link https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance#structure-your-data-set \nIn general, organize your data into larger sized files for better performance. As a rule of thumb, organize data sets in files of 256 MB or larger","timestamp":"1613886180.0","upvote_count":"2","comment_id":"295557","poster":"Deepu1987"},{"comment_id":"247776","timestamp":"1608361380.0","upvote_count":"1","poster":"chaoxes","content":"Given answer B. No is correct.\n\nIn POSIX-style model it is recommended to avoid small size files, due to following considerations:\n-Lowering the authentication checks across multiple files\n-Reduced open file connections\n-Faster copying/replication\n-Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions"},{"upvote_count":"1","poster":"SudhakarMani","timestamp":"1607287860.0","content":"Is it correct answer?","comment_id":"236799"},{"content":"Provided link says at least 265 MB but greater than 250 MB seems good enough.\nI would agree with the answer","upvote_count":"2","timestamp":"1607166660.0","comment_id":"235614","poster":"syu31svc"},{"timestamp":"1606495980.0","upvote_count":"1","comment_id":"229149","content":"Not really, it's a trap. Files should be grater than 256 mb regarding to best practises. So bigger file thant 250 like 251 it's not a solution.","poster":"Torent2005"},{"comment_id":"224823","upvote_count":"2","timestamp":"1606032120.0","content":"Agree with @Piiri565","poster":"BaisArun"}],"unix_timestamp":1605196560,"answer_description":"Depending on what services and workloads are using the data, a good size to consider for files is 256 MB or greater. If the file sizes cannot be batched when landing in Data Lake Storage Gen1, you can have a separate compaction job that combines these files into larger ones.\nNote: POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files. As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1. Avoiding small file sizes can have multiple benefits, such as:\n✑ Lowering the authentication checks across multiple files\n✑ Reduced open file connections\n✑ Faster copying/replication\n✑ Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices","exam_id":66,"answer_ET":"A","isMC":true,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are designing an HDInsight/Hadoop cluster solution that uses Azure Data Lake Gen1 Storage.\nThe solution requires POSIX permissions and enables diagnostics logging for auditing.\nYou need to recommend solutions that optimize storage.\nProposed Solution: Ensure that files stored are larger than 250MB.\nDoes the solution meet the goal?","question_images":[],"topic":"1","timestamp":"2020-11-12 16:56:00","answer_images":[],"choices":{"A":"Yes","B":"No"},"answer":"A","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/36865-exam-dp-201-topic-1-question-1-discussion/"},{"id":"W2UwyP1wDUIBoKPNJhuU","question_text":"You are designing a data processing solution that will implement the lambda architecture pattern. The solution will use Spark running on HDInsight for data processing.\nYou need to recommend a data storage technology for the solution.\nWhich two technologies should you recommend? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","answer_ET":"AE","timestamp":"2020-04-01 14:32:00","answers_community":[],"choices":{"D":"Apache Cassandra","A":"Azure Cosmos DB","C":"Azure Storage Queue","B":"Azure Service Bus","E":"Kafka HDInsight"},"discussion":[{"upvote_count":"26","content":"for batch processing - cosmos DB ,\nfor Stream processing - Kafka HDinsight","poster":"Abhilvs","comment_id":"115199","timestamp":"1592711700.0"},{"content":"Is Kafka considered a data storage solution? I thought it was a streaming technology.","timestamp":"1585744320.0","poster":"mclawson1966","comment_id":"70119","upvote_count":"11","comments":[{"content":"https://www.confluent.io/blog/okay-store-data-apache-kafka/ [ it states something like this - \"It is much closer in architecture to a distributed filesystem or database then to traditional message queue.\" ]","timestamp":"1589808480.0","comment_id":"91342","poster":"JamesCho","upvote_count":"2"}]},{"comment_id":"348907","poster":"Wendy_DK","content":"Question here is :You need to recommend a data storage technology for the solution.\nAnswer: cosmos DB and Blob blob. Yet Azure Kafka is for stream processing","upvote_count":"1","timestamp":"1620068700.0"},{"timestamp":"1619861100.0","upvote_count":"2","content":"for batch: Cosmos DB\nfor stream: Kafka HD insight","comment_id":"346739","poster":"sjain91"},{"comment_id":"344826","content":"A. Azure Cosmos DB\nD. Apache Cassandra","timestamp":"1619634120.0","upvote_count":"3","poster":"davita8"},{"poster":"Deepu1987","comment_id":"292655","upvote_count":"3","content":"Given solution is right & pls go through this link \nhttps://www.bluegranite.com/blog/exploring-the-lambda-architecture-in-azure \nKafka hdsight is for ingestion \nCosmos DB for processing","timestamp":"1613576340.0"},{"content":"https://www.bluegranite.com/blog/exploring-the-lambda-architecture-in-azure\nKafka for ingestion\nAs for processing, Cosmos DB would be it","poster":"syu31svc","timestamp":"1607434920.0","upvote_count":"1","comment_id":"238299"},{"upvote_count":"8","content":"Lambda architecture is usually built with Cassandra as a storage solution and Kafka as a Data stream technology, so Cosmos DB is the correct answer. There is no such thing as Apache Cassandra.","comment_id":"78680","comments":[{"timestamp":"1608364980.0","content":"What do you mean? There is Apache Cassandra - a distributed, wide column storage on Apache license. \n\nHowever, Cosmos DB & HDI Kafka are the answers for this question.","upvote_count":"2","comment_id":"247813","poster":"chaoxes"}],"timestamp":"1587653820.0","poster":"Tombarc"}],"isMC":true,"answer":"AE","unix_timestamp":1585744320,"exam_id":66,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0005600009.png"],"question_id":2,"url":"https://www.examtopics.com/discussions/microsoft/view/17760-exam-dp-201-topic-1-question-10-discussion/","topic":"1","question_images":[],"answer_description":"To implement a lambda architecture on Azure, you can combine the following technologies to accelerate real-time big data analytics:\n✑ Azure Cosmos DB, the industry's first globally distributed, multi-model database service.\n✑ Apache Spark for Azure HDInsight, a processing framework that runs large-scale data analytics applications\nAzure Cosmos DB change feed, which streams new data to the batch layer for HDInsight to process\n\n✑ The Spark to Azure Cosmos DB Connector\nE: You can use Apache Spark to stream data into or out of Apache Kafka on HDInsight using DStreams.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architecture"},{"id":"xQP150hVDa6u3uGz72zP","unix_timestamp":1570706880,"answer_description":"","question_text":"A company manufactures automobile parts. The company installs IoT sensors on manufacturing machinery.\nYou must design a solution that analyzes data from the sensors.\nYou need to recommend a solution that meets the following requirements:\n✑ Data must be analyzed in real-time.\n✑ Data queries must be deployed using continuous integration.\n✑ Data must be visualized by using charts and graphs.\n✑ Data must be available for ETL operations in the future.\n✑ The solution must support high-volume data ingestion.\nWhich three actions should you recommend? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer_ET":"BCD","answer":"BDE","question_id":3,"choices":{"B":"Configure an Azure Event Hub to capture data to Azure Data Lake Storage.","E":"Develop an Azure Stream Analytics application that queries the data and outputs to Power BI. Use Azure Pipelines to deploy the Azure Stream Analytics application.","C":"Develop an Azure Stream Analytics application that queries the data and outputs to Power BI. Use Azure Data Factory to deploy the Azure Stream Analytics application.","F":"Develop an application that sends the IoT data to an Azure Data Lake Storage container.","A":"Use Azure Analysis Services to query the data. Output query results to Power BI.","D":"Develop an application that sends the IoT data to an Azure Event Hub."},"question_images":[],"answer_images":[],"exam_id":66,"answers_community":["BDE (100%)"],"isMC":true,"discussion":[{"upvote_count":"120","poster":"uge","content":"Reading\"Data queries must be deployed using continuous integration\", i think than correct answer it´s BDE and not BCD.","timestamp":"1570706880.0","comment_id":"14605"},{"comments":[{"timestamp":"1622740680.0","comment_id":"373770","poster":"Ashtrixx","upvote_count":"1","content":"ADF pipelines are used for ETL jobs and all, not for CI/CD, for that we need to use pipelines in azure devops"}],"comment_id":"182535","content":"Pipelines are subset activity in Azure factory, so C is correct which makes the answer BCD correct.","timestamp":"1600554840.0","upvote_count":"14","poster":"Paakofi"},{"comment_id":"602665","upvote_count":"1","content":"Selected Answer: BDE\nI believe it is BDE","timestamp":"1652725440.0","poster":"nefarious_smalls"},{"timestamp":"1621579560.0","comment_id":"362736","poster":"ismaelrihawi","upvote_count":"1","content":"CI = Azure Pipeline!"},{"timestamp":"1621497360.0","upvote_count":"2","comment_id":"361938","poster":"cadio30","content":"B,D,E are correct\n\nAzure pipeline is related to CI/CD process and one should differentiate the \"pipeline\" of Azure Data Factory. Also the azure stream analytics can output the data into PowerBI dataset. Hence, the Azure Analysis Service is not needed in the solution."},{"content":"BDE is the correct answer.","timestamp":"1619634300.0","upvote_count":"4","comment_id":"344829","poster":"davita8"},{"poster":"chirag1234","comment_id":"332057","upvote_count":"2","timestamp":"1617986460.0","content":"Answer will be BDE"},{"upvote_count":"2","comment_id":"238304","content":"https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment:\n\"There are two suggested methods to promote a data factory to another environment:\n\nAutomated deployment using Data Factory's integration with Azure Pipelines\"\nBDE it is","poster":"syu31svc","timestamp":"1607435340.0"},{"poster":"sandGrain","timestamp":"1604485800.0","comment_id":"212607","content":"BDE is the correct answer.","upvote_count":"2"},{"poster":"Arsa","upvote_count":"3","content":"The correct answer should be BDE\nAutomate continuous integration by using Azure Pipelines releases","comment_id":"162376","timestamp":"1597942740.0"},{"content":"In the same time they said :Data must be available for ETL operations in the future.\nSo the response is OK","upvote_count":"1","comment_id":"152373","timestamp":"1596781800.0","poster":"Taddi10"},{"timestamp":"1594824840.0","comment_id":"135796","comments":[{"comment_id":"135799","poster":"envy","content":"https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment","upvote_count":"1","timestamp":"1594824960.0"}],"upvote_count":"2","poster":"envy","content":"Tutorial: Deploy an Azure Stream Analytics job with CI/CD using Azure Pipelines https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-visual-studio-cicd-vsts"},{"poster":"bob_","content":"Continuous integration and deployment using Azure Data Factory | Azure Friday\n\nhttps://www.youtube.com/watch?v=WhUAX8YxxLk","timestamp":"1591963860.0","upvote_count":"3","comments":[{"comment_id":"126746","timestamp":"1593943080.0","poster":"Polash","content":"Right bob","upvote_count":"2"}],"comment_id":"108655"},{"timestamp":"1591779120.0","comment_id":"106614","upvote_count":"3","content":"I believe BDE is the right onec.\nhttps://azure.microsoft.com/en-us/blog/refreshing-reference-data-with-azure-data-factory-for-azure-stream-analytics-jobs-3/","poster":"Runi"},{"poster":"Abhitm","content":"I think the correct answer is ABD\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-power-bi-dashboard","timestamp":"1590665100.0","comment_id":"97468","upvote_count":"1"},{"timestamp":"1586242620.0","content":"Should be E instead of C.\nData queries must be deployed using continuous integration.\nAny thoughts?","poster":"zenomas","upvote_count":"3","comment_id":"72017"},{"timestamp":"1584507480.0","poster":"Nehuuu","content":"Answer - BDE","comment_id":"65471","upvote_count":"7"},{"comment_id":"64537","poster":"JuanVega","upvote_count":"7","content":"Clearly the answer should be BDE, Azure Pipelines is part of DevOps Services used for deploying production code. Here's the definition of Azure Pipelines \"Azure Pipelines combines continuous integration (CI) and continuous delivery (CD) to constantly and consistently test and build your code and ship it to any target.\" This was found in the link: https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops","timestamp":"1584317820.0"},{"poster":"mustaphaa","comment_id":"38775","upvote_count":"5","content":"C is wrong you cant deploy stream analytics using data factory -_-","timestamp":"1578986280.0"},{"upvote_count":"3","timestamp":"1575131040.0","poster":"JaBe","comment_id":"25488","content":"C is indeed wrong, E is correct but not for the reason uge gives. \"Use Azure Data Factory to deploy ...\" ADF is not a deployment tool. Azure Pipelines is. That's all there is to it."},{"upvote_count":"7","content":"I agree, the option C is wrong instead correct one is E, hence correct answer is BDE","timestamp":"1574855280.0","comment_id":"24800","poster":"Anagha"},{"timestamp":"1571818140.0","poster":"davidtstafford","comment_id":"16896","upvote_count":"5","content":"agreed"}],"timestamp":"2019-10-10 13:28:00","topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/6406-exam-dp-201-topic-1-question-11-discussion/"},{"id":"sYh0BX375XI5SWlObYM7","answer_ET":"B","answer":"B","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/24237-exam-dp-201-topic-1-question-12-discussion/","question_id":4,"topic":"1","unix_timestamp":1593323580,"answer_description":"To keep an interactive cluster configuration even after it has been terminated for more than 30 days, an administrator can pin a cluster to the cluster list.\nReference:\nhttps://docs.azuredatabricks.net/user-guide/clusters/terminate.html","answer_images":[],"answers_community":[],"timestamp":"2020-06-28 07:53:00","choices":{"A":"Start the cluster after it is terminated.","C":"Clone the cluster after it is terminated.","D":"Terminate the cluster manually at process completion.","B":"Pin the cluster"},"question_images":[],"exam_id":66,"discussion":[{"poster":"Arsa","content":"Pin a cluster\n30 days after a cluster is terminated, it is permanently deleted. To keep an interactive cluster configuration even after a cluster has been terminated for more than 30 days, an administrator can pin the cluster. Up to 20 clusters can be pinned.","comment_id":"162378","timestamp":"1597942980.0","upvote_count":"24"},{"comment_id":"123017","poster":"awron_durat","upvote_count":"10","timestamp":"1593459600.0","content":"You're trying to keep the configuration, not keep the cluster running. According to Satabricks, the answer is to pin the cluster.\nhttps://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster"},{"comment_id":"361945","poster":"cadio30","timestamp":"1621497900.0","upvote_count":"3","content":"B. Pin the cluster is the appropriate answer for the requirement\n\nReference: https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluste"},{"upvote_count":"2","comment_id":"247815","poster":"chaoxes","content":"B. Pin the cluster is an answer. \n\nTo keep a cluster configuration even after a cluster has been terminated (which is after 30 days) administrator must pin the cluster. \n\nSource: https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage","timestamp":"1608365040.0"},{"content":"Databricks documentation points to the use of pinning as the way to keep configurations: https://docs.databricks.com/clusters/index.html - 'Important!' section.","comment_id":"123677","poster":"Wirehinge","timestamp":"1593539400.0","upvote_count":"4"},{"poster":"shampoolegend","comments":[{"poster":"mojedapr","content":"just watched the session and you are right !","upvote_count":"3","comment_id":"191197","timestamp":"1601584140.0"}],"comment_id":"121653","upvote_count":"6","content":"according to the instructor at one of the training sessions provided by MS, the answer is A.\nHere is a screenshot from the session:\nhttps://ibb.co/LgL5gCz","timestamp":"1593323580.0"}],"question_text":"You are designing an Azure Databricks interactive cluster.\nYou need to ensure that the cluster meets the following requirements:\n✑ Enable auto-termination\n✑ Retain cluster configuration indefinitely after cluster termination.\nWhat should you recommend?"},{"id":"8RdV5WYLuMtTk1S2PNwG","question_images":[],"answer_description":"Spark in SQL Server big data cluster enables AI and machine learning.\nYou can use Apache Spark MLlib to create a machine learning application to do simple predictive analysis on an open dataset.\nMLlib is a core Spark library that provides many utilities useful for machine learning tasks, including utilities that are suitable for:\n✑ Classification\n✑ Regression\n✑ Clustering\n✑ Topic modeling\n✑ Singular value decomposition (SVD) and principal component analysis (PCA)\n✑ Hypothesis testing and calculating sample statistics\nReference:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-machine-learning-mllib-ipython","url":"https://www.examtopics.com/discussions/microsoft/view/8467-exam-dp-201-topic-1-question-13-discussion/","question_text":"You are designing a solution for a company. The solution will use model training for objective classification.\nYou need to design the solution.\nWhat should you recommend?","unix_timestamp":1574078880,"answer":"E","answer_ET":"E","answers_community":["E (100%)"],"choices":{"B":"a Spark Streaming job","C":"interactive Spark queries","E":"a Spark application that uses Spark MLib.","D":"Power BI models","A":"an Azure Cognitive Services application"},"question_id":5,"discussion":[{"comment_id":"66595","timestamp":"1584809280.0","poster":"agnaldo","content":"ooops... the correct is Spark ML Lib","upvote_count":"18"},{"upvote_count":"12","content":"one observation: the correct is Apache ML Lib","poster":"agnaldo","timestamp":"1584809220.0","comment_id":"66593"},{"upvote_count":"1","comment_id":"545292","timestamp":"1644582120.0","poster":"brieucboonen1","content":"Selected Answer: E\none observation: the correct is Apache ML Lib"},{"content":"A. an Azure Cognitive Services application","timestamp":"1630273800.0","comment_id":"434882","poster":"bdsrca","upvote_count":"1"},{"comment_id":"361954","upvote_count":"4","timestamp":"1621498260.0","content":"appropriate answer is E\n\nReference: https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-machine-learning-mllib-ipython","poster":"cadio30"},{"poster":"sjain91","upvote_count":"3","comment_id":"346742","timestamp":"1619861460.0","content":"Spark Mlib should be the correct answer"},{"upvote_count":"1","poster":"Deepu1987","comment_id":"295684","timestamp":"1613903460.0","content":"the keyword is Machine Learning \"objective classification\" to choose the ans choice Spark ML Lib"},{"content":"E. a Spark application that uses Spark MLib. is correct answer","timestamp":"1608365400.0","poster":"chaoxes","upvote_count":"3","comment_id":"247819"},{"poster":"syu31svc","timestamp":"1607435580.0","content":"Model training so Machine Learning is what should come into mind\nE is the answer","comment_id":"238307","upvote_count":"2"},{"comment_id":"174402","timestamp":"1599380040.0","poster":"Gluckos","content":"Cognitive services.. it's possibile train model with Custom Vision API","upvote_count":"2"},{"timestamp":"1590665580.0","comment_id":"97480","poster":"Abhitm","content":"It is for model training hence Spark ML Lib","upvote_count":"3"},{"timestamp":"1590560940.0","poster":"serger","upvote_count":"4","content":"For training model this is Spark MLlib that contains ML models for spark. Cognitive services is not for training a new model but to use some existing pretrained models.","comment_id":"96607"},{"comment_id":"32996","upvote_count":"6","timestamp":"1577448480.0","poster":"DrC","content":"The Computer Vision API in Cognitive Services can do this too, but it's out of scope for this exam."},{"comments":[{"comment_id":"76962","comments":[{"upvote_count":"4","poster":"Gluckos","timestamp":"1600280220.0","comment_id":"180521","content":"Cognitive services.. it's possibile train model with Custom Vision API"}],"timestamp":"1587386820.0","poster":"josecipiace","content":"It says model training, if you need to do model training cognitive services are not the correct solutions. They are already trained. The question refers to a custom scenario.","upvote_count":"17"}],"upvote_count":"4","content":"Why not use Cognitive Services ? it is built to achieve such classification tasks, is'nt it ?","poster":"STH","comment_id":"22416","timestamp":"1574078880.0"}],"topic":"1","exam_id":66,"timestamp":"2019-11-18 13:08:00","answer_images":[],"isMC":true}],"exam":{"numberOfQuestions":206,"id":66,"isImplemented":true,"isMCOnly":false,"isBeta":false,"provider":"Microsoft","name":"DP-201","lastUpdated":"12 Apr 2025"},"currentPage":1},"__N_SSP":true}