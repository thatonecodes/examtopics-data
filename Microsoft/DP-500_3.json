{"pageProps":{"questions":[{"id":"0kpdjT2tnIYX7mhGAqhG","answer_images":[],"question_text":"You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.\nYou need to present the data distribution statistics from a DataFrame in a tabular view.\nWhich method should you invoke on the DataFrame?","answer_description":"","question_images":[],"answers_community":["C (100%)"],"isMC":true,"answer_ET":"C","question_id":11,"url":"https://www.examtopics.com/discussions/microsoft/view/93149-exam-dp-500-topic-1-question-108-discussion/","topic":"1","timestamp":"2022-12-29 11:28:00","discussion":[{"comment_id":"762503","timestamp":"1672465200.0","poster":"cherious","content":"Selected Answer: C\nCorrect answer is Summary. Corr shows correlation between columns and it has nothing to do with data distribution statistics.\nSource: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html","upvote_count":"9"},{"comment_id":"784041","poster":"per_ing","timestamp":"1674373020.0","content":"Duplicate of 62, 101 and 103 only with other answer options. I believe the correct is still \"describe\" even though that is not an option here","upvote_count":"6"},{"upvote_count":"1","content":"question returns often with different answers\nhowever I believe it should always be summary\n.describe() function takes cols:String*(columns in df) as optional args.\n\n\n.summary() function takes statistics:String*(count,mean,stddev..etc) as optional args.","comment_id":"1013989","timestamp":"1695379920.0","poster":"Deloro"},{"poster":"solref","comment_id":"859590","timestamp":"1680502980.0","upvote_count":"1","content":"Selected Answer: C\nDataFrame.summary(*statistics)[source]\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html\n\nIt is the same question than 62 , 101 and 103. But in those cases the answer was \"describe\" and it's the same if you are looking for dataframe statistics detail \ndf.describe(['age']).show()\nhttps://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.describe.html"},{"timestamp":"1676376600.0","upvote_count":"1","comment_id":"808349","poster":"ThariCD","content":"Selected Answer: C\nIt should definitely be summary or describe, either works. Summary shows count, mean, stddev, min, max and quartiles: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html \nDescribe shows count, mean, stddev, min and max: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.describe.html#pyspark.sql.DataFrame.describe\nThe differences seem to be that summary is newer and includes the percentiles at 25%, 50% and 75%."},{"content":"Selected Answer: C\nsummary","timestamp":"1675390380.0","comment_id":"796635","upvote_count":"1","poster":"JuanData"},{"poster":"AshwinN1992","comment_id":"760879","content":"Please confirm","upvote_count":"1","timestamp":"1672309680.0"}],"answer":"C","unix_timestamp":1672309680,"exam_id":70,"choices":{"A":"freqItems","C":"summary","D":"rollup","B":"corr"}},{"id":"9YrUnEQKaJy01qYBKcTv","answer":"B","choices":{"C":"the query history","B":"the query plan","A":"the query statistics"},"question_text":"You are using DAX Studio to analyze a slow-running report query.\nYou need to identify inefficient join operations in the query.\nWhat should you review?","topic":"1","unix_timestamp":1673141520,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/94359-exam-dp-500-topic-1-question-109-discussion/","answers_community":["B (80%)","A (20%)"],"question_id":12,"isMC":true,"answer_description":"","answer_ET":"B","discussion":[{"poster":"PrudenceK","comment_id":"939477","upvote_count":"1","timestamp":"1688173980.0","content":"Selected Answer: B\nCorrect"},{"content":"Selected Answer: B\nI think is B","poster":"DarioReymago","comment_id":"869880","timestamp":"1681434540.0","upvote_count":"1"},{"timestamp":"1674722280.0","content":"Selected Answer: B\nB, Query Plan.","upvote_count":"2","comment_id":"788534","poster":"Az301301X"},{"poster":"Saffar","comment_id":"775117","content":"I think the answer B is correct, there is nothing called Query Statistics in DAX Studio","timestamp":"1673675220.0","upvote_count":"3"},{"comment_id":"771647","timestamp":"1673372220.0","upvote_count":"3","poster":"moreinva43","content":"A.) it doesn't say that the data source for the Power BI Report is a SQL database and B.) the website you referenced is about finding slow running queries on a SQL Database and C.) that tool has nothing to do with Dax Studios. And in trying to find long running DQX queries, it is looking at the query plan https://www.sqlbi.com/tv/query-performance-tuning-in-dax-studio/"},{"timestamp":"1673141520.0","comments":[{"comments":[{"upvote_count":"1","comment_id":"869879","poster":"DarioReymago","content":"thats true that link is for sql","timestamp":"1681434480.0"}],"content":"Isn't it for SQL server, not DAX Studio?","upvote_count":"1","poster":"cookiemonster42","timestamp":"1674616980.0","comment_id":"787250"}],"comment_id":"769014","upvote_count":"1","poster":"louisaok","content":"Selected Answer: A\nwe need to enable the Query Statistics for this session\n\nhttps://www.sqlshack.com/how-to-identify-slow-running-queries-in-sql-server/"}],"exam_id":70,"timestamp":"2023-01-08 02:32:00","question_images":[]},{"id":"Zm3dV6uqI5SqSKLMdDxY","answer":"","question_id":13,"unix_timestamp":1670874120,"question_images":["https://img.examtopics.com/dp-500/image8.png","https://img.examtopics.com/dp-500/image9.png","https://img.examtopics.com/dp-500/image12.png"],"topic":"1","discussion":[{"poster":"nbagchi","timestamp":"1670874120.0","comment_id":"743269","upvote_count":"15","content":"Correct\nFor more info check: https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-ols?tabs=table"},{"poster":"bb120kg","comment_id":"1126591","timestamp":"1705658760.0","upvote_count":"1","content":"Chatgpt-4 says :\n\n1. Create a role\n2. Add a table or filter\n3. Publish"},{"timestamp":"1692123600.0","upvote_count":"2","content":"It should be-\n1.Create a role\n2. Add a table and filter\n3.Set OLS to None for Customer Email column\nWhy are we skipping the add table and filter stage?","poster":"Lolus","comment_id":"981913"},{"content":"correctcorrect","comment_id":"871880","poster":"DarioReymago","comments":[{"poster":"DarioReymago","comment_id":"871882","timestamp":"1681659360.0","upvote_count":"1","content":"but I will publish the report, not the dataset"}],"upvote_count":"2","timestamp":"1681659300.0"},{"comment_id":"855250","poster":"solref","timestamp":"1680148860.0","upvote_count":"1","content":"it is OK :)"},{"comment_id":"849105","timestamp":"1679645220.0","upvote_count":"1","poster":"solref","content":"It is right !"},{"content":"It's the only logical sequence to perform that action. The answer is correct.","poster":"Hongzu13","comment_id":"832878","upvote_count":"1","timestamp":"1678274400.0"},{"comment_id":"785330","content":"correct. For more info check: https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-ols?tabs=table","timestamp":"1674476280.0","upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1716381120.0","poster":"Ihueghian","comment_id":"1215717","content":"Read, correct and thanks for the link"}],"poster":"stfglv"}],"answers_community":[],"answer_images":["https://img.examtopics.com/dp-500/image13.png"],"isMC":false,"timestamp":"2022-12-12 20:42:00","exam_id":70,"url":"https://www.examtopics.com/discussions/microsoft/view/91275-exam-dp-500-topic-1-question-11-discussion/","answer_ET":"","answer_description":"","question_text":"DRAG DROP -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\n\nExisting environment -\nLitware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.\n\nAzure Resources -\nLitware has the following Azure resources:\nAn Azure Synapse Analytics workspace named synapseworkspace1\nAn Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1\nA Synapse Analytics dedicated SQL pool named SQLDW\n\nDedicated SQL Pool -\nSQLDW contains a dimensional model that contains the following tables.\n//IMG//\n\nSQLDW contains the following additional tables.\n//IMG//\n\nSQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].\nThe sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.\n\nPower BI -\nLitware has a new Power BI tenant that contains an empty workspace named Sales Analytics.\nAll users have Power BI Premium per user licenses.\nIT data analytics are workspace administrators. The IT data analysts will create datasets and reports.\nA single imported dataset will be created to support the company’s sales analytics goals. The dataset will be refreshed every 30 minutes.\n\nRequirements -\n\nAnalytics Goals -\nLitware identifies the following analytics goals:\nProvide historical reporting of sales by product and channel over time.\nAllow sales managers to perform ad hoc sales reporting with minimal effort.\nPerform market basket analysis to understand which products are commonly purchased in the same transaction.\nIdentify which customers should receive promotional emails based on their likelihood of purchasing promoted products.\nLitware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.\n\nSecurity Requirements -\nLitware identifies the following security requirements for the analytics environment:\nAll the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.\nCustomer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.\nSales associates must be prevented from seeing the CustomerEmail column in Power BI reports.\nSales managers must be prevented from modifying reports created by other users.\nDevelopment Process Requirements\nLitware identifies the following development process requirements:\nSQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.\nPower BI content must be deployed to test and production by using deployment pipelines.\nAll SQL scripts must be stored in Azure Repos.\nThe IT data analysts prefer to build Power BI reports in Synapse Studio.\nYou need to implement object-level security (OLS) in the Power BI dataset for the sales associates.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n//IMG//"},{"id":"snK6B6x84SmmOeGfF6dM","answer_description":"","answer":"B","answer_images":[],"exam_id":70,"topic":"1","answer_ET":"B","choices":{"C":"Remove the alternative result of BLANK() from the DIVIDE function.","B":"Replace both CALCULATE functions by using a variable that contains the CALCULATE function.","D":"Remove 'Calendar'[Flag] = \"YTD\" from the code.","A":"Create a variable and replace the values for [Sales Amount]."},"url":"https://www.examtopics.com/discussions/microsoft/view/95769-exam-dp-500-topic-1-question-110-discussion/","timestamp":"2023-01-18 05:20:00","question_images":["https://img.examtopics.com/dp-500/image135.png"],"isMC":true,"answers_community":["B (100%)"],"question_text":"You have a Power Bi dataset that contains the following measure.\n//IMG//\n\nYou need to improve the performance of the measure without affecting the logic or the results.\nWhat should you do?","unix_timestamp":1674015600,"question_id":14,"discussion":[{"upvote_count":"2","content":"Selected Answer: B\nB is correct answer.","comment_id":"936434","poster":"Eltooth","timestamp":"1687945080.0"},{"timestamp":"1681434660.0","poster":"DarioReymago","comment_id":"869882","content":"Selected Answer: B\nB is correct","upvote_count":"2"},{"comment_id":"854197","timestamp":"1680080520.0","poster":"solref","upvote_count":"2","content":"Selected Answer: B\nIt is OK!"},{"comment_id":"779599","timestamp":"1674015600.0","poster":"louisaok","content":"Selected Answer: B\nYes, B is correct. use a function to replace the repeated query","upvote_count":"4"}]},{"id":"6epI7SswpLqHETtEJOd0","question_images":[],"isMC":true,"answer_description":"","question_text":"You are optimizing a dataflow in a Power BI Premium capacity. The dataflow performs multiple joins.\nYou need to reduce the load time of the dataflow.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","unix_timestamp":1672309680,"choices":{"C":"Execute non-foldable operations before foldable operations.","D":"Place the ingestion operations and transformation operations in separate dataflows.","E":"Reduce the memory assigned to the dataflows.","B":"Place the ingestion operations and transformation operations in a single dataflow.","A":"Execute foldable operations before non-foldable operations."},"answer":"AD","discussion":[{"content":"Selected Answer: AD\nTo reduce the load time of a dataflow in Power BI Premium, you should consider the following actions:\n\nA. Execute foldable operations before non-foldable operations: Foldable operations can be pushed down to the data source and executed there, reducing the amount of data transferred to Power BI. By executing foldable operations early in the dataflow, you can minimize the data transfer and improve performance.\n\nD. Place the ingestion operations and transformation operations in separate dataflows: Separating ingestion operations (such as loading data from a source) and transformation operations (such as data cleansing or joining) into separate dataflows can help optimize the load time. In this way, the ingestion operations can be executed less frequently, while the transformation operations can be performed on an as-needed basis, reducing the overall load time.\n\nThese actions aim to optimize the dataflow's performance by leveraging foldable operations and optimizing the workflow structure.","comment_id":"939479","timestamp":"1688174160.0","poster":"PrudenceK","upvote_count":"2"},{"upvote_count":"4","poster":"louisaok","comment_id":"779600","content":"Selected Answer: AD\nFor A:\n\n*Query folding is the mechanism to push Power Query transformations back to the source to reduce the load of Power Query…. The query is much more complicated, as it has to merge (Join) the source tables to form the result. But the database engine is optimised for doing such stuff and will perform this task much better than Power Query.\n\nhttps://towardsdatascience.com/exploring-query-folding-in-power-query-8288fb3c9c2f","timestamp":"1674015900.0"},{"upvote_count":"3","timestamp":"1672464300.0","poster":"cherious","comment_id":"762502","content":"Selected Answer: AD\nThe answer is correct. You should split the ETL stage into two separate dataflows.\nSource: https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-premium-features?tabs=gen2#using-the-enhanced-compute-engine"},{"timestamp":"1672309680.0","poster":"AshwinN1992","upvote_count":"2","comment_id":"760880","content":"Please confirm"}],"timestamp":"2022-12-29 11:28:00","topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/93150-exam-dp-500-topic-1-question-111-discussion/","question_id":15,"answer_ET":"AD","answers_community":["AD (100%)"],"answer_images":[],"exam_id":70}],"exam":{"isImplemented":true,"isBeta":false,"lastUpdated":"12 Apr 2025","isMCOnly":false,"id":70,"numberOfQuestions":183,"provider":"Microsoft","name":"DP-500"},"currentPage":3},"__N_SSP":true}