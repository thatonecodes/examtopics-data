{"pageProps":{"questions":[{"id":"cfEeseMZluSsSLlsNvOY","answer_description":"","timestamp":"2025-01-15 02:59:00","topic":"4","url":"https://www.examtopics.com/discussions/microsoft/view/154518-exam-dp-203-topic-4-question-68-discussion/","answer_ET":"A","question_id":366,"answers_community":["A (100%)"],"unix_timestamp":1736906340,"question_images":[],"isMC":true,"choices":{"B":"metrics","A":"diagnostic settings","D":"alerts","C":"logs"},"exam_id":67,"question_text":"You have an Azure subscription that contains an Azure data factory named ADF1 and a Log Analytics workspace named Workspace1.\n\nYou need to configure ADF1 to send execution information for pipelines to Workspace1.\n\nWhat should you configure?","discussion":[{"poster":"aca357f","upvote_count":"3","content":"Selected Answer: A\nTo send execution information (such as activity runs, pipeline runs, and trigger runs) from Azure Data Factory (ADF) to a Log Analytics workspace, you need to configure diagnostic settings in Azure Data Factory. Diagnostic settings allow you to stream logs and metrics to external services such as Log Analytics, Event Hub, or Azure Storage for monitoring and analysis.","timestamp":"1736906340.0","comment_id":"1340603"}],"answer_images":[],"answer":"A"},{"id":"yofB2mPJpjW0L50bGLKk","topic":"4","question_images":[],"question_id":367,"choices":{"D":"a database scoped credential","A":"a stored access policy","C":"a managed identity","B":"a server-level credential"},"question_text":"You have an Azure Blob storage account named storage1 and an Azure Synapse Analytics serverless SQL poo1 named Pool1.\n\nFrom Pool1, you plan to run ad-hoc queries that target storage1.\n\nYou need to ensure that you can use shared access signature (SAS) authorization without defining a data source.\n\nWhat should you create first?","url":"https://www.examtopics.com/discussions/microsoft/view/155464-exam-dp-203-topic-4-question-69-discussion/","discussion":[{"content":"Selected Answer: D\nD. a database scoped credential\nWhen querying Azure Blob Storage (storage1) from Azure Synapse Analytics serverless SQL pool (Pool1) using Shared Access Signature (SAS) authorization, you need a way for Synapse to authenticate to the storage without explicitly defining a data source.\n\nTo achieve this, you must:\n\nCreate a database-scoped credential that stores the SAS token.\nUse OPENROWSET or external tables to query the data.","upvote_count":"2","poster":"imatheushenrique","comment_id":"1399871","timestamp":"1742251800.0"}],"exam_id":67,"answer_images":[],"answer_ET":"D","answer_description":"","isMC":true,"answers_community":["D (100%)"],"answer":"D","unix_timestamp":1737944820,"timestamp":"2025-01-27 03:27:00"},{"id":"kjwnGnVIY4ElWoxV3oww","answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/62062-exam-dp-203-topic-4-question-7-discussion/","choices":{"B":"Assign a smaller resource class to the automated data load queries.","C":"Assign a larger resource class to the automated data load queries.","A":"Hash distribute the large fact tables in DW1 before performing the automated data loads.","D":"Create sampled statistics for every column in each table of DW1."},"answer_images":[],"answer":"C","answers_community":["C (100%)"],"question_id":368,"isMC":true,"exam_id":67,"answer_ET":"C","question_text":"You have an Azure data solution that contains an enterprise data warehouse in Azure Synapse Analytics named DW1.\nSeveral users execute ad hoc queries to DW1 concurrently.\nYou regularly perform automated data loads to DW1.\nYou need to ensure that the automated data loads have enough memory available to complete quickly and successfully when the adhoc queries run.\nWhat should you do?","unix_timestamp":1631665680,"discussion":[{"timestamp":"1663201680.0","comment_id":"444875","poster":"Podavenna","content":"Correct answer!","upvote_count":"21"},{"poster":"kkk5566","content":"Selected Answer: C\nic correct","upvote_count":"1","timestamp":"1725088860.0","comment_id":"994887"},{"content":"Selected Answer: C\nAssigning a larger resource class to the automated data load queries prioritizes their resource allocation, allowing them to complete without being heavily impacted by the concurrent ad hoc queries. This helps avoid contention and ensures that the data loads can utilize the necessary resources to complete successfully.","upvote_count":"2","poster":"vctrhugo","timestamp":"1719612060.0","comment_id":"937170"},{"upvote_count":"3","content":"Selected Answer: C\nagreed","comment_id":"861554","poster":"AHUI","timestamp":"1712269920.0"},{"poster":"Deeksha1234","content":"correct","comment_id":"645999","upvote_count":"3","timestamp":"1691862840.0"},{"poster":"juanlu46","timestamp":"1682442420.0","upvote_count":"3","content":"Selected Answer: C\nIs correct!","comment_id":"591869"},{"comment_id":"455267","timestamp":"1664595420.0","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/resource-classes-for-workload-management","poster":"aortega","upvote_count":"3"}],"question_images":[],"topic":"4","timestamp":"2021-09-15 02:28:00"},{"id":"u5ZH1vU55gIeDiDSrOSu","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/156124-exam-dp-203-topic-4-question-70-discussion/","unix_timestamp":1738971420,"question_images":[],"answers_community":["D (57%)","C (43%)"],"exam_id":67,"answer_description":"","topic":"4","question_text":"You have an Azure subscription that contains an Azure Synapse Analytics workspace named Workspace1, a Log Analytics workspace named Workspace2, and an Azure Data Lake Storage Gen2 container named Container1.\n\nWorkspace1 contains an Apache Spark job named Job1 that writes data to Container1. Workspace1 sends diagnostics to Workspace2.\n\nFrom Synapse Studio, you submit Job1.\n\nWhat should you use to review the LogQuery output of the job?","answer_ET":"A","isMC":true,"answer":"D","discussion":[{"timestamp":"1741105980.0","upvote_count":"3","comment_id":"1365004","poster":"nadavw","content":"Selected Answer: C\nHere's why:\n\nAzure Synapse Analytics integrates with Azure Monitor and Log Analytics.\nDiagnostics from Synapse workspaces, including Spark job logs, are sent to the configured Log Analytics workspace (Workspace2 in this case).\nWithin Log Analytics, the data is stored in tables that can be queried using KQL.\nYou can write KQL queries to retrieve the LogQuery output of Job1.\n(Gemini)"},{"timestamp":"1738971420.0","comment_id":"1353161","content":"Selected Answer: D\nWithin Synapse Studio Monitoring, you can check the Spark Application logs under Monitor â†’ Apache Spark applications.\n\nDefintely not A: This would contain output data, not log queries or diagnostic logs.\nNot B: There is returned URL that provides live monitoring but is not the best choice for reviewing past logs after execution.\nNot C: While Log Analytics in Workspace2 stores logs, there is no direct \"table\" for Spark logs without running KQL queries manually.","poster":"EnigmaOracle","upvote_count":"4"}],"timestamp":"2025-02-08 00:37:00","choices":{"C":"a table in Workspace2","B":"the Spark monitoring URL returned after Job1 is submitted","D":"the Apache Spark applications option on the Monitor tab","A":"the files in the result subfolder of Container1"},"question_id":369},{"id":"TJIFWZtTIVDnabUcjssA","answer_description":"","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/156125-exam-dp-203-topic-4-question-71-discussion/","topic":"4","exam_id":67,"unix_timestamp":1738971540,"question_id":370,"isMC":true,"answer_ET":"B","timestamp":"2025-02-08 00:39:00","choices":{"D":"Use Resilient Distributed Datasets (RDDs).","B":"Cache the dataset.","C":"Increase the spark.sql.autoBroadcastJoinThreshold value.","A":"For Container1, disable hierarchical namespaces."},"answer":"B","question_text":"You have an Azure subscription that contains an Azure Data Lake Storage Gen2 container named Container1 and an Azure Synapse Analytics workspace named Workspace1.\n\nWorkspace1 contains multiple Apache Spark jobs that reference a large dataset in Container1.\n\nYou need to optimize the run times of the jobs.\n\nWhat should you do?","answer_images":[],"question_images":[],"discussion":[{"upvote_count":"1","comment_id":"1399870","content":"Selected Answer: B\nB. Cache the dataset\nn Spark, caching is a mechanism for storing data in memory to speed up access to that data. When you cache a dataset, Spark keeps the data in memory so that it can be quickly retrieved the next time it is needed\n\nWorkspace1 contains multiple Apache Spark jobs that reference a large dataset in Container1.","timestamp":"1742251680.0","poster":"imatheushenrique"}]}],"exam":{"lastUpdated":"12 Apr 2025","provider":"Microsoft","isImplemented":true,"isMCOnly":false,"numberOfQuestions":384,"isBeta":false,"id":67,"name":"DP-203"},"currentPage":74},"__N_SSP":true}