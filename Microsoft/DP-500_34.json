{"pageProps":{"questions":[{"id":"5o7XdYnluWstVy9xmeVY","answers_community":["D (100%)"],"answer_description":"","question_text":"You have a Power BI workspace that contains one dataset and four reports that connect to the dataset.\nThe dataset uses import storage mode and contains the following data source:\nA CSV file in an Azure Storage account.\nAn Azure Database for PostgreSQL database.\nYou plan to use deployment pipelines to promote the content from development to test to production. There will be different data source locations for each stage.\nWhat should you include in the deployment pipeline to ensure that the appropriate data source locations are used during each stage?","answer_ET":"D","answer":"D","url":"https://www.examtopics.com/discussions/microsoft/view/92413-exam-dp-500-topic-1-question-83-discussion/","question_images":[],"choices":{"A":"auto-binding across pipelines","B":"data source rules","D":"parameter rules","C":"selective deployment"},"topic":"1","question_id":166,"isMC":true,"timestamp":"2022-12-22 07:15:00","answer_images":[],"unix_timestamp":1671689700,"exam_id":70,"discussion":[{"comment_id":"762498","content":"Selected Answer: D\nCorrect answer is D and here is why:\nIn the best practices for deployment, there is written that you can use both data source rules and parameters as well, however, they recommend parameters.\n\nSource: https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-best-practices#use-parameters-in-your-model\n\nAlso, data source rules is supported for limited data sources where I can not find Azure storage Account so I think data source won't be applicable to our case.\nSource: https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-get-started#supported-data-sources-for-dataflow-and-dataset-rules","upvote_count":"9","poster":"cherious","timestamp":"1672463820.0"},{"comment_id":"753006","poster":"Maazi","timestamp":"1671689700.0","upvote_count":"5","content":"D is the answer. This is based on what I read from https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-best-practices"},{"content":"Selected Answer: D\nThe option D (parameter rules) in the right answer.\nData source rules work for some specific sources only, below you can see the supported data sources for data set rules:\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-create-rules#supported-data-sources-for-dataflow-and-dataset-rules","upvote_count":"2","timestamp":"1676028060.0","poster":"Fer079","comment_id":"804238"},{"poster":"stfglv","upvote_count":"1","content":"Step 4 from this link https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-get-started makes me think we should go with D.","comment_id":"785206","comments":[{"content":"Thanks for the link :)","comment_id":"853181","timestamp":"1680003360.0","upvote_count":"1","poster":"solref"}],"timestamp":"1674468840.0"},{"upvote_count":"3","content":"Correct answer is B. Data Source rules are used to switch datasources when using Deployment Pipelines. Parameter Rules only work when you have created parameters within the Power BI Desktop File (.pbix)","timestamp":"1672004820.0","comments":[{"content":"Having cross-check this again, I'm leaning towards B too. Will be good to see what others think too.","upvote_count":"2","timestamp":"1672323600.0","poster":"Maazi","comment_id":"761108"}],"poster":"BigEd","comment_id":"756026"}]},{"id":"HN1ufgq8szZshlsRRDij","exam_id":70,"answer_description":"","answers_community":["A (74%)","C (26%)"],"answer_images":[],"timestamp":"2022-12-19 06:15:00","topic":"1","question_images":[],"question_text":"You are planning a Power BI solution for a customer.\nThe customer will have 200 Power BI users. The customer identifies the following requirements:\nEnsure that all the users can create paginated reports.\nEnsure that the users can create reports containing AI visuals.\nProvide autoscaling of the CPU resources during heavy usage spikes.\nYou need to recommend a Power BI solution for the customer. The solution must minimize costs.\nWhat should you recommend?","url":"https://www.examtopics.com/discussions/microsoft/view/92041-exam-dp-500-topic-1-question-84-discussion/","unix_timestamp":1671426900,"answer":"A","discussion":[{"timestamp":"1673946900.0","comment_id":"778729","poster":"louisaok","content":"Selected Answer: A\nhttps://powerbi.microsoft.com/en-au/pricing/\n\n\"Enable autoscale with your Azure subscription to automatically scale Power BI Premium capacity.\"","upvote_count":"6"},{"upvote_count":"1","content":"This question is incorrect itself. You still need 200 pro licenses with Premium Capacity if people are building reports. That would make you lean to PPU, but it doesn't have autoscale (because it uses shared capacity).","poster":"reemprive","timestamp":"1705852800.0","comment_id":"1127925"},{"upvote_count":"3","poster":"Maddy723","content":"Selected Answer: A\nAnswer is A - Autoscale is not available with PPU","comment_id":"972616","timestamp":"1691199540.0"},{"content":"Selected Answer: C\nPower BI Premium per capacity provides dedicated resources for the entire organization, which can be beneficial when you have a large number of users who need access to the same Premium features. It allows for greater control and flexibility in managing resources and workloads across the organization.\n\nHowever, if the customer has 200 Power BI users and they all require access to paginated reports and AI visuals, Power BI Premium per user (option C) would be more cost-effective. With Power BI Premium per user, each individual user gets access to the Premium features, and the customer only pays for the licenses needed for the 200 users.\n\nIn summary, both options (Power BI Premium per capacity and Power BI Premium per user) can meet the stated requirements, but option C (Power BI Premium per user) is likely to be the more cost-effective choice for a customer with 200 Power BI users.","comment_id":"971733","timestamp":"1691130840.0","poster":"Alborz","upvote_count":"2"},{"poster":"Eltooth","comment_id":"936409","content":"Selected Answer: A\nA is correct answer.","upvote_count":"1","timestamp":"1687944000.0"},{"comment_id":"935569","timestamp":"1687881540.0","upvote_count":"3","poster":"Plb2","content":"Selected Answer: C\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-faq#how-can-i-control-the-costs-of-autoscaling-\n\n\"Autoscaling is an optional feature of Power BI Premium\""},{"timestamp":"1680004200.0","comment_id":"853200","upvote_count":"3","poster":"solref","content":"Selected Answer: A\nhttps://powerbi.microsoft.com/en-au/pricing/#premium-add-on-card-autoscale \nPremium capacity enables customers to automatically add compute capacity to avoid slowdowns under heavy use, using Autoscale."},{"poster":"Fer079","comment_id":"804258","upvote_count":"2","content":"Selected Answer: C\nPower BI Premium per user supports autoscaling compute capacity (It's supported for all Power BI premiums):\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-auto-scale\n\nAlso, it supports until 200 users, and it´s cheaper than Power BI Premium per capacity","timestamp":"1676029620.0","comments":[{"comment_id":"1192230","poster":"vishal10","timestamp":"1712659980.0","content":"Power BI Premium Per User (PPU):\n\nPaginated Reports: Power BI Premium Per User (PPU) allows all users to create and consume paginated reports, which are a premium feature not included in Power BI Pro.\nAI Visuals: PPU includes access to AI visuals such as key influencers, decomposition trees, and anomaly detection, enabling users to leverage advanced analytics capabilities.\nAutoscaling and Cost Savings: PPU provides autoscaling of CPU resources based on usage demands, ensuring optimal performance during peak times while scaling down during lighter usage periods.\nCost-Effective: PPU is priced per user per month, making it cost-effective for organizations with a moderate number of users like your customer (200 users).","upvote_count":"1"}]},{"upvote_count":"4","timestamp":"1672478820.0","content":"Selected Answer: A\nBecause of auto scale, Premium Per Capacity must be selected","poster":"cherious","comment_id":"762582"},{"timestamp":"1672004940.0","upvote_count":"3","poster":"BigEd","comment_id":"756028","content":"It's better to use PPU when it's less than 250 users. Answer C is correct.","comments":[{"timestamp":"1672005000.0","upvote_count":"4","content":"Scratch that. Because of autoscale A is correct.","comment_id":"756029","poster":"BigEd"}]},{"comment_id":"749482","poster":"ABHI2023","timestamp":"1671426900.0","upvote_count":"3","content":"Selected Answer: A\nReference : https://powerbi.microsoft.com/en-au/pricing/"}],"question_id":167,"answer_ET":"A","isMC":true,"choices":{"C":"Power BI Premium per user","A":"a Power BI Premium per capacity","B":"Power BI Report Server","D":"Power BI Pro per user"}},{"id":"SJSIkxDH9DBCgjXVtBYM","answer_description":"","question_text":"HOTSPOT -\nYou need to configure a source control solution for Azure Synapse Analytics. The solution must meet the following requirements:\nCode must always be merged to the main branch before being published, and the main branch must be used for publishing resources.\nThe workspace templates must be stored in the publish branch.\nA branch named dev123 will be created to support the development of a new feature.\nWhat should you do? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n//IMG//","question_images":["https://img.examtopics.com/dp-500/image101.png"],"isMC":false,"discussion":[{"poster":"Ivgo","upvote_count":"8","content":"Set the collaboration branch to: main \nBase the dev123 branch on: main (you can not base dev123 on the Publish branch)","timestamp":"1689005160.0","comment_id":"948254"},{"upvote_count":"7","content":"main & workspace_publish.\n\nI think the answer is correct.\n\n\"By default, Synapse Studio generates the workspace templates and saves them into a branch called workspace_publish.\"\n\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/source-control#configure-publishing-settings","poster":"cherious","comment_id":"762640","timestamp":"1672490400.0"},{"upvote_count":"3","comment_id":"984251","poster":"PaulinhoDummett","timestamp":"1692344520.0","content":"main & main\nYou will usually always want to base a feature branch off the main branch. The publish branch does not contain any of the actual resources, just a template for deployment."},{"content":"I think answer is main and main. Collaboration branch is main and dev branch should be based on main know. Why do we need to base it on publish branch generated by workspace?","comment_id":"871590","poster":"Dileep1","upvote_count":"4","timestamp":"1681633020.0"},{"comment_id":"867130","timestamp":"1681205340.0","upvote_count":"4","poster":"DarioReymago","content":"main & workspace_publish\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/source-control#creating-feature-branches"},{"timestamp":"1671796560.0","poster":"Ramiel","comment_id":"754170","upvote_count":"5","content":"I think the answer should be:\n- main\n- publish\nAs the question states:\nThe workspace templates must be stored in the publish branch."}],"unix_timestamp":1671796560,"exam_id":70,"question_id":168,"answer_ET":"","answer":"","answer_images":["https://img.examtopics.com/dp-500/image102.png"],"timestamp":"2022-12-23 12:56:00","url":"https://www.examtopics.com/discussions/microsoft/view/92568-exam-dp-500-topic-1-question-85-discussion/","topic":"1","answers_community":[]},{"id":"jRkWueYYrgG2BgYUR2Xd","isMC":true,"answer_images":[],"answer":"C","answers_community":["C (100%)"],"exam_id":70,"topic":"1","question_images":[],"question_id":169,"answer_description":"","answer_ET":"C","discussion":[{"upvote_count":"8","content":"Selected Answer: C\nTo schedule refresh of your R visuals or dataset, enable scheduled refresh and install an on-premises data gateway (personal mode) on the computer containing the workbook and R.\n\nSource: https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-r-in-query-editor#considerations-and-limitations","poster":"cherious","timestamp":"1672466100.0","comment_id":"762509"},{"poster":"louisaok","upvote_count":"4","content":"Selected Answer: C\nC is correct.\n\n*On-premise data gateway (personal mode): 1 user, no sharing with other users, for you to publish reports without sharing the data source with others. Allows only A specific user to manage data within the data source. It means other users Can’t connect to it.\n\n*On-premise data gateway (standard mode): allow a team of users to connect, ideal for a large number of users using the reports and need to acces to the data sources.","comment_id":"779579","timestamp":"1674012840.0"},{"content":"Is this correct ?","timestamp":"1672309020.0","comment_id":"760864","poster":"AshwinN1992","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/microsoft/view/93144-exam-dp-500-topic-1-question-86-discussion/","question_text":"You have five Power BI reports that contain R script data sources and R visuals.\nYou need to publish the reports to the Power BI service and configure a daily refresh of datasets.\nWhat should you include in the solution?","choices":{"C":"an on-premises data gateway (personal mode)","B":"a workspace that connects to an Azure Data Lake Storage Gen2 account","D":"an on-premises data gateway (standard mode)","A":"a Power BI Embedded capacity"},"unix_timestamp":1672309020,"timestamp":"2022-12-29 11:17:00"},{"id":"BRatLdpg6ePxz15nUa3d","answer_ET":"CD","question_id":170,"isMC":true,"answers_community":["CD (100%)"],"choices":{"B":"Connect the target workspace to an Azure Data Lake Storage Gen2 account.","D":"Enable XMLA read-write.","A":"Enable service principal authentication for read-only admin APIs.","C":"Turn on Large dataset storage format."},"timestamp":"2023-01-11 06:42:00","url":"https://www.examtopics.com/discussions/microsoft/view/94759-exam-dp-500-topic-1-question-87-discussion/","topic":"1","exam_id":70,"question_text":"You have a 2-GB Power BI dataset.\nYou need to ensure that you can redeploy the dataset by using Tabular Editor. The solution must minimize how long it will take to apply changes to the dataset from powerbi.com.\nWhich two actions should you perform in powerbi.com? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","unix_timestamp":1673415720,"answer_description":"","question_images":[],"answer":"CD","answer_images":[],"discussion":[{"upvote_count":"1","timestamp":"1701188640.0","comment_id":"1082723","poster":"hoss29","content":"Selected Answer: CD\nEnable XMLA read-write to connect the dataset to Tabular Editor\nEnable the large dataset format to get better performance, as this is recommended even for datasets that do not qualify as large (10GB)\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools"},{"comment_id":"935230","content":"Selected Answer: CD\nC and D are correct answers.","timestamp":"1687861260.0","poster":"Eltooth","upvote_count":"3"},{"timestamp":"1676035260.0","content":"You need XMLA read-write for Tabular Editor for metadata operations: https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools\n\nAnd as pointed out below, you need to enable the Large dataset storage format when using XMLA endpoint write operations","upvote_count":"3","poster":"ThariCD","comment_id":"804343"},{"comments":[{"poster":"stfglv","comment_id":"785215","timestamp":"1674469260.0","content":"The part after \"even\" from that link removes all doubt considering that the dataset in question is only 2GB: While required for datasets to grow beyond 10 GB, enabling the Large dataset storage format setting has other benefits. If you're planning to use XMLA endpoint-based tools for dataset write operations, be sure to enable the setting, even for datasets that you wouldn't necessarily characterize as a large dataset. When enabled, the large dataset storage format can improve XMLA write operations performance.","upvote_count":"2"}],"content":"correct\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models","comment_id":"772068","upvote_count":"3","timestamp":"1673415720.0","poster":"Saffar"}]}],"exam":{"name":"DP-500","numberOfQuestions":183,"isMCOnly":false,"isBeta":false,"lastUpdated":"12 Apr 2025","id":70,"provider":"Microsoft","isImplemented":true},"currentPage":34},"__N_SSP":true}