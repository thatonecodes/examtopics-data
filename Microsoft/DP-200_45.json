{"pageProps":{"questions":[{"id":"HY9f2q1GBbMGCzPwadZf","choices":{"C":"Sliding","A":"Hopping","D":"Tumbling","B":"Session"},"unix_timestamp":1625129160,"exam_id":65,"answers_community":[],"answer_description":"You need to use the Tumbling windowing function for this requirement.\nThe Microsoft documentation mentions the following:\n\nTumbling window -\nTumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window.\n\nSince this is clearly given in the documentation, all other options are incorrect.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions","question_text":"You have to implement Azure Stream Analytics Functions as part of your data streaming solution.\nThe solution has the following requirements:\n- Segment the data stream into distinct time segments that do not repeat or overlap\n- Segment the data stream into distinct time segments that repeat and can overlap\n- Segment the data stream to produce an output when an event occurs\nWhich of the following windowing function would you use for the following requirement?\n`Segment the data stream into distinct time segments that do not repeat or overlap`","question_images":[],"isMC":true,"discussion":[{"upvote_count":"1","comment_id":"446158","poster":"Podavenna","timestamp":"1631819700.0","content":"Session window: Neither overlaps nor repeats\nTumbling window: Repeats and does not overlap\nHopping: Repeats and can overlap\nSliding: Does not repeat but overlaps"},{"upvote_count":"1","timestamp":"1625129160.0","poster":"eng1","content":"Hopping","comment_id":"395729"}],"answer_ET":"D","answer":"D","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0059900001.png"],"timestamp":"2021-07-01 10:46:00","question_id":221,"topic":"6","url":"https://www.examtopics.com/discussions/microsoft/view/56714-exam-dp-200-topic-6-question-70-discussion/"},{"id":"KBVjpqicZ0pmJDW1J0sQ","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0060500001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0060600001.jpg"],"unix_timestamp":1627393680,"url":"https://www.examtopics.com/discussions/microsoft/view/58792-exam-dp-200-topic-6-question-74-discussion/","answer_description":"The Microsoft documentation highlights the steps required to load data from Azure Data Lake Gen2 to an Azure SQL Data warehouse.\nOne of the steps is to create a database scoped credential:\n\nAnother step is to create the external data source using 'abfs' as the file location:\n\nCreate the external data source -\nUse this CREATE EXTERNAL DATA SOURCE command to store the location of the data.\n\nAnd you can use the FIRST_ROW parameter to skip the first row of the file.\n\nFIRST_ROW = First_row_int -\nSpecifies the row number that is read first in all files during a PolyBase load. This parameter can take values 1-15. If the value is set to two, the first row in every file (header row) is skipped when the data is loaded. Rows are skipped based on the existence of row terminators (/r/n, /r, /n). When this option is used for export, rows are added to the data to make sure the file can be read with no data loss. If the value is set to >2, the first row exported is the Column names of the external table.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql?view=sql-server-ver15","isMC":true,"question_images":[],"timestamp":"2021-07-27 15:48:00","answer":"ACD","question_id":222,"topic":"6","answer_ET":"ACD","discussion":[{"timestamp":"1627393680.0","upvote_count":"3","content":"correct Answer is ACE","poster":"elimey","comment_id":"415511"}],"exam_id":65,"answers_community":[],"question_text":"You have an Azure Data Lake Storage Gen2 account. You have a number of CSV files loaded in the account. Each file has a header row. After the header row is a property that is formatted by carriage return (/r) and line feed (/n).\nYou need to load the files daily as a batch into Azure SQL Data warehouse using Polybase. You have to skip the header row when the files are imported.\nWhich of the following actions would you take to implement this requirement? (Choose three.)","choices":{"C":"Create an external file format and set the First_row option","A":"Create an external data source and ensure to use the abfs location","E":"Use the CREATE EXTERNAL TABLE AS SELECT and create a view that removes the empty row","B":"Create an external data source and ensure to use the Hadoop location","D":"Create a database scoped credential that uses OAuth2 token and a key"}},{"id":"H3FfAr1mteske6Xd5sF6","answer_description":"An example of this is given in a blog post. To achieve this, we first need to copy the data onto a new table using the ג€CREATE TABLE AS SELECTג€ command.\nThen we switch the partition and then delete the staging table.\nOption ג€Create a new empty table named XYZ_salesfact_new that has the same schema as XYZ_salesfactג€ is incorrect because we also need to copy the data onto the new table.\nOption ג€Truncate the partition containing the stale dataג€ is incorrect because we need to switch the partition.\nOption ג€Execute the DELETE statement where the value in the Date column is greater than 12 monthsג€ is incorrect because issuing the DELETE statement would take time.\nReference:\nhttps://blogs.msdn.microsoft.com/apsblog/2018/06/18/azure-sql-dw-performance-ctaspartition-switching-vs-updatedelete/","topic":"6","question_id":223,"discussion":[{"timestamp":"1626702900.0","upvote_count":"8","content":"I would say A instead of C because there's no need to copy data, just 'delete' the last month by sending it to the new table wich later is gonna be dropped. \nSo for me the correct answer is A, B E.","poster":"Sumercin","comment_id":"409643"},{"content":"Provided answer BCE is correct.\n\"\"The preferred method is to utilize a methodology of CTAS and partition switching in lieu of UPDATE and DELETE operations wherever possible.\"\"\n\nhttps://docs.microsoft.com/en-gb/archive/blogs/apsblog/azure-sql-dw-performance-ctaspartition-switching-vs-updatedelete","comment_id":"424984","timestamp":"1628986260.0","upvote_count":"1","poster":"Gitty"}],"answers_community":[],"unix_timestamp":1626702900,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/58174-exam-dp-200-topic-6-question-76-discussion/","question_text":"A company has an Azure SQL Datawarehouse. They have a table named whizlab_salesfact that contains data for the past 12 months. The data is partitioned by month. The table contains around a billion rows. The table has clustered columnstore indexes. At the beginning of each month you need to remove the data from the table that is older than 12 months.\nWhich of the following actions would you implement for this requirement? (Choose three.)","exam_id":65,"choices":{"B":"Drop the XYZ_salesfact_new table","A":"Create a new empty table named XYZ_salesfact_new that has the same schema as XYZ_salesfact","D":"Truncate the partition containing the stale data","E":"Switch the partition containing the stale data from XYZ_salesfact to XYZ_salesfact_new","C":"Copy the data to the new table by using CREATE TABLE AS SELECT (CTAS)","F":"Execute the DELETE statement where the value in the Date column is greater than 12 months"},"timestamp":"2021-07-19 15:55:00","answer_ET":"BCE","isMC":true,"answer":"BCE","question_images":[]},{"id":"YxJvzCOFAo0JxQ7C8wP9","unix_timestamp":1626014340,"timestamp":"2021-07-11 16:39:00","question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nXYZ is an online training provider. They also provide a yearly gaming competition for their students. The competition is held every month in different locations.\n\nCurrent Environment -\nThe company currently has the following environment in place:\n* The racing cars for the competition send their telemetry data to a MongoDB database. The telemetry data has around 100 attributes.\n* A custom application is then used to transfer the data from the MongoDB database to a SQL Server 2017 database. The attribute names are changed when they are sent to the SQL Server database.\n* Another application named \"XYZ workflow\" is then used to perform analytics on the telemetry data to look for improvements on the racing cars.\n* The SQL Server 2017 database has a table named \"cardata\" which has around 1 TB of data. \"XYZ workflow\" performs the required analytics on the data in this table. Large aggregations are performed on a column of the table.\n\nProposed Environment -\nThe company now wants to move the environment to Azure. Below are the key requirements:\n* The racing car data will now be moved to Azure Cosmos DB and Azure SQL database. The data must be written to the closest Azure data center and must converge in the least amount of time.\n* The query performance for data in the Azure SQL database must be stable without the need of administrative overhead\n* The data for analytics will be moved to an Azure SQL Data warehouse\n* Transparent data encryption must be enabled for all data stores wherever possible\n* An Azure Data Factory pipeline will be used to move data from the Cosmos DB database to the Azure SQL database. If there is a delay of more than 15 minutes for the data transfer, then configuration changes need to be made to the pipeline workflow.\n* The telemetry data must be monitored for any sort of performance issues.\n* The Request Units for Cosmos DB must be adjusted to maintain the demand while also minimizing costs.\n* The data in the Azure SQL Server database must be protected via the following requirements:\n- Only the last four digits of the values in the column CarID must be shown\n- A zero value must be shown for all values in the column CarWeight\nWhich of the following would you use for the consistency level for the database?","exam_id":65,"answer_ET":"A","question_images":[],"choices":{"C":"Strong","D":"Consistent prefix","B":"Session","A":"Eventual"},"answers_community":[],"answer_description":"Since there is a requirement for data to be written to the closest data center for Cosmos DB, we need to ensure there is a multi-master setup for Cosmos DB wherein data can be written from multiple regions. For such accounts, we can't set the consistency level to Strong.\nThe Microsoft documentation mentions the following:\nStrong consistency and multi-master\nCosmos accounts configured for multi-master cannot be configured for strong consistency as it is not possible for a distributed system to provide an RPO of zero and an RTO of zero. Additionally, there are no write latency benefits for using strong consistency with multi-master as any write into any region must be replicated and committed to all configured regions within the account. This results in the same write latency as a single master account.\nHence if we want data to converge in the least amount of time, we need to use Eventual consistency. This offers the least latency in terms of consistency.\nThe Microsoft documentation mentions the following on the consistency levels.\nWith Azure Cosmos DB, developers can choose from five well-defined consistency models on the consistency spectrum. From strongest to more relaxed, the models include strong, bounded staleness, session, consistent prefix, and eventual consistency. The models are well-defined and intuitive and can be used for specific real-world scenarios. Each model provides availability and performance tradeoffs and is backed by the SLAs. The following image shows the different consistency levels as a spectrum.\n\nBecause of the proposed logic to the consistency level, all other options are incorrect.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels-tradeoffs https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels","topic":"6","discussion":[{"timestamp":"1626814560.0","content":"As you cannot use Strong due to the multi-master model, the next higher availability in the answers is Session","poster":"Sumercin","upvote_count":"2","comment_id":"410540"},{"upvote_count":"2","timestamp":"1626014340.0","comment_id":"404053","poster":"Avinash75","content":"In my view the answer should be \"Session\" as data must converge in the least amount of time"}],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/57636-exam-dp-200-topic-6-question-90-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0062700001.png"],"answer":"A","question_id":224},{"id":"yucgNxqJ7GlHz6sMvmJT","question_id":225,"answers_community":[],"unix_timestamp":1629697440,"question_text":"A development team is making use of Azure Stream Analytics. They have defined the following SQL query:\nWITH step1 AS (SELECT * FROM input1 PARTITION BY XYZID INTO 10), step2 AS (SELECT * FROM input2 PARTITION BY XYZID INTO 10)\nSELECT * INTO output FROM step1 PARTITION BY XYZID UNION step2 PARTITION BY XYZID\nDoes the query represent the joining of two streams of repartitioned data?","answer_ET":"A","discussion":[{"content":"This is a union of 2 streams, not a join.","poster":"TonReurts","timestamp":"1629697440.0","upvote_count":"1","comment_id":"429687"}],"answer_description":"Yes, this query does represent the joining of two streams of repartitioned data.\nAn example of this is given in the Microsoft documentation:\nThe following example query joins two streams of repartitioned data. When joining two streams of repartitioned data, the streams must have the same partition key and count. The outcome is a stream that has the same partition scheme.\n\nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently. The stream could also be merged and repartitioned again by a different scheme before flushing, but you should avoid that method because it adds to the general latency of the processing and increases resource utilization.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/repartition","question_images":[],"isMC":true,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0064000001.jpg"],"url":"https://www.examtopics.com/discussions/microsoft/view/60326-exam-dp-200-topic-6-question-98-discussion/","topic":"6","choices":{"B":"No","A":"Yes"},"exam_id":65,"timestamp":"2021-08-23 07:44:00","answer":"A"}],"exam":{"provider":"Microsoft","numberOfQuestions":228,"isImplemented":true,"id":65,"isMCOnly":false,"name":"DP-200","isBeta":false,"lastUpdated":"12 Apr 2025"},"currentPage":45},"__N_SSP":true}