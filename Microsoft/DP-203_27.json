{"pageProps":{"questions":[{"id":"TQrC8dFuASBjUM7ABMTQ","answer_description":"","question_id":131,"answer_ET":"D","answer_images":[],"choices":{"A":"a resource tag","C":"a run group ID","B":"a correlation ID","D":"an annotation"},"timestamp":"2021-09-12 08:27:00","unix_timestamp":1631428020,"exam_id":67,"topic":"2","isMC":true,"discussion":[{"content":"Annotation","upvote_count":"22","comments":[{"content":"Cause ADF pipelines are not first class resources","poster":"anto69","timestamp":"1673666100.0","upvote_count":"3","comment_id":"523297"}],"poster":"umeshkd05","timestamp":"1662964020.0","comment_id":"443311"},{"content":"What is the difference between resource tags and annotations?","timestamp":"1678987620.0","poster":"AhmedDaffaie","comment_id":"569214","upvote_count":"13"},{"poster":"kkk5566","upvote_count":"1","timestamp":"1725529560.0","comment_id":"999322","content":"Selected Answer: D\nD -Annotation"},{"content":"Selected Answer: D\nOPTION -D","poster":"akhil5432","upvote_count":"1","comment_id":"974715","timestamp":"1723035420.0"},{"poster":"Ankit_Az","upvote_count":"1","content":"Selected Answer: D\nCorrect","timestamp":"1716810000.0","comment_id":"907941"},{"timestamp":"1716604860.0","poster":"joponlu","content":"Selected Answer: D\nD is correct!!","upvote_count":"1","comment_id":"906266"},{"comment_id":"831644","content":"To label each pipeline with its main purpose of either ingest, transform, or load and make the labels available for grouping and filtering when using the monitoring experience in Data Factory, you should add an annotation to each pipeline.\n\nTherefore, the correct answer is D. an annotation.\n\nAnnotations are key-value pairs that you can add to pipelines, datasets, and activities to help you organize and categorize them. They can be used for a variety of purposes, including labeling pipelines with their main purpose of either ingest, transform, or load. Annotations can also be used for filtering, grouping, and searching for resources in the Data Factory monitoring experience.","timestamp":"1709797920.0","upvote_count":"3","poster":"esaade"},{"timestamp":"1705921620.0","content":"D -Annotation","comment_id":"784210","poster":"DindaS","upvote_count":"3"},{"content":"Selected Answer: D\nADF annotations are tags that you can add to your Azure Data Factory components to identify them.\n\nA tag allows you to classify or group different objects in order to easily monitor them after an execution. You can create multiple Azure Data Factory annotations.","upvote_count":"5","timestamp":"1702825740.0","comment_id":"748145","poster":"vigilante89"},{"timestamp":"1696667100.0","content":"hjghfgh","upvote_count":"2","poster":"arunesh789","comments":[{"comment_id":"688460","upvote_count":"4","timestamp":"1696667160.0","content":"KIndly delete above comment. Answer is D.","poster":"arunesh789"}],"comment_id":"688458"},{"upvote_count":"1","comment_id":"639752","timestamp":"1690735620.0","poster":"Deeksha1234","content":"Selected Answer: D\ncorrect"},{"timestamp":"1676878920.0","content":"Correct!","comment_id":"551630","poster":"paras_gadhiya","upvote_count":"1"},{"upvote_count":"1","comment_id":"535467","timestamp":"1674997320.0","content":"Selected Answer: D\ncorrect","poster":"PallaviPatel"},{"upvote_count":"1","content":"Selected Answer: D\nAnotacion","poster":"huesazo","comment_id":"525905","timestamp":"1673969820.0"},{"poster":"aarthy2","upvote_count":"2","comment_id":"458330","content":"yes correct, annotation provides label functionality than show in pipeline monitoring.","timestamp":"1665070020.0"}],"question_images":[],"answer":"D","answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/61879-exam-dp-203-topic-2-question-11-discussion/","question_text":"You have an Azure Data Factory that contains 10 pipelines.\nYou need to label each pipeline with its main purpose of either ingest, transform, or load. The labels must be available for grouping and filtering when using the monitoring experience in Data Factory.\nWhat should you add to each pipeline?"},{"id":"kolxVB45labvbxBIlE7v","url":"https://www.examtopics.com/discussions/microsoft/view/112077-exam-dp-203-topic-2-question-110-discussion/","answer_description":"","answer_ET":"","question_id":132,"discussion":[{"upvote_count":"22","comment_id":"922513","content":"According to Microsoft, AutoResolveIntegrationRuntime will attempt to use the sink location to get an IR in the same region (or the closest available) to execute the Copy activity, not the source location. I would go with the region of data factory, since that is the default option when the sink's location is not detectable. Source: https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-ir-location","timestamp":"1686688500.0","comments":[{"poster":"matiandal","timestamp":"1698918000.0","content":"\"the sink's location is not detectable\" is any wording in the Q that confirms ?\n\nIf not, no confirmation about the undetectable sink source, the correct answer is \nthe selected from ET ( \"in the region of the source database\" ).\n\nr: https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-ir-location - 1st bullet","upvote_count":"1","comment_id":"1060442"}],"poster":"peches"},{"poster":"EliteAllen","timestamp":"1694418120.0","comment_id":"1004529","upvote_count":"13","content":"1. upon publishing changes to the service\n2. in the region of data factory"},{"timestamp":"1707757920.0","content":"- Upon publishing the changes.\n- In the region of the source database.","upvote_count":"2","comment_id":"1148425","poster":"Azure_2023"},{"upvote_count":"5","timestamp":"1706746800.0","poster":"be8a152","content":"1. upon saving the changes ( refer to the warning message in the screenshot given in the question )\n2. In the region of data factory. ( https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-ir-location) \n\nAzure IR location\nYou can set the location region of an Azure IR, in which case the activity execution or dispatch will happen in the selected region.\n\nThe default is to auto-resolve the Azure IR in the public network. With this option:\n\nFor copy activity, a best effort is made to automatically detect your sink data store's location, then use the IR in either the same region, if available, or the closest one in the same geography, otherwise; if the sink data store's region is not detectable, the IR in the instance's region is used instead.","comment_id":"1137162"},{"content":"Once you make a changes it needs to make a pull request then merge the changes in the main pipeline then you will be able to see the changes in this case merging into collaboration branch is the right answer","upvote_count":"1","comment_id":"1118540","poster":"dakku987","comments":[{"poster":"dakku987","comment_id":"1118545","timestamp":"1704889980.0","upvote_count":"1","content":"C. in the region of the source database\n\nExplanation:\n\nWhen a copy activity uses a linked service as the source in Azure Data Factory, the data movement will be performed in the region of the source database. The linked service contains the connection information needed to connect to the source data store (such as Azure Blob Storage, Azure SQL Database, etc.). Therefore, the location where the data movement is initiated corresponds to the region where the source data store is located."}],"timestamp":"1704889800.0"},{"upvote_count":"3","poster":"msb","comment_id":"1013615","content":"For Linked Services, changes are published immediately unless you use key vault, which means basically upon saving. \n\n\"Changes to Linked Services are published immediately, unless you use Key Vault. This can mean a branch change to a Linked Service could impact other branch tests. The reason for this is credential protection. While \"Live-Mode\" retrieves definitions from the back-end, \"Github\" or \"Devops\" mode construct the definitions from the repository. Putting credentials into repository code is a very bad idea. This is why, without Key Vault, credentials are stored and encrypted in Data Factory back end.\"\nhttps://learn.microsoft.com/en-us/answers/questions/568057/advanced-feature-branch-development","comments":[{"content":"No. Changes are published immediately only from the collaboration branch. But the question says that we are working in a feature branch. \nThe answer should be \"when the changes are merged into the collaboration branch\"","poster":"Legato","comment_id":"1273334","upvote_count":"1","timestamp":"1724751240.0"}],"timestamp":"1695359640.0"},{"upvote_count":"2","poster":"kkk5566","comment_id":"993893","timestamp":"1693387320.0","content":"upon publishing changes to the service"},{"timestamp":"1690101780.0","comment_id":"960253","content":"the first one should be \"upon publishing changes to the service\" . See https://learn.microsoft.com/en-us/azure/data-factory/source-control","upvote_count":"8","poster":"Galvanir"},{"comment_id":"947036","poster":"abrakadabra200","timestamp":"1688893140.0","content":"Shouldn't we choose the 'upon saving the changes' option in the first dropdown?\nLink: https://learn.microsoft.com/en-us/azure/data-factory/source-control#stale-publish-branch","upvote_count":"5"},{"content":"I did not manage to find a clear answer to this one, but based on cross-reading a few articles, I think \"in the region of data factory\" should be the correct answer, and this article explains it a bit better than the others I found: https://asankap.wordpress.com/2021/10/26/why-you-shouldnt-use-auto-resolve-integration-runtime-in-azure-data-factory-or-synapse/","upvote_count":"4","timestamp":"1687338300.0","poster":"andjurovicela","comment_id":"929248"},{"poster":"JG1984","timestamp":"1687240980.0","upvote_count":"1","content":"When using the AutoResolveIntegrationRuntime with a Copy activity in Azure Data Factory that uses a linked service as the source, the copy operation will be performed in the region of the source data store.\n\nThe AutoResolveIntegrationRuntime is a system-assigned integration runtime that automatically routes data movement and activity dispatch to the optimal region based on the location of the source and sink data stores. When using a linked service as the source, the service will attempt to detect the location of the source data store and use an Integration Runtime in the same region to perform the copy operation.","comment_id":"928192"},{"comment_id":"927899","content":"For copy activity, a best effort is made to automatically detect your sink data store's location, then use the IR in either the same region, if available, or the closest one in the same geography, otherwise; if the sink data store's region is not detectable, the IR in the instance's region is used instead.","timestamp":"1687206720.0","poster":"vctrhugo","upvote_count":"1"},{"upvote_count":"1","timestamp":"1686702000.0","content":"correct","poster":"mehroosali","comment_id":"922628"}],"exam_id":67,"topic":"2","answers_community":[],"question_text":"HOTSPOT\n-\n\nYou have an Azure data factory that contains the linked service shown in the following exhibit.\n\n//IMG//\n\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\n\nNOTE: Each correct answer is worth one point.\n\n//IMG//","isMC":false,"unix_timestamp":1686688500,"answer_images":["https://img.examtopics.com/dp-203/image320.png"],"timestamp":"2023-06-13 22:35:00","answer":"","question_images":["https://img.examtopics.com/dp-203/image318.png","https://img.examtopics.com/dp-203/image319.png"]},{"id":"UmddxCnqDNH3GdFlkK98","topic":"2","isMC":false,"answer_ET":"","unix_timestamp":1686685080,"timestamp":"2023-06-13 21:38:00","url":"https://www.examtopics.com/discussions/microsoft/view/112073-exam-dp-203-topic-2-question-111-discussion/","discussion":[{"poster":"ludaka","timestamp":"1687190640.0","upvote_count":"24","comment_id":"927668","content":"1. two times\n2. will automatically adjust \n\"For time zones that observe daylight saving, trigger time will auto-adjust for the twice a year change, if the recurrence is set to Days or above. To opt out of the daylight saving change, please select a time zone that does not observe daylight saving, for instance UTC.\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory#azure-data-factory-and-synapse-portal-experience","comments":[{"comment_id":"1082392","poster":"mrplmcc","upvote_count":"5","content":"Why is it two times instead of one time, if it will automatically adjust?","timestamp":"1701165900.0","comments":[{"timestamp":"1706801760.0","poster":"Gikan","comment_id":"1137740","upvote_count":"2","content":"at 3:00 and at 21:00"},{"timestamp":"1706802120.0","upvote_count":"3","poster":"Gikan","comment_id":"1137744","content":"Just for fun in Hungary in 2024. Oct 27. Sunday at 3:00 you should change the clock to 2:00. It means 2 times there is 3:00 next to each other. :D"}]}]},{"comment_id":"1357133","content":"1. Two times (3am and 9pm)\n2. Auto adjusts - as Microsoft documentation says for ADF \"For time zones that observe daylight saving, trigger time auto-adjusts for the twice-a-year change, if the recurrence is set to Days or above. \" https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory\"","timestamp":"1739678220.0","poster":"aeab260","upvote_count":"1"},{"comment_id":"1280691","content":"The trigger will execute [answer choice] on Sunday, March 3, 2024:\ntwo times\nThe trigger [answer choice] daylight saving time:\nwill automatically adjust for","poster":"a85becd","timestamp":"1725850140.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1150119","poster":"mghf61","content":"bing copilot:\n\nThe trigger is set to execute at 3 AM and 9 PM Pacific Time on Sundays. However, on March 3, 2024, daylight saving time begins in the Pacific Time zone, moving the clock forward by one hour. So, the trigger will execute only one time on Sunday, March 3, 2024.\nThe trigger operates in the Pacific Standard Time zone, which observes daylight saving time. However, the JSON file does not specify any adjustments for daylight saving time. Therefore, the trigger is unaffected by daylight saving time.","timestamp":"1707908820.0"},{"upvote_count":"1","comment_id":"1148440","content":"Two times.\nWill automatically adjust. \n\nor time zones that observe daylight saving, trigger time will auto-adjust for the twice a year change, if the recurrence is set to Days or above. To opt out of the daylight saving change, please select a time zone that does not observe daylight saving, for instance UTC\nDaylight saving adjustment only happens for trigger with recurrence set to Days or above. If the trigger is set to Hours or Minutes frequency, it will continue to fire at regular intervals.","timestamp":"1707759600.0","poster":"Azure_2023"},{"upvote_count":"1","content":"1. two times\n2. will automatically adjust","poster":"be8a152","comment_id":"1137164","timestamp":"1706747100.0"},{"poster":"JezWalters","comment_id":"972262","content":"There's a catch here, as daylight savings actually starts on the SECOND Sunday of March, and March 3 2024 is before this date.","timestamp":"1691161320.0","upvote_count":"3"},{"timestamp":"1687207140.0","poster":"vctrhugo","comment_id":"927907","content":"\"[...] we are also adding support for Daylight Saving auto-adjustment: for time zones that observe Daylight Saving, auto change schedule trigger time twice a year (e.g. 8AM daily trigger will fire at 8AM, whether it's PST or PDT)\"\n\nhttps://techcommunity.microsoft.com/t5/azure-data-factory-blog/time-zone-and-daylight-saving-support-for-schedule-trigger/ba-p/1840199","upvote_count":"4"},{"content":"2nd answer should be : will require an adjustment for\n\nref to : https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory\nThe timeZone element specifies the time zone that the trigger is created in. This setting affects both startTime and endTime.","upvote_count":"2","poster":"iVath","comment_id":"925792","timestamp":"1686981960.0"},{"upvote_count":"4","timestamp":"1686689580.0","content":"Agree, as of 2020 ADF supports auto-adjustemt for Daylight Saving in Schedule Triggers for time zones that aren't UTC. Since here we are using Pacific time, answer seems correct. Source: https://techcommunity.microsoft.com/t5/azure-data-factory-blog/time-zone-and-daylight-saving-support-for-schedule-trigger/ba-p/1840199","comment_id":"922524","poster":"peches"},{"timestamp":"1686685080.0","poster":"wendyy","content":"Azure Data Factory only supports time zones UTC. I think should requiret the adjustment.","upvote_count":"2","comments":[{"poster":"vctrhugo","timestamp":"1687207200.0","upvote_count":"2","content":"Incorrect. As of 2020 you can create schedule triggers in your local time zone, without the need to convert timestamps to Coordinated Universal Time (UTC) first.","comment_id":"927908"}],"comment_id":"922487"}],"question_id":133,"answer_images":["https://img.examtopics.com/dp-203/image323.png"],"question_text":"HOTSPOT\n-\n\nIn Azure Data Factory, you have a schedule trigger that is scheduled in Pacific Time.\n\nPacific Time observes daylight saving time.\n\nThe trigger has the following JSON file.\n\n//IMG//\n\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer":"","answers_community":[],"question_images":["https://img.examtopics.com/dp-203/image321.png","https://img.examtopics.com/dp-203/image322.png"],"exam_id":67,"answer_description":""},{"id":"p8tdcfKVi3gWrTDUFgm1","unix_timestamp":1686440460,"discussion":[{"upvote_count":"14","comments":[{"timestamp":"1687339440.0","content":"I also think this one is correct. One of the things script activity can do is \"...Save the rowset returned from a query as activity output for downstream consumption.\" which is pretty much what is needed here. This is not viable with 'execute SP' activity as it doesn't cannot return any data.","poster":"andjurovicela","upvote_count":"2","comment_id":"929271"}],"comment_id":"927677","timestamp":"1687191540.0","content":"Selected Answer: C\nFor me the correct answer is C.\nThe store procedure activity doesn't return any data. \nIn the description of the script activity is written that it can be used for : \"Run stored procedures. If the SQL statement invokes a stored procedure that returns results from a temporary table, use the WITH RESULT SETS option to define metadata for the result set. \"\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script","poster":"ludaka"},{"comment_id":"937598","poster":"auwia","content":"Selected Answer: C\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script\n\nThe script may contain either a single SQL statement or multiple SQL statements that run sequentially. You can use the Script task for the following purposes:\n\nTruncate a table in preparation for inserting data.\nCreate, alter, and drop database objects such as tables and views.\nRe-create fact and dimension tables before loading data into them.\nRun stored procedures. If the SQL statement invokes a stored procedure that returns results from a temporary table, use the WITH RESULT SETS option to define metadata for the result set.\nSave the rowset returned from a query as activity output for downstream consumption.","timestamp":"1688018820.0","upvote_count":"7"},{"content":"Selected Answer: C\nWhen the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet.\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure","poster":"seranvijay","timestamp":"1731295440.0","upvote_count":"2","comment_id":"1309808"},{"comment_id":"1256942","content":"I will go with Stored Procedure.. The requirement is - The solution must minimize development effort. If we go with Script, we will have to write a script which involve development efforts.","poster":"RG_123","timestamp":"1722187680.0","upvote_count":"2"},{"timestamp":"1721209320.0","comment_id":"1249508","upvote_count":"2","poster":"Danweo","content":"Selected Answer: C\n\"When the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet. \"\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"},{"timestamp":"1721025540.0","poster":"evangelist","content":"Selected Answer: B\nC is wrong","upvote_count":"4","comment_id":"1248134"},{"content":"Answer is script 100%","comment_id":"1228895","timestamp":"1718179860.0","upvote_count":"1","poster":"MT007"},{"timestamp":"1712390040.0","upvote_count":"4","content":"B. Stored Procedure\n\nExplanation:\nTo execute a stored procedure in an Azure Synapse Analytics dedicated SQL pool and use the returned result set as input for a downstream activity, you should use the Stored Procedure activity in the pipeline.\n\nThe Stored Procedure activity allows you to execute a stored procedure within the dedicated SQL pool and retrieve the results, which can then be used as input for subsequent activities in the pipeline. This approach minimizes development effort as it directly integrates with the SQL pool and provides a seamless way to execute stored procedures as part of your data processing","poster":"Elanche","comment_id":"1190280"},{"comment_id":"1153098","timestamp":"1708243140.0","poster":"j888","upvote_count":"2","content":"Got the feeling the question itself is referring to this GUI: ttps://stackoverflow.com/questions/72642013/pipeline-for-stored-procedure-dedicated-sql-pool\n\nSo the answer may be A as store procedure"},{"timestamp":"1706747460.0","comment_id":"1137167","upvote_count":"1","poster":"be8a152","content":"Ans is C. Script"},{"timestamp":"1704979560.0","poster":"dakku987","content":"I got this question today in exam 11-jan-2024","comment_id":"1119807","upvote_count":"1","comments":[{"comment_id":"1169514","poster":"Delphin_8150","timestamp":"1709991300.0","upvote_count":"1","content":"What did you put for your answer?"}]},{"poster":"jongert","upvote_count":"1","content":"Selected Answer: C\nThe key is 'Output Result set support' which the stored procedure activity does not have. Therefore we have to use a script which supports running stored procedures.\n\nhttps://techcommunity.microsoft.com/t5/azure-data-factory-blog/execute-sql-statements-using-the-new-script-activity-in-azure/ba-p/3239969","comment_id":"1111787","timestamp":"1704192480.0"},{"poster":"kkk5566","content":"Selected Answer: C\nC is corret","comment_id":"993901","timestamp":"1693387860.0","upvote_count":"1"},{"upvote_count":"2","poster":"CoinUmbrella","comment_id":"942899","timestamp":"1688483160.0","content":"Selected Answer: B\nB. Chat GPT says the given answer is correct. Stored Procedure is specifically designed to execute stored procedures within Azure Synapse Analytics and is the most suitable option for the scenario, minimizing development effort."},{"comment_id":"931637","content":"Selected Answer: B\nThe \"Script\" activity in Azure Data Factory is primarily used to run HDInsight scripts such as Hive, Pig, MapReduce, and Spark. These are typically used for big data processing tasks.","upvote_count":"1","poster":"auwia","comments":[{"upvote_count":"5","poster":"auwia","comment_id":"937597","content":"False, finally I've found the link, it's C:\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script","timestamp":"1688018760.0"}],"timestamp":"1687526820.0"},{"timestamp":"1687207320.0","poster":"vctrhugo","comment_id":"927910","content":"Selected Answer: B\n\"In Azure Synapse Analytics, you can use the SQL pool Stored Procedure Activity to invoke a stored procedure in a dedicated SQL pool.\"\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/sql-pool-stored-procedure-activity","upvote_count":"2"},{"comment_id":"920320","upvote_count":"2","timestamp":"1686440460.0","poster":"[Removed]","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/sql-pool-stored-procedure-activity"}],"answer_ET":"C","choices":{"B":"Stored Procedure","C":"Script","D":"Notebook","A":"U-SQL"},"topic":"2","answer_images":[],"isMC":true,"exam_id":67,"question_images":[],"answer":"C","url":"https://www.examtopics.com/discussions/microsoft/view/111857-exam-dp-203-topic-2-question-112-discussion/","question_id":134,"question_text":"You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to create a pipeline that will execute a stored procedure in the dedicated SQL pool and use the returned result set as the input for a downstream activity. The solution must minimize development effort.\n\nWhich type of activity should you use in the pipeline?","answers_community":["C (71%)","B (29%)"],"answer_description":"","timestamp":"2023-06-11 01:41:00"},{"id":"nVuWahgfunOlOnj6gpKx","unix_timestamp":1686853260,"answer_description":"","answer":"AC","answers_community":["AC (65%)","AD (20%)","Other"],"question_text":"You have an Azure SQL database named DB1 and an Azure Data Factory data pipeline named pipeline1.\n\nFrom Data Factory, you configure a linked service to DB1.\n\nIn DB1, you create a stored procedure named SP1. SP1 returns a single row of data that has four columns.\n\nYou need to add an activity to pipeline1 to execute SP1. The solution must ensure that the values in the columns are stored as pipeline variables.\n\nWhich two types of activities can you use to execute SP1? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.","choices":{"D":"Stored Procedure","C":"Lookup","B":"Copy","A":"Script"},"topic":"2","answer_ET":"AC","timestamp":"2023-06-15 20:21:00","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/112294-exam-dp-203-topic-2-question-113-discussion/","discussion":[{"content":"Selected Answer: AC\nhttps://learn.microsoft.com/en-us/answers/questions/925742/how-to-process-output-from-stored-procedure-in-azu\n\nSP Activity does not capture result.. use lookup instead of script","timestamp":"1688534700.0","upvote_count":"8","poster":"[Removed]","comment_id":"943325"},{"timestamp":"1690565220.0","comments":[{"content":"Tested on ADF\nD: Stored Procedure activity is not correct. This activity cannot return rows as an output and therefor it cannot be used to store values in columns as pipeline variables. Script and Lookup activities can output results of stored procedure. A & C are correct answers","timestamp":"1730968320.0","comment_id":"1308291","poster":"tmz1","upvote_count":"1"}],"content":"why not CD? \nA Lookup activity can be used to execute a query or stored procedure against a data source and retrieve a single row of data. The returned values can then be stored as pipeline variables and used in subsequent activities.\n\nA Stored Procedure activity can be used to directly execute a stored procedure against a data source. The returned values can be captured as output parameters and stored as pipeline variables for use in subsequent activities.","upvote_count":"6","comment_id":"965717","poster":"andie123"},{"content":"Selected Answer: C\nWhen the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet.\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure","upvote_count":"1","comment_id":"1309809","timestamp":"1731295500.0","poster":"seranvijay"},{"timestamp":"1724520180.0","poster":"GSE_aQ","upvote_count":"1","comment_id":"1271766","content":"correct answers are A and C\n\n\"When the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet.\n\nIf you call a stored procedure with output parameters using stored procedure activity, following error occurs.\n\nExecution fail against sql server. Please contact SQL Server team if you need further support. Sql error number: 201. Error Message: Procedure or function 'sp_name' expects parameter '@output_param_name', which was not supplied.\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"},{"poster":"swathi_rs","comment_id":"1202653","timestamp":"1714140960.0","content":"Selected Answer: CD\ncorrect answer is C and D","upvote_count":"2"},{"comment_id":"1202309","timestamp":"1714091340.0","poster":"Dusica","content":"AC\ncannot capture SP output params with sP activity","upvote_count":"1"},{"content":"Selected Answer: AC\nWhen the stored procedure has Output parameters, instead of using stored procedure activity, use lookup activity and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script","comment_id":"1148456","timestamp":"1707760500.0","poster":"Azure_2023","upvote_count":"3"},{"upvote_count":"1","timestamp":"1706748420.0","poster":"be8a152","comment_id":"1137170","content":"A. Script & \nC. Lookup \nare the correct answers\nIncorrect\nD.Stored procedure activity doesn't return any values. \nB.Copy Activity is meant for copying data only"},{"poster":"[Removed]","content":"Selected Answer: AC\nScript lookup","timestamp":"1694217960.0","comment_id":"1002773","upvote_count":"1"},{"comment_id":"997783","timestamp":"1693757880.0","poster":"Mal2002","upvote_count":"1","content":"Here is an example of how we can use the Script activity to execute SP1:\n\nScript activity (name: \"ExecuteSP1Script\")\n{\n ScriptSource = \"<![CDATA[\n var results = SqlCommand('EXEC SP1', connection);\n var myVar = results[0];\n ]]>\"\n}\nIn this example, the ScriptSource property specifies the script that is used to execute SP1. The script first executes the SQL statement EXEC SP1. The script then stores the results of SP1 in the variable myVar.\n\nCorrect Answers are: A & D"},{"timestamp":"1693388760.0","poster":"kkk5566","comment_id":"993913","upvote_count":"1","content":"Selected Answer: AC\nis correct"},{"timestamp":"1688534640.0","poster":"[Removed]","comment_id":"943323","upvote_count":"1","content":"sorry, Answer - AC"},{"comment_id":"943318","content":"Answer is wrong - https://learn.microsoft.com/en-us/answers/questions/925742/how-to-process-output-from-stored-procedure-in-azu\n\nAnswer is CD. SP act cannot emit result..","poster":"[Removed]","timestamp":"1688534580.0","upvote_count":"3"},{"poster":"Mani_V","timestamp":"1687755540.0","content":"CD is the rite answer","upvote_count":"2","comment_id":"934044"},{"poster":"vctrhugo","upvote_count":"4","comment_id":"927919","content":"Selected Answer: AD\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script","timestamp":"1687207620.0"},{"comments":[{"upvote_count":"4","timestamp":"1687207560.0","comment_id":"927917","content":"There is Script activity in ADF.\n\n\"The script may contain either a single SQL statement or multiple SQL statements that run sequentially. You can use the Script task for the following purposes:\n[...]\nRun stored procedures. [...]\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script","poster":"vctrhugo"}],"comment_id":"924876","content":"C. Lookup\nD. Stored Procedure\n\nExplanation:\n\nC. Lookup: The Lookup activity is used to retrieve a dataset from a data source and the output can be used in subsequent activities. It is often used to fetch a small amount of data to be used as parameters in other activities. In this case, it can be used to execute the stored procedure and capture the result into pipeline variables.\n\nD. Stored Procedure: The Stored Procedure activity is used specifically to execute stored procedures. You can capture the output of the stored procedure and assign it to pipeline variables. This activity is designed specifically for executing stored procedures, making it a direct option for this requirement.\n\nA. Script: There is no \"Script\" activity in Azure Data Factory.\n\nB. Copy: The Copy activity is primarily used for copying data from a source to a destination and is not suitable for executing a stored procedure and capturing its output into pipeline variables.","upvote_count":"3","timestamp":"1686893880.0","poster":"WayOps"},{"upvote_count":"1","poster":"mehroosali","timestamp":"1686853260.0","comment_id":"924480","content":"I think the correct answer is C and D.","comments":[{"poster":"vctrhugo","timestamp":"1687207680.0","comment_id":"927921","upvote_count":"1","content":"You use lookup to consume, not to get."}]}],"exam_id":67,"question_id":135,"question_images":[]}],"exam":{"id":67,"lastUpdated":"12 Apr 2025","isMCOnly":false,"isBeta":false,"name":"DP-203","isImplemented":true,"numberOfQuestions":384,"provider":"Microsoft"},"currentPage":27},"__N_SSP":true}