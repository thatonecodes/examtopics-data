{"pageProps":{"questions":[{"id":"97q4a6txEQynhNmBPf7j","timestamp":"2021-09-14 17:05:00","question_id":226,"url":"https://www.examtopics.com/discussions/microsoft/view/62041-exam-dp-203-topic-2-question-7-discussion/","discussion":[{"comment_id":"445563","content":"Is this an ad?","timestamp":"1631755920.0","poster":"datachamp","comments":[{"comment_id":"1349983","content":"MS is a MKTcompany, tech is just an excuse","poster":"Fagner85","timestamp":"1738427700.0","upvote_count":"1"},{"content":"You mean like all the Azure certification exams?","poster":"phydev","timestamp":"1698668340.0","comment_id":"1057642","upvote_count":"8"},{"timestamp":"1688119380.0","content":"sounds like one, replace databricks with Azure Fabric and then it will be for sure","upvote_count":"5","poster":"Bhabani83","comment_id":"938988"}],"upvote_count":"78"},{"comment_id":"444641","timestamp":"1631631900.0","content":"Correct solution!","upvote_count":"49","poster":"Podavenna"},{"content":"\"I don't believe the storage solution is Azure Data Lake. Given that the question mentions PaaS, the Azure Data Encryption at Rest doc from Microsoft states: 'Platform as a Service (PaaS) customer's data typically resides in a storage service such as Blob Storage but may also be cached or stored in the application execution environment, such as a virtual machine.' To see the encryption at rest options available to you, examine the Data encryption models: supporting services table for the storage and application platforms that you use. So I believe the options are: Data Factory, Blob Storage, Databricks, and Synapse.\"\n\nhttps://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest#encryption-at-rest-for-paas-customers","poster":"renan_ineu","comment_id":"1273911","timestamp":"1724832240.0","upvote_count":"1"},{"comment_id":"1198143","content":"All correct, congrats!","upvote_count":"1","poster":"Alongi","timestamp":"1713469920.0"},{"comment_id":"1102990","upvote_count":"6","content":"Got this question today on the exam","timestamp":"1703194140.0","poster":"positivitypeople"},{"content":"Correct","timestamp":"1693906980.0","upvote_count":"1","comment_id":"999315","poster":"kkk5566"},{"timestamp":"1662430020.0","upvote_count":"5","content":"Given answer is correct !","poster":"anks84","comment_id":"660704"},{"content":"Correct","timestamp":"1659172080.0","poster":"Deeksha1234","upvote_count":"2","comment_id":"639614"},{"content":"Correct","comment_id":"573198","poster":"SebK","timestamp":"1647978480.0","upvote_count":"1"},{"comment_id":"568513","timestamp":"1647360840.0","comments":[{"timestamp":"1649659860.0","comment_id":"584079","poster":"NewTuanAnh","upvote_count":"4","content":"Because ADLS Gen2 support Big Data Workload better"},{"content":"ADLSG2 supports big data and large data storage as compared with blob","timestamp":"1704763020.0","poster":"kreative_feco","comment_id":"1117138","upvote_count":"1"}],"poster":"Massy","upvote_count":"3","content":"for the store, couldn't we use also Azure Blob Storage? It supports all the three requisites"},{"timestamp":"1645342560.0","upvote_count":"1","poster":"paras_gadhiya","comment_id":"551626","content":"Correct"},{"poster":"PallaviPatel","timestamp":"1643455380.0","comment_id":"535417","content":"Correct solution.","upvote_count":"1"},{"poster":"joeljohnrm","content":"Correct Solution","comment_id":"519885","upvote_count":"1","timestamp":"1641699600.0"},{"comments":[{"poster":"rockyc05","upvote_count":"3","content":"Support for SQL","timestamp":"1645462560.0","comment_id":"553030"},{"upvote_count":"3","comment_id":"553031","poster":"rockyc05","timestamp":"1645462680.0","content":"Also seamless integration with AAD"},{"poster":"dara_44_6880","content":"JAVA only supported in databricks","comment_id":"672216","upvote_count":"3","timestamp":"1663496280.0"}],"poster":"[Removed]","content":"for model and server, HDI has all of this. Why DataBricks?","comment_id":"514988","upvote_count":"1","timestamp":"1641129660.0"},{"poster":"corebit","timestamp":"1639765440.0","content":"Would be best if people including answers that go against the popular responses provide some reference instead of blinding saying false","comment_id":"503854","upvote_count":"5"},{"timestamp":"1636615740.0","upvote_count":"2","poster":"Akash0105","comment_id":"476041","content":"Answer is correct.\nAzure Databricks supports java: https://azure.microsoft.com/en-us/services/databricks/#overview"},{"upvote_count":"5","timestamp":"1635846480.0","comment_id":"471584","content":"Databricks doesn't support Java so in the Prep and Train should be HDInsight Apache Spark Cluster","comments":[{"poster":"KOSTA007","timestamp":"1636618380.0","comment_id":"476065","content":"Azure Databricks supports Python, Scala, R, Java, and SQL, as well as data science frameworks and libraries including TensorFlow, PyTorch, and scikit-learn.","upvote_count":"11"}],"poster":"Pratikh"},{"comments":[{"timestamp":"1639245660.0","comment_id":"499587","upvote_count":"5","poster":"Aslam208","content":"I would like to correct my answer here... java is supported in Azure Databricks, therefore Prepare and Train can be done with Azure Databricks"}],"comment_id":"471033","content":"Databricks does not support Java, Prepare and Train should be Azure HDInsight Apache spark cluster","timestamp":"1635741300.0","upvote_count":"1","poster":"Aslam208"},{"comment_id":"465267","content":"false. kafka hd insight is the correct option in the last box","poster":"Samanda","timestamp":"1634747280.0","upvote_count":"2"},{"content":"Correct!","timestamp":"1632126240.0","upvote_count":"10","comment_id":"448066","poster":"irantov"}],"unix_timestamp":1631631900,"answer_description":"Ingest: Azure Data Factory -\nAzure Data Factory pipelines can execute SSIS packages.\nIn Azure, the following services and tools will meet the core requirements for pipeline orchestration, control flow, and data movement: Azure Data Factory, Oozie on HDInsight, and SQL Server Integration Services (SSIS).\n\nStore: Data Lake Storage -\nData Lake Storage Gen1 provides unlimited storage.\nNote: Data at rest includes information that resides in persistent storage on physical media, in any digital format. Microsoft Azure offers a variety of data storage solutions to meet different needs, including file, disk, blob, and table storage. Microsoft also provides encryption to protect Azure SQL Database, Azure Cosmos\nDB, and Azure Data Lake.\nPrepare and Train: Azure Databricks\nAzure Databricks provides enterprise-grade Azure security, including Azure Active Directory integration.\nWith Azure Databricks, you can set up your Apache Spark environment in minutes, autoscale and collaborate on shared projects in an interactive workspace.\nAzure Databricks supports Python, Scala, R, Java and SQL, as well as data science frameworks and libraries including TensorFlow, PyTorch and scikit-learn.\nModel and Serve: Azure Synapse Analytics\nAzure Synapse Analytics/ SQL Data Warehouse stores data into relational tables with columnar storage.\nAzure SQL Data Warehouse connector now offers efficient and scalable structured streaming write support for SQL Data Warehouse. Access SQL Data\nWarehouse from Azure Databricks using the SQL Data Warehouse connector.\nNote: As of November 2019, Azure SQL Data Warehouse is now Azure Synapse Analytics.\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/azure/architecture/data-guide/technology-choices/pipeline-orchestration-data-movement https://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks","isMC":false,"question_text":"HOTSPOT -\nA company plans to use Platform-as-a-Service (PaaS) to create the new data pipeline process. The process must meet the following requirements:\nIngest:\n✑ Access multiple data sources.\n✑ Provide the ability to orchestrate workflow.\n✑ Provide the capability to run SQL Server Integration Services packages.\nStore:\n✑ Optimize storage for big data workloads.\n✑ Provide encryption of data at rest.\n✑ Operate with no size limits.\nPrepare and Train:\n✑ Provide a fully-managed and interactive workspace for exploration and visualization.\n✑ Provide the ability to program in R, SQL, Python, Scala, and Java.\nProvide seamless user authentication with Azure Active Directory.\n//IMG//\n\nModel & Serve:\n✑ Implement native columnar storage.\n✑ Support for the SQL language\n✑ Provide support for structured streaming.\nYou need to build the data integration pipeline.\nWhich technologies should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0015600010.png","https://www.examtopics.com/assets/media/exam-media/04259/0015800001.jpg"],"answer_ET":"","topic":"2","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0015900001.jpg"],"exam_id":67},{"id":"9WU58HUvl1nsGp0VdBx9","question_id":227,"url":"https://www.examtopics.com/discussions/microsoft/view/76198-exam-dp-203-topic-2-question-70-discussion/","discussion":[{"upvote_count":"11","comment_id":"606172","poster":"SAYAK7","content":"Selected Answer: B\nObviously B, iOT is event hub of stream data so we need stream analytics for sure.","timestamp":"1684855920.0"},{"comment_id":"1001570","poster":"kkk5566","timestamp":"1725713100.0","upvote_count":"1","content":"Selected Answer: B\nB is correct"},{"timestamp":"1717298880.0","comment_id":"912502","content":"Selected Answer: B\nCorrect","upvote_count":"1","poster":"Ankit_Az"},{"poster":"Coderhbti","timestamp":"1713313920.0","comment_id":"872253","content":"Selected Answer: B\nB is correct","upvote_count":"2"},{"timestamp":"1691394960.0","poster":"Deeksha1234","content":"B is correct","upvote_count":"1","comment_id":"643629"},{"timestamp":"1687374780.0","upvote_count":"2","content":"i agree","comment_id":"620019","poster":"Remedios79"}],"unix_timestamp":1653319920,"choices":{"C":"Azure SQL Database","A":"Azure Databricks","B":"Azure Stream Analytics"},"answer":"B","answer_description":"","answer_ET":"B","question_images":[],"isMC":true,"topic":"2","answers_community":["B (100%)"],"question_text":"You are designing an anomaly detection solution for streaming data from an Azure IoT hub. The solution must meet the following requirements:\n✑ Send the output to Azure Synapse.\n✑ Identify spikes and dips in time series data.\n✑ Minimize development and configuration effort.\nWhich should you include in the solution?","answer_images":[],"timestamp":"2022-05-23 17:32:00","exam_id":67},{"id":"5dE1TMxqAsqjWxvyflPt","unix_timestamp":1653988020,"isMC":true,"answers_community":["C (100%)"],"question_images":[],"timestamp":"2022-05-31 11:07:00","answer":"C","answer_ET":"C","choices":{"C":"Watermark delay","A":"Early Input Events","D":"Input Deserialization Errors","B":"Late Input Events"},"answer_description":"","exam_id":67,"answer_images":[],"discussion":[{"poster":"nicky87654","comment_id":"778405","upvote_count":"9","content":"C-->it measures the amount of delay in the processing of the input events. If the watermark delay increases, it could indicate that the Stream Analytics job is not able to keep up with the incoming data and may not have enough processing resources to handle the additional load.","timestamp":"1673914260.0"},{"timestamp":"1741070100.0","content":"Selected Answer: C\nWatermark delay is correct","upvote_count":"1","poster":"Moli62","comment_id":"1364776"},{"content":"Selected Answer: C\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-time-handling#watermark-delay-details","comment_id":"1286866","timestamp":"1726840380.0","poster":"renan_ineu","upvote_count":"1"},{"content":"Selected Answer: C\nsure it","timestamp":"1694090760.0","poster":"kkk5566","upvote_count":"1","comment_id":"1001571"},{"comment_id":"758566","content":"Selected Answer: C\ncorrect !","timestamp":"1672148400.0","poster":"MScapris","upvote_count":"2"},{"content":"correct","upvote_count":"2","poster":"Deeksha1234","timestamp":"1659862560.0","comment_id":"643645"},{"upvote_count":"2","timestamp":"1657392660.0","poster":"dsp17","content":"Watermark delay - correct","comment_id":"629311"},{"content":"Selected Answer: C\nseems ok","upvote_count":"4","comment_id":"609633","poster":"sagur","timestamp":"1653988020.0"}],"url":"https://www.examtopics.com/discussions/microsoft/view/76457-exam-dp-203-topic-2-question-71-discussion/","question_id":228,"topic":"2","question_text":"A company uses Azure Stream Analytics to monitor devices.\nThe company plans to double the number of devices that are monitored.\nYou need to monitor a Stream Analytics job to ensure that there are enough processing resources to handle the additional load.\nWhich metric should you monitor?"},{"id":"IulvYKjthp6iGxiYKaPP","question_text":"HOTSPOT -\nYou are designing an enterprise data warehouse in Azure Synapse Analytics that will store website traffic analytics in a star schema.\nYou plan to have a fact table for website visits. The table will be approximately 5 GB.\nYou need to recommend which distribution type and index type to use for the table. The solution must provide the fastest query performance.\nWhat should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0025600001.png"],"discussion":[{"timestamp":"1675767660.0","comment_id":"643646","upvote_count":"12","content":"Hash and clustered columnstore..right","poster":"Deeksha1234"},{"poster":"j888","content":"Distribution Ideal data Recommended size\nHash: Fact tables with join column Large (hundreds MBs - TBs)\nRound-Robin: Staging tables, no join column Smaller (up to hundreds of MBs)\nReplicated Distribution: Dimension tables, small references Very small (up to 2 GB)","comment_id":"1152342","upvote_count":"1","timestamp":"1723855560.0"},{"comment_id":"912503","upvote_count":"4","timestamp":"1701495120.0","content":"Correct\nhash and CCI","poster":"Ankit_Az"},{"timestamp":"1671619140.0","content":"I'm not sure about the hash distribution since we don't have enough information on what columns we get. In any case I would choose Round Robin to just have a even distribution.","upvote_count":"1","comment_id":"619708","comments":[{"timestamp":"1702596300.0","poster":"vctrhugo","comment_id":"923576","content":"Anything above 2GB you should go with Hash. Round Robin/Heap is for staging tables.","upvote_count":"4"},{"upvote_count":"1","poster":"henryphchan","timestamp":"1699478640.0","content":"Round Robin is used for Staging Table","comment_id":"892466"},{"comment_id":"624538","timestamp":"1672314120.0","content":"I would go with Hash as the table is >2gb and is a fact table...","upvote_count":"9","poster":"Revave2"}],"poster":"objecto"}],"answer_ET":"","answer":"","question_id":229,"isMC":false,"timestamp":"2022-06-21 10:39:00","exam_id":67,"answer_description":"Box 1: Hash -\nConsider using a hash-distributed table when:\nThe table size on disk is more than 2 GB.\nThe table has frequent insert, update, and delete operations.\n\nBox 2: Clustered columnstore -\nClustered columnstore tables offer both the highest level of data compression and the best overall query performance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0025700001.png"],"answers_community":[],"unix_timestamp":1655800740,"url":"https://www.examtopics.com/discussions/microsoft/view/76974-exam-dp-203-topic-2-question-72-discussion/","topic":"2"},{"id":"tZGLAt9o14kteVLupjbX","topic":"2","discussion":[{"poster":"demirsamuel","content":"Selected Answer: AB\nA and B are correct","upvote_count":"13","comment_id":"609916","timestamp":"1685570160.0"},{"comment_id":"1328759","timestamp":"1734562560.0","upvote_count":"1","content":"Selected Answer: AB\nCorrect.\nInfo about point A:\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/monitor-azure-stream-analytics-reference","poster":"17lan"},{"poster":"kkk5566","timestamp":"1725713220.0","upvote_count":"1","comment_id":"1001573","content":"Selected Answer: AB\nA and B are correct"},{"timestamp":"1717299180.0","comment_id":"912504","poster":"Ankit_Az","upvote_count":"2","content":"Selected Answer: AB\nCorrect"},{"upvote_count":"2","timestamp":"1713314220.0","poster":"Coderhbti","content":"Selected Answer: AB\nCorrect Answers","comment_id":"872258"},{"timestamp":"1691399040.0","content":"Selected Answer: AB\ncorrect answer","poster":"Deeksha1234","upvote_count":"2","comment_id":"643647"}],"answer_ET":"AB","question_id":230,"url":"https://www.examtopics.com/discussions/microsoft/view/76468-exam-dp-203-topic-2-question-73-discussion/","timestamp":"2022-05-31 23:56:00","exam_id":67,"isMC":true,"answers_community":["AB (100%)"],"answer_images":[],"choices":{"E":"Late Input Events","B":"Watermark Delay","A":"Backlogged Input Events","C":"Function Events","D":"Out of order Events"},"answer_description":"","unix_timestamp":1654034160,"answer":"AB","question_images":[],"question_text":"You have an Azure Stream Analytics job.\nYou need to ensure that the job has enough streaming units provisioned.\nYou configure monitoring of the SU % Utilization metric.\nWhich two additional metrics should you monitor? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point."}],"exam":{"isImplemented":true,"isMCOnly":false,"isBeta":false,"lastUpdated":"12 Apr 2025","id":67,"numberOfQuestions":384,"name":"DP-203","provider":"Microsoft"},"currentPage":46},"__N_SSP":true}