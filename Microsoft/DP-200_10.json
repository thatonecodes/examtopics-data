{"pageProps":{"questions":[{"id":"8cuo4ZXnGat1eCORqgBv","unix_timestamp":1620574980,"answer_ET":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure subscription that contains an Azure Storage account.\nYou plan to implement changes to a data storage solution to meet regulatory and compliance standards.\nEvery day, Azure needs to identify and delete blobs that were NOT modified during the last 100 days.\nSolution: You schedule an Azure Data Factory pipeline.\nDoes this meet the goal?","discussion":[{"timestamp":"1620574980.0","poster":"kaigalmane","comments":[{"poster":"111222333","comment_id":"355663","timestamp":"1620839040.0","upvote_count":"4","content":"I also think \"yes\": https://docs.microsoft.com/en-us/azure/data-factory/delete-activity#clean-up-the-expired-files-that-were-last-modified-before-201811\n\n\"You can create a pipeline to clean up the old or expired files by leveraging file attribute filter: “LastModified” in dataset.\"\n\nAnd it uses blob files: \n\"dataset\": { \n \"referenceName\":\"BlobFilesLastModifiedBefore201811\", \n ... \n}"}],"comment_id":"353148","content":"This should be Yes right?","upvote_count":"5"},{"upvote_count":"1","content":"when it can be done with Azure Blob storage lifecycle policy, why data factory ?","poster":"satyamkishoresingh","comment_id":"448983","timestamp":"1632238260.0"},{"timestamp":"1621450860.0","poster":"niwe","comment_id":"361626","content":"There is no activity mentioned, it should be No.\nAm I right?","comments":[{"timestamp":"1622125560.0","poster":"niwe","comment_id":"368033","content":"Correct question is question #58","upvote_count":"1"}],"upvote_count":"2"},{"upvote_count":"2","poster":"Saravjeet","comment_id":"360246","content":"It should be Yes.","timestamp":"1621325880.0"}],"question_id":46,"topic":"1","timestamp":"2021-05-09 17:43:00","answer_description":"Instead you can use the Delete Activity in Azure Data Factory to delete files or folders from on-premises storage stores or cloud storage stores or apply an Azure\nBlob storage lifecycle policy.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/delete-activity https://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts?tabs=azure-portal","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/52210-exam-dp-200-topic-1-question-52-discussion/","answers_community":[],"isMC":true,"exam_id":65,"question_images":[],"choices":{"B":"No","A":"Yes"},"answer":"B"},{"id":"f05sbU8o2oZ96TKyuuiB","answer_description":"All file formats have different performance characteristics. For the fastest load, use compressed delimited text files.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data","answer":"A","timestamp":"2021-06-05 14:30:00","unix_timestamp":1622896200,"choices":{"B":"No","A":"Yes"},"question_id":47,"url":"https://www.examtopics.com/discussions/microsoft/view/54604-exam-dp-200-topic-1-question-53-discussion/","answer_images":[],"answers_community":[],"answer_ET":"A","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Storage account that contains 100 GB of files. The files contain text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\nYou need to prepare the files to ensure that the data copies quickly.\nSolution: You convert the files to compressed delimited text files.\nDoes this meet the goal?","isMC":true,"exam_id":65,"question_images":[],"discussion":[{"upvote_count":"1","content":"It should be \"Yes\". \nCompress the source file is good practice in this use case,. Polybase is not mandatory in this question and the data to transfert is not so huge. As mentionned by Microsoft below, we can turn off \"using polybase\" which avoid the overhead of splitting the files. \n\n\"Row size and data type limits\nPolyBase loads are limited to rows smaller than 1 MB. It cannot be used to load to VARCHR(MAX), NVARCHAR(MAX), or VARBINARY(MAX). For more information, see Azure Synapse Analytics service capacity limits.\n\nWhen your source data has rows greater than 1 MB, you might want to vertically split the source tables into several small ones. Make sure that the largest size of each row doesn't exceed the limit. The smaller tables can then be loaded by using PolyBase and merged together in Azure Synapse Analytics.\n\nAlternatively, for data with such wide columns, you can use non-PolyBase to load the data by turning off \"allow PolyBase\" setting.\"\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory","poster":"Marcus1612","timestamp":"1633527720.0","comment_id":"458301"},{"content":"It should be \"No\" as the row length should be less than 1 MB","upvote_count":"1","comment_id":"375082","timestamp":"1622896200.0","comments":[{"poster":"hello_there_","content":"The max 1 MB limit only applies if you plan to use polybase","timestamp":"1624445880.0","comment_id":"388741","upvote_count":"2"},{"content":"Check answer 31. It seems that the 1 MB limit is no longer necessary.","comment_id":"387480","upvote_count":"1","poster":"lgtiza","timestamp":"1624319820.0"}],"poster":"vrmei"}],"topic":"1"},{"id":"dGxUDMLlKSMTz4M2m3aw","exam_id":65,"answer_description":"","unix_timestamp":1606395540,"answer":"A","answer_images":[],"answer_ET":"A","answers_community":[],"choices":{"A":"Round-robin distributed table","C":"Replicated table","B":"Hash-distributed table","D":"External table"},"question_images":[],"topic":"1","discussion":[{"comment_id":"228345","poster":"syu31svc","comments":[{"poster":"memo43","timestamp":"1621177980.0","upvote_count":"1","comment_id":"358845","content":"keyword is staging table"}],"timestamp":"1606395540.0","upvote_count":"17","content":"From https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview:\nUse round-robin for the staging table. The load with CTAS is fast. Once the data is in the staging table, use INSERT...SELECT to move the data to production tables."},{"timestamp":"1624788900.0","comment_id":"391948","upvote_count":"1","content":"keyword --> staging table --> round robin is best","poster":"jayeshstudies"},{"timestamp":"1619753580.0","content":"round robin is entirely correct for staging tables","poster":"cadio30","upvote_count":"3","comment_id":"345849"},{"poster":"cjh1912","upvote_count":"4","comment_id":"263630","timestamp":"1610250060.0","content":"answer is correct its specifically asked for fast loading , not read, which leads to round robin being the correct answer"}],"question_text":"You are designing an enterprise data warehouse in Azure Synapse Analytics. You plan to load millions of rows of data into the data warehouse each day.\nYou must ensure that staging tables are optimized for data loading.\nYou need to design the staging tables.\nWhat type of tables should you recommend?","timestamp":"2020-11-26 13:59:00","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/37840-exam-dp-200-topic-1-question-54-discussion/","question_id":48},{"id":"AkXxnGgGqvuJSVMcKEQB","isMC":false,"question_id":49,"exam_id":65,"unix_timestamp":1617514380,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0010400001.jpg"],"answer_ET":"","answers_community":[],"question_text":"HOTSPOT -\nYou have a SQL pool in Azure Synapse.\nYou plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load.\nYou need to create the staging table. The solution must minimize how long it takes to load the data to the staging table.\nHow should you configure the table? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"Box 1: Hash -\nHash-distributed tables improve query performance on large fact tables. hey can have very large numbers of rows and still achieve high performance.\nIncorrect:\nRound-robin tables are useful for improving loading speed.\n\nBox 2: Clustered columnstore -\nWhen creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed.\n\nBox 3: Date -\nTable partitions enable you to divide your data into smaller groups of data. In most cases, table partitions are created on a date column.\nPartition switching can be used to quickly remove or replace a section of a table.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0010500001.jpg"],"answer":"","timestamp":"2021-04-04 07:33:00","topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/49009-exam-dp-200-topic-1-question-55-discussion/","discussion":[{"poster":"mamhh","upvote_count":"68","timestamp":"1618481940.0","content":"Round-Robin\nHeap\nNone","comment_id":"336153"},{"comment_id":"327774","comments":[{"timestamp":"1621366980.0","upvote_count":"2","comments":[{"timestamp":"1656136020.0","poster":"uzairahm","comment_id":"621975","upvote_count":"1","content":"As @TessieB has indicated quoting MS Docs \"Partitioning is also supported on all distribution types, including both hash or round robin distributed.\"\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"}],"comment_id":"360801","poster":"maciejt","content":"If Round Robin, then you don't choose column to partition on, it can only be None"}],"poster":"LongBao","upvote_count":"10","content":"From https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview:\n\"Use round-robin for the staging table. The load with CTAS is fast. Once the data is in the staging table, use INSERT...SELECT to move the data to production tables.\"\nThis is a staging table, not a fact, so I think the answer is Round robin, Heap, Date.","timestamp":"1617514380.0"},{"content":"round robin, heap, none \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices#load-to-a-staging-table","upvote_count":"1","poster":"dark_one","comment_id":"1198701","timestamp":"1713538200.0"},{"comment_id":"427341","upvote_count":"2","content":"Round-Robin , Heap , None","timestamp":"1629363480.0","poster":"saranya23"},{"comment_id":"413192","content":"round-robin, haep, non \n\nWhy should someone hash distribute a stage table?","timestamp":"1627131300.0","upvote_count":"1","poster":"elimey"},{"upvote_count":"2","timestamp":"1626589260.0","poster":"TessieB","comment_id":"408746","content":"I think the answer should be:\nRound robin\nHeap\nDate\n\nSee here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition \n\nPartitioning and distribution are two different things! Partitioning is supported by all types of distribution and by all types of indexes! Partitioning can speed up the loading process and it's often done by using a date column. In this question it's not clear however, if the Date column is the right fit for the loading process, but since it is often used like that, I'm gonna go and say that Date might be the correct answer! :)"},{"upvote_count":"4","timestamp":"1623488280.0","comment_id":"380333","poster":"alok1988","content":"Round-Robin , Heap , None"},{"timestamp":"1621326540.0","content":"I think it should be Round-RObin, Heap and None. Refer the link\nhttps://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-data-loading-guidance?view=azure-sqldw-latest","upvote_count":"4","poster":"Saravjeet","comment_id":"360264"},{"comment_id":"355974","content":"The correct answer is :\nRound-Robin\nClustered columnstore\nData\nTable partitions enable you to divide your data into smaller groups of data. In most cases, table partitions are created on a date column. Partitioning is supported on all dedicated SQL pool table types; including clustered columnstore, clustered index, and heap. Partitioning is also supported on all distribution types, including both hash or round robin distributed.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition","comments":[{"poster":"maciejt","comment_id":"360803","content":"Round RObin is random equal distribution, it doesn't include choosing a column to partition on","timestamp":"1621367100.0","upvote_count":"1"}],"timestamp":"1620878640.0","poster":"Qrm_1972","upvote_count":"2"},{"timestamp":"1620845100.0","upvote_count":"7","comment_id":"355730","poster":"robin_examtopics","content":"The answer should be Round-Robin/ Heap/ None.\n\nReference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/guidance-for-loading-data#loading-to-a-staging-table\n\n“To achieve the fastest loading speed for moving data into a dedicated SQL pool table, load data into a staging table. Define the staging table as a heap and use round-robin for the distribution option.”"},{"upvote_count":"1","comment_id":"348726","timestamp":"1620054600.0","content":"If you are partitioning by Date and using Clustered Column Store, Would that be faster than using a Heap, None?","poster":"itmemememe"},{"upvote_count":"3","comment_id":"345856","poster":"cadio30","content":"Round-Robin\nHeap\nNone\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index","timestamp":"1619754300.0"},{"upvote_count":"1","comments":[{"timestamp":"1620036300.0","upvote_count":"1","content":"I'll go with \"Round-Robin, Heap and None\", Round-Robin is the best and obvious choice, then Heap as there's no need for indexing since it's just for loading purposes, and then None as Round-Robin does not support partitioning.","comment_id":"348501","poster":"Garnew"}],"comment_id":"342397","content":"Anyone know the correct answer?, I'm between (Round-Robin, Heap and None) the other option (Round-Robin, Heap, Date)","poster":"jamorey","timestamp":"1619336880.0"},{"timestamp":"1618332720.0","content":"I think that the first is round robin (fastest way to load data) and the third box should be None, since round robin doesn't need partitioning","comment_id":"334814","upvote_count":"4","poster":"Pairon"},{"comment_id":"328639","content":"Answer should be \"round-robin\"","timestamp":"1617624840.0","poster":"alf99","upvote_count":"7"},{"timestamp":"1617587160.0","comment_id":"328327","upvote_count":"3","content":"Consider using the round-robin distribution for your table in the following scenarios:\n\nWhen getting started as a simple starting point since it is the default\nIf there is no obvious joining key\nIf there is no good candidate column for hash distributing the table\nIf the table does not share a common join key with other tables\nIf the join is less significant than other joins in the query\nWhen the table is a temporary staging table","poster":"princy18"}]},{"id":"7t87fxvP8UuqeuRpcdSn","topic":"1","answer_images":[],"answer_ET":"ACD","discussion":[{"upvote_count":"69","comment_id":"257381","poster":"ACSC","timestamp":"1609582080.0","content":"Correct answers are A, C, F."},{"comments":[{"poster":"ck8.kakade","comment_id":"1255828","content":"Sorry for the typo - I meant FAC","timestamp":"1722016200.0","upvote_count":"1"}],"timestamp":"1722013560.0","comment_id":"1255810","upvote_count":"1","content":"F. Use the managed identity as the credentials for the data load process.\nA. Create a managed identity.\nB. Use the shared access signature (SAS) as the credentials for the data load process.","poster":"ck8.kakade"},{"timestamp":"1637679840.0","content":"Selected Answer: ACF\nACF, or FAD in the correct order are the good answers","poster":"FredNo","upvote_count":"1","comment_id":"485111","comments":[{"comments":[{"timestamp":"1637679900.0","upvote_count":"1","comment_id":"485114","poster":"FredNo","content":"FAC***"}],"upvote_count":"1","timestamp":"1637679840.0","comment_id":"485112","poster":"FredNo","content":"I meant FAD"}]},{"comment_id":"440437","poster":"hsetin","timestamp":"1630942260.0","content":"ACD is fine. data load process has nothing to do with Sales Group. you just need to add sales group to Active directory.","comments":[{"poster":"uzairahm","comment_id":"621957","content":"Please Azure AD Group Sales already created: \"All the members of the sales team are in an Azure Active Directory group named Sales.\"","upvote_count":"1","timestamp":"1656132900.0"}],"upvote_count":"1"},{"comment_id":"358849","upvote_count":"3","content":"I also think ACF","timestamp":"1621178340.0","poster":"hoangton"},{"comment_id":"349172","upvote_count":"4","timestamp":"1620108240.0","content":"Agree with A,C,F\n\nReference: https://docs.microsoft.com/en-us/azure/purview/register-scan-adls-gen2","poster":"cadio30"},{"timestamp":"1619540580.0","content":"Correct answers are C, D, F.","poster":"davita8","upvote_count":"2","comment_id":"344097"},{"upvote_count":"3","timestamp":"1618852440.0","comment_id":"338965","content":"Correct answers are A, C, F.","poster":"Wendy_DK"}],"unix_timestamp":1609582080,"answers_community":["ACF (100%)"],"exam_id":65,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/41207-exam-dp-200-topic-1-question-56-discussion/","question_id":50,"isMC":true,"answer_description":"A: The managed identity grants permissions to the dedicated SQL pools in the workspace.\nNote: Managed identity for Azure resources is a feature of Azure Active Directory. The feature provides Azure services with an automatically managed identity in\n\nAzure AD -\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-identity","timestamp":"2021-01-02 11:08:00","choices":{"C":"Add the managed identity to the Sales group.","E":"Create a shared access signature (SAS).","A":"Create a managed identity.","F":"Use the managed identity as the credentials for the data load process.","D":"Add your Azure Active Directory (Azure AD) account to the Sales group.","B":"Use the shared access signature (SAS) as the credentials for the data load process."},"question_text":"You have an enterprise-wide Azure Data Lake Storage Gen2 account. The data lake is accessible only through an Azure virtual network named VNET1.\nYou are building a SQL pool in Azure Synapse that will use data from the data lake.\nYour company has a sales team. All the members of the sales team are in an Azure Active Directory group named Sales. POSIX controls are used to assign the\nSales group access to the files in the data lake.\nYou plan to load data to the SQL pool every hour.\nYou need to ensure that the SQL pool can load the sales data from the data lake.\nWhich three actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","answer":"ACF"}],"exam":{"provider":"Microsoft","lastUpdated":"12 Apr 2025","name":"DP-200","id":65,"isBeta":false,"isImplemented":true,"isMCOnly":false,"numberOfQuestions":228},"currentPage":10},"__N_SSP":true}