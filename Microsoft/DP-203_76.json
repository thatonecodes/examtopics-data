{"pageProps":{"questions":[{"id":"WUldGRYPCpwJ8D5Xb9cF","url":"https://www.examtopics.com/discussions/microsoft/view/80136-exam-dp-203-topic-5-question-4-discussion/","answer_images":[],"discussion":[{"content":"Selected Answer: A\nA surrogate key is a system-generated unique identifier that is used as a substitute for a natural key. In this case, the surrogate key will be used to account for changes to the retail store addresses.\n\nBy creating a table with an IDENTITY property, you can ensure that a unique surrogate key is automatically generated for each row inserted into the table. The IDENTITY property assigns a unique value to the column automatically, incrementing by one for each new row.\n\nUsing an IDENTITY column as the surrogate key will provide an efficient way to join and filter sales transaction records based on product ID, as required by the sales transaction dataset requirements.","poster":"vctrhugo","upvote_count":"6","comment_id":"931069","timestamp":"1687469340.0"},{"comment_id":"1249543","timestamp":"1721214480.0","upvote_count":"1","content":"Selected Answer: A\nCREATE TABLE RetailStore (\n StoreID INT IDENTITY(1,1) PRIMARY KEY,\n StoreName NVARCHAR(100),\n Location NVARCHAR(100)\n);","poster":"evangelist"},{"poster":"kkk5566","comment_id":"995736","timestamp":"1693548000.0","content":"Selected Answer: A\nis correct","upvote_count":"2"},{"timestamp":"1683103860.0","upvote_count":"2","poster":"sntlkumar","content":"Given answer is correct","comment_id":"888314"},{"comment_id":"737167","poster":"uira","timestamp":"1670355420.0","content":"Selected Answer: A\nIdentity should be used.","upvote_count":"3"},{"poster":"7yut","content":"Selected Answer: A\nCorrect","timestamp":"1669884840.0","upvote_count":"3","comment_id":"732425"},{"comment_id":"659485","upvote_count":"4","content":"Selected Answer: A\nA is the correct Answer !","timestamp":"1662319440.0","poster":"anks84"}],"question_text":"You need to implement the surrogate key for the retail store table. The solution must meet the sales transaction dataset requirements.\nWhat should you create?","question_images":[],"exam_id":67,"answer":"A","answers_community":["A (100%)"],"isMC":true,"choices":{"A":"a table that has an IDENTITY property","D":"a table that has a FOREIGN KEY constraint","C":"a user-defined SEQUENCE object","B":"a system-versioned temporal table"},"topic":"5","timestamp":"2022-09-04 21:24:00","unix_timestamp":1662319440,"question_id":376,"answer_ET":"A","answer_description":""},{"id":"ASynsT8JUvmSGDTIIVC0","url":"https://www.examtopics.com/discussions/microsoft/view/80996-exam-dp-203-topic-5-question-5-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0001200001.jpg"],"discussion":[{"comment_id":"688900","poster":"greenlever","timestamp":"1665175680.0","upvote_count":"54","content":"replicated\nhash"},{"content":"Data is more than 100GB : hash\nDimension data less than 2GB: replicated\nStaging table data less than 5Gb:Round Robin\n\nSo replicated and Hash","timestamp":"1674580200.0","upvote_count":"22","poster":"Jerrie86","comment_id":"786718"},{"timestamp":"1743424020.0","content":"retail - replicated, promotional - hash","upvote_count":"1","poster":"Sathya_sree","comment_id":"1415114"},{"timestamp":"1729780800.0","content":"For large size dimension tables, Round-Robin is inappropriate because only Hash can ensure that the row with the same hash value can end up on the same distribution so it will optimize the performance of join and group.","upvote_count":"1","poster":"8ac3742","comment_id":"1302500"},{"comment_id":"1202592","timestamp":"1714135680.0","upvote_count":"1","poster":"Dusica","content":"Replicated\nRound Robin - because data is short lived only while campaign"},{"timestamp":"1694079840.0","content":"replicated & hash","upvote_count":"1","comment_id":"1001390","poster":"hassexat"},{"poster":"hassexat","timestamp":"1694003760.0","comment_id":"1000602","content":"Replicated --> Because is not a staging table and is moreless 2GB\nHash --> Because is 200GB","upvote_count":"1"},{"poster":"kkk5566","content":"replicated ,hash tables are best for queries with joins and\naggregations.","timestamp":"1693548120.0","upvote_count":"1","comment_id":"995737"},{"poster":"peacejh","comment_id":"969061","timestamp":"1690892820.0","content":"In the text it says that the table is 200GB, so hash. In the answer explanation it suddenly is only 5 GB","upvote_count":"2"},{"upvote_count":"4","content":"Box1: (clearly) replicated\nBox2: I can see why someone would say round-robin since it is not uncommon for large dim_tables (and that is what this promotions table will essentially be) to use this distribution BUT per Microsoft doc below using round-robin makes sense in situation tht simply do not apply here:\n- When getting started as a simple starting point since it is the default --> NOT THE CASE\n- If there is no obvious joining key --> THERE IS, product id which will be present in the fact transactions table as well\nIf there is no good candidate column for hash distributing the table - THERE IS, PromotionID\n- If the table does not share a common join key with other tables - IT DOES, ProductID\n- If the join is less significant than other joins in the query - NO INF on this\n- When the table is a temporary staging table - IT IS NOT\nsource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\ntherefore, box2: Hash","poster":"andjurovicela","timestamp":"1690531620.0","comment_id":"965377"},{"comment_id":"935245","upvote_count":"4","content":"Retail Store contains info about store (like address), it's clearly a dimension table, by the consequence it is REPLICATED.\nThe second is correct: HASH.","timestamp":"1687862040.0","poster":"auwia"},{"upvote_count":"3","poster":"pavankr","comment_id":"908946","content":"So on which answer we should reply on??????? Why this web site guy is guiding us all wrong answers?????","timestamp":"1685322360.0"},{"timestamp":"1673976420.0","poster":"JosephVishal","upvote_count":"2","comment_id":"779132","content":"Box1: Replicated \nBox2: Hash. Since, the Retail store table, will be used in queries and there is no mention of data loads to this table. It should be replicated and not Round-Robin."},{"poster":"Taou","content":"1st is Replicated","comment_id":"761670","timestamp":"1672372560.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"734826","content":"Box1: Replicated. As the Retail Store is going to be replicated in each distribution to facilitate SQL queries.\n\nBox2: Hash for large fact tables","timestamp":"1670125080.0","poster":"AzureJobsTillRetire"},{"comment_id":"678982","poster":"smsme323","timestamp":"1664121960.0","content":"replicated\nHAsh","upvote_count":"2"},{"timestamp":"1663913820.0","comment_id":"676823","content":"-Replicated\n-Hash","upvote_count":"3","poster":"juanlu46"},{"comment_id":"664122","content":"Looks like \"Retail Store\" is a dimension table with 2MB size.\nSo, Replicated should be better option in my opinion,","timestamp":"1662686280.0","upvote_count":"5","poster":"anks84"},{"poster":"federc","comment_id":"663286","content":"Agree with you Julia01, Replicated would be more reasonable for a 2MB table","timestamp":"1662621060.0","upvote_count":"2"},{"timestamp":"1662615540.0","upvote_count":"3","poster":"R12346","content":"The retail table should be of replicated type since it is just 2 MB","comment_id":"663165"},{"timestamp":"1662574860.0","content":"Shouldn't it be replicated in the first one?","comment_id":"662791","poster":"Julia01","comments":[{"comment_id":"663355","upvote_count":"3","timestamp":"1662624660.0","poster":"pangas2567","content":"Agree, only 2MB of size could definitely be replicated. I don't see any load speed requirements in the description for this table."}],"upvote_count":"5"}],"question_text":"HOTSPOT -\nYou need to design an analytical storage solution for the transactional data. The solution must meet the sales transaction dataset requirements.\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0001100001.jpg"],"exam_id":67,"answer":"","answers_community":[],"isMC":false,"topic":"5","timestamp":"2022-09-07 20:21:00","unix_timestamp":1662574860,"question_id":377,"answer_ET":"","answer_description":"Box 1: Round-robin -\nRound-robin tables are useful for improving loading speed.\nScenario: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month.\n\nBox 2: Hash -\nHash-distributed tables improve query performance on large fact tables.\nScenario:\n✑ You plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 5 GB.\n✑ Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"},{"id":"wztuuNEQsAXft21oQMK4","isMC":false,"question_id":378,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0001300002.jpg"],"exam_id":67,"answer":"","unix_timestamp":1662621180,"timestamp":"2022-09-08 09:13:00","topic":"5","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0001300001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/81127-exam-dp-203-topic-5-question-6-discussion/","question_text":"HOTSPOT -\nYou need to implement an Azure Synapse Analytics database object for storing the sales transactions data. The solution must meet the sales transaction dataset requirements.\nWhat should you do? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"upvote_count":"12","content":"Its funny cause in the scenario, there is a BIG hint on what to use for box 2. Read it up.","timestamp":"1685919180.0","comment_id":"735558","poster":"XiltroX"},{"poster":"Azurre","content":"Hint as per XiltroX:\nContoso identifies the following requirements for the sales transaction dataset:\nPartition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right.","upvote_count":"11","timestamp":"1695433020.0","comment_id":"847747"},{"timestamp":"1739766900.0","poster":"aeab260","upvote_count":"1","content":"Create table because \"Azure Synapse Analytics database object for storing the sales transactions data\". External table will only hold metadata not store data in SQL Pool.","comment_id":"1357573"},{"comments":[{"upvote_count":"1","comment_id":"1111414","timestamp":"1719857220.0","content":"as far as i see, syntax only belongs to create table\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7#PartitionedTable","poster":"meatpoof"},{"poster":"jongert","content":"External table is used when using an external data source such as data stored in Azure Data Lake Storage Gen2. In this case, as seen in a previous question, the data will be stored in the columnar store of the dedicated SQL pool.","upvote_count":"4","comment_id":"1114986","timestamp":"1720236780.0"}],"content":"can someone help me with on first one, why is it create table and not external table?","upvote_count":"1","comment_id":"1098064","timestamp":"1718526420.0","poster":"SenMia"},{"poster":"AndreiG","content":"Why create table and not create external table ?","timestamp":"1713206100.0","upvote_count":"2","comment_id":"1044379"},{"upvote_count":"2","comment_id":"995742","poster":"kkk5566","content":"https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7#ExTablePartitions","comments":[{"content":"create table ,right","upvote_count":"2","comment_id":"1003039","poster":"kkk5566","timestamp":"1709980860.0"}],"timestamp":"1709280660.0"},{"comment_id":"699101","upvote_count":"2","poster":"Xinyuehong","content":"agreed","timestamp":"1681914540.0"},{"content":"correct","comment_id":"663287","upvote_count":"2","poster":"federc","timestamp":"1678266780.0"}],"answer_description":"Box 1: Create table -\nScenario: Load the sales transaction dataset to Azure Synapse Analytics\n\nBox 2: RANGE RIGHT FOR VALUES -\nScenario: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right.\nRANGE RIGHT: Specifies the boundary value belongs to the partition on the right (higher values).\nFOR VALUES ( boundary_value [,...n] ): Specifies the boundary values for the partition.\nScenario: Load the sales transaction dataset to Azure Synapse Analytics.\nContoso identifies the following requirements for the sales transaction dataset:\n✑ Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right.\n✑ Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible.\n✑ Implement a surrogate key to account for changes to the retail store addresses.\n✑ Ensure that data storage costs and performance are predictable.\n✑ Minimize how long it takes to remove old records.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse","answer_ET":""},{"id":"wyHdKD8Nh9kCfioXDRnA","question_id":379,"isMC":true,"answer_images":[],"choices":{"C":"time-based retention","B":"soft delete","A":"change feed","D":"lifecycle management"},"exam_id":67,"answer":"D","unix_timestamp":1667943480,"topic":"5","timestamp":"2022-11-08 22:38:00","answers_community":["D (83%)","C (17%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/87183-exam-dp-203-topic-5-question-7-discussion/","question_text":"You need to design a data retention solution for the Twitter feed data records. The solution must meet the customer sentiment analytics requirements.\nWhich Azure Storage functionality should you include in the solution?","discussion":[{"comment_id":"788007","upvote_count":"8","comments":[{"comment_id":"790069","upvote_count":"2","poster":"yogiazaad","content":"A time-based retention policy protects against deletion of blob while it is in effect. Note that it will not automatically delete the blob after the retention period.","timestamp":"1674862740.0"}],"poster":"yogiazaad","content":"Selected Answer: D\nGiven answer is correct. \nTime bases retention is to retain data for a specific time. it wont delete the data. The requirement is to deleted the data after 2 Years. Which can be accomplished by Data life cycle management. \nA time-based retention policy stores blob data in a Write-Once, Read-Many (WORM) format for a specified interval. When a time-based retention policy is set, clients can create and read blobs, but can't modify or delete them. After the retention interval has expired, blobs can be deleted but not overwritten.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-time-based-retention-policy-overview","timestamp":"1674671520.0"},{"poster":"evangelist","timestamp":"1721213460.0","content":"Selected Answer: D\nD. lifecycle management\nHere's why lifecycle management is the best choice:\n\nAutomated data management: Lifecycle management allows you to automatically move or delete data based on predefined rules and schedules.\nCost optimization: It helps in managing storage costs by transitioning older or less frequently accessed data to cooler storage tiers.\nCompliance: It can help meet data retention policies by automatically deleting data after a specified period.\nFlexibility: You can create custom policies based on factors like last modified date, which is ideal for managing social media data like Twitter feeds.\nIntegration: It works well with Azure Data Lake Storage Gen2, which is commonly used for big data analytics scenarios like sentiment analysis.","comment_id":"1249538","upvote_count":"1"},{"poster":"Momoanwar","timestamp":"1703530200.0","content":"Selected Answer: D\nChatgpt say d\nBased on the case study information provided, the best Azure Storage functionality to include in the solution for data retention of Twitter feed data records is:\n\nD. Lifecycle Management\n\nThis feature enables the creation of rules to manage the lifecycle of the data stored in Azure Blob Storage. It can automate the process of purging Twitter feed data records that are older than two years, aligning with the requirement for minimal administrative effort and ensuring compliance with the data retention policy.","upvote_count":"1","comment_id":"1105479"},{"timestamp":"1693548660.0","content":"Selected Answer: D\nis correct","comment_id":"995743","upvote_count":"1","poster":"kkk5566"},{"comment_id":"931070","poster":"vctrhugo","timestamp":"1687469760.0","content":"Selected Answer: D\nAzure Storage provides a feature called lifecycle management, which allows you to define rules to manage the retention and deletion of data in your storage account. Lifecycle management enables you to automatically transition, delete, or take other actions on objects in your storage account based on specified conditions.","upvote_count":"2"},{"comment_id":"860741","upvote_count":"3","poster":"cale","content":"Selected Answer: D\nAnswer is D","timestamp":"1680592320.0"},{"timestamp":"1677511800.0","comments":[{"content":"Time-based retention is a feature of Azure Blob storage that allows you to automatically delete blobs that have reached a certain age. While this feature can be useful for automatically deleting old data, it is not the most appropriate solution for the scenario you described because it only applies to block blobs and append blobs, and it is not available for other types of data in Azure Storage, such as files or tables.\n\nIn contrast, lifecycle management is a more comprehensive solution that allows you to define rules to automatically transition data to different access tiers or expire data at the end of its lifecycle. This functionality applies to block blobs, append blobs, and versioned block blobs, and it provides more flexibility in defining data retention policies.","upvote_count":"3","timestamp":"1687394460.0","poster":"JG1984","comment_id":"930029","comments":[{"content":"Time-based retention enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data cannot be modified or deleted for a user-specified interval.","upvote_count":"2","poster":"vctrhugo","timestamp":"1688150100.0","comment_id":"939327"}]},{"content":"You are wrong. As it was said few times. Time-based retention will protect the data during set period from deletion but it won't delete it automatically after set time.","timestamp":"1677928380.0","poster":"Ast999","comment_id":"828847","upvote_count":"4"}],"content":"Selected Answer: C\ndo the research. it is C time-based retention","upvote_count":"1","poster":"haidebelognime","comment_id":"823891"},{"comment_id":"781464","timestamp":"1674153000.0","content":"Selected Answer: C\nSim_al explanation is correct","upvote_count":"1","poster":"MrWood47"},{"upvote_count":"2","poster":"nicky87654","timestamp":"1674028440.0","comment_id":"779730","content":"Selected Answer: C\nC: time-based retention"},{"comments":[{"upvote_count":"4","content":"Time bases retention is not the correct answer.\nA time-based retention policy protects against deletion of blob while it is in effect. Note that it will not automatically delete the blob after the retention period.","timestamp":"1674862800.0","comment_id":"790071","poster":"yogiazaad"}],"poster":"Sima_al","upvote_count":"3","comment_id":"764516","content":"C: time-based retention \nBased on the customer sentiment analytics requirements, you should include time-based retention in the data retention solution for the Twitter feed data records. Time-based retention allows you to specify a retention period for data in Azure Storage and ensures that data is not deleted before its retention period expires. This functionality can be used to meet the requirement to purge Twitter feed data records that are older than two years.\n\nOption A (change feed) is a feature of Azure Table Storage and Azure Cosmos DB that provides a stream of change events on a table or container.\n\nOption B (soft delete) is a feature of Azure Table Storage and Azure Cosmos DB that allows you to mark an entity as deleted without permanently deleting it. This allows you to recover deleted data if necessary.\n\nOption D (lifecycle management) is a feature of Azure Blob Storage that allows you to specify policies for automatically transitioning blobs to different storage tiers or deleting them based on their age or access patterns.","timestamp":"1672744620.0"},{"poster":"Snomax","content":"Selected Answer: D\nAgreed.","timestamp":"1667943480.0","comment_id":"714129","upvote_count":"3"}],"answer_description":"","answer_ET":"D"},{"id":"WasN44DG8ckvrss14G7L","isMC":false,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/62692-exam-dp-203-topic-6-question-1-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0014000001.png"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0013900001.png"],"topic":"6","timestamp":"2021-09-25 04:15:00","exam_id":67,"question_text":"HOTSPOT -\nWhich Azure Data Factory components should you recommend using together to import the daily inventory data from the SQL server to Azure Data Lake Storage?\nTo answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"Box 1: Self-hosted integration runtime\nA self-hosted IR is capable of running copy activity between a cloud data stores and a data store in private network.\n\nBox 2: Schedule trigger -\n\nSchedule every 8 hours -\n\nBox 3: Copy activity -\nScenario:\n✑ Customer data, including name, contact information, and loyalty number, comes from Salesforce and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table.\n✑ Product data, including product ID, name, and category, comes from Salesforce and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table.","answer_ET":"","question_id":380,"discussion":[{"comment_id":"734852","timestamp":"1701666120.0","poster":"AzureJobsTillRetire","content":"Agreed with the given answers.\n\nBox1: Self-hosted integration runtime\nWhy not Azure-SSIS integration rutime? SSIS is not mentioned, and the ETL tool in use is ADF. \nWhy not Azure integration runtime? On-premise SQL Server database is used.\n\nBox2: Schedule trigger \nWhy not event-based trigger? Schedule runs every 8 hours \nWhy not tumbling window schedule? There is no requirement for a tumbling window schedule. If the ETL jobs run close to 8 hours, a tumbling window schedule may be required. If jobs need to automatically re-run on failures, a tumbling window schedule may be required. Those requirements are not there. Schedule trigger fits for purpose.\n\nBox3: Copy activity\nNo need for explanation","comments":[{"content":"agree with you","upvote_count":"2","poster":"vrodriguesp","comment_id":"811058","timestamp":"1708113660.0"}],"upvote_count":"27"},{"poster":"ArvindK06","comment_id":"473163","content":"Should be Tumbling Window in my opinion. Since Inventory data should be updated in real time as close as possible. Only Customer & Product data are available every 8 hours.","upvote_count":"23","comments":[{"poster":"andjurovicela","comment_id":"965379","upvote_count":"2","content":"I think you are mixing up different requirements. Daily inventory, should be ingested daily, I would say and for that schedule trigger makes sense. It would make less sense to use a thumbling window, not to mention that you can't even use \"day\" as the first argument (time interval) for the tumbling window function when coding, only hour as the biggest measurement unit","timestamp":"1722154680.0"},{"upvote_count":"3","content":"I also think it should be a Tumbling Window. Because it said 'Litware will build a custom application named FoodPrep to provide store employees with the calculation results of how many prepared food items to produce every four hours.'","comment_id":"553332","timestamp":"1677020940.0","poster":"ANath"}],"timestamp":"1667674080.0"},{"upvote_count":"1","content":"Self-hosted,Schedule ,Copy data","timestamp":"1725871500.0","comment_id":"1003042","poster":"kkk5566"},{"poster":"Bhuvanesh2104","comment_id":"774276","upvote_count":"2","comments":[{"comment_id":"949698","upvote_count":"1","content":"the link you provided clearly states that virtual networks should be in place whereas the task says \"Litware does not plan to implement Azure ExpressRoute or a VPN between the on-premises network and Azure.\" Therefore self-hosted integration runtime is the answer (as ExamTopics suggested)","timestamp":"1720782180.0","poster":"andjurovicela"}],"timestamp":"1705137120.0","content":"The below link refers to the (a) Azure Integration Runtime: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-managed-virtual-network-on-premise-sql-server"},{"upvote_count":"2","timestamp":"1695023340.0","content":"I think it should be SSIS integration runtime... Because currently there are SSIS pipelines which does the data integration","comment_id":"672094","poster":"sunil_smile"},{"poster":"Deeksha1234","upvote_count":"3","comment_id":"646335","timestamp":"1691932260.0","content":"In opinion the given answer is correct since its daily inventory data, i.e. will be loaded once daily."},{"comment_id":"512784","upvote_count":"2","content":"Answers are correct because 'Daily inventory data comes from a Microsoft SQL server located on a private network.'.","timestamp":"1672350600.0","poster":"Canary_2021"},{"upvote_count":"8","content":"I believe a Microsoft SQL server located on a private network means on Azure not on premises, which means the integration run time should be azure not self hosted.","timestamp":"1669630860.0","comment_id":"489054","poster":"dija123","comments":[{"comments":[{"timestamp":"1742171400.0","comment_id":"1399477","poster":"6b7c02a","upvote_count":"1","content":"If you use azure IR with managed vnet, if you create a managed private endpoint to SQL server, it should be able to connect? Why use SHIR in this case?"}],"upvote_count":"2","comment_id":"621941","poster":"Davico93","content":"maybe.... even if it is an azure resource, but in private network, we need SelfHosted","timestamp":"1687664220.0"},{"comment_id":"503340","upvote_count":"1","poster":"datnguye","timestamp":"1671249180.0","content":"Should it be Self-hosted as Microsoft SQL server, not Azure though?"}]},{"content":"The answers are correct.","upvote_count":"18","timestamp":"1666744860.0","comment_id":"467741","poster":"azurearmy"},{"comments":[{"upvote_count":"5","poster":"rikku33","timestamp":"1664267820.0","comment_id":"452271","content":"Schedule trigger - because daily. so the given answer is correct"},{"comment_id":"472133","content":"It is confusing cos at the top it says they want the inventory as real time as possible , but then further down it says every 8 hours. Conflicting info","comments":[{"poster":"kl8585","comments":[{"timestamp":"1701446760.0","poster":"OldSchool","comment_id":"732797","upvote_count":"1","content":"It says: \"Daily inventory data comes from a Microsoft SQL server located on a private network.\" So the answer is as given: Self-hosted, Schedule, Copy"}],"upvote_count":"1","content":"read carefully - import every 8 hours for customer date, not inventory data. Evenet triggers can be used only with storage account, so Event based is for sure wrong. It's tumbling windows","comment_id":"720533","timestamp":"1700231820.0"}],"upvote_count":"3","timestamp":"1667489100.0","poster":"samko92"}],"comment_id":"451181","poster":"AppleVan","content":"Shouldn't it be event based?","upvote_count":"3","timestamp":"1664072100.0"}],"unix_timestamp":1632536100,"answer":""}],"exam":{"id":67,"isMCOnly":false,"lastUpdated":"12 Apr 2025","name":"DP-203","provider":"Microsoft","numberOfQuestions":384,"isImplemented":true,"isBeta":false},"currentPage":76},"__N_SSP":true}