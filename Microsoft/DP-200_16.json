{"pageProps":{"questions":[{"id":"Pn7L8YoIE2N3CNXWiG0Y","answer":"B","answer_ET":"B","unix_timestamp":1606485180,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/37913-exam-dp-200-topic-17-question-1-discussion/","topic":"17","isMC":true,"answer_description":"Scenario:\nAn Azure Data Factory pipeline must be used to move data from Cosmos DB to SQL Database for Race Central. If the data load takes longer than 20 minutes, configuration changes must be made to Data Factory.\nThe telemetry data is sent to a MongoDB database. A custom application then moves the data to databases in SQL Server 2017. The telemetry data in MongoDB has more than 500 attributes. The application changes the attribute names when the data is moved to SQL Server 2017.\nYou can copy data to or from Azure Cosmos DB (SQL API) by using Azure Data Factory pipeline.\nColumn mapping applies when copying data from source to sink. By default, copy activity map source data to sink by column names. You can specify explicit mapping to customize the column mapping based on your need. More specifically, copy activity:\nRead the data from source and determine the source schema\n1. Use default column mapping to map columns by name, or apply explicit column mapping if specified.\n2. Write the data to sink\n3. Write the data to sink\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping","question_id":76,"discussion":[{"content":"There is no copy activity that uses a stored procedure\ndelete and filter don't make sense\nAnswer is B","upvote_count":"10","timestamp":"1606485180.0","comment_id":"229053","comments":[{"upvote_count":"1","comment_id":"376275","timestamp":"1623003540.0","content":"Copy activity can use query as a source, you can query a function, which can be understood as a procedure","poster":"maciejt"}],"poster":"syu31svc"},{"poster":"JGECM","content":"Yes, answer is B","timestamp":"1607628720.0","comment_id":"240406","upvote_count":"2"}],"timestamp":"2020-11-27 14:53:00","question_text":"What should you include in the Data Factory pipeline for Race Central?","answer_images":[],"exam_id":65,"question_images":[],"choices":{"D":"a filter activity that has a condition","A":"a copy activity that uses a stored procedure as a source","B":"a copy activity that contains schema mappings","C":"a delete activity that has logging enabled"}},{"id":"O45N7DUiXeO02kCwtk7C","exam_id":65,"question_text":"Which windowing function should you use to perform the streaming aggregation of the sales data?","answers_community":[],"answer":"A","topic":"18","isMC":true,"timestamp":"2021-05-19 10:39:00","answer_ET":"A","question_id":77,"unix_timestamp":1621413540,"answer_description":"Scenario: The analytic process will perform aggregations that must be done continuously, without gaps, and without overlapping.\nThe key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window.\nIncorrect Answers:\nB, C: Like hopping windows, events can belong to more than one sliding window.\nD: Session windows can have gaps.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions","answer_images":[],"discussion":[{"timestamp":"1621413540.0","content":"without overlapping. so answer A are correct","poster":"hoangton","comment_id":"361180","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/microsoft/view/53101-exam-dp-200-topic-18-question-1-discussion/","question_images":[],"choices":{"D":"Session","A":"Tumbling","C":"Sliding","B":"Hopping"}},{"id":"QjUxrkP1mphGiH48Mfgo","answer":"","exam_id":65,"answer_description":"Scenario: A daily process creates reporting data in REPORTINGDB from the data in SALESDB. The process is implemented as a SQL Server Integration\nServices (SSIS) package that runs a stored procedure from SALESDB.\nStep 1: Create a linked service to each database\n\nStep 2: Create two datasets -\nYou can create two datasets: InputDataset and OutputDataset. These datasets are of type AzureBlob. They refer to the Azure Storage linked service that you created in the previous section.\n\nStep 3: Create a pipeline -\nYou create and validate a pipeline with a copy activity that uses the input and output datasets.\n\nStep 4: Add a copy activity -\nReferences:\nhttps://docs.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0028600001.jpg"],"question_id":78,"answer_ET":"","question_text":"DRAG DROP -\nYou need to replace the SSIS process by using Data Factory.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","unix_timestamp":1619303460,"timestamp":"2021-04-25 00:31:00","discussion":[{"comment_id":"342242","timestamp":"1619303460.0","content":"Step 1: Create a pipeline\nStep 2: Create a linked service to each database\nStep 3: Create two datasets\nStep 4: Add a copy activity","comments":[{"content":"More than 1 sequence is valid","comment_id":"376277","upvote_count":"2","timestamp":"1623003720.0","poster":"maciejt"}],"upvote_count":"10","poster":"Wendy_DK"},{"content":"1. Create an Azure Data Factory\n Create linked services to connect to the data resources\n2. Create/Access Data sets\n3. Define and create the pipeline\n4. Copy Activity\nhttps://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf","poster":"vrmei","upvote_count":"2","timestamp":"1623067320.0","comment_id":"376752"},{"content":"The answer provided seems to be wrong. The stored procedure activity in ADF needs to be invoked as existing environment has a stored procedure - \"The process is implemented as a SQL Server Integration Services (SSIS) package that runs a stored procedure from SALESDB.\"","comment_id":"373170","poster":"davem0193","comments":[{"upvote_count":"1","poster":"maciejt","timestamp":"1623003780.0","content":"But the answer didn;t include stored proc","comment_id":"376278"}],"timestamp":"1622686260.0","upvote_count":"1"},{"content":"The answer provided is the correct one.","poster":"dbdev","timestamp":"1619958180.0","upvote_count":"4","comment_id":"347700"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0028500001.jpg"],"isMC":false,"topic":"18","url":"https://www.examtopics.com/discussions/microsoft/view/50905-exam-dp-200-topic-18-question-2-discussion/"},{"id":"TU1IxvBHGlNj52wHJI5Q","answer_ET":"B","question_images":[],"isMC":true,"choices":{"A":"Yes","B":"No"},"exam_id":65,"url":"https://www.examtopics.com/discussions/microsoft/view/65327-exam-dp-200-topic-2-question-1-discussion/","question_text":"Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.\nYou need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.\nSolution:\n1. Create an external data source pointing to the Azure storage account\n2. Create a workload group using the Azure storage account name as the pool name\n3. Load the data using the INSERT`¦SELECT statement\nDoes the solution meet the goal?","answer_description":"You need to create an external file format and external table using the external data source.\nYou then load the data using the CREATE TABLE AS SELECT statement.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store","answer_images":[],"discussion":[{"content":"This is correct, please refer to: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store","upvote_count":"1","timestamp":"1635855840.0","comment_id":"471626","poster":"certstowinirl"}],"question_id":79,"topic":"2","answers_community":[],"answer":"B","timestamp":"2021-11-02 13:24:00","unix_timestamp":1635855840},{"id":"e6FBu0Qg7EwJYTD07Qzh","answer":"B","isMC":true,"exam_id":65,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/33323-exam-dp-200-topic-2-question-10-discussion/","discussion":[{"timestamp":"1608451260.0","content":"A workload that data scientists will use to perform ad hoc analysis in Scala and R. High Concurrency clusters don't support Scala. Answer is \"No\".","comment_id":"248437","poster":"ACSC","upvote_count":"9"},{"comment_id":"345948","content":"Definitely the answer is NO","timestamp":"1619765940.0","upvote_count":"1","poster":"cadio30"},{"content":"A workload that data scientists will use to perform ad hoc analysis in Scala and R. High Concurrency clusters don't support Scala. Answer is \"No\".","upvote_count":"2","comment_id":"336836","poster":"Hassan_Mazhar_Khan","timestamp":"1618559040.0"},{"comments":[{"content":"sorry, its \"no\", because for data Scientist should be Standard cluster","comment_id":"293430","poster":"watata","upvote_count":"1","timestamp":"1613657460.0"}],"poster":"watata","timestamp":"1613657280.0","content":"Answers should be \"yes\"...","comment_id":"293428","upvote_count":"1"},{"upvote_count":"2","comment_id":"291629","poster":"karishura","timestamp":"1613466720.0","content":"No - Right answer"},{"timestamp":"1601541840.0","content":"\"There is no need for a High concurrency cluster\" does not mean, it does not fulfil the goal. High concurrency clusters can be used for data scientists workload. It is just more expensive. There is no requirement regarding keeping the costs low, mentioned in the question. So the correct answer to this question is also \"A - Yes.\"","upvote_count":"3","poster":"M0e","comment_id":"190865"}],"choices":{"B":"No","A":"Yes"},"question_id":80,"answer_images":[],"unix_timestamp":1601541840,"topic":"2","answer_ET":"B","timestamp":"2020-10-01 10:44:00","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\n✑ A workload for data engineers who will use Python and SQL\n✑ A workload for jobs that will run notebooks that use Python, Scala, and SQL\n✑ A workload that data scientists will use to perform ad hoc analysis in Scala and R\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\n✑ The data engineers must share a cluster.\n✑ The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\n✑ All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\nYou need to create the Databricks clusters for the workloads.\nSolution: You create a High Concurrency cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.\nDoes this meet the goal?","answer_description":"No need for a High Concurrency cluster for each data scientist.\nStandard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.\nA high concurrency cluster is a managed cloud resource. The key benefits of high concurrency clusters are that they provide Apache Spark-native fine-grained sharing for maximum resource utilization and minimum query latencies.\nReferences:\nhttps://docs.azuredatabricks.net/clusters/configure.html","answers_community":[]}],"exam":{"isBeta":false,"isImplemented":true,"id":65,"numberOfQuestions":228,"name":"DP-200","provider":"Microsoft","isMCOnly":false,"lastUpdated":"12 Apr 2025"},"currentPage":16},"__N_SSP":true}