{"pageProps":{"questions":[{"id":"4PY04OIzz5cmSl3QHf5u","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/14754-exam-az-203-topic-11-question-4-discussion/","question_text":"DRAG DROP -\nYou need to implement the Log policy.\nHow should you complete the Azure Event Grid subscription? To answer, drag the appropriate JSON segments to the correct locations. Each JSON segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:","topic":"11","timestamp":"2020-02-23 20:19:00","discussion":[{"upvote_count":"24","content":"Options: https://www.examtopics.com/assets/media/exam-media/02522/0008200001.png","timestamp":"1583688360.0","poster":"purav1009","comment_id":"60764"},{"timestamp":"1607788320.0","comment_id":"241709","content":"I got this question in my AZ-204 exam","poster":"MalaSvinjica","upvote_count":"10"},{"timestamp":"1612245240.0","upvote_count":"1","comment_id":"281644","content":"https://stackoverflow.com/questions/64030748/webhook-url-for-azure-eventgrid-subscription-with-azure-bicep-fails","poster":"Bubbles"},{"comment_id":"259667","timestamp":"1609788900.0","comments":[{"comment_id":"285795","timestamp":"1612731120.0","content":"Same confusion here. Question asks about log policy, but answer is about anomaly detection","poster":"cbn","upvote_count":"1"}],"content":"I find this confusing.\nThe solution presented here will invoke the WebHook for every log file archived in logdrop, right? But the case study says, the WebHook for mail notification is only supposed to be invoked once an anomaly is detected. So where is the Anomaly detection here?\n\n(By the way... At first sight, I thought the purpose of this subscription would be to store incoming logs in the container \"logdrop\". But this must also be wrong...)","upvote_count":"3","poster":"bugimachi"},{"poster":"Subhijith","upvote_count":"5","comment_id":"166550","content":"Answer given is correct...","timestamp":"1598430300.0"},{"upvote_count":"6","timestamp":"1596169200.0","content":"I got this question in my exam","poster":"triptimandal01","comment_id":"147800"}],"answer":"Explanation","isMC":false,"answer_ET":"Explanation","answer_description":"Box 1:WebHook -\nScenario: If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook. endpointType: The type of endpoint for the subscription (webhook/HTTP, Event Hub, or queue).\n\nBox 2: SubjectBeginsWith -\nBox 3: Microsoft.Storage.BlobCreated\n\nScenario: Log Policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nExample subscription schema -\n{\n\"properties\": {\n\"destination\": {\n\"endpointType\": \"webhook\",\n\"properties\": {\n\"endpointUrl\": \"https://example.azurewebsites.net/api/HttpTriggerCSharp1?code=VXbGWce53l48Mt8wuotr0GPmyJ/nDT4hgdFj9DpBiRt38qqnnm5OFg==\"\n}\n},\n\"filter\": {\n\"includedEventTypes\": [ \"Microsoft.Storage.BlobCreated\", \"Microsoft.Storage.BlobDeleted\" ],\n\"subjectBeginsWith\": \"blobServices/default/containers/mycontainer/log\",\n[1]\n\"isSubjectCaseSensitive \": \"true\"\n}\n}\n}\nReferences:\nhttps://docs.microsoft.com/en-us/azure/event-grid/subscription-creation-schema","question_id":16,"exam_id":47,"unix_timestamp":1582485540,"answers_community":[],"answer_images":[]},{"id":"gkbA9VyoSyntPT3wvzOZ","discussion":[{"poster":"sinh","upvote_count":"4","content":"https://www.examtopics.com/assets/media/exam-media/02838/n41147000003.jpg","comment_id":"192672","timestamp":"1601786160.0"},{"upvote_count":"1","timestamp":"1585898700.0","content":"Also: how does registring event id's at a different level help fill the requirements?","poster":"realbart","comment_id":"70664"},{"content":"public void Initialize(ITelemetry telemetry)\n {\n // 'TelemetryContext.Properties' is Obsolete:\n // 'Use GlobalProperties to set global level properties. For properties at item level, use ISupportProperties.Properties.'\n //telemetry.Context.Properties[\"EventId\"] = ((EventTelemetry)telemetry).Properties[\"EventId\"];\n \n telemetry.Context.GlobalProperties[\"EventId\"] = ((EventTelemetry)telemetry).Properties[\"EventId\"];\n }","timestamp":"1585898400.0","upvote_count":"1","poster":"realbart","comment_id":"70662"},{"comment_id":"60765","poster":"purav1009","content":"Options please :-)","timestamp":"1583688480.0","upvote_count":"1"}],"timestamp":"2020-02-25 11:41:00","answer":"Explanation","question_images":[],"answer_description":"Scenario: You have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\n✑ Exclude non-user actions from Application Insights telemetry.\n✑ Provide methods that allow a web service to scale itself.\n✑ Ensure that scaling actions do not disrupt application usage.\n\nBox 1: ITelemetryInitializer -\nUse telemetry initializers to define global properties that are sent with all telemetry; and to override selected behavior of the standard telemetry modules.\n\nBox 2: Initialize -\n\nBox 3: Telemetry.Context -\nBox 4: [(EventTelemetry)telemetry.Properties(\"EventID\")\nReferences:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling","answer_ET":"Explanation","question_id":17,"url":"https://www.examtopics.com/discussions/microsoft/view/14841-exam-az-203-topic-11-question-5-discussion/","topic":"11","unix_timestamp":1582627260,"question_text":"DRAG DROP -\nYou need to ensure that PolicyLib requirements are met.\nHow should you complete the code segment? To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:","isMC":false,"answers_community":[],"answer_images":[],"exam_id":47},{"id":"17ABwNv6hXFWbd0xCHZv","unix_timestamp":1582616580,"discussion":[{"poster":"bombjack70","timestamp":"1584462120.0","content":"should be c in the requirements \"Application Insights must always contain all log\nmessages.\"","upvote_count":"11","comment_id":"65231","comments":[{"content":"I agreed the answer should be C, but with a different reason.\n\nOption B & D are both wrong because sampling is only useful to reduce telemetry data (CPU %, # of requests, etc); the issue in this question however is about trace output (User created! File uploaded! Permission denied! etc), not telemetry.","comments":[{"comments":[{"upvote_count":"1","content":"This is not a capacity issue question but Logging. One of the requirements is that App Insights must always contain all log messages. The issue is lost log msgs.\nI'll go with Sampling.","timestamp":"1608924180.0","poster":"Cornholioz","comment_id":"252252"}],"content":"Capacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nbusy hours and long delays doesn't impose CPU and therefore telemetry? \nim confused","timestamp":"1608403080.0","poster":"ahadjithoma","comment_id":"248144","upvote_count":"1"}],"timestamp":"1590002520.0","comment_id":"92942","poster":"lau13","upvote_count":"3"}]},{"timestamp":"1598048220.0","poster":"greentim","upvote_count":"7","comment_id":"163206","content":"I think the answer is correct it is D:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling\nAdaptive sampling\nAdaptive sampling affects the volume of telemetry sent from your web server app to the Application Insights service endpoint."},{"poster":"walexkino","timestamp":"1610461380.0","upvote_count":"7","comment_id":"265558","content":"Alot of ppl just come here to confuse themselves darm it"},{"upvote_count":"1","timestamp":"1608403200.0","comment_id":"248145","content":"I think it's correct \nhttps://docs.microsoft.com/en-us/azure/azure-monitor/learn/tutorial-performance\n\nFind and diagnose performance issues with Azure Application Insights\nIdentify the performance of server-side operations\nAnalyze server operations to determine the root cause of slow performance\nIdentify slowest client-side operations\nAnalyze details of page views using query language","poster":"ahadjithoma"},{"poster":"031920","content":"public async static Task<SqlDataReader> ExecuteReaderWithRetryAsync(this SqlCommand command)\n{\n GuardConnectionIsNotNull(command);\n\n var policy = Policy.Handle<Exception>().WaitAndRetryAsync(\n retryCount: 3, // Retry 3 times\n sleepDurationProvider: attempt => TimeSpan.FromMilliseconds(200 * Math.Pow(2, attempt - 1)), // Exponential backoff based on an initial 200 ms delay.\n onRetry: (exception, attempt) =>\n {\n // Capture some information for logging/telemetry.\n logger.LogWarn($\"ExecuteReaderWithRetryAsync: Retry {attempt} due to {exception}.\");\n });\n\n // Retry the following call according to the policy.\n await policy.ExecuteAsync<SqlDataReader>(async token =>\n {\n // This code is executed within the Policy\n\n if (conn.State != System.Data.ConnectionState.Open) await conn.OpenAsync(token);\n return await command.ExecuteReaderAsync(System.Data.CommandBehavior.Default, token);\n\n }, cancellationToken);\n}","timestamp":"1598664540.0","upvote_count":"2","comment_id":"168872"},{"timestamp":"1582616580.0","upvote_count":"5","poster":"joilec435","comment_id":"54867","content":"maybe change minumum log level?"}],"isMC":true,"answer_ET":"D","question_id":18,"answer":"D","answer_description":"Scenario, the log capacity issue: Developers report that the number of log message in the trace output for the processor is too high, resulting in lost log messages.\nSampling is a feature in Azure Application Insights. It is the recommended way to reduce telemetry traffic and storage, while preserving a statistically correct analysis of application data. The filter selects items that are related, so that you can navigate between items when you are doing diagnostic investigations. When metric counts are presented to you in the portal, they are renormalized to take account of the sampling, to minimize any effect on the statistics.\nSampling reduces traffic and data costs, and helps you avoid throttling.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling","exam_id":47,"answers_community":[],"answer_images":[],"topic":"12","question_text":"You need to resolve the log capacity issue.\nWhat should you do?","choices":{"B":"Create an Application Insights Telemetry Filter.","A":"Set a LogCategoryFilter during startup.","D":"Implement Application Insights Sampling.","C":"Change the minimum log level in the host.json file for the function."},"timestamp":"2020-02-25 08:43:00","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/14836-exam-az-203-topic-12-question-1-discussion/"},{"id":"3eB0s51ItIt1gq96YhD7","answer_ET":"D","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/11870-exam-az-203-topic-12-question-2-discussion/","question_id":19,"choices":{"B":"Convert the trigger on the Azure Function to a File Trigger.","A":"Move the Azure Function to a dedicated App Service Plan.","D":"Update the loop starting on line PC09 to process items in parallel.","C":"Ensure that the consumption plan is configured correctly to allow for scaling."},"answer_images":["https://www.examtopics.com/assets/media/exam-media/02838/0018400001.png"],"discussion":[{"content":"A scaling in this case brings nothing. This Function is called once in 5 min and get a huge number of files to be processed. \nIt’ possible to convert the loop to call an parallel processing of files or just convert a function to be triggered from files. I think both B and D can be an answer.","timestamp":"1586589180.0","poster":"boldarev","upvote_count":"15","comment_id":"73191"},{"upvote_count":"5","content":"A is the answer. You can scale out and up with a dedicate App Service Plan.\n\"You need more CPU or memory options than what is provided on the Consumption plan.\"\nhttp://twocents.nl/?p=2078\n\nD is not the answer for this reason:\n\"... will depend on the number of cores the machine has, and you can't control that when you're deployed to the consumption plan.\"\nhttps://github.com/Azure/Azure-Functions/issues/815\n\nC is also not the answer because auto-scaling a default for consumption plans. You don't need to configuration anything, so it is already on for this scenario:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan\n\nB is also not the answer, as there is not file share trigger for Azure Functions.\nhttps://stackoverflow.com/questions/50872265/is-there-any-trigger-for-azure-file-share-in-azure-functions-or-azure-logic-app","poster":"ExamStudent123","timestamp":"1596683340.0","comment_id":"151635"},{"timestamp":"1642960620.0","poster":"edengoforit","upvote_count":"1","content":"Cleared AZ-204 today, the question appeared, the option \"D\" was not there","comment_id":"530727"},{"poster":"sinh","content":"The correct answer is D. A is incorrect. TimerTrigger does not get faster when you change the plan.","comment_id":"205469","comments":[{"upvote_count":"3","timestamp":"1610917920.0","poster":"Juanlu","content":"OK, so the correct one is D:\n- A (incorrect): TimerTrigger does not get faster when you change the plan.\n- C (incorrect): Current situation, so, same reason as before (A).\n- B (Incorrect): There is not file share trigger for Azure Functions","comment_id":"269833"}],"upvote_count":"3","timestamp":"1603606620.0"},{"timestamp":"1602480960.0","upvote_count":"2","comment_id":"198156","content":"Read the explanation:\nIf you want to read the files in parallel, you cannot use forEach. \n\nThen read at stackoverflow:\nIt says you can not use foreach - you need to use for\n\nHence correct answer D - switch your loop from foreach to a for loop.","poster":"ExamPwnr"},{"comment_id":"133975","poster":"chaudh","content":"It's C.\n\nBase on the fact \"During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\", the capacity issue doesn't come from the code algorithm, the root cause is underlying infrastructure of Azure function.\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#hosting-plans-comparison","upvote_count":"2","timestamp":"1594647540.0"},{"content":"The only answer I am sure is not right is D. Why? Look into requirements:\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDoes anyone know which answer is correct?","poster":"barubs","comments":[{"content":"Concurrent != Parallel","comment_id":"113593","timestamp":"1592533140.0","upvote_count":"6","poster":"PHARKGAOYOU"},{"poster":"rrongcheng","upvote_count":"4","timestamp":"1595511060.0","content":"I think that means 2 parallel processing on the same receipt.","comment_id":"142044"}],"timestamp":"1589704980.0","comment_id":"90439","upvote_count":"2"},{"poster":"dabako1435","timestamp":"1588464300.0","content":"It doesn't look like there is azure fileshare triggers, there is triggers for blob storage\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings\nhttps://stackoverflow.com/questions/50872265/is-there-any-trigger-for-azure-file-share-in-azure-functions-or-azure-logic-app\n\nAlso if azure fileshare trigger is supported and many files got dropped at once it would trigger many instances of the function which could have constraints.\n\nIt seems it would be easier to run the function once every x minutes and process whats there in parallel\n\nAlso this issue happens \"During busy periods\" so on non busy periods the user is fine to wait the 5 minutes that it could take for the timer to trigger to process the file","upvote_count":"4","comment_id":"82813"},{"timestamp":"1585900440.0","content":"Can be rewritten to someting like:\n\npublic static Task Run([TimeTrigger(\"0 /5 * * * *\")] TimerInfo timer, ILogger log)\n{\n var container = await GetCloudBlobContainer();\n async Task ProcessFile(ListFileItem fileItem)\n {\n var file = new CloudFile(fileItem.StorageUri.PrimaryUri)\n var ms = new MemoryStream();\n await file.DownloadToStramAsync(ms)\n var blob = container.GetBlockBlobReference(fileItem.Uri.ToString());\n await blob.UploadFromStreamAsync(ms)\n }\n var fileItems = await ListFiles();\n return Task.WhenAll(fileItems.AsParallel().Select(fi => ProcessFile(fi)));","poster":"realbart","upvote_count":"2","comment_id":"70668"},{"content":"Given answer D is Correct","poster":"Dumindu","upvote_count":"3","comment_id":"58260","timestamp":"1583251980.0"},{"content":"C should be the correct solution since it allows scaling","upvote_count":"2","comment_id":"54598","poster":"KK_uniq","timestamp":"1582555620.0"},{"comment_id":"38343","content":"How is this a solution, the stackoverflow link and the promise library are javascript solutions..","poster":"ztx","upvote_count":"2","comments":[{"poster":"TRUESON","upvote_count":"5","content":"The given C# code is also async ... I think in real life switching to a dedicated app service plan with always on would be best","comment_id":"48251","timestamp":"1581242520.0"},{"content":"The C# version:\n1. https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/how-to-write-a-simple-parallel-foreach-loop\n2. https://stackoverflow.com/questions/15136542/parallel-foreach-with-asynchronous-lambda","poster":"rrongcheng","timestamp":"1595511480.0","upvote_count":"1","comment_id":"142050"}],"timestamp":"1578893760.0"}],"unix_timestamp":1578893760,"exam_id":47,"topic":"12","isMC":true,"timestamp":"2020-01-13 06:36:00","answers_community":[],"answer_description":"If you want to read the files in parallel, you cannot use forEach. Each of the async callback function calls does return a promise. You can await the array of promises that you'll get with Promise.all.\nScenario: Capacity issue: During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nReferences:\nhttps://stackoverflow.com/questions/37576685/using-async-await-with-a-foreach-loop","question_text":"You need to resolve the capacity issue.\nWhat should you do?","answer":"D"},{"id":"08asZDQbtwfIe735AN6c","topic":"12","answer_images":[],"question_text":"You need to ensure receipt processing occurs correctly.\nWhat should you do?","answer":"B","exam_id":47,"isMC":true,"question_images":[],"choices":{"D":"Use blob properties to prevent concurrency problems.","C":"Use blob leases to prevent concurrency problems.","A":"Use blob metadata to prevent concurrency problems.","B":"Use blob SnapshotTime to prevent concurrency problems."},"answers_community":[],"unix_timestamp":1573817760,"url":"https://www.examtopics.com/discussions/microsoft/view/8263-exam-az-203-topic-12-question-3-discussion/","question_id":20,"answer_description":"You can create a snapshot of a blob. A snapshot is a read-only version of a blob that's taken at a point in time. Once a snapshot has been created, it can be read, copied, or deleted, but not modified. Snapshots provide a way to back up a blob as it appears at a moment in time.\nScenario: Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in\nAzure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\nReferences:\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/creating-a-snapshot-of-a-blob","timestamp":"2019-11-15 12:36:00","answer_ET":"B","discussion":[{"comment_id":"21775","comments":[{"timestamp":"1653681240.0","poster":"SoftwareEngineeringMaster","upvote_count":"1","comment_id":"608186","content":"A point-in-time snapshot is a copy of a storage volume, file or database as they appeared at a given point in time and are used as method of data protection. In the event of a failure, users can restore their data from the most recent snapshot before the failure. Many point-in-time snapshots are read-only.\n\nSo C is not correct"}],"content":"answer should be C","upvote_count":"49","timestamp":"1573817820.0","poster":"IrfanSheikh"},{"content":"I agree, answer should be C, using Lease","timestamp":"1576009620.0","poster":"Regimiento","comment_id":"28667","upvote_count":"15"},{"comment_id":"151638","timestamp":"1596684060.0","upvote_count":"9","poster":"ExamStudent123","comments":[{"upvote_count":"3","poster":"cbn","comment_id":"286013","timestamp":"1612767540.0","content":"Question is about \"processing\" the receipt. Email is only sent after the processing."}],"content":"B is correct.\n\"The link to the report must remain valid if the email is forwarded to another user.\"\nWith a snapshot, it can never be updated. With a lease, someone could release the lease and modify the blob. Then the URL would return not return the original BLOB"},{"poster":"ather_13","upvote_count":"4","comment_id":"117892","content":"\"C\" Leases should be answer\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-concurrency","timestamp":"1592950980.0"},{"content":"should be lease","timestamp":"1582556040.0","upvote_count":"5","poster":"KK_uniq","comment_id":"54600"},{"content":"Agree , it is mentioned concurrent processing of receipt should be prevented","timestamp":"1582485240.0","comment_id":"54238","upvote_count":"5","poster":"Vkv"},{"content":"Concurrency can be achieved by either using Lease or using ETag(or last updated time) .","comment_id":"21774","poster":"IrfanSheikh","timestamp":"1573817760.0","upvote_count":"8"}]}],"exam":{"id":47,"numberOfQuestions":144,"isBeta":false,"isMCOnly":false,"lastUpdated":"12 Apr 2025","provider":"Microsoft","isImplemented":true,"name":"AZ-203"},"currentPage":4},"__N_SSP":true}