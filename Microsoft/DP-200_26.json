{"pageProps":{"questions":[{"id":"srUwgEeJdb8vfsJdJ2m7","timestamp":"2021-04-13 12:23:00","isMC":false,"answers_community":[],"answer_ET":"","discussion":[{"upvote_count":"5","comment_id":"334569","content":"imo. the answer makes sense.","timestamp":"1618309380.0","poster":"Fippu"},{"timestamp":"1622246700.0","content":"to me it should be sliding windows, because if it's tumbling it will return only the last vehicle infor, using sliding windows will return every vehicle's infor passing by the booth, not just the last one","comments":[{"comment_id":"369078","content":"the question is a little ambiguous, if it needs to return every event in a window of 10m, it is Sliding windows, but if it only needs the last event in a window of 10m, so it is Tumbling windows","poster":"NewTuanAnh","timestamp":"1622246940.0","upvote_count":"2"}],"comment_id":"369077","poster":"NewTuanAnh","upvote_count":"1"},{"upvote_count":"1","comment_id":"337996","comments":[{"poster":"Mily94","upvote_count":"4","comment_id":"342772","content":"because you don't care about the number of vehicles, you need to find max timestamp to properly calculate 10 minutes diff.","timestamp":"1619374560.0"}],"content":"Why not COUNT ?","poster":"Alka3","timestamp":"1618729740.0"}],"exam_id":65,"unix_timestamp":1618309380,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0025100001.png"],"answer":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0025200001.png"],"topic":"2","question_text":"HOTSPOT -\nYou are processing streaming data from vehicles that pass through a toll booth.\nYou need to use Azure Stream Analytics to return the license plate, vehicle make, and hour the last vehicle passed during each 10-minute window.\nHow should you complete the query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/50007-exam-dp-200-topic-2-question-54-discussion/","answer_description":"Box 1: MAX -\nThe first step on the query finds the maximum time stamp in 10-minute windows, that is the time stamp of the last event for that window. The second step joins the results of the first query with the original stream to find the event that match the last time stamps in each window.\nQuery:\n\nWITH LastInWindow AS -\n(\n\nSELECT -\n\nMAX(Time) AS LastEventTime -\n\nFROM -\n\nInput TIMESTAMP BY Time -\n\nGROUP BY -\nTumblingWindow(minute, 10)\n)\n\nSELECT -\nInput.License_plate,\nInput.Make,\n\nInput.Time -\n\nFROM -\n\nInput TIMESTAMP BY Time -\n\nINNER JOIN LastInWindow -\nON DATEDIFF(minute, Input, LastInWindow) BETWEEN 0 AND 10\nAND Input.Time = LastInWindow.LastEventTime\n\nBox 2: TumblingWindow -\nTumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals.\n\nBox 3: DATEDIFF -\nDATEDIFF is a date-specific function that compares and returns the time difference between two DateTime fields, for more information, refer to date functions.\nReference:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics","question_id":126},{"id":"wsjW9juZrKU8jEjUtdVS","answer_description":"Step 1: Add an Azure Stream Analytics Custom Deserializer Project (.NET) project to the solution.\n\nCreate a custom deserializer -\n1. Open Visual Studio and select File > New > Project. Search for Stream Analytics and select Azure Stream Analytics Custom Deserializer Project (.NET). Give the project a name, like Protobuf Deserializer.\n\n2. In Solution Explorer, right-click your Protobuf Deserializer project and select Manage NuGet Packages from the menu. Then install the\nMicrosoft.Azure.StreamAnalytics and Google.Protobuf NuGet packages.\n3. Add the MessageBodyProto class and the MessageBodyDeserializer class to your project.\n4. Build the Protobuf Deserializer project.\nStep 2: Add .NET deserializer code for Protobuf to the custom deserializer project\nAzure Stream Analytics has built-in support for three data formats: JSON, CSV, and Avro. With custom .NET deserializers, you can read data from other formats such as Protocol Buffer, Bond and other user defined formats for both cloud and edge jobs.\nStep 3: Add an Azure Stream Analytics Application project to the solution\nAdd an Azure Stream Analytics project\n1. In Solution Explorer, right-click the Protobuf Deserializer solution and select Add > New Project. Under Azure Stream Analytics > Stream Analytics, choose\nAzure Stream Analytics Application. Name it ProtobufCloudDeserializer and select OK.\n2. Right-click References under the ProtobufCloudDeserializer Azure Stream Analytics project. Under Projects, add Protobuf Deserializer. It should be automatically populated for you.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer","timestamp":"2021-06-14 15:08:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0025500001.png","https://www.examtopics.com/assets/media/exam-media/03872/0025600001.jpg"],"discussion":[{"upvote_count":"6","poster":"emski","content":"I think for the last step it should be to \"change event serialization and reference the dll\" since the stream analytics project is assumed to already exist.","comments":[{"content":"I agree, it seems more logical","comment_id":"394773","upvote_count":"1","poster":"Marcello83","timestamp":"1625060280.0"}],"comment_id":"388467","timestamp":"1624419960.0"},{"upvote_count":"2","comment_id":"381889","content":"Hmmmm answer seems right","timestamp":"1623676080.0","poster":"Hotjo"}],"question_text":"DRAG DROP -\nYou have an Azure Stream Analytics job that is a Stream Analytics project solution in Microsoft Visual Studio. The job accepts data generated by IoT devices in the JSON format.\nYou need to modify the job to accept data generated by the IoT devices in the Protobuf format.\nWhich three actions should you perform from Visual Studio in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","isMC":false,"topic":"2","answer_ET":"","unix_timestamp":1623676080,"exam_id":65,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0025400001.png"],"answer":"","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/55316-exam-dp-200-topic-2-question-55-discussion/","question_id":127},{"id":"nEpbcnhzhvA51lM3xK0g","answer_description":"Box 1: Premium -\nCredential passthrough requires an Azure Databricks Premium Plan.\nIncorrect Answers:\nSupport for Azure Data Lake Storage credential passthrough on standard clusters is in Public Preview.\nStandard clusters with credential passthrough are supported on Databricks Runtime 5.5 and above and are limited to a single user.\nNode: Azure Databricks supports three cluster modes: Standard, High Concurrency, and Single Node.\nBox 2: Azure Data Lake Storage Gen1 Credential Passthrough\nYou can authenticate automatically to Azure Data Lake Storage Gen1 and Azure Data Lake Storage Gen2 from Azure Databricks clusters using the same Azure\nActive Directory (Azure AD) identity that you use to log into Azure Databricks. When you enable your cluster for Azure Data Lake Storage credential passthrough, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage.\nReference:\nhttps://docs.azuredatabricks.net/spark/latest/data-sources/azure/adls-passthrough.html","exam_id":65,"answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/51738-exam-dp-200-topic-2-question-57-discussion/","topic":"2","isMC":false,"discussion":[{"comment_id":"348945","upvote_count":"5","poster":"suvenk","content":"this will be standard cluster not premium. For credential passthrough you only need a azure databricks premium plan but not premium cluster.","comments":[{"comment_id":"354439","content":"i agree with the statement, cluster mode options are standard, high concurrency and single node. Thus, it make sense the choose \"STANDARD\" in the first box. Azure Databricks premium plan is needed to enable the passthrough feature.\n\nReference: https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough","poster":"cadio30","upvote_count":"5","timestamp":"1620718380.0"}],"timestamp":"1620073860.0"},{"content":"Credential passthrough requires an Azure Databricks Premium Plan. See Upgrade or Downgrade an Azure Databricks Workspace for details on upgrading a standard plan to a premium plan\nhttps://docs.microsoft.com/en-gb/azure/databricks/security/credential-passthrough/adls-passthrough","timestamp":"1628880780.0","comment_id":"424509","upvote_count":"1","poster":"Ankush1994"},{"poster":"vrmei","timestamp":"1623003360.0","comment_id":"376273","upvote_count":"1","content":"Given explanation is correct: But as mentioned Azure Databricks supports three cluster modes: Standard, High Concurrency, and Single Node.\n\nThere is no premium cluster mode, thus we need to go with standard"},{"content":"From DataBricks Version 7.3, passthrough is supported on Standard clusters.","upvote_count":"3","timestamp":"1621614060.0","poster":"Hevz","comment_id":"363166"},{"comment_id":"361097","poster":"memo43","content":"1-standard\n2-data lake","upvote_count":"1","timestamp":"1621406880.0"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0025800001.jpg"],"question_text":"HOTSPOT -\nYou need to implement an Azure Databricks cluster that automatically connects to Azure Data Lake Storage Gen2 by using Azure Active Directory (Azure AD) integration.\nHow should you configure the new cluster? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0025800002.jpg"],"question_id":128,"answer":"","timestamp":"2021-05-03 22:31:00","unix_timestamp":1620073860,"answers_community":[]},{"id":"NXrsiy4IAOyBZAJelSo0","discussion":[{"content":"Answer is correct","upvote_count":"3","timestamp":"1623344460.0","comment_id":"379167","poster":"Jasvir"}],"answer":"","exam_id":65,"question_text":"HOTSPOT -\nYou are building an Azure Stream Analytics query that will receive input data from Azure IoT Hub and write the results to Azure Blob storage.\nYou need to calculate the difference in readings per sensor per hour.\nHow should you complete the query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0026000001.jpg"],"url":"https://www.examtopics.com/discussions/microsoft/view/55069-exam-dp-200-topic-2-question-58-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0026000002.jpg"],"timestamp":"2021-06-10 19:01:00","answer_description":"Box 1: LAG -\nThe LAG analytic operator allows one to look up a ג€previousג€ event in an event stream, within certain constraints. It is very useful for computing the rate of growth of a variable, detecting when a variable crosses a threshold, or when a condition starts or stops being true.\n\nBox 2: LIMIT DURATION -\nExample: Compute the rate of growth, per sensor:\nSELECT sensorId,\ngrowth = reading -\nLAG(reading) OVER (PARTITION BY sensorId LIMIT DURATION(hour, 1))\n\nFROM input -\nReference:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics","question_id":129,"answer_ET":"","isMC":false,"topic":"2","unix_timestamp":1623344460,"answers_community":[]},{"id":"Uu9ihGeFM5PEigVX6odB","topic":"2","unix_timestamp":1618757580,"timestamp":"2021-04-18 16:53:00","isMC":true,"discussion":[{"content":"This solution meet the goal, but it is not an optimized solution.\nThe answer should be Yes","comment_id":"354024","timestamp":"1620669120.0","poster":"SorinXp","upvote_count":"2"},{"comment_id":"350140","content":"1) Create the target table to load data from Azure Data Lake Storage.\n2) Use the COPY statement to load data into the data warehouse.\n\nt-sql COPY statement can directly read Parquet\n\n\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/copy-into-transact-sql?view=azure-sqldw-latest&preserve-view=true","timestamp":"1620205800.0","poster":"MMM777","upvote_count":"1"},{"timestamp":"1619759880.0","content":"No, instead of using data factory, use external data source and it requires creation of master key and database scope credential.","upvote_count":"2","poster":"cadio30","comment_id":"345898"},{"poster":"TJ0071","comments":[{"content":"No, the target table doesn't exist so insert statement won't work.","poster":"hello_there_","upvote_count":"2","timestamp":"1624448760.0","comment_id":"388775"}],"content":"The question says does this meet the goal?\nThe solution provided might not be the optimized solution, but does this meet the goal?\nPlease answer.","comment_id":"338269","timestamp":"1618757580.0","upvote_count":"2"}],"answers_community":[],"answer_ET":"B","answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/50408-exam-dp-200-topic-2-question-59-discussion/","exam_id":65,"choices":{"B":"No","A":"Yes"},"question_id":130,"question_images":[],"answer_images":[],"answer_description":"There is no need to convert the parquet files to CSV files.\nYou load the data using the CREATE TABLE AS SELECT statement.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store","question_text":"Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.\nYou need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.\nSolution:\n1. Use Azure Data Factory to convert the parquet files to CSV files\n2. Create an external data source pointing to the Azure storage account\n3. Create an external file format and external table using the external data source\n4. Load the data using the INSERT`¦SELECT statement\nDoes the solution meet the goal?"}],"exam":{"isBeta":false,"isMCOnly":false,"lastUpdated":"12 Apr 2025","numberOfQuestions":228,"isImplemented":true,"id":65,"provider":"Microsoft","name":"DP-200"},"currentPage":26},"__N_SSP":true}