{"pageProps":{"questions":[{"id":"rwhGxTr5t1f6lCFuCUM3","topic":"4","answer_images":[],"answers_community":["D (82%)","A (18%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/60637-exam-dp-203-topic-4-question-8-discussion/","timestamp":"2021-08-25 16:31:00","answer":"D","answer_description":"","answer_ET":"D","choices":{"C":"Connect to Pool1 and query sys.dm_pdw_node_status.","A":"Connect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.","B":"Connect to the built-in pool and run DBCC CHECKALLOC.","D":"Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats."},"exam_id":67,"question_text":"You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a database named DB1. DB1 contains a fact table named Table1.\nYou need to identify the extent of the data skew in Table1.\nWhat should you do in Synapse Studio?","unix_timestamp":1629901860,"isMC":true,"question_id":371,"discussion":[{"upvote_count":"32","poster":"wuespe","content":"The right answer is D, I tested it in Synapse and it's the only one that actually runs without an error","timestamp":"1648124160.0","comments":[{"content":"Use sys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet#distributed-or-replicated-tables:~:text=Use%20sys.dm_pdw_nodes_db_partition_stats%20to%20analyze%20any%20skewness%20in%20the%20data.","comment_id":"1364807","timestamp":"1741077180.0","upvote_count":"1","poster":"nadavw"}],"comment_id":"450820"},{"content":"-- Find data skew for a distributed table\nDBCC PDW_SHOWSPACEUSED('dbo.FactInternetSales');","upvote_count":"18","poster":"wijaz789","timestamp":"1646734380.0","comment_id":"441311","comments":[{"timestamp":"1655137560.0","poster":"ItHYMeRIsh","comment_id":"500813","content":"This will only work if you connect to the dedicated pool. The answer you've chosen says you are connecting to the built-in (serverless) pool.","upvote_count":"11"}]},{"upvote_count":"2","comment_id":"1098739","content":"Selected Answer: D\ndm_pdw_nodes_db_partition_stats because we need to verify it on Pool1 (not built-in pool!)","poster":"d046bc0","timestamp":"1718604540.0"},{"timestamp":"1709199840.0","upvote_count":"1","poster":"kkk5566","comments":[{"poster":"kkk5566","timestamp":"1709977200.0","comment_id":"1003018","content":"You can use DBCC PDW_SHOWSPACEUSED to find the skew, however only on dedicated pools.","upvote_count":"2"}],"comment_id":"994905","content":"Selected Answer: D\ndm_pdw_nodes_db_partition_stats"},{"comment_id":"928812","comments":[{"comment_id":"937179","timestamp":"1703809320.0","upvote_count":"1","poster":"vctrhugo","content":"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet#distributed-or-replicated-tables"}],"content":"Use sys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data.","timestamp":"1703114040.0","poster":"vctrhugo","upvote_count":"1"},{"timestamp":"1697741340.0","poster":"aemilka","content":"Selected Answer: D\nCorrect answer is D.\n\nA quick way to check for data skew is to use DBCC PDW_SHOWSPACEUSED, but DBCC PDW_SHOWSPACEUSED is not supported by serverless SQL pool in Azure Synapse Analytics. So A option can't be performed.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\n\nThe only correct option here is to check sys.dm_pdw_nodes_db_partition_stats using dedicated SQL pool.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet","comments":[{"comment_id":"928990","upvote_count":"2","content":"DBCC PDW_SHOWSPACEUSED is a command that can be used to show space usage information for a Database in an Azure Synapse Analytics dedicated SQL pool. However, it is not the best option for identifying data skew in a specific table.","poster":"JG1984","timestamp":"1703134140.0"}],"comment_id":"875022","upvote_count":"2"},{"comment_id":"794462","upvote_count":"2","content":"A quick way to check for data skew is to use DBCC PDW_SHOWSPACEUSED.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","timestamp":"1690818000.0","poster":"Okea"},{"poster":"Lestrang","content":"This has been explained by others, but not clear enough to get it. I certainly had to look around and ponder for a bit. So, to give a more lucid explanation for why this is D and why the later question is DBCC PDW_SHOWSPACEUSED , it comes down to the small differences.\n\nYou can use DBCC PDW_SHOWSPACEUSED to find the skew, however only on dedicated pools. Well if you are like me, you would be shouting WELLL THE QUESTION SAID DEDICATED POOL DUH. But if you read it carefully, it says connect to the \"built-in pool\" AKA serverless pool and run DBCC PDW_SHOWSPACEUSED. \nWell, we ain't in a serverless pool are we? so that leaves D as the solution.\n\nin the other question the given answers are so\nA. Connect to Pool1 and run DBCC PDW_SHOWSPACEUSED.\nB. Connect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.\nC. Connect to Pool1 and run DBCC CHECKALLOC.\nD. Connect to the built-in pool and query sys.dm_pdw_sys_info.\n\nHere we see that db_partition_stats is in a built in, which is a no go, so obviously we use PDW_SHOWSPACEUSED.\n\nHopefully this help any airheaded kindred spirits.","timestamp":"1690360860.0","comment_id":"788618","upvote_count":"11"},{"comment_id":"779353","upvote_count":"2","content":"A is a quicker way, but you can run DBCC in a serverless SQL pool, the built-in pool.","poster":"youngbug","timestamp":"1689623040.0"},{"upvote_count":"1","comment_id":"775114","timestamp":"1689306180.0","poster":"steve7","content":"Right answer is A. DBCC PDW_SHOWSPACEUSED. google it"},{"timestamp":"1676274120.0","upvote_count":"4","comment_id":"646140","poster":"Deeksha1234","content":"Selected Answer: D\nD is correct"},{"upvote_count":"1","timestamp":"1674739680.0","content":"I think that first we need to connect to Pool 1, this excludes the first two options (and especially DBCC PDW_SHOWSPACEUSED). In the other two options, after connecting to Pool1, we execute query sys.dm_pdw_nodes_db_partition_stats.","comment_id":"637364","poster":"Franz58"},{"comment_id":"623114","poster":"StudentFromAus","timestamp":"1672131120.0","upvote_count":"2","content":"Selected Answer: D\nFor dedicated SQL Pool this is the correct answer. \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet"},{"upvote_count":"4","comment_id":"596828","poster":"Andushi","content":"Selected Answer: D\nUse sys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data.\nref: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet","timestamp":"1667574600.0"},{"content":"Selected Answer: A\nDBCC PDW_SHOWSPACEUSED","upvote_count":"1","poster":"FelixI","timestamp":"1666590600.0","comment_id":"590898"},{"timestamp":"1665016200.0","comment_id":"581512","content":"Selected Answer: A\nFirstly, this is for DEDICATED SQL Pool.\nHere is what both likely outputs give you:\nsys.dm_pdw_nodes_db_partition_stats:\nobject_id, partition_id, in_row_data_page_count, in_row_used_page_count\nThese columns are not useful in identifying skew\n\nHowever, if you're using PDW_SHOWSPACEUSED:\nROWS, RESERVED_SPACE, DATA_SPACE, INDEX_SPACE, UNUSED_SPACE\nThese columns are definitely useful in identifying skew as you can calculate the Space allocation per row and look at any unused space","poster":"AlCubeHead","upvote_count":"1"},{"poster":"ladywhiteadder","timestamp":"1664624160.0","content":"Selected Answer: D\nA does not work as in this answer we connect to the build in pool NOT the dedicated pool. This leaves D as valid option","comments":[{"comment_id":"581515","upvote_count":"1","comments":[{"content":"Please read the answer options carefully. In options A + B, you connect to the serverless SQL pool, in options C + D, you connect to the dedicated SQL pool.","upvote_count":"4","comment_id":"585107","timestamp":"1665650760.0","poster":"Amsterliese"}],"timestamp":"1665016320.0","poster":"AlCubeHead","content":"The question specifies dedicated Pool NOT Built-in Pool, so it is A"}],"comment_id":"579448","upvote_count":"3"},{"timestamp":"1661429580.0","comment_id":"556014","upvote_count":"2","poster":"ovokpus","content":"Selected Answer: A\nThis is right from the learning material\n\nhttps://docs.microsoft.com/en-us/learn/modules/analyze-optimize-data-warehouse-storage-azure-synapse-analytics/2-understand-skewed-data-space-usage"},{"timestamp":"1659240000.0","comments":[{"comment_id":"536692","poster":"kilowd","timestamp":"1659240060.0","upvote_count":"3","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet"}],"comment_id":"536690","upvote_count":"5","poster":"kilowd","content":"Selected Answer: D\nUse sys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data."},{"comments":[{"poster":"LiLy91","comment_id":"531416","timestamp":"1658669700.0","upvote_count":"3","content":"Correction, the answer should be D"}],"content":"Selected Answer: A\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","poster":"LiLy91","comment_id":"525196","timestamp":"1657991520.0","upvote_count":"2"},{"poster":"jv2120","comment_id":"509293","upvote_count":"2","content":"ANS D\nhttps://www.red-gate.com/simple-talk/databases/sql-server/bi-sql-server/azure-sql-data-warehouse-explaining-architecture-system-views/","timestamp":"1656186240.0"},{"comment_id":"495758","timestamp":"1654581660.0","upvote_count":"2","poster":"rashjan","content":"Selected Answer: D\nas already mentioned D is the correct answer"},{"upvote_count":"1","content":"A.\nhttps://github.com/rgl/azure-content/blob/master/articles/sql-data-warehouse/sql-data-warehouse-manage-distributed-data-skew.md","poster":"m2shines","timestamp":"1653788460.0","comment_id":"489575"},{"comment_id":"488987","timestamp":"1653719820.0","upvote_count":"3","content":"Selected Answer: D\nThe right answer should be D","poster":"dija123"},{"comment_id":"475531","comments":[{"upvote_count":"2","content":"I thought so too, but the answer specifically states that you connect to the BUILT IN pool before running the command. Every workspace comes with a pre-configured SERVERLESS SQL pool called Built-in. So I think its D.","comment_id":"476258","timestamp":"1652270580.0","poster":"KOSTA007"}],"timestamp":"1652185500.0","poster":"Deevine78","upvote_count":"5","content":"The answer is correct.\nPlease read carefully the question since it concerns an \"Azure Synapse Analytics dedicated SQL pool\" and not a serverless one."},{"poster":"petulda","timestamp":"1645867920.0","comments":[{"comment_id":"432050","upvote_count":"4","poster":"petulda","content":"https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-pdw-showspaceused-transact-sql?view=azure-sqldw-latest here it says it is not supported in serverless (built-in) pool","comments":[{"poster":"Liz42","comment_id":"461724","upvote_count":"3","content":"This post needs more attention, it is still showing that the command doesn’t work for serverless pools, so it has to be one of the other answers","timestamp":"1649883360.0"}],"timestamp":"1645869000.0"},{"poster":"wuespe","timestamp":"1648124040.0","content":"That's right, DBCC PDW_SHOWSPACEUSED('Table1') would work if you are connected to the dedicated pool, but NOT while connected to Built-in.\nThe right answer is D, because it's the only one that actually runs without an error","comment_id":"450819","comments":[{"poster":"dija123","upvote_count":"1","content":"I totally agree with this, The right answer should be D.","timestamp":"1653323880.0","comment_id":"485322"}],"upvote_count":"9"}],"content":"I'm not sure if this correct. Of course PDW_SHOWSPACEUSED returns distribution, but I cannot run it within Build-in pool.\nI can run sys.dm_pdw_nodes_db_partition_stats on Dedicetd pool and it gives me the distribution, too.","comment_id":"432032","upvote_count":"5"},{"content":"-- Find data skew for a distributed table\nDBCC PDW_SHOWSPACEUSED('dbo.FactInternetSales');","timestamp":"1645806660.0","poster":"kittu007","upvote_count":"2","comment_id":"431555"}]},{"id":"iBHtWxeR6kVWOqlOIbDO","topic":"4","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0034700001.png"],"answer_ET":"","answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/62480-exam-dp-203-topic-4-question-9-discussion/","question_id":372,"answers_community":[],"unix_timestamp":1632218940,"isMC":false,"question_text":"HOTSPOT -\nYou need to collect application metrics, streaming query events, and application log messages for an Azure Databrick cluster.\nWhich type of library and workspace should you implement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"You can send application logs and metrics from Azure Databricks to a Log Analytics workspace. It uses the Azure Databricks Monitoring Library, which is available on GitHub.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs","exam_id":67,"discussion":[{"poster":"leandrors","comment_id":"468300","timestamp":"1651020240.0","upvote_count":"12","content":"Correct!"},{"timestamp":"1648855920.0","poster":"Start","content":"Answer is correct\nhttps://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs","comment_id":"455745","upvote_count":"5"},{"comment_id":"1195393","upvote_count":"1","content":"And why not Azure Databricks as Workspace?","timestamp":"1728894720.0","poster":"Alongi"},{"comment_id":"1003020","upvote_count":"1","content":"correct","poster":"kkk5566","timestamp":"1709977440.0"},{"upvote_count":"3","content":"the solution works for databricks runtime 10.x only, though. \nnewer version isn't supported yet","poster":"Igor85","timestamp":"1686421620.0","comment_id":"741223"},{"content":"The given answer is correct","comment_id":"713193","timestamp":"1683472800.0","upvote_count":"2","poster":"dmitriypo"},{"content":"Correct","timestamp":"1676274780.0","upvote_count":"3","poster":"Deeksha1234","comment_id":"646146"},{"poster":"wwdba","content":"Correct!","comment_id":"550319","timestamp":"1660831440.0","upvote_count":"2"},{"upvote_count":"1","poster":"MFO_FM","timestamp":"1647864540.0","content":"is it correct","comment_id":"448818"}],"timestamp":"2021-09-21 12:09:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0034600001.png"]},{"id":"nYtAvy55qqb9l0ZmHkpU","answer":"","answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0000400004.jpg"],"topic":"5","timestamp":"2022-09-08 10:01:00","url":"https://www.examtopics.com/discussions/microsoft/view/81139-exam-dp-203-topic-5-question-1-discussion/","answer_description":"Box 1: Hash -\nScenario:\nEnsure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible.\nA hash distributed table can deliver the highest query performance for joins and aggregations on large tables.\nBox 2: Set the distribution column to the sales date.\nScenario: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right.\nReference:\nhttps://rajanieshkaushikk.com/2020/09/09/how-to-choose-right-data-distribution-strategy-for-azure-synapse/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0000500001.jpg"],"unix_timestamp":1662624060,"question_id":373,"answers_community":[],"discussion":[{"comment_id":"787705","timestamp":"1674655020.0","content":"This case study was in my exam and I scored 970. I chose productid.","upvote_count":"60","comments":[{"timestamp":"1686573360.0","comment_id":"921449","upvote_count":"8","content":"Good Job, Congrats!","poster":"RoyP654"},{"comment_id":"1114723","content":"Congrats! The answer is productid, since ms documentation states NOT to distribute by a date column. When doing so, all data for a given date is partitioned into one distribution. When processing, this hinders parallelism.","poster":"jongert","upvote_count":"4","timestamp":"1704479520.0"}],"poster":"Jerrie86"},{"comments":[{"timestamp":"1667218740.0","comments":[{"upvote_count":"16","comment_id":"720505","content":"because it's asking about distribution, not partition. The requirements say \"ensure that queries joining and filtering sales transaction records based on product ID complete as quikly as possible\". The best way to do so is hash distrinuting on product ID, this way all rows with the same product id will be on the same node and there will be no data shuffling, hence fast queries","poster":"kl8585","timestamp":"1668693780.0"}],"content":"Why not sales date for distribution column ?\nPartition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right...","upvote_count":"1","comment_id":"708400","poster":"mokrani"}],"comment_id":"664246","timestamp":"1662700800.0","upvote_count":"21","content":"Id choose product id as well since it will be used in joins \"Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible.\"","poster":"Julia01"},{"upvote_count":"1","content":"Hash and productId. Consider using the round-robin distribution for your table in the following scenarios: When getting started as a simple starting point since it is the default\nIf there is no obvious joining key; If there is no good candidate column for hash distributing the table; If the table does not share a common join key with other tables ;If the join is less significant than other joins in the query; When the table is a temporary staging table","comment_id":"1399868","poster":"imatheushenrique","timestamp":"1742251380.0"},{"upvote_count":"2","comment_id":"1259571","timestamp":"1722544860.0","poster":"7082935","content":"I'll repeat advice I read from another question: NEVER set distribution on a DATE column. However, partition on DATE is good."},{"comment_id":"995731","timestamp":"1693547640.0","upvote_count":"3","poster":"kkk5566","content":"Hash and Distrubution on Product ID"},{"comments":[{"timestamp":"1685305080.0","content":"So then why this guy is misleading us?? I find lot of answers misleading us.","upvote_count":"3","poster":"pavankr","comment_id":"908851"}],"comment_id":"735552","timestamp":"1670201280.0","upvote_count":"8","content":"In MS's own documentation, it is not recommended to use a date column for distribution. Therefore, the second option should be ProductID","poster":"XiltroX"},{"poster":"OldSchool","timestamp":"1669729440.0","upvote_count":"12","comment_id":"730468","content":"Hash and Distrubution on Product ID, never make distribution on Date.:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choose-a-distribution-column-with-data-that-distributes-evenly\nPartition on Date as explained here:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition","comments":[{"upvote_count":"2","timestamp":"1680857580.0","comment_id":"863666","poster":"kornat","content":"True! ! !"}]},{"comment_id":"705233","content":"Partition column: date, distribution column: ProductID","timestamp":"1666846440.0","upvote_count":"5","poster":"berend1"},{"comment_id":"686302","poster":"greenlever","content":"I think so, Set distribution to Product ID","timestamp":"1664899200.0","upvote_count":"3"},{"comment_id":"663341","upvote_count":"9","content":"Why not Set distribution to Product ID? With the date as the distribution column we lose the advantage of using all 60 nodes, right?","timestamp":"1662624060.0","poster":"pangas2567"}],"exam_id":67,"isMC":false,"question_text":"HOTSPOT -\nYou need to design a data storage structure for the product sales transactions. The solution must meet the sales transaction dataset requirements.\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//"},{"id":"jG6OWllHPzbisuuXJ8UX","topic":"5","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0000600002.png"],"answer":"","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/83272-exam-dp-203-topic-5-question-2-discussion/","question_id":374,"answers_community":[],"unix_timestamp":1663913100,"isMC":false,"question_text":"DRAG DROP -\nYou need to ensure that the Twitter feed data can be analyzed in the dedicated SQL pool. The solution must meet the customer sentiment analytics requirements.\nWhich three Transact-SQL DDL commands should you run in sequence? To answer, move the appropriate commands from the list of commands to the answer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\nSelect and Place:\n//IMG//","discussion":[{"upvote_count":"31","comments":[{"comments":[{"comment_id":"984277","timestamp":"1692346440.0","upvote_count":"2","poster":"[Removed]","content":"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop external tables are supported in both SQL pools"},{"content":"Please see below.\nCREATE TABLE AS SELECT (Azure Synapse Analytics)\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?view=aps-pdw-2016-au7","timestamp":"1672524360.0","poster":"AzureJobsTillRetire","upvote_count":"3","comment_id":"762965"},{"poster":"AzureJobsTillRetire","comment_id":"762969","content":"Also this one.\n\nCREATE EXTERNAL TABLE AS SELECT (Transact-SQL)\nApplies to: SQL Server 2022 (16.x) and later, Azure Synapse Analytics, Analytics Platform System (PDW)\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=aps-pdw-2016-au7","upvote_count":"2","timestamp":"1672524540.0"},{"poster":"matiandal","upvote_count":"3","comment_id":"993771","content":"your r making a mistake mate.\n\nCheck the following link from MS Learn\nR: https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/2-transform-data-using-create-external-table-select-statement\n\n\"You can use a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in \n--> a dedicated SQL pool \nOR \n--> serverless SQL pool to persist the results of a query in an external table, which stores its data in a file in the data lake.\"","timestamp":"1693377720.0"}],"upvote_count":"2","comment_id":"758901","timestamp":"1672164900.0","poster":"JasonVu","content":"CETAS is not available in dedicated SQL pool"},{"comments":[{"upvote_count":"4","poster":"JG1984","comment_id":"930004","content":"It is not necessary if the users are already authenticated by using their own Azure AD credentials.","timestamp":"1687391400.0"}],"poster":"vrodriguesp","upvote_count":"3","comment_id":"809624","content":"are you sure we can create EXTERNAL DATA SOURCE without DATABSE SCOPED CREDENTIAL?","timestamp":"1676469660.0"}],"timestamp":"1671577980.0","poster":"AzureJobsTillRetire","comment_id":"751626","content":"Given answers are correct\nBox 1: CREATE EXTERNAL DATA SOURCE\nBox 2: CREATE EXTERNAL FILE FORMAT\nBox 3: CREATE EXTERNAL TABLE AS SELECT\n\nRequirements: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds. Data must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials.\n\nWhy CREAT DATABSE SCOPED CREDENTIAL is not required?\nRequirement: The users must be authenticated by using their own Azure AD credentials\n\nWhy not CREATE EXTERNAL TABLE?\nRequirement: Allow Contoso users to use PolyBase ... to query ...\nPolyBase has limitations. CREATE EXTERNAL TABLE AS SELECT stored the data within the SQL pool and avoids those limitations.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver16"},{"comment_id":"676811","poster":"juanlu46","timestamp":"1663913100.0","upvote_count":"9","content":"1. Scoped Database Credencial\n2. External Data Source\n3 External File Format","comments":[{"content":"Scoped Database Credencial is a DCL command, not DDL","timestamp":"1670276460.0","upvote_count":"4","poster":"scarycat","comment_id":"736341"},{"comment_id":"730469","timestamp":"1669729560.0","content":"Correct","poster":"OldSchool","upvote_count":"2"}]},{"timestamp":"1721214420.0","poster":"evangelist","upvote_count":"3","comment_id":"1249542","content":"Here is the correct order:\n\nCREATE EXTERNAL DATA SOURCE\nCREATE EXTERNAL FILE FORMAT\nCREATE EXTERNAL TABLE"},{"upvote_count":"3","content":"I think that the correct order is : \ncreate external data source \ncreate external file format \ncreate external table \n\nwe don't need to create database scoped credential, since the users are already using an AAD to authenticate to the storage, if there were no mention of AAD usage then we should have created the scoped credential to specify a type of authentication \n\nAlso here there is no mention of applying transformation on the data, we are only required to read the tweeter data, so create table an external table, CETAS is used to create an external table by exporting the result of a SELECT statement to an external data source, which is not the case here","timestamp":"1720601520.0","poster":"Nadine_nm","comment_id":"1245369"},{"timestamp":"1708187700.0","content":"Create external data source.\nCreate external file format.\nUse CETAS statement\n\nhttps://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/2-transform-data-using-create-external-table-select-statement","upvote_count":"2","poster":"Azure_2023","comment_id":"1152698"},{"poster":"Abdulwahab1983","timestamp":"1700655960.0","comment_id":"1077365","content":"twitter feeds are going to be stored in azure storage which also going to need data life cycle management. If we are not storing the data in the dedicated sql pool table then we do not use CETAS we only create an external table to query the data in the azure storage.","upvote_count":"2"},{"timestamp":"1693547760.0","poster":"kkk5566","comment_id":"995732","upvote_count":"1","content":"DS,format,CETAS"},{"content":"According to Microsoft documentation:\n\nYou can create external tables in Synapse SQL pools via the following steps:\n\nCREATE EXTERNAL DATA SOURCE to reference an external Azure storage and specify the credential that should be used to access the storage.\nCREATE EXTERNAL FILE FORMAT to describe format of CSV or Parquet files.\nCREATE EXTERNAL TABLE on top of the files placed on the data source with the same file format.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#external-tables-in-dedicated-sql-pool-and-serverless-sql-pool","poster":"patjoo","timestamp":"1692518340.0","comment_id":"985628","upvote_count":"6"},{"upvote_count":"1","content":"Answer should CET - RLS is supported on external tables and you do not need CETAS to implement RLS refer https://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16","poster":"[Removed]","comment_id":"984291","timestamp":"1692346980.0"},{"upvote_count":"2","poster":"[Removed]","timestamp":"1692346680.0","comment_id":"984284","content":"https://learn.microsoft.com/en-us/answers/questions/739341/rowlevelsecurity-on-external-table. RLS is not supported on an external table, then how CETAS be an answer"},{"timestamp":"1691731440.0","poster":"Matt2000","comment_id":"978289","content":"Concerning not needing CREATE DATABASE SCOPED CREDENTIAL for CREATE EXTERNAL DATA SOURCE: \"External data source without credential can access public storage account or use the caller's Azure AD identity to access files on Azure storage.\"\nRef: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&tabs=dedicated","upvote_count":"1"},{"comment_id":"881064","upvote_count":"8","timestamp":"1682479740.0","content":"Box 1: CREATE EXTERNAL DATA SOURCE\nBox 2: CREATE EXTERNAL FILE FORMAT\nBox 3: CREATE EXTERNAL TABLE","poster":"BPW"},{"content":"The reason you use CTAS is that you must implement row level security.","timestamp":"1680260760.0","comment_id":"856950","poster":"MartianNC","upvote_count":"2"},{"comment_id":"787164","timestamp":"1674611040.0","content":"Starting with SQL Server 2022 (16.x), Create External Table as Select (CETAS) is supported to create an external table and then export, in parallel, the result of a Transact-SQL SELECT statement to Azure Data Lake Storage (ADLS) Gen2, Azure Storage Account V2, and S3-compatible object storage.\nSo shouldnt third be Create External TABLE ?\nWe dont want to write data to ADLS. We want to read.\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=azure-sqldw-latest&preserve-view=true","comments":[{"content":"You are right. The question is asking to \"read\" the tweeter feed stored as parquet file in ADLS via PolyBase. This is supported with CREATE EXTERNAL TABLE - which in turn reads data from ADLS. Please refer https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver16&tabs=dedicated\nIt is mentioned - \"This command creates an external table for PolyBase to access data stored in a Hadoop cluster or Azure Blob Storage PolyBase external table that references data stored in a Hadoop cluster or Azure Blob Storage.\"","poster":"JitBiswas","upvote_count":"1","timestamp":"1683826080.0","comment_id":"895257"}],"poster":"Jerrie86","upvote_count":"5"},{"upvote_count":"1","timestamp":"1674186180.0","poster":"youngbug","content":"PolyBase is a technology that accesses external data stored in Azure Blob storage or Azure Data Lake Store via the T-SQL language. So no need to copy table into Dedicated SQL Pool.","comment_id":"781834"},{"timestamp":"1670816640.0","upvote_count":"1","content":"why use CETAS instead of Create External Table?","poster":"bigw","comment_id":"742379","comments":[{"poster":"Pais","content":"https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?toc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Ftoc.json&bc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Fbreadcrumb%2Ftoc.json&view=azure-sqldw-latest&preserve-view=true#examples-using-ctas-to-replace-sql-server-code","comment_id":"743907","timestamp":"1670928300.0","upvote_count":"2","comments":[{"content":"your link points to CTAS, which is a different topic","comment_id":"758905","upvote_count":"1","timestamp":"1672165200.0","poster":"JasonVu"}]}]},{"content":"CREATE DATABASE SCOPED CREDENTIALS should be run before all other steps in the given answer","poster":"Igor85","upvote_count":"1","comment_id":"742086","timestamp":"1670791800.0"},{"content":"I think the provided answer in answer are is correct","timestamp":"1669884900.0","comment_id":"732426","upvote_count":"2","poster":"7yut"},{"content":"External file format is required when external table needs to refer to Hadoop files","comment_id":"686300","timestamp":"1664898960.0","upvote_count":"1","poster":"greenlever"}],"exam_id":67,"answer_description":"Scenario: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds. Data must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials.\nBox 1: CREATE EXTERNAL DATA SOURCE\nExternal data sources are used to connect to storage accounts.\nBox 2: CREATE EXTERNAL FILE FORMAT\nCREATE EXTERNAL FILE FORMAT creates an external file format object that defines external data stored in Azure Blob Storage or Azure Data Lake Storage.\nCreating an external file format is a prerequisite for creating an external table.\nBox 3: CREATE EXTERNAL TABLE AS SELECT\nWhen used in conjunction with the CREATE TABLE AS SELECT statement, selecting from an external table imports data into a table within the SQL pool. In addition to the COPY statement, external tables are useful for loading data.\nIncorrect Answers:\n\nCREATE EXTERNAL TABLE -\nThe CREATE EXTERNAL TABLE command creates an external table for Synapse SQL to access data stored in Azure Blob Storage or Azure Data Lake Storage.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables","timestamp":"2022-09-23 08:05:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0000600001.png"]},{"id":"jVHGA8gjvzZrBJdq47Dq","answer_ET":"","discussion":[{"comment_id":"786013","poster":"Jerrie86","timestamp":"1706056860.0","content":"Partition is different than distribution. Distribution=ProductID and partition by Date. \nDistribution: \nWhen you store a table on Azure DW you are storing it amongst 60 nodes. Your table data is distributed across these nodes (using Hash distribution or Round Robin distribution depending on your needs). You can also choose to have your table (preferably a very small table) replicated across these nodes.\n\nParitition : Partitioning is completely divorced from this concept of distribution. When we partition a table we decide which rows belong into which partitions based on some scheme ( like date in this case) Chunk of records for that date range gets its own space in the backend behind the scenes. we can partition data based on anything as long as we know how the data is in our system.\n\nAnd when we put both in use together, all the partitions are horizontally partitioned so that the incoming data is divided into 60 nodes to provide extreme parallelization to the queries.\n\nhttps://www.linkedin.com/pulse/partitioning-distribution-azure-synapse-analytics-swapnil-mule","upvote_count":"20"},{"content":"Load the sales transaction dataset to Azure Synapse Analytics---HERE you have the answer on where to store the \"transactional\"data---ONLY POSSIBILITY is Azure Synapse Analytics Dedicated SQL Pool.","upvote_count":"2","comment_id":"1008143","timestamp":"1726377480.0","poster":"DataEngDP"},{"upvote_count":"3","content":"Partition by date &dedicated pool","comment_id":"995734","poster":"kkk5566","timestamp":"1725170340.0"},{"content":"As far as I see it, we need to distribute the fact table accross the 60 distributions of a dedicated sql pool which means using NO date key (because of MPP) so using the productId key and within each distribution we need to partition the data by the date column so that data can quickly be deleted and queried by all 60 distributions at once","comments":[{"comment_id":"786715","timestamp":"1706115960.0","upvote_count":"3","content":"First question is partition not distribution. So Date is correct","poster":"Jerrie86"}],"comment_id":"711730","upvote_count":"2","timestamp":"1699185840.0","poster":"gerrie1979"},{"comment_id":"709131","timestamp":"1698842700.0","comments":[{"content":"Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right.\nAlso we will delete data using sales date\nI think distribution = ProductID , Partition = Sales_date","timestamp":"1698916860.0","comments":[{"timestamp":"1704457680.0","poster":"sensaint","upvote_count":"7","content":"Correct. Forget above statement. Partition should be Sales Date!!","comment_id":"766604"}],"comment_id":"709727","upvote_count":"16","poster":"mokrani"},{"upvote_count":"5","comment_id":"742091","timestamp":"1702328160.0","poster":"Igor85","content":"don't confuse partitions and distribution for hash-distributed table"}],"content":"I would partition by ProductID since joins and filtering must be optimized for that column","upvote_count":"1","poster":"sensaint"}],"unix_timestamp":1667306700,"exam_id":67,"isMC":false,"question_id":375,"url":"https://www.examtopics.com/discussions/microsoft/view/86790-exam-dp-203-topic-5-question-3-discussion/","answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0000800001.png"],"question_text":"HOTSPOT -\nYou need to design the partitions for the product sales transactions. The solution must meet the sales transaction dataset requirements.\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","topic":"5","timestamp":"2022-11-01 13:45:00","answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0000900001.png"],"answer_description":"Box 1: Sales date -\nScenario: Contoso requirements for data integration include:\n✑ Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right.\nBox 2: An Azure Synapse Analytics Dedicated SQL pool\nScenario: Contoso requirements for data integration include:\n✑ Ensure that data storage costs and performance are predictable.\nThe size of a dedicated SQL pool (formerly SQL DW) is determined by Data Warehousing Units (DWU).\nDedicated SQL pool (formerly SQL DW) stores data in relational tables with columnar storage. This format significantly reduces the data storage costs, and improves query performance.\nSynapse analytics dedicated sql pool\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is"}],"exam":{"numberOfQuestions":384,"lastUpdated":"12 Apr 2025","isMCOnly":false,"isImplemented":true,"name":"DP-203","id":67,"isBeta":false,"provider":"Microsoft"},"currentPage":75},"__N_SSP":true}