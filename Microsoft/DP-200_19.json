{"pageProps":{"questions":[{"id":"ImsgB405M2to338mjM0h","isMC":false,"answer_ET":"","answer_description":"Step 1: Configure Blob storage as input; select items with the TIMESTAMP BY clause\nThe default timestamp of Blob storage events in Stream Analytics is the timestamp that the blob was last modified, which is BlobLastModifiedUtcTime. To process the data as a stream using a timestamp in the event payload, you must use the TIMESTAMP BY keyword.\nExample:\nThe following is a TIMESTAMP BY example which uses the EntryTime column as the application time for events:\nSELECT TollId, EntryTime AS VehicleEntryTime, LicensePlate, State, Make, Model, VehicleType, VehicleWeight, Toll, Tag\nFROM TollTagEntry TIMESTAMP BY EntryTime\nStep 2: Set up cosmos DB as the output\nCreating Cosmos DB as an output in Stream Analytics generates a prompt for information as seen below.\n\nStep 3: Create a query statement with the SELECT INTO statement.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs","timestamp":"2020-11-09 22:22:00","answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/36611-exam-dp-200-topic-2-question-22-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0018800001.png","https://www.examtopics.com/assets/media/exam-media/03872/0018900001.jpg"],"unix_timestamp":1604956920,"exam_id":65,"answers_community":[],"topic":"2","discussion":[{"timestamp":"1633163760.0","comment_id":"455950","upvote_count":"1","poster":"AidenPearce","content":"correct"},{"poster":"promiseve","content":"given answer is correct","comment_id":"287945","upvote_count":"2","timestamp":"1613003640.0"},{"comment_id":"216224","upvote_count":"3","timestamp":"1604956920.0","poster":"lingjun","content":"query: Select .. INTO.. FROM..Having... https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal"}],"question_text":"DRAG DROP -\nYou implement an event processing solution using Microsoft Azure Stream Analytics.\nThe solution must meet the following requirements:\n✑ Ingest data from Blob storage\n✑ Analyze data in real time\n✑ Store processed data in Azure Cosmos DB\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","question_id":91,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0018700001.png"]},{"id":"1fGNuA9K55Tex3NBKitB","question_id":92,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0019100001.jpg"],"unix_timestamp":1585292940,"answer":"","exam_id":65,"isMC":false,"question_text":"HOTSPOT -\nA company plans to use Platform-as-a-Service (PaaS) to create the new data pipeline process. The process must meet the following requirements:\nIngest:\n✑ Access multiple data sources.\n✑ Provide the ability to orchestrate workflow.\n✑ Provide the capability to run SQL Server Integration Services packages.\nStore:\n✑ Optimize storage for big data workloads\n✑ Provide encryption of data at rest.\n✑ Operate with no size limits.\nPrepare and Train:\n✑ Provide a fully-managed and interactive workspace for exploration and visualization.\n✑ Provide the ability to program in R, SQL, Python, Scala, and Java.\n✑ Provide seamless user authentication with Azure Active Directory.\nModel & Serve:\n✑ Implement native columnar storage.\n✑ Support for the SQL language.\n✑ Provide support for structured streaming.\nYou need to build the data integration pipeline.\nWhich technologies should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/17540-exam-dp-200-topic-2-question-23-discussion/","timestamp":"2020-03-27 08:09:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0019300001.jpg"],"answer_description":"Ingest: Azure Data Factory -\nAzure Data Factory pipelines can execute SSIS packages.\nIn Azure, the following services and tools will meet the core requirements for pipeline orchestration, control flow, and data movement: Azure Data Factory, Oozie on HDInsight, and SQL Server Integration Services (SSIS).\n\nStore: Data Lake Storage -\nData Lake Storage Gen1 provides unlimited storage.\nNote: Data at rest includes information that resides in persistent storage on physical media, in any digital format. Microsoft Azure offers a variety of data storage solutions to meet different needs, including file, disk, blob, and table storage. Microsoft also provides encryption to protect Azure SQL Database, Azure Cosmos\nDB, and Azure Data Lake.\nPrepare and Train: Azure Databricks\nAzure Databricks provides enterprise-grade Azure security, including Azure Active Directory integration.\nWith Azure Databricks, you can set up your Apache Spark environment in minutes, autoscale and collaborate on shared projects in an interactive workspace.\nAzure Databricks supports Python, Scala, R, Java and SQL, as well as data science frameworks and libraries including TensorFlow, PyTorch and scikit-learn.\nModel and Serve: Azure Synapse Analytics\nAzure Synapse Analytics/ SQL Data Warehouse stores data into relational tables with columnar storage.\nAzure SQL Data Warehouse connector now offers efficient and scalable structured streaming write support for SQL Data Warehouse. Access SQL Data\nWarehouse from Azure Databricks using the SQL Data Warehouse connector.\nNote: Note: As of November 2019, Azure SQL Data Warehouse is now Azure Synapse Analytics.\nReferences:\nhttps://docs.microsoft.com/bs-latn-ba/azure/architecture/data-guide/technology-choices/pipeline-orchestration-data-movement https://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks","topic":"2","discussion":[{"comment_id":"229546","timestamp":"1606558440.0","poster":"syu31svc","content":"Answer given is correct","upvote_count":"15"},{"timestamp":"1596795720.0","content":"Search Results\nFeatured snippet from the web\nAzure SQL Data Warehouse connector now offers efficient and scalable structured streaming write support for SQL Data Warehouse.","upvote_count":"1","poster":"Arsa","comment_id":"152481"},{"content":"Prepare and train: Databricks, also as Spark does not support Azure AD and Databricks does","upvote_count":"3","poster":"Treadmill","timestamp":"1595328660.0","comment_id":"140224"},{"content":"Not sure Databricks is the good answer. Databricks does not support Java. Should be HDInsight Spark.","poster":"serger","upvote_count":"3","timestamp":"1591342500.0","comment_id":"102983","comments":[{"timestamp":"1591829460.0","upvote_count":"7","poster":"Ikrom","comment_id":"107257","comments":[{"timestamp":"1593622620.0","content":"I think the confusion of databricks not supporting java is because, there was a question in here answered saying so. But yes i agree. Databrick is the right answer as it supports the required languages including Java.","upvote_count":"4","comments":[{"timestamp":"1602786840.0","comment_id":"200702","content":"Currently when I go to my Databricks cluster and I create a new notebook the dropdown does not list Java, I think that is the reason people think it is not supported. BUT you can create a java JAR, and upload that to Databricks THEN access the classes in it.","poster":"induna","upvote_count":"3"}],"comment_id":"124400","poster":"vkmicrosoft"}],"content":"Databricks supports Java and all mentioned languages since it's optimized Apache Spark: https://azure.microsoft.com/en-us/services/databricks/#overview"}]},{"comment_id":"68529","timestamp":"1585292940.0","comments":[{"comment_id":"73522","content":"https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/synapse-analytics#streaming-support","poster":"Yuri1101","upvote_count":"8","timestamp":"1586664300.0"}],"upvote_count":"1","content":"Anyone, any idea about Azure SQL Data Warehouse feature for \"structured streaming\"?","poster":"zenomas"}],"answers_community":[]},{"id":"orBwDNVisYIyhL2cp2Rb","question_id":93,"unix_timestamp":1574198700,"exam_id":65,"isMC":false,"answer_ET":"","topic":"2","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0019800001.png"],"answer":"","discussion":[{"upvote_count":"17","poster":"epgd","timestamp":"1575266580.0","comment_id":"25882","content":"Check this:\n{ \n \"properties\":{ \n \"type\":\"stream\", \n \"serialization\":{ \n \"type\":\"CSV\", \n \"properties\":{ \n \"fieldDelimiter\":\",\", \n \"encoding\":\"UTF8\" \n } \n }, \n \"datasource\":{ \n \"type\":\"Microsoft.ServiceBus/EventHub\", \n \"properties\":{ \n \"serviceBusNamespace\":\"sampleServiceBus\", \n \"sharedAccessPolicyName\":\"SampleReceiver\", \n \"sharedAccessPolicyKey\":\"***/**********/*****************************\", \n \"eventHubName\":\"sampleEventHub\" \n } \n }, \n \"compression\":{ \n \"type\":\"GZip\" \n } \n } \n}\nhttps://docs.microsoft.com/es-es/rest/api/streamanalytics/stream-analytics-input"},{"content":"CSV does not have to be\"only comma separated\". Given answer is correct.","poster":"AAJ","upvote_count":"15","timestamp":"1583317800.0","comment_id":"58633"},{"upvote_count":"2","comment_id":"270294","poster":"Prada","timestamp":"1610975760.0","content":"AVRO is a serialized JSON. Therefore, it must be CSV"},{"timestamp":"1589402340.0","content":"fieldDelimiter is ONLY for CSV type data in ASA Input Serialization JSON file. So, CSV is the right answer.","upvote_count":"2","poster":"Luke97","comment_id":"88482"},{"content":"CSV is comma separated, not dot.\nAvro is made for serialization, so it's probably the right format.","comment_id":"22842","comments":[{"poster":"Gluckos","timestamp":"1602009480.0","upvote_count":"1","comment_id":"194556","content":"wrong response for me.. csv may have comma, dot, star so may have any characters"},{"content":"\"One record per row\" -> CSV. Avro is in columnar format for reads optimization.","poster":"Pairon","comment_id":"323374","upvote_count":"3","timestamp":"1617017040.0"}],"poster":"STH","upvote_count":"3","timestamp":"1574198700.0"}],"answer_description":"Box 1: CSV -\nA comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. A CSV file stores tabular data (numbers and text) in plain text.\nEach line of the file is a data record.\nJSON and AVRO are not formatted as one record per row.\nBox 2: \"type\":\"Microsoft.ServiceBus/EventHub\",\nProperties include \"EventHubName\"\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs https://en.wikipedia.org/wiki/Comma-separated_values","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0019600001.png"],"question_text":"HOTSPOT -\nA company plans to analyze a continuous flow of data from a social media platform by using Microsoft Azure Stream Analytics. The incoming data is formatted as one record per row.\nYou need to create the input stream.\nHow should you complete the REST API segment? To answer, select the appropriate configuration in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/8619-exam-dp-200-topic-2-question-24-discussion/","timestamp":"2019-11-19 22:25:00"},{"id":"IPbdVzx0aJQMSg0mfPRY","question_id":94,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/49796-exam-dp-200-topic-2-question-25-discussion/","discussion":[{"upvote_count":"2","poster":"tucho","timestamp":"1618056540.0","content":"Correct answer is B) , \"add reference input\" is missing.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data","comment_id":"332514"}],"timestamp":"2021-04-10 14:09:00","isMC":true,"answers_community":[],"topic":"2","question_images":[],"answer":"B","choices":{"A":"Yes","B":"No"},"answer_description":"We need one reference data input for LocationIncomes, which rarely changes.\nNote: Stream Analytics also supports input known as reference data. Reference data is either completely static or changes slowly.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#stream-and-reference-inputs","unix_timestamp":1618056540,"exam_id":65,"answer_ET":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a solution that will use Azure Stream Analytics. The solution will accept an Azure Blob storage file named Customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address.\nYou have a file in Blob storage named LocationIncomes that contains median incomes based on location. The file rarely changes.\nYou need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure\nData Lake Storage Gen2 for long-term retention.\nSolution: You implement a Stream Analytics job that has one streaming input, one query, and two outputs.\nDoes this meet the goal?"},{"id":"2QG21pNjPzckVRbvRkdd","exam_id":65,"answer_ET":"B","unix_timestamp":1595076180,"topic":"2","question_images":[],"answer":"B","answer_images":[],"question_id":95,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a solution that will use Azure Stream Analytics. The solution will accept an Azure Blob storage file named Customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address.\nYou have a file in Blob storage named LocationIncomes that contains median incomes based on location. The file rarely changes.\nYou need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure\nData Lake Storage Gen2 for long-term retention.\nSolution: You implement a Stream Analytics job that has one streaming input, one reference input, one query, and two outputs.\nDoes this meet the goal?","isMC":true,"answers_community":[],"choices":{"B":"No","A":"Yes"},"timestamp":"2020-07-18 14:43:00","url":"https://www.examtopics.com/discussions/microsoft/view/26050-exam-dp-200-topic-2-question-26-discussion/","discussion":[{"content":"This is the correct answer --> \"You implement a Stream Analytics job that has one streaming input, one reference input, one query, and two outputs.\"","poster":"Uday0809","upvote_count":"21","timestamp":"1605153660.0","comment_id":"217666"},{"content":"hi to all,\nit's correct: A\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs\nin one query we can join reference data\ntwo outputs because of two different database systems\n\nregards","comment_id":"241464","timestamp":"1607765820.0","upvote_count":"6","comments":[{"content":"https://docs.microsoft.com/en-us/stream-analytics-query/reference-data-join-azure-stream-analytics","timestamp":"1607766000.0","poster":"dumpsm42","comment_id":"241466","upvote_count":"2"}],"poster":"dumpsm42"},{"comments":[{"comment_id":"488813","upvote_count":"1","timestamp":"1638069540.0","content":"No, Stream analytics jobs must have at least one stream input.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#data-stream-input","poster":"victor90"}],"upvote_count":"2","content":"We don't need streaming input, hence answer - no","comment_id":"208318","poster":"Akva","timestamp":"1603955940.0"},{"upvote_count":"2","comment_id":"172716","content":"According to the documentation you can have two output types in a query. \"You can use a single output per job, or multiple outputs per streaming job (if you need them) by adding multiple INTO clauses to the query.\" I think the answer is A.","timestamp":"1599142980.0","poster":"BungyTex","comments":[{"comment_id":"190141","timestamp":"1601449080.0","content":"I agree with BungyTex ... you only need one query with 2 output types .. i have done it in practice ... so can validate it works ...","upvote_count":"1","poster":"sandGrain","comments":[{"comment_id":"225230","timestamp":"1606079880.0","content":"To back BungyTex and sandGrain, here' the doc: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs#:~:text=An%20Azure%20Stream%20Analytics%20job,%2C%20query%2C%20and%20an%20output.&text=When%20you%20design%20your%20Stream,INTO%20clauses%20to%20the%20query.","upvote_count":"1","poster":"jumby","comments":[{"poster":"mohowzeh","content":"Hi Jumby, in the link you provide, the text says that there can be multiple INTO statements in the \"query\". However, in the following link, it can be seen that two INTO statements require two SELECT statements: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#query-example-send-data-to-multiple-outputs\nIt seems that different authors at Microsoft disagree on exactly what a query is. If one query is equal to one SELECT statement, as most SQL literature defines it, then then two INTO statements would mean two queries. The author of the article on Streaming Analytics uses a different definition of \"query\". IMHO he should have said \"job\" instead of \"query\". All very confusing, I'd say, particularly since an exam question rests entirely of which version of the definition one uses.","upvote_count":"1","comment_id":"250165","timestamp":"1608646800.0"}]}]},{"upvote_count":"1","poster":"mohowzeh","timestamp":"1608646380.0","comment_id":"250159","content":"In your quote \"multiple outputs per streaming job\", the key word \"job\" seems different to me from a \"query\". The test question refers to number of queries, not the number of jobs. If one query is equal to one SELECT statement, then one job can have multiple queries."}]},{"comment_id":"139999","content":"We need to two query to load in different targets as stated in question:\n \"You must output the data to Azure SQL Database for immediate use and to Azure\nData Lake Storage Gen2 for long-term retention.\"","timestamp":"1595300700.0","comments":[{"timestamp":"1618335120.0","content":"Yeah but that means you save the SAME result (only one query) in two services because of different purposes. So I think that one query is enough.","comment_id":"334844","upvote_count":"1","poster":"Pairon"}],"poster":"singhadi003","upvote_count":"3"},{"comment_id":"137901","timestamp":"1595076180.0","upvote_count":"3","poster":"HeleneB","content":"I don't see why you couldn't use one query since both sources are in the one customer file. It doesn't say you need two sets of queries for each source in the scenario. I disagree."}],"answer_description":"We need one reference data input for LocationIncomes, which rarely changes.\nWe need two queries, on for in-store customers, and one for online customers.\nFor each query two outputs is needed.\nNote: Stream Analytics also supports input known as reference data. Reference data is either completely static or changes slowly.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#stream-and-reference-inputs https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs"}],"exam":{"isMCOnly":false,"lastUpdated":"12 Apr 2025","name":"DP-200","isBeta":false,"isImplemented":true,"numberOfQuestions":228,"provider":"Microsoft","id":65},"currentPage":19},"__N_SSP":true}