{"pageProps":{"questions":[{"id":"qYah9STLHkN5F626CWou","isMC":true,"exam_id":65,"choices":{"B":"self-hosted integration runtime","C":"Azure-SSIS integration runtime","A":"Azure integration runtime"},"discussion":[{"poster":"samkslee","comment_id":"330959","content":"Use the self-hosted integration runtime even if the data store is in the cloud on an Azure Infrastructure as a Service (IaaS) virtual machine.\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#considerations-for-using-a-self-hosted-ir","upvote_count":"15","comments":[{"poster":"Ambujinee","content":"Correct","comment_id":"365367","timestamp":"1621841220.0","upvote_count":"2"}],"timestamp":"1617865980.0"},{"content":"The answer is correct either the SQL server resides in the traditional virtual server or azure virtual machine it requires \"self hosted integration runtime\" for the ADF to access the server.","comment_id":"354314","timestamp":"1620708720.0","poster":"cadio30","upvote_count":"1"},{"content":"Why is B the correct answer? There is no mention that the VM running inside of a private network so shouldn't A be the correct answer?","timestamp":"1620069360.0","poster":"dangal95","comment_id":"348917","upvote_count":"2"},{"comments":[{"comment_id":"347331","upvote_count":"1","content":"This is incorrect. Correct Answer is B.. Please refer to samkslee's reply.","poster":"Chiranjib","timestamp":"1619917920.0"}],"timestamp":"1619556060.0","poster":"Wendy_DK","comment_id":"344218","upvote_count":"1","content":"correct answer is A"},{"upvote_count":"1","content":"should be \"A\". All components are on cloud","timestamp":"1617628800.0","comments":[{"timestamp":"1617871980.0","poster":"SuperAlex","content":"It's SQL Server in the Azure VM server, not Azure SQL DB. Self-hosted integration runtime can be used for SQL Servers hosted on an on-premises machine or on a Azure VM.","upvote_count":"6","comment_id":"331017"}],"comment_id":"328695","poster":"alf99"}],"url":"https://www.examtopics.com/discussions/microsoft/view/49207-exam-dp-200-topic-2-question-31-discussion/","unix_timestamp":1617628800,"timestamp":"2021-04-05 15:20:00","answers_community":[],"answer":"B","topic":"2","answer_description":"Copying between a cloud data source and a data source in private network: if either source or sink linked service points to a self-hosted IR, the copy activity is executed on that self-hosted Integration Runtime.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#determining-which-ir-to-use","question_id":101,"answer_images":[],"question_images":[],"answer_ET":"B","question_text":"You have an Azure virtual machine that has Microsoft SQL Server installed. The database on the virtual machine contains a table named Table1.\nYou need to copy the data from Table1 to an Azure Data Lake Storage Gen2 account by using an Azure Data Factory V2 copy activity.\nWhich type of integration runtime should you use?"},{"id":"mc29q10GyzKGwk2TXJAH","discussion":[{"comment_id":"304618","content":"Given answer is correct","timestamp":"1615025100.0","upvote_count":"6","poster":"ck1729"},{"poster":"maciejt","comment_id":"364271","content":"But you don't need a lbrary in Spark to connect to Twotter as the data is already in EvenHub that you only need to connect to","upvote_count":"1","timestamp":"1621756560.0"},{"upvote_count":"1","content":"https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-stream-from-eventhubs supports answer provided","comment_id":"229554","timestamp":"1606559160.0","poster":"syu31svc"},{"poster":"UKiran","content":"Its mandatory to create Spark Cluster, and then create a Notebook. As the notebook needs a cluster to run.","timestamp":"1606193880.0","upvote_count":"1","comment_id":"226358"},{"content":"Always mandatory to create a spark cluster from databricks and then you can run a notebook (the notebook should be attached to an existing spark cluster).","poster":"serger","upvote_count":"4","timestamp":"1591610880.0","comment_id":"105148"},{"comment_id":"88484","content":"Refer this article, https://docs.databricks.com/spark/latest/structured-streaming/streaming-event-hubs.html \nIn order to connect Azure Event Hub via Azure Event Hubs Spark Connector in Databricks, we need to deploy Spark Cluster and install the new created library in your Databricks workspace using the Maven coordinate.","poster":"Luke97","upvote_count":"2","timestamp":"1589402700.0","comments":[{"content":"This tutorial explains more details. https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-stream-from-eventhubs","poster":"Luke97","upvote_count":"6","timestamp":"1589402820.0","comment_id":"88486"}]},{"timestamp":"1585672020.0","content":"why is the Spark Cluster included in the answer?","upvote_count":"1","poster":"mclawson1966","comment_id":"69890","comments":[{"upvote_count":"1","content":"I guess Spark is the only way to connect Databricks to an Event Hub?","timestamp":"1585672200.0","comment_id":"69894","poster":"mclawson1966"},{"comment_id":"73526","content":"Spark has built-in API to retrieve Twitter data","timestamp":"1586664540.0","upvote_count":"4","poster":"Yuri1101"},{"timestamp":"1588309260.0","upvote_count":"4","poster":"gunjan075","comment_id":"82021","content":"i guess to run notebook in databricks you need to first create spark cluster"},{"upvote_count":"8","content":"Databricks is a spark-based technology. It is always mandatory to have a spark cluster for Databricks to work.","timestamp":"1594104960.0","comment_id":"128692","poster":"SebK"}]}],"answer_description":"Step 1: Deploy the Azure Databricks service\nCreate an Azure Databricks workspace by setting up an Azure Databricks Service.\nStep 2: Deploy a Spark cluster and then attach the required libraries to the cluster.\nTo create a Spark cluster in Databricks, in the Azure portal, go to the Databricks workspace that you created, and then select Launch Workspace.\nAttach libraries to Spark cluster: you use the Twitter APIs to send tweets to Event Hubs. You also use the Apache Spark Event Hubs connector to read and write data into Azure Event Hubs. To use these APIs as part of your cluster, add them as libraries to Azure Databricks and associate them with your Spark cluster.\nStep 3: Create and configure a Notebook that consumes the streaming data.\nYou create a notebook named ReadTweetsFromEventhub in Databricks workspace. ReadTweetsFromEventHub is a consumer notebook you use to read the tweets from Event Hubs.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/azure-databricks/databricks-stream-from-eventhubs","isMC":false,"question_id":102,"answer_ET":"","answers_community":[],"unix_timestamp":1585672020,"url":"https://www.examtopics.com/discussions/microsoft/view/17736-exam-dp-200-topic-2-question-32-discussion/","exam_id":65,"answer":"","timestamp":"2020-03-31 18:27:00","question_text":"DRAG DROP -\nYour company plans to create an event processing engine to handle streaming data from Twitter.\nThe data engineering team uses Azure Event Hubs to ingest the streaming data.\nYou need to implement a solution that uses Azure Databricks to receive the streaming data from the Azure Event Hubs.\nWhich three actions should you recommend be performed in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0020600001.jpg"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0020700001.jpg"],"topic":"2"},{"id":"kJI4tKWVWtJEQRWhw3Qx","answer_description":"Box 1: New-AzStorageContainer -\n# Example: Create a blob container. This holds the default data store for the cluster.\nNew-AzStorageContainer `\n-Name $clusterName `\n-Context $defaultStorageContext\n$sparkConfig = New-Object \"System.Collections.Generic.Dictionary``2[System.String,System.String]\"\n$sparkConfig.Add(\"spark\", \"2.3\")\n\nBox 2: Spark -\nSpark provides primitives for in-memory cluster computing. A Spark job can load and cache data into memory and query it repeatedly. In-memory computing is much faster than disk-based applications than disk-based applications, such as Hadoop, which shares data through Hadoop distributed file system (HDFS).\nBox 3: New-AzureRMHDInsightCluster\n# Create the HDInsight cluster. Example:\nNew-AzHDInsightCluster `\n-ResourceGroupName $resourceGroupName `\n-ClusterName $clusterName `\n-Location $location `\n-ClusterSizeInNodes $clusterSizeInNodes `\n-ClusterType $\"Spark\" `\n-OSType \"Linux\" `\n\nBox 4: Spark -\nHDInsight is a managed Hadoop service. Use it deploy and manage Hadoop clusters in Azure. For batch processing, you can use Spark, Hive, Hive LLAP,\nMapReduce.\nReferences:\nhttps://docs.microsoft.com/bs-latn-ba/azure/hdinsight/spark/apache-spark-jupyter-spark-sql-use-powershell https://docs.microsoft.com/bs-latn-ba/azure/hdinsight/spark/apache-spark-overview","timestamp":"2020-05-13 07:23:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0021000001.png"],"answer_ET":"","discussion":[{"comment_id":"128694","upvote_count":"13","comments":[{"poster":"ZekroMancer","timestamp":"1605448020.0","upvote_count":"2","content":"why not?","comment_id":"219718"}],"poster":"SebK","content":"This question is no more part of the DP-200 exam.","timestamp":"1594105020.0"},{"timestamp":"1620014460.0","poster":"cadio30","comment_id":"348239","content":"Given answer is correct","upvote_count":"2"},{"content":"imho the answer is correct, see https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-create-linux-clusters-azure-powershell","timestamp":"1607802300.0","comment_id":"241962","poster":"mohowzeh","upvote_count":"1"},{"poster":"runningman","upvote_count":"3","content":"second answer is clearly spark because haddop is misspelled :>}","timestamp":"1589368920.0","comment_id":"88225"}],"exam_id":65,"topic":"2","isMC":false,"answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0020900001.png"],"answer":"","question_id":103,"question_text":"HOTSPOT -\nYou develop data engineering solutions for a company.\nA project requires an in-memory batch data processing solution.\nYou need to provision an HDInsight cluster for batch processing of data on Microsoft Azure.\nHow should you complete the PowerShell segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/20460-exam-dp-200-topic-2-question-33-discussion/","unix_timestamp":1589347380},{"id":"AijrV60jwday5b0RyEkn","question_text":"HOTSPOT -\nA company plans to develop solutions to perform batch processing of multiple sets of geospatial data.\nYou need to implement the solutions.\nWhich Azure services should you use? To answer, select the appropriate configuration in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_ET":"","topic":"2","unix_timestamp":1585368480,"answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0021200001.png"],"question_id":104,"isMC":false,"answer":"","timestamp":"2020-03-28 05:08:00","url":"https://www.examtopics.com/discussions/microsoft/view/17566-exam-dp-200-topic-2-question-34-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0021300001.png"],"answer_description":"Box 1: HDInsight Tools for Visual Studio\nAzure HDInsight Tools for Visual Studio Code is an extension in the Visual Studio Code Marketplace for developing Hive Interactive Query, Hive Batch Job and\nPySpark Job against Microsoft HDInsight.\n\nBox 2: Hive View -\nYou can use Apache Ambari Hive View with Apache Hadoop in HDInsight. The Hive View allows you to author, optimize, and run Hive queries from your web browser.\n\nBox 3: HDInsight REST API -\nAzure HDInsight REST APIs are used to create and manage HDInsight resources through Azure Resource Manager.\nReferences:\nhttps://visualstudiomagazine.com/articles/2019/01/25/vscode-hdinsight.aspx https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-use-hive-ambari-view https://docs.microsoft.com/en-us/rest/api/hdinsight/","discussion":[{"upvote_count":"16","comment_id":"94380","timestamp":"1590242040.0","poster":"Luke97","content":"I think box 3 should also be HDInsight Tools for VS. \"Azure HDInsight Tools for Visual Studio Code is an extension in the Visual Studio Code Marketplace for developing Hive Interactive Query, Hive Batch Job and PySpark Job against Microsoft HDInsight.\" https://marketplace.visualstudio.com/items?itemName=mshdinsight.azure-hdinsight"},{"comment_id":"76422","poster":"Manue","upvote_count":"15","timestamp":"1587306840.0","content":"Honestly, I think quesions and answes make no sense at all. \"Hive View\" is not a tool to run interactive queries and batch processes. On the other hand, \"REST API\" is not a tool to designed to develop \"batch applications\". It's very hard to try to make sense of these questions/answers..."},{"comment_id":"348931","upvote_count":"1","poster":"dangal95","content":"I think that the third answer should also be HDInsight Tools for Visual Studio simply because the questions starts with \"Develop...\" which means we need to develop the batch job (and possibly push it) to the cluster. The HDInsight API only lets us publish batch jobs through pre-existing JARs which container the batch processing logic / code within itself.","timestamp":"1620070620.0"},{"poster":"syu31svc","comments":[{"content":"On second thought, Azure HDInsight REST APIs for batch processing of applications that use HDInsight\nhttps://docs.microsoft.com/en-us/rest/api/hdinsight/hdinsight-application","poster":"syu31svc","comment_id":"231931","upvote_count":"2","timestamp":"1606827180.0"}],"upvote_count":"3","comment_id":"229562","content":"I would say HDInsight Tools for Visual Studio is the answer for both the 1st and 3rd dropdown\nHive view is answer for the 2nd dropdown\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-for-vscode\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-use-hive-ambari-view","timestamp":"1606560360.0"},{"poster":"wyxh","upvote_count":"5","timestamp":"1588074000.0","comment_id":"80824","content":"Hive View is designed to help you author, optimize, and execute queries. With Hive Views you can:\n\nBrowse databases.\nWrite queries or browse query results in full-screen mode, which can be particularly helpful with complex queries or large query results.\nManage query execution jobs and history.\nView existing databases, tables, and their statistics.\nCreate/upload tables and export table DDL to source control.\nView visual explain plans to learn more about query plan."},{"content":"I think box 1 and box 3 need to switch.\n- Visual Studio is a development environment where development is done.\n- REST API is used to call from the application being developed to HDInsight Cluster to perform actions.\nAny thoughts!","poster":"zenomas","comment_id":"68742","comments":[{"upvote_count":"1","comment_id":"75642","timestamp":"1587115500.0","content":"Don't think so. The only native application in the list is VS code, and REST api could be used to create a job processing.","poster":"Leonido"}],"timestamp":"1585368480.0","upvote_count":"6"}],"exam_id":65},{"id":"1TArNg1hfpqQkxXWRwnv","answer_description":"Step 1: Create a master key on the database\nCreate a master key on the database. This is required to encrypt the credential secret.\nStep 2: Create an external data source for Azure Blob storage\nCreate an external data source with CREATE EXTERNAL DATA SOURCE..\nStep 3: Create an external file format to map parquet files.\nCreate an external file format with CREATE EXTERNAL FILE FORMAT.\nFORMAT TYPE: Type of format in Hadoop (DELIMITEDTEXT, RCFILE, ORC, PARQUET).\nStep 4: Create the external table FactSalesOrderDetails\nTo query the data in your Hadoop data source, you must define an external table to use in Transact-SQL queries.\nCreate an external table pointing to data stored in Azure storage with CREATE EXTERNAL TABLE.\nNote: PolyBase is a technology that accesses and combines both non-relational and relational data, all from within SQL Server. It allows you to run queries on external data in Hadoop or Azure blob storage.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-azure-blob-storage","answer":"","topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/55162-exam-dp-200-topic-2-question-35-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0021600001.png"],"unix_timestamp":1623506160,"discussion":[{"poster":"hoangton","timestamp":"1623506160.0","content":"The answer is correct \nStep to Configure an external table\n1. Create a master key on the database. The master key is required to encrypt the credential secret.\n2. Create a database scoped credential for Azure blob storage.\n3. Create an external data source with CREATE EXTERNAL DATA SOURCE..\n4. Create an external file format with CREATE EXTERNAL FILE FORMAT.\n5. Create an external table pointing to data stored in Azure storage with CREATE EXTERNAL TABLE\n6. Create statistics on an external table.","upvote_count":"5","comment_id":"380495"}],"exam_id":65,"question_text":"DRAG DROP -\nYou are creating a managed data warehouse solution on Microsoft Azure.\nYou must use PolyBase to retrieve data from Azure Blob storage that resides in parquet format and load the data into a large table called FactSalesOrderDetails.\nYou need to configure Azure Synapse Analytics to receive the data.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","answers_community":[],"timestamp":"2021-06-12 15:56:00","answer_ET":"","isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0021500001.png"],"question_id":105}],"exam":{"numberOfQuestions":228,"isBeta":false,"name":"DP-200","provider":"Microsoft","id":65,"isImplemented":true,"lastUpdated":"12 Apr 2025","isMCOnly":false},"currentPage":21},"__N_SSP":true}