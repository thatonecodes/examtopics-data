{"pageProps":{"questions":[{"id":"hI9STl8sG0aBhPZgB1Uu","discussion":[{"content":"Selected Answer: A\nPBIP (Power BI Project):\n-> PBIP format is designed to work with version control systems like Azure Repos. It breaks down Power BI artifacts into individual files that can be managed and versioned separately, facilitating better collaboration and change tracking.\n-> Folder Hierarchy: It saves the project structure in a folder hierarchy, where each component of the Power BI project (like datasets, reports, data sources) is stored in separate files.\n-> Text-Based: Being a text-based format, it integrates well with Git repositories and supports diff and merge operations.","comment_id":"1220407","timestamp":"1732816920.0","upvote_count":"6","poster":"282b85d"},{"content":"Selected Answer: A\nA - no-brainer","upvote_count":"2","poster":"PaweuG","timestamp":"1732454460.0","comment_id":"1217406"},{"timestamp":"1731448140.0","upvote_count":"1","comment_id":"1210365","poster":"stilferx","content":"Selected Answer: A\nIMHO, A) PBIP is correct.\n\nWhy:\nPower BI Desktop introduces a new way to author, collaborate, and save your projects. When you save your work as a Power BI Project (PBIP), report and semantic model item definitions are saved as individual plain text files in a simple, intuitive folder structure.\n\nLink: https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-overview"},{"upvote_count":"2","content":"A.PBIP Correct","timestamp":"1728132420.0","comment_id":"1189924","poster":"luiruipu"},{"content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/power-bi/developer/projects/projects-overview#:~:text=Power%20BI%20Desktop%20introduces%20a%20new%20way%20to%20author%2C%20collaborate%2C%20and%20save%20your%20projects.%20You%20can%20now%20save%20your%20work%20as%20a%20Power%20BI%20Project%20(PBIP).%20As%20a%20project%2C%20report%20and%20semantic%20model%20item%20definitions%20are%20saved%20as%20individual%20plain%20text%20files%20in%20a%20simple%2C%20intuitive%20folder%20structure.","comment_id":"1189662","timestamp":"1728104280.0","upvote_count":"1","poster":"ES_Capgemini_IandD"}],"question_images":[],"timestamp":"2024-04-05 06:58:00","isMC":true,"unix_timestamp":1712293080,"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/137945-exam-dp-600-topic-1-question-69-discussion/","question_id":146,"answer_ET":"A","answers_community":["A (100%)"],"answer_description":"","answer":"A","choices":{"C":"PBIT","B":"PBIDS","A":"PBIP","D":"PBIX"},"question_text":"You have an Azure Repos repository named Repo1 and a Fabric-enabled Microsoft Power BI Premium capacity. The capacity contains two workspaces named Workspace1 and Workspace2. Git integration is enabled at the workspace level.\n\nYou plan to use Microsoft Power BI Desktop and Workspace1 to make version-controlled changes to a semantic model stored in Repo1. The changes will be built and deployed to Workspace2 by using Azure Pipelines.\n\nYou need to ensure that report and semantic model definitions are saved as individual text files in a folder hierarchy. The solution must minimize development and maintenance effort.\n\nIn which file format should you save the changes?","exam_id":71,"topic":"1"},{"id":"w7vv38we5Q6ZSURvHkky","timestamp":"2024-02-09 14:03:00","isMC":false,"answer_description":"","question_images":["https://img.examtopics.com/dp-600/image4.png","https://img.examtopics.com/dp-600/image5.png","https://img.examtopics.com/dp-600/image10.png"],"question_text":"HOTSPOT -\n\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nLitware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.\n\nExisting Environment -\n\nFabric Environment -\nLitware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.\n\nAvailable Data -\nLitware has data that must be analyzed as shown in the following table.\n//IMG//\n\nThe Product data contains a single table and the following columns.\n//IMG//\n\nThe customer satisfaction data contains the following tables:\n\nSurvey -\n\nQuestion -\n\nResponse -\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.\n\nUser Problems -\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.\nProduct data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.\n\nRequirements -\n\nPlanned Changes -\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity\nThe following three workspaces will be created:\nAnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store\nDataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake\nDataSciPOC: Will contain all the notebooks and reports created by the data scientists\nThe following will be created in the AnalyticsPOC workspace:\nA data store (type to be decided)\n\nA custom semantic model -\n\nA default semantic model -\n\nInteractive reports -\nThe data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers’ discretion.\nAll the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.\n\nTechnical Requirements -\nThe data store must support the following:\nRead access by using T-SQL or Python\nSemi-structured and unstructured data\nRow-level security (RLS) for users executing T-SQL queries\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\nData will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model\nThe data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model\nThe dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\nThe product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:\nList prices that are less than or equal to 50 are in the low pricing group.\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\nList prices that are greater than 1,000 are in the high pricing group.\n\nSecurity Requirements -\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\nFabric administrators will be the workspace administrators.\nThe data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.\nThe analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.\nThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\nThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.\nThe date dimension must be available to all users of the data store.\nThe principle of least privilege must be followed.\nBoth the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:\nFabricAdmins: Fabric administrators\nAnalyticsTeam: All the members of the analytics team\nDataAnalysts: The data analysts on the analytics team\nDataScientists: The data scientists on the analytics team\nDataEngineers: The data engineers on the analytics team\nAnalyticsEngineers: The analytics engineers on the analytics team\n\nReport Requirements -\nThe data analysts must create a customer satisfaction report that meets the following requirements:\nEnables a user to select a product to filter customer survey responses to only those who have purchased that product.\nDisplays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.\nShows data as soon as the data is updated in the data store.\nEnsures that the report and the semantic model only contain data from the current and previous year.\nEnsures that the report respects any table-level security specified in the source data store.\nMinimizes the execution time of report queries.\nYou need to resolve the issue with the pricing group classification.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n//IMG//","answer_ET":"","discussion":[{"content":"* VIEW: from an existing table.\n* CASE: correct syntax before the WHENs.\n* WHEN ListPrice BETWEEN 50 AND 1000 THEN 'medium': the other two options miss value 1000; on the other hand, the BETWEEN includes both 50 and 1000.","upvote_count":"61","comment_id":"1153088","timestamp":"1708242360.0","comments":[{"content":"I agree with your answer, but with some nuance:\n- I think that you're right that it is a view, but not because it is created from an existing table. Nothing prevents you from creating a table from a table (CTAS statement). However, it should be a view because 'data should be visible as soon as the datastore is updated'. A table from a table is not automatically refreshed when its source table is updated, but a view on this source table will immediately show the new results.\n- I agree that in the last option, there now is no correct option since none meet the requirements and BETWEEN is the least wrong. Either a bad question or a badly remembered question :)","comment_id":"1305379","timestamp":"1730372940.0","upvote_count":"7","comments":[{"timestamp":"1730374140.0","content":"I'm however not 100% sure it shouldn't be table, because reports also need minimal query execution times. A 'SELECT *' on a table is simple, a 'SELECT *' on the view always executes the underlying query. Makes me wonder what 'as soon as the datastore' is updated mean: as soon as new data is available, or as soon as the Power BI source table has been updated?","poster":"semauni","upvote_count":"1","comment_id":"1305389"}],"poster":"semauni"},{"timestamp":"1741202760.0","comment_id":"1365556","content":"VIEW: hence create view as \"Then SQL\", plus its being used in a report, so its not meant to a static representation of the data at a single point in time.\nCASE: correct syntax before the WHENs\nWHEN: There is no correct answer, none of the solutions either exclude 50 and always include 1000. Look at the bottom of high and it says > 1000, so what catches 51 through 1000? Garbage question","poster":"DinoD","upvote_count":"1"},{"content":"* VIEW: from an existing table.\n* CASE: correct syntax before the WHENs.\n* WHEN(ListPrice > 50 AND ListPrice < 1000 THEN 'medium': The second option will generate 2 rows for any '50' answer; the third option has the same problem plus a miss spelled ' ) ' that will end up generating a syntax error.\n\nThe only answer that will not generate a cartesian product is the first one, although it will miss all the '1000' answers.","comments":[{"upvote_count":"2","comment_id":"1218038","timestamp":"1716617040.0","content":"wrong 3rd option. You get NULL PriceGroup when price is exactly 1000","poster":"dev2dev"},{"upvote_count":"5","comment_id":"1217866","timestamp":"1716592740.0","poster":"utsuha","content":"The 3rd option is still the best--when doing a select with CASE WHEN, the logic is evaluated linearly. If the value is 50, the first case is true and the returned value is therefore \"low\". This can be tested with dummy data--just upload a csv file of numbers from 1 to 20 into a test warehouse and query in Fabric.\n\nselect number, \ncase when number <= 5 then 'low'\nwhen number between 5 and 10 then 'med'\nwhen number > 10 then 'high'\nend as test\nfrom dbo.numbers\norder by number asc\n\nAs for the missing '(' or extra ')', I assume this is a typo."}],"timestamp":"1714738920.0","comment_id":"1206108","poster":"2fe10ed","upvote_count":"7"},{"comment_id":"1196340","poster":"BennyBenz","timestamp":"1713242640.0","upvote_count":"2","content":"I agree that 1000 must be included (in the medium), but if the BETWEEN includes 50, and low should include 50, and it states before that <= 50 is low, then 50 shouldn't be included... ?"}],"poster":"SamuComqi"},{"poster":"thuss","comments":[{"timestamp":"1711365240.0","content":"The range BETWEEN 50 AND 1000 is also a valid answer. However, if the value is 50, it will be returned from the previous ‘when’ clause, ListPrice <= 50 Then 'low', before reaching the second ‘when’ clause.”","poster":"Tuki93","upvote_count":"12","comment_id":"1182418"}],"comment_id":"1156388","upvote_count":"30","timestamp":"1708609080.0","content":"All options for the last one are wrong, which is irritating. Should be > 50 AND <= 1000."},{"poster":"ba2bfdf","upvote_count":"1","timestamp":"1743351120.0","content":"1. None of the option in the question satisfying the requirement. Especially for ListPrice 1000\n2. Option with Between having an additional \")\" which will throw syntax error.\nList prices that are less than or equal to 50 are in the low pricing group.\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\nList prices that are greater than 1,000 are in the high pricing group.","comment_id":"1413414"},{"timestamp":"1738013160.0","content":"I took the test the last week. This question was included, with a important modification:\nThey changes the information,¡ of the first option for \"WHEN\": ListPrice > 50 AND ListPrice <= 1000) THEN 'medium'.\n\n-View\n-Case\n-WHEN(ListPrice > 50 AND ListPrice <= 1000) THEN 'medium'.","upvote_count":"7","poster":"pirate84","comment_id":"1347581"},{"timestamp":"1731650100.0","upvote_count":"1","content":"View, CASE, WHEN ListPrice BETWEEN 50 AND 100 THEN ‘medium’ is the answer","poster":"Rakesh16","comment_id":"1312421"},{"poster":"hjhjh123","timestamp":"1726179720.0","upvote_count":"4","content":"view, case - no correct answer - my guess is they miseed the = on a","comment_id":"1282866"},{"poster":"ca_acc","upvote_count":"1","comment_id":"1265189","timestamp":"1723557000.0","content":"* VIEW\n* CASE\n* WHEN ListPrice Between 50 and 1000 Then Medium: The first when is includes 50, so even though the between should be from 51 to 1000, it is (in this case) irrelevant, because in this execution order the frist WHEN wins. Therefor between should produce correct results."},{"comment_id":"1232946","upvote_count":"1","timestamp":"1718812560.0","poster":"AhmadAllied","content":"The valid answer for 3rd is first one as the 1st case is When List Price <= 50 then 'low', so the next case should not be include equal to 50 in anyway, but option 2 has List Price >= 50 which is wrong and between function will also consider = 50 values so it is also wrong."},{"upvote_count":"1","poster":"kronoszerg","content":"View\nTo satisfy \"Shows data as soon as the data is updated in the data store\"\nYou cannot use CTAS Statement as it will not update when source data is updated.\n\nCase\nThe usual SQL Case Statement\n\nLast one is...... no correct answer\nEither question is wrong or answer options are wrong.\nBETWEEN is inclusive by the way","comment_id":"1232855","timestamp":"1718797500.0"},{"upvote_count":"21","timestamp":"1718591640.0","content":"I took the test a few days ago. This question was included, but Microsoft has made a change to: WHEN(ListPrice > 50 AND ListPrice <= 1000) THEN 'medium'. Therefore, I chose this option.","poster":"calvintcy","comment_id":"1231636"},{"poster":"vahox","content":"View\nCASE \nWHEN(ListPrice >= 50 AND ListPrice < 1000) then as between is inclusive of 50 it will provide incorrect result","timestamp":"1718455860.0","comment_id":"1230946","upvote_count":"1"},{"comment_id":"1227080","content":"It should be \n1. Table : As it should be part of Default Semantic Model. With view it's not possible to achieve this.\n2. Case Statement : As we have \"When\" and \"Then\"\n3. Between 51* and 1000 or it should be Listprice>50 and Listprice <=1000. Thanks!","upvote_count":"2","comments":[{"comment_id":"1233626","upvote_count":"1","content":"Within the warehouse, a user can add warehouse objects - tables or views to their default Power BI semantic model. I would go with VIEW as view Shows data as soon as the data is updated in the data store.https://learn.microsoft.com/en-us/fabric/data-warehouse/default-power-bi-semantic-model","timestamp":"1718889420.0","poster":"Priyanka007"}],"timestamp":"1717910760.0","poster":"Darshan6232"},{"timestamp":"1717865940.0","poster":"b6daab0","upvote_count":"1","comment_id":"1226836","content":"Is the solution provided the true correct answer? In many cases it's not, which is misleading. It definitely should be a view and not a table because the Product table changes and thus the pricing group being updated. Where do the solutions come from??"},{"timestamp":"1716822240.0","poster":"282b85d","comment_id":"1219633","content":"CREATE VIEW [dbo].[ProductsWithPricingGroup]\nAS\nSELECT \n ProductId,\n ProductName,\n ProductCategory,\n ListPrice,\n CASE \n WHEN ListPrice <= 50 THEN 'low'\n WHEN ListPrice > 50 AND ListPrice <= 1000 THEN 'medium'\n WHEN ListPrice > 1000 THEN 'high'\n END AS PricingGroup\nFROM dbo.Products;","upvote_count":"3"},{"poster":"David_Webb","content":"VIEW\nCASE\nWHEN ListPrice BETWEEN 50 AND 1000 THEN 'medium'\n\nLook at the requirement:\nList prices that are less than or equal to 50 -> low\nList prices that are greater than 50 and less than or equal to 1,000 -> medium\nList prices that are greater than 1,000 -> high\n\nOnly using option \"BETWEEN 50 AND 1000\" will be able to get the correct answer. If the ListPrice is 50, the case will resolve it as \"low\" in the first condition and return the value. Even though 50 is included in the next case \"BETWEEN 50 AND 1000\", it will not fall into this condition as it has already been resolved with the first condition.","comment_id":"1213593","upvote_count":"4","timestamp":"1716089940.0"},{"poster":"omsingh","content":"select country,first_name, age , case when age <=25 then \"child\"\nwhen age between 25 and 28 then \"adult\"\nwhen age > 28 then \"old\" \nend as ages\nfrom customers; \ncountry first_name age ages\nUSA John 31 old\nUSA Robert 22 child\nUK David 22 child\nUK John 25 child\nUAE Betty 28 adult\n\n\nCorrect answer will be VIEW, CASE AND BETWEEN.","upvote_count":"2","comment_id":"1209448","timestamp":"1715359200.0"},{"comment_id":"1208042","upvote_count":"2","content":"IMHO,\nVIEW -> CASE -> ... BEETWEEN ...\n\nWhy BETWEEN? It is a tricky stuff. It should be between 51 & 1000. This option works well as well because the SQL engine after processing the first one, will ignore the second condition.\n\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.","timestamp":"1715117640.0","poster":"stilferx"},{"upvote_count":"1","timestamp":"1714739160.0","poster":"2fe10ed","content":"* VIEW: Shows data as soon as the data is updated in the data store.\n* CASE: correct syntax before the WHENs.\n* WHEN(ListPrice > 50 AND ListPrice < 1000 THEN 'medium': The second option will generate 2 rows for any '50' answer; the third option has the same problem plus a miss spelled ' ) ' that will end up generating a syntax error.\n\nThe only answer that will not generate a cartesian product is the first one, although it will miss all the '1000' answers. That's a smaller problem than a Cartesian product.","comment_id":"1206112"},{"comment_id":"1203972","upvote_count":"2","timestamp":"1714388940.0","content":"View\nCase\nWHEN ListPrice BETWEEN 50 AND 1000 THEN 'medium","poster":"rmeng"},{"content":"This is the 7th question and in all of them there were mistakes!\nIs there someone responsible to verify the quality?!\nI know it's a beta version, but still...","upvote_count":"1","comment_id":"1200090","timestamp":"1713781260.0","poster":"AnetaK","comments":[{"upvote_count":"1","content":"welcome to examtopics where the community decides on what is right and wrong to enable providing this service for free","comment_id":"1223049","timestamp":"1717312980.0","poster":"Plb2"}]},{"poster":"TeeVanBee","timestamp":"1711608360.0","upvote_count":"2","content":"It can't be between because you will get a syntax error. The first ( is missing.","comment_id":"1184162"},{"upvote_count":"3","comment_id":"1181892","comments":[{"upvote_count":"4","poster":"STH","timestamp":"1711553400.0","comment_id":"1184204","content":"The question is not about the cleanest statement but the working one\n\nListPrice BETWEEN 50 AND 1000 is the only condition that includes 1000 and then meet the requirement !\nEven if it also includes 50, this is not a problem since value 50 is catched by the first WHEN clause : wich means a simple ListPrice <= 1000 also works\n\nIn fact, the best sentence would be the following :\nCASE\nWHEN ListPrice <= 50 THEN 'low'\nWHEN ListPrice > 1000 THEN 'high'\nELSE 'medium'"}],"timestamp":"1711302300.0","content":"The statement says \"List prices greater than 50 and less than or equal to 1,000 are in the middle price group.\" Therefore, it should not include 50 but 1,000. The correct options are VIEW, CASE, >50 and <=1,000","poster":"mtroyano"},{"poster":"shem576","timestamp":"1710923460.0","content":"between 51 and 1000 should be the one..","comment_id":"1178042","upvote_count":"1"},{"upvote_count":"2","comment_id":"1177567","timestamp":"1710869160.0","poster":"a_51","content":"View - a view is reusable for consistent values\nCASE\nWHEN(ListPrice > 50 AND ListPrice < 1000) THEN ‘medium’ - the Between would include the 50 value but this would not include the 1000 so questioning if the question is correctly worded or options shown"},{"timestamp":"1708206420.0","upvote_count":"2","poster":"Momoanwar","content":"View\nCase\nBetween","comment_id":"1152847"},{"timestamp":"1707931980.0","comment_id":"1150382","content":"It has to be BETWEEN because the other two choices will miss 1000","upvote_count":"1","poster":"Bharat"},{"timestamp":"1707927000.0","poster":"Nicofr","content":"View => \"Shows data as soon as the data is updated in the data store.\"\nCASE\nBETWEEN(50,1000) => the when clause for =50 is done in the first WHEN so it will never go in the second when if value is 50","comment_id":"1150327","upvote_count":"1"},{"upvote_count":"3","content":"1 and 2 correct but should 3 not be BETWEEN 51 AND 1000 since the BETWEEN command is inclusive","poster":"theseon","comment_id":"1145476","timestamp":"1707483780.0"}],"answer":"","answer_images":["https://img.examtopics.com/dp-600/image226.png"],"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/133446-exam-dp-600-topic-1-question-7-discussion/","exam_id":71,"question_id":147,"topic":"1","unix_timestamp":1707483780},{"id":"uPWNOySLel0UZwxgYN1M","answer_ET":"","exam_id":71,"unix_timestamp":1713513780,"answers_community":[],"discussion":[{"timestamp":"1714403280.0","content":"Bronze Layer answer is wrong. the Bronze Layer already has pipeline defined with it so you need an invoke pipeline activity to call them","poster":"c8f5bdf","upvote_count":"28","comment_id":"1204077"},{"content":"Should be;\n\nOrchestration pipeline: Schedule\nBronze layer: An invoke pipeline activity (see table -> PIPELINE is already there)\nSilver: Dataflow activity\nGold: Stored procedure activity","timestamp":"1717568760.0","poster":"vernillen","upvote_count":"13","comment_id":"1224553"},{"content":"• Orchestration Pipeline: Invoke pipeline activity - because the requirement is to update each layer in a sequence. So orchestration pipeline invokes Bronze, Silver and Gold pipelines one after another. \n• Bronze Layer: Invoke pipeline activity - because there are multiple pipelines with Copy activity in this layer. We must group them together in one Bronze Layer pipeline\n• Silver Layer: A pipeline Dataflow activity\n• Gold Layer: A pipeline Stored procedure activity\n\nSchedule is type of trigger not a pipeline","upvote_count":"6","comment_id":"1302609","timestamp":"1729794480.0","poster":"BRW"},{"comments":[{"timestamp":"1729793580.0","upvote_count":"1","comment_id":"1302599","poster":"BRW","content":"Agree. Bronze, Silver and Gold pipelines are invoked in parent orchestration pipeline"}],"upvote_count":"2","comment_id":"1294444","poster":"AbhiShar","timestamp":"1728327840.0","content":"Orchestration pipeline: Use An Invoke pipeline activity to manage the orchestration and invoke other pipelines in the correct sequence.\n\nBronze layer: Use A pipeline Copy activity because the Bronze layer typically involves raw data ingestion, where data is copied into the lakehouse.\n\nSilver layer: Use A pipeline Dataflow activity as this layer involves transforming and cleaning the data, for which Dataflows are typically used.\n\nGold layer: Use A pipeline Stored procedure activity because the Gold layer often involves aggregations and advanced processing, which can be handled using stored procedures in a warehouse."},{"timestamp":"1717339620.0","upvote_count":"3","comment_id":"1223186","poster":"byyleoo","content":"I also think that bronze is wrong. Since the pipeline with Copy activities is already created, we just need an invoke pipeline to trigger it"},{"content":"Orchestration: A schedule\nBronze Layer: An invoke pipeline activity (the actual pipeline is already there)\nSilver Layer: A pipeline Dataflow activity\nGold Layer: A pipeline Stored Procedure activity","comment_id":"1223022","timestamp":"1717304220.0","poster":"PaweuG","upvote_count":"4"},{"upvote_count":"3","comment_id":"1220411","poster":"282b85d","content":"• Orchestration Pipeline: A schedule\n• Bronze Layer: A pipeline Copy activity\n• Silver Layer: A pipeline Dataflow activity\n• Gold Layer: A pipeline Stored procedure activity","timestamp":"1716912420.0"},{"timestamp":"1713513780.0","content":"I'd say answer is correct.\nHowever, IMO there is no suitable item for \"orchestration pipeline\". \"A schedule\" seems the most appropriate. But I'd imagine \"data pipeline\" would be a better option since one would use an data pipeline to orchestrate the different activities.","comment_id":"1198421","poster":"Nefirs","upvote_count":"7"}],"isMC":false,"topic":"1","question_images":["https://img.examtopics.com/dp-600/image89.png","https://img.examtopics.com/dp-600/image90.png"],"timestamp":"2024-04-19 10:03:00","url":"https://www.examtopics.com/discussions/microsoft/view/139155-exam-dp-600-topic-1-question-70-discussion/","answer_description":"","question_text":"DRAG DROP\n-\n\nYou are implementing a medallion architecture in a single Fabric workspace.\n\nYou have a lakehouse that contains the Bronze and Silver layers and a warehouse that contains the Gold layer.\n\nYou create the items required to populate the layers as shown in the following table.\n\n//IMG//\n\n\nYou need to ensure that the layers are populated daily in sequential order such that Silver is populated only after Bronze is complete, and Gold is populated only after Silver is complete. The solution must minimize development effort and complexity.\n\nWhat should you use to execute each set of items? To answer, drag the appropriate options to the correct items. Each option may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answer_images":["https://img.examtopics.com/dp-600/image91.png"],"question_id":148,"answer":""},{"id":"Bzn4txKyOQ4t3Mmvvftr","question_images":["https://img.examtopics.com/dp-600/image92.png"],"answer":"","answer_ET":"","topic":"1","question_id":149,"discussion":[{"poster":"neoverma","timestamp":"1728327120.0","content":"from pyspark.sql.types import IntegerType\nfrom pyspark.sql import functions as F\n# first method\ndf = df.withColumn(\"Age\", df.age.cast(\"int\"))\n# second method\ndf = df.withColumn(\"Age\", df.age.cast(IntegerType()))\n# third method\ndf = df.withColumn(\"Age\", F.col(\"Age\").cast(IntegerType()))\n\nhttps://aporia.com/resources/how-to/change-column-data-types-in-dataframe/\n\nconsidering the 3rd method for the answer","upvote_count":"13","comment_id":"1191146"},{"upvote_count":"4","poster":"vernillen","timestamp":"1733387220.0","content":"Correct\n\nwithColumn | col | cast\n\nIt's the very basic syntax of data type allocation","comment_id":"1224555"},{"content":"withColumn > column> cast","poster":"byyleoo","upvote_count":"1","comment_id":"1223187","timestamp":"1733158140.0"},{"upvote_count":"3","timestamp":"1731448860.0","content":"IMHO, withColumn -> col -> cast \nis good!","comment_id":"1210370","poster":"stilferx"},{"poster":"Nefirs","content":"correct - withColumn, col, cast","upvote_count":"4","comment_id":"1196585","timestamp":"1729082700.0"}],"exam_id":71,"url":"https://www.examtopics.com/discussions/microsoft/view/138094-exam-dp-600-topic-1-question-71-discussion/","answer_images":["https://img.examtopics.com/dp-600/image93.png"],"timestamp":"2024-04-07 20:52:00","unix_timestamp":1712515920,"question_text":"DRAG DROP\n-\n\nYou are building a solution by using a Fabric notebook.\n\nYou have a Spark DataFrame assigned to a variable named df. The DataFrame returns four columns.\n\nYou need to change the data type of a string column named Age to integer. The solution must return a DataFrame that includes all the columns.\n\nHow should you complete the code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","answers_community":[],"isMC":false,"answer_description":""},{"id":"VvwtKYR0py2JXFVyWU9f","isMC":false,"answer_description":"","answer_images":["https://img.examtopics.com/dp-600/image234.png"],"question_images":["https://img.examtopics.com/dp-600/image94.png"],"discussion":[{"upvote_count":"45","timestamp":"1712327880.0","comments":[{"upvote_count":"6","timestamp":"1718528460.0","comment_id":"1231279","poster":"DarioReymago","content":"thats ok\ndelta is the format needed to creat managed table. \nusing saveastable you don't need to specify the path"},{"upvote_count":"2","content":"why? please provide some explanation","comment_id":"1191276","timestamp":"1712535900.0","poster":"neoverma","comments":[{"timestamp":"1712697360.0","poster":"harshalt10","comment_id":"1192502","upvote_count":"15","content":"I think it should only be sales because if saveastable is used, the argument should only be table name. Link: https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-notebook-load-data."}]}],"poster":"taphyoe","comment_id":"1189966","content":"delta\nsales"},{"timestamp":"1712646180.0","upvote_count":"8","comments":[{"comment_id":"1224557","timestamp":"1717568940.0","poster":"vernillen","content":"Yep, you're wrong. This would have been right with .save but ALWAYS look carefully: saveAsTable can just be used with the table name","upvote_count":"7"},{"comment_id":"1223583","content":"I think this is wrong since it's usingsaveAsTable","timestamp":"1717419600.0","upvote_count":"1","poster":"lelima"}],"comment_id":"1192074","poster":"dp600dataroots","content":"delta\ntables/sales"},{"upvote_count":"2","timestamp":"1722794940.0","poster":"Pkah","comment_id":"1260762","content":"df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"},{"upvote_count":"7","content":"delta and sales (100%)","timestamp":"1717270200.0","poster":"woliveiras","comment_id":"1222870"},{"comment_id":"1220418","timestamp":"1716913560.0","upvote_count":"2","content":"delta - sales : \ndf = spark.read.parquet(\"abfss://fs1@storage1.dfs.core.windows.net/files/sales.parquet\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"sales\")","poster":"282b85d"},{"content":"df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Sales\"): Writes the DataFrame to the default Lakehouse using the Delta format, and saves it as a table named Sales.","upvote_count":"2","comment_id":"1220141","poster":"trysec","timestamp":"1716892140.0"},{"content":"Delta, sales: the code already using \"SaveasTable\" so no need to reference the table again.","poster":"2dc6125","comment_id":"1212939","timestamp":"1715957460.0","upvote_count":"1"},{"timestamp":"1715481840.0","content":"DELTA, Sales. This is the correct answer since the destination was to loaded Delta the comand SaveAsTable you only need the table name no path. https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-notebook-load-data","poster":"2dc6125","upvote_count":"3","comment_id":"1210089"},{"comment_id":"1204081","poster":"c8f5bdf","timestamp":"1714403820.0","content":"I gave it a try, it is 'sales' not 'File/sales'","upvote_count":"1"},{"timestamp":"1713511260.0","content":"Saveastable so it is directly the name of the tables don't need to add tables/name","poster":"4fd861f","upvote_count":"3","comment_id":"1198397"},{"upvote_count":"4","timestamp":"1713271860.0","comment_id":"1196587","poster":"Nefirs","content":"delta & sales.\nother options do not work in the context of the given code fragments (e.g. files/sales is for external tables but the path-parameter is missing here)"},{"comments":[{"content":"i missed that it has saveastable, so \"Tables/\" is not needed, it`s included to save function.\nSo only \n1.Delta\n2.sales.","poster":"earlqq","upvote_count":"9","comment_id":"1194186","timestamp":"1712902380.0"}],"poster":"earlqq","comment_id":"1192031","timestamp":"1712642880.0","content":"\"The solution must ensure that the content will display automatically as a table named Sales in Lakehouse explorer.\" - so only Delta in Tables section, otherwise table won`t be displayed automatically.\nDelta, Tables.","upvote_count":"4"},{"poster":"dp600","comment_id":"1192024","timestamp":"1712642460.0","content":"A) delta\nB) Table/ sales --> The solution must ensure that the content will display automatically as a table named Sales in Lakehouse explorer","upvote_count":"2"}],"exam_id":71,"question_id":150,"question_text":"HOTSPOT -\n\nYou have an Azure Data Lake Storage Gen2 account named storage1 that contains a Parquet file named sales.parquet.\n\nYou have a Fabric tenant that contains a workspace named Workspace1.\n\nUsing a notebook in Workspace1, you need to load the content of the file to the default lakehouse. The solution must ensure that the content will display automatically as a table named Sales in Lakehouse explorer.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\n//IMG//","topic":"1","unix_timestamp":1712327880,"timestamp":"2024-04-05 16:38:00","answers_community":[],"answer":"","url":"https://www.examtopics.com/discussions/microsoft/view/137977-exam-dp-600-topic-1-question-72-discussion/","answer_ET":""}],"exam":{"id":71,"isImplemented":true,"numberOfQuestions":179,"provider":"Microsoft","isMCOnly":false,"isBeta":false,"name":"DP-600","lastUpdated":"12 Apr 2025"},"currentPage":30},"__N_SSP":true}