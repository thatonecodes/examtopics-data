{"pageProps":{"questions":[{"id":"LqaPMGx06rSTFT5XWlR0","question_images":[],"answer_ET":"AB","answer_description":"","answers_community":["AB (70%)","BC (30%)"],"unix_timestamp":1737781200,"isMC":true,"question_id":81,"choices":{"B":"Copy data","A":"ForEach","C":"WebHook","D":"Stored procedure"},"topic":"2","answer_images":[],"answer":"AB","discussion":[{"content":"Selected Answer: AB\nForEach to loop through all files and COPY to load","timestamp":"1737781200.0","comment_id":"1346329","upvote_count":"7","poster":"4371883"},{"upvote_count":"2","comment_id":"1360217","timestamp":"1740249540.0","content":"Selected Answer: BC\nFocus on Data Movement and Monitoring: Your goal is to ensure data from MAR1 is ingested into the bronze layer efficiently and any failure triggers notifications. Copy data and WebHook are more straightforward and purpose-fit for these tasks:\n\nCopy data ensures the data is copied to the desired format and location.\n\nWebHook allows you to set up notifications if there are any failures in the process.","poster":"MultiCloudIronMan"},{"content":"Selected Answer: BC\nCopy Data (Extracts and loads data into the Bronze Layer)\nBuilt-in Fabric data pipelines support Copy Data to ingest data from REST APIs into a Lakehouse.\nEnsures data lands in Delta format without transformation (as required).\nHandles retries automatically if there is a connectivity error.\n\nWebHook (Triggers and orchestrates API calls for each MAR1 entity)\nMAR1 has multiple REST API endpoints—each entity requires a separate call.\nWebHook can be used to call external APIs dynamically for fetching data.\nHelps control execution flow and ensure each entity is ingested before moving forward.","timestamp":"1740148860.0","upvote_count":"1","comment_id":"1359791","poster":"568f95c"}],"timestamp":"2025-01-25 06:00:00","url":"https://www.examtopics.com/discussions/microsoft/view/155366-exam-dp-700-topic-2-question-6-discussion/","question_text":"Case Study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview. Company Overview -\nContoso, Ltd. is an online retail company that wants to modernize its analytics platform by moving to Fabric. The company plans to begin using Fabric for marketing analytics.\n\nOverview. IT Structure -\nThe company’s IT department has a team of data analysts and a team of data engineers that use analytics systems.\nThe data engineers perform the ingestion, transformation, and loading of data. They prefer to use Python or SQL to transform the data.\nThe data analysts query data and create semantic models and reports. They are qualified to write queries in Power Query and T-SQL.\n\nExisting Environment. Fabric -\nContoso has an F64 capacity named Cap1. All Fabric users are allowed to create items.\nContoso has two workspaces named WorkspaceA and WorkspaceB that currently use Pro license mode.\nExisting Environment. Source Systems\nContoso has a point of sale (POS) system named POS1 that uses an instance of SQL Server on Azure Virtual Machines in the same Microsoft Entra tenant as Fabric. The host virtual machine is on a private virtual network that has public access blocked. POS1 contains all the sales transactions that were processed on the company’s website.\nThe company has a software as a service (SaaS) online marketing app named MAR1. MAR1 has seven entities. The entities contain data that relates to email open rates and interaction rates, as well as website interactions. The data can be exported from MAR1 by calling REST APIs. Each entity has a different endpoint.\nContoso has been using MAR1 for one year. Data from prior years is stored in Parquet files in an Amazon Simple Storage Service (Amazon S3) bucket. There are 12 files that range in size from 300 MB to 900 MB and relate to email interactions.\nExisting Environment. Product Data\nPOS1 contains a product list and related data. The data comes from the following three tables:\n\nProducts -\n\nProductCategories -\n\nProductSubcategories -\nIn the data, products are related to product subcategories, and subcategories are related to product categories.\n\nExisting Environment. Azure -\nContoso has a Microsoft Entra tenant that has the following mail-enabled security groups:\nDataAnalysts: Contains the data analysts\nDataEngineers: Contains the data engineers\nContoso has an Azure subscription.\nThe company has an existing Azure DevOps organization and creates a new project for repositories that relate to Fabric.\nExisting Environment. User Problems\nThe VP of marketing at Contoso requires analysis on the effectiveness of different types of email content. It typically takes a week to manually compile and analyze the data. Contoso wants to reduce the time to less than one day by using Fabric.\nThe data engineering team has successfully exported data from MAR1. The team experiences transient connectivity errors, which causes the data exports to fail.\n\nRequirements. Planned Changes -\nContoso plans to create the following two lakehouses:\nLakehouse1: Will store both raw and cleansed data from the sources\nLakehouse2: Will serve data in a dimensional model to users for analytical queries\nAdditional items will be added to facilitate data ingestion and transformation.\nContoso plans to use Azure Repos for source control in Fabric.\nRequirements. Technical Requirements\nThe new lakehouses must follow a medallion architecture by using the following three layers: bronze, silver, and gold. There will be extensive data cleansing required to populate the MAR1 data in the silver layer, including deduplication, the handling of missing values, and the standardizing of capitalization.\nEach layer must be fully populated before moving on to the next layer. If any step in populating the lakehouses fails, an email must be sent to the data engineers.\nData imports must run simultaneously, when possible.\nThe use of email data from the Amazon S3 bucket must meet the following requirements:\nMinimize egress costs associated with cross-cloud data access.\nPrevent saving a copy of the raw data in the lakehouses.\nItems that relate to data ingestion must meet the following requirements:\nThe items must be source controlled alongside other workspace items.\nIngested data must land in the bronze layer of Lakehouse1 in the Delta format.\nNo changes other than changes to the file formats must be implemented before the data lands in the bronze layer.\nDevelopment effort must be minimized and a built-in connection must be used to import the source data.\nIn the event of a connectivity error, the ingestion processes must attempt the connection again.\nLakehouses, data pipelines, and notebooks must be stored in WorkspaceA. Semantic models, reports, and dataflows must be stored in WorkspaceB.\nOnce a week, old files that are no longer referenced by a Delta table log must be removed.\nRequirements. Data Transformation\nIn the POS1 product data, ProductID values are unique. The product dimension in the gold layer must include only active products from product list. Active products are identified by an IsActive value of 1.\nSome product categories and subcategories are NOT assigned to any product. They are NOT analytically relevant and must be omitted from the product dimension in the gold layer.\n\nRequirements. Data Security -\nSecurity in Fabric must meet the following requirements:\nThe data engineers must have read and write access to all the lakehouses, including the underlying files.\nThe data analysts must only have read access to the Delta tables in the gold layer.\nThe data analysts must NOT have access to the data in the bronze and silver layers.\nThe data engineers must be able to commit changes to source control in WorkspaceA.\nYou need to populate the MAR1 data in the bronze layer.\nWhich two types of activities should you include in the pipeline? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","exam_id":72},{"id":"FFpuYdIOGNIc4li7ADXc","answer_ET":"","isMC":false,"answers_community":[],"discussion":[{"comment_id":"1342053","content":"First part is CONVERT and not CAST","upvote_count":"17","poster":"werfragt","comments":[{"poster":"doctordodge","content":"Agreed, based on the syntax of (<data type>, <column name>).","timestamp":"1737754500.0","upvote_count":"3","comment_id":"1346246"}],"timestamp":"1737102180.0"},{"comment_id":"1361113","timestamp":"1740415200.0","content":"CONVERT, LEFT OUTER, HAVING","upvote_count":"7","poster":"Blue_MSBI"},{"comment_id":"1373384","upvote_count":"1","content":"Syntaxt for Cast is different. It does not fit here.\nCONVERT(DATE, c.startdate) AS startdate\nCAST(c.startdate AS DATE) AS startdate","timestamp":"1741548060.0","poster":"vish9"},{"content":"convert, inner, having\nConvert: No doubt.\ninner: You only need contracttypes with more than 2 employees. So, why use left join? Inner join already filters out contracts and employees you won't need for aggregation.\nhaving: No doubt","comment_id":"1357375","poster":"giraf","timestamp":"1739737380.0","upvote_count":"2"},{"poster":"2e6975f","upvote_count":"3","content":"CONVERT\nLEFT OUTER JOIN\nHAVING","comment_id":"1356714","timestamp":"1739593740.0"},{"poster":"MultiCloudIronMan","upvote_count":"1","comment_id":"1356606","timestamp":"1739568360.0","content":"Right outer join and cast"},{"poster":"8d6881f","timestamp":"1738160580.0","comment_id":"1348592","content":"CAST(c.StartDate as date)\nvs \nCONVERT(date, c.StartDate)","upvote_count":"5"}],"topic":"2","timestamp":"2025-01-17 09:23:00","url":"https://www.examtopics.com/discussions/microsoft/view/154776-exam-dp-700-topic-2-question-7-discussion/","question_id":82,"question_images":["https://img.examtopics.com/dp-700/image34.png","https://img.examtopics.com/dp-700/image35.png"],"answer_images":["https://img.examtopics.com/dp-700/image36.png"],"answer_description":"","question_text":"HOTSPOT -\nYou have a Fabric workspace that contains a warehouse named Warehouse1. Warehouse1 contains the following tables and columns.\n//IMG//\n\n\nYou need to denormalize the tables and include the ContractType and StartDate columns in the Employee table. The solution must meet the following requirements:\nEnsure that the StartDate column is of the date data type.\nEnsure that all the rows from the Employee table are preserved and include any matching rows from the Contract table.\nEnsure that the result set displays the total number of employees per contract type for all the contract types that have more than two employees.\nHow should you complete the statement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n//IMG//","unix_timestamp":1737102180,"answer":"","exam_id":72},{"id":"BkJhsEtpDZLOajkDFODV","question_images":["https://img.examtopics.com/dp-700/image37.png"],"answer_ET":"","answer_description":"","answers_community":[],"unix_timestamp":1737790320,"isMC":false,"question_id":83,"topic":"2","answer_images":["https://img.examtopics.com/dp-700/image38.png"],"discussion":[{"upvote_count":"6","comment_id":"1346373","timestamp":"1737790320.0","content":"1. eventstream with an external data source\n2. eventstream processor\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-source-azure-event-hubs?pivots=enhanced-capabilities\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/process-events-using-event-processor-editor?pivots=enhanced-capabilities","poster":"4371883"},{"poster":"18e18d0","content":"The answer is correct. The question states that eventstream already exists and uses the lakehouse as destination. The question also states that the rows need to be batch ingested. Thus 1) Dataflow and 2) Filter activity are the best in this situation","timestamp":"1739021640.0","upvote_count":"5","comment_id":"1353421"},{"upvote_count":"1","timestamp":"1742650860.0","comment_id":"1401913","poster":"hebertorosillo","content":"1. eventstream with an external data source\n2. eventstream processor . Dataflow not support Event Hubs"},{"upvote_count":"3","content":"The provided answer. This question is asking how you can batch ingest only rows from the data source where the City attribute has a value of Kansas. To minimize development effeort, the data processor must be DataFlow Gen2 and the filtering should use the Filter in DataFlow Gen2","comment_id":"1357590","poster":"henryphchan","comments":[{"comment_id":"1415289","timestamp":"1743460560.0","poster":"zxc01","content":"the problem is \"You need to batch ingest only rows from the data source where the City attribute has a value of Kansas.\" where is the data source in this one? it is very hard to build dataflow Gen2 if this data source is event hub. key word \"batch ingest\" looks like point to dataflow Gen2. And question said they already set eventstream to save data in lakehouse. If this data source means the table in lakehouse, then I can agree dataflow Gen2 is best option.","upvote_count":"1"}],"timestamp":"1739770800.0"}],"answer":"","timestamp":"2025-01-25 08:32:00","url":"https://www.examtopics.com/discussions/microsoft/view/155368-exam-dp-700-topic-2-question-8-discussion/","question_text":"HOTSPOT -\nYou have an Azure Event Hubs data source that contains weather data.\nYou ingest the data from the data source by using an eventstream named Eventstream1. Eventstream1 uses a lakehouse as the destination.\nYou need to batch ingest only rows from the data source where the City attribute has a value of Kansas. The filter must be added before the destination. The solution must minimize development effort.\nWhat should you use for the data processor and filtering? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n//IMG//","exam_id":72},{"id":"t23ohYKMvEv5EmmJgxlz","answer_images":[],"question_id":84,"discussion":[{"content":"Selected Answer: B\nThe Group by transform operator contains the Standard deviation aggregation. The Aggregate transform operator only contains Average, Max, Min and Sum aggregation.","timestamp":"1735666560.0","upvote_count":"7","poster":"GHill1982","comment_id":"1334934"},{"comment_id":"1344094","upvote_count":"5","content":"Selected Answer: B\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/process-events-using-event-processor-editor?pivots=standard-capabilities#group-by","timestamp":"1737454620.0","poster":"405a659"},{"content":"Selected Answer: B\nGroup by is the correct answer","upvote_count":"1","poster":"abdulbasit170","comment_id":"1559712","timestamp":"1744319640.0"},{"upvote_count":"1","poster":"Loursticks1111","content":"Selected Answer: B\nStandard Deviation is only available in Group by, not Aggregate operator","timestamp":"1744021140.0","comment_id":"1558523"},{"comment_id":"1360219","upvote_count":"1","poster":"MultiCloudIronMan","timestamp":"1740250020.0","comments":[{"comments":[{"upvote_count":"1","poster":"abdulbasit170","timestamp":"1744331280.0","comment_id":"1559752","content":"Why did you change to B?"}],"upvote_count":"1","poster":"MultiCloudIronMan","comment_id":"1360220","timestamp":"1740250260.0","content":"Changed to B"}],"content":"Selected Answer: D\nwhile Group by is useful for creating groups, it does not provide the full range of aggregate functions that Aggregate does, including the standard deviation. This is why the Aggregate operator is the correct choice."}],"timestamp":"2024-12-31 18:36:00","answer":"B","answer_ET":"B","unix_timestamp":1735666560,"choices":{"D":"Aggregate","C":"Union","A":"Expand","B":"Group by"},"topic":"2","answers_community":["B (93%)","7%"],"exam_id":72,"question_images":[],"question_text":"You have a Fabric workspace that contains an eventstream named Eventstream1. Eventstream1 processes data from a thermal sensor by using event stream processing, and then stores the data in a lakehouse.\nYou need to modify Eventstream1 to include the standard deviation of the temperature.\nWhich transform operator should you include in the Eventstream1 logic?","url":"https://www.examtopics.com/discussions/microsoft/view/153727-exam-dp-700-topic-2-question-9-discussion/","isMC":true,"answer_description":""},{"id":"IsMCy4ub6ZEAdlzMXSmQ","answer_ET":"","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/154684-exam-dp-700-topic-3-question-1-discussion/","answer_description":"","unix_timestamp":1737033420,"question_images":["https://img.examtopics.com/dp-700/image54.png"],"question_id":85,"answer":"","answer_images":["https://img.examtopics.com/dp-700/image55.png"],"topic":"3","isMC":false,"question_text":"HOTSPOT -\n\nCase Study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nLitware, Inc. is a publishing company that has an online bookstore and several retail bookstores worldwide. Litware also manages an online advertising business for the authors it represents.\nExisting Environment. Fabric Environment\nLitware has a Fabric workspace named Workspace1. High concurrency is enabled for Workspace1.\nThe company has a data engineering team that uses Python for data processing.\nExisting Environment. Data Processing\nThe retail bookstores send sales data at the end of each business day, while the online bookstore constantly provides logs and sales data to a central enterprise resource planning (ERP) system.\nLitware implements a medallion architecture by using the following three layers: bronze, silver, and gold. The sales data is ingested from the ERP system as Parquet files that land in the Files folder in a lakehouse. Notebooks are used to transform the files in a Delta table for the bronze and silver layers. The gold layer is in a warehouse that has V-Order disabled.\nLitware has image files of book covers in Azure Blob Storage. The files are loaded into the Files folder.\nExisting Environment. Sales Data\nMonth-end sales data is processed on the first calendar day of each month. Data that is older than one month never changes.\nIn the source system, the sales data refreshes every six hours starting at midnight each day.\nThe sales data is captured in a Dataflow Gen1 dataflow. When the dataflow runs, new and historical data is captured. The dataflow captures the following fields of the source:\n\nSales Date -\n\nAuthor -\n\nPrice -\n\nUnits -\n\nSKU -\nA table named AuthorSales stores the sales data that relates to each author. The table contains a column named AuthorEmail. Authors authenticate to a guest Fabric tenant by using their email address.\nExisting Environment. Security Groups\nLitware has the following security groups:\n\nSales -\n\nFabric Admins -\n\nStreaming Admins -\nExisting Environment. Performance Issues\nBusiness users perform ad-hoc queries against the warehouse. The business users indicate that reports against the warehouse sometimes run for two hours and fail to load as expected. Upon further investigation, the data engineering team receives the following error message when the reports fail to load: “The SQL query failed while running.”\nThe data engineering team wants to debug the issue and find queries that cause more than one failure.\nWhen the authors have new book releases, there is often an increase in sales activity. This increase slows the data ingestion process.\nThe company’s sales team reports that during the last month, the sales data has NOT been up-to-date when they arrive at work in the morning.\n\nRequirements. Planned Changes -\nLitware recently signed a contract to receive book reviews. The provider of the reviews exposes the data in Amazon Simple Storage Service (Amazon S3) buckets.\nLitware plans to manage Search Engine Optimization (SEO) for the authors. The SEO data will be streamed from a REST API.\n\nRequirements. Version Control -\nLitware plans to implement a version control solution in Fabric that will use GitHub integration and follow the principle of least privilege.\nRequirements. Governance Requirements\nTo control data platform costs, the data platform must use only Fabric services and items. Additional Azure resources must NOT be provisioned.\n\nRequirements. Data Requirements -\nLitware identifies the following data requirements:\nProcess the SEO data in near-real-time (NRT).\nMake the book reviews available in the lakehouse without making a copy of the data.\nWhen a new book cover image arrives in the Files folder, process the image as soon as possible.\nYou need to troubleshoot the ad-hoc query issue.\nHow should you complete the statement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n//IMG//","exam_id":72,"timestamp":"2025-01-16 14:17:00","discussion":[{"comment_id":"1341704","upvote_count":"15","poster":"MDWPartners","content":"queryinsights.frequently_run_queries\n\nnumber_of_failed_runs > 1\n\nonly this table have the fields specified in the SELECT AND WHERE statements\n\nThe data engineering team wants to debug the issue and find queries that cause more than one failure.\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-views/queryinsights-frequently-run-queries-transact-sql?view=fabric&preserve-view=true","timestamp":"1737033420.0"},{"comment_id":"1349333","timestamp":"1738283640.0","content":"Provided answers are correct.\nqueryinsights.frequently_run_queries\n\nlast_run_start_time datetime2 Time of the most recent query execution.\nlast_run_command varchar(8000) Text of the last query execution.\nnumber_of_runs int Total number of times the query was executed.\navg_total_elapsed_time_ms int Average query execution time (ms) across all runs.\nlast_run_total_elapsed_time_ms int Time taken by the last execution (ms).\nlast_dist_statement_id uniqueidentifier ID linking the query to queryinsights.exec_requests_history.\nlast_run_session_id smallint User session ID for the last execution.\nmin_run_total_elapsed_time_ms int Shortest query execution time (ms).\nmax_run_total_elapsed_time_ms int Longest query execution time (ms).\nnumber_of_successful_runs int Number of successful query executions.\nnumber_of_failed_runs int Number of failed query executions.\nnumber_of_cancelled_runs int Number of canceled query executions.","comments":[{"comment_id":"1349335","content":"I'm sorry, the provided answer is wrong. It should be queryinsights.frequently_run_queries as explained above.","upvote_count":"2","timestamp":"1738283820.0","poster":"Tuki93"}],"poster":"Tuki93","upvote_count":"1"}]}],"exam":{"isImplemented":true,"isBeta":false,"isMCOnly":false,"name":"DP-700","provider":"Microsoft","lastUpdated":"12 Apr 2025","id":72,"numberOfQuestions":97},"currentPage":17},"__N_SSP":true}