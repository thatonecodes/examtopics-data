{"pageProps":{"questions":[{"id":"R4mRN988p6igfr2CCcJf","url":"https://www.examtopics.com/discussions/microsoft/view/48008-exam-dp-300-topic-19-question-1-discussion/","isMC":true,"choices":{"A":"Business Critical 4-vCore","B":"Hyperscale","D":"Serverless","C":"General Purpose v-vCore"},"answer_ET":"D","timestamp":"2021-03-23 12:39:00","unix_timestamp":1616499540,"answer_images":[],"answer_description":"","discussion":[{"timestamp":"1616797800.0","poster":"hydrillo","upvote_count":"14","content":"In my opinion it should be General Purpose Serverless as you can configure max and min vCores and configure Pause to save costs. So D Serverless seems the most appropriate option.","comment_id":"321471"},{"content":"Ok hear me out. The question is:\nBased on the PaaS prototype, which Azure SQL Database compute tier should you use? it does not ask which service tier. There are 2 different compute tiers: Provisioned and Serverless. While the provisioned compute tier provides a specific amount of compute resources that are continuously provisioned independent of workload activity, the serverless compute tier auto-scales compute resources based on workload activity.\n\nThe answer is clearly serverless unless whoever wrote this question does not know the difference between service tier and compute tier.\n\nThank you for listening!","comment_id":"762163","timestamp":"1672419240.0","poster":"TheDataGuy","upvote_count":"11"},{"timestamp":"1729080600.0","content":"D - Serverless - Here's why:\n\nServerless is optimal when there are variable workloads, which seems to be the case based on the graphs that show intermittent usage spikes. Serverless compute automatically scales resources based on usage and pauses during idle periods, helping to optimize cost and resource allocation for databases that experience irregular load patterns.\nThe other tiers (Business Critical, Hyperscale, and General Purpose) are more suited for workloads that have consistent or high resource demands but may not fit well in scenarios with fluctuating demand, as seen in the usage chart.","comment_id":"1298708","upvote_count":"1","poster":"bingomutant"},{"comment_id":"629460","timestamp":"1657435080.0","content":"Selected Answer: D\nThis question is either confusing or misleading due to the scenario in the case study. But definitely it is an interesting question since it checks how deep we understand the difference between the service tier / computer tier and their capabilities.\nThe question is which compute tier should you use. \nThe Business requirement says - The PaaS solution must provide automatic failover during Azure regional outage.\n\nServerless compute tier comes under Gen. Purpose Service tier which has HA solution where failover happens automatically during node failure or outage.\nThe answer is D","poster":"Itsalwaymethecap","upvote_count":"3"},{"timestamp":"1645483620.0","comment_id":"553327","content":"Selected Answer: D\nIdle periods and the fact that it's POC, minimize costs it should be serverless","poster":"AlCubeHead","upvote_count":"1"},{"timestamp":"1641022380.0","comment_id":"514410","poster":"xegiwo9758","content":"Selected Answer: D\nServerless based on monitoring dashboard. Most time, the database is idle","upvote_count":"2"},{"timestamp":"1626610560.0","content":"D is correct. The question asks “compute tier” not service tier","upvote_count":"1","poster":"bc5468521","comment_id":"408914"},{"comment_id":"381474","poster":"KNG","upvote_count":"4","timestamp":"1623636840.0","content":"In the event of an Azure regional outage, ensure that the customers can access the PaaS solution with minimal downtime. The solution must provide automatic failover.\n\nShould be \"A\", Always On Availability Group and Geo replication are needed.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview"},{"content":"I will go with serverless since there are consistent idle periods.","poster":"gills","comment_id":"366149","upvote_count":"10","timestamp":"1621922340.0"},{"comments":[{"comments":[{"timestamp":"1628825220.0","upvote_count":"3","content":"Its a poorly written question and quite ambiguous, but just because it is \"based on the PaaS prototype\" it doesnt mean you ignore the business goals, and this is to provide automatic failover and protect from regional outage, hence \"A\"","comment_id":"424052","poster":"SQLDev0000"}],"content":"question is \"Based on the PaaS prototype\" and not about failover... so serverless is the right choice due to the capacity to pause and saving costs","timestamp":"1626605640.0","comment_id":"408874","upvote_count":"1","poster":"STH"}],"timestamp":"1618184400.0","comment_id":"333601","upvote_count":"4","poster":"U_C","content":"The solution must provide automatic failover. That is why I select \"A\"."},{"content":"There are daily spikes in CPU usage.\nHyperscale with automatic scaling seems the most appropriate solution.\n\nhttps://techcommunity.microsoft.com/t5/azure-sql/autoscaling-azure-sql-hyperscale/ba-p/1149025","comment_id":"318028","timestamp":"1616499540.0","upvote_count":"1","poster":"Raffer"}],"topic":"19","exam_id":68,"answer":"D","question_images":[],"question_id":91,"answers_community":["D (100%)"],"question_text":"Based on the PaaS prototype, which Azure SQL Database compute tier should you use?"},{"id":"BehRTfxiMV09b7JJ5ZDc","question_text":"Which audit log destination should you use to meet the monitoring requirements?","unix_timestamp":1620745800,"timestamp":"2021-05-11 17:10:00","exam_id":68,"isMC":true,"choices":{"A":"Azure Storage","C":"Azure Log Analytics","B":"Azure Event Hubs"},"discussion":[{"poster":"osta","content":"I think it's correct","timestamp":"1620745800.0","upvote_count":"10","comment_id":"354816"},{"upvote_count":"1","comment_id":"1298714","poster":"bingomutant","timestamp":"1729080960.0","content":"C. Azure Log Analytics\n\nHere's why:\n\nThe requirement states that there should be a single dashboard to monitor query performance, security, audit data, and identify poorly performing queries across all the PaaS databases.\nAzure Log Analytics integrates well with Azure Monitor, providing a centralized platform to analyze logs, metrics, and performance data, including query performance and security data, across multiple resources.\nWith Azure Log Analytics, you can create a comprehensive, unified view (dashboard) of the environment that meets the requirement to view security, audit data, and query performance in a single dashboard.\nAzure Storage and Azure Event Hubs are more suitable for raw storage or streaming of logs and events but wouldn't offer the built-in analytics capabilities that Azure Log Analytics provides for real-time querying and monitoring."},{"content":"Selected Answer: C\n答えはCです。","comment_id":"1251803","upvote_count":"2","timestamp":"1721487060.0","poster":"RikuYashiro"},{"content":"Monitoring requirements is only for PaaS databases","timestamp":"1692677580.0","comment_id":"987061","upvote_count":"1","poster":"palomino"},{"poster":"alexatl","comment_id":"791870","upvote_count":"1","content":"is Azure Log Analytics for cloud only, but can't be use for hybrid?\nthere is another question early explaining the difference between Event hub and Log Analytics. Event Hubs was chosen because Log Analytics only support cloud.","timestamp":"1675012680.0"}],"answer_images":[],"question_id":92,"url":"https://www.examtopics.com/discussions/microsoft/view/52446-exam-dp-300-topic-19-question-2-discussion/","answer_ET":"C","answers_community":["C (100%)"],"question_images":[],"answer_description":"Scenario: Use a single dashboard to review security and audit data for all the PaaS databases.\nWith dashboards can bring together operational data that is most important to IT across all your Azure resources, including telemetry from Azure Log Analytics.\nNote: Auditing for Azure SQL Database and Azure Synapse Analytics tracks database events and writes them to an audit log in your Azure storage account, Log\nAnalytics workspace, or Event Hubs.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/visualize/tutorial-logs-dashboards","topic":"19","answer":"C"},{"id":"kwzQtC6jHVKv3TjEx4Ia","question_images":[],"answers_community":["ACD (45%)","AD (33%)","ADE (23%)"],"discussion":[{"upvote_count":"13","comment_id":"496052","timestamp":"1638883560.0","poster":"jerkyflexoff","comments":[{"poster":"sca88","timestamp":"1712988360.0","comment_id":"1194677","upvote_count":"1","content":"I think that the E answer is correct.\nhttps://learn.microsoft.com/en-us/azure/defender-for-cloud/sql-information-protection-policy?tabs=sqlip-tenant"},{"upvote_count":"1","content":"I agree, I believe answer is A and D. Azure Cloud Defender is unrelated to auditing.","poster":"BrenFa101","comment_id":"796950","timestamp":"1675419060.0"},{"content":"current Measure Up practice test correct answers:\nA. Turn on auditing and write audit logs to Log Analytics.\nD. Add sensitivity-classification labels to the columns containing sensitive data.","upvote_count":"1","comment_id":"789904","poster":"fayNik","timestamp":"1674844680.0"}],"content":"Selected Answer: AD\nCompared against Measure Up practice test the correct answer is A and D, so its a two answer question and E is just the mistake."},{"comments":[{"upvote_count":"3","comment_id":"793455","poster":"jxh5337","timestamp":"1675122120.0","content":"Event hub max for 7 days"}],"content":"Selected Answer: ACD\nACD is the right answer.","upvote_count":"7","comment_id":"485142","poster":"jddc","timestamp":"1637681640.0"},{"timestamp":"1734715440.0","upvote_count":"2","content":"Selected Answer: ADE\nI think it is A, D, E.\nEnabling Azure Defender -> enable SQL information protection -> enable SQL information protection's data discovery and classification mechanism provides advanced capabilities for discovering, classifying, labeling, and reporting the sensitive data in your databases.","poster":"voodoo_sh","comment_id":"1329582"},{"comment_id":"1328385","upvote_count":"2","poster":"NannyHG","timestamp":"1734516780.0","content":"Selected Answer: ADE\nThe requirement says that you need to remain 365 days, but writing on Event Hub has a maximum retention time of 90 days, so C is wrong. By elimination, turn on Azure Defender. A and D are necessary to reach the objective."},{"timestamp":"1719849540.0","content":"Selected Answer: ACD\nACD, defender seems unrelated","poster":"scottytohotty","comment_id":"1240231","upvote_count":"1"},{"poster":"DataSturdy","timestamp":"1717584600.0","content":"A C D is the right answer","upvote_count":"1","comment_id":"1224665"},{"content":"Selected Answer: ACD\nTo track each time values from a column are returned in a query, you can use the auditing feature in Azure SQL Database. Auditing allows you to track database events and write the audit logs to an Azure Storage account or an Event Hub.\n\nTo store the audit logs for 365 days from the date the query was executed, you can configure the retention period for the audit logs to 365 days.\n\nTherefore, the correct answers are A, C, D.\n\nD is not so relevant in terms of retention but for filtering purpose it helps.","timestamp":"1704532800.0","comment_id":"1115087","upvote_count":"2","poster":"yyc585"},{"poster":"bsk1983","content":"Selected Answer: ADE\nC (event hubs) CANNOT be part of answer because logs are only retained for 7 days and for premium 90 days so, it would not meet 365 days logs retention from question. I think AD (E) are answers","timestamp":"1693688640.0","upvote_count":"3","comment_id":"997136"},{"upvote_count":"1","poster":"testdumps2017","comment_id":"975424","timestamp":"1691488860.0","content":"ADE.\nhttps://learn.microsoft.com/en-us/azure/defender-for-cloud/sql-information-protection-policy?tabs=sqlip-tenant - you can create policies than can be exported for this exact purpose."},{"content":"Selected Answer: ACD\nCloud Defender is not related to auditing.","timestamp":"1690088460.0","upvote_count":"1","poster":"fede_are","comment_id":"960082"},{"poster":"nano0511","upvote_count":"1","comment_id":"935697","content":"Selected Answer: ACD\nA, C response\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/auditing-setup?view=azuresql\n\nD\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/add-sensitivity-classification-transact-sql?view=sql-server-ver16","timestamp":"1687889880.0"},{"comment_id":"880775","upvote_count":"1","content":"I think answer is correct, as Defender takes advantage of sensitivity classifications in its policies\nhttps://learn.microsoft.com/en-us/azure/defender-for-cloud/sql-information-protection-policy?tabs=sqlip-tenant","timestamp":"1682448420.0","poster":"amazonalex"},{"timestamp":"1680566700.0","upvote_count":"1","content":"The answer is correct. \n\nOne of the reasons is:\n\nFor the Basic pricing tier, the maximum retention period is 1 day, meaning that audit logs will be retained for up to 24 hours.\n\nFor the Standard and Dedicated pricing tiers, the retention period can be configured to up to 7 days, meaning that audit logs can be retained for up to a week.\n\nAnswer C doesn't meet the 365 days retention requirement.","poster":"U_C","comment_id":"860487"},{"timestamp":"1671446760.0","content":"Retention period of audit logs is set as part of Auditing service:\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/auditing-overview?view=azuresql\nhttps://learn.microsoft.com/en-us/powershell/module/azurerm.sql/set-azurermsqlserverauditingpolicy?view=azurermps-6.13.0","poster":"OneplusOne","upvote_count":"1","comment_id":"749733"},{"timestamp":"1659276420.0","content":"I think answer i correct: Azure Defender for Cloud + adding column classification can log access and writing logs on storage account can automaticly delete older by...\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview?view=azuresql","upvote_count":"1","comment_id":"640135","poster":"Chris_toff"},{"timestamp":"1657335240.0","upvote_count":"4","content":"Selected Answer: ACD\nIf you get option C as below instead of the one which is shown now - \nTurn on Advance Data Security for Azure SQL Server \nthen the answers are ACD. At this moment its just AD","comment_id":"628973","poster":"Itsalwaymethecap"},{"poster":"Oralinux","timestamp":"1646845740.0","content":"Eventhub is streaming service and it's pub/sub, you should go with Azure Storage if you want to keep your logs for long time.","upvote_count":"3","comment_id":"564183"},{"content":"Selected Answer: ACD\nACD is the right answer.","comment_id":"543017","upvote_count":"2","timestamp":"1644320700.0","poster":"tesen_tolga"},{"comments":[{"timestamp":"1648746360.0","content":"I think it is only AD for this question. The reason C wouldn't be valid is because you can't specify retention period when saving Audit Logs to Event Hub like you can when saving Audit Logs to Storage.","comment_id":"579073","poster":"cusman","upvote_count":"1"}],"upvote_count":"2","comment_id":"472095","timestamp":"1635947100.0","poster":"jerkyflexoff","content":"I am going to go ACD on this. Azure Defender? \n\nTo configure writing audit logs to an event hub, select Event Hub. Select the event hub where logs will be written and then click Save. Be sure that the event hub is in the same region as your database and server.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview"},{"content":"Enable the Azure Defender to track information?","comment_id":"445662","timestamp":"1631770380.0","comments":[{"timestamp":"1632505080.0","poster":"Mapep","comment_id":"451021","content":"I'm also a bit confused on this.","upvote_count":"1","comments":[{"upvote_count":"2","poster":"quermi","comment_id":"474712","timestamp":"1636450620.0","content":"I think that is ACD. Defender????\nAuditing for Azure SQL Database and Azure Synapse Analytics tracks database events and writes them to an audit log in your Azure storage account, Log Analytics workspace, or Event Hubs.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview"}]}],"upvote_count":"1","poster":"Aggie0702"}],"timestamp":"2021-09-16 07:33:00","question_id":93,"answer":"ACD","answer_ET":"ACD","exam_id":68,"isMC":true,"choices":{"C":"Turn on auditing and write audit logs to an Event Hub","A":"Turn on auditing and write audit logs to an Azure Storage account.","B":"Add extended properties to the column.","E":"Turn on Azure Defender for SQL","D":"Apply sensitivity labels named Highly Confidential to the column."},"unix_timestamp":1631770380,"url":"https://www.examtopics.com/discussions/microsoft/view/62153-exam-dp-300-topic-2-question-1-discussion/","answer_images":[],"topic":"2","answer_description":"","question_text":"You have a new Azure SQL database. The database contains a column that stores confidential information.\nYou need to track each time values from the column are returned in a query. The tracking information must be stored for 365 days from the date the query was executed.\nWhich three actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point."},{"id":"ktEY3qEEGjDY9WWg1cuA","answers_community":["C (100%)"],"exam_id":68,"choices":{"A":"role assignments","C":"shared access signatures (SAS)","B":"account keys","D":"Azure Active Directory (Azure AD) identities"},"answer":"C","timestamp":"2021-11-17 12:41:00","answer_description":"A shared access signature (SAS) provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:\nWhat resources the client may access.\nWhat permissions they have to those resources.\nHow long the SAS is valid.\nNote: Data Lake Storage Gen2 supports the following authorization mechanisms:\n✑ Shared Key authorization\n✑ Shared access signature (SAS) authorization\n✑ Role-based access control (Azure RBAC)\nAccess control lists (ACL) Data Lake Storage Gen2 supports the following authorization mechanisms:\n✑ Shared Key authorization\n✑ Shared access signature (SAS) authorization\n✑ Role-based access control (Azure RBAC)\n✑ Access control lists (ACL)\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview","answer_images":[],"question_text":"You are developing an application that uses Azure Data Lake Storage Gen 2.\nYou need to recommend a solution to grant permissions to a specific application for a limited time period.\nWhat should you include in the recommendation?","topic":"2","discussion":[{"comment_id":"712279","upvote_count":"7","content":"Data Lake Storage is not part of DP-300 exam.","timestamp":"1699272960.0","poster":"Ciupaz"},{"upvote_count":"6","content":"The given answer looks correct","timestamp":"1668685260.0","poster":"o2091","comment_id":"479983"},{"comment_id":"970421","timestamp":"1722620160.0","content":"answer is correct","poster":"kraps9109","upvote_count":"1"},{"comment_id":"907628","upvote_count":"1","poster":"MS_KoolaidMan","timestamp":"1716766680.0","content":"Selected Answer: C\nC is the only option for temporary access to a storage account.\nYou could use AAD/PIM/JIT, but that's not one of the available options."}],"isMC":true,"answer_ET":"C","question_id":94,"url":"https://www.examtopics.com/discussions/microsoft/view/66226-exam-dp-300-topic-2-question-10-discussion/","unix_timestamp":1637149260,"question_images":[]},{"id":"7XOJ2xmHwiKBY3Dk697N","discussion":[{"timestamp":"1645828440.0","content":"Selected Answer: D\ninferring work is the key. With data-masking you cannot prevent to ask if a column is equal to a certain value. Also, the salespeople don't need any information of the credit card, maybe customer support would have a use of the masked credit card. So, column-level security is the solution.","comment_id":"556287","poster":"madab","upvote_count":"16","comments":[{"comment_id":"571836","upvote_count":"4","poster":"CaptainJameson","timestamp":"1647806220.0","content":"To back this up, it is also mentioned by ms that data-masking will not prevent against inferring with an example:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver15"}]},{"poster":"charliebasssssss","comment_id":"666672","upvote_count":"6","timestamp":"1662965340.0","content":"Selected Answer: B\nData- Masking is correct\n\nQuestion says \"...solution to provide salespeople with the ability to view all the entries in Customers\"\nSee use cases of column-level security\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/column-level-security#use-cases","comments":[{"poster":"jm2015","timestamp":"1689484980.0","content":"Agree with you\ncolumn-level security prevents users incl. sales from viewing credit card entry.","comment_id":"953046","upvote_count":"1"}]},{"poster":"learnazureportal","comment_id":"1331783","upvote_count":"1","content":"Selected Answer: B\nData masking is a feature in Azure Synapse Analytics that allows you to obfuscate sensitive data, such as credit card information, while still enabling users (like salespeople) to view the rest of the data in the table.","timestamp":"1735189560.0"},{"timestamp":"1727877720.0","upvote_count":"1","poster":"bingomutant","content":"Agree its D. Data masking may still leave part of the data visible , allowing educated guesses or brute-force attempts to complete it. Only column-level security completely obscures the data to prevent inference.","comment_id":"1292445"},{"poster":"Koe24","comment_id":"1193026","upvote_count":"2","content":"Exam Topics team needs to work on this. Misleading answers and explanation is disappointing.","timestamp":"1712756040.0"},{"comment_id":"1115101","upvote_count":"1","poster":"yyc585","timestamp":"1704534540.0","content":"Selected Answer: D\nTo provide salespeople with the ability to view all the entries in the Customers table while preventing them from viewing or inferring the credit card information, you can use **column-level security**. Column-level security allows you to restrict access to specific columns in a table based on user roles or permissions. \n\nTo implement column-level security, you can create a new role in the database and grant the role SELECT permission on the Customers table. You can then use the **DENY** statement to deny the role access to the credit card information column. \n\nTherefore, the solution you should recommend is **column-level security**."},{"comment_id":"1083640","timestamp":"1701275220.0","content":"Answer is row level security\nTo achieve the goal of allowing salespeople to view entries in the Customers table while preventing access to credit card information, you can implement Row-Level Security (RLS) in Azure Synapse Analytics. RLS allows you to control access to rows in a table based on the characteristics of the user executing a query.","poster":"samers","upvote_count":"1"},{"upvote_count":"1","content":"Answer is column-level security\n\n\"Column-level security simplifies the design and coding of security in your application, allowing you to restrict column access to protect sensitive data. \"\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/column-level-security","timestamp":"1700452080.0","poster":"thatguythere","comment_id":"1075124"},{"comment_id":"995089","upvote_count":"1","content":"Selected Answer: D\nData masking do not prevent inferring with critical data. Column level security is more adapted to this situation (https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/column-level-security).","timestamp":"1693483800.0","poster":"[Removed]"},{"content":"Final answer for me is column level security.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/encrypt-a-column-of-data?view=sql-server-ver16.","timestamp":"1693225740.0","comment_id":"992159","poster":"testdumps2017","upvote_count":"1"},{"poster":"mhaskins","comments":[{"upvote_count":"1","content":"This is simply solved with a GRANT SELECT excluding the credit card column","poster":"mhaskins","timestamp":"1678890120.0","comment_id":"839963"}],"upvote_count":"1","comment_id":"839959","content":"Selected Answer: D\nData masking isn't enough to ensure the data cannot be viewed or inferred","timestamp":"1678889940.0"},{"comment_id":"796984","content":"Selected Answer: D\nColumn-level security is the most appropriate answer (it can be used to restrict the salespeople from seeing the SSN column) although you could argue for Always On with Randomised encryption. Data Masking is incorrect becasue of the potential ability of users to infer the data","poster":"BrenFa101","timestamp":"1675423980.0","upvote_count":"1"},{"timestamp":"1662879780.0","comment_id":"665893","content":"Selected Answer: D\nMy vote goes to D. And here is why - data masking prevent unauthorized access of viewing data, but it \"doesn't aim to prevent database users from connecting directly to the database and running exhaustive queries that expose pieces of the sensitive data\" and we assume that in this question salespeople does have access to DB. So column-level security on my opinion is the best choice as it is allowing you to restrict column access to protect sensitive data even if you have access to DB.\nLinks https://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver16 and https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/column-level-security","upvote_count":"1","poster":"rctm_bm"},{"upvote_count":"1","content":"B is correct as in data masking there is even a specific option to mask credit card numbers in the Azure Portal for your database that uses the partial masking function:\nMASKED WITH (FUNCTION = 'partial(0, \"xxxx-xxxx-xxxx-\", 4)')","poster":"AlCubeHead","comment_id":"553008","timestamp":"1645460760.0"},{"upvote_count":"1","timestamp":"1645235880.0","comment_id":"550609","comments":[{"timestamp":"1645459260.0","upvote_count":"1","poster":"AlCubeHead","content":"Always encrypted would restrict to ALL users not just salespeople. Data mask can be applied to roles","comment_id":"552988"}],"content":"C is met the requirement, however D (Always Encrypted) would provide best protection to secure the credit card info.","poster":"calvintcy"},{"content":"It needs to be D","comment_id":"480235","upvote_count":"3","poster":"Surjit24","timestamp":"1637177520.0","comments":[{"poster":"Zonq","content":"I would say both would work at least in some way but I agree for me D seems to be more proper in this case.","upvote_count":"1","comment_id":"486112","timestamp":"1637772300.0"}]},{"poster":"valente_sven1","timestamp":"1632853260.0","comment_id":"453597","content":"I wunder why isn't Always On?","upvote_count":"1","comments":[{"comments":[{"upvote_count":"5","comment_id":"474885","timestamp":"1636472100.0","poster":"quermi","content":"If you use Always encrypted, you must change the access to aplication because you need a certificate. Howeber if you use data mask only need alter column by example: \nALTER TABLE Data.Membership \nALTER COLUMN LastName varchar(100) MASKED WITH (FUNCTION = 'default()'); \nThen the salespeople see the data with masking, Only the administrartors o the user with UNMASK permision see the real data."}],"timestamp":"1632853380.0","upvote_count":"1","comment_id":"453599","content":"I'm sorry Always Encrypted.","poster":"valente_sven1"}]}],"url":"https://www.examtopics.com/discussions/microsoft/view/63134-exam-dp-300-topic-2-question-11-discussion/","isMC":true,"choices":{"B":"data masking","D":"column-level security","A":"row-level security","C":"Always Encrypted"},"exam_id":68,"answer_ET":"D","timestamp":"2021-09-28 20:21:00","unix_timestamp":1632853260,"answer":"D","answer_description":"","answer_images":[],"question_id":95,"question_images":[],"topic":"2","question_text":"You are designing an enterprise data warehouse in Azure Synapse Analytics that will contain a table named Customers. Customers will contain credit card information.\nYou need to recommend a solution to provide salespeople with the ability to view all the entries in Customers. The solution must prevent all the salespeople from viewing or inferring the credit card information.\nWhat should you include in the recommendation?","answers_community":["D (75%)","B (25%)"]}],"exam":{"provider":"Microsoft","name":"DP-300","numberOfQuestions":360,"id":68,"isImplemented":true,"lastUpdated":"12 Apr 2025","isMCOnly":false,"isBeta":false},"currentPage":19},"__N_SSP":true}