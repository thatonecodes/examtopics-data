{"pageProps":{"questions":[{"id":"tvAjENyxerKaP9XyJvkd","isMC":true,"answer_images":[],"answer_ET":"B","timestamp":"2021-07-11 17:02:00","answers_community":[],"exam_id":65,"unix_timestamp":1626015720,"url":"https://www.examtopics.com/discussions/microsoft/view/57640-exam-dp-200-topic-6-question-116-discussion/","question_images":[],"answer":"B","topic":"6","choices":{"B":"HDInsight Apache storm cluster","C":"Azure stream Analytics","D":"HDInsight Spark cluster","A":"Azure SQL Datawarehouse"},"question_text":"A company plans to store hundreds of files in an Azure Storage account and in Azure Data Lake Storage account. The files will be stored in the parquet format. A solution must be in place that would adopt the following requirements:\n- Provide the ability to process the data every 5 hours\n- Give the ability for interactive data analysis\n- Give the ability to process data using solid-state drive caching\n- Make use of Directed Acyclic Graph processing mechanisms\n- Provide support for REST API calls for monitoring purposes\n- Ensure support for Python and Integration with Microsoft Power BI\nWhich of the following would you consider for the solution?","question_id":216,"answer_description":"All of these features are provided with the HDInsight Apache Storm cluster. The Microsoft documentation mentions the following:\nWhy use Apache Storm on HDInsight?\nStorm on HDInsight provides the following features:\nג€¢ 99% Service Level Agreement (SLA) on Storm uptime: For more information, see the SLA information for HDInsight document.\nג€¢ Supports easy customization by running scripts against a Storm cluster during or after creation. For more information, see Customize HDInsight clusters using script action.\nג€¢ Create solutions in multiple languages: You can write Storm components in the language of your choice, such as Java, C#, and Python.\n- Integrates Visual Studio with HDInsight for the development, management, and monitoring of C# topologies. For more information, see Develop C# Storm topologies with the HDInsight Tools for Visual Studio.\n- Supports the Trident Java interface. You can create Storm topologies that support exactly once processing of messages, transactional datastore persistence, and a set of common stream analytics operations.\nג€¢ Dynamic scaling: You can add or remove worker nodes with no impact to running Storm topologies.\n- You must deactivate and reactivate running topologies to take advantage of new nodes added through scaling operations.\nג€¢ Create streaming pipelines using multiple Azure services: Storm on HDInsight integrates with other Azure services such as Event Hubs, SQL Database, Azure\nStorage, and Azure Data Lake Storage.\nAll of the other options are incorrect because they don't provide all of the capabilities.\nReference:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview","discussion":[{"upvote_count":"1","comment_id":"484006","content":"\"Ensure support for Python \" this is seems you have to use spark","timestamp":"1637571780.0","poster":"massnonn"},{"comment_id":"404083","poster":"Avinash75","timestamp":"1626015720.0","content":"should be D .. Spark Cluster has all the requirements as mentioned in the reference link : https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview","upvote_count":"1"}]},{"id":"0v38cZlxfTeLhLjZbtX3","answer":"B","question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nXYZ is an online training provider.\n\nCurrent Environment -\nThe company currently has Microsoft SQL databases that are split into different categories or tiers. Some of the databases are used by Internal users, some by external partners and external distributions.\nBelow is the List of applications, tiers and their individual requirements:\n//IMG//\n\nBelow are the current requirements of the company:\n* For Tier 4 and Tier 5 databases, the backup strategy must include the following:\n- Transactional log backup every hour\n- Differential backup every day\n- Full backup every week\n* Backup strategies must be in place for all standalone Azure SQL databases using methods available with Azure SQL databases\n* Tier 1 database must implement the following data masking logic:\n- For Data type XYZ-A `\" Mask 4 or less string data type characters\n- For Data type XYZ-B `\" Expose the first letter and mask the domain\n- For Data type XYZ-C `\" Mask everything except characters at the beginning and the end\n* All certificates and keys are internally managed in on-premise data stores\n* For Tier 2 databases, if there are any conflicts between the data transfer from on-premise, preference should be given to on-premise data.\n* Monitoring must be setup on every database\n* Applications with Tiers 6 through 8 must ensure that unexpected resource storage usage is immediately reported to IT data engineers.\n* Azure SQL Data warehouse would be used to gather data from multiple internal and external databases.\n* The Azure SQL Data warehouse must be optimized to use data from its cache\n* The below metrics must be available when it comes to the cache:\n- Metric XYZ-A `\" Low cache hit %, high cache usage %\n- Metric XYZ-B `\" Low cache hit %, low cache usage %\n- Metric XYZ-C `\" high cache hit %, high cache usage %\n* The reporting data for external partners must be stored in Azure storage. The data should be made available during regular business hours in connecting regions.\n* The reporting for Tier 9 needs to be moved to Event Hubs.\n* The reporting for Tier 10 needs to be moved to Azure Blobs.\nThe following issues have been identified in the setup:\n* The External partners have control over the data formats, types and schemas.\n* For External based clients, the queries can't be changed or optimized.\n* The database development staff are familiar with T-SQL language.\n* Because of the size and amount of data, some applications and reporting features are not performing at SLA levels.\nThe data for the external applications needs to be encrypted at rest. You decide to implement the following steps:\n- Use the Always Encrypted Wizard in SQL Server Management Studio\n- Select the column that needs to be encrypted\n- Set the encryption type to Randomized\n- Configure the master key to be used from the Windows Certificate Store\n- Confirm the configuration and deploy the solution\nWould these steps fulfill the requirement?","isMC":true,"answer_description":"As per the documentation, the encryption type needs to set as Deterministic when enabling Always Encrypted:\n\nColumn Selection -\nClick Next on the Introduction page to open the Column Selection page. On this page, you will select which columns you want to encrypt, the type of encryption, and what column encryption key (CEK) to use.\nEncrypt SSN and BirthDate information for each patient. The SSN column will use deterministic encryption, which supports equality lookups, joins, and group by.\nThe BirthDate column will use randomized encryption, which does not support operations.\nSet the Encryption Type for the SSN column to Deterministic and the BirthDate column to Randomized. Click Next.\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-always-encrypted","answer_ET":"B","exam_id":65,"discussion":[{"timestamp":"1626016140.0","comment_id":"404087","content":"i think answer should be yes ..but deterministic and randomized can be chosen","upvote_count":"1","poster":"Avinash75"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0066800001.png"],"unix_timestamp":1626016140,"timestamp":"2021-07-11 17:09:00","url":"https://www.examtopics.com/discussions/microsoft/view/57641-exam-dp-200-topic-6-question-124-discussion/","topic":"6","choices":{"A":"Yes","B":"No"},"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0066600001.jpg"],"question_id":217,"answers_community":[]},{"id":"9Njiqg6ha5xdvYu8SfaX","question_images":[],"question_id":218,"answer_ET":"ABDF","exam_id":65,"choices":{"F":"Create an Azure Stream Analytics cloud job and configure job definition save location","C":"Configure Streaming Units","D":"Create an IoT Hub and add the Azure Stream Analytics modules to the IoT Hub namespace","A":"Ensure to configure routes","B":"Create an Azure Blob storage container","E":"Create an Azure Stream Analytics edge job and configure job definition save location"},"answer_description":"There is an article in the Microsoft documentation on configuring Azure Stream Analytics on IoT Edge devices.\nYou need to have a storage container for the job definition:\n\nInstallation instructions -\nThe high-level steps are described in the following table. More details are given in the following sections.\n\nYou also need to create a cloud part job definition:\n\nYou also need to set the modules for your IoT edge device:\nDeployment ASA on your IoT Edge device(s)\n\nAdd ASA to your deployment -\nג€¢ In the Azure portal, open IoT Hub, navigate to IoT Edge and click on the device you want to target for this deployment.\nג€¢ Select Set modules, then select + Add and choose Azure Stream Analytics Module.\nג€¢ Select the subscription and the ASA Edge job that you created. Click Save.\n\nYou also need to configure the Routes:\n\nConfigure routes -\nIoT Edge provides a way to declaratively route messages between modules, and between modules and IoT Hub. The full syntax is described here. Names of the inputs and outputs created in the ASA job can be used as endpoints for routing.\nSince this is clear from the Microsoft documentation, all other options are incorrect.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge","timestamp":"2021-07-27 14:46:00","isMC":true,"answer":"ABDF","url":"https://www.examtopics.com/discussions/microsoft/view/58790-exam-dp-200-topic-6-question-51-discussion/","question_text":"You need to deploy a Microsoft Azure Stream Analytics job for an IoT based solution. The solution must minimize latency. The solution must also minimize the bandwidth usage between the job and the IoT device.\nWhich of the following actions must you perform for this requirement? (Choose four.)","topic":"6","discussion":[{"upvote_count":"7","poster":"elimey","timestamp":"1627389960.0","comment_id":"415472","content":"The Correct Answer is :\n1) storage\n2) Edge\n3) IOT\n4) route\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge"}],"unix_timestamp":1627389960,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0055800001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0055900001.jpg","https://www.examtopics.com/assets/media/exam-media/03872/0056000001.jpg"],"answers_community":[]},{"id":"qEc41UmKiMeKkj7MMoAV","answer_ET":"C","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0059200001.png"],"discussion":[{"poster":"Gitty","upvote_count":"1","comment_id":"424969","timestamp":"1628982360.0","content":"Here are the logs emitted by dedicated SQL pools:\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/monitoring/how-to-monitor-using-azure-monitor#dedicated-sql-pool-logs"}],"topic":"6","url":"https://www.examtopics.com/discussions/microsoft/view/59756-exam-dp-200-topic-6-question-64-discussion/","answers_community":[],"exam_id":65,"isMC":true,"choices":{"A":"Requeststeps","D":"ExecRequest","B":"DmWorkers","C":"SQLRequest"},"question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an\nAll Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nXYZ is an online training provider.\n\nCurrent Environment -\nThe company currently has Microsoft SQL databases that are split into different categories or tiers. Some of the databases are used by Internal users, some by external partners and external distributions.\nBelow is the List of applications, tiers and their individual requirements:\n//IMG//\n\nBelow are the current requirements of the company:\n* For Tier 4 and Tier 5 databases, the backup strategy must include the following:\n- Transactional log backup every hour\n- Differential backup every day\n- Full backup every week\n* Backup strategies must be in place for all standalone Azure SQL databases using methods available with Azure SQL databases\n* Tier 1 database must implement the following data masking logic:\n- For Data type XYZ-A `\" Mask 4 or less string data type characters\n- For Data type XYZ-B `\" Expose the first letter and mask the domain\n- For Data type XYZ-C `\" Mask everything except characters at the beginning and the end\n* All certificates and keys are internally managed in on-premise data stores\n* For Tier 2 databases, if there are any conflicts between the data transfer from on-premise, preference should be given to on-premise data.\n* Monitoring must be setup on every database\n* Applications with Tiers 6 through 8 must ensure that unexpected resource storage usage is immediately reported to IT data engineers.\n* Azure SQL Data warehouse would be used to gather data from multiple internal and external databases.\n* The Azure SQL Data warehouse must be optimized to use data from its cache\n* The below metrics must be available when it comes to the cache:\n- Metric XYZ-A `\" Low cache hit %, high cache usage %\n- Metric XYZ-B `\" Low cache hit %, low cache usage %\n- Metric XYZ-C `\" high cache hit %, high cache usage %\n* The reporting data for external partners must be stored in Azure storage. The data should be made available during regular business hours in connecting regions.\n* The reporting for Tier 9 needs to be moved to Event Hubs.\n* The reporting for Tier 10 needs to be moved to Azure Blobs.\nThe following issues have been identified in the setup:\n* The External partners have control over the data formats, types and schemas\n* For External based clients, the queries can't be changed or optimized\n* The database development staff are familiar with T-SQL language\n* Because of the size and amount of data, some applications and reporting features are not performing at SLA levels.\nYou have to implement logging for monitoring the data warehousing solution.\nWhich of the following would you log?","unix_timestamp":1628982360,"question_id":219,"timestamp":"2021-08-15 01:06:00","answer_description":"Since the SQL requests would affect the cache, these requests need to be monitored\nThe Microsoft documentation mentions the following on caching:\n\nCache hit and used percentage -\nThe matrix below describes scenarios based on the values of the cache metrics:\n\nScenario 1: You are optimally using your cache. Troubleshoot other areas which may be slowing down your queries.\nScenario 2: Your current working data set cannot fit into the cache which causes a low cache hit percentage due to physical reads. Consider scaling up your performance level and rerun your workload to populate the cache.\nScenario 3: It is likely that your query is running slow due to reasons unrelated to the cache. Troubleshoot other areas which may be slowing down your queries.\nYou can also consider scaling down your instance to reduce your cache size to save costs.\nScenario 4: You had a cold cache which could be the reason why your query was slow. Consider rerunning your query as your working dataset should now be in cached.\nSince this is the ideal metric to monitor, all other options are incorrect.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0059000001.jpg"],"answer":"C"},{"id":"MOoa6MWsmyTnTWGpabhB","answer_ET":"BCDE","url":"https://www.examtopics.com/discussions/microsoft/view/58769-exam-dp-200-topic-6-question-7-discussion/","topic":"6","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0050100001.png"],"discussion":[{"content":"How can you read the parquet files without defining the format?\nSo A should be part of the answer.","poster":"TonReurts","upvote_count":"6","timestamp":"1629652680.0","comment_id":"429417"},{"content":"the question literally said the data needs to be load to xyz_sails. Yet the answer select xyz_sails_Details !!\nthe answer is not correct","timestamp":"1627373160.0","comment_id":"415283","comments":[{"comment_id":"490607","content":"I agree. Although the external tables are required but because it's clearly a different name, I guess the answer is ABDE.\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver15#create-external-tables-for-azure-data-lake-store","timestamp":"1638269640.0","upvote_count":"2","poster":"victor90"}],"upvote_count":"3","poster":"elimey"}],"answer_description":"There is an article on github as part of the Microsoft documentation that provides details on how to load data into an Azure SQL data warehouse from an Azure\nBlob storage account. The key steps are:\nCreating a master key in the database.\nCreating an external data source for the Azure Blob storage account:\n3. Create a master key for the MySampleDataWarehouse database. You only need to create a master key once per database.\nCREATE MASTER KEY;\n4. Run the following CREATE EXTERNAL DATA SOURCE statement to define the location of the Azure blob. This is the location of the external taxi cab data. To run a command that you have appended to the query window, highlight the commands you wish to run and click Execute.\n\nNext you load the data. But it is always beneficial to load the data into a staging table first:\nLoad the data into your data warehouse.\nThis section uses the external tables you just defined to load the sample data from Azure Storage Blob to SQL Data Warehouse.\n[!NOTE] This tutorial loads the data directly into the final table. In a production environment, you will usually use CREATE TABLE AS SELECT to load into a staging table. While data is in the staging table you can perform any necessary transformations. To append the data in the staging table to a production table, you can use the INSERT...SELECT statement. For more information, see Inserting data into a production table.\nSince this is clearly provided in the documentation, all other options are incorrect.","question_id":220,"exam_id":65,"isMC":true,"choices":{"C":"Create an external table called XYZ_sales_details","F":"Configure Polybase to use the Azure Blob storage account","D":"Create an external data source for the Azure Blob storage account","A":"Create an external file format that would map to the parquet-based files","B":"Load the data into a staging table","E":"Create a master key on the database"},"answers_community":[],"answer":"BCDE","question_text":"A company has an Azure SQL data warehouse. They want to use PolyBase to retrieve data from an Azure Blob storage account and ingest into the Azure SQL data warehouse. The files are stored in parquet format. The data needs to be loaded into a table called XYZ_sales.\nWhich of the following actions need to be performed to implement this requirement? (Choose four.)","question_images":[],"timestamp":"2021-07-27 10:06:00","unix_timestamp":1627373160}],"exam":{"lastUpdated":"12 Apr 2025","isBeta":false,"isMCOnly":false,"id":65,"numberOfQuestions":228,"provider":"Microsoft","name":"DP-200","isImplemented":true},"currentPage":44},"__N_SSP":true}