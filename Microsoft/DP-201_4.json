{"pageProps":{"questions":[{"id":"2vrBiJfStzp04cN9jNos","answer":"AB","question_images":[],"answers_community":[],"discussion":[{"poster":"alexvno","timestamp":"1595407680.0","upvote_count":"20","content":"Correct","comment_id":"141003"},{"timestamp":"1608365820.0","content":"A. Increase the request units (RUs)\nB. Turn off indexing","comment_id":"247827","upvote_count":"12","poster":"chaoxes"},{"upvote_count":"1","content":"Propose solution is correct, by default azure cosmos db create an index though the feature could be toggle to prevent it from happening.\n\nReference: https://docs.microsoft.com/bs-latn-ba/Azure/cosmos-db/mongodb-post-migration","comment_id":"362575","poster":"cadio30","timestamp":"1621560420.0"},{"poster":"sjain91","upvote_count":"1","timestamp":"1619864700.0","content":"turn off indexing and increase the request units - Answer A is correct","comment_id":"346838"},{"content":"Indexing makes write operation slower. \n\n\"By default, indexing policy is set to automatic. It's achieved by setting the automatic property in the indexing policy to true. Setting this property to true allows Azure CosmosDB to automatically index documents as they are written.\"\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/index-policy","timestamp":"1604900220.0","upvote_count":"4","comment_id":"215678","poster":"MSFTLearn","comments":[{"timestamp":"1609863900.0","comment_id":"260398","upvote_count":"2","content":"yes, we can turn that indexing off and turn it back on after data load is complete","poster":"sunil_kalra"}]},{"content":"The explanation for B. is incorrect. MongoDB API only creates an index for _id field. From the documentation: \"The Azure Cosmos DB's API for MongoDB server version 3.6 automatically indexes the _id field only. This field can't be dropped. It automatically enforces the uniqueness of the _id field per shard key. To index additional fields, you apply the MongoDB index-management commands. This default indexing policy differs from the Azure Cosmos DB SQL API, which indexes all fields by default.\"\nSo, I think B can not the correct answer.","upvote_count":"5","poster":"M0e","comment_id":"204631","timestamp":"1603452840.0"},{"upvote_count":"3","comments":[{"timestamp":"1607167800.0","upvote_count":"1","content":"I agree on this one","poster":"syu31svc","comment_id":"235642"}],"comment_id":"203921","content":"I think its A & D. The link that is provided in explanation, does not mention about the turning off indexes, however it mentions Creating Unix Indexes. Any suggestion?","poster":"djangodev","timestamp":"1603319700.0"},{"upvote_count":"2","content":"Just a thought - If I migrate from Amazon and I have many locations there, it will make sense to have multiple write sites and run migration in parallel from several different locations. That will server as a migration accelerator.","comment_id":"79533","timestamp":"1587824700.0","poster":"Leonido"}],"topic":"1","choices":{"C":"Add a write region.","A":"Increase the Request Units (RUs).","B":"Turn off indexing.","D":"Create unique indexes.","E":"Create compound indexes."},"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/19060-exam-dp-201-topic-1-question-23-discussion/","question_text":"You have a MongoDB database that you plan to migrate to an Azure Cosmos DB account that uses the MongoDB API.\nDuring testing, you discover that the migration takes longer than expected.\nYou need to recommend a solution that will reduce the amount of time it takes to migrate the data.\nWhat are two possible recommendations to achieve this goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","answer_images":[],"answer_description":"A: Increase the throughput during the migration by increasing the Request Units (RUs).\nFor customers that are migrating many collections within a database, it is strongly recommend to configure database-level throughput. You must make this choice when you create the database. The minimum database-level throughput capacity is 400 RU/sec. Each collection sharing database-level throughput requires at least 100 RU/sec.\nB: By default, Azure Cosmos DB indexes all your data fields upon ingestion. You can modify the indexing policy in Azure Cosmos DB at any time. In fact, it is often recommended to turn off indexing when migrating data, and then turn it back on when the data is already in Cosmos DB.\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/Azure/cosmos-db/mongodb-pre-migration","exam_id":66,"question_id":16,"answer_ET":"AB","timestamp":"2020-04-25 16:25:00","unix_timestamp":1587824700},{"id":"yN98pqvNOlhzohjLpF8X","question_images":[],"question_id":17,"timestamp":"2020-07-01 12:48:00","unix_timestamp":1593600480,"answer_images":[],"answer_ET":"A","question_text":"You need to recommend a storage solution for a sales system that will receive thousands of small files per minute. The files will be in JSON, text, and CSV formats. The files will be processed and transformed before they are loaded into a data warehouse in Azure Synapse Analytics. The files must be stored and secured in folders.\nWhich storage solution should you recommend?","url":"https://www.examtopics.com/discussions/microsoft/view/24519-exam-dp-201-topic-1-question-24-discussion/","answer":"A","exam_id":66,"choices":{"C":"Azure SQL Database","A":"Azure Data Lake Storage Gen2","D":"Azure Blob storage","B":"Azure Cosmos DB"},"answer_description":"Azure provides several solutions for working with CSV and JSON files, depending on your needs. The primary landing place for these files is either Azure Storage or Azure Data Lake Store.1\nAzure Data Lake Storage is an optimized storage for big data analytics workloads.\nIncorrect Answers:\nD: Azure Blob Storage containers is a general purpose object store for a wide variety of storage scenarios. Blobs are stored in containers, which are similar to folders.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/scenarios/csv-and-json","isMC":true,"topic":"1","discussion":[{"comment_id":"152876","timestamp":"1596866040.0","poster":"Yaswant","upvote_count":"38","content":"Blob Storage -> Object Storage (Binary / Flatfiles)\nDataLake -> Various formats (CSV, Json, Avro, Parquet.../ Folders)"},{"timestamp":"1623136860.0","poster":"cadio30","upvote_count":"2","content":"ADLS is the appropriate solution for this requirement as it was indicated the use of \"folder\" and is a good holder of big data files.","comment_id":"377296"},{"content":"Even I am thinking to opt ADLS2 but the only thing to mention is several files per minute which might impact adls2 which is good for bigdata work loads as compared to blob? Which one is correct any source?","upvote_count":"1","comment_id":"366788","timestamp":"1621994940.0","poster":"Saravjeet"},{"poster":"sjain91","upvote_count":"4","content":"Azure datalake storage gen 2","comment_id":"346840","timestamp":"1619864760.0"},{"poster":"Deepu1987","content":"The k/word to lookout for is \"folders\" which means ADLS Gen 2 which is built on top of Az Blob Strg it's a container - vir dir.. where as ADLS Gen 2 is like accumulation of files & it's like a folder.","timestamp":"1613992620.0","upvote_count":"4","comment_id":"296584"},{"content":"ADLS is good for analytics solutions. This requirement has that, that is why ADLS","timestamp":"1608172080.0","upvote_count":"1","comment_id":"246129","poster":"Hardik17"},{"comment_id":"235669","timestamp":"1607168640.0","upvote_count":"4","content":"The files must be stored and secured in folders.\nA is the answer for sure","poster":"syu31svc"},{"upvote_count":"1","timestamp":"1601552940.0","comment_id":"190928","poster":"timebeing","content":"You can also query JSON files directly from Azure Blob Storage without importing them into Azure SQL. For a complete example of this approach, see Work with JSON files with Azure SQL. Currently this option isn't available for CSV files."},{"timestamp":"1599012300.0","comment_id":"171665","upvote_count":"1","content":"I think it will be be ADLS as it supports all file format and will handle any flow of small files as long as we are not retaining them for a longer period there should not be any problem.","poster":"Jatinmaya"},{"content":"Blob is correct","comments":[{"poster":"chaoxes","comment_id":"245911","content":"It is not. Correct answer is Azure Data Lake Gen 2. It is build on top of Blob Storage + hierarchical namespace (folders). The questions includes file in folders as requirement.","timestamp":"1608148380.0","upvote_count":"5"}],"comment_id":"164745","upvote_count":"1","timestamp":"1598226780.0","poster":"Bob123456"},{"poster":"Bob123456","comment_id":"150890","content":"I too agree . because of small files per minute , Which is not ideal for Datalake . Correct answer is BLOB STORAGE.","comments":[{"content":"Question say folder, not containers. I think correct answer is ADLS","comment_id":"200748","upvote_count":"5","timestamp":"1602793740.0","poster":"Jzerpa_ccs"},{"content":"Small files issue was with DLS Gen 1. I think because Gen 2 is using Blob Storage in the background, it is not the case with Gen 2 any more. So, the given solution is correct.","upvote_count":"2","timestamp":"1603453140.0","poster":"M0e","comment_id":"204633"},{"content":"ADLS Gen 2 is everything Blob is, plus hierarchical capabilities","comment_id":"212577","timestamp":"1604481600.0","upvote_count":"7","poster":"arkadipb"}],"timestamp":"1596601260.0","upvote_count":"2"},{"content":"There is many small files, and file types are text types, what means the DLS solution, which is BigData like storage is a bad idea. DLS distributed file storage like big files and types of Parquet (columnar optimized). So the correct answer should be Blob Storage in my opinion","comments":[{"poster":"peppele","comment_id":"137910","upvote_count":"33","content":"Folders = ADLS","timestamp":"1595077560.0"}],"timestamp":"1593600480.0","upvote_count":"3","comment_id":"124170","poster":"mirr84"}],"answers_community":[]},{"id":"UnAJJL3LvvRIdVt1XLQS","answers_community":[],"exam_id":66,"answer":"C","isMC":true,"question_id":18,"url":"https://www.examtopics.com/discussions/microsoft/view/38885-exam-dp-201-topic-1-question-25-discussion/","answer_images":[],"choices":{"C":"Gremlin","B":"Cassandra","D":"Table","A":"SQL"},"unix_timestamp":1607167440,"answer_ET":"C","discussion":[{"comment_id":"247828","poster":"chaoxes","content":"1000000% is C Gremlin API.\nWhen we talk about edges/nodes and relationships, its Gremlin API","upvote_count":"15","timestamp":"1608365880.0"},{"timestamp":"1613992860.0","comment_id":"296589","content":"True. It's also used in Socai n/ws , Recommendation engines , Geospatial, IoT","poster":"Deepu1987","upvote_count":"2"},{"comment_id":"235634","poster":"syu31svc","content":"100% is C","upvote_count":"4","timestamp":"1607167440.0"}],"topic":"1","answer_description":"The Azure Cosmos DB Gremlin API can be used to store massive graphs with billions of vertices and edges.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction","timestamp":"2020-12-05 12:24:00","question_text":"You are designing an Azure Cosmos DB database that will support vertices and edges.\nWhich Cosmos DB API should you include in the design?","question_images":[]},{"id":"UQj2m6KJnWyre4zNZq5W","answers_community":[],"unix_timestamp":1607167920,"answer_ET":"A","isMC":true,"question_text":"You are designing a big data storage solution. The solution must meet the following requirements:\n✑ Provide unlimited account sizes.\n✑ Support a hierarchical file system.\n✑ Be optimized for parallel analytics workloads.\nWhich storage solution should you use?","answer_description":"Azure Data Lake Storage is optimized performance for parallel analytics workloads\nA key mechanism that allows Azure Data Lake Storage Gen2 to provide file system performance at object storage scale and prices is the addition of a hierarchical namespace. This allows the collection of objects/files within an account to be organized into a hierarchy of directories and nested subdirectories in the same way that the file system on your computer is organized.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-namespace","answer":"A","discussion":[{"poster":"syu31svc","timestamp":"1607167920.0","content":"100% is A","upvote_count":"16","comment_id":"235649"},{"timestamp":"1626187020.0","upvote_count":"1","comment_id":"405495","poster":"achamizo","content":"ADLS Gen 2 is the correct"},{"upvote_count":"2","content":"Keyword is \"Hierarchical\", so it should be ADLS Gen 2","timestamp":"1621275840.0","poster":"IAMKPR","comment_id":"359773"},{"comment_id":"346841","content":"Azure data lake storage gen2","poster":"sjain91","timestamp":"1619864940.0","upvote_count":"2"}],"question_images":[],"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/38886-exam-dp-201-topic-1-question-26-discussion/","exam_id":66,"timestamp":"2020-12-05 12:32:00","question_id":19,"choices":{"A":"Azure Data Lake Storage Gen2","D":"Azure Cosmos DB","B":"Azure Blob storage","C":"Apache HBase in Azure HDInsight"}},{"id":"2DRlAyiVZkcOcTpL4mvX","isMC":true,"topic":"1","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to store delimited text files in an Azure Data Lake Storage account that will be organized into department folders.\nYou need to configure data access so that users see only the files in their respective department folder.\nSolution: From the storage account, you enable a hierarchical namespace, and you use RBAC.\nDoes this meet the goal?","discussion":[{"comment_id":"152940","timestamp":"1596874500.0","upvote_count":"30","poster":"Yaswant","content":"RBAC -> Container level.\nACL -> Each file and directory in your account.\n*NO*"},{"comment_id":"115239","upvote_count":"9","poster":"Abhilvs","content":"'No' is correct. When you set ACL, if RBAC is enabled on that container, it takes precedence over ACL. So, RBAC should be disabled when using ACL","timestamp":"1592716980.0"},{"timestamp":"1619865060.0","upvote_count":"1","poster":"sjain91","content":"Answer: No\nAzure RBAC : Storage accounts, containers. Cross resource Azure role assignments at subscription or resource group level.","comment_id":"346842"},{"poster":"vaseva1","upvote_count":"1","timestamp":"1617777960.0","comment_id":"330102","content":"Answer: No\nAzure RBAC : Storage accounts, containers. Cross resource Azure role assignments at subscription or resource group level.\n\nACL : Directory, file"},{"poster":"Pavanm34","content":"Data lake gen2 with hierarchical namespace support ACLS . Currently we can not set it up from storage explorer and portal.\n\nSupport for setting access control lists (ACLs) recursively\nThe ability to apply ACL changes recursively from parent directory to child items is generally available. In the current release of this capability, you can apply ACL changes by using PowerShell, Azure CLI, and the .NET, Java, and Python SDK. Support is not yet available for the Azure portal, or Azure Storage Explorer.","upvote_count":"1","timestamp":"1609871220.0","comment_id":"260473"},{"comment_id":"238323","poster":"syu31svc","upvote_count":"1","content":"Answer given is correct\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-namespace\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control","timestamp":"1607436540.0"},{"upvote_count":"5","content":"Answer: No\nThe only correct case: ACL & HNS enabled.\n\n\nAzure RBAC and ACL both require the user (or application) to have an identity in Azure AD. Azure RBAC lets you grant \"coarse-grain\" access to storage account data, such as read or write access to all of the data in a storage account, while ACLs let you grant \"fine-grained\" access, such as write access to a specific directory or file.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model","timestamp":"1606939980.0","comment_id":"233409","poster":"brcdbrcd"},{"comment_id":"178955","poster":"rmk4ever","upvote_count":"1","content":"Ans: NO. \nGeneral-purpose V2 -->Blob container ACL- Not yet supported\nYou can set ACLs on the root folder of the container but not the container itself.\n\nCan't use ACL in data lake. (can't use in HNS enabled storage account)\nRef: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-supported-blob-storage-features","timestamp":"1600041180.0","comments":[{"timestamp":"1600041780.0","content":"Sorry, please ignore the first one.\n\nFor Gen2 - can use ACL with HNS\nref: https://docs.microsoft.com/en-us/azure/storage/blobs/recursive-access-control-lists?tabs=azure-powershell","poster":"rmk4ever","comment_id":"178961","upvote_count":"1"}]},{"comment_id":"155863","poster":"Ash666","content":"No. We need ACL.","timestamp":"1597180380.0","upvote_count":"1"},{"poster":"freia","timestamp":"1594366680.0","comment_id":"131245","content":"HNS should not be disabled. \"Access control via ACLs is enabled for a storage account as long as the Hierarchical Namespace (HNS) feature is turned ON.\" (https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control)","upvote_count":"4"},{"comment_id":"115241","upvote_count":"1","content":"ACLs are granular and only evaluated when RBAC if permissions aren't granted with RBAC.","poster":"Abhilvs","timestamp":"1592717100.0"},{"timestamp":"1589000580.0","comment_id":"85934","content":"answer should be yes. In RBAC, minimum level of scope to implement security is at container level. Folder level auth is not possible. It needs ACL for that. No reason to disable HNS (data lake ) for that, we can use POSIX permissions provided by data lake to implement folder level permissions.","upvote_count":"4","comments":[{"upvote_count":"3","timestamp":"1590059460.0","comments":[{"content":"Yes that makes sense to me","upvote_count":"1","poster":"drdean","timestamp":"1591820580.0","comment_id":"107160"}],"content":"Isn't the solution saying that RBAC is wrong? If at folder level Auth, RBAC is not possible, then No is correct. Thoughts?","comment_id":"93344","poster":"runningman"}],"poster":"azurearch"}],"url":"https://www.examtopics.com/discussions/microsoft/view/17980-exam-dp-201-topic-1-question-27-discussion/","answer":"B","answer_description":"Disable the hierarchical namespace. And instead of RBAC use access control lists (ACLs).\nNote: Azure Data Lake Storage implements an access control model that derives from HDFS, which in turn derives from the POSIX access control model.\nBlob container ACLs does not support the hierarchical namespace, so it must be disabled.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control","question_id":20,"choices":{"A":"Yes","B":"No"},"answer_ET":"B","exam_id":66,"answer_images":[],"question_images":[],"timestamp":"2020-04-06 14:41:00","answers_community":[],"unix_timestamp":1586176860}],"exam":{"isBeta":false,"lastUpdated":"12 Apr 2025","isMCOnly":false,"id":66,"name":"DP-201","isImplemented":true,"numberOfQuestions":206,"provider":"Microsoft"},"currentPage":4},"__N_SSP":true}