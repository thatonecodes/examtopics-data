{"pageProps":{"questions":[{"id":"r57BlhH2glj8U8JEOjXn","isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011800001.jpg"],"topic":"1","answer":"","timestamp":"2022-04-21 09:55:00","answers_community":[],"discussion":[{"comments":[{"timestamp":"1650894780.0","comment_id":"591756","upvote_count":"5","comments":[{"timestamp":"1651342200.0","comments":[{"poster":"Boompiee","comments":[{"upvote_count":"2","poster":"Aditya0891","comment_id":"612648","timestamp":"1654591080.0","content":"whet he meant to say is all the data be it in hot, cool or achieve resides in the blob. So if you delete the blob it will delete all the data be it 5 years or 7 years or more recent data in hot tier. Delete blob option is just to make it a tricky question","comments":[{"timestamp":"1657938600.0","content":"Why it will delete data in hot tier? I have a blob. Blob between 5 and 7 go to cool, more than 7 delete.","poster":"temacc","comment_id":"631979","upvote_count":"1"},{"content":"Why is 'Delete the blob' not a valid option? Given that seven years data is not accessed, why don't we delete the blob?","poster":"cgarciamarco1","timestamp":"1664343300.0","upvote_count":"1","comment_id":"681413"}]}],"content":"I'm confused by your comment. It clearly does state an option to delete the blob after 7 years.","timestamp":"1652191140.0","upvote_count":"2","comment_id":"599624"},{"content":"God no... A single piece of data could be a BLOB (Binary Large OBJECT).","timestamp":"1687639560.0","comment_id":"932917","upvote_count":"2","poster":"vctrhugo"}],"comment_id":"595192","content":"Deleting data older than 7 years is not an option available in the answer list. Becareful of the gotcha; 'Delete the blob' is an option but it would delete all the data, included the ones that are e.g. 5 years old. So you can't choose that answer. So the next best thing to do is to put it into archive.","poster":"KashRaynardMorse","upvote_count":"30"},{"poster":"shakes103","content":"Be careful with wording. Any answer given must be an \"Archiving solution\" & Delete the blob is not an archiving solution.","timestamp":"1674228300.0","comments":[{"comment_id":"1311012","content":"Yes, right!","poster":"Piantoni","timestamp":"1731459540.0","upvote_count":"1"},{"comment_id":"893349","timestamp":"1683656940.0","upvote_count":"4","poster":"AnonymousJhb","content":"of course it is."}],"upvote_count":"7","comment_id":"782407"}],"poster":"Massy","content":"I agree, should be deleted"},{"timestamp":"1650903660.0","content":"Makes sense to me","comment_id":"591831","poster":"noobprogrammer","upvote_count":"2"},{"content":"Most important part about that question is \"Costs must be minimized while maintaining the required availability.\". So deleting isn't an option, since it won't be available anymore. So ARCHIVE seems correct","timestamp":"1705230180.0","poster":"vernillen","upvote_count":"2","comment_id":"1122476"},{"comment_id":"1132013","upvote_count":"1","poster":"Gikan","content":"The documentation, which is linked into the answer: \"Archive tier - An offline tier optimized for storing data that is rarely accessed, and ...\"","timestamp":"1706212560.0"},{"timestamp":"1650892260.0","content":"Would agree, but the question states: \"a data archiving solution\", so maybe to keep the data was implied with this?","upvote_count":"16","poster":"Feljoud","comments":[{"timestamp":"1683656880.0","content":"no. part of data management is deleted data that is no longer required. not keeping all data forever. you are allowed to delete data once it meets required guardrails. delete deprecated data > 7 years.","upvote_count":"4","poster":"AnonymousJhb","comment_id":"893348"}],"comment_id":"591728"}],"poster":"sagur","comment_id":"589193","upvote_count":"40","content":"If \"Data that is older than seven years is NOT accessed\" then this data can be deleted to minimize the storage costs, right?","timestamp":"1650527700.0"},{"poster":"PeteZaria","comments":[{"content":"The question states that data older than 7 years is NOT accessed. However, there is nothing stating that the company no longer needs the data. It should be archived because the company may still have a regulatory or legal requirement to keep this data.","timestamp":"1722715980.0","poster":"7082935","upvote_count":"1","comment_id":"1260430"}],"comment_id":"691609","timestamp":"1665447300.0","content":"Answer is correct. RE: Why is 'Delete the blob' not a valid option? Well I agree that 7 years may seem a long time to most who commented here BUT there is ABSOLUTELY NO mention to DELETE here, In several context NOT ACCESSED can easily refer to be drawn OFFLINE in ARCHIVE IOW:\n\"Your data files may be stored in the archive access tier in Azure Blob storage based on different context needs. According to the Azure documentation: While a blob is in archive storage, the blob data is OFFLINE and CANNOT BE ACCESSED that is: read, copied, overwritten, or modified. You also can't take snapshots of a blob in archive storage. However, the blob METADA remains online and available, allowing you to list the blob and its properties. For blobs in archive, the only valid operations are GetBlobProperties, GetBlobMetadata, ListBlobs, SetBlobTier, and DeleteBlob. For more information about Azure Blob storage tiers, see the Azure documentation: \nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers.","upvote_count":"9"},{"comment_id":"1307366","upvote_count":"2","poster":"tmz1","timestamp":"1730810580.0","content":"I think the keyword in this question is \"data archiving solution\". Which means that option to delete anything is not considered"},{"poster":"rlnd2000","upvote_count":"2","timestamp":"1719491460.0","content":"In my opinion, the retention policy clearly states that any file or blob older than seven years is a violation. The policy specifies: \"Data that is older than seven years is NOT accessed.\" Therefore, any data older than seven years should be deleted to avoid unnecessary costs, as we should not pay for storage we don't need. This approach aligns with the requirement to \"minimize costs while maintaining the required availability.\"\n\nFor data retention:\n\nData older than five years should be moved to the cool storage tier.\nData older than seven years should be deleted.","comment_id":"1238144"},{"comment_id":"1224576","timestamp":"1717570980.0","content":"Why would keep any data in archive if it is NOT accessed? That would be just waste of money and possible violation of give data protection regulations.","poster":"otapi","upvote_count":"2"},{"upvote_count":"4","comments":[{"comment_id":"1231231","timestamp":"1718520240.0","upvote_count":"2","content":"Damn Right!! With my 10 years exp never ever delete the data until its being asked by business owners for cost reduction. It will end up reduction in work force in form of you.","poster":"Sr18"}],"comment_id":"1193675","poster":"dgerok","timestamp":"1712829960.0","content":"1) Cool, because it takes hours to access Archive data, but cool is cheaper than Hot\n2) archive. For those, who thinks, that \"Data that is older than seven years is NOT accessed\" is equal to \"the data can be deleted\", you're fired :)"},{"comment_id":"1183380","poster":"Little_Soap","content":"Data older than 7 years old is NOT accessed.\nArchived blobs are not accessible and deleting the blobs would be too aggressive so I'll go with archive for the second answer with the question as is.","upvote_count":"1","timestamp":"1711462740.0"},{"timestamp":"1711311060.0","poster":"Alongi","comment_id":"1181953","upvote_count":"1","content":"Cool Storage and Delete job, because in this way you can seriously minimize costs"},{"content":"i think you guys are really overthinking it. just f....g delete data that will never be accesed.","poster":"dataalex","comment_id":"1163361","timestamp":"1709285700.0","upvote_count":"1"},{"poster":"Charley92","upvote_count":"1","timestamp":"1706324700.0","content":"Costs must be minimized while maintaining the required availability. -- > DO NOT DELETE","comment_id":"1133007"},{"poster":"d046bc0","upvote_count":"1","comment_id":"1093329","timestamp":"1702287780.0","content":"correct. deleting files is not a type of archiving them (chatGPT)"},{"upvote_count":"1","comment_id":"1001263","poster":"hassexat","timestamp":"1694070420.0","content":"Cool & Archive\n\nNo deletion of data here because question require to perform a data archiving solution"},{"upvote_count":"2","poster":"kkk5566","comment_id":"989669","content":"correct","timestamp":"1692939720.0"},{"timestamp":"1692666180.0","upvote_count":"1","content":"The question says: ✑ Provides the highest degree of data resiliency and this would be RA-GZRS with customer failover. Data is replicated to 3 zones in the primary region and async to a secondary region. If primary zone fails, there is read and write to other zones in the region. Microsoft only engages a failover if \"original primary region is deemed unrecoverable within a reasonable amount of time due to a major disaster.\", which is not the case here. The answer provided is correct","comment_id":"986989","poster":"Saintu"},{"upvote_count":"1","comment_id":"929319","timestamp":"1687342080.0","content":"\"Costs must be minimized while maintaining the required availability.\" ==> it means Move to archive storage\"","poster":"auwia"},{"content":"The Given answer is correct,the big tip here is archiving solution,deleting is not archiving solution","poster":"GodfreyMbizo","comment_id":"793086","upvote_count":"5","timestamp":"1675098900.0"},{"timestamp":"1667476380.0","poster":"sumanthss","comment_id":"710456","upvote_count":"1","content":"Archive might be the right option as we need to retrive before we access the data. so we cannot access without retrieval."},{"poster":"cgarciamarco1","content":"Why is 'Delete the blob' not a valid option? Given that seven years data is not accessed, why don't we delete the blob?","upvote_count":"1","comment_id":"681412","timestamp":"1664343240.0"},{"comment_id":"639504","comments":[{"poster":"Deeksha1234","timestamp":"1660536960.0","content":"If data needs to be available even though we don't access it then we can put it in Archive layer","upvote_count":"1","comment_id":"647018"},{"poster":"Rohitsk42069","upvote_count":"1","timestamp":"1661240040.0","comment_id":"650664","content":"Data will be accessed rarely.. why waste money on high storage on hot? cool has low storage cost and high cost for accessing which suites better"}],"upvote_count":"1","poster":"Deeksha1234","content":"Hot for 5 years and delete if older than 7 yrs since it'll not be accssed","timestamp":"1659161400.0"},{"poster":"AvinashBolleddula","timestamp":"1658809380.0","upvote_count":"1","comments":[{"content":"doesn't say that. It's not accessed, never ever. So delete seems the only logical option.","poster":"makkelijkzat","timestamp":"1683100800.0","upvote_count":"1","comment_id":"888280"}],"comment_id":"637142","content":"I think shouldn't be deleted because we need data even though it is 7 years old, but we don't access it.."},{"comment_id":"635065","timestamp":"1658474580.0","poster":"kishan_peter_pandey","content":"Data should be archived as in question it's mentioned \"while maintaining required availability\"","upvote_count":"2"},{"poster":"namtn6","content":"correct answer without doubt","comment_id":"625617","timestamp":"1656659340.0","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/microsoft/view/73989-exam-dp-203-topic-1-question-57-discussion/","question_id":71,"exam_id":67,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0011900001.jpg","https://www.examtopics.com/assets/media/exam-media/04259/0012000001.jpg"],"unix_timestamp":1650527700,"answer_description":"Box 1: Move to cool storage -\n\nBox 2: Move to archive storage -\nArchive - Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.\nThe following table shows a comparison of premium performance block blob storage, and the hot, cool, and archive access tiers.\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers","answer_ET":"","question_text":"HOTSPOT -\nYou have an Azure Data Lake Storage Gen2 service.\nYou need to design a data archiving solution that meets the following requirements:\n✑ Data that is older than five years is accessed infrequently but must be available within one second when requested.\n✑ Data that is older than seven years is NOT accessed.\n✑ Costs must be minimized while maintaining the required availability.\nHow should you manage the data? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//"},{"id":"iPv42CqS2mrY5Jvq78pL","answer_ET":"","answers_community":[],"question_id":72,"url":"https://www.examtopics.com/discussions/microsoft/view/80300-exam-dp-203-topic-1-question-58-discussion/","timestamp":"2022-09-05 12:31:00","exam_id":67,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0012200001.jpg"],"isMC":false,"answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=/azure/storage/blobs/toc.json https://docs.microsoft.com/en-us/answers/questions/32583/azure-data-lake-gen2-disaster-recoverystorage-acco.html","unix_timestamp":1662373860,"discussion":[{"poster":"aniagnesighile1","comments":[{"poster":"dgerok","upvote_count":"3","timestamp":"1712830500.0","comment_id":"1193679","content":"using your link find this\n\"If the primary region becomes unavailable, you can choose to fail over to the secondary region. After the failover has completed, the secondary region becomes the primary region, and you can again read and write data. \"\nSo, the answer is right, but it's a pity, not yours.","comments":[{"upvote_count":"10","timestamp":"1721990880.0","content":"Question talks about Data Center, not region (\"if a primary data center\"). When you chose ZRS you are replicating in 3 Data Centers. You don't need GRS or GZRS","comment_id":"1255670","poster":"practia"}]},{"poster":"Billybob0604","timestamp":"1669831980.0","upvote_count":"1","comment_id":"731839","content":"you're right. \nhttps://bluexp.netapp.com/blog/azure-anf-blg-azure-storage-replication-explained-lrs-zrs-grs-ra-grs#h_h3"},{"comment_id":"932918","upvote_count":"12","content":"You can still write data to second region if first one fails. RA only allows you to read data in second region even if the first does not fail.","timestamp":"1687639680.0","poster":"vctrhugo"},{"upvote_count":"10","content":"What about the highest level of data resiliency that cant be in ZRS .... answer is RA-GZRS","comments":[{"upvote_count":"1","content":"Having \"Provides the highest degree of data resiliency\" as a requirement automatically sends you to the most redundant storage option.","poster":"7082935","timestamp":"1722716160.0","comment_id":"1260431"}],"poster":"suvec","timestamp":"1681111440.0","comment_id":"866085"},{"timestamp":"1702088700.0","upvote_count":"3","content":"its data center failure not region failure. ZRS is correct","poster":"SM94","comment_id":"1091403"},{"timestamp":"1669105140.0","comment_id":"724168","upvote_count":"15","content":"Reading from the following, I do not think RAGZRS is ready only. I think it is read-only for the second region.\n\nGZRS writes three copies of your data synchronously across multiple Azure Availability zones, similar to zone-redundant storage (ZRS), providing you continued read and write access even if a datacenter or availability zone is unavailable. In addition, GZRS asynchronously replicates your data to the secondary geo-pair region to protect against regional unavailability. RA-GZRS exposes a read endpoint on this secondary replica allowing you to read data in the event of primary region unavailability.","comments":[{"timestamp":"1669821900.0","comments":[{"comments":[{"comment_id":"888572","timestamp":"1683121500.0","content":"I have created an account only to type exactly this but you beat me to it, so at least i will say... Exactly this above.","poster":"Milderengen","upvote_count":"1"},{"comment_id":"893365","upvote_count":"7","poster":"AnonymousJhb","timestamp":"1683657960.0","content":"the answer is wrong.\nMicrosoft recommends using ZRS in the primary region for Azure Data Lake Storage Gen2 Workloads.\nWith Microsoft failover."}],"timestamp":"1681804620.0","content":"aniagnesighile1 but you dont need the secondary region to write to in case of the fail of a data center; with ra-grs geo-zone reduntant storage you still have 2 other zones with 2 other data centers that you can write to; so the requirement of being able to continue to write to is fullfilled; however with ra-grs you have also a secondary region, so in case of region fail you can failover to secondary region, and this gives you a higher resiliance than zrs only; so in my opinion the given answer is correct;","upvote_count":"7","comment_id":"873400","poster":"mamahani"}],"comment_id":"731676","content":"Again, based on the statement, 'Ensures that content remains available for writes if a primary data center fails', when the primary data center fails, you only have the secondary data center to work with. Now with RAGZRS, you only have the ability to read from the second region. You said it yourself ' I think it is read-only for the second region.'. Remember the primary data center is down, but the requirements states ENSURE THAT CONTENT REMAINS AVAILABLE FOR WRITES IF PRIMARY DATA CENTER FAILS. How are you going to write to the secondary datacenter.","poster":"aniagnesighile1","upvote_count":"6"},{"poster":"yogiazaad","upvote_count":"1","comment_id":"794290","content":"You are right.","timestamp":"1675175700.0"},{"upvote_count":"1","poster":"Gikan","timestamp":"1706213100.0","comment_id":"1132019","content":"GZRS, GRS needs time to switch between regions at failover. That is why RA-GZRS(GRS) is more expensive."},{"content":"Correct. ra-GZRS is the highest level that covered ZRS. When a DataCenter is downed, you still can write in another zone within the same region.","poster":"Sebastian1677","upvote_count":"6","comment_id":"750547","timestamp":"1671515820.0"}],"poster":"SomethingRight100"}],"comment_id":"714139","timestamp":"1667945280.0","content":"I am surprised you all missed this requirement 'Ensures that content remains available for writes if a primary data center fails'. RA-GRS and RAGZRS provide read access only after failover. The correct answer is ZRS as t=stated in the link below \"Microsoft recommends using ZRS in the primary region for Azure Data Lake Storage Gen2 workloads.\" https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json","upvote_count":"81"},{"timestamp":"1662373860.0","upvote_count":"26","comments":[{"upvote_count":"22","poster":"Gg2","comment_id":"665548","comments":[{"content":"\"Until the Microsoft-managed failover has completed, you won't have write access to your storage account.\" Hence, I guess it should NOT be Microsoft-managed failover.\n\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json","poster":"Metaalverf","upvote_count":"1","comment_id":"1054685","timestamp":"1698332400.0"}],"content":"RA-GZRS\nFailover initiated by Microsoft.","timestamp":"1662828900.0"}],"comment_id":"660051","content":"Failover initiated by Microsoft.\nCustomer-managed account failover is not yet supported in accounts that have a hierarchical namespace (Azure Data Lake Storage Gen2). To learn more, see Blob storage features available in Azure Data Lake Storage Gen2.","poster":"chinomoreno"},{"content":"Use customer-managed failover options to develop, test, and implement your disaster recovery plans. Do not rely on Microsoft-managed failover, which might only be used in extreme circumstances. A Microsoft-managed failover would be initiated for an entire physical unit, such as a region or a datacenter. It can't be initiated for individual storage accounts, subscriptions, or tenants. If you need the ability to selectively failover your individual storage accounts","timestamp":"1738499280.0","poster":"prem__raj","upvote_count":"1","comment_id":"1350410"},{"comment_id":"1306873","content":"RA-GRS and failure initiated by Microsoft","timestamp":"1730714580.0","upvote_count":"2","poster":"vishnumaya"},{"comment_id":"1291852","upvote_count":"1","timestamp":"1727768100.0","content":"Region wasn't mention in the question, so how did you guys come about it? \nAnswer: ZRS and Failover initiated by Microsoft.","poster":"clowin"},{"timestamp":"1719757620.0","upvote_count":"2","poster":"ypan","comment_id":"1239688","content":"Failover Process:\n\nFailover initiated by Microsoft: In this scenario, Microsoft would initiate the failover process, which might not meet the requirement for maintaining availability for writes during a failure.\n\nCORRECT: Failover manually initiated by the customer: Allows the customer to control when the failover happens, ensuring that writes can be directed to the secondary region during a primary region failure.\n\nFailover automatically initiated by an Azure Automation job: This could provide automation for failover, but it may not give the same level of control as manual initiation by the customer."},{"timestamp":"1714478400.0","comment_id":"1204549","content":"Zone redundant, microsoft initiated","upvote_count":"1","poster":"Dusica"},{"poster":"Alongi","upvote_count":"1","timestamp":"1713624240.0","comment_id":"1199196","content":"Should be:\n- ZRS because the question talk about on a failure on the primary region for Azure Data Lake Storage Gen2 Workloads, so a Zone.\n- Failover initiated by Microsoft."},{"content":"Type Failover Scope Use case Expected data loss HNS supported\nCustomer-managed Storage account The storage service endpoints for the primary region become unavailable, but the secondary region is available.\n\nYou received an Azure Advisory in which Microsoft advises you to perform a failover operation of storage accounts potentially affected by an outage. Yes Yes (In preview)\nMicrosoft-managed Entire region or scale unit The primary region becomes completely unavailable due to a significant disaster, but the secondary region is available. Yes Yes","comment_id":"1187258","poster":"Elanche","timestamp":"1711954800.0","upvote_count":"1"},{"comment_id":"1156224","content":"Your disaster recovery plan should be based on customer-managed failover. Do not rely on Microsoft-managed failover, which might only be used in extreme circumstances. A Microsoft-managed failover would be initiated for an entire physical unit, such as a region or scale unit. It can't be initiated for individual storage accounts, subscriptions, or tenants. For the ability to selectively failover your individual storage accounts, use customer-managed account failover.\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance#microsoft-managed-failover","poster":"[Removed]","timestamp":"1708588740.0","upvote_count":"1"},{"upvote_count":"4","timestamp":"1708570140.0","poster":"j888","content":"\"Provides the highest degree of data resiliency\" and it didn't meantioned about the cost.\nI think GZRS will be the one to go for and it is microsoft initiated failover","comment_id":"1156045"},{"content":"RA-GZRS\nFailover manually initiated by the customer","poster":"Anto____","comment_id":"1136315","timestamp":"1706666100.0","upvote_count":"6"},{"upvote_count":"1","poster":"Souvik_79","timestamp":"1705061340.0","comment_id":"1120741","content":"Why is the ET Azure community so divided for most of the Multiple Correct Questions? This was not the case for GCP ET community.","comments":[{"poster":"[Removed]","comment_id":"1126520","upvote_count":"2","content":"Because the questions in Azure exams are written like crap. The wording in GCP questions is much more clearer","timestamp":"1705650600.0"}]},{"content":"Failover initiated by Microsoft:\nIn general, automatic failover initiated by Microsoft is the best option for most organizations, as it is the most reliable and efficient. However, if you need more control over the failover process, then you can choose to use automatic failover initiated by an Azure Automation job.\n\nRA-GZRS\nAvailable using RA-GZRS. RA-GZRS replicates data asynchronously to a secondary data center in a different Azure region. This means that there is a longer latency for replication, but it also means that your data is protected from regional outages. Additionally, RA-GZRS supports data compression and encryption, which can further improve storage efficiency.","timestamp":"1704921600.0","poster":"Joanna0","upvote_count":"1","comment_id":"1119042"},{"timestamp":"1703236560.0","upvote_count":"1","poster":"BitacTeam","comment_id":"1103267","content":"accroding to microsoft Documentation: \nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance\n\nit mentioned\n1-Customer-managed -- scope--> Storage account-- Case--> The storage service endpoints for the primary region become unavailable, but the secondary region is available.\n\nYou received an Azure Advisory in which Microsoft advises you to perform a failover operation of storage accounts potentially affected by an outage.\n\n2-Microsoft-managed-- Scope--> Entire region or scale unit -- Case--> The primary region becomes completely unavailable due to a significant disaster, but the secondary region is available.\n\nin the question it talks about Storage i.e. Data Center not a region failure. so the anwer is Customer managed"},{"poster":"ec255af","upvote_count":"2","comment_id":"1078758","timestamp":"1700770560.0","content":"anser is correct.\nRA-GZRS (Read Access Geo Zone Redundancy) provides read AND WRITE and has the highest availability (16 9's).\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#geo-zone-redundant-storage\nFor failover: iniciated by customer. In Microsofr case they MAY failover only if greater disaster happens.\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&bc=%2Fazure%2Fstorage%2Fblobs%2Fbreadcrumb%2Ftoc.json#microsoft-managed-failover"},{"timestamp":"1698064200.0","upvote_count":"2","comment_id":"1051825","poster":"Andrew_Chen","content":"GZRS Because it mentioned that when a data center failed, not the entire region. When a data centre failed, with ZRS, we can still read and write to the other two datacentres in the same region. So for the primary region, it must be ZRS. We still need a backup for the secondary region because it reqiures the highest degree of data resiliency. \nCustomer Managed Failover, No Doubt!!!"},{"timestamp":"1696769400.0","poster":"ellala","upvote_count":"1","comment_id":"1027965","content":"Careful as this question will change pretty soon:\n\"Customer-managed account failover for accounts that have a hierarchical namespace (Azure Data Lake Storage Gen2) is currently in PREVIEW and only supported in specific regions.\"\nTherefore it might soon be an option for ADLS2 as well."},{"upvote_count":"1","timestamp":"1694752980.0","comment_id":"1008129","content":"Since it should be available for Writing in the secondary region, ZRS should be the one that we should opt for, since others only provide read access.","poster":"Chemmangat"},{"content":"https://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance#microsoft-managed-failover\ninitiate by MS ,RAGZRS","timestamp":"1692940920.0","comment_id":"989680","poster":"kkk5566","upvote_count":"2"},{"timestamp":"1692666300.0","upvote_count":"3","comment_id":"986990","poster":"Saintu","content":"The question says: ✑ Provides the highest degree of data resiliency and this would be RA-GZRS with customer failover. Data is replicated to 3 zones in the primary region and async to a secondary region. If primary zone fails, there is read and write to other zones in the region. Microsoft only engages a failover if \"original primary region is deemed unrecoverable within a reasonable amount of time due to a major disaster.\", which is not the case here. The answer provided is correct"},{"content":"Microsoft recommends using ZRS in the primary region for Azure Data Lake Storage Gen2 Workloads.\nWith Microsoft failover - customer failover for hns is not yet supported","poster":"[Removed]","comment_id":"978138","upvote_count":"2","timestamp":"1691710380.0"},{"timestamp":"1687450200.0","content":"1. If your application requires resiliency, Microsoft recommends using geo-redundant storage. https://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance\n2. How an account failover works: https://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance","comment_id":"930765","poster":"Paulkuzzio","upvote_count":"3"},{"content":"should be initiated by Microsoft.\n==>\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance\n\" Note\nCustomer-managed account failover is not yet supported in accounts that have a hierarchical namespace (Azure Data Lake Storage Gen2). To learn more, see Blob storage features available in Azure Data Lake Storage Gen2.\n\nIn the event of a disaster that affects the primary region, Microsoft will manage the failover for accounts with a hierarchical namespace. For more information, see Microsoft-managed failover.\"","timestamp":"1685345760.0","upvote_count":"3","comment_id":"909143","poster":"dp_learner"},{"upvote_count":"1","content":"I think failover is automatically initiated by azure automation job. custoer do not have to direct control over initiating failover for GRS or RA-GRS storage","poster":"janaki","timestamp":"1684866060.0","comment_id":"905140"},{"poster":"rocky48","upvote_count":"2","timestamp":"1683592920.0","content":"It should be ZRS and Failover initiated by Microsoft.","comment_id":"892593"},{"timestamp":"1681904820.0","comments":[{"upvote_count":"1","timestamp":"1682675280.0","comment_id":"883386","content":"Lool.\n\nFunny how I just used ChatGPT and the answer is different. I typed in all the questions and options and it recommended Read-Access Geo-Zone-Redundant Storage and Failover Initiated by Microsoft.","poster":"Honour"}],"upvote_count":"6","content":"ChatGPT: for the given requirements, it is recommended to use ZRS replication mechanism and an automatic failover initiated by Microsoft to provide the highest degree of data resiliency and ensure that the content remains available for writes if a primary data center fails.","comment_id":"874571","poster":"Rossana"},{"comment_id":"869959","poster":"frankanalysis","content":"A region contains multiple availability zones, and an availability zone contains multiple data centres. So if a data centre goes down, you still have other data centres in the same availability zone and same region to write to. RA-GZRS is the correct answer. If the question asked which solution is best when the primary REGION fails, then yes, ZRS would be the correct choice.","timestamp":"1681446120.0","upvote_count":"2"},{"content":"Azure Storage supports account failover for geo-redundant storage accounts. With account failover, you can initiate the failover process for your storage account if the primary endpoint becomes unavailable. The failover updates the secondary endpoint to become the primary endpoint for your storage account. Once the failover is complete, clients can begin writing to the new primary endpoint.\n\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json%20https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fanswers%2Fquestions%2F32583%2Fazure-data-lake-gen2-disaster-recoverystorage-acco.html","poster":"Dhaval_Azure","comment_id":"848184","upvote_count":"1","timestamp":"1679574420.0"},{"poster":"peaceful_pierre","comment_id":"825875","content":"I think the answers are correct based on this: \nBefore you can perform an account failover on your storage account, make sure that your storage account is configured for geo-replication. Your storage account can use any of the following redundancy options:\n Geo-redundant storage (GRS) or read-access geo-redundant storage (RA-GRS)\n Geo-zone-redundant storage (GZRS) or read-access geo-zone-redundant storage (RA-GZRS)\nlink:https://learn.microsoft.com/en-us/azure/storage/common/storage-initiate-account-failover?tabs=azure-portal","upvote_count":"1","timestamp":"1677676860.0"},{"poster":"SHENOOOO","comment_id":"797180","content":"ZRS\nFailover initiated by Microsoft","upvote_count":"8","timestamp":"1675440180.0"},{"upvote_count":"4","comment_id":"789936","timestamp":"1674846840.0","poster":"juadaves","content":"Customer-managed account failover is not yet supported in accounts that have a hierarchical namespace (Azure Data Lake Storage Gen2)."},{"content":"https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#durability-and-availability-parameters\nParameter LRS ZRS GRS/RA-GRS GZRS/RA-GZRS\nAvailability for write requests At least 99.9% (99% for Cool or Archive access tiers) At least 99.9% (99% for Cool or Archive access tiers) At least 99.9% (99% for Cool or Archive access tiers) At least 99.9% (99% for Cool or Archive access tiers)\n\nwhen it comes to writes all of them are equally efficient. so ZRS should suffice. \nthe deciding factor would be resiliency, and the winner would be GZRS for sure. RA doesn't add anything but that is not an option. GZRS has 16 9s Percent durability of objects over a given year compared to 12 9s of ZRS.\n\nCost is not a criteria.\nSo RA-GZRS should be the answer.\n\nFailover initiated by Microsoft as customer initiated not supported on ADLS2\nhttps://www.examtopics.com/exams/microsoft/dp-203/view/#","timestamp":"1674666960.0","comment_id":"787938","poster":"Lestrang","upvote_count":"2"},{"timestamp":"1673536200.0","poster":"Harshal1920","content":"In the question, 'Primary data centers fail' is mentioned NOT the 'Primary Region fails' so the answer would be: \nA) ZRS\nB) Initiated by customer","comment_id":"773622","upvote_count":"6"},{"comment_id":"767984","poster":"yogiazaad","timestamp":"1673031360.0","content":"Customer-managed account failover is not yet supported in accounts that have a hierarchical namespace (Azure Data Lake Storage Gen2)\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#read-access-to-data-in-the-secondary-region","upvote_count":"4"},{"content":"Given answer is correct, read documentation\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-initiate-account-failover?tabs=azure-portal","upvote_count":"3","comment_id":"763806","poster":"Artur12345","timestamp":"1672672680.0"},{"comment_id":"731595","content":"In the link provided https://docs.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=/azure/storage/blobs/toc.json https://docs.microsoft.com/en-us/answers/questions/32583/azure-data-lake-gen2-disaster-recoverystorage-acco.html\nFailover is initiated for Gen 1 and Gen 2 accounts by MS Automation job. So the 2nd box should be Failover initiated by Microsoft.","timestamp":"1669818540.0","poster":"XiltroX","upvote_count":"1"},{"comment_id":"721992","content":"Given answer is wrong.\nFailover must be initiated by MS because of ADSL2. \n\"Customer-managed account failover is not yet supported in accounts that have a hierarchical namespace (Azure Data Lake Storage Gen2). To learn more, see Blob storage features available in Azure Data Lake Storage Gen2.\n\nIn the event of a disaster that affects the primary region, Microsoft will manage the failover for accounts with a hierarchical namespace.\" https://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json#:~:text=Customer%2Dmanaged%20account%20failover%20is,accounts%20with%20a%20hierarchical%20namespace.\nZRS is the only option that allows Write operations so can't see how any RA- can be correct answer.\nWith Change feed there is no option for any geo-recovery","timestamp":"1668861900.0","comments":[{"poster":"OldSchool","upvote_count":"2","timestamp":"1669978140.0","content":"Update:\nFailover by MS\nRA-GZRS explained here: https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#geo-zone-redundant-storage","comment_id":"733653"}],"poster":"OldSchool","upvote_count":"1"},{"upvote_count":"2","content":"Zone redundant + Initiated by Microsoft; The other 2 are read only so out of the question","poster":"Dusica","timestamp":"1668826620.0","comment_id":"721739"},{"poster":"RBKasemodel","upvote_count":"2","timestamp":"1668451200.0","comment_id":"718178","content":"\"Microsoft recommends using GZRS for applications requiring maximum consistency, durability, and availability, excellent performance, and resilience for disaster recovery.\"\n\nThe question asks for the highest degree of resilience. But there is no need for using the RA, it could be only GZRS"},{"comment_id":"710509","poster":"Jlozano","comments":[{"poster":"Billybob0604","content":"ZRS IS highest resiliency","timestamp":"1670761920.0","upvote_count":"1","comment_id":"741681"}],"timestamp":"1667480700.0","content":"Read Access Geo-Zone-Redundant-Storage is not RA-GRS but RA-GZRS.\nZRS cant be because is not a highest resiliency. Maybe GZRS could be an option but its not an possible answer. So i go for \"B\":\nGeo-Redundant-Storage RA-GRS","upvote_count":"2"},{"comment_id":"709744","content":"Failover managed by Microsoft.\nClearly mentioned in the below link.\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=/azure/storage/blobs/toc.json https://docs.microsoft.com/en-us/answers/questions/32583/azure-data-lake-gen2-disaster-recoverystorage-acco.html","upvote_count":"1","poster":"CodingOwl","timestamp":"1667385480.0"},{"upvote_count":"4","poster":"ted0809","content":"ZRS - because it has to \"Wrtie\" not just read","timestamp":"1667150160.0","comment_id":"707940"},{"content":"you can choose enable [hierarchical namespace] or disable, in this topic it didnt mention this, while it requires can write, means write to other region centers, so I go with manually initiated by customer, customer can point the data write functions to other point if primary point failed.","upvote_count":"1","timestamp":"1667017440.0","poster":"rzeng","comment_id":"706945"},{"timestamp":"1665061560.0","poster":"Ashwat_Thama","content":"Given Ans is correct\nNote : Until the Microsoft-managed failover has completed, you won't have write access to your storage account.","comment_id":"687831","upvote_count":"1"},{"content":"It should be ZRS. ReadA-GZRS is good for read access(as per Microsoft link) but the situation here require write access.","timestamp":"1663925040.0","upvote_count":"4","poster":"NintyFour","comment_id":"676955"},{"upvote_count":"5","content":"if only the data centre has failed, then even using ZRS and failover by microsoft we can achieve the result . RIGHT?","timestamp":"1663492860.0","poster":"dara_44_6880","comment_id":"672164"},{"upvote_count":"8","timestamp":"1662410340.0","content":"Answer is Correct !\nAvailability : \"Microsoft recommends RA-GZRS for maximum availability and durability for your applications.\"\n\nFailover: \"The customer initiates the account failover to the secondary endpoint. \"\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=/azure/storage/blobs/toc.json https://docs.microsoft.com/en-us/answers/questions/32583/azure-data-lake-gen2-disaster-recoverystorage-acco.html","comments":[{"comment_id":"721982","timestamp":"1668860400.0","poster":"cosarac","content":"Customer-managed account failover is not available for for hierarchical namespaces which is what Gen2 is !! there is a purple warning right above the scenario. It is microsofts fault to have single document for 2 separate types of storage.","upvote_count":"1"}],"poster":"anks84","comment_id":"660538"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0012100003.png"],"question_text":"HOTSPOT -\nYou plan to create an Azure Data Lake Storage Gen2 account.\nYou need to recommend a storage solution that meets the following requirements:\n✑ Provides the highest degree of data resiliency\n✑ Ensures that content remains available for writes if a primary data center fails\nWhat should you include in the recommendation? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","topic":"1","answer":""},{"id":"1Tg8kSd0TOr208U62lCH","answer_description":"A Type 3 SCD supports storing two versions of a dimension member as separate columns. The table includes a column for the current value of a member plus either the original or previous value of the member. So Type 3 uses additional columns to track one key instance of history, rather than storing additional rows to track each change like in a Type 2 SCD.\nThis type of tracking may be used for one or two columns in a dimension table. It is not common to use it for many members of the same table. It is often used in combination with Type 1 or Type 2 members.\n\nReference:\nhttps://k21academy.com/microsoft-azure/azure-data-engineer-dp203-q-a-day-2-live-session-review/","answer_ET":"BE","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/79019-exam-dp-203-topic-1-question-59-discussion/","answer":"BE","unix_timestamp":1662026820,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0012300001.png","https://www.examtopics.com/assets/media/exam-media/04259/0012300002.png","https://www.examtopics.com/assets/media/exam-media/04259/0012300003.png","https://www.examtopics.com/assets/media/exam-media/04259/0012300004.png","https://www.examtopics.com/assets/media/exam-media/04259/0012300005.png","https://www.examtopics.com/assets/media/exam-media/04259/0012300006.png"],"timestamp":"2022-09-01 12:07:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0012400001.jpg"],"isMC":false,"discussion":[{"upvote_count":"22","poster":"OldSchool","timestamp":"1670170920.0","content":"B & E is correct answer","comment_id":"735234"},{"comments":[{"poster":"7082935","comment_id":"1260432","timestamp":"1722716280.0","content":"Both BE and CE can be correct solutions. However, you should choose the one that is most descriptive to any business user who is consuming this data. So, describing each column as \"Current\" or \"Original\" is the better design.","upvote_count":"2","comments":[{"content":"I agree with you; however, in the table's design, there's already a column called [RowInsertedDateTime], which implies that it is tracking the insertion of the ProductCategory values. So, it would make no sense for one of the answers to be Option B. Therefore, I believe the correct answers are C and E.","comment_id":"1271796","upvote_count":"1","timestamp":"1724524140.0","poster":"renan_ineu"}]}],"timestamp":"1668275460.0","poster":"erhard","content":"If BE is correct, then CE is also correct.","upvote_count":"13","comment_id":"716847"},{"comment_id":"1219601","poster":"sergio_eduardo","timestamp":"1716818700.0","upvote_count":"2","content":"BE is correct but CE is also correct"},{"comment_id":"1195844","upvote_count":"1","poster":"dgerok","timestamp":"1713161640.0","content":"B & E is correct answer"},{"upvote_count":"1","content":"B & E is the correct answer","comment_id":"1001265","poster":"hassexat","timestamp":"1694070540.0"},{"content":"B & E is correct answer for SCD 3","upvote_count":"2","timestamp":"1693814520.0","comment_id":"998345","poster":"kkk5566"},{"comment_id":"660688","timestamp":"1662428700.0","poster":"anks84","upvote_count":"4","content":"Answer is correct"},{"upvote_count":"1","comment_id":"659203","poster":"dom271219","timestamp":"1662290580.0","comments":[{"comment_id":"702265","timestamp":"1666536540.0","upvote_count":"12","content":"A SCD can be of type 0, 1, 2, 3 and so on, please read the documentation","poster":"gerrie1979"}],"content":"Correct but SCD is always type 2. Type 3 is not SCD."},{"poster":"MuhammadK","timestamp":"1662026820.0","upvote_count":"1","comment_id":"656109","content":"Given answer is correct"}],"question_id":73,"topic":"1","question_text":"You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool.\nYou have a table that was created by using the following Transact-SQL statement.\n//IMG//\n\nWhich two columns should you add to the table? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.\nA.\n//IMG//\n\nB.\n//IMG//\n\nC.\n//IMG//\n\nD.\n//IMG//\n\nE.\n//IMG//","exam_id":67},{"id":"eK4sS5HCbBm84p2nGPtF","isMC":true,"question_text":"You are designing the folder structure for an Azure Data Lake Storage Gen2 container.\nUsers will query data by using a variety of services including Azure Databricks and Azure Synapse Analytics serverless SQL pools. The data will be secured by subject area. Most queries will include data from the current year or current month.\nWhich folder structure should you recommend to support fast queries and simplified folder security?","discussion":[{"upvote_count":"63","comment_id":"352624","timestamp":"1620497160.0","poster":"sagga","content":"D is correct\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#batch-jobs-structure"},{"comment_id":"1410538","timestamp":"1743018480.0","poster":"technoguy","upvote_count":"1","content":"Selected Answer: C\nsince we are querying b MM or year and data is secured by subject area. its better that we design the folder structure that does minimal scan of folder.\nhere in this case we should first create year , month nd then service area"},{"upvote_count":"1","poster":"ngabonzic","comment_id":"1400612","timestamp":"1742404740.0","content":"Selected Answer: D\nD is correct"},{"comment_id":"1317930","poster":"EmnCours","content":"Selected Answer: D\nD is correct","timestamp":"1732600500.0","upvote_count":"1"},{"comment_id":"1257025","timestamp":"1722194820.0","poster":"207680a","content":"D - Secured by subject area so top level folder security, Year first to avoid too many subfolder/partitions by each date","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nDefinitely D.","timestamp":"1704235620.0","poster":"sdg2844","comment_id":"1112335"},{"upvote_count":"2","poster":"dakku987","timestamp":"1702888140.0","content":"Selected Answer: D\nbcz i said so","comment_id":"1099537"},{"poster":"kkk5566","timestamp":"1693314780.0","content":"Selected Answer: D\nis correct","comment_id":"993116","upvote_count":"1"},{"timestamp":"1691156340.0","comment_id":"972207","upvote_count":"1","poster":"akhil5432","content":"Selected Answer: D\nD is correct"},{"timestamp":"1683064980.0","poster":"tbluhm","upvote_count":"1","content":"D Is the best way, filter subject area first the size queries will reduced just one and after by date.","comment_id":"887967"},{"timestamp":"1682556720.0","comment_id":"882124","upvote_count":"2","poster":"steveo123","content":"Selected Answer: D\nShould be D."},{"comment_id":"872723","content":"Selected Answer: D\nD is correct","poster":"pedr0oliveira","timestamp":"1681737480.0","upvote_count":"1"},{"timestamp":"1674334620.0","comment_id":"783730","content":"yes its D :)","poster":"DindaS","upvote_count":"1"},{"poster":"vigilante89","comment_id":"746800","timestamp":"1671164580.0","upvote_count":"3","content":"Selected Answer: D\nServerless SQL Pools offers a straight-forward method of querying data including CSV, JSON, and Parquet format stored in Azure Storage.\n\nSo, setting up the csv files within azure storage in hive-formated folder hierarchy i.e. /{yyyy}/{mm}/{dd}/ actually helps in sql querying the data much faster since only the partitioned segment of the data is queried."},{"poster":"Fernando_Caemerer","content":"Selected Answer: D\nD is correct","timestamp":"1670614440.0","upvote_count":"1","comment_id":"740386"},{"timestamp":"1660409640.0","comment_id":"646412","poster":"Deeksha1234","content":"Selected Answer: D\ncorrect","upvote_count":"1"},{"timestamp":"1660118760.0","comment_id":"644855","content":"Selected Answer: D\nD is correct","poster":"examtopicscap","upvote_count":"1"},{"upvote_count":"1","timestamp":"1656397200.0","poster":"StudentFromAus","content":"Selected Answer: D\nCorrect","comment_id":"623770"},{"comment_id":"600060","upvote_count":"1","content":"D is correct","timestamp":"1652266140.0","poster":"Dothy"},{"upvote_count":"1","timestamp":"1650815940.0","comment_id":"591147","content":"Selected Answer: D\nD is correct","poster":"Olukunmi"},{"timestamp":"1650147600.0","poster":"Egocentric","upvote_count":"1","content":"D is correct","comment_id":"586974"},{"timestamp":"1647888420.0","poster":"SebK","upvote_count":"2","content":"Selected Answer: D\nD is correct","comment_id":"572442"},{"poster":"RalphLiang","comment_id":"561283","timestamp":"1646464020.0","content":"Selected Answer: D\nD is correct","upvote_count":"1"},{"poster":"NeerajKumar","content":"Selected Answer: D\nCorrect","upvote_count":"1","timestamp":"1646227620.0","comment_id":"559437"},{"poster":"PallaviPatel","upvote_count":"1","content":"Selected Answer: D\nCorrect","comment_id":"532056","timestamp":"1643108940.0"},{"comment_id":"528545","upvote_count":"1","content":"D is correct","timestamp":"1642692840.0","poster":"Skyrocket"},{"timestamp":"1640793360.0","poster":"VeroDon","comment_id":"512359","content":"Selected Answer: D\nThats correct","upvote_count":"2"},{"content":"D is correct","comment_id":"509613","upvote_count":"1","poster":"Mahesh_mm","timestamp":"1640525160.0"},{"poster":"alexleonvalencia","comment_id":"497886","upvote_count":"1","content":"Respuesta correcta D.","timestamp":"1639067340.0"},{"timestamp":"1638883200.0","comment_id":"496043","upvote_count":"1","content":"Selected Answer: D\nD is correct","poster":"rashjan"},{"poster":"Sunnyb","upvote_count":"2","content":"D is absolutely correct","comment_id":"376131","timestamp":"1622991720.0"}],"question_images":[],"choices":{"D":"/{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv","A":"/{SubjectArea}/{DataSource}/{DD}/{MM}/{YYYY}/{FileData}_{YYYY}_{MM}_{DD}.csv","C":"/{YYYY}/{MM}/{DD}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv","B":"/{DD}/{MM}/{YYYY}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv"},"url":"https://www.examtopics.com/discussions/microsoft/view/52149-exam-dp-203-topic-1-question-6-discussion/","topic":"1","answer":"D","question_id":74,"answer_description":"","unix_timestamp":1620497160,"answer_images":[],"answer_ET":"D","exam_id":67,"answers_community":["D (96%)","4%"],"timestamp":"2021-05-08 20:06:00"},{"id":"tCyQsDv27W7OTWGN7GPo","timestamp":"2022-09-05 22:45:00","answers_community":[],"answer_description":"Box 1: Replicated -\nThe best table storage option for a small table is to replicate it across all the Compute nodes.\n\nBox 2: Hash -\nHash-distribution improves query performance on large fact tables.\n\nBox 3: Round-robin -\nRound-robin distribution is useful for improving loading speed.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute","question_text":"DRAG DROP -\nYou have an Azure subscription.\nYou plan to build a data warehouse in an Azure Synapse Analytics dedicated SQL pool named pool1 that will contain staging tables and a dimensional model.\nPool1 will contain the following tables.\n//IMG//\n\nYou need to design the table storage for pool1. The solution must meet the following requirements:\n✑ Maximize the performance of data loading operations to Staging.WebSessions.\n✑ Minimize query times for reporting queries against the dimensional model.\nWhich type of table distribution should you use for each table? To answer, drag the appropriate table distribution types to the correct tables. Each table distribution type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","answer_ET":"","question_id":75,"isMC":false,"exam_id":67,"url":"https://www.examtopics.com/discussions/microsoft/view/80428-exam-dp-203-topic-1-question-60-discussion/","answer":"","discussion":[{"poster":"anks84","comment_id":"660541","timestamp":"1662410700.0","upvote_count":"37","content":"Replicated (Because its a Dimension table)\nHash (Fact table with High volume of data)\nRound-Robin (Staging table)"},{"timestamp":"1731460920.0","upvote_count":"1","poster":"Piantoni","content":"Correct","comment_id":"1311023"},{"upvote_count":"1","content":"CORRECT","poster":"ff5037f","comment_id":"1302830","timestamp":"1729853460.0"},{"comment_id":"1196190","upvote_count":"1","poster":"Alongi","content":"Correct","timestamp":"1713210660.0"},{"timestamp":"1693814520.0","content":"Replicated (Because its a Dimension table)\nHash (Fact table with High volume of data)\nRound-Robin (Staging table)","poster":"kkk5566","comment_id":"998346","upvote_count":"3"},{"upvote_count":"2","comment_id":"933300","content":"Shouldn't the fact table has round-robin as well since we need to prioritize data loading, hash will definitely improve the read query performance but will impact the data load speed?","timestamp":"1687673220.0","poster":"spramanik_de","comments":[{"comment_id":"984247","upvote_count":"1","timestamp":"1692343740.0","poster":"wanchihh","content":"The requirements stated are:\n- Maximize the performance of data loading operations to Staging.WebSessions.\n- Minimize query times for reporting queries against the dimensional model.\n\nSo only the staging table needs fast data loading."}]},{"content":"Given answer is correct","upvote_count":"4","timestamp":"1675440540.0","poster":"SHENOOOO","comment_id":"797185"},{"content":"The dimension should be a replicated one. so that it will be available in all noes for a better performance \n\nfact table should be a HASH\nstaging table is always Round_ROBIN","timestamp":"1674342240.0","upvote_count":"1","poster":"DindaS","comment_id":"783806"},{"upvote_count":"1","content":"correct","comment_id":"762195","timestamp":"1672421880.0","poster":"nb1000"},{"upvote_count":"4","comment_id":"683500","poster":"allagowf","timestamp":"1664537640.0","content":"the giving answer is correct, as the requirements"}],"unix_timestamp":1662410700,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0012600002.jpg"],"topic":"1","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0012500001.png","https://www.examtopics.com/assets/media/exam-media/04259/0012600001.jpg"]}],"exam":{"id":67,"lastUpdated":"12 Apr 2025","provider":"Microsoft","numberOfQuestions":384,"isBeta":false,"isImplemented":true,"name":"DP-203","isMCOnly":false},"currentPage":15},"__N_SSP":true}