{"pageProps":{"questions":[{"id":"c3ws77M03H2d03De9Mbg","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/microsoft/view/53073-exam-dp-203-topic-2-question-88-discussion/","answer":"B","question_id":246,"unix_timestamp":1621397640,"answer_images":[],"choices":{"A":"Yes","B":"No"},"topic":"2","answer_ET":"B","discussion":[{"upvote_count":"46","timestamp":"1621397640.0","content":"High-concurrency clusters do not support Scala. So the answer is still 'No' but the reasoning is wrong. \nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure","comments":[{"upvote_count":"3","comment_id":"378010","comments":[{"content":"Because the High Concurrency cluster for each data scientist is not correct, it should be standard for a single user!","timestamp":"1623989100.0","upvote_count":"6","poster":"eng1","comment_id":"384563"}],"poster":"Preben","content":"I agree that High concurrency does not support Scala. But they specified using a Standard cluster for the jobs, which does support Scala. Why is the answer 'No'?","timestamp":"1623217560.0"},{"comment_id":"1155932","upvote_count":"1","timestamp":"1708556340.0","poster":"mav2000","content":"High concurrency supports scala now, so the answer should be A)"}],"comment_id":"360994","poster":"dfdsfdsfsd"},{"comment_id":"384009","poster":"FRAN__CO_HO","content":"Answer should be NO, which\nData scientist: STANDARD as need to run scala\nJobs: STANDARD as need to run scala\nData Engineers: High-concurrency clusters as better resource sharing","timestamp":"1623915600.0","upvote_count":"15"},{"upvote_count":"4","timestamp":"1718746020.0","comment_id":"1232611","content":"Down the line 3 years back No must have been correct answer but now its not.\nScala is supported at High Concurrency cluster only thing it doesn't benefit with the concurrency due the JVM thingy.\nSo ideally speaking...\nData Engineers : High Concurrency - due to the fact of sharing \nData Scientist : Standard - because its better to leave them in their space\nJob Cluster : Standard is best, but High Concurrency is also good.\nSo answer should be Yes, because here they don't talk about cost, performance etc.","poster":"Sr18"},{"upvote_count":"1","timestamp":"1707652680.0","comment_id":"1147199","poster":"Azure_2023","content":"Selected Answer: B\nThere are potential cost and resource utilization concerns with the separate High Concurrency cluster for jobs."},{"upvote_count":"2","content":"High concurrency doesn't support Scala","comment_id":"1051299","poster":"Tactable","timestamp":"1698022560.0"},{"upvote_count":"1","timestamp":"1693376880.0","poster":"kkk5566","content":"Selected Answer: B\nAnswer is No.","comment_id":"993759"},{"content":"Selected Answer: B\nCorrect answer: false (no)","comment_id":"931416","timestamp":"1687512600.0","upvote_count":"1","poster":"auwia"},{"content":"Selected Answer: B\nHigh-concurrency cluster does not support Scala.","comment_id":"731321","upvote_count":"1","poster":"Pais","timestamp":"1669802520.0"},{"upvote_count":"1","comment_id":"718096","poster":"OldSchool","content":"Selected Answer: B\nJobs require Scala so the answer is B) No.","timestamp":"1668443520.0"},{"poster":"greenlever","comment_id":"694390","upvote_count":"1","timestamp":"1665708840.0","content":"Selected Answer: B\nCluster for Jobs should support scala - STANDARD"},{"content":"We would need a Standard cluster for the jobs to support Scala. High-concurrency cluster does not support Scala.\nHence, Answer is NO","timestamp":"1662506220.0","comment_id":"661712","upvote_count":"1","poster":"anks84"},{"comment_id":"658592","upvote_count":"1","timestamp":"1662216900.0","poster":"Hema_V","content":"Selected Answer: B\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"},{"upvote_count":"1","poster":"Deeksha1234","content":"No is correct","comment_id":"647020","timestamp":"1660537620.0"},{"poster":"ClassMistress","upvote_count":"1","timestamp":"1652802000.0","comment_id":"602980","content":"Selected Answer: B\nHigh Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala."},{"timestamp":"1648778640.0","upvote_count":"2","poster":"narendra399","comment_id":"579226","content":"1 and 2 are same questions but answers are different why?"},{"upvote_count":"2","comment_id":"565017","poster":"Hanse","content":"As per Link: https://docs.azuredatabricks.net/clusters/configure.html\nStandard and Single Node clusters terminate automatically after 120 minutes by default. --> Data Scientists\nHigh Concurrency clusters do not terminate automatically by default.\nA Standard cluster is recommended for a single user. --> Standard for Data Scientists & High Concurrency for Data Engineers\nStandard clusters can run workloads developed in any language: Python, SQL, R, and Scala.\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. --> Jobs needs Standard","timestamp":"1646942520.0"},{"timestamp":"1641214260.0","comment_id":"515719","poster":"lukeonline","upvote_count":"2","content":"Selected Answer: B\nhigh concurrency does not support scala"},{"content":"Selected Answer: B\nwrong: no","comment_id":"496101","timestamp":"1638887820.0","upvote_count":"1","poster":"rashjan"},{"poster":"FredNo","content":"Selected Answer: B\nAnswer is no because high concurrency does not support scala","timestamp":"1637595120.0","upvote_count":"5","comment_id":"484328"},{"poster":"Aslam208","content":"Answer is No","comment_id":"471024","upvote_count":"2","timestamp":"1635739020.0"},{"upvote_count":"2","comment_id":"383681","poster":"damaldon","content":"Answer: NO\n-Data scientist should have their own cluster and should terminate after 120 mins - STANDARD\n-Cluster for Jobs should support scala - STANDARD\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure","timestamp":"1623873660.0"},{"upvote_count":"3","timestamp":"1623171480.0","poster":"nas28","content":"Answer correct : No. but the reason is wrong, They want data scientists cluster to shut down automatically after 120 minutes so Standard cluster not high concurrency","comment_id":"377696"},{"content":"Answer is correct - NO","upvote_count":"2","comment_id":"374447","poster":"Sunnyb","timestamp":"1622819700.0"}],"exam_id":67,"answer_description":"","isMC":true,"timestamp":"2021-05-19 06:14:00","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\n✑ A workload for data engineers who will use Python and SQL.\n✑ A workload for jobs that will run notebooks that use Python, Scala, and SQL.\n✑ A workload that data scientists will use to perform ad hoc analysis in Scala and R.\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\n✑ The data engineers must share a cluster.\n✑ The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\n✑ All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\nYou need to create the Databricks clusters for the workloads.\nSolution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a High Concurrency cluster for the jobs.\nDoes this meet the goal?","question_images":[]},{"id":"0xGFa1XO5yNJGUrp7nO1","answer_ET":"A","answer_images":[],"unix_timestamp":1662506340,"timestamp":"2022-09-07 01:19:00","answers_community":["A (79%)","B (21%)"],"topic":"2","answer":"A","isMC":true,"question_text":"You are designing a folder structure for the files in an Azure Data Lake Storage Gen2 account. The account has one container that contains three years of data.\nYou need to recommend a folder structure that meets the following requirements:\n✑ Supports partition elimination for queries by Azure Synapse Analytics serverless SQL pools\n✑ Supports fast data retrieval for data from the current month\n✑ Simplifies data security management by department\nWhich folder structure should you recommend?","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/80747-exam-dp-203-topic-2-question-89-discussion/","exam_id":67,"answer_description":"","question_id":247,"discussion":[{"content":"Selected Answer: A\nAnswer is Correct !","poster":"anks84","timestamp":"1678151940.0","comment_id":"661714","upvote_count":"9"},{"poster":"dmitriypo","comment_id":"710792","upvote_count":"5","content":"Selected Answer: A\nOf course A","timestamp":"1683140880.0"},{"timestamp":"1729843680.0","comment_id":"1201842","content":"Partition elimination > date is the main here so it is D","upvote_count":"1","poster":"Dusica"},{"timestamp":"1709959260.0","comment_id":"1002850","upvote_count":"1","poster":"kkk5566","content":"Selected Answer: A\nA is correct"},{"upvote_count":"4","comment_id":"931447","comments":[{"content":"Except is contradicts the requirement \"Simplifies data security management by department\". You need minimum (#DataSource) * (#Department) ACLs with option B\nWhereas with option A, you need just minimum #Department ACLs.\nAlso, Option A allows you to have additional datasource specific ACLs within a particular department, if that is so desired.","poster":"MarkJoh","timestamp":"1717702380.0","comment_id":"1089790","upvote_count":"1"}],"timestamp":"1703333220.0","content":"Selected Answer: B\nThe raw zone may be organised by source system, then entity. Here is an example folder structure, optimal for folder security:\n\\Raw\\DataSource\\Entity\\YYYY\\MM\\DD\\File.extension\nTypically each source system will be granted write permissions at the DataSource folder level with default ACLs (see section on ACLs below) specified. This will ensure permissions are inherited as new daily folders and files are created. \nWhilst many use time based partitioning there are a number of options which may provide more efficient access paths. ->this justify the YYYYMM to get easily the current month. Correct answer for me is B.","poster":"auwia"},{"timestamp":"1682673780.0","comment_id":"706311","poster":"rzeng","content":"A is right","upvote_count":"4"}],"choices":{"B":"\\DataSource\\Department\\YYYYMM\\DataFile_YYYYMMDD.parquet","C":"\\DD\\MM\\YYYY\\Department\\DataSource\\DataFile_DDMMYY.parquet","A":"\\Department\\DataSource\\YYYY\\MM\\DataFile_YYYYMMDD.parquet","D":"\\YYYY\\MM\\DD\\Department\\DataSource\\DataFile_YYYYMMDD.parquet"}},{"id":"MSuwRLuoZk17XZUqM2Xz","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/61795-exam-dp-203-topic-2-question-9-discussion/","answer":"","unix_timestamp":1631273940,"question_id":248,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0016600001.png"],"topic":"2","discussion":[{"comments":[{"comment_id":"449815","content":"answer is correct as per this link","poster":"Lrng15","timestamp":"1632359820.0","upvote_count":"2"}],"content":"Answer is correct\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files","timestamp":"1631347920.0","upvote_count":"47","poster":"Maunik","comment_id":"442883"},{"poster":"gf2tw","comment_id":"442451","content":"The question and answer seem out of place, there was no mention of the CSV and the query in the answer doesn't match up with openjson at all","upvote_count":"11","comments":[{"upvote_count":"1","poster":"gssd4scoder","timestamp":"1635914280.0","content":"agree with you, very misleading","comment_id":"471913"},{"poster":"dev2dev","content":"Look at the WITH statement, the csv column can contain json data.","comment_id":"525522","upvote_count":"1","timestamp":"1642404060.0"},{"content":"The easiest way to see to the content of your JSON file is to provide the file URL to the OPENROWSET function, specify csv FORMAT, and set values 0x0b for fieldterminator and fieldquote.","poster":"vctrhugo","comment_id":"932978","timestamp":"1687647780.0","upvote_count":"3"},{"comments":[{"poster":"gf2tw","timestamp":"1638974880.0","content":"Thanks, you're right:\n\"The easiest way to see to the content of your JSON file is to provide the file URL to the OPENROWSET function, specify csv FORMAT, and set values 0x0b for fieldterminator and fieldquote.\"","upvote_count":"5","comment_id":"496902"}],"content":"Actually, the csv format is specified if you're using OPENROWSET to read json files in Synapse. The OPENJSON is required if you want to parse data from every array in the document. See the OPENJSON example in this link:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#query-json-files-using-openjson","comment_id":"483205","poster":"dead_SQL_pool","upvote_count":"12","timestamp":"1637494320.0"}],"timestamp":"1631273940.0"},{"poster":"evangelist","timestamp":"1720955640.0","content":"To complete the Transact-SQL statement for reading JSON-formatted files using the serverless SQL pool in WS1, you should use OPENROWSET to access the data and OPENJSON to parse the JSON content. Here is the correct completion of the statement:","upvote_count":"1","comment_id":"1247744"},{"comment_id":"1220536","comments":[{"comment_id":"1293619","upvote_count":"1","content":"Me too, dammit!","poster":"drosen","timestamp":"1728165720.0"}],"timestamp":"1716930900.0","poster":"KarlGardnerDataEngineering","upvote_count":"1","content":"This took me about 10 hours to understand this query"},{"upvote_count":"2","timestamp":"1705415040.0","content":"Answer is correct:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#query-json-files-using-openjson","comment_id":"1124277","poster":"ELJORDAN23"},{"poster":"kkk5566","comment_id":"999317","timestamp":"1693907040.0","upvote_count":"1","content":"Answer is correct"},{"poster":"mamahani","comment_id":"895027","upvote_count":"1","content":"openrowset / openjson","timestamp":"1683806460.0"},{"content":"does openjson do the same thing as jsoncontent ? \nI tried running a query on a json file and the auto filled code used jsoncontent instead of openjson","upvote_count":"1","timestamp":"1666375680.0","comment_id":"701096","poster":"zorko10"},{"upvote_count":"1","timestamp":"1659175740.0","poster":"Deeksha1234","content":"correct","comment_id":"639635"},{"poster":"SebK","upvote_count":"1","comment_id":"573201","timestamp":"1647978720.0","content":"Correct"},{"content":"correct","upvote_count":"1","timestamp":"1643456220.0","comment_id":"535426","poster":"PallaviPatel"}],"answer_ET":"","answer_description":"Box 1: openrowset -\nThe easiest way to see to the content of your CSV file is to provide file URL to OPENROWSET function, specify csv FORMAT.\nExample:\nSELECT *\nFROM OPENROWSET(\nBULK 'csv/population/population.csv',\nDATA_SOURCE = 'SqlOnDemandDemo',\nFORMAT = 'CSV', PARSER_VERSION = '2.0',\nFIELDTERMINATOR =',',\nROWTERMINATOR = '\\n'\n\nBox 2: openjson -\nYou can access your JSON files from the Azure File Storage share by using the mapped drive, as shown in the following example:\n\nSELECT book.* FROM -\nOPENROWSET(BULK N't:\\books\\books.json', SINGLE_CLOB) AS json\nCROSS APPLY OPENJSON(BulkColumn)\nWITH( id nvarchar(100), name nvarchar(100), price float,\npages_i int, author nvarchar(100)) AS book\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-single-csv-file https://docs.microsoft.com/en-us/sql/relational-databases/json/import-json-documents-into-sql-server","exam_id":67,"isMC":false,"timestamp":"2021-09-10 13:39:00","question_text":"DRAG DROP -\nYou have an Azure Synapse Analytics workspace named WS1.\nYou have an Azure Data Lake Storage Gen2 container that contains JSON-formatted files in the following format.\n//IMG//\n\nYou need to use the serverless SQL pool in WS1 to read the files.\nHow should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0016400001.png","https://www.examtopics.com/assets/media/exam-media/04259/0016500001.png"]},{"id":"SArhalhuT9bkG5mBIFjP","unix_timestamp":1662506460,"answer_ET":"BD","timestamp":"2022-09-07 01:21:00","topic":"2","answer":"BD","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/80750-exam-dp-203-topic-2-question-90-discussion/","question_text":"You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 receives new data once every 24 hours.\nYou have the following function.\n//IMG//\n\nYou have the following query.\n//IMG//\n\nThe query is executed once every 15 minutes and the @parameter value is set to the current date.\nYou need to minimize the time it takes for the query to return results.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","exam_id":67,"discussion":[{"upvote_count":"15","poster":"esaade","timestamp":"1678437120.0","comment_id":"834762","content":"Selected Answer: BD\nB. Convert the avg_c column into a calculated column.\nD. Enable result set caching.\n\nExplanation:\nA calculated column is a column that uses an expression to calculate its value based on other columns in the same table. In this case, the udfFtoC function can be used to calculate the avg_c value based on the avg_temperature column, eliminating the need to call the UDF in the SELECT statement.\n\nEnabling result set caching can improve query performance by caching the result set of the query, so subsequent queries that use the same parameters can be retrieved from the cache instead of executing the query again.\n\nCreating an index on the avg_f column or the sensorid column is not useful because there are no join or filter conditions on these columns in the WHERE clause. Changing the table distribution to replicate is also not necessary because it does not affect the query performance in this scenario"},{"timestamp":"1722116400.0","poster":"f2a9aa5","upvote_count":"2","content":"No, Azure Synapse Analytics dedicated SQL pool does not support calculated (or computed) columns1. This means you cannot define columns in your table that automatically compute their values based on other columns in the same table.\n\nIf you need to include calculated values, you can handle the calculations in your ETL (Extract, Transform, Load) process before loading the data into the dedicated SQL pool. Alternatively, you can perform the calculations in your queries when retrieving data from the table.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview#unsupported-table-features","comment_id":"1256500"},{"poster":"iceberge","content":"Copilot: (DC)\nConverting the avg_c column into a calculated column (Option B) might not be as effective for improving query performance in this scenario. Calculated columns can be useful for simplifying queries and ensuring consistent calculations, but they don’t necessarily improve performance, especially if the calculation is complex or if the column is frequently queried.\nIn contrast, enabling result set caching (Option D) and creating an index on the sensorid column (Option C) directly target performance improvements by reducing query execution time and speeding up data retrieval","upvote_count":"1","comment_id":"1254291","timestamp":"1721813820.0"},{"poster":"AccountHatz","comments":[{"content":"notice that the sink table in dedicated SQL pool gets updated every 24 hours, but this query(due to some reasons) runs every 15 minutes; as a result, for 24 * 4 - 1 times, there is not new result to show. so, if the data is already in cache, db engine can access them faster.","comment_id":"1264235","upvote_count":"1","timestamp":"1723394880.0","poster":"SajadAhm"}],"upvote_count":"2","timestamp":"1708493160.0","content":"Selected Answer: BE\nI don't think D is a solution , \"What's not cached .... Queries using user defined functions\" from https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching","comment_id":"1155264"},{"timestamp":"1704098340.0","poster":"dakku987","upvote_count":"2","comment_id":"1111045","content":"Selected Answer: AC\nchatgpt \nTo minimize the time it takes for the query to return results, you should consider the following actions:\n\nA. Create an index on the avg_f column:\n\nCreating an index on the avg_f column can improve the query performance, especially if there are frequent searches or filtering based on this column.\nC. Create an index on the sensorid column:\n\nIf the sensorid column is frequently used in filtering or joins, creating an index on this column can improve the query performance."},{"upvote_count":"1","poster":"OldSchool","content":"Selected Answer: DE\nFirst the wording of the question is ridiculous. \"Query is executed once every 15 minutes\".\nSo what is it, \"Once\" or \"every 15 minutes\"?\nEither way, they are asking what to do to speed up the query.\nD Setting result caching\nE Replicated distribution","timestamp":"1695803460.0","comment_id":"1018610"},{"content":"Selected Answer: DE\ncorrect","comment_id":"993785","poster":"kkk5566","upvote_count":"1","timestamp":"1693378560.0"},{"comment_id":"993767","timestamp":"1693377540.0","poster":"kkk5566","content":"Selected Answer: BD\ncorrect","upvote_count":"1","comments":[{"timestamp":"1693378680.0","upvote_count":"1","content":"D & E should be correct","poster":"kkk5566","comment_id":"993786"}]},{"poster":"Matt2000","upvote_count":"4","timestamp":"1691580360.0","content":"Calculated columns exist in Power BI, not dedicated SQL pools. Computed columns are not supported in dedicated SQL pools.\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview","comment_id":"976562"},{"upvote_count":"2","poster":"dumbled","content":"Selected Answer: BD\ncorrect","timestamp":"1682353800.0","comment_id":"879528"},{"comments":[{"content":"Creating an index on the avg_f column will improve the performance of the query, as it will allow the query to find the relevant data more quickly. Converting the avg_c column into a calculated column will allow the query to return the temperature in Celsius without the need to perform the calculation at runtime, which will also improve the performance of the query.","upvote_count":"1","timestamp":"1674059400.0","comment_id":"780218","poster":"Lestrang","comments":[{"content":"After re-considering, I am unsure whether the indexing would help. That would only leave Replication as the viable option even though it is not viable design but the request is to minimize query time and that is what it will do, so I guess final answer is\nBE","poster":"Lestrang","upvote_count":"1","timestamp":"1674722280.0","comment_id":"788535"}]},{"content":"\"2 GB is not a hard limit. If the data is static and does not change, you can replicate larger tables.\" https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/design-guidance-for-replicated-tables","comment_id":"1155263","upvote_count":"1","timestamp":"1708493040.0","poster":"AccountHatz"}],"timestamp":"1674058140.0","poster":"Lestrang","comment_id":"780190","content":"Selected Answer: AB\nWith that point by erhard being made (caching does work with queries using UDF), the most commonly voted D is wrong, so B and what now?\nReplicated cannot be right because it received date everyday and has aggregations so not a dim table and we have no clue about its size. \n by elimination that leaves us A and C\n\nIndexing is less useful with no joins but it does improve some performance being on where clause target. so I'd go with A and B.","upvote_count":"1"},{"poster":"Karforcerts","upvote_count":"4","content":"Selected Answer: BD\nneed to first chage UDF to a calculated column and then enable result set caching. agreed with the answer","timestamp":"1669045560.0","comment_id":"723713"},{"content":"Queries using user defined functions are not cached.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching","upvote_count":"4","timestamp":"1668601320.0","poster":"erhard","comment_id":"719603"},{"comment_id":"716175","timestamp":"1668182220.0","content":"Selected Answer: DE\nA,C not right since index don't help if join are not involved.\nD for sure help query performance.\n\nI don't get why B:\n\"A computed column is a virtual column whose value is calculated from other values in the table. By default, the expression’s outputted value is not physically stored. Instead, SQL Server runs the expression when the column is queried and returns the value as part of the result set ... In many cases, non-persistent computed columns put too much burden on the processor, resulting in SLOWER QUERIES and unresponsive applications\"\n\nSince the only requirements is faster execution times for queries, i don't think calculated columns will improve performance.\nSi second option for me would be D (replicate). Although it will cause more effort writing, because updates should be written to every partition, optimized writes aren't a requirement in the question.","upvote_count":"3","poster":"kl8585"},{"upvote_count":"1","comment_id":"706319","timestamp":"1666949580.0","poster":"rzeng","content":"pool ingest data once per 24 hrs, while query happens every 15mins, caching result can definitely avoid the some duplicate calculation, I'll go with BD."},{"timestamp":"1665783900.0","content":"Selected Answer: DE\nI think should be DE. \nsince \"the query is executed once every 15 minutes and the @parameter value is set to the current date\", and the it receives new data once every 24 hours, it means the query result isn't change in one day even you run it every 15 mins. The data is static within a day. Replication could help the performance.","poster":"Xinyuehong","upvote_count":"2","comment_id":"695024"},{"content":"Selected Answer: BD\nAnswer is Correct !","upvote_count":"4","comment_id":"661717","timestamp":"1662506460.0","poster":"anks84"}],"answer_description":"","choices":{"D":"Enable result set caching.","C":"Create an index on the sensorid column.","E":"Change the table distribution to replicate.","A":"Create an index on the avg_f column.","B":"Convert the avg_c column into a calculated column."},"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0027800001.jpg","https://www.examtopics.com/assets/media/exam-media/04259/0027800002.jpg"],"isMC":true,"answers_community":["BD (68%)","DE (18%)","5%"],"question_id":249},{"id":"18KXsnFbBBvCRKJFILzN","question_images":[],"answer_ET":"B","answers_community":["B (61%)","A (39%)"],"answer_images":[],"unix_timestamp":1673778840,"exam_id":67,"discussion":[{"upvote_count":"16","timestamp":"1678967160.0","content":"Selected Answer: B\nB. Structured Streaming in Azure Databricks is the best option for this scenario as it allows for processing of streaming data and outputting it to Azure Data Lake Storage, while also providing the ability for analysts to interactively query the data using Databricks notebooks.\n\nAzure Stream Analytics and Azure Synapse notebooks (option A) can also process streaming data and output to Data Lake Storage, but they may not provide the same level of interactivity for analysts.\n\nEvent triggers in Azure Data Factory (option C) can help automate data movement between Event Hubs and Data Lake Storage, but they do not provide the necessary functionality for processing and querying streaming data.\n\nAzure Queue Storage and read-access geo-redundant storage (RA-GRS) (option D) are not relevant for this scenario as they do not provide capabilities for processing and querying streaming data.","poster":"esaade","comment_id":"840906"},{"content":"Selected Answer: A\nA. Azure Stream Analytics and Azure Synapse notebooks:\nAzure Stream Analytics can be used to process the streaming data from Azure Event Hub and output the data to Azure Data Lake Storage. Azure Synapse notebooks provide interactive querying capabilities, and they can be integrated with the Azure Data Lake Storage to enable analysts to run their analytics on the stored data.\n\nB. Structured Streaming in Azure Databricks:\nStructured Streaming in Azure Databricks indeed supports streaming data and can write outputs to Azure Data Lake Storage. However, the question emphasizes \"interactively querying\" the streaming data, and while Databricks notebooks allow for interactive queries, Azure Synapse notebooks are better integrated with Microsoft's suite of data tools for broader analytics purposes.","comment_id":"1045615","poster":"Andrew_Chen","upvote_count":"6","comments":[{"comment_id":"1368532","content":"This is an Azure test si preferred the Azure services","poster":"nadavw","upvote_count":"1","timestamp":"1741504680.0"},{"poster":"ExamDestroyer69","comment_id":"1101039","content":"@Andrew_Chen Azure Databricks with Structured Streaming is preferred over Azure Stream Analytics and Azure Synapse Notebooks for real-time streaming data processing from Azure Event Hubs due to its native support for continuous processing, live querying, and seamless integration with Azure Data Lake Storage.","timestamp":"1703024520.0","upvote_count":"2"}],"timestamp":"1697525340.0"},{"content":"B. Structured Streaming in Azure Databricks\n\nHere’s why:\n\nStructured Streaming in Azure Databricks:\n\nReal-time Processing: \nInteractive Querying: Databricks notebooks allow analysts to interactively query and visualize the streaming data, making it easy to gain insights in real-time.\n\nIntegration: It integrates seamlessly with Azure Event Hubs for data ingestion and Azure Data Lake Storage for data output.\n\nOther Options:\nA. Azure Stream Analytics and Azure Synapse notebooks: While Azure Stream Analytics is excellent for real-time processing and Azure Synapse notebooks for interactive querying, combining them might not be as seamless and efficient as using Databricks for both tasks.","poster":"f2a9aa5","upvote_count":"1","comment_id":"1256501","timestamp":"1722116820.0"},{"content":"Selected Answer: B\n\"Interactive querying\" makes me think it is B","timestamp":"1708493760.0","upvote_count":"1","comment_id":"1155267","poster":"AccountHatz"},{"upvote_count":"1","timestamp":"1703451960.0","comment_id":"1104840","content":"selected answer : B \nto visualize data in stream analytics you use SQL query in the Azure portal inside synapse , not a notebook , therefore the answer is B","poster":"Sirstyle"},{"comment_id":"993793","upvote_count":"1","content":"Selected Answer: A\nshould be correct","poster":"kkk5566","timestamp":"1693379820.0"},{"upvote_count":"1","timestamp":"1687517460.0","content":"Selected Answer: A\nWhat streaming sources and sinks does Azure Databricks support?\nDatabricks recommends using Auto Loader to ingest supported file types from cloud object storage into Delta Lake. For ETL pipelines, Databricks recommends using Delta Live Tables (which uses Delta tables and Structured Streaming). You can also configure incremental ETL workloads by streaming to and from Delta Lake tables.\n\nIn addition to Delta Lake and Auto Loader, Structured Streaming can connect to messaging services such as Apache Kafka.\nhttps://learn.microsoft.com/en-us/azure/databricks/structured-streaming/\nI don't see data lake in the list, so probably the answer is A.","poster":"auwia","comment_id":"931489"},{"upvote_count":"3","comment_id":"859821","timestamp":"1680518160.0","content":"Selected Answer: B\nI am in favour of B because of this piece of information I have encountered:\nhttps://www.databricks.com/spark/getting-started-with-apache-spark/streaming","poster":"vadiminski_a","comments":[{"upvote_count":"1","content":"On the other hand, there is this: https://learn.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics\nSo I believe both to be valid, Azure Stream Analytics seems to be more straightforward","poster":"vadiminski_a","comment_id":"859824","timestamp":"1680518400.0"}]},{"upvote_count":"1","poster":"Kate0204","content":"Selected Answer: A\nAn Azure Stream Analytics job consists of an input, query, and an output.","comment_id":"832677","timestamp":"1678263480.0"},{"timestamp":"1674781320.0","content":"\"The solution must ensure that analysts can interactively query the streaming data\"\nStreaming analysis can't query streaming data interactively","poster":"Karl_Cen","comment_id":"789215","upvote_count":"2"},{"timestamp":"1674059700.0","upvote_count":"2","poster":"Lestrang","comments":[{"timestamp":"1674749160.0","comments":[{"poster":"Mal2002","content":"It's implied. Solutions said Azure Stream Analytics and Azure Synapse Notebook, Azure Synapse notebook cannot be created without Azure Synapse Workspace.","upvote_count":"2","timestamp":"1686726300.0","comment_id":"922827"}],"comment_id":"788892","upvote_count":"1","poster":"Lestrang","content":"Although this might be true, after some pondering, the given solution A. Azure Stream Analytics and Azure Synapse notebooks requires a Synpase workspace which is not implied. \nSo I guess it would be databricks."}],"content":"Selected Answer: A\nB. Structured Streaming in Azure Databricks is incorrect because while it allows you to process streaming data using Spark's structured streaming API, it is not designed to directly output the data to Azure Data Lake Storage. Instead, it typically outputs the data to storage systems like HDFS, S3, or Cosmos DB. Additionally, Databricks is a separate service that does not integrate with Azure Synapse for interactive querying. While it's possible to use Databricks to read the data from Data Lake Storage and use Spark to process the data and then write it back to Data Lake Storage, it will not be as efficient as using Azure Stream Analytics for this use case as it is specifically designed for streaming data processing and also has built-in connectors to various data storage and analytics services like Data Lake Storage","comment_id":"780221"},{"comment_id":"776443","poster":"alexnicolita","content":"Selected Answer: A\nWhy not Azure Stream Analytics and Azure Synapse Analytics?","upvote_count":"2","timestamp":"1673778840.0"}],"timestamp":"2023-01-15 11:34:00","isMC":true,"question_text":"You need to design a solution that will process streaming data from an Azure Event Hub and output the data to Azure Data Lake Storage. The solution must ensure that analysts can interactively query the streaming data.\n\nWhat should you use?","question_id":250,"url":"https://www.examtopics.com/discussions/microsoft/view/95404-exam-dp-203-topic-2-question-91-discussion/","answer_description":"","topic":"2","choices":{"C":"event triggers in Azure Data Factory","A":"Azure Stream Analytics and Azure Synapse notebooks","D":"Azure Queue storage and read-access geo-redundant storage (RA-GRS)","B":"Structured Streaming in Azure Databricks"},"answer":"B"}],"exam":{"isMCOnly":false,"provider":"Microsoft","isImplemented":true,"numberOfQuestions":384,"id":67,"lastUpdated":"12 Apr 2025","isBeta":false,"name":"DP-203"},"currentPage":50},"__N_SSP":true}