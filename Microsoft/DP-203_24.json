{"pageProps":{"questions":[{"id":"oRIWYgZZKprRilWp12EJ","url":"https://www.examtopics.com/discussions/microsoft/view/125112-exam-dp-203-topic-1-question-98-discussion/","answer":"A","topic":"1","discussion":[{"poster":"Blablatest123","comment_id":"1059861","content":"Partitioned by month and with 60 nodes, means it’s 1M per combination","timestamp":"1698852600.0","upvote_count":"14"},{"comment_id":"1139748","timestamp":"1707017580.0","poster":"moneytime","upvote_count":"5","content":"Answer is A.\nEach partition has one million rows( minimum).\n*A partition with 1 distribution will have 1M rows \n* Partition with 2 distribution will have 2M rows\n----'\nPartition with 60 distributions will have 60M rows\n....Partition with n-distribution will have n ×1M rows = n M rows \nSo in general, if there is p-partitions and d--distributions .then the combination of P and d describes the total rows or records (Q) which can be mathematical represented below \nQ = 1000000 * p *d \nFor each combination of partition and distribution, \nP = 1 and d = 1 \nTherefore ,\nThe total rows,Q = 1000000*1*1= 1M.\nN.B .1M rows is the minimum per partion and distribution required for a table compression and optimization in table partitioning"},{"content":"Selected Answer: A\nanswer is A, as there will be 60 nodes for every distrubution","upvote_count":"1","poster":"ff5037f","timestamp":"1729872300.0","comment_id":"1302923"},{"timestamp":"1713100440.0","poster":"Alongi","upvote_count":"2","content":"Selected Answer: A\nShould be A","comment_id":"1195523"},{"upvote_count":"2","timestamp":"1706868600.0","comment_id":"1138411","poster":"saqib839","content":"Azure Synapse Analytics dedicated SQL pools distribute data across multiple distributions - by default, there are 60 distributions. Since the sales transactions table will be partitioned by month and will contain approximately 60 million rows per month, and considering round-robin distribution is used (which distributes rows evenly across all distributions), we can estimate the number of rows per combination of distribution and partition by dividing the total number of rows by the number of distributions.\n\nHere's the calculation:\n\n60 million rows per month / 60 distributions = 1 million rows per distribution per month.\n\nSo, the correct answer is:\n\nA. 1 million\n\nEach combination of distribution and partition (monthly in this case) would have approximately 1 million rows."},{"poster":"d046bc0","content":"Selected Answer: A\n60 nodes so 1M per distribiution and partition","upvote_count":"1","timestamp":"1702411620.0","comment_id":"1094923"},{"poster":"BitacTeam","upvote_count":"1","content":"Selected Answer: A\n1 Mio per combination","comment_id":"1085143","timestamp":"1701426960.0"},{"timestamp":"1701165120.0","upvote_count":"1","comment_id":"1082372","content":"Selected Answer: A\nsame as Blablatest123 said","poster":"SimonQBDS"},{"poster":"mishoka23","comment_id":"1059837","upvote_count":"2","timestamp":"1698850800.0","content":"Selected Answer: D\nDuplicate Question"}],"timestamp":"2023-11-01 16:00:00","answer_images":[],"choices":{"C":"20 million","B":"5 million","A":"1 million","D":"60 million"},"question_text":"You are designing a sales transactions table in an Azure Synapse Analytics dedicated SQL pool. The table will contain approximately 60 million rows per month and will be partitioned by month. The table will use a clustered column store index and round-robin distribution.\n\nApproximately how many rows will there be for each combination of distribution and partition?","isMC":true,"answer_description":"","question_id":116,"answer_ET":"A","exam_id":67,"question_images":[],"unix_timestamp":1698850800,"answers_community":["A (75%)","D (25%)"]},{"id":"VlesBZQ3N2K8dysGBas9","question_text":"You have an Azure Synapse Analytics workspace.\n\nYou plan to deploy a lake database by using a database template in Azure Synapse.\n\nWhich two elements are included in the template? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.","isMC":true,"answer_ET":"AE","answers_community":["AE (100%)"],"discussion":[{"poster":"metiii","timestamp":"1714988700.0","comment_id":"1063800","comments":[{"comment_id":"1140551","content":"Please can you explain this part of the template requirements.\nPrerequisites\nAt least Synapse User role permissions are required for exploring a lake database template from Gallery.\nSynapse Administrator, or Synapse Contributor permissions are required on the Synapse workspace for creating a lake database.\nStorage Blob Data Contributor permissions are required on data lake when using the create table From data lake option.\n\n\n.I copied directly from the link you dropped .","upvote_count":"1","timestamp":"1722800580.0","poster":"moneytime"}],"upvote_count":"9","content":"Selected Answer: AE\nCorrect, AE. Only table definition and their relationship is included in the template. The rest of the options should be configured\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/database-designer/create-lake-database-from-lake-database-templates"},{"timestamp":"1730460480.0","upvote_count":"2","poster":"Dusica","comment_id":"1204974","content":"B and E 100%\nRelationships you have to define"},{"comment_id":"1201472","upvote_count":"2","poster":"Dusica","content":"B E ; relationships have to be added in the designer after tables are added","timestamp":"1729783320.0"},{"upvote_count":"1","timestamp":"1722649140.0","comment_id":"1138983","poster":"Charley92","content":"Selected Answer: AE\nAccording to the Microsoft Learn documentation1, a lake database template in Azure Synapse includes the following elements:\n\nTable definitions: These are the schemas for the tables that store the data in the lake database. They include the table name, column names, data types, and descriptions.\nRelationships: These are the associations between the tables that define how they are related to each other. They include the primary and foreign keys, cardinality, and referential integrity rules.\nData formats: These are the specifications for how the data is stored and accessed in the lake database. They include the file format, compression type, delimiter, encoding, and schema inference options.\nTherefore, the correct answers are A and E."},{"timestamp":"1721130660.0","content":"Selected Answer: AE\nCorrect","poster":"ELJORDAN23","comment_id":"1124259","upvote_count":"1"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/125515-exam-dp-203-topic-1-question-99-discussion/","choices":{"C":"linked services","B":"data formats","D":"table permissions","E":"table definitions","A":"relationships"},"question_images":[],"topic":"1","question_id":117,"answer_description":"","timestamp":"2023-11-06 12:45:00","answer":"AE","exam_id":67,"unix_timestamp":1699271100},{"id":"NuiKHSA9JF1A3CUNVs7j","answer":"D","url":"https://www.examtopics.com/discussions/microsoft/view/60716-exam-dp-203-topic-10-question-1-discussion/","discussion":[{"upvote_count":"13","content":"There is a request 'Minimize number of Azure services'. With https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview Event capture, data can be stored in DL without using Stream Analytics. In this case just Reional redundancy for DL would be needed.","comment_id":"432163","comments":[{"upvote_count":"12","comment_id":"1010997","poster":"subhub","timestamp":"1726713840.0","content":"I passed today.. 820.. Thoughts.. 80% of exam questions were in Examtopics. Other 20% were not difficult. I got the Contoso case study... Good Luck."},{"poster":"GDJ2022","timestamp":"1675032840.0","upvote_count":"3","comment_id":"535776","content":"The question is asking \"improve high availability of the real-time data processing solution\" and not high availability of data. Hence the correct answer is D"},{"timestamp":"1663400700.0","poster":"ian_viana","upvote_count":"1","comment_id":"446438","content":"Agree, they also want a stage on data lake 2.\n\"Stage Inventory data in Azure Data Lake Storage Gen2\"\nwe don't need Stream Analytics to do that. Event Hub enables you to automatically capture the streaming data in Event Hubs in an Azure Blob storage or Azure Data Lake Storage Gen 1 or Gen 2 account of your choice, with the added flexibility of specifying a time or size interval.","comments":[{"content":"Please desconsidere my answer!\nEvent Hub can capture data to Data Lake and Blob. But I think the key word in the question is: eal-time data PROCESSING solution azure. Event hub is just for capture. Stream Analytics do the processing so I'm going with answer D","timestamp":"1663401060.0","poster":"ian_viana","comment_id":"446441","comments":[{"upvote_count":"8","poster":"Marcus1612","timestamp":"1663450980.0","comment_id":"446810","content":"I agree, Regional redundancy will be great for data but the processing would be lost. We need a solution for High Availability for PROCESSING and DATA."}],"upvote_count":"9"}]},{"content":"NB : it's an asynchronous copy.","poster":"sachabess79","timestamp":"1664013480.0","comment_id":"450809","upvote_count":"1"}],"poster":"petulda","timestamp":"1661509020.0"},{"comment_id":"995774","timestamp":"1725173340.0","poster":"kkk5566","content":"Selected Answer: D\nshould be D","upvote_count":"1"},{"poster":"vctrhugo","upvote_count":"3","timestamp":"1719094980.0","comment_id":"931091","content":"Selected Answer: D\nBy deploying identical Azure Stream Analytics jobs to paired regions in Azure, you ensure redundancy and fault tolerance for the real-time data processing solution. Paired regions in Azure are geographically separated and designed to provide resilience and data protection in the event of a regional outage or failure. If one region becomes unavailable, the other paired region can seamlessly take over the processing workload, ensuring continuous availability of the real-time data processing solution."},{"upvote_count":"4","comment_id":"646351","content":"Selected Answer: D\nanswer D is correct","poster":"Deeksha1234","timestamp":"1691934660.0"},{"content":"Selected Answer: D\nAnswer is correct","comment_id":"623709","poster":"StudentFromAus","upvote_count":"3","timestamp":"1687926300.0"},{"content":"Selected Answer: D\nI go with D and info provided by Canary_2021 is correct.","poster":"PallaviPatel","timestamp":"1674631260.0","comment_id":"531932","upvote_count":"2"},{"upvote_count":"2","comments":[{"timestamp":"1687665120.0","content":"You are right, but HC doesn't improve one shot processing, this would work better with multiple users","upvote_count":"1","poster":"Davico93","comment_id":"621944"}],"poster":"HaBroNounen","timestamp":"1673194500.0","content":"guys, the correct answer is A. It says to limit the amount of different services to use. Databricks is being used as a analytical tool for the datascientist already, so it can also be used for processing jobs.","comment_id":"519643"},{"content":"I think the answer is correct!","comment_id":"512946","upvote_count":"2","timestamp":"1672366560.0","poster":"edba"},{"content":"The answer should be D if the real time data load solution to move data from Azure Data Lake Storage Gen2 to Data Lake Gen2 to Azure SQL DB or Synapse Analytics as analytical data store. \nIf this way, Power BI and Azure Databricks notebooks will run query against Azure SQL DB or Synapse Analytics.\n\n• Daily inventory data comes from a Microsoft SQL server located on a private network.\n• Stage Inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store.\n• See inventory levels across the stores. Data must be updated as close to real time as possible.\n• Litware employs business analysts who prefer to analyze data by using Microsoft Power BI, and data scientists who prefer analyzing data in Azure Databricks notebooks.","upvote_count":"4","timestamp":"1672361520.0","comment_id":"512890","poster":"Canary_2021"},{"comments":[{"timestamp":"1671578700.0","poster":"jx1982","upvote_count":"3","comment_id":"505735","content":"sorry, typo, right answer is D"}],"upvote_count":"4","poster":"jx1982","timestamp":"1671465900.0","content":"I think the answer C is correct, high availability of \"the real-time data processing\", not high availability of \"the data storage\"","comment_id":"504977"},{"content":"What is the correct answer?","upvote_count":"2","poster":"FredNo","timestamp":"1669403100.0","comment_id":"486882"}],"topic":"10","timestamp":"2021-08-26 12:17:00","answer_images":[],"choices":{"C":"Set Data Lake Storage to use geo-redundant storage (GRS).","D":"Deploy identical Azure Stream Analytics jobs to paired regions in Azure.","A":"Deploy a High Concurrency Databricks cluster.","B":"Deploy an Azure Stream Analytics job and use an Azure Automation runbook to check the status of the job and to start the job if it stops."},"question_text":"What should you do to improve high availability of the real-time data processing solution?","isMC":true,"answer_description":"","question_id":118,"exam_id":67,"answer_ET":"D","question_images":[],"unix_timestamp":1629973020,"answers_community":["D (100%)"]},{"id":"jU9T46ckq6fyNkE1c5Bt","unix_timestamp":1631630400,"answers_community":[],"timestamp":"2021-09-14 16:40:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0014700001.jpg"],"isMC":false,"question_text":"HOTSPOT -\nYou plan to create a real-time monitoring app that alerts users when a device travels more than 200 meters away from a designated location.\nYou need to design an Azure Stream Analytics job to process the data for the planned app. The solution must minimize the amount of code developed and the number of technologies used.\nWhat should you include in the Stream Analytics job? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0014800001.jpg"],"topic":"2","answer":"","discussion":[{"comment_id":"444616","content":"Correct solution!","upvote_count":"52","comments":[{"timestamp":"1731879600.0","upvote_count":"1","poster":"jpgsa11","comment_id":"1213032","content":"Agree!"}],"timestamp":"1647276000.0","poster":"Podavenna"},{"comments":[{"upvote_count":"1","timestamp":"1727332500.0","content":"Very clear ,thank you for the explanation !","poster":"Little_Soap","comment_id":"1183135"}],"content":"Answers provided are correct!\nThe input type for the Stream Analytics job should be Stream, as it will be processing real-time data from devices.\nThe function to include in the Stream Analytics job should be Geospatial, which allows you to perform calculations on geographic data and make spatial queries, such as determining the distance between two points. This is necessary to determine if a device has traveled more than 200 meters away from a designated location.","upvote_count":"19","poster":"MrWood47","timestamp":"1689268620.0","comment_id":"774789"},{"poster":"Dusica","comment_id":"1201679","timestamp":"1729819380.0","upvote_count":"1","content":"Stream it is\nBut streaming must use windowing function, I think"},{"upvote_count":"1","content":"1-stream\n2-geospatial","comment_id":"998426","poster":"kkk5566","timestamp":"1709551920.0"},{"content":"1-stream\n2-geospatial","comment_id":"974686","timestamp":"1707315780.0","poster":"akhil5432","upvote_count":"2"},{"timestamp":"1706578680.0","upvote_count":"1","comment_id":"966713","poster":"hiyoww","content":"this is microsoft link for reference:\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios"},{"timestamp":"1701108720.0","upvote_count":"3","content":"Correct","comment_id":"908094","poster":"dheeraj7654"},{"timestamp":"1701091260.0","upvote_count":"2","content":"Stream & Geospatial is Correct","poster":"Ankit_Az","comment_id":"907927"},{"poster":"rocky48","upvote_count":"2","comment_id":"900371","content":"Stream & Geospatial","timestamp":"1700246520.0"},{"comment_id":"765107","content":"I doubt that given solution is correct.\nNo reason to stream the designated location, that is used for lookup. Think of it as a dimension table.\n\nInput type: Reference \nFunction: Geospatial","poster":"hanzocuk","upvote_count":"2","timestamp":"1688417820.0"},{"upvote_count":"1","timestamp":"1687004220.0","content":"Input Type: Stream\nFunction: Geospatial","comment_id":"748106","poster":"vigilante89"},{"content":"Good application and need to learn more about it!","upvote_count":"4","timestamp":"1684347420.0","comment_id":"720832","poster":"Amnoyana"},{"poster":"Deeksha1234","upvote_count":"4","content":"solution is correct","timestamp":"1675067100.0","comment_id":"639516"},{"comment_id":"602981","upvote_count":"2","content":"Correct","poster":"ClassMistress","timestamp":"1668706980.0"},{"comment_id":"584075","poster":"NewTuanAnh","content":"Correct!","timestamp":"1665470040.0","upvote_count":"2"},{"content":"Correct","comment_id":"534655","timestamp":"1659001380.0","poster":"PallaviPatel","upvote_count":"2"}],"answer_ET":"","question_id":119,"url":"https://www.examtopics.com/discussions/microsoft/view/62034-exam-dp-203-topic-2-question-1-discussion/","exam_id":67,"answer_description":"Input type: Stream -\nYou can process real-time IoT data streams with Azure Stream Analytics.\n\nFunction: Geospatial -\nWith built-in geospatial functions, you can use Azure Stream Analytics to build applications for scenarios such as fleet management, ride sharing, connected cars, and asset tracking.\nNote: In a real-world scenario, you could have hundreds of these sensors generating events as a stream. Ideally, a gateway device would run code to push these events to Azure Event Hubs or Azure IoT Hubs.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-get-started-with-azure-stream-analytics-to-process-data-from-iot-devices https://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios"},{"id":"FtEmpSpcSCMtyXgEw12J","url":"https://www.examtopics.com/discussions/microsoft/view/62528-exam-dp-203-topic-2-question-10-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0016700001.png","https://www.examtopics.com/assets/media/exam-media/04259/0016800001.png","https://www.examtopics.com/assets/media/exam-media/04259/0016800002.jpg"],"answer_ET":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0016900001.jpg"],"unix_timestamp":1632282120,"question_text":"DRAG DROP -\nYou have an Apache Spark DataFrame named temperatures. A sample of the data is shown in the following table.\n//IMG//\n\nYou need to produce the following table by using a Spark SQL query.\n//IMG//\n\nHow should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all.\nYou may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","topic":"2","timestamp":"2021-09-22 05:42:00","question_id":120,"answers_community":[],"exam_id":67,"answer":"","isMC":false,"discussion":[{"poster":"SujithaVulchi","upvote_count":"46","content":"correct answer, pivot and cast","comment_id":"451971","timestamp":"1648325640.0"},{"poster":"ggggyyyyy","upvote_count":"5","timestamp":"1647927720.0","comment_id":"449259","content":"correct. cast not convert"},{"timestamp":"1727552460.0","poster":"Alongi","upvote_count":"1","content":"Correct","comment_id":"1185046"},{"poster":"MarkJoh","comment_id":"1085392","upvote_count":"1","content":"Pivot and Cast are correct.\nThere is an issue with the problem though.\nIt should be CAST(avg(temp)...\nand not\nAvg (Cast(temp)...","comments":[{"upvote_count":"2","content":"No this is not correct, if you CAST(AVG(temp)) then you will first get AVG(temp) as an int. Casting it then results in the decimal value being 0 (like 2.0, 3.0...).\nTherefore, we have to AVG(CAST(temp)).","timestamp":"1719566520.0","comment_id":"1107696","poster":"jongert"}],"timestamp":"1717256880.0"},{"content":"Answer is correct","upvote_count":"1","comment_id":"1028222","poster":"ellala","timestamp":"1712599260.0"},{"timestamp":"1709639040.0","content":"pivot and cast","comment_id":"999318","poster":"kkk5566","upvote_count":"1"},{"comment_id":"907940","poster":"Ankit_Az","timestamp":"1701092400.0","upvote_count":"1","content":"Correct"},{"upvote_count":"2","timestamp":"1674139440.0","comment_id":"633556","poster":"jainparag1","content":"correct explanation"}],"answer_description":"Box 1: PIVOT -\nPIVOT rotates a table-valued expression by turning the unique values from one column in the expression into multiple columns in the output. And PIVOT runs aggregations where they're required on any remaining column values that are wanted in the final output.\nIncorrect Answers:\nUNPIVOT carries out the opposite operation to PIVOT by rotating columns of a table-valued expression into column values.\n\nBox 2: CAST -\nIf you want to convert an integer value to a DECIMAL data type in SQL Server use the CAST() function.\nExample:\n\nSELECT -\nCAST(12 AS DECIMAL(7,2) ) AS decimal_value;\nHere is the result:\ndecimal_value\n12.00\nReference:\nhttps://learnsql.com/cookbook/how-to-convert-an-integer-to-a-decimal-in-sql-server/ https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot"}],"exam":{"provider":"Microsoft","isImplemented":true,"name":"DP-203","isBeta":false,"lastUpdated":"12 Apr 2025","numberOfQuestions":384,"id":67,"isMCOnly":false},"currentPage":24},"__N_SSP":true}