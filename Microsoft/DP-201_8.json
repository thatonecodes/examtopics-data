{"pageProps":{"questions":[{"id":"8SeQgdVQ7UhX8peYgAbT","topic":"1","question_id":36,"question_images":[],"discussion":[{"poster":"davem0193","comment_id":"381408","timestamp":"1623629700.0","upvote_count":"2","content":"I guess anywhere a question says 'multi region', the answer has to be cosmosdb :)"},{"timestamp":"1619866260.0","poster":"sjain91","content":"Cosmos DB 100%","upvote_count":"4","comment_id":"346853"}],"choices":{"A":"Azure Cosmos DB","D":"Azure Data Lake Storage","B":"Azure Synapse Analytics","C":"Azure SQL Database"},"timestamp":"2021-05-01 12:51:00","answer_images":[],"question_text":"You are designing a storage solution for streaming data that is processed by Azure Databricks. The solution must meet the following requirements:\n✑ The data schema must be fluid.\n✑ The source data must have a high throughput.\n✑ The data must be available in multiple Azure regions as quickly as possible.\nWhat should you include in the solution to meet the requirements?","answers_community":[],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/51391-exam-dp-201-topic-1-question-41-discussion/","answer_description":"Azure Cosmos DB is Microsoft's globally distributed, multi-model database. Azure Cosmos DB enables you to elastically and independently scale throughput and storage across any number of Azure's geographic regions. It offers throughput, latency, availability, and consistency guarantees with comprehensive service level agreements (SLAs).\nYou can read data from and write data to Azure Cosmos DB using Databricks.\nNote on fluid schema:\nIf you are managing data whose structures are constantly changing at a high rate, particularly if transactions can come from external sources where it is difficult to enforce conformity across the database, you may want to consider a more schema-agnostic approach using a managed NoSQL database service like Azure\nCosmos DB.\nReference:\nhttps://docs.databricks.com/data/data-sources/azure/cosmosdb-connector.html https://docs.microsoft.com/en-us/azure/cosmos-db/relational-nosql","answer_ET":"A","exam_id":66,"unix_timestamp":1619866260,"answer":"A"},{"id":"Z1pgT788xWkpGY9QjJnX","answer":"B","url":"https://www.examtopics.com/discussions/microsoft/view/46594-exam-dp-201-topic-1-question-42-discussion/","unix_timestamp":1615502520,"answers_community":[],"isMC":true,"exam_id":66,"topic":"1","discussion":[{"poster":"Geo_Barros","upvote_count":"42","timestamp":"1615804920.0","comments":[{"comment_id":"365232","poster":"cadio30","timestamp":"1621835880.0","content":"Referencing the link that was provided in the solution, it was stated in the blob path that it started using the \"profile name\" then proceed with the datetime stamp. It make sense that 'D' is the appropriate answer in this question.","upvote_count":"4"}],"content":"In my opinion, option \"D\" would be the right one.","comment_id":"311325"},{"timestamp":"1618055820.0","content":"I think B is correct. We want to minimize the time it takes for customers to query log files. 'Typically, the customers will query data generated on the day the data was created'. So it makes sense to include the path for a particular day i.e {Year}/{Month}/{Day} close to the start. Once we have reached a particular day then we will want to filter for a particular Customer so {Year}/{Month}/{Day}/{CustomerID}. Then we will want to aggregate down to hour and minute. The only other viable option will be D. The reason I think {CustomerID} should NOT be at the beginning of the path is in the case a Customer wants to query data related to multiple CustomerIDs on the same day.","poster":"rahul_t","upvote_count":"11","comment_id":"332506"},{"poster":"Marcus1612","content":"I think the key word is \"Multi-tenant\". It appears to me that the logs for a single customer need to be under its own branch. D is the right answer","upvote_count":"3","timestamp":"1632507240.0","comment_id":"451043"},{"upvote_count":"1","timestamp":"1630293000.0","content":"what is correct answer i'm confused between B and D?","poster":"J4C7","comment_id":"435003"},{"poster":"msn1712","upvote_count":"1","content":"Why now A be the correct answer? On the link - https://docs.microsoft.com/en-us/azure/cdn/cdn-azure-diagnostic-logs, it's mentioned:\n\nThe name of the blob follows the following naming convention:\n\nresourceId=/SUBSCRIPTIONS/{Subscription Id}/RESOURCEGROUPS/{Resource Group Name}/PROVIDERS/MICROSOFT.CDN/PROFILES/{Profile Name}/ENDPOINTS/{Endpoint Name}/ y={Year}/m={Month}/d={Day}/h={Hour}/m={Minutes}/PT1H.json\n\ny={Year}/m={Month}/d={Day}/h={Hour}/m={Minutes}/PT1H.json","timestamp":"1624723380.0","comment_id":"391385"},{"poster":"Alekx42","timestamp":"1623482700.0","upvote_count":"4","comment_id":"380287","comments":[{"comment_id":"385458","content":"\"this does not exclude the possibility of making queries \" that is additional assumption made the person who is supposed to answer it.","poster":"tes","timestamp":"1624102020.0","upvote_count":"1"}],"content":"Since it is stated that this is a multi-tenant application, customers would not (and probably should not be able to) query data of other customers. This makes D the right answer. \nMoreover, while it said that typically the queries are done on the same day the data is created, this does not exclude the possibility of making queries that range across multiple days or months. With solution B this becomes unpleasant, since you cannot just query year/month since that will return data of all customers for that month. With solution D all queries are easier, since customerID/year/month returns immediately all the data for that customer of that month. \nBasically, while it is true that both B and D allow for rapid quering of data for a single customer for a single day, B is worse for all queries that want data of more than 1 day."},{"comment_id":"378207","poster":"BigMF","timestamp":"1623237420.0","content":"All of these options are poor in my opinion and therefore hard to choose a “best” option. If it were me, I’d go with this: {CustomerID}/{year}/{month}/{day}/{CustomerID}_{year}{month}{day}{hour}{minute}.csv. This allows a customer to go directly to their folder and drill down quickly to the day they need. It also has the added benefit of the files being named intelligently and not just a “single bit of info”.csv. It also allows for easier maintenance down the road when customers leave by allowing you to easily archive or delete their data simply by archiving or deleting their folder. All that being said, I would go with D because I don’t think it is any slower for a customer to search for their data following that path than any of the others and in fact probably quicker. Also, it would provide easier maintenance down the road.","upvote_count":"1"},{"poster":"Mandar77","upvote_count":"3","content":"I think, Answer B is correct. This is how you would like to restrict the access. question says, customer will access log information on the same day. So if you organize containers on year - month -day -customer - hour - time way, every customer has to come to day folder of that year and month and go to his container to get logs for the day. \nIf you organize container based on customer - year - month -day - hour - time, every customer has to traverse the long search path to get to day to get the logs. With option B, searching path would be optimum considering requirement","comment_id":"376448","timestamp":"1623034140.0","comments":[{"content":"This logic is flawed because the customer still has to traverse a long search path when they drill down into the folder structure. You either traverse it to begin with or later in the drill down.","poster":"BigMF","timestamp":"1623236400.0","upvote_count":"1","comment_id":"378200"}]},{"poster":"tanza","comment_id":"355762","timestamp":"1620848640.0","upvote_count":"4","content":"I think answer is A"},{"comment_id":"343792","comments":[{"upvote_count":"4","poster":"KRV","timestamp":"1621639860.0","comment_id":"363312","content":"By the looks of the question overall your argument holds good however if you read the question carefully it says ... \n1. customers will query data generated on the day the data was created --> means it should start with a year to day granularity then \n2. log files will be generated for each customer at five-minute intervals --> Now you are left with 2 options either organize by customer ID / hr/min or hr/min customer ID , given the case and nothing is explicility mentioned it is safe to assume that queries will be more customer centric and then within customer at a point in time and hence answer A happens to be logically more correct in the context of question !\n{year}/{month}/{day}/{CustomerID}/{hour}/{minute}.csv"}],"content":"I am certain that B is wrong. Why should Customer ID be put randomly in between the data formats? \n\nI think D is the right answer and the reason is that each \"/\" takes you to a new directory (folder). As a hierarchy it would make the most sense to have a folder per customer, and then sort by date/time. Source: \"Blob Path Format\" Section here: https://docs.microsoft.com/en-us/azure/cdn/cdn-azure-diagnostic-logs#blob-path-format","timestamp":"1619504760.0","upvote_count":"4","poster":"Apox"},{"poster":"maynard13x8","content":"Answer is correct. D is wrong because you duplicate year and month folders. It is also worse option because consumers query data of the day so, when you set the name, you already have all the data you are interested in.","comment_id":"332051","timestamp":"1617985320.0","upvote_count":"2"},{"content":"The name of the blob follows the following naming convention:\n\nresourceId=/SUBSCRIPTIONS/{Subscription Id}/RESOURCEGROUPS/{Resource Group Name}/PROVIDERS/MICROSOFT.CDN/PROFILES/{Profile Name}/ENDPOINTS/{Endpoint Name}/ y={Year}/m={Month}/d={Day}/h={Hour}/m={Minutes}/PT1H.json\n\nso it should actually be answer a","comment_id":"330848","poster":"Kevin89","upvote_count":"3","timestamp":"1617852720.0"},{"timestamp":"1616502240.0","poster":"Nik71","content":"confuse between A and B after reviewing https://docs.microsoft.com/en-us/azure/cdn/cdn-azure-diagnostic-logs feels like why we avoid A here.","comment_id":"318067","upvote_count":"1"},{"poster":"Neha14n","timestamp":"1615681380.0","comment_id":"310107","comments":[{"timestamp":"1618012440.0","comment_id":"332202","upvote_count":"1","content":"agree, in this case B is more suitable","poster":"DongDuong"}],"content":"Typically, the customers will query data generated on the day the data was created.\nThis line clears query will be specific to date not customer. Or else D would be correct answer","upvote_count":"3"},{"timestamp":"1615502520.0","poster":"AlexD332","content":"still not clear as query should be optimized for customers - they won't request not their data.","upvote_count":"4","comment_id":"308413"}],"answer_description":"Reference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-azure-diagnostic-logs","question_text":"You are designing a log storage solution that will use Azure Blob storage containers.\nCSV log files will be generated by a multi-tenant application. The log files will be generated for each customer at five-minute intervals. There will be more than\n5,000 customers. Typically, the customers will query data generated on the day the data was created.\nYou need to recommend a naming convention for the virtual directories and files. The solution must minimize the time it takes for the customers to query the log files.\nWhat naming convention should you recommend?","question_images":[],"question_id":37,"choices":{"D":"{CustomerID}/{year}/{month}/{day}/{hour}/{minute}.csv","A":"{year}/{month}/{day}/{hour}/{minute}/{CustomerID}.csv","B":"{year}/{month}/{day}/{CustomerID}/{hour}/{minute}.csv","C":"{minute}/{hour}/{day}/{month}/{year}/{CustomeriD}.csv"},"timestamp":"2021-03-11 23:42:00","answer_images":[],"answer_ET":"B"},{"id":"k7Q5OMrdOimYJTJLhjbb","isMC":true,"exam_id":66,"answer_ET":"B","answer_description":"You can form a partition key by concatenating multiple property values into a single artificial partitionKey property. These keys are referred to as synthetic keys.\nIncorrect Answers:\nD: Publish Datetime will be populated only when Publish Status is set to published.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/synthetic-partition-keys","question_id":38,"answer":"B","answer_images":[],"unix_timestamp":1618732020,"question_text":"You are designing an Azure Cosmos DB database that will contain news articles.\nThe articles will have the following properties: Category, Created Datetime, Publish Datetime, Author, Headline, Body Text, and Publish\nStatus. Multiple articles will be published in each category daily, but no two stories in a category will be published simultaneously.\nHeadlines may be updated over time. Publish Status will have the following values: draft, published, updated, and removed. Most articles will remain in the published or updated status. Publish Datetime will be populated only when Publish Status is set to published.\nYou will serve the latest articles to websites for users to consume.\nYou need to recommend a partition key for the database container. The solution must ensure that the articles are served to the websites as quickly as possible.\nWhich partition key should you recommend?","timestamp":"2021-04-18 09:47:00","answers_community":[],"discussion":[{"upvote_count":"6","comment_id":"338011","timestamp":"1618732020.0","comments":[{"content":"And publish date as well, from null to some value. Changing partition keys is not allowed, so only possible answer is B","upvote_count":"2","poster":"hello_there_","timestamp":"1628528280.0","comment_id":"422239"}],"poster":"anamaster","content":"and the publish status and headline will change"},{"content":"the propose solution is correct. Publish datetime is not an option here as the partition key should be in string or integer","comment_id":"365235","poster":"cadio30","timestamp":"1621836060.0","upvote_count":"3"}],"choices":{"D":"Publish Date + random suffix","B":"Category + Created Datetime","A":"Publish Status","C":"Headline"},"url":"https://www.examtopics.com/discussions/microsoft/view/50352-exam-dp-201-topic-1-question-43-discussion/","topic":"1","question_images":[]},{"id":"2WojxJz7GhbHaEfEVk82","question_id":39,"unix_timestamp":1618160160,"timestamp":"2021-04-11 18:56:00","answer_images":[],"choices":{"C":"Gremlin API","A":"Cassandra API","B":"Core (SQL) API"},"url":"https://www.examtopics.com/discussions/microsoft/view/49915-exam-dp-201-topic-1-question-44-discussion/","discussion":[{"poster":"rmk4ever","content":"Ans: Core (SQL) API\nref: \nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/4-use-the-core-sql-api-to-store-a-product-catalog","comment_id":"335052","upvote_count":"33","timestamp":"1618369680.0"},{"comments":[{"content":"you are right","poster":"rmk4ever","upvote_count":"1","comments":[{"timestamp":"1620590340.0","comment_id":"353250","upvote_count":"5","poster":"Mily94","content":"are you sure? \"he product properties will be different for each product and additional properties will be added to products as needed\" indicates NoSQL (Cassandra)"}],"comment_id":"335053","timestamp":"1618369800.0"}],"comment_id":"333466","timestamp":"1618160160.0","content":"As Microsoft recommend, when you create a cosmos dB from scratch and there isn’t any previous work that you could reuse, you should use sql api , unless you need relationships between data, in which case you should use gremnlin.","upvote_count":"9","poster":"maynard13x8"},{"timestamp":"1640829600.0","upvote_count":"1","content":"\"You've decided to look at how the new project is going to store the catalog for your customer facing e-commerce site. The sales team is likely to need support for adding new product categories quickly. The team had issues in the past as the old system that was using a relational database was too structured. Any necessary changes to add properties to products required downtime to update the table schemas, queries, and databases.\"\n\n\"Supporting new product categories is an important requirement for your project, and the Core (SQL) schema is flexible and requires a schemaless data store.\"\n\nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/4-use-the-core-sql-api-to-store-a-product-catalog","poster":"corebit","comment_id":"512935"},{"content":"\"Cassandra This API isn't a good choice in this particular scenario, because the schema is unknown and will change over time.\"\nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/4-use-the-core-sql-api-to-store-a-product-catalog","poster":"tes","timestamp":"1624111260.0","comment_id":"385534","upvote_count":"2"},{"poster":"cadio30","comment_id":"365238","timestamp":"1621836300.0","content":"By all means it is \"SQL API\" \n\nReference: https://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/","upvote_count":"1"},{"poster":"toandm","upvote_count":"3","content":"Answer is Core (SQL) API. Core (SQL) API is a document database, which is also NoSQL database. It has the name SQL because you can use SQL language to query it, not because it is relational DB","comment_id":"363565","timestamp":"1621677840.0"},{"content":"A. Cassandra API is correct. the product catalog was an example for cassandra api in the MS Learning Path for this exam.","poster":"Hrabia","upvote_count":"2","timestamp":"1620958680.0","comments":[{"upvote_count":"2","comment_id":"379237","content":"are you sure about that? https://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/4-use-the-core-sql-api-to-store-a-product-catalog\n\n\"Cassandra - This API isn't a good choice in this particular scenario, because the schema is unknown and will change over time.\"","poster":"HeywwooodJab","timestamp":"1623354000.0"}],"comment_id":"356830"}],"answer_description":"Cassandrsa is a type of NoSQL database.\nNoSQL database (sometimes called as Not Only SQL) is a database that provides a mechanism to store and retrieve data other than the tabular relations used in relational databases.\nIncorrect Answers:\nB: Core (SQL) API is a relational database which does not fit this scenario.\nC: Gremlin is the graph traversal language of Apache TinkerPop. Gremlin is a functional, data-flow language that enables users to succinctly express complex traversals on (or queries of) their application's property graph.\nReference:\nhttps://www.tutorialspoint.com/cassandra/cassandra_introduction.htm","question_images":[],"answers_community":[],"topic":"1","question_text":"You are designing a product catalog for a customer. The product data will be stored in Azure Cosmos DB. The product properties will be different for each product and additional properties will be added to products as needed.\nWhich Cosmos DB API should you use to provision the database?","answer_ET":"A","exam_id":66,"isMC":true,"answer":"A"},{"id":"06efMzSOeO8iH3ZbmGIt","choices":{"D":"Azure Data Lake Storage Gen2","B":"Azure Synapse","C":"Azure Analysis Services","A":"Azure Cosmos DB"},"question_id":40,"isMC":true,"timestamp":"2021-06-17 23:47:00","exam_id":66,"url":"https://www.examtopics.com/discussions/microsoft/view/55544-exam-dp-201-topic-1-question-45-discussion/","answer_description":"Gremlin is one of the most popular query languages for exploring and analyzing data modeled as property graphs. There are many graph-database vendors out there that support Gremlin as their query language, in particular Azure Cosmos DB which is one of the world's first self-managed, geo-distributed, multi-master capable graph databases.\nAzure Synapse Link for Azure Cosmos DB is a cloud native hybrid transactional and analytical processing (HTAP) capability that enables you to run near real-time analytics over operational data. Synapse Link creates a tight seamless integration between Azure Cosmos DB and Azure Synapse Analytics.\nReference:\nhttps://jayanta-mondal.medium.com/analyzing-and-improving-the-performance-azure-cosmos-db-gremlin-queries-7f68bbbac2c https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link-use-cases","discussion":[{"comment_id":"384463","content":"Gremlin API is supported by Cosmos DB only.","upvote_count":"5","poster":"SandeshVartak","timestamp":"1623966420.0"}],"answer":"A","topic":"1","answers_community":[],"answer_images":[],"unix_timestamp":1623966420,"answer_ET":"A","question_images":[],"question_text":"You work for a finance company.\nYou need to design a business network analysis solution that meets the following requirements:\n✑ Analyzes the flow of transactions between the Azure environments of the company's various partner organizations\n✑ Supports Gremlin (graph) queries\nWhat should you include in the solution?"}],"exam":{"numberOfQuestions":206,"id":66,"provider":"Microsoft","isMCOnly":false,"lastUpdated":"12 Apr 2025","isImplemented":true,"isBeta":false,"name":"DP-201"},"currentPage":8},"__N_SSP":true}