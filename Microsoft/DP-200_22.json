{"pageProps":{"questions":[{"id":"MpkBzekaRxvUrnbv4dA1","timestamp":"2020-11-26 13:15:00","answer_description":"We need one reference data input for LocationIncomes, which rarely changes\nNote: Stream Analytics also supports input known as reference data. Reference data is either completely static or changes slowly.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#stream-and-reference-inputs","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/37835-exam-dp-200-topic-2-question-36-discussion/","unix_timestamp":1606392900,"answer":"B","discussion":[{"content":"You need a reference so answer is no","upvote_count":"4","poster":"syu31svc","timestamp":"1606392900.0","comment_id":"228283"}],"question_id":106,"answers_community":[],"answer_ET":"B","choices":{"B":"No","A":"Yes"},"topic":"2","answer_images":[],"exam_id":65,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a solution that will use Azure Stream Analytics. The solution will accept an Azure Blob storage file named Customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address.\nYou have a file in Blob storage named LocationIncomes that contains median incomes based on location. The file rarely changes.\nYou need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure\nData Lake Storage Gen2 for long-term retention.\nSolution: You implement a Stream Analytics job that has two streaming inputs, one query, and two outputs.\nDoes this meet the goal?"},{"id":"OvYHMRDninXNoF8wkt5C","url":"https://www.examtopics.com/discussions/microsoft/view/14378-exam-dp-200-topic-2-question-37-discussion/","answer_description":"Step 1: Create an Azure Blob Storage container\nTo prepare your Stream Analytics job to be deployed on an IoT Edge device, you need to associate the job with a container in a storage account. When you go to deploy your job, the job definition is exported to the storage container.\nStep 2: Create an Azure Stream Analytics edge job and configure job definition save location\nWhen you create an Azure Stream Analytics job to run on an IoT Edge device, it needs to be stored in a way that can be called from the device.\nStep 3: Create and IoT hub and add the Azure Stream Analytics module to the IoT Hub namespace\nAn IoT Hub in Azure is required.\nStream Analytics accepts data incoming from several kinds of event sources including Event Hubs, IoT Hub, and Blob storage.\n\nStep 4: Configure routes -\nYou are now ready to deploy the Azure Stream Analytics job on your IoT Edge device.\nThe routes that you declare define the flow of data through the IoT Edge device.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge","unix_timestamp":1581998100,"answers_community":[],"topic":"2","answer":"","question_text":"DRAG DROP -\nYou develop data engineering solutions for a company.\nYou need to deploy a Microsoft Azure Stream Analytics job for an IoT solution. The solution must:\n✑ Minimize latency.\n✑ Minimize bandwidth usage between the job and IoT device.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","question_id":107,"answer_ET":"","isMC":false,"exam_id":65,"discussion":[{"timestamp":"1581998100.0","upvote_count":"74","comment_id":"51954","poster":"SAMBIT","content":"Wrong order:\n\nCorrect order can be found in the article \n\n1) storage\n2) Edge \n3) IOT \n4) route\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge","comments":[{"timestamp":"1611988200.0","content":"but the link doesnt give any mention about the order","upvote_count":"4","comment_id":"279782","comments":[{"poster":"Billybob0604","content":"what would in sequence mean to you ??","upvote_count":"1","timestamp":"1671566460.0","comment_id":"751419"}],"poster":"dev2dev"},{"comment_id":"351740","upvote_count":"4","content":"This is wrong order, provided answer is correct. Refer the below link\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal","timestamp":"1620371460.0","poster":"Sasidhar39"},{"upvote_count":"2","comment_id":"363510","content":"SAMBIT's answer is correct. When you add the Stream Analytics module to the IoT hub you must select an existing Stream Analytics job. So you must create the Stream Analytics Edge job first","timestamp":"1621671600.0","poster":"Alekx42"},{"comment_id":"365431","timestamp":"1621845900.0","poster":"Ambujinee","content":"Correct ans","upvote_count":"2"}]},{"timestamp":"1599627120.0","upvote_count":"12","poster":"kova123","content":"1. Create Storage\n2. Create ASA Job\n3. Create IOT Hug and Add ASA Job\n4. Configure Routes\nThis is correct order","comment_id":"176267"},{"content":": Create an IoT hub and add the Azure Stream Analytics module to the IoT Hub namespace\nAn IoT Hub in Azure is required. It’s a Prerequisites. So the answer is correct","comment_id":"339228","upvote_count":"2","timestamp":"1618878780.0","poster":"Wendy_DK"},{"comment_id":"318983","content":"answer is in correct.","upvote_count":"2","poster":"Lolo_alzin19","timestamp":"1616577900.0"},{"comment_id":"229568","timestamp":"1606561740.0","upvote_count":"5","content":"From https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge:\nAzure Stream Analytics (ASA) on IoT Edge empowers developers to deploy near-real-time analytical intelligence closer to IoT devices so that they can unlock the full value of device-generated data. Azure Stream Analytics is designed for low latency, resiliency, efficient use of bandwidth, and compliance\n1) Create a storage container (create blob container)\n2) Create an ASA edge job (create edge job)\n3) Setup your IoT Edge environment on your device(s) (create IoT hub)\n4) Deploy ASA on your IoT Edge device(s) (configure routes)","poster":"syu31svc"},{"comments":[{"poster":"dumpsm42","comment_id":"236599","timestamp":"1607269920.0","content":"yes. this is the answer, the link is perfect, it has the sequence, the only difference is that we use a Edge instead of Cloud Job, like the text says. Regards","upvote_count":"1"},{"timestamp":"1620361440.0","poster":"cadio30","content":"Stated clearly in the documentation the procedure on the configuration. Cheers!","upvote_count":"2","comments":[{"content":"Reference: https://rangv.github.io/azureiotedgelab/streamanalytics/","comment_id":"354347","upvote_count":"2","timestamp":"1620711840.0","poster":"cadio30"}],"comment_id":"351635"}],"poster":"Arsa","comment_id":"152487","timestamp":"1596797460.0","content":"Given answer is correct as per the article.. you need to create IOT hub first\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal..","upvote_count":"4"},{"poster":"krisspark","content":"why Azure blob storage only why not Azure Data Lake storage?","upvote_count":"1","comment_id":"132018","comments":[{"poster":"sandGrain","content":"Data lake is not a accepted input in ASA only Blob, Iot Hib and Event Hub","timestamp":"1601188440.0","comment_id":"188161","upvote_count":"2"}],"timestamp":"1594464180.0"},{"timestamp":"1587117360.0","comments":[{"content":"i agree,\nCreate a storage account\nWhen you create an Azure Stream Analytics job to run on an IoT Edge device, it needs to be stored in a way that can be called from the device. You can use an existing Azure Storage account, or create a new one now.\nhttps://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics","comment_id":"87450","timestamp":"1589262060.0","poster":"wyxh","upvote_count":"2"}],"content":"Based on what I see, storage can be defined before or after the Edge, as long as it's defined before the input definition. However, it makes more sense to create Storage account before Hub, so it will be streamlines.","upvote_count":"1","comment_id":"75666","poster":"Leonido"},{"comment_id":"59856","content":"Given answer is correct refer below article\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal","poster":"Amitkhanna","comments":[{"timestamp":"1585121160.0","content":"this article refers to stream analytics job hosting in the cloud (!). This question refers to the job on edge.","poster":"jdpl","comment_id":"68075","upvote_count":"6"}],"timestamp":"1583493960.0","upvote_count":"4"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0021900001.jpg"],"timestamp":"2020-02-18 04:55:00","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0022000001.jpg"]},{"id":"XfRb4ZuyDCn3xuIZElya","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0022200001.jpg"],"question_text":"DRAG DROP -\nYou have data stored in thousands of CSV files in Azure Data Lake Storage Gen2. Each file has a header row followed by a property formatted carriage return (/r) and line feed (/n).\nYou are implementing a pattern that batch loads the files daily into an enterprise data warehouse in Azure Synapse Analytics by using PolyBase.\nYou need to skip the header row when you import the files into the data warehouse. Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","answers_community":[],"unix_timestamp":1642758420,"topic":"2","isMC":false,"question_id":108,"exam_id":65,"discussion":[{"poster":"javedjss","comment_id":"529062","upvote_count":"1","content":"Correct Answer.","timestamp":"1642758420.0"}],"url":"https://www.examtopics.com/discussions/microsoft/view/70365-exam-dp-200-topic-2-question-38-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0022300001.jpg"],"answer_ET":"","timestamp":"2022-01-21 10:47:00","answer":"","answer_description":"Step 1: Create an external data source that uses the abfs location\nCreate External Data Source to reference Azure Data Lake Store Gen 1 or 2\nStep 2: Create an external file format and set the First_Row option.\nCreate External File Format.\nStep 3: Use CREATE EXTERNAL TABLE AS SELECT (CETAS) and configure the reject options to specify reject values or percentages\nTo use PolyBase, you must create external tables to reference your external data.\nUse reject options.\nNote: REJECT options don't apply at the time this CREATE EXTERNAL TABLE AS SELECT statement is run. Instead, they're specified here so that the database can use them at a later time when it imports data from the external table. Later, when the CREATE TABLE AS SELECT statement selects data from the external table, the database will use the reject options to determine the number or percentage of rows that can fail to import before it stops the import.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql"},{"id":"O8XacgmX7ddKKQEIV4B1","answer_images":[],"isMC":true,"answer_ET":"A","answer_description":"You can override the primary language by specifying the language magic command %<language> at the beginning of a cell. The supported magic commands are:\n%python, %r, %scala, and %sql.\nReferences:\nhttps://docs.databricks.com/user-guide/notebooks/notebook-use.html#mix-languages","unix_timestamp":1608038580,"exam_id":65,"answer":"A","timestamp":"2020-12-15 14:23:00","choices":{"A":"%<language>","C":"\\\\(<language>)","B":"\\\\[<language>]","D":"@<Language>"},"topic":"2","url":"https://www.examtopics.com/discussions/microsoft/view/39910-exam-dp-200-topic-2-question-39-discussion/","question_images":[],"answers_community":[],"question_text":"You are creating a new notebook in Azure Databricks that will support R as the primary language but will also support Scala and SQL.\nWhich switch should you use to switch between languages?","question_id":109,"discussion":[{"poster":"akash0680","content":"Correct Answer :)","timestamp":"1608038580.0","upvote_count":"18","comment_id":"244573"},{"timestamp":"1626250200.0","comment_id":"406065","comments":[{"comment_id":"447529","timestamp":"1632046260.0","upvote_count":"1","content":"in Databricks you use just '%' e.g. %scala ... CODE","poster":"tanssive"}],"upvote_count":"1","content":"not exactly. its %%","poster":"Allapanda"}]},{"id":"TEXdk407TKrIdwnosVqp","unix_timestamp":1617926940,"answers_community":[],"question_id":110,"isMC":false,"discussion":[{"content":"The propose solution is correct.\nA table cannot be created if the notebook is not yet available, the scenario is in assumption the table is within the hdinsight spark cluster.","timestamp":"1619761740.0","comment_id":"345910","comments":[{"upvote_count":"2","content":"Reference: https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-load-data-run-query","poster":"cadio30","timestamp":"1620353640.0","comment_id":"351560"}],"poster":"cadio30","upvote_count":"6"},{"content":"Agree with Pairon. Ideal to have Target table table created first before Jupyter notebook","timestamp":"1618893900.0","upvote_count":"1","poster":"Aragorn_2021","comment_id":"339326"},{"upvote_count":"1","comment_id":"334824","poster":"Pairon","content":"I agree with the answer, but maybe we can swapp second and third step?","timestamp":"1618333680.0"},{"comment_id":"331762","poster":"tucho","content":"With the proposed solution, \"who\" runs the Jupyter notebook? :-(","upvote_count":"1","timestamp":"1617947460.0"},{"comment_id":"331617","poster":"JohnCrawford","content":"answer appears correct. https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-use-bi-tools","upvote_count":"3","timestamp":"1617926940.0"}],"answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0014700001.png"],"exam_id":65,"topic":"2","timestamp":"2021-04-09 02:09:00","answer_ET":"","answer_description":"Step 1: Create an HDInisght cluster with the Spark cluster type\nStep 2: Create a Jyputer Notebook\n\nStep 3: Create a table -\nThe Jupyter Notebook that you created in the previous step includes code to create an hvac table.\nStep 4: Run a job that uses the Spark Streaming API to ingest data from Twitter\nStep 5: Load the hvac table into Power BI Desktop\nYou use Power BI to create visualizations, reports, and dashboards from the Spark cluster data.\nReferences:\nhttps://acadgild.com/blog/streaming-twitter-data-using-spark\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-use-with-data-lake-store","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0014800001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/49672-exam-dp-200-topic-2-question-4-discussion/","question_text":"DRAG DROP -\nYou develop data engineering solutions for a company.\nA project requires analysis of real-time Twitter feeds. Posts that contain specific keywords must be stored and processed on Microsoft Azure and then displayed by using Microsoft Power BI. You need to implement the solution.\nWhich five actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//"}],"exam":{"lastUpdated":"12 Apr 2025","isImplemented":true,"numberOfQuestions":228,"isBeta":false,"name":"DP-200","isMCOnly":false,"provider":"Microsoft","id":65},"currentPage":22},"__N_SSP":true}