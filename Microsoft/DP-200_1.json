{"pageProps":{"questions":[{"id":"rZpNcAQLM84hyCYkRYIp","question_images":[],"topic":"1","choices":{"A":"Interactive Query","D":"Apache Spark","C":"Apache HBase","B":"Apache Hadoop"},"answers_community":[],"discussion":[{"timestamp":"1619255580.0","upvote_count":"9","comments":[{"upvote_count":"2","content":"It also mentions SQL queries which is not Hive ( Interactive Query)","timestamp":"1620733140.0","poster":"Amy007","comment_id":"354662"}],"comment_id":"341921","poster":"dangal95","content":"Could the write answer be D becase Spark has;\n1) Interactive queries through spark-sql\n2) Datawarehousing capabilities through Delta Lake (and also spark-sql creates in memory tables)\n3) Less management because these are out-of-the-box features?"},{"poster":"jedi01","timestamp":"1597783500.0","comment_id":"161098","content":"I think the answer should be A. Interactive Query.\nHere I am implementing Lambda architecture using a open source technology which can be Apache Spark and already in use. The prevailing issue here Analytical Processing is very slow , in another words queries are slow. So I created an HDInsight Cluster of type \"Interactive Query\" to support Analytical processing/ fast query access, data warehousing etc. We can use HiveQL on Interactive Query. Refer to https://docs.microsoft.com/en-us/azure/hdinsight/interactive-query/apache-interactive-query-get-started","upvote_count":"8"},{"poster":"agmadeira","content":"A - Interactive Query - \"Deliver SQL query responses in less than one second\"\nhttps://docs.microsoft.com/en-us/azure/hdinsight/interactive-query/apache-interactive-query-get-started","timestamp":"1624796220.0","upvote_count":"2","comment_id":"392028"},{"poster":"Hinzzz","content":"D is correct based on Data warehousing requirement.","comment_id":"385853","timestamp":"1624153140.0","upvote_count":"1"},{"timestamp":"1617686700.0","poster":"sandeep1111","upvote_count":"1","content":"correct","comment_id":"329297"},{"upvote_count":"1","comment_id":"286727","poster":"Satya217","content":"https://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architectur","timestamp":"1612867800.0"},{"content":"Correct Answer in Spark because it is in memory","timestamp":"1611161700.0","poster":"Trivender","upvote_count":"1","comment_id":"272109"},{"timestamp":"1607716920.0","poster":"dumpsm42","upvote_count":"5","comment_id":"241177","content":"hi to all,\n\nanswer:\n\nhttps://azure.microsoft.com/pt-pt/blog/general-availability-of-hdinsight-interactive-query-blazing-fast-data-warehouse-style-queries-on-hyper-scale-data-2/\nsub-second !\n\nSummary\nThis week at Ignite, we are pleased to announce general availability of Azure HDInsight Interactive Query. Backed by our enterprise-grade SLA, HDInsight Interactive Query brings sub-second speed to data warehouse style SQL queries to the hyper-scale data stored in commodity cloud storage.\n\nregards"},{"upvote_count":"6","comment_id":"230666","content":"Exam was updated on Nov 24, 2020. Didn't see too many questions from the test on ExamTopics...maybe 20-30% of the test questions. Suggest waiting a bit to take the test so that all the exam prep questions are updated. Exam definitely requires hands-on knowledge of the products. A lot of questions on CosmosDB consistency settings, encryption/security, monitoring/metrics.","timestamp":"1606692420.0","comments":[{"poster":"wahwah","content":"Do you passed the exam after 24 Nov, is there any difference regarding these questions and the updated ones ? I mean if I prepared the exam with these version of a questions what is my chances to pass it ? thank you","upvote_count":"4","comment_id":"231950","timestamp":"1606829220.0"}],"poster":"J_i_L_L"},{"timestamp":"1604866800.0","upvote_count":"3","poster":"uomer","comment_id":"215511","content":"I also vote for Interactive Query as \"An Interactive Query cluster is different from an Apache Hadoop cluster. It contains only the Hive service.\nRequirements: \n✑ Provide data warehousing ( Yes) \n✑ Reduce ongoing management activities (Not sure)\n✑ Deliver SQL query responses in less than one second ( Yes)"},{"timestamp":"1604638140.0","poster":"sunil08","comment_id":"213855","upvote_count":"1","content":"D: Apache spark"},{"comment_id":"201871","upvote_count":"1","poster":"nehab0101","content":"https://azure.microsoft.com/en-in/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/","timestamp":"1603005600.0"},{"upvote_count":"4","timestamp":"1602895140.0","content":"Apache Spark is correct Answer !!\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview","comment_id":"201270","poster":"Mittun"},{"timestamp":"1597053120.0","content":"i think the logic to answer this question is:\nLambda architecture:\nhttps://databricks.com/glossary/lambda-architecture\nAzure implementation:\nhttps://azure.microsoft.com/en-us/services/databricks/\nAzure Databricks = Fast, easy, and collaborative Apache SparkTM based analytics service","comment_id":"154401","upvote_count":"1","poster":"r8d1"},{"timestamp":"1592833740.0","comment_id":"116452","poster":"dfrp92","comments":[{"comment_id":"118634","upvote_count":"9","content":"Neither does any of the options, the last part of the question is key: Which type of cluster will you create, hence, Spark","timestamp":"1593019560.0","poster":"induna"}],"content":"How does Spark meet the requirements? Spark does not provide data warehousing by itself, it is not a data store.","upvote_count":"4"},{"comments":[{"poster":"john_smith","upvote_count":"1","timestamp":"1594705440.0","comment_id":"134595","content":"The link you provided is redirected to What is Azure Synapse Link for Azure Cosmos DB (Preview)?"}],"content":"https://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architecture","timestamp":"1583163840.0","comment_id":"57671","upvote_count":"2","poster":"AAJ"},{"comments":[{"upvote_count":"1","poster":"john_smith","timestamp":"1594705380.0","comment_id":"134594","content":"The link you provided is redirected to What is Azure Synapse Link for Azure Cosmos DB (Preview)? wy?"}],"content":"Would suggest to use the original link from MS: https://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architecture as better background documentation","poster":"Leonido","upvote_count":"2","timestamp":"1580978400.0","comment_id":"47237"}],"isMC":true,"unix_timestamp":1580978400,"question_text":"You are a data engineer implementing a lambda architecture on Microsoft Azure. You use an open-source big data solution to collect, process, and maintain data.\nThe analytical data store performs poorly.\nYou must implement a solution that meets the following requirements:\n✑ Provide data warehousing\n✑ Reduce ongoing management activities\n✑ Deliver SQL query responses in less than one second\nYou need to create an HDInsight cluster to meet the requirements.\nWhich type of cluster should you create?","url":"https://www.examtopics.com/discussions/microsoft/view/13483-exam-dp-200-topic-1-question-1-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0000300001.jpg"],"answer":"D","exam_id":65,"answer_description":"Lambda Architecture with Azure:\nAzure offers you a combination of following technologies to accelerate real-time big data analytics:\n1. Azure Cosmos DB, a globally distributed and multi-model database service.\n2. Apache Spark for Azure HDInsight, a processing framework that runs large-scale data analytics applications.\n3. Azure Cosmos DB change feed, which streams new data to the batch layer for HDInsight to process.\n4. The Spark to Azure Cosmos DB Connector\n\nNote: Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch processing and stream processing methods, and minimizing the latency involved in querying big data.\nReferences:\nhttps://sqlwithmanoj.com/2018/02/16/what-is-lambda-architecture-and-what-azure-offers-with-its-new-cosmos-db/","question_id":1,"timestamp":"2020-02-06 09:40:00","answer_ET":"D"},{"id":"rQH2l02ZtlK7JYILKi2p","answers_community":[],"choices":{"B":"Azure Traffic Manager","C":"Azure Resource Manager templates","D":"Ambari web user interface","A":"Azure Databricks"},"answer_images":[],"discussion":[{"comment_id":"269422","upvote_count":"3","timestamp":"1610875980.0","content":"C. Azure Resource Manager templates, \nls correct","poster":"dmnantilla9"},{"timestamp":"1606132980.0","content":"C is correct; afterall Templates are meant to make life easier ain't it?","poster":"syu31svc","upvote_count":"4","comment_id":"225774"},{"content":"Are the topics related to HDInsight still part of DP-200 exam?","poster":"M0e","timestamp":"1601025060.0","comment_id":"186848","comments":[{"comment_id":"199880","upvote_count":"2","timestamp":"1602688440.0","content":"yes, they are","poster":"JParzival"}],"upvote_count":"3"}],"answer":"C","exam_id":65,"isMC":true,"topic":"1","question_text":"The data engineering team manages Azure HDInsight clusters. The team spends a large amount of time creating and destroying clusters daily because most of the data pipeline process runs in minutes.\nYou need to implement a solution that deploys multiple HDInsight clusters with minimal effort.\nWhat should you implement?","answer_description":"A Resource Manager template makes it easy to create the following resources for your application in a single, coordinated operation:\n✑ HDInsight clusters and their dependent resources (such as the default storage account).\n✑ Other resources (such as Azure SQL Database to use Apache Sqoop).\nIn the template, you define the resources that are needed for the application. You also specify deployment parameters to input values for different environments.\nThe template consists of JSON and expressions that you use to construct values for your deployment.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-create-linux-clusters-arm-templates","answer_ET":"C","timestamp":"2020-07-05 11:39:00","question_images":[],"unix_timestamp":1593941940,"question_id":2,"url":"https://www.examtopics.com/discussions/microsoft/view/24765-exam-dp-200-topic-1-question-10-discussion/"},{"id":"ak3TfZRvXWu6rR4uhJeH","url":"https://www.examtopics.com/discussions/microsoft/view/5507-exam-dp-200-topic-1-question-11-discussion/","question_id":3,"isMC":true,"choices":{"B":"MongoDB API","A":"Table API","C":"Gremlin API","D":"SQL API","E":"Cassandra API"},"topic":"1","question_images":[],"answer_description":"B: Azure Cosmos DB is the globally distributed, multimodel database service from Microsoft for mission-critical applications. It is a multimodel database and supports document, key-value, graph, and columnar data models.\nE: Wide-column stores store data together as columns instead of rows and are optimized for queries over large datasets. The most popular are Cassandra and\nHBase.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction https://www.mongodb.com/scale/types-of-nosql-databases","answer":"BE","answers_community":[],"discussion":[{"content":"it should ba A and E","comments":[{"poster":"Vijaya","content":"as per documentation mongo API is multi-model and supports document, key-value, graph, and columnar data models so answer is correct ref https://docs.microsoft.com/en-us/azure/cosmos-db/mongodb-introduction","comment_id":"171133","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"380243","timestamp":"1623476940.0","content":"Answer is clearly A and E. No documentation refers to Mongo API .","poster":"ThiruthuvaRajan"},{"timestamp":"1620087000.0","poster":"Sasidhar39","comment_id":"349033","content":"No, the documentation didn't refer to that","upvote_count":"1"}],"timestamp":"1598926500.0"}],"upvote_count":"97","timestamp":"1574757720.0","poster":"mustaphaa","comment_id":"24516"},{"timestamp":"1583596200.0","comments":[{"poster":"Nayon","content":"right answer is A,E but slight change in your description. \n key value --> Cassandra API\ngraph --> Gremlin API\ndocument --> SQL API and MongoDB API\ncolumnar --> Table API \nhttps://cloud.netapp.com/blog/azure-cvo-blg-azure-nosql-types-services-and-a-quick-tutorial#H_H1","upvote_count":"3","timestamp":"1623492960.0","comment_id":"380375"}],"comment_id":"60358","upvote_count":"87","content":"key value --> Table API\ngraph --> Gremlin API\ndocument --> SQL API and MongoDB API\ncolumnar --> Cassandra API\n\nso, the answer is A,E","poster":"avestabrzn"},{"upvote_count":"1","content":"the correct answer is Table Api e Cassandra Api. so A - E","timestamp":"1637406660.0","poster":"massnonn","comment_id":"482439"},{"content":"It's A and E.","upvote_count":"1","poster":"LG5","comment_id":"323727","timestamp":"1617043680.0"},{"content":"Key-Value -> Table API\nWide column -> Cassandra API","timestamp":"1615916580.0","upvote_count":"1","comment_id":"312568","poster":"deepz8"},{"timestamp":"1614191220.0","upvote_count":"1","comment_id":"298466","poster":"SilNilanjan","content":"A & E certainly"},{"timestamp":"1611678240.0","comment_id":"277072","content":"Table API and Cassandra API for sure","upvote_count":"1","poster":"ssantos"},{"poster":"afonsomeireles","timestamp":"1611406740.0","comment_id":"274590","upvote_count":"1","content":"It's A (Table API for key-value pairs) and E (Cassandra API for wide-columns)"},{"comment_id":"237543","upvote_count":"1","timestamp":"1607363580.0","content":"AE. Table uses key/value. Not MongoDB. B is wrong.","poster":"rmn900"},{"comment_id":"236341","upvote_count":"1","content":"Should be A (Table API) and E (Cassandra API).\n\nTable API is the best solution for key-value model.","poster":"chaoxes","timestamp":"1607249760.0"},{"upvote_count":"1","comment_id":"225896","timestamp":"1606142640.0","poster":"ViniJsr","content":"Azure CosmosDB table API is a key-value storage hosted in the cloud. It's a part of Azure Cosmos DB, that is Microsoft's multi-model database. It's a globally distributed, low latency, high throughput solution with client SDKs available for . NET, Java, Python, and Node. So Answer will be A and E"},{"timestamp":"1606133340.0","comment_id":"225779","poster":"syu31svc","upvote_count":"1","content":"It's A & E\nCassandra is wide-table and Table is key"},{"content":"it should be A and E","comment_id":"225214","upvote_count":"1","timestamp":"1606077540.0","poster":"Aditya167"},{"content":"Answer is Table APi and cassandra API","upvote_count":"2","timestamp":"1605790260.0","poster":"UKiran","comment_id":"222775"},{"comments":[{"poster":"lingjun","timestamp":"1605278640.0","upvote_count":"1","comment_id":"218544","content":"https://www.mongodb.com/key-value-database"},{"timestamp":"1605278820.0","upvote_count":"1","comment_id":"218549","content":"ignore it. I would go for Table","poster":"lingjun"}],"content":"I think it should be MongoDB, since it uses key-value (which is the json format in document). but not storing the key-value pairs(then should be Table API)","timestamp":"1604918040.0","upvote_count":"1","comment_id":"215854","poster":"lingjun"},{"timestamp":"1603989180.0","poster":"narun","comment_id":"208711","upvote_count":"4","content":"It should be D and E.\nBefore Azure Cosmos DB existed, Redis or the Table API might have been a good fit for this kind of data; however, Core (SQL) API is now the better choice, as it offers a richer query experience, with improved indexing over the Table API."},{"content":"The answer should be Table API for key value and Cassandra API for wide column","comment_id":"186790","upvote_count":"2","timestamp":"1601016000.0","poster":"nacondras"},{"upvote_count":"2","comment_id":"184410","poster":"panya","content":"It should be A & E","timestamp":"1600778760.0"},{"timestamp":"1600777620.0","upvote_count":"1","content":"Key value pair - SQL API. \n\"Before Azure Cosmos DB existed, Redis or the Table API might have been a good fit for this kind of data; however, Core (SQL) API is now the better choice, as it offers a richer query experience, with improved indexing over the Table API.\"","comment_id":"184393","poster":"hart232"},{"poster":"Sree5","upvote_count":"3","timestamp":"1599234600.0","content":"Key-Value -> Table API,\nWide-Column -> Cassandra API","comment_id":"173497"},{"upvote_count":"1","content":"The correct answer is B and E as seen in other websites","timestamp":"1599117000.0","poster":"EnricVives","comment_id":"172487"},{"comment_id":"171683","upvote_count":"5","content":"really bad and vague question. But i would vote for Table API as it typically supports key Value store. I know others would argue that the Table API should only be used when the data is stored in Azure Table Storage, but this condition is unclear in the question. MangoDB is not a typical key-value store, it should only be consider if the question mentioned that it is a legacy system built in MangoDB. Very badly worded question for sure.","timestamp":"1599016200.0","poster":"EYIT"},{"upvote_count":"2","content":"A&E\nTable API is key value\nCassandra is wide column","poster":"lastname","comment_id":"141698","timestamp":"1595483040.0"},{"content":"key Value is Table where as Wide column is Casandra","comment_id":"136371","upvote_count":"2","timestamp":"1594894860.0","poster":"abhinavkj"},{"poster":"krisspark","comment_id":"133507","timestamp":"1594607340.0","upvote_count":"4","content":"seems lot of debate on this question.. Answer is A & E ( Table & Cassandra).. The same is endorsed in Other Website.. Why not Mongo - It's doument but not Key-value pair storage or not wide-column."},{"comment_id":"101432","upvote_count":"6","timestamp":"1591162620.0","poster":"Juturi","content":"B & E is correct as the application uses NoSQL database. We can't choose the table API as the requirement not stated that the application uses blob storage (blob storage is not a database)."},{"upvote_count":"3","comment_id":"99112","timestamp":"1590896520.0","content":"A and E https://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/2-identify-the-technology-options","poster":"TheCyanideLancer"},{"upvote_count":"3","comment_id":"85602","content":"Think the question is unclear - is this a legacy question or not? If it isn't legacy then there is 1 answer - D as guidance is to use Core SQL even for wide-column. Because it requires 2 answers then it must be legacy so B and E.","poster":"pawhit","timestamp":"1588926180.0"},{"content":"A & E as per https://academy.datastax.com/planet-cassandra/what-is-nosql","poster":"Siva_s","upvote_count":"5","comment_id":"84691","timestamp":"1588792200.0"},{"comments":[{"comments":[{"poster":"Manue","comment_id":"76429","upvote_count":"4","timestamp":"1587307560.0","content":"What is the requirement to use MongoDB here? It is not neither key-value nor wide column."}],"timestamp":"1584618480.0","content":"Actually, after re-reading the question... it is B & E","comment_id":"65968","upvote_count":"2","poster":"Huepig"},{"comment_id":"80713","poster":"wyxh","content":"D & E , sql api is key value, for any new deployment, use SQL API , wide column is cassandra","timestamp":"1588061760.0","upvote_count":"4","comments":[{"poster":"ade2020","content":"Does the data consist of simple key-value pairs?\n\nBefore Azure Cosmos DB existed, Redis or the Table API might have been a good fit for this kind of data; however, Core (SQL) API is now the better choice, as it offers a richer query experience, with improved indexing over the Table API.\nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/3-analyze-the-decision-criteria","timestamp":"1589348640.0","comment_id":"88112","upvote_count":"2"}]}],"content":"It’s D & E.\n\nMicrosoft advises that Table API should only be used when migrating from Tables to CosmosDB. For any new deployment, use SQL API - this is the same when selecting MongoDB API. You would only select MongoDB API if a legacy system needs to interface to the CosmosDB database.","timestamp":"1584618360.0","comment_id":"65967","poster":"Huepig","upvote_count":"5"},{"timestamp":"1584606540.0","comment_id":"65912","upvote_count":"5","content":"A and E, check the answer to question 17 later","poster":"fargoca"},{"poster":"goodzilla","timestamp":"1583428200.0","upvote_count":"1","content":"With just two options to select\nWhy not A & B ?\n\n(option B) As Mongo DB is a cross platform document-oriented DB as Cosmos with json-like documents with schema seems to me more applicable than Cassandra (high-availability)","comment_id":"59480"},{"content":"As far as I understand, the Key Value interface should be Azure table, unless there is a legacy requirement for something else.","upvote_count":"12","poster":"Frederi","comment_id":"22756","timestamp":"1574182620.0"},{"content":"Should B not be Azure Cosmos DB?","upvote_count":"1","timestamp":"1568970120.0","comment_id":"11930","poster":"JCM","comments":[{"content":"No, it is asking for an API. Cosmos is not an API... If a NoSQL solution uses Mongo or Cassandra the idea is that it's easy to migrate it onto Cosmos later.","upvote_count":"3","poster":"Neuron","comment_id":"13628","timestamp":"1570044540.0"}]}],"timestamp":"2019-09-20 11:02:00","exam_id":65,"question_text":"You are the data engineer for your company. An application uses a NoSQL database to store data. The database uses the key-value and wide-column NoSQL database type.\nDevelopers need to access data in the database using an API.\nYou need to determine which API to use for the database model and type.\nWhich two APIs should you use? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","unix_timestamp":1568970120,"answer_ET":"BE","answer_images":[]},{"id":"KOkMsA96tIDFeRkCwaDk","unix_timestamp":1594035540,"question_text":"A company is designing a hybrid solution to synchronize data and on-premises Microsoft SQL Server database to Azure SQL Database.\nYou must perform an assessment of databases to determine whether data will move without compatibility issues. You need to perform the assessment.\nWhich tool should you use?","answer_description":"The Data Migration Assistant (DMA) helps you upgrade to a modern data platform by detecting compatibility issues that can impact database functionality in your new version of SQL Server or Azure SQL Database. DMA recommends performance and reliability improvements for your target environment and allows you to move your schema, data, and uncontained objects from your source server to your target server.\nReferences:\nhttps://docs.microsoft.com/en-us/sql/dma/dma-overview","topic":"1","choices":{"C":"SQL Vulnerability Assessment (VA)","B":"Microsoft Assessment and Planning Toolkit","A":"SQL Server Migration Assistant (SSMA)","D":"Azure SQL Data Sync","E":"Data Migration Assistant (DMA)"},"exam_id":65,"answers_community":[],"answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/24884-exam-dp-200-topic-1-question-12-discussion/","isMC":true,"timestamp":"2020-07-06 13:39:00","question_images":[],"answer":"E","discussion":[{"comments":[{"upvote_count":"1","timestamp":"1621102380.0","comment_id":"358101","content":"keyword is >> Assessment of db","poster":"memo43"}],"timestamp":"1601529660.0","upvote_count":"24","comment_id":"190734","content":"Correct answer is DMA. Data Migration Assistant is a client-side tool that you can install on a Windows-compatible workstation or server. It has two major functions in the migration of the social database to the Azure SQL Database platform in this module. First, it assesses your existing database and identifies any incompatibilities between that database and Azure SQL Database. It then generates a report of the things you need to fix before you can migrate. As you make changes, you can rerun Data Migration Assistant to generate an updated report of changes that you need to make. This capability helps you to not only track your progress, but also catch any new issues that might have been introduced during your coding phase. \nRefrence: https://docs.microsoft.com/en-us/learn/modules/migrate-sql-server-relational-data/3-migration-overview","poster":"Debjit"},{"timestamp":"1615916640.0","upvote_count":"3","comment_id":"312571","content":"DMA is the correct answer. It is used for compatibility check prior to migration.","poster":"deepz8"},{"upvote_count":"1","timestamp":"1613569860.0","comment_id":"292580","poster":"JESUSBB","content":"Answer E Data Migration Assistant."},{"upvote_count":"2","poster":"Needium","content":"The correct answer is definitely the chosen answer E. The question is about assessement not the actual migration and/or synchronization. Azure Data Sync (E) addresses the actual migration and synchronization; however, the question is about assessment before migration","comment_id":"256376","timestamp":"1609436700.0"},{"timestamp":"1606133760.0","content":"From link https://docs.microsoft.com/en-us/sql/dma/dma-overview?view=sql-server-ver15:\nAssess on-premises SQL Server instance(s) migrating to Azure SQL database(s). The assessment workflow helps you to detect the following issues that can affect Azure SQL database migration and provides detailed guidance on how to resolve them.","upvote_count":"2","poster":"syu31svc","comment_id":"225786"},{"comment_id":"219297","poster":"Egocentric","timestamp":"1605380880.0","content":"\"perform an assessment\" is the keyword here not sync . Data Migration Assistant is the correct one","upvote_count":"2"},{"comment_id":"136743","upvote_count":"2","comments":[{"poster":"r8d1","upvote_count":"6","comment_id":"154450","timestamp":"1597058700.0","content":"i think the question is about first migrating data, so we need to address this issue first. The sync comes later."}],"poster":"abhinavkj","timestamp":"1594932780.0","content":"I also feel it should be Azure Data Sync"},{"upvote_count":"2","content":"Should not be D: Azure Data Sync as it is said to synchronize data ? Data Migration Assistant should be used to migrate data from on-premise to Azure.","timestamp":"1594035540.0","comment_id":"127752","poster":"SebK","comments":[{"upvote_count":"12","timestamp":"1594137360.0","poster":"cutebird","content":"It is mentioned that \"perform an assessment of databases to determine whether data will move without compatibility issues\".So E is correct\nRef:https://docs.microsoft.com/en-us/sql/dma/dma-overview?view=sql-server-ver15","comment_id":"129102"}]}],"answer_ET":"E","question_id":4},{"id":"IOzT0R1m1y8Wc7FeyljQ","answer_description":"The Set-AzStorageAccountManagementPolicy cmdlet creates or modifies the management policy of an Azure Storage account.\nExample: Create or update the management policy of a Storage account with ManagementPolicy rule objects.\n\nAction -BaseBlobAction Delete -daysAfterModificationGreaterThan 100\nPS C:\\>$action1 = Add-AzStorageAccountManagementPolicyAction -InputObject $action1 -BaseBlobAction TierToArchive -daysAfterModificationGreaterThan 50\nPS C:\\>$action1 = Add-AzStorageAccountManagementPolicyAction -InputObject $action1 -BaseBlobAction TierToCool -daysAfterModificationGreaterThan 30\nPS C:\\>$action1 = Add-AzStorageAccountManagementPolicyAction -InputObject $action1 -SnapshotAction Delete -daysAfterCreationGreaterThan 100\nPS C:\\>$filter1 = New-AzStorageAccountManagementPolicyFilter -PrefixMatch ab,cd\nPS C:\\>$rule1 = New-AzStorageAccountManagementPolicyRule -Name Test -Action $action1 -Filter $filter1\nPS C:\\>$action2 = Add-AzStorageAccountManagementPolicyAction -BaseBlobAction Delete -daysAfterModificationGreaterThan 100\nPS C:\\>$filter2 = New-AzStorageAccountManagementPolicyFilter\nReferences:\nhttps://docs.microsoft.com/en-us/powershell/module/az.storage/set-azstorageaccountmanagementpolicy","isMC":false,"discussion":[{"content":"I would rather choose \"delete\" in the last section","poster":"Slava_bcd81","timestamp":"1618212120.0","upvote_count":"6","comment_id":"333766","comments":[{"upvote_count":"12","poster":"memo43","comment_id":"358107","timestamp":"1621102620.0","content":"Remember: Data for the current year must be available for weekly reports. \nif you delete after 90 days you cant prepare weekly reports! so you must not delete them before 10 years.\nANSWER IS CORRECT"}]},{"timestamp":"1624426560.0","poster":"hello_there_","comment_id":"388522","content":"I think \"data for the current year\" refers to de data on the baseBlob, which is why the json part for the baseBlob is archived after 356 days. It explicitly says that snapshot data should be kept for 90 days, whitch is analogous to saying it can be deleted after 90.","upvote_count":"1"},{"poster":"Amy007","comment_id":"355670","content":"Data should be kept for current year for reporting. So Greater than 90 cannot be deleted.","timestamp":"1620839340.0","upvote_count":"2"},{"upvote_count":"2","comment_id":"346804","content":"I agree. The daily snapshots are not needed after the 90-day period, so should be deleted instead of being moved to cool tier.","poster":"Internet_User","timestamp":"1619863440.0"}],"exam_id":65,"question_images":["https://www.examtopics.com/assets/media/exam-media/03872/0001800001.png"],"unix_timestamp":1618212120,"answers_community":[],"answer_ET":"","topic":"1","answer":"","question_id":5,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03872/0001900001.png","https://www.examtopics.com/assets/media/exam-media/03872/0002000001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/49946-exam-dp-200-topic-1-question-13-discussion/","question_text":"DRAG DROP -\nYou manage a financial computation data analysis process. Microsoft Azure virtual machines (VMs) run the process in daily jobs, and store the results in virtual hard drives (VHDs.)\nThe VMs product results using data from the previous day and store the results in a snapshot of the VHD. When a new month begins, a process creates a new\nVHD.\nYou must implement the following data retention requirements:\n✑ Daily results must be kept for 90 days\n✑ Data for the current year must be available for weekly reports\n✑ Data from the previous 10 years must be stored for auditing purposes\n✑ Data required for an audit must be produced within 10 days of a request.\nYou need to enforce the data retention requirements while minimizing cost.\nHow should you configure the lifecycle policy? To answer, drag the appropriate JSON segments to the correct locations. Each JSON segment may be used once, more than once, or not at all. You may need to drag the split bat between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","timestamp":"2021-04-12 09:22:00"}],"exam":{"name":"DP-200","isBeta":false,"isImplemented":true,"lastUpdated":"12 Apr 2025","id":65,"provider":"Microsoft","isMCOnly":false,"numberOfQuestions":228},"currentPage":1},"__N_SSP":true}