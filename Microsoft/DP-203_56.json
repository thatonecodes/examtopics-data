{"pageProps":{"questions":[{"id":"SxgQ0hreJfuEGLpn5SUk","timestamp":"2022-04-25 18:28:00","answer_ET":"CD","answer_description":"","topic":"3","discussion":[{"comment_id":"718051","upvote_count":"24","poster":"kl8585","timestamp":"1668438000.0","content":"Selected Answer: CD\nPhrased different, the question for me says: if you create \"Folder3\" inside Folder2, you should be able to read files created in Folder3.\n\nThis means that you for sure need Executive and Read premissions to Folder2 (Executive to traverse child folder, read to read the files).\n\nNow, starting from the least privilege, suppose you give \"Access\" permission both for read and execute. In this case, you can't read files created in Folder3. This is a requirement (\"child items that are created in Folder2\"), so you need Default Read access.\n\nYou don't need Default Execute, otherwise you would have access to a Folder created in Folder3 (say Folder 4) and this is not required so for the least privilege you must give Access Execute and not Defualt Execute.","comments":[{"comment_id":"1235580","content":"Given Answers (D&F) are correct....Reason is basic difference between Access and Default ACLs\nAccess ACL: is for existing items.\nDefault ACL: is template ACL for new Items to be created.\nHere question says traverse and read child items that created in folder2. So Access ACLs will fail to provide access to new files so we need to add Default ACL's for new files","upvote_count":"3","poster":"Sr18","timestamp":"1719086880.0"},{"timestamp":"1673621460.0","content":"Requirement 1 says Traverse child items that are created in Folder2. Means that you need to be able to travers the subFolders under Folder2. So Defaut:Execute is a required permission.","upvote_count":"3","comment_id":"774595","poster":"yogiazaad"}]},{"upvote_count":"15","comment_id":"705309","comments":[{"content":"cannot agree more, and do not need to over think :)","upvote_count":"1","poster":"Lewiasskick","timestamp":"1706308860.0","comment_id":"1132940"}],"poster":"bokLuci","content":"Selected Answer: CD\nC - You need to traverse the FOlder2 only and no potential children folders - Principals of least privelage.\nD- You need to pass on the READ access to the files in Folder2. Default ACLs are not passed to files but we are not setting the permission on a file level, we are setting it on Folder2.","timestamp":"1666853640.0"},{"poster":"KauK","content":"Selected Answer: DF\nAnswer is DF, recheck the documentation.","timestamp":"1734199980.0","comment_id":"1326580","upvote_count":"2"},{"comment_id":"1242129","content":"Selected Answer: DF\nThe link https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control explains that \"Default ACLs are templates of ACLs associated with a directory that determine the access ACLs for any child items that are created under that directory. Files do not have default ACLs.\"\nNow the requirement here is to \n1) Traverse the child items that will be created within folder2 \n2) Read the files that will be created within folder2. \nThe question states that the child items(both folders and files) within the folder2 will get created i.e, IT IS NOT YET CREATED and WILL GET CREATED IN THE FUTURE which means the access has to be at the root directory level which is folder2 here. And as per the Microsoft documentation, only Default ACLs will work because Access ACLs control access to an object(file or directory). Choosing Access ACL would mean each time a new child item getting created with folder2, the Access ACL has to explicitly set for that child item at that time.","upvote_count":"2","timestamp":"1720101960.0","poster":"learnwell"},{"upvote_count":"2","timestamp":"1720005300.0","poster":"Souvik_79","content":"Same problem. Everyone has different answers. No one knows which answer is correct. Worst part is even Gemini disagrees with ChatGPT :(","comment_id":"1241363"},{"timestamp":"1714102080.0","content":"A and F","upvote_count":"1","comment_id":"1202373","poster":"Dusica"},{"upvote_count":"1","poster":"Alongi","timestamp":"1713384360.0","comment_id":"1197445","content":"Selected Answer: DF\nAccess ACLs control access to an object. Files and directories both have access ACLs.\nDefault ACLs are templates of ACLs associated with a directory that determine the access\nACLs for any child items that are created under that directory. Files do not have default ACLs."},{"upvote_count":"2","comment_id":"1158248","timestamp":"1708822260.0","content":"Selected Answer: AF\nTraverse child items that are created in Folder2 --> Default Execute\nRead files that are created in Folder2 --> Access Read","poster":"Gman1986"},{"poster":"MarkJoh","comment_id":"1113148","timestamp":"1704317940.0","content":"Selected Answer: AF\nI'm going with AF and here is why.\nThe requirement \"Traverse child items that are created in Folder2\" -> This requires default execute so that if any child folders under folder2 get created, the user can list those folders and files.\nNow, because of principle of least privilege, it does NOT say that if a file is created under a subfolder (like folder2/folder2/file1.json) that they need access to it.\nSo, it should be Access Read on folder2 so that the users only get read access to the files in folder2 and not in /folder2/folder3/*.json, for instance.","upvote_count":"7"},{"upvote_count":"1","poster":"[Removed]","timestamp":"1704005340.0","comment_id":"1110326","content":"Selected Answer: DF\nDefault Execute and Default Read as you don´t know in advance the files/folder to be created, and you need to access to all of them."},{"comment_id":"994720","timestamp":"1693453620.0","poster":"kkk5566","upvote_count":"2","content":"\"Default - Read\" and \"Default - Execute\""},{"content":"Selected Answer: CD\nTraverse require access execute, file reads need default read","upvote_count":"2","timestamp":"1692003600.0","comment_id":"980631","poster":"[Removed]"},{"comment_id":"944402","upvote_count":"4","content":"Selected Answer: DF\nDefault Execute is mandatory to traverse child items through cascade.. Default Read by process of elimination","timestamp":"1688628300.0","poster":"[Removed]"},{"timestamp":"1687768860.0","comment_id":"934216","content":"Selected Answer: AF\n✑ Traverse child items that are created in Folder2. => DEFAULT EXECUTE\n✑ Read files that are created in Folder2. => ACCESS READ (that was already given).","upvote_count":"4","poster":"auwia"},{"poster":"esaade","upvote_count":"6","timestamp":"1678700940.0","content":"Selected Answer: DF\nBased on the permissions table provided, the ServicePrincipal1 has \"Access - Execute\" permission on container1, \"Access - Execute\" permission on Folder1, and \"Access - Read\" permission on Folder2. To allow ServicePrincipal1 to traverse child items that are created in Folder2 and read files created in Folder2, you should grant the \"Default - Read\" and \"Default - Execute\" permissions on Folder2. The \"Default - Read\" permission allows ServicePrincipal1 to read files created in Folder2, and the \"Default - Execute\" permission allows ServicePrincipal1 to traverse child items that are created in Folder2.\n\nTherefore, the correct answer is:\nD. Default - Read\nF. Default - Execute","comment_id":"837815"},{"comment_id":"774592","poster":"yogiazaad","upvote_count":"4","content":"Traverse child items that are created in Folder2.\nThis needs Default:Execute Because user needs to traverse any child Items(Sub Folders) created under under Folder2. \nRead files that are created in Folder2.\nSince the The Access:read ACL is already set on Folder2.Any files that are created under Folder2 can be access by User. But to see (or list) the items/files under Folder2 we need Access:Execute .\nSO the answer is Access: Execute and Default: Execute","timestamp":"1673621340.0"},{"timestamp":"1670110320.0","content":"Selected Answer: DF\nDefault Read and Execute are required. The reason is as below.\n\nIn the POSIX-style model that's used by Data Lake Storage Gen2, permissions for an item are stored on the item itself. In other words, permissions for an item cannot be inherited from the parent items if the permissions are set after the child item has already been created. Permissions are only inherited if default permissions have been set on the parent items before the child items have been created.\nReference: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control","comment_id":"734743","poster":"AzureJobsTillRetire","upvote_count":"6"},{"content":"Selected Answer: DF\nso the answer is correct","timestamp":"1660315080.0","comment_id":"645945","poster":"Deeksha1234","upvote_count":"4"},{"poster":"Deeksha1234","content":"I think the given answer is correct. Since we should be able to traverse and read the child items from the folder 2 .\n\nFrom one of the DP 203 Microsoft lab exercise -\nAccess ACLs control access to an object. Files and directories both have access ACLs.\n\nDefault ACLs are templates of ACLs associated with a directory that determine the access ACLs for any child items that are created under that directory. Files do not have default ACLs.","timestamp":"1660315020.0","upvote_count":"4","comment_id":"645944"},{"comment_id":"621446","poster":"Davico93","content":"Selected Answer: AF\nDefault is not related to files so, if we want to read files, we need ACCESS - READ","upvote_count":"4","timestamp":"1656046920.0"},{"content":"Please make a note how the sentence is framed \"Traverse child items that are created in Folder2\". Access ACL doesn't propagate the permissions to child items but default ACL does. So it is obvious that new files or folders can be created in Folder2 and that requires default ACL. So according to me default execute and default read on folder2 should be the correct answer","comment_id":"613690","poster":"Aditya0891","comments":[{"timestamp":"1655105580.0","comment_id":"615695","upvote_count":"4","content":"Please ignore this. It's not correct. Examtopics should provide a delete option here.","poster":"Aditya0891"}],"upvote_count":"1","timestamp":"1654761840.0"},{"upvote_count":"4","comments":[{"poster":"virendrapsingh","timestamp":"1654188240.0","content":"Agreed with your comment on least privilege as it is mentioned specifically in the question.\nChoices A & F should be the answer.","upvote_count":"5","comment_id":"610699"},{"content":"sdokmak not sure but it's not mentioned that there are only files inside folder2 and in the next line it specifically mentioned that to read files inside folder 2. I think the answers are correct as per requirement. Please correct me if I'm wrong","comment_id":"613678","timestamp":"1654760580.0","poster":"Aditya0891","upvote_count":"1"}],"content":"Following principal of least privilege, isn't Access Execute and Default Read enough? You only need to traverse the files in Folder2, not the folders within Folder2 (even though there aren't any)","poster":"sdokmak","comment_id":"607787","timestamp":"1653595320.0"},{"upvote_count":"1","timestamp":"1653315060.0","content":"Selected Answer: DF\nCorrect","comment_id":"606143","poster":"MadEgg"},{"poster":"juanlu46","comment_id":"591835","content":"Selected Answer: DF\nIs correct!","upvote_count":"3","timestamp":"1650904080.0"}],"url":"https://www.examtopics.com/discussions/microsoft/view/74501-exam-dp-203-topic-3-question-25-discussion/","isMC":true,"question_id":276,"unix_timestamp":1650904080,"answer":"CD","choices":{"C":"Access ג€\" Execute","F":"Default ג€\" Execute","E":"Default ג€\" Write","D":"Default ג€\" Read","B":"Access ג€\" Write","A":"Access ג€\" Read"},"question_text":"You have an Azure subscription linked to an Azure Active Directory (Azure AD) tenant that contains a service principal named ServicePrincipal1. The subscription contains an Azure Data Lake Storage account named adls1. Adls1 contains a folder named Folder2 that has a URI of https://adls1.dfs.core.windows.net/ container1/Folder1/Folder2/.\nServicePrincipal1 has the access control list (ACL) permissions shown in the following table.\n//IMG//\n\nYou need to ensure that ServicePrincipal1 can perform the following actions:\n✑ Traverse child items that are created in Folder2.\n✑ Read files that are created in Folder2.\nThe solution must use the principle of least privilege.\nWhich two permissions should you grant to ServicePrincipal1 for Folder2? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.","exam_id":67,"answers_community":["CD (47%)","DF (34%)","AF (19%)"],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0031900003.png"],"answer_images":[]},{"id":"UXTzdQI5tjpyPItqwQEv","unix_timestamp":1640704560,"discussion":[{"timestamp":"1640704560.0","comment_id":"511246","content":"Correct","upvote_count":"22","poster":"Skeinofi"},{"poster":"Amsterliese","content":"\"SQL Database and Azure Synapse Analytics support Azure Active Directory identities as contained database users\"\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/contained-database-users-making-your-database-portable?view=sql-server-ver15#contained-database-user-model","comment_id":"585036","upvote_count":"8","timestamp":"1649831160.0"},{"upvote_count":"1","content":"Azure AD will enforce MFA\nContainer Users can map to AD and enforce MFA through that","poster":"renan_ineu","comment_id":"1285171","timestamp":"1726572600.0"},{"upvote_count":"1","comment_id":"994722","poster":"kkk5566","content":"Correct","timestamp":"1693453860.0"},{"timestamp":"1687290840.0","upvote_count":"2","poster":"JG1984","comment_id":"928759","content":"Azure Synapse Analytics supports two types of database-level authentication:\n\nAzure Active Directory (Azure AD) authentication: This uses your Azure AD identity to authenticate to Synapse SQL. This is the recommended authentication method, as it provides a single sign-on experience and allows you to manage permissions using Azure AD groups.\nSQL Server authentication: This uses a traditional SQL Server username and password to authenticate to Synapse SQL. This authentication method is less secure than Azure AD authentication, but it may be necessary if you are using legacy applications that do not support Azure AD."},{"upvote_count":"4","poster":"Deeksha1234","timestamp":"1659864600.0","comment_id":"643656","content":"answer is correct"},{"comment_id":"531980","upvote_count":"4","timestamp":"1643099940.0","comments":[{"poster":"PallaviPatel","comments":[{"upvote_count":"3","content":"correct","comment_id":"539707","timestamp":"1643893320.0","poster":"dev2dev"}],"timestamp":"1643612220.0","content":"https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview this document says contained users are supported by synapse analytics, so this is correct answer.","comment_id":"536714","upvote_count":"18"}],"content":"B is wrong. Contained users not supported by synapse analytics. D is correct ('MS SQL Server logins')","poster":"dev2dev"}],"question_id":277,"isMC":false,"answer_ET":"","answers_community":[],"timestamp":"2021-12-28 16:16:00","url":"https://www.examtopics.com/discussions/microsoft/view/68731-exam-dp-203-topic-3-question-26-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0032200001.png"],"answer_description":"Box 1: Azure AD authentication -\nAzure AD authentication has the option to include MFA.\n\nBox 2: Contained database users -\nAzure AD authentication uses contained database users to authenticate identities at the database level.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-mfa-ssms-overview https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview","question_text":"HOTSPOT -\nYou have an Azure subscription that is linked to a hybrid Azure Active Directory (Azure AD) tenant. The subscription contains an Azure Synapse Analytics SQL pool named Pool1.\nYou need to recommend an authentication solution for Pool1. The solution must support multi-factor authentication (MFA) and database-level authentication.\nWhich authentication solution or solutions should you include in the recommendation? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","topic":"3","answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0032100001.png"],"exam_id":67},{"id":"Bv02te7T0aXPNFBNZwzk","timestamp":"2021-05-15 14:30:00","answer_ET":"","answer_description":"Step 1: Create an Azure Storage account that has a lifecycle policy\nTo automate common data management tasks, Microsoft created a solution based on Azure Data Factory. The service, Data Lifecycle Management, makes frequently accessed data available and archives or purges other data according to retention policies. Teams across the company use the service to reduce storage costs, improve app performance, and comply with data retention policies.\nStep 2: Create a Log Analytics workspace that has Data Retention set to 120 days.\nData Factory stores pipeline-run data for only 45 days. Use Azure Monitor if you want to keep that data for a longer time. With Monitor, you can route diagnostic logs for analysis to multiple different targets, such as a Storage Account: Save your diagnostic logs to a storage account for auditing or manual inspection. You can use the diagnostic settings to specify the retention time in days.\nStep 3: From Azure Portal, add a diagnostic setting.\nStep 4: Send the data to a log Analytics workspace,\nEvent Hub: A pipeline that transfers events from services to Azure Data Explorer.\nKeeping Azure Data Factory metrics and pipeline-run data.\nConfigure diagnostic settings and workspace.\nCreate or add diagnostic settings for your data factory.\n1. In the portal, go to Monitor. Select Settings > Diagnostic settings.\n2. Select the data factory for which you want to set a diagnostic setting.\n3. If no settings exist on the selected data factory, you're prompted to create a setting. Select Turn on diagnostics.\n4. Give your setting a name, select Send to Log Analytics, and then select a workspace from Log Analytics Workspace.\n5. Select Save.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor","topic":"3","discussion":[{"timestamp":"1623003480.0","poster":"Sunnyb","comment_id":"376274","comments":[{"comment_id":"535222","poster":"rainbowyu","upvote_count":"5","content":"Shouldn't it need to swap step 3 & 4?","timestamp":"1643436120.0"},{"upvote_count":"1","content":"seems correct to me","timestamp":"1659865080.0","comment_id":"643658","poster":"Deeksha1234"},{"content":"correct","comment_id":"994726","poster":"kkk5566","timestamp":"1693454340.0","upvote_count":"2"},{"poster":"Rajashekharc","content":"This is correct order, I have tried this on Azure portal.","timestamp":"1661229360.0","upvote_count":"5","comment_id":"650575"},{"upvote_count":"10","content":"https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/tutorial-resource-logs?source=recommendations\nAbove order is mentioned here.","comment_id":"707945","poster":"datapc","timestamp":"1667150760.0","comments":[{"upvote_count":"1","poster":"learnwell","comment_id":"1242140","content":"Thank you for the link, explains it very well.","timestamp":"1720102980.0"}]},{"poster":"matiandal","comment_id":"1065397","content":"why not the ET Answer? \n-- > the ET answer misses the step \"Add diagnostic setting-> PIpelineRuns option\n\n4 a Confirmation of Sunnyb's answer see the also the following link ( with screenshots )\n--> https://davidalzamendi.com/long-running-azure-data-factory-pipelines/","upvote_count":"1","timestamp":"1699430220.0"}],"upvote_count":"193","content":"Step 1: Create a Log Analytics workspace that has Data Retention set to 120 days.\nStep 2: From Azure Portal, add a diagnostic setting.\nStep 3: Select the PipelineRuns Category\nStep 4: Send the data to a Log Analytics workspace."},{"comments":[{"timestamp":"1627898880.0","comment_id":"418640","poster":"Armandoo","upvote_count":"1","content":"This is the correct answer"},{"timestamp":"1641412860.0","content":"Don't you have to select PipelineRuns Category while adding a diagnostic setting?","poster":"LiLy91","comment_id":"517777","upvote_count":"1"},{"poster":"BK10","comment_id":"546748","upvote_count":"2","content":"Video matches the steps above. Thanks for sharing.","timestamp":"1644789000.0"},{"comment_id":"605849","content":"I don't agree with you, why \"Send data to a Log analytics workspace\" is step2, but \"Create the Log Analytics workspace\" is step3 ? how to use Log analytics workspace if the Log analytics workspace hasn't been created?","poster":"Towin","upvote_count":"4","timestamp":"1653285360.0"},{"timestamp":"1670101200.0","content":"steps 2 & 3 must be swapped. you can't send data to log analytics workspace that isn't created yet","comment_id":"734688","upvote_count":"2","poster":"Igor85"},{"upvote_count":"4","comments":[{"upvote_count":"1","timestamp":"1688396700.0","content":"45 for Monitoring not Azure Log Analytics","comment_id":"941966","poster":"klayytech"}],"content":"Read the text surrounding the video; it is for Azure Monitoring which provides only base-level services; of only 45 days. So the video is incorrect, for the qestion asked.","poster":"KashRaynardMorse","comment_id":"598576","timestamp":"1652017800.0"},{"content":"WRONG.\nIn order to add the \"PipelineRuns\" setting you have to follow the steps below:\nSTEPS: ADF -> \"under monitoring group\" select Diagnostic Settings -> Add diagnostic Setting -> Select PipelineRuns\n\nFor confirmation diy or see the provided link -)\nR: https://davidalzamendi.com/long-running-azure-data-factory-pipelines/\n\nCheers !","comment_id":"1065407","timestamp":"1699430640.0","poster":"matiandal","upvote_count":"1"}],"poster":"herculian_effort","upvote_count":"39","content":"step 1. From Azure Portal, add a diagnostic setting.\nstep 2. Send data to a Log analytics workspace.\nstep 3. Create a Log Analytics workspace that has Data Retention set to 120 days.\nstep 4. Select the PipelineRuns Category.\n\nThe video in the below link walks you through the process step by step, start watching at 2min 30sec mark\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#keeping-azure-data-factory-metrics-and-pipeline-run-data","comment_id":"404989","timestamp":"1626130440.0"},{"poster":"be8a152","content":"Sunnyb is correct","upvote_count":"1","timestamp":"1707259140.0","comment_id":"1142806"},{"poster":"Sriramiyer92","comment_id":"637481","upvote_count":"2","timestamp":"1658846760.0","content":"Can see multiple answers that are correct in the discussion!\nAlso note the question states : \"More than one order of answer choices is correct\""},{"timestamp":"1655632800.0","comment_id":"618618","upvote_count":"1","content":"Output is either SA, LA or Eventhub\nRetention is configured during setting up the diag on any Azure resource , so take out option 1 which says configure SA retention.\nJust stick to LA solution and include all the points related to it.","poster":"NamitSehgal"},{"upvote_count":"12","comment_id":"431032","content":"I am not very familiar with this topic, but follow the link below, we can know With Monitor, you can route diagnostic logs for analysis to multiple different targets: Storage account, Event Hub and Log Analytics. It also needs to query the data by use Kusto query language, so we can know we should use Log Analytics for this scenario. With this in mind, we can exclude anything related with storage account and Event Hub. Then the question talks about Pipeline runs log, so we can also exclude the Trigger run log one. Then there are 4 options left there as listed in the solution raised by @Sunnyb.\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#keeping-azure-data-factory-metrics-and-pipeline-run-data","poster":"[Removed]","timestamp":"1629845460.0"},{"comment_id":"426351","poster":"Amalbenrebai","upvote_count":"9","content":"in this case we will not use a storage Account to save the diagnostic logs to a storage account, but we will send them to Log Analytics:\n1: Create a Log Analytics workspace that has Data Retention set to 120 days.\n2: From Azure Portal, add a diagnostic setting.\n3: Select the PipelineRuns Category\n4: Send the data to a Log Analytics workspace","timestamp":"1629206880.0"},{"comments":[{"comment_id":"421158","timestamp":"1628333340.0","upvote_count":"2","poster":"mss1","comments":[{"comment_id":"445265","poster":"Marcus1612","upvote_count":"1","content":"When you create diagnostic, you have to select \"Log Analytics\" as destination target. Log Analytics Workspace has it own Data Retention Properties under General/Usage and Estimated Cost/Data Retention. So the good answer is:Step 1: Create a Log Analytics workspace that has Data Retention set to 120 days.\nStep 2: From Azure Portal, add a diagnostic setting.\nStep 3: Select the PipelineRuns Category\nStep 4: Send the data to a Log Analytics workspace.","timestamp":"1631716260.0"}],"content":"To complete my answer. I also agree with \"Sunnyb\". There are more solutions to this question."}],"poster":"mss1","upvote_count":"2","content":"If you create diagnostics from the Datafactory you wil notice that you can only set the retentiondays when you select a storage account for the PipelineRuns. So you need a storage account first. You do not have an option in the selection to create a diagnostic from the datafactory and thus the option \"select the pipelineruns\" is not an option. I agree with the current selection.","comment_id":"420125","timestamp":"1628149020.0"},{"content":"According to the linked article, it's: first Storage Account, then Event Hub, and finally Log Analytics.\nSo I would say:\n1- Create an Azure Storage Account with a lifecycle policy\n2- Stream to an Azure Event Hub\n3- Create a Log Analytics workspace that has a Data Retention set to 120 days\n4- Send the data to a Log Analytics Workspace\nSource: https://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#keeping-azure-data-factory-metrics-and-pipeline-run-data","timestamp":"1624604880.0","upvote_count":"4","poster":"mric","comment_id":"390201"},{"upvote_count":"2","timestamp":"1622142840.0","poster":"det_wizard","comment_id":"368258","content":"Take off the storage account and After add diagnostic setting it would be select pipelineruns then send to log analytics"},{"upvote_count":"1","poster":"teofz","timestamp":"1621081800.0","comment_id":"357834","content":"regarding the storage account, what is it for?!","comments":[{"timestamp":"1621112340.0","poster":"sagga","content":"I don't know if you need to, see this discussion: https://www.examtopics.com/discussions/microsoft/view/49811-exam-dp-200-topic-3-question-19-discussion/","upvote_count":"2","comment_id":"358182"},{"poster":"Amsterliese","timestamp":"1649831820.0","content":"In this case, not needed (imo). MS advises to store log data in a storage account (if needed) since Data Factory only retains it for 45 days. However, in this case you don't have to store it longer than 2 years and you want to use Kusto, so Log Analytics makes more sense.","upvote_count":"1","comment_id":"585041"}]}],"url":"https://www.examtopics.com/discussions/microsoft/view/52742-exam-dp-203-topic-3-question-27-discussion/","isMC":false,"question_id":278,"unix_timestamp":1621081800,"answer":"","question_text":"DRAG DROP -\nYou have an Azure data factory.\nYou need to ensure that pipeline-run data is retained for 120 days. The solution must ensure that you can query the data by using the Kusto query language.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\nSelect and Place:\n//IMG//","exam_id":67,"answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0032300001.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0032400001.png"]},{"id":"SDXf8rC3omIQKVmRrfsA","exam_id":67,"answer_description":"","unix_timestamp":1623875160,"question_images":[],"timestamp":"2021-06-16 22:26:00","question_id":279,"answer":"B","question_text":"You have an Azure Synapse Analytics dedicated SQL pool.\nYou need to ensure that data in the pool is encrypted at rest. The solution must NOT require modifying applications that query the data.\nWhat should you do?","discussion":[{"content":"Correct!","timestamp":"1639693560.0","poster":"damaldon","comment_id":"383693","upvote_count":"39"},{"content":"Selected Answer: B\nB is correct","timestamp":"1721015400.0","poster":"jsav1","comment_id":"1123092","upvote_count":"1"},{"content":"Selected Answer: B\ncorrect","upvote_count":"1","timestamp":"1709186460.0","comment_id":"994729","poster":"kkk5566"},{"upvote_count":"1","timestamp":"1703806260.0","comment_id":"937152","poster":"vctrhugo","content":"Selected Answer: B\nTransparent Data Encryption (TDE) is a feature provided by Azure SQL Database and Azure Synapse Analytics that encrypts the database files at rest. It performs real-time I/O encryption and decryption of the database files, ensuring that the data is encrypted on disk. TDE operates transparently and does not require any changes to the application code or queries.\n\nBy enabling TDE for the dedicated SQL pool in Azure Synapse Analytics, you can achieve encryption at rest for the data stored in the pool without impacting the applications that access the data."},{"timestamp":"1675770120.0","upvote_count":"2","poster":"Deeksha1234","content":"Selected Answer: B\nB is correct","comment_id":"643659"},{"upvote_count":"1","content":"B is right, however using CMK configed at workspace level to achieve double encryption is also right. \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/workspaces-encryption","timestamp":"1667991540.0","comment_id":"598951","comments":[{"content":"you can only enable double encryption when you are creating a new workspace.","upvote_count":"2","timestamp":"1686283980.0","comment_id":"739837","poster":"bigw"}],"poster":"youlitai003"},{"timestamp":"1660829820.0","content":"Selected Answer: B\nTDE is used for encrypting data at rest.","poster":"djblue","upvote_count":"3","comment_id":"550314"}],"url":"https://www.examtopics.com/discussions/microsoft/view/55467-exam-dp-203-topic-3-question-28-discussion/","isMC":true,"answer_ET":"B","choices":{"D":"Create an Azure key vault in the Azure subscription grant access to the pool.","C":"Use a customer-managed key to enable double encryption for the Azure Synapse workspace.","A":"Enable encryption at rest for the Azure Data Lake Storage Gen2 account.","B":"Enable Transparent Data Encryption (TDE) for the pool."},"topic":"3","answers_community":["B (100%)"],"answer_images":[]},{"id":"YYA6PY09TraWwa9nBaHE","answers_community":[],"discussion":[{"upvote_count":"13","timestamp":"1678166280.0","comments":[{"timestamp":"1707551280.0","comments":[{"comments":[{"comment_id":"1159970","content":"you don't need to read a file to be able to append data to it","upvote_count":"2","timestamp":"1724687460.0","poster":"mav2000"}],"upvote_count":"5","timestamp":"1709238900.0","content":"In the above link, the use case is given for appending to Data.txt file, then the answers would be \n-Execute\n-Execute\n-Read and Write","comment_id":"993412","poster":"AlviraTony"}],"content":"Supported by the following two references:\n\nwithout additional permissions: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\nwith additional permissions such as storage blob data reader: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model#permissions-table-combining-azure-rbac-abac-and-acls","poster":"Matt2000","upvote_count":"2","comment_id":"977320"}],"comment_id":"661912","poster":"anks84","content":"-Execute\n-Execute\n-Write"},{"upvote_count":"6","poster":"dom271219","content":"Correct : Execute to traverse the folders and Write to append the file","timestamp":"1678525080.0","comment_id":"665889"},{"content":"Exe; Exe; Write","comment_id":"1191015","poster":"Alongi","upvote_count":"1","timestamp":"1728312060.0"},{"upvote_count":"2","content":"https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\nmust be rw- for the file","poster":"Lewiasskick","comment_id":"1119080","timestamp":"1720641420.0"},{"comment_id":"1002780","poster":"[Removed]","upvote_count":"3","timestamp":"1709951100.0","content":"X X RW need both rw for append"},{"poster":"hassexat","upvote_count":"1","content":"Execute\nExecute\nWrite\n\nThe provided answer is correct!","timestamp":"1709821260.0","comment_id":"1001533"},{"timestamp":"1709187120.0","comment_id":"994733","upvote_count":"1","content":"correct","poster":"kkk5566"},{"poster":"bakamon","content":"container1 : Read access [ by default because User1 that is assigned the Storage Blob Data Reader role for storage1 ]\n\ndirectory1: Execute [ since requirement is only to append file1 so traverse (execute) permission will be enough for it ]\n\nfile1 : Write [ because execute cannot append the file in Azure Data Lake Storage Gen2 ]\nonly write permission can append a file.","timestamp":"1701108120.0","comment_id":"908085","upvote_count":"1"},{"comment_id":"736829","content":"Can't remember if the wording on actual exam was the same or very similar but instead of Append was Delete and the Q was like this:\nYou have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named storage1. Storage1 contains a container named container1.\nContainer1 contains a directory named directory1. Directory1 contains a file named file1.\nYou have an Azure Active Directory (Azure AD) user named User1 that is assigned the Storage Blob Data Reader role for storage1.\nYou need to ensure that User1 can delete file1. The solution must use the principle of least privilege.\nPermission:\n----\n--WX\n---X\nAnswer Area and my answers:\ncontainer1 ---X\ndirectory1 ---X\nfile1 --WX","comments":[{"timestamp":"1698490440.0","comments":[{"comment_id":"977321","poster":"Matt2000","content":"mamahani is correct. See the following references:\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model#permissions-table-combining-azure-rbac-abac-and-acls","timestamp":"1707551340.0","upvote_count":"1"},{"comment_id":"958059","upvote_count":"1","content":"if you give write access to entire folder , the user can delete/modify other folders , whihc is not correct","timestamp":"1705820100.0","poster":"renukahouse"},{"timestamp":"1703806440.0","upvote_count":"2","poster":"vctrhugo","content":"The solution must use the principle of least privilege. You shouldn't do -WX on folder, only on file.","comment_id":"937155"}],"poster":"mamahani","comment_id":"883434","content":"i dont think you gave correct answers;\nsee this doc: https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control#common-scenarios-related-to-permissions\nto delete a file you dont need any permissions on the file itself; only on the folder where it resides (read + execute)","upvote_count":"1"}],"timestamp":"1686049920.0","upvote_count":"5","poster":"OldSchool"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0032600001.jpg"],"answer":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0032700001.jpg"],"isMC":false,"question_text":"DRAG DROP -\nYou have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named storage1. Storage1 contains a container named container1.\nContainer1 contains a directory named directory1. Directory1 contains a file named file1.\nYou have an Azure Active Directory (Azure AD) user named User1 that is assigned the Storage Blob Data Reader role for storage1.\nYou need to ensure that User1 can append data to file1. The solution must use the principle of least privilege.\nWhich permissions should you grant? To answer, drag the appropriate permissions to the correct resources. Each permission may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nSelect and Place:\n//IMG//","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/80798-exam-dp-203-topic-3-question-29-discussion/","question_id":280,"answer_description":"Box 1: Execute -\nIf you are granting permissions by using only ACLs (no Azure RBAC), then to grant a security principal read or write access to a file, you'll need to give the security principal Execute permissions to the root folder of the container, and to each folder in the hierarchy of folders that lead to the file.\n\nBox 2: Execute -\nOn Directory: Execute (X): Required to traverse the child items of a directory\n\nBox 3: Write -\nOn file: Write (W): Can write or append to a file.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control","timestamp":"2022-09-07 05:18:00","unix_timestamp":1662520680,"exam_id":67,"topic":"3"}],"exam":{"isMCOnly":false,"id":67,"lastUpdated":"12 Apr 2025","isBeta":false,"name":"DP-203","provider":"Microsoft","numberOfQuestions":384,"isImplemented":true},"currentPage":56},"__N_SSP":true}