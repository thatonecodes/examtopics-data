{"pageProps":{"questions":[{"id":"0Z3tBdc184dMjYM6LCZI","answers_community":[],"timestamp":"2021-12-13 16:32:00","discussion":[{"comment_id":"506461","comments":[{"comments":[{"timestamp":"1708450200.0","upvote_count":"3","poster":"mav2000","content":"P1: Set the partition to \"Dynamic range\"\nP2: Polybase\n\nthe reason for P1 is that Polybase is a technology on Azure Synapse Analytics that can read from external sources but can't insert data there, so it is able to read from Data Lake Storage, but won't be able to write there.\n\non the second case, since we want to write to a SQL Pool, it will work, that's why P2 is Polybase.\n\nBesides, Dynamic range partitioning is a technique to partition a non-partition table that allows to parallelize the reading of the source data, which makes it more faster.","comment_id":"1154873"}],"poster":"Matt2000","comment_id":"975672","upvote_count":"3","timestamp":"1691498940.0","content":"It should be: \nP1: PolyBase\nP2: PolyBase\n\n\"PolyBase is the best choice when you are loading or exporting large volumes of data, or you need faster performance.\"\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool\n\nRegarding \"dynamic range partitions\": \n\" As repartitioning data takes time, Use [sic] current partitioning is recommended in most scenarios.\" -> dynamic partitioning is NOT selected\nRef: https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance"}],"upvote_count":"98","timestamp":"1640123700.0","poster":"marcin1212","content":"how to use PolyBase when copy data from Synapse to file ? I don't have idea.\nMoreover PolyBase option is available only when the target is Synapse\n\nit should be\nP1: Set the partition option to \"Dynamic range \"\nP2: PolyBase \n\nregarding to P1\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#parallel-copy-from-synapse-analytics\nScenario: \"Full load from large table, without physical partitions..\" -> \nSuggested settings: Partition options: Dynamic range partition."},{"upvote_count":"46","content":"P1: Copy data from SQL to Data Lake. \n• Bulk insert and PolyBase are not a choice in Sink tab if target is Data Lake. So they are not correct. \n• Isolation level can be setup if SQL database is the source. Repeatable Read means that locks are placed on all data that is used in a query. Don't think it maximize parallelism and performance.\n• Set the Partition option to Dynamic range\nCan be setup if source is SQL in copy activity. And it maximizes parallelism and performance. So I select this option.\n\nP2: Copy data from Data Lake to SQL. It is for sure to select PolyBase.","timestamp":"1640186580.0","poster":"Canary_2021","comment_id":"507207"},{"comment_id":"1243733","timestamp":"1720336560.0","poster":"evangelist","content":"P1:\nSet the Partition option to Dynamic range: 171 votes\nSet the Copy method to PolyBase: 26 votes\nSet the Copy method to Bulk insert: 3 votes\nSet the Isolation level to Repeatable read: 0 votes\nP2:\nSet the Copy method to PolyBase: 95 votes\nSet the Copy method to Bulk insert: 8 votes\nSet the Isolation level to Repeatable read: 0 votes\nSet the Partition option to Dynamic range: 1 vote","upvote_count":"7"},{"upvote_count":"1","content":"ChatGPT:\nTo maximize parallelism and performance in your Azure Data Factory pipelines, configure the dataset settings as follows:\n\nFor Pipeline P1 (copying from WS1 to Azure Data Lake Storage Gen2):\n\nCopy Method: Set to PolyBase. This method is optimized for large data loads from SQL pools to Azure Data Lake Storage.\nFor Pipeline P2 (copying from Azure Data Lake Storage Gen2 to WS2):\n\nCopy Method: Set to Bulk Insert. This is efficient for loading data into SQL pools from Azure Data Lake Storage, especially with text-delimited files.\nThese settings will help enhance performance by leveraging the most efficient data transfer methods for each scenario.","timestamp":"1719775320.0","poster":"e56bb91","comment_id":"1239806"},{"content":"For the Azure Data Factory pipelines P1 and P2, the dataset settings for the copy activity should be configured as follows:\n\nP1:\n\nb. Set the Copy method to PolyBase\nPolyBase is a technology that accesses data outside of the database via the T-SQL language. It’s designed to leverage parallelism, which can lead to significant performance improvements when copying large amounts of data12.\n\nP2:\n\na. Set the Copy method to Bulk insert\nBulk insert is a process that can be used to import large amounts of data into a SQL Server table. It’s a highly efficient way to push data into a table, especially when dealing with text-delimited files12.\n\nPlease note that the actual performance may vary depending on the specific requirements and the structure of your data12.\n\nLearn more\n\n\n1\n\nlearn.microsoft.com\n2\n\nlearn.microsoft.com\n3\n\nsocial.msdn.microsoft.com","upvote_count":"1","comment_id":"1106296","poster":"6d954df","timestamp":"1703616240.0"},{"timestamp":"1703193900.0","content":"Got this question today on the exam","comment_id":"1102976","poster":"positivitypeople","upvote_count":"2"},{"comment_id":"1096689","timestamp":"1702575120.0","content":"P1: Dynamic range according to \nhttps://techcommunity.microsoft.com/t5/fasttrack-for-azure/leverage-copy-data-parallelism-with-dynamic-partitions-in-adf/ba-p/3692133","upvote_count":"1","poster":"d046bc0"},{"content":"Correct, chatgpt\nFor P1, where data is copied from a non-partitioned table in a SQL pool to Azure Data Lake Storage Gen2:\n- **Set the Copy method to PolyBase**: This is because PolyBase is designed to efficiently transfer large amounts of data to and from SQL-based data stores into Azure Data Lake Storage.\nFor P2, which copies data from text-delimited files in Azure Data Lake Storage Gen2 to a non-partitioned table in a SQL pool:\n- **Set the Copy method to Bulk insert**: Bulk insert is an efficient way to load data from files into SQL tables, especially when dealing with non-partitioned tables where PolyBase might not be applicable or the most optimal choice.","upvote_count":"3","poster":"Momoanwar","comment_id":"1092160","timestamp":"1702165500.0"},{"comment_id":"1023965","timestamp":"1696338720.0","upvote_count":"1","content":"P1 : set the partition option to dynamic range (see here : https://techcommunity.microsoft.com/t5/fasttrack-for-azure/leverage-copy-data-parallelism-with-dynamic-partitions-in-adf/ba-p/3692133)\n\nP2: Polybase give the best performance\nPolyBase loads data from UTF-8 and UTF-16 encoded delimited text files. PolyBase also loads from the Hadoop file formats RC File, ORC, and Parquet. PolyBase can also load data from Gzip and Snappy compressed files. PolyBase currently does not support extended ASCII, fixed-width format, and nested formats such as WinZip, JSON, and XML.","poster":"fahfouhi94"},{"timestamp":"1694008200.0","content":"P1) Set the partition option to dynamic range p2) set the copy method to PolyBase","upvote_count":"1","poster":"kkk5566","comment_id":"1000683"},{"content":"There's a really interesting video regarding PolyBase/COPY INTO here:\n\nhttps://microsoft.github.io/PartnerResources/skilling/modern-analytics-academy/vignettes/polybase-vs-copy\n\nThis video indicates that PolyBase can actually be used to pull/push data from/to Azure Data Lake Storage to/from Azure Synapse Analytics Dedicated SQL Pool tables (via CTAS & CETAS statements).","upvote_count":"4","timestamp":"1691046000.0","poster":"JezWalters","comment_id":"970834"},{"timestamp":"1689091500.0","poster":"faabbasi","comment_id":"949124","upvote_count":"1","content":"P1 dynamic range, link is pretty clear: https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#parallel-copy-from-azure-synapse-analytics"},{"poster":"Rossana","comment_id":"880377","content":"for P1, you should set the copy method to Polybase, and for P2, you should set the copy method to Bulk.\n\nThe reason is that Polybase is better suited for copying data between Azure Synapse Analytics and Azure Data Lake Storage Gen2, and can achieve better performance than Bulk copy in this scenario. On the other hand, Bulk copy is the fastest method for copying data from text-delimited files in Azure Data Lake Storage Gen2 to Azure Synapse Analytics.\n\nSetting the partition option to Dynamic range for both pipelines can help to maximize parallelism and performance by allowing the copy activity to split the data into multiple partitions based on the data range.","upvote_count":"2","timestamp":"1682426280.0"},{"content":"I tried to create a copy activity in adf and these were results:\n \nP1) Synapse to ADLS --> Source Partition option: None/Dynamic range\n Sink Copy behavior: Add dynamic content/None/Flatten hierarchy/Merge files/Preserve hierarchy\n \n \nP2) ADLS to Synapse --> Source Copy method: NA\n Sink Copy method: Copy command/PolyBase/Bulk insert/Upsert \n \n\nSo I think correct answers should be:\nP1) Set the partition option to dynamic range\np2) set the copy method to PolyBase","timestamp":"1676038740.0","poster":"vrodriguesp","comment_id":"804406","upvote_count":"12"},{"poster":"DAYENKAR","comment_id":"779939","content":"Both answer are polybase","upvote_count":"2","timestamp":"1674041460.0"},{"comment_id":"734680","poster":"XiltroX","content":"I think you can put both as PolyBase. PolyBase is much faster and supports text delimited files as well now. \nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#use-polybase-to-load-data-into-azure-synapse-analytics","timestamp":"1670100780.0","upvote_count":"2"},{"timestamp":"1659538560.0","upvote_count":"5","content":"Agree with marcin1212 \nit should be\nP1: Set the partition option to \"Dynamic range \"\nP2: PolyBase","comment_id":"641970","poster":"Deeksha1234"},{"comment_id":"618212","content":"P2 should be Polybase\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#use-polybase-to-load-data-into-azure-synapse-analytics\n\nP1 \net the partition option to \"Dynamic range \"","poster":"NamitSehgal","upvote_count":"2","timestamp":"1655550420.0"},{"comment_id":"601033","content":"Both are PolyBase\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver15\n\"Azure Synapse Analytics can Read/Write Azure Storage\"\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#parallel-copy-from-synapse-analytics\n\"As a sink, load data by using COPY statement or PolyBase or bulk insert. We recommend COPY statement or PolyBase for better copy performance\"","upvote_count":"3","timestamp":"1652430000.0","poster":"Towin"},{"content":"Answer is completely wrong. If writing to a non-partitioned table in a dedicated SQL Pool you ALWAYS want to choose Polybase whenever possible. So the answers are:\nP1: \"Dynamic Range\"\nP2: \"Polybase\"","upvote_count":"4","timestamp":"1649189640.0","comment_id":"581447","poster":"AlCubeHead"},{"content":"P1: Set the partition option to \"Dynamic range \"\nP2: PolyBase - PolyBase is significantly faster than BulkInsert","comment_id":"575807","upvote_count":"3","poster":"AlCubeHead","timestamp":"1648331340.0"},{"timestamp":"1647876180.0","upvote_count":"2","comment_id":"572345","poster":"alex1491","content":"From Synapse to Data lake it´s not a even option bulk insert or polybase. The only way to use those option are from Data lake to Synapse\n\nP1:Set the partition option to \"Dynamic range \"\nP2: Polybase"},{"timestamp":"1644578400.0","comment_id":"545255","content":"P1: Set the partition option to \"Dynamic range \"\nP2: PolyBase","poster":"Oldrich22","upvote_count":"3"},{"upvote_count":"14","timestamp":"1639417080.0","content":"I believe both answers are PolyBase. PolyBase supports both export to and import from ADLS as documented here: https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary\n\nPolyBase does support delimited text files, which contradicts the question's official answer. \"Currently PolyBase can load data from UTF-8 and UTF-16 encoded delimited text files as well as the popular Hadoop file formats RC File, ORC, and Parquet (non-nested format). \"\n\nhttps://techcommunity.microsoft.com/t5/datacat/azure-sql-data-warehouse-loading-patterns-and-strategies/ba-p/305456#:~:text=Currently%20PolyBase%20can%20load%20data%20from%20UTF-8%20and,data%20from%20gzip%2C%20zlib%20and%20Snappy%20compressed%20files.","poster":"ItHYMeRIsh","comment_id":"500781"},{"timestamp":"1639409520.0","poster":"Andreas_K","comments":[{"comment_id":"506463","upvote_count":"4","poster":"marcin1212","comments":[{"poster":"temacc","timestamp":"1672896900.0","comment_id":"766261","content":"PolyBase uses\n\nPolyBase enables the following scenarios in SQL Server:\n\n Query data stored in Azure Blob Storage. Azure Blob Storage is a convenient place to store data for use by Azure services. PolyBase makes it easy to access the data by using T-SQL.\n\n Query data stored in Hadoop from a SQL Server instance or PDW. Users are storing data in cost-effective distributed and scalable systems, such as Hadoop. PolyBase makes it easy to query the data by using T-SQL.\n\n Import data from Hadoop, Azure Blob Storage, or Azure Data Lake Store. Leverage the speed of Microsoft SQL's columnstore technology and analysis capabilities by importing data from Hadoop, Azure Blob Storage, or Azure Data Lake Store into relational tables. There is no need for a separate ETL or import tool.\n\n Export data to Hadoop, Azure Blob Storage, or Azure Data Lake Store. Archive data to Hadoop, Azure Blob Storage, or Azure Data Lake Store to achieve cost-effective storage and keep it online for easy access.\n\n Integrate with BI tools. Use PolyBase with Microsoft's business intelligence and analysis stack, or use any third-party tools that are compatible with SQL Server.","upvote_count":"4"}],"content":"@Andreas\nhow to use PolyBase when copy data from Synapse to file ?","timestamp":"1640123760.0"}],"comment_id":"500689","upvote_count":"5","content":"Right answer should be PolyBase in both cases. It provides the highest performance and supports delimited text files.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/load-data-overview\nhttps://docs.microsoft.com/en-us/azure/data-factory/load-azure-sql-data-warehouse?tabs=data-factory"}],"isMC":false,"answer_description":"Box 1: Set the Copy method to PolyBase\nWhile SQL pool supports many loading methods including non-Polybase options such as BCP and SQL BulkCopy API, the fastest and most scalable way to load data is through PolyBase. PolyBase is a technology that accesses external data stored in Azure Blob storage or Azure Data Lake Store via the T-SQL language.\nBox 2: Set the Copy method to Bulk insert\nPolybase not possible for text files. Have to use Bulk insert.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/load-data-overview","url":"https://www.examtopics.com/discussions/microsoft/view/67825-exam-dp-203-topic-2-question-42-discussion/","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0022000001.png"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0022100001.png"],"question_id":196,"question_text":"HOTSPOT -\nYou have an Azure Data Factory instance named ADF1 and two Azure Synapse Analytics workspaces named WS1 and WS2.\nADF1 contains the following pipelines:\n✑ P1: Uses a copy activity to copy data from a nonpartitioned table in a dedicated SQL pool of WS1 to an Azure Data Lake Storage Gen2 account\n✑ P2: Uses a copy activity to copy data from text-delimited files in an Azure Data Lake Storage Gen2 account to a nonpartitioned table in a dedicated SQL pool of WS2\nYou need to configure P1 and P2 to maximize parallelism and performance.\nWhich dataset settings should you configure for the copy activity if each pipeline? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","topic":"2","answer":"","exam_id":67,"unix_timestamp":1639409520,"answer_ET":""},{"id":"6WuPYdzcgpcxUrUTMjKs","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0022300001.png","https://www.examtopics.com/assets/media/exam-media/04259/0022400001.png"],"topic":"2","answer":"","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0022200001.png"],"unix_timestamp":1639293120,"question_id":197,"url":"https://www.examtopics.com/discussions/microsoft/view/67703-exam-dp-203-topic-2-question-43-discussion/","exam_id":67,"isMC":false,"discussion":[{"poster":"onyerleft","timestamp":"1655761200.0","upvote_count":"32","comments":[{"comment_id":"1154876","poster":"mav2000","comments":[{"content":"interesting, do you have references to support such a behaviour? From MS documentation, I can't find that DataFactory tracks file creation inside a tumbling window. From docs, It seems like a matter of scheduling instead of what you're saying.","comments":[{"timestamp":"1732117980.0","content":"https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger?tabs=data-factory%2Cazure-powershell","comment_id":"1214369","poster":"_Ahan_","upvote_count":"2"}],"upvote_count":"1","comment_id":"1187208","timestamp":"1727760780.0","poster":"MBRSDG"}],"content":"I believe it's only tumbling window, because if you were to choose fixed schedule, then you wouldn't know which files to load, the tumbling window allows you to know what happened in that window of time and know which files to load","upvote_count":"1","timestamp":"1724168160.0"},{"upvote_count":"1","comment_id":"1135547","poster":"Gikan","content":"Yes, it is true. If the Data Factory contains more than 1 pipeline and I like to trigger it together, the schedule trigger is the only solution: \"Supports many-to-many relationships. Multiple triggers can kick off a single pipeline. A single trigger can kick off multiple pipelines.\"\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers","timestamp":"1722314580.0"}],"comment_id":"505739","content":"1) Incremental Load\n2) Tumbling Window\n\nSeems like you could go with either Schedule trigger or Tumbling Window here. I would use the latter option, and pass the windowStart system variable to the pipeline as a parameter, allowing me to more easily navigate to the proper directory in the storage account."},{"content":"Since, we are loading NEW data and not going back in time, it should be Schedule as we are scheduling it for every 1 hour in the future. It would've been Tumbling if we scheduled it for every 1 hour in the past.","upvote_count":"17","poster":"xcsakubara","timestamp":"1661262000.0","comment_id":"554615","comments":[{"poster":"phydev","comment_id":"1058227","timestamp":"1714499100.0","content":"Besides, a scheduled trigger is a better option for this specific scenario than a tumbling window due to precision, efficiency and cost savings.","upvote_count":"2"}]},{"content":"Got this question today on the exam","comment_id":"1102977","poster":"positivitypeople","timestamp":"1718997900.0","upvote_count":"4"},{"content":"Correct, chatgpt :\nFor the scenario described, to load data from an Azure Storage account to an Azure Data Lake hourly and to minimize load times and costs, you would configure the Azure Data Factory solution as follows:\n\n- **Load methodology**: Incremental Load - Because you are loading new data every hour, and the goal is to minimize the load times and costs, you would incrementally load only the new data that has arrived since the last load.\n\n- **Trigger**: Tumbling window - This trigger is suitable for fixed-duration, repeating intervals in Azure Data Factory, which fits the requirement of loading data hourly. \n\nUsing a tumbling window trigger ensures that each window of time is processed once and only once, and by doing an incremental load, you are only processing the new data that has appeared since the last hour, rather than reprocessing all existing data.","comment_id":"1092165","timestamp":"1717969860.0","poster":"Momoanwar","upvote_count":"2"},{"upvote_count":"1","poster":"Andrew_Chen","comment_id":"1041404","timestamp":"1712900760.0","content":"I think one thing very important here is that Tumbling window manages state between runs, that means that it will not be count twice."},{"content":"From Azure Data Factory Studio when you create a new trigger, you can choise TYPE in ('Schedule', 'Tumbling window', 'Storage events', 'Custom events'). \nWe should exclude \"Fixed Schedule\" becuase of 'fixed'! :) \nSo my final answer will be Incremental Load and Tumbling Window.","timestamp":"1703250960.0","poster":"auwia","comment_id":"930419","upvote_count":"6"},{"timestamp":"1699182060.0","comment_id":"889913","content":"Hi there","poster":"vedantnj","upvote_count":"7"},{"content":"To minimize load times and costs for loading new data from the storage account to an Azure Data Lake once hourly, you should configure the solution to use incremental load and a trigger based on new files arriving.\n\nLoad methodology: With 200,000 new files generated daily, a full load every hour could be time-consuming and expensive. Incremental load is a better option in this scenario because it only loads new or changed data since the last successful execution of the pipeline, which can significantly reduce load times and costs.\nTrigger: A trigger based on new files arriving is the most efficient option because it only runs the pipeline when new files are detected in the storage account. This avoids unnecessary pipeline executions and reduces costs. A fixed schedule trigger runs the pipeline at fixed intervals, regardless of whether there is new data to process or not. A tumbling window trigger runs the pipeline at specified intervals, but still processes all data within the window, regardless of whether there is new data or not. Therefore, a new file trigger is the best option in this scenario.","comments":[{"content":"Wrong, the questions specifies that it has to run hourly, so it's tumbling window","upvote_count":"2","comment_id":"1154875","timestamp":"1724168100.0","poster":"mav2000"}],"timestamp":"1698240240.0","comment_id":"880418","poster":"Rossana","upvote_count":"5"},{"content":"A schedule for an activity creates a series of tumbling windows with in the pipeline start and end times \n\nI think is \"Fixed schedule\" because \"Tumbling windows\" are more related to streams analytics questions according to MS doc.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-scheduling-and-execution","poster":"martcerv","timestamp":"1687545180.0","comment_id":"754524","upvote_count":"6"},{"upvote_count":"4","poster":"Deeksha1234","comment_id":"641974","timestamp":"1675443600.0","content":"1) Incremental Load\n2) Tumbling Window"},{"poster":"jskibick","comment_id":"604745","content":"With Scheduled trigger executions can overlaps if the process does not finish within 1 hour, Tumbling window is better, with concurrency setting it can allow only one ongoing execution.","upvote_count":"14","timestamp":"1669021860.0"},{"comment_id":"572993","content":"both Tumbling Window and Schedule trigger will reach the goal. Which one is more cost effective?","upvote_count":"1","poster":"Massy","comments":[{"comment_id":"600497","timestamp":"1668242880.0","poster":"Boompiee","content":"I think because every hour you're only processing the past hour's data. With a tumbling window you can define which messages to process, whereas with a schedule trigger you'd have to implement that filter separately.","upvote_count":"2"}],"timestamp":"1663849200.0"},{"poster":"xcsakubara","content":"why not schedule trigger?","comment_id":"554318","comments":[{"content":"for backfill purpose? just guessing.","poster":"sparkchu","timestamp":"1664596860.0","upvote_count":"1","comment_id":"579313"}],"timestamp":"1661236680.0","upvote_count":"1"},{"timestamp":"1655483760.0","upvote_count":"6","poster":"jv2120","comment_id":"503856","content":"incremental, fixed schedule every hour.","comments":[{"content":"correct answer..tumbling window","timestamp":"1655484180.0","comment_id":"503859","poster":"jv2120","upvote_count":"1"}]},{"upvote_count":"2","comment_id":"499838","comments":[{"comment_id":"500783","timestamp":"1655135100.0","comments":[{"poster":"ANath","upvote_count":"2","comment_id":"526720","content":"That's correct. Well explained","timestamp":"1658147400.0"}],"poster":"ItHYMeRIsh","upvote_count":"36","content":"The question says, \"load new data from the storage account to the Azure Data Lake once hourly.\" This already indicates a tumbling window to run every hour.\n\nOn top of that, if you executed this as an event every time a file arrived, you'd have 200,000 ADF pipeline executions per day - one per file. If you ran the pipeline once per hour per day, you'd have just 24.\n\n1,000 ADF runs is $1. In this situation, 1 day is 24 runs when executed on a tumbling window. That's 2.4 cents. If we ran 200,000 pipelines, that'd be $200/day. This excludes other costs.\n\nhttps://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"}],"poster":"Ayan3B","timestamp":"1655010720.0","content":"As a input we are receiving csv files so why not trigger mechanism to the pipeline when file arrived."}],"question_text":"HOTSPOT -\nYou have an Azure Storage account that generates 200,000 new files daily. The file names have a format of {YYYY}/{MM}/{DD}/{HH}/{CustomerID}.csv.\nYou need to design an Azure Data Factory solution that will load new data from the storage account to an Azure Data Lake once hourly. The solution must minimize load times and costs.\nHow should you configure the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"Box 1: Incremental load -\n\nBox 2: Tumbling window -\nTumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. The following diagram illustrates a stream with a series of events and how they are mapped into 10-second tumbling windows.\n\nReference:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics","timestamp":"2021-12-12 08:12:00","answer_ET":""},{"id":"iC1F0HhAvVZmoGGxN97C","exam_id":67,"topic":"2","unix_timestamp":1639098060,"answers_community":["B (96%)","4%"],"choices":{"A":"Yes","B":"No"},"timestamp":"2021-12-10 02:01:00","discussion":[{"timestamp":"1657010160.0","comment_id":"517367","poster":"lukeonline","comments":[{"content":"B is correct, but there's an update, high concurrency now supports scala https://learn.microsoft.com/en-us/azure/databricks/compute/configure#--high-concurrency-clusters\n\n- **Data scientist**: Cada Data scientist tendrá su propio cluster, lo que significa que con un **standard** con la configuración de autoterminate y ya están bien.\n- **Data engineer**: los Data engineer deberian usar un **High Concurrency** cluster, porque son muchos usuarios.\n- **Jobs**: esta bien que los jobs usen **High concurrency** porque muchos jobs podrian estar ejecutandose en el cluster y ahora high concurrency si soporta scala","comment_id":"1154964","upvote_count":"2","poster":"mav2000","timestamp":"1724175900.0"},{"upvote_count":"1","comment_id":"1135555","content":"You do not need to use High concurrency, because \"Scala code will be executed inside the Spark JVM (per machine) that is shared between all users\": \nhttps://learn.microsoft.com/en-us/answers/questions/924587/azure-databricks-scala-on-high-concurrency-cluster","timestamp":"1722315420.0","poster":"Gikan"},{"comment_id":"568577","timestamp":"1663258680.0","upvote_count":"2","poster":"kamil_k","content":"or rather Scala does not support concurrent instances (but yes, it implies HC cluster will not support Scala)"}],"upvote_count":"40","content":"Selected Answer: B\nB is correct but the explanation is wrong.\n✑ A workload for data engineers who will use Python and SQL. --> high concurrency \n✑ A workload for jobs that will run notebooks that use Python, Scala, and SQL. --> standard\n✑ A workload that data scientists will use to perform ad hoc analysis in Scala and R. --> standard because high concurrency does not support Scala\n\nhttps://stackoverflow.com/questions/65869399/high-concurrency-clusters-in-databricks"},{"comment_id":"1096802","upvote_count":"2","poster":"d046bc0","content":"Standard is enough for all workloads. High concurrency (due to Scala )possible only for data engineers","timestamp":"1718389260.0"},{"comment_id":"1092168","upvote_count":"1","content":"Correct, chatgpt :\nFor the given scenario, where data engineers must share a cluster, data scientists need their own clusters with auto-termination, and a managed job cluster is required for running notebooks, the solution provided may not fully meet the goal. Here's why:\n\n- Data engineers should share a cluster, so creating a single Standard cluster for all data engineers would meet this requirement.\n- For data scientists, the solution suggests a Standard cluster for each, but it should specify that these clusters have auto-termination settings configured to minimize costs.\n- The High Concurrency cluster is suitable for running jobs because it allows multiple users to share the cluster and run jobs concurrently. However, it should be managed as per the enterprise team's standards.\n\nThe provided solution does not fully adhere to these standards, especially regarding the auto-termination requirement for data scientists' clusters. Thus, the answer would be:\n\nB. No, the solution does not meet the goal.","timestamp":"1717970100.0","poster":"Momoanwar"},{"upvote_count":"2","comment_id":"1000685","content":"Selected Answer: B\nhigh concurrency does not support Scala","timestamp":"1709740380.0","poster":"kkk5566"},{"content":"Selected Answer: B\nCorrect option is B-NO","upvote_count":"1","comment_id":"975344","poster":"akhil5432","timestamp":"1707388200.0"},{"upvote_count":"1","content":"A)Yes\nThe use of a shared Standard cluster for data engineers, a High Concurrency cluster for jobs, and individual Standard clusters for each data scientist that auto-terminates after 120 minutes of inactivity aligns with the specified standards and is a valid approach for creating a tiered Databricks workspace.","poster":"Rossana","comment_id":"880435","timestamp":"1698241920.0"},{"upvote_count":"1","content":"https://docs.databricks.com/clusters/configure.html","timestamp":"1686362040.0","comment_id":"740663","poster":"kckalahasthi"},{"upvote_count":"2","timestamp":"1684693500.0","content":"high concurrency cluster is already a legacy cluster mode. question is not relevant anymore","poster":"Igor85","comment_id":"723885"},{"poster":"greenlever","comment_id":"694360","content":"Selected Answer: A\nStandard mode can be shared by multiple users and terminate automatically, on the other hand High do not terminate automatically and Scala workload is not supported.","timestamp":"1681429800.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1679410500.0","comment_id":"675142","poster":"Babu99","content":"NO IS CORRECT ANSWER"},{"comment_id":"642315","upvote_count":"1","content":"correct , answer B, agree with lukeonline","timestamp":"1675513500.0","poster":"Deeksha1234"},{"timestamp":"1671545580.0","content":"https://docs.microsoft.com/en-us/azure/databricks/clusters/configure","upvote_count":"1","poster":"mkthoma3","comment_id":"619246"},{"timestamp":"1662832800.0","poster":"Hanse","upvote_count":"3","comment_id":"565014","content":"As per Link: https://docs.azuredatabricks.net/clusters/configure.html\nStandard and Single Node clusters terminate automatically after 120 minutes by default. --> Data Scientists\nHigh Concurrency clusters do not terminate automatically by default.\nA Standard cluster is recommended for a single user. --> Standard for Data Scientists & High Concurrency for Data Engineers\nStandard clusters can run workloads developed in any language: Python, SQL, R, and Scala.\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. --> Jobs needs Standard"},{"timestamp":"1655893920.0","comment_id":"507072","content":"B is correct","poster":"bad_atitude","upvote_count":"2"},{"comment_id":"498157","comments":[{"timestamp":"1656142440.0","poster":"Sanand","content":"Agree! - Correct answer; Standard for Scientists and jobs. High concurrency for data engineers.","comment_id":"509038","upvote_count":"3"}],"poster":"alexleonvalencia","timestamp":"1654815660.0","upvote_count":"3","content":"Selected Answer: B\nRespuesta correcta; Standar para Cientificos y jobs. Alta concurrencia para ingenieros de datos."}],"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/67483-exam-dp-203-topic-2-question-44-discussion/","answer":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\n✑ A workload for data engineers who will use Python and SQL.\n✑ A workload for jobs that will run notebooks that use Python, Scala, and SQL.\n✑ A workload that data scientists will use to perform ad hoc analysis in Scala and R.\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\n✑ The data engineers must share a cluster.\n✑ The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\n✑ All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\nYou need to create the Databricks clusters for the workloads.\nSolution: You create a Standard cluster for each data scientist, a Standard cluster for the data engineers, and a High Concurrency cluster for the jobs.\nDoes this meet the goal?","answer_images":[],"question_id":198,"isMC":true,"answer_ET":"B","question_images":[]},{"id":"swMlV7v0m0MPtedm90g4","answer_description":"","timestamp":"2021-12-14 18:51:00","choices":{"B":"Add a schedule trigger to all four pipelines.","A":"Add an event trigger to all four pipelines.","D":"Create a patient pipeline that contains the four pipelines and use an event trigger.","C":"Create a patient pipeline that contains the four pipelines and use a schedule trigger."},"question_images":[],"isMC":true,"exam_id":67,"question_text":"You have the following Azure Data Factory pipelines:\n✑ Ingest Data from System1\n✑ Ingest Data from System2\n✑ Populate Dimensions\n✑ Populate Facts\nIngest Data from System1 and Ingest Data from System2 have no dependencies. Populate Dimensions must execute after Ingest Data from System1 and Ingest\nData from System2. Populate Facts must execute after Populate Dimensions pipeline. All the pipelines must execute every eight hours.\nWhat should you do to schedule the pipelines for execution?","discussion":[{"timestamp":"1640053740.0","upvote_count":"63","content":"Selected Answer: C\nC is correct, but with poor wording. Should be 'parent pipeline' with a schedule trigger.\n\nThe parent pipeline has 4 execute pipeline activities. Ingest 1 and Ingest 2 have no dependencies. Dimension pipeline has two dependencies from 'on completion' outputs of both Ingest 1 and Ingest 2 pipelines. Fact pipeline has one 'on completion' dependency on the Dimension pipeline. Absolutely nothing to do with a tumbling window trigger","poster":"onyerleft","comment_id":"505783","comments":[{"timestamp":"1657378740.0","upvote_count":"2","comment_id":"629219","poster":"dsp17","content":"Big thanks onyerleft :)"},{"poster":"Remedios79","upvote_count":"3","timestamp":"1655725980.0","content":"Thank you. I was wondering about \"patient\" and related it on my poor english!","comment_id":"619238"},{"poster":"lukeonline","content":"Lol, I searched in the internet for the \"patient pipeline\".... should have read the comments first :)","comment_id":"517371","upvote_count":"20","timestamp":"1641379200.0"},{"timestamp":"1696509720.0","upvote_count":"1","poster":"Vanq69","content":"Also looked up \"patient pipeline\" and was confused xD","comment_id":"1025653"}]},{"poster":"evangelist","upvote_count":"2","timestamp":"1720338840.0","comment_id":"1243743","content":"Selected Answer: C\npatient=>parent"},{"comment_id":"1058963","upvote_count":"2","poster":"phydev","content":"Selected Answer: C\nWas on my exam today (31.10.2023).","timestamp":"1698764220.0"},{"content":"Selected Answer: C\nc is correct","poster":"kkk5566","comment_id":"1001497","timestamp":"1694087700.0","upvote_count":"1"},{"timestamp":"1687140600.0","comment_id":"927036","content":"C is correct","poster":"steveo123","upvote_count":"1"},{"upvote_count":"4","content":"Selected Answer: C\nIts not patient pipeline, it should be parent pipeline. Since there are 3 types of triggers in ADF:\n1) Schedule Trigger - trigger a pipeline at a fixed hour/minute of the day.\n2) Tumbling Window Trigger - trigger a pipeline which usually works for real time data\n3) Event-based Trigger - trigger a pipeline incase of an event i.e. new file coming to blob/adls etc.\n\nSince the 4 pipelines must be triggered every 8 hrs, then it should be schedule trigger.","timestamp":"1671346500.0","poster":"vigilante89","comment_id":"748645"},{"upvote_count":"1","content":"right C","timestamp":"1659608880.0","poster":"Deeksha1234","comment_id":"642317"},{"comment_id":"515939","comments":[{"content":"lol I think they mean \"parent\"","poster":"anto69","comment_id":"534674","timestamp":"1643371860.0","upvote_count":"1"}],"timestamp":"1641231660.0","poster":"DrTaz","content":"what the hk is a patient pipeline?","upvote_count":"2"},{"comment_id":"504203","timestamp":"1639830360.0","comments":[{"poster":"AzureJobsTillRetire","timestamp":"1669261140.0","upvote_count":"1","comment_id":"725534","content":"If those pipelines finish quickly, schedule trigger should be fine. If there is possibility that those pipelines may run for close to or more than 8 hours, definitely tumbling window should be used instead"}],"content":"It should be tumbling window since 2 dependent pipelines on run state. from given option only schedule event fits but its not correct.","poster":"jv2120","upvote_count":"3"},{"content":"Shouldn't the answer be A/D?","poster":"VJPR","timestamp":"1639567380.0","comments":[{"timestamp":"1662791880.0","poster":"TashaP","content":"So the question tries to trick you, they don't want to ask about individual pipeline configurations where you need to account for dependencies, they literally want to know how you will schedule the pipelines for execution. The additional information is there to confuse you and make you overthink, focus on the question. In this case, it is C.","upvote_count":"1","comment_id":"665151"}],"upvote_count":"1","comment_id":"502078"},{"comments":[{"comment_id":"503941","content":"The question or answers do not mention Tumbling Window. What is the basis for the response? Any more context?","upvote_count":"2","poster":"corebit","timestamp":"1639785120.0"}],"upvote_count":"2","poster":"dpBBC","comment_id":"501581","timestamp":"1639504260.0","content":"I think it should be Tumbling window"}],"url":"https://www.examtopics.com/discussions/microsoft/view/67965-exam-dp-203-topic-2-question-45-discussion/","question_id":199,"answer_images":[],"answer_ET":"C","unix_timestamp":1639504260,"topic":"2","answers_community":["C (100%)"],"answer":"C"},{"id":"PU0Z0pcSsQB2KMNhWNDV","answers_community":[],"answer":"","question_id":200,"question_text":"DRAG DROP -\nYou are responsible for providing access to an Azure Data Lake Storage Gen2 account.\nYour user account has contributor access to the storage account, and you have the application ID and access key.\nYou plan to use PolyBase to load data into an enterprise data warehouse in Azure Synapse Analytics.\nYou need to configure PolyBase to connect the data warehouse to storage account.\nWhich three components should you create in sequence? To answer, move the appropriate components from the list of components to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//","exam_id":67,"unix_timestamp":1639098600,"topic":"2","isMC":false,"question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0022700001.jpg"],"timestamp":"2021-12-10 02:10:00","answer_description":"Step 1: an asymmetric key -\nA master key should be created only once in a database. The Database Master Key is a symmetric key used to protect the private keys of certificates and asymmetric keys in the database.\nStep 2: a database scoped credential\nCreate a Database Scoped Credential. A Database Scoped Credential is a record that contains the authentication information required to connect an external resource. The master key needs to be created first before creating the database scoped credential.\n\nStep 3: an external data source -\nCreate an External Data Source. External data sources are used to establish connectivity for data loading using Polybase.\nReference:\nhttps://www.sqlservercentral.com/articles/access-external-data-from-azure-synapse-analytics-using-polybase","discussion":[{"content":"1.- A database scoped credential\n2.- an External data sorce\n3.- a external file format","timestamp":"1639098600.0","comments":[{"timestamp":"1658814360.0","comments":[{"comment_id":"1201762","content":"don't need encription ke","timestamp":"1714023300.0","poster":"Dusica","upvote_count":"1"},{"timestamp":"1679996100.0","upvote_count":"1","comment_id":"853058","poster":"DiscussoR","content":"File format is not related to a specific file"},{"comment_id":"930430","poster":"auwia","timestamp":"1687433160.0","upvote_count":"5","content":"It should be: \"Create a Master Key\" not 'DataBase Encryption Key' if you are trying to findout a predecessor step before \"create a Database Scoped Credential\"."}],"content":"you need to connect to the DW, not to a specific file. Therefore :\n1- Create a Database Encryption Key\n2 - Create a Database Scoped Credential\n3 - Create an External Data Source","comment_id":"637193","upvote_count":"21","poster":"Franz58"},{"poster":"Unessoo","timestamp":"1734911100.0","upvote_count":"1","content":"the correct order is:\n\nan asymmetric key\na database scoped credential\nan external data source\nan external file format","comment_id":"1330619"},{"poster":"Bilal2","content":"agreed. \nhttps://www.sqlshack.com/sql-server-polybase-external-tables-with-azure-blob-storage/","timestamp":"1673272200.0","comments":[{"comment_id":"1241128","timestamp":"1719975660.0","content":"Thank you for the link, explains it very well.","poster":"learnwell","upvote_count":"1"}],"comment_id":"770477","upvote_count":"2"},{"poster":"DiscussoR","content":"Agree:\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-hadoop","comment_id":"853040","timestamp":"1679994720.0","upvote_count":"3"}],"poster":"alexleonvalencia","comment_id":"498182","upvote_count":"216"},{"content":"According to the documentation, the first thing you are to create is \nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'S0me!nfo'; \nI don't think this is means an asymmetric key. It is simply a database encryption key. So I think the answer is \n\n1- Create a Database Encryption Key\n2 - Create a Database Scoped Credential\n3 - Create an External Data Source","poster":"engrbrain","comments":[{"poster":"vanrell","upvote_count":"4","content":"Does the text not say you already have an access key? Should the correct answer not be \n1.- A database scoped credential\n2.- an External data sorce\n3.- a external file format\n\nas alex mentions?","comments":[{"timestamp":"1653705720.0","upvote_count":"4","comments":[{"poster":"sdokmak","content":"*sorry, not asymmetric","upvote_count":"3","comment_id":"608563","timestamp":"1653782340.0"}],"poster":"sdokmak","content":"access key is for storage account so you still need a master/asymmetric key for the database.","comment_id":"608236"}],"timestamp":"1647527520.0","comment_id":"569789"},{"upvote_count":"2","timestamp":"1647449820.0","poster":"kamil_k","content":"Btw yes even in the description it says that the master key is a symmetric key, not an asymmetric one. It","comment_id":"569182"},{"timestamp":"1647408180.0","poster":"kamil_k","content":"also, the question only mentions storage account in general not a file or folder, so I believe we don't need to go as far as creating file format anyway","comment_id":"568783","upvote_count":"3"}],"upvote_count":"56","timestamp":"1641622020.0","comment_id":"519340"},{"poster":"sachin_mt","comment_id":"1401525","upvote_count":"1","content":"It is Database Master key required and not Database Encryption Key and hence it is not aprt of answer even though we need Database Master key to be created before loding into Dedicated SQL pool","timestamp":"1742557980.0"},{"comment_id":"1356805","upvote_count":"1","content":"1.- A database scoped credential\n2.- an External data sorce\n3.- a external file format\nA database encryption key is required before a database can be encrypted by using transparent data encryption (TDE). See https://learn.microsoft.com/en-us/sql/t-sql/statements/create-database-encryption-key-transact-sql?view=sql-server-ver16\nAsymmetric keys are not compatiable with dedicated SQL pools (formerly SQL DW). https://learn.microsoft.com/en-us/sql/t-sql/statements/create-asymmetric-key-transact-sql?view=sql-server-ver16","timestamp":"1739614320.0","poster":"aeab260"},{"poster":"renan_ineu","timestamp":"1727001720.0","content":"Answer:\n1. DB scoped credential\n2. External data source\n3. External file format\n\nDocs:\n\nMASTER [symmetric] KEY w/ PASSWORD (seams optional: see link 2; is symmetric: see link 3)\n > DATABASE SCOPED CREDENTIAL w/ IDENTITY and SECRET key\n > EXTERNAL DATA SOURCE w/ TYPE, LOCATION, CREDENTIAL\n > EXTERNAL FILE w/ TYPE and OPTIONS > TERMINATOR\n > EXTERNAL TABLE w/ DATA_SOURCE and LOCATION\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-sql-server?view=sql-server-ver16#configure-a-sql-server-external-data-source\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-master-key-transact-sql?view=sql-server-ver16#remarks\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/open-master-key-transact-sql?view=sql-server-ver16#remarks","comment_id":"1287665","upvote_count":"3"},{"timestamp":"1721011620.0","poster":"evangelist","content":"A database scoped credential: This is needed to authenticate and access the storage account.\nAn external data source: This defines the location of the data source using the database scoped credential.\nAn external file format: This specifies the format of the data files (e.g., CSV, Parquet) in the data lake.","upvote_count":"2","comment_id":"1248033"},{"timestamp":"1720347240.0","upvote_count":"2","poster":"evangelist","content":"the correct order for the components is:\n\nA database scoped credential\nAn external data source\nAn external file format","comment_id":"1243777"},{"timestamp":"1708177560.0","comment_id":"1152573","upvote_count":"3","poster":"Azure_2023","content":"Database Scoped Credential: This comes first as it holds the secure credentials needed for authentication.\nExternal Data Source: This relies on the credential to define the connection to the storage account.\nExternal File Format (Optional): This step further enhances performance and efficiency by informing PolyBase of the data format."},{"timestamp":"1703853060.0","content":"Given answer appears correct. This section specifies we need the master key (which is an asymmetric key), then create the scoped credential. The scoped credential can finally be used to create an external data source.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql?view=sql-server-ver16#c-creating-a-database-scoped-credential-for-polybase-connectivity-to-azure-data-lake-store","poster":"jongert","comment_id":"1108669","upvote_count":"1"},{"comment_id":"1106311","content":"check this out, the answers provided are correct https://learn.microsoft.com/en-us/sql/analytics-platform-system/polybase-configure-azure-blob-storage?view=aps-pdw-2016-au7","poster":"6d954df","comments":[{"timestamp":"1709322300.0","content":"This links also mentions a 4th step: create external file format, that doesn't fit in the number of asked answers.","upvote_count":"1","poster":"lola_mary5","comment_id":"1163709"}],"upvote_count":"1","timestamp":"1703617920.0"},{"poster":"Momoanwar","comment_id":"1092170","upvote_count":"5","timestamp":"1702166280.0","content":"Chatgpt :\nTo configure PolyBase to connect an Azure Synapse Analytics data warehouse to an Azure Data Lake Storage Gen2 account, you need to create:\n\n1. **A database scoped credential**: This stores the necessary authentication to access the data lake, such as the storage account access key.\n\n2. **An external data source**: This defines the location of the data in the storage account and uses the scoped credential for authentication.\n\n3. **An external file format**: This specifies the format of the data files (e.g., CSV, Parquet) in the data lake so that PolyBase knows how to parse the data.\n\nThese components should be created in the sequence listed above to ensure that PolyBase has the information it needs to authenticate, locate, and read the data from the storage account."},{"comment_id":"1073964","upvote_count":"2","content":"Seems like answers are differing. Do we get partial points if answer is partially correct ?","timestamp":"1700308680.0","poster":"abhijeetbgmcanada"},{"content":"The database master key is a SYMMETRIC KEY (https://learn.microsoft.com/en-us/sql/t-sql/statements/create-master-key-transact-sql?view=sql-server-ver16#remarks) therefore the answer should be:\n\n1- Create a Database Encryption Key (CREATE MASTER KEY ENCRYPTION BY PASSWORD)\n2 - Create a Database Scoped Credential (CREATE DATABASE SCOPED CREDENTIAL)\n3 - Create an External Data Source (CREATE EXTERNAL DATA SOURCE)\n(https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#1-create-database-scoped-credential)","poster":"ellala","timestamp":"1696782420.0","comment_id":"1028157","upvote_count":"2"},{"timestamp":"1696491360.0","content":"CREATE MASTER KEY;\n\n-- SECRET: Provide your Azure storage account key.\nCREATE DATABASE SCOPED CREDENTIAL AzureStorageCredential\nWITH\n IDENTITY = 'user',\n SECRET = '<azure_storage_account_key>'\n;\nCREATE EXTERNAL DATA SOURCE AzureStorage\nWITH (\n TYPE = HADOOP,\n LOCATION = 'wasbs://<blob_container_name>@<azure_storage_account_name>.blob.core.windows.net',\n CREDENTIAL = AzureStorageCredential\n);\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-load-from-azure-blob-storage-with-polybase","upvote_count":"1","poster":"oturbo","comment_id":"1025363"},{"upvote_count":"2","timestamp":"1694087700.0","poster":"kkk5566","content":"1.- A database scoped credential 2.- an External data sorce 3.- a external file format","comment_id":"1001498"},{"poster":"[Removed]","upvote_count":"2","content":"1.A database scoped credential\n2.an External data source\n3 a external file format\n\nCreate master key is an symmetric key https://learn.microsoft.com/en-us/sql/t-sql/statements/create-master-key-transact-sql?view=sql-server-ver16\n\nDEK comes under the concept of azure SQL TDE and no way related to this question\n\nHence proved","comment_id":"979140","timestamp":"1691818260.0"},{"upvote_count":"2","poster":"mcwest002","timestamp":"1688887260.0","comment_id":"946942","content":"Create external tables for Azure Data Lake Store\nFrom <https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-azure-data-lake-store> \n\n1. Create database scoped credential\n2. Create external data source to reference Azure Data Lake Store (ADLS)\n3. Create external file format"},{"content":"1.- A database scoped credential\n2.- an External data sorce\n3.- a external file format","upvote_count":"1","comment_id":"909797","timestamp":"1685406660.0","poster":"rocky48"},{"poster":"Rossana","comment_id":"880490","content":"Create an external data source (C) that specifies the location of the data in the storage account.\nCreate an external file format (E) that describes the format of the data in the external data source.\nCreate a database scoped credential (A) that contains the credentials needed to access the storage account.\nNote that asymmetric keys and database encryption keys are not required for configuring PolyBase with Azure Data Lake Storage Gen2.","timestamp":"1682433420.0","upvote_count":"1"},{"poster":"DipikaChavan","comment_id":"872421","timestamp":"1681714200.0","upvote_count":"2","content":"1.A database scoped credential \n2.an External data source \n3 a external file format"},{"content":"The final answer is:\nMaster key (to encrypt credentials)\nScoped credential (to provide credentials for storage account)\nExternal data source (to point to a specific storage account)\n\nSource: https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-hadoop","poster":"DiscussoR","timestamp":"1679996400.0","comment_id":"853064","upvote_count":"2"},{"comments":[{"comment_id":"1187194","timestamp":"1711948740.0","upvote_count":"1","content":"I agree, and the motivation is not trivial at all. At first, also I thought that the correct answer should include file format, but reasoning a bit, the answer is wrong if you understand how certification questions work. \n\nIt's true that, for accessing data by SQL external data source, you don't need the master key. But.... there's a thing called encryption at rest that allows to encrypt files. It is a required step in practise. \n\nMoreover, choosing encryption has another weight than choosing a optional step such as file format, at least in terms of technical skills for a professional certification.","poster":"MBRSDG"}],"content":"o configure PolyBase to connect the data warehouse to the storage account, you should create the following components in sequence:\n\nAn asymmetric key in the data warehouse database.\nA database scoped credential using the application ID and access key.\nAn external data source that references the database scoped credential and specifies the storage account details.","poster":"esaade","timestamp":"1678956960.0","comment_id":"840712","upvote_count":"2"},{"poster":"esaade","upvote_count":"1","comment_id":"831810","timestamp":"1678188120.0","content":"To configure PolyBase to connect the data warehouse to the storage account, you should create the following components in sequence:\n\nAn asymmetric key (to secure the database scoped credential).\nA database scoped credential (to provide authentication to the storage account).\nAn external data source (to define the connection to the storage account)."},{"comment_id":"779203","upvote_count":"2","content":"The database master key is a SYMMETRIC key that is used to protect the private keys of certificates and asymmetric keys that are present in the database. \n\nWe should start with Database Encryption Key, which is a Master Key. Not the assymetric key, as this can not be a master key. Agreed with engrbrain:\n1. Database Encryption Key\n2. Database Scoped Credential\n3. External Data Source","poster":"SophieM","timestamp":"1673980500.0"},{"upvote_count":"5","timestamp":"1669041900.0","poster":"OldSchool","comment_id":"723630","content":"-- Create a database master key if one does not already exist, using your own password. This key is used to encrypt the credential secret in next step.\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = '<password>' ;\n\n-- Create a database scoped credential with Azure storage account key as the secret.\nCREATE DATABASE SCOPED CREDENTIAL AzureStorageCredential\nWITH\n IDENTITY = '<my_account>',\n SECRET = '<azure_storage_account_key>' ;\n\n-- Create an external data source with CREDENTIAL option.\nCREATE EXTERNAL DATA SOURCE MyAzureStorage\nWITH\n ( LOCATION = 'wasbs://daily@logs.blob.core.windows.net/' ,\n CREDENTIAL = AzureStorageCredential ,\n TYPE = HADOOP\n ) ;"},{"poster":"pmc08","content":"1.- An external data source\n2.- An External File Format\n3.- A database scoped credential \n\nFirst you have to create a db master key and db scoped credential,\nthen you have to create an external data source and den you need to configure the external data format\n(answer given from the skillcertpro platform)","upvote_count":"2","timestamp":"1663973880.0","comment_id":"677499"},{"timestamp":"1662671400.0","comment_id":"664020","content":"1.Create a master key on the database. The master key is required to encrypt the credential secret.\n2.Create a database scoped credential for Azure blob storage; IDENTITY can be anything as it's not used.\n3.Create an external data source with CREATE EXTERNAL DATA SOURCE.\n4.Create an external file format with CREATE EXTERNAL FILE FORMAT.\n5.Create an external table pointing to data stored in Azure storage with CREATE EXTERNAL TABLE.\nRefer https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-azure-blob-storage?view=sql-server-ver15","upvote_count":"2","poster":"anks84"},{"poster":"Deeksha1234","upvote_count":"2","comment_id":"642446","comments":[{"comment_id":"642452","timestamp":"1659620760.0","upvote_count":"3","content":"1- Create a Database Encryption Key\n2 - Create a Database Scoped Credential\n3 - Create an External Data Source","poster":"Deeksha1234"}],"timestamp":"1659620460.0","content":"correct, agree with engerbrain"},{"poster":"Franz58","content":"It requires access to the DW, not to a sin\ngle file, therefore external file format is not needed here. \n1- Create a Database Encryption Key\n2 - Create a Database Scoped Credential\n3 - Create an External Data Source","upvote_count":"4","timestamp":"1658600520.0","comment_id":"635706"},{"timestamp":"1657106040.0","poster":"VM_GCP","comment_id":"627846","upvote_count":"4","comments":[{"comment_id":"645356","content":"I guess the reference should be https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated#b-create-external-data-source-to-reference-azure-data-lake-store-gen-1-or-2-using-a-service-principal","upvote_count":"1","timestamp":"1660206060.0","poster":"ads5891"}],"content":"I think, the correct reference to this question is https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated#c-create-external-data-source-to-reference-azure-data-lake-store-gen-2-using-the-storage-account-key\n\n1. Create encryption master key \n2. Create Database Scoped Credentials\n3. Create External Data Source.\n\nI see some are ref to this doc https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-azure-blob-storage?view=sql-server-ver15, but check, its only applicable to SQL Server, and not to Synapse."},{"upvote_count":"15","content":"1.Create a master key on the database. The master key is required to encrypt the credential secret.\n2.Create a database scoped credential for Azure blob storage; IDENTITY can be anything as it's not used.\n3.Create an external data source with CREATE EXTERNAL DATA SOURCE. \n4.Create an external file format with CREATE EXTERNAL FILE FORMAT.\n5.Create an external table pointing to data stored in Azure storage with CREATE EXTERNAL TABLE. \nRefer https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-azure-blob-storage?view=sql-server-ver15","poster":"metallicjade","timestamp":"1644258000.0","comment_id":"542604"},{"poster":"ANath","timestamp":"1641464280.0","comment_id":"518144","content":"Why not a asymmetric key at first? The following link says we should use one:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/sql-server-and-database-encryption-keys-database-engine?view=sql-server-ver15\n\nCan anyone clarify.","upvote_count":"2"},{"timestamp":"1641453840.0","content":"agreed with Alex","upvote_count":"1","comment_id":"518021","poster":"Teraflow"},{"content":"Why not asymmetric key at first ?","upvote_count":"1","comment_id":"509229","poster":"[Removed]","timestamp":"1640457660.0"},{"timestamp":"1640359140.0","upvote_count":"2","content":"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-azure-blob-storage?view=sql-server-ver15","comment_id":"508554","poster":"[Removed]"},{"upvote_count":"3","poster":"[Removed]","comment_id":"508553","timestamp":"1640359020.0","content":"Well, I think that answer is correct, the asymmetric key is the same thing as the master key and it should be the 1st step."},{"content":"what is the right answer?","comment_id":"501584","comments":[{"poster":"ItHYMeRIsh","upvote_count":"5","comment_id":"502993","timestamp":"1639666380.0","content":"As alexleonvalencia said,\n1.- A database scoped credential\n2.- an External data sorce\n3.- a external file format"}],"upvote_count":"1","poster":"dpBBC","timestamp":"1639504440.0"},{"timestamp":"1639438560.0","poster":"avijitd","upvote_count":"8","content":"Agree with Alex\n\n1.- A database scoped credential\n2.- an External data source\n3.- a external file format","comment_id":"500946"},{"poster":"anooja","comment_id":"498864","timestamp":"1639158960.0","content":"correct","upvote_count":"2"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0022700002.jpg"],"url":"https://www.examtopics.com/discussions/microsoft/view/67485-exam-dp-203-topic-2-question-46-discussion/","answer_ET":""}],"exam":{"name":"DP-203","isBeta":false,"lastUpdated":"12 Apr 2025","id":67,"isMCOnly":false,"isImplemented":true,"numberOfQuestions":384,"provider":"Microsoft"},"currentPage":40},"__N_SSP":true}