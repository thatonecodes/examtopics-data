{"pageProps":{"questions":[{"id":"qixTIDOKyNqa5H39q0jg","topic":"2","isMC":false,"answers_community":[],"exam_id":54,"timestamp":"2022-09-01 08:15:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04224/0011000001.jpg"],"answer_description":"Box 1: General purpose v2 with Hot access tier for blobs\nNote:\n* All the data written to storage must be retained for five years.\n* Data access charges must be minimized\nHot tier has higher storage costs, but lower access and transaction costs.\nIncorrect:\nNot Archive: Lowest storage costs, but highest access, and transaction costs.\nNot Cool: Lower storage costs, but higher access and transaction costs.\nBox 2: Storage account resource lock\nAs an administrator, you can lock a subscription, resource group, or resource to prevent other users in your organization from accidentally deleting or modifying critical resources. The lock overrides any permissions the user might have.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources","answer_images":["https://www.examtopics.com/assets/media/exam-media/04224/0011000002.jpg"],"unix_timestamp":1662012900,"question_text":"HOTSPOT -\nYou are planning an Azure Storage solution for sensitive data. The data will be accessed daily. The dataset is less than 10 GB.\nYou need to recommend a storage solution that meets the following requirements:\n✑ All the data written to storage must be retained for five years.\n✑ Once the data is written, the data can only be read. Modifications and deletion must be prevented.\n✑ After five years, the data can be deleted, but never modified.\n✑ Data access charges must be minimized.\nWhat should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/78936-exam-az-305-topic-2-question-17-discussion/","answer":"","discussion":[{"poster":"mse89","upvote_count":"146","comments":[{"upvote_count":"8","timestamp":"1662031320.0","content":"agree 100%","comment_id":"656178","comments":[{"poster":"kJigneshk","upvote_count":"1","timestamp":"1665499440.0","content":"yes you set the resources lock as read-only and delete prevention but can to for data, that is only for resources change not for in the data.","comment_id":"692193"}],"poster":"ike001"},{"timestamp":"1665058980.0","comments":[{"comment_id":"692192","poster":"kJigneshk","upvote_count":"12","content":"yes you set the resources lock as read-only and delete prevention but can to for data, that is only for resources change not for in the data.","timestamp":"1665499380.0"}],"comment_id":"687775","upvote_count":"4","poster":"webbies","content":"You can set the storage resource lock to CannotDelete and ReadOnly isnt?\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json"}],"timestamp":"1662012900.0","comment_id":"655862","content":"gpv2 hot tier, container access policy to configure a time-based retention policy for immutable storage.\nStorage account resource lock does not prevent data editing or deletion, but only the storage account deletion."},{"poster":"Gowind","upvote_count":"50","timestamp":"1662124980.0","comment_id":"657468","content":"Answer is GPv2 HOT to have frequent access :\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview\n\nAnswer is container access (immutable) policy at least at the container scope.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview"},{"timestamp":"1743586200.0","comment_id":"1422186","poster":"Ghada1","upvote_count":"1","content":"2. Container access policy\nthe second select should be container access policy, because with access policy you're not only preventing data from deletion/modification (like what you could've done using locks) but you also can set a retention policy which is a requirement in this question"},{"poster":"[Removed]","timestamp":"1731423600.0","comment_id":"1310667","upvote_count":"2","content":"WRONG\n\n1. General purpose v2 with Hot access tier\n\n2. Container access policy"},{"comment_id":"1203805","content":"i would go for gpv2 hot tier and container access policy","poster":"Lazylinux","timestamp":"1714367040.0","upvote_count":"4"},{"content":"1 Hot tier\n2 should be access policy.\nPS Locking a storage account does not protect containers or blobs within that account from being deleted or overwritten!!!","upvote_count":"2","comment_id":"1188467","timestamp":"1712118720.0","poster":"Azure2020"},{"poster":"Zein135","comment_id":"1187787","timestamp":"1712021820.0","comments":[{"poster":"Robtin","upvote_count":"1","comment_id":"1337651","content":"A Container Access Policy defines how data within a container can be accessed by managing roles and permissions.\n\nWhat it does: It controls read and write permissions for data in a container.\nLimitations: It does not prevent data from being modified or deleted. It only regulates who can access the data and what they can do with it.\nConclusion: This does not guarantee data immutability or enforce a retention period.","timestamp":"1736272980.0"}],"upvote_count":"1","content":"I think the right choice for second question is\n\"Container Access Level\"\nwe can adjust access level to read only"},{"poster":"Daychill","upvote_count":"1","timestamp":"1705894200.0","comment_id":"1128310","content":"Immutability policies can be scoped to a blob version or to a container. How an object behaves under an immutability policy depends on the scope of the policy. For more information about policy scope for each type of immutability policy, see the following sections:\n\nTime-based retention policy scope\nLegal hold scope\nDepending on the scope, you can configure both a time-based retention policy and a legal hold for a resource (container or blob version)."},{"content":"Got this on Nov. 17, 2023","poster":"nav109","timestamp":"1700476380.0","comment_id":"1075324","upvote_count":"5"},{"timestamp":"1698592560.0","content":"1. \"Hot tier\". Lower access transaction costs, meats requirement \"Data access charges must be minimized\"\n\n2. \"Container access policy\" seems to be the best one. I still struggle to find it in the documentation (only found immutable storage references)","upvote_count":"3","poster":"mark_af","comment_id":"1056909","comments":[{"timestamp":"1709124540.0","content":"1. Wrong, the cheapest tier is Archive, then Cool, then Hot. The reason why we need Hot access is because this data has to be accessed a lot on a daily basis.","comment_id":"1161628","upvote_count":"3","poster":"xRiot007"}]},{"timestamp":"1697983020.0","content":"This question appeared on my Exam today 10/22/2023\nTotal of 48 questions","comment_id":"1050696","poster":"jcxxxxx2020","upvote_count":"9","comments":[{"comment_id":"1073163","poster":"babakeyfgir","upvote_count":"1","content":"answer?","timestamp":"1700213880.0"}]},{"content":"General purpose v2 with Hot access tier and Storage resource lock.","comment_id":"1046020","timestamp":"1697547720.0","poster":"GeorgiAngelov","upvote_count":"1"},{"timestamp":"1692093720.0","upvote_count":"3","poster":"aksrav","content":"its general puprose v2 with hot tier\ncontainer access policy","comment_id":"981507"},{"upvote_count":"15","comment_id":"950768","poster":"NotMeAnyWay","content":"1. Storage Account type: c. GP v2 Hot.\n\nConsidering the data will be accessed daily, the Hot access tier is the most cost-effective for storing frequently accessed data.\n\n2. Configuration to prevent the modification and deletions: Container access policy.\n\nThe Container access policy is indeed the place to configure Azure's Immutable Blob Storage to ensure data is retained without modifications or deletions for a specified amount of time, which suits your needs. The Azure Blob Storage's Immutable Blob Storage feature provides a WORM (Write Once, Read Many) capability which aligns with your requirements perfectly.","timestamp":"1689259680.0"},{"content":"Moderator/Admins: could you please update the answer. We have a lot of consense here that the answers are General Purpose v2 Blobs + hot tier AND Container Access Policy are needed to get the desired outcome.","timestamp":"1685105220.0","poster":"sw1000","upvote_count":"9","comment_id":"907362"},{"comments":[{"upvote_count":"2","timestamp":"1682865300.0","comment_id":"885338","poster":"ZUMY","content":"To implement this solution, you can follow these steps:\n\nCreate an Azure Blob Storage account and enable the WORM feature. This can be done through the Azure Portal or via Azure CLI or PowerShell.\n\nCreate a new blob container for your sensitive data.\n\nSet the WORM policy for the container to enforce write-once-read-many access for all blobs in the container.\n\nUpload your sensitive data to the blob container.\n\nConfigure a retention period of five years for the data in the container."}],"upvote_count":"6","timestamp":"1682865240.0","poster":"ZUMY","comment_id":"885335","content":"General Purpose V2 Hot tier\nContainer access policy\n\nI would recommend using Azure hot Blob Storage with a WORM (Write Once Read Many) policy. WORM policies prevent data from being modified or deleted after it has been written, and they can be applied to individual blobs or entire containers."},{"poster":"malcubierre","content":"General Purpose V2 Hot tier\nContainer access policy","timestamp":"1679154720.0","upvote_count":"8","comment_id":"842906"},{"poster":"Stone82","comment_id":"827112","upvote_count":"5","content":"Second is Container Access Policy","timestamp":"1677775080.0"},{"comment_id":"816810","timestamp":"1676995980.0","content":"It should be Hot Tier with Container Access Policy. Storage Account Resource Lock does not prevent the data from being modified/deleted inside the container.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-protection-overview#overview-of-data-protection-options","poster":"MadPanda","upvote_count":"3"},{"comment_id":"812579","upvote_count":"1","content":"Same as Question 21.\nhttps://www.examtopics.com/discussions/microsoft/view/95594-exam-az-305-topic-2-question-21-discussion","poster":"zellck","timestamp":"1676688480.0"},{"content":"-Gen2 Hot Access\n-Container access policy (immutable storage time-based retention policy)\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-time-based-retention-policy-overview#container-level-policy-scope\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-policy-configure-container-scope?tabs=azure-portal\n\nNote:\nA storage account resource lock will apply restrictions to the control plane only (i.e. no changes to the storage resource properties/settings). It would not prevent changes to the data plane.\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources#understand-scope-of-locks\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources#considerations-before-applying-your-locks","comment_id":"804635","poster":"[Removed]","upvote_count":"1","timestamp":"1676052840.0"},{"timestamp":"1675841280.0","comment_id":"801734","upvote_count":"1","poster":"orionduo","content":"Answer should be: Hot&Container access policy \nRequirement: The data will be accessed daily\nHot access tier is optimized for storing data that is accessed frequently\nImmutable storage for Azure Blob Storage enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data cannot be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes."},{"poster":"OPT_001122","upvote_count":"6","comment_id":"790485","content":"Box 1: General purpose v2 with Hot access tier for blobs\nBox 2: container access policy","timestamp":"1674899340.0"},{"content":"Container access level and container access policy can be used to control access to the container but they don't prevent modification and deletion of the data itself.\n\nStorage account resource lock provides a way to prevent accidental deletion of resources at the subscription level and it's the best way to prevent modifications and deletions on the data once it's written to storage.","upvote_count":"2","comment_id":"777834","poster":"uacanillo","timestamp":"1673881500.0"},{"comments":[{"upvote_count":"1","timestamp":"1670365020.0","content":"https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-policy-configure-container-scope?tabs=azure-portal","poster":"CineZorro824","comment_id":"737247"}],"upvote_count":"1","comment_id":"737246","content":"Data lock should be Container Access policy, which is the scope of the Immutable Policy on a Storage Account.\nStorage Account resource lock is wrong, this only prevents the Storage Account from being deleted, it says nothing about data modification.","poster":"CineZorro824","timestamp":"1670365020.0"},{"timestamp":"1668702000.0","upvote_count":"3","content":"Agree with C & B\n\nC: Hot due to request in question.\nB: See url and extraction from URL;\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json\n\nA cannot-delete lock on a storage account doesn't protect account data from deletion or modification. It only protects the storage account from deletion. If a request uses data plane operations, the lock on the storage account doesn't protect blob, queue, table, or file data within that storage account. If the request uses control plane operations, however, the lock protects those resources.\n\nIf a request uses File Shares - Delete, for example, which is a control plane operation, the deletion fails. If the request uses Delete Share, which is a data plane operation, the deletion succeeds. We recommend that you use a control plane operation.\n\nA read-only lock on a storage account doesn't prevent its data from deletion or modification. It also doesn't protect its blob, queue, table, or file data.","poster":"RandomNickname","comment_id":"720643"},{"upvote_count":"2","content":"The second should be Container Access policy\n\nShould use immutable storage for Azure Blob Storage. Time-based retention Policy\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview","timestamp":"1668310200.0","comment_id":"717072","poster":"A_GEE"},{"content":"Both C&C and C&B are suitable for this","comments":[{"content":"yes you set the resources lock as read-only and delete prevention but can to for data, that is only for resources change not for in the data.","timestamp":"1665499500.0","comment_id":"692195","upvote_count":"2","poster":"kJigneshk"}],"poster":"M_r_Cloud","timestamp":"1665230700.0","comment_id":"689274","upvote_count":"1"},{"comment_id":"662657","content":"GPv2 hot and container access policy\n\nImmutable storage for Azure Blob Storage enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data cannot be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes. Immutability policies include time-based retention policies and legal holds.\n\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/immutable-policy-configure-container-scope?tabs=azure-portal","timestamp":"1662565440.0","poster":"scottims","upvote_count":"3"},{"content":"clearly HOT and \nstorage account policy ( not resource lock )","poster":"codingdown","comment_id":"661254","upvote_count":"5","timestamp":"1662469920.0"}],"answer_ET":"","question_id":86},{"id":"nm1uToF0GMgN43bzP7Id","question_images":["https://www.examtopics.com/assets/media/exam-media/04224/0011200001.jpg"],"answer_ET":"","question_id":87,"unix_timestamp":1662306840,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04224/0011200002.jpg"],"topic":"2","exam_id":54,"answer":"","discussion":[{"upvote_count":"44","timestamp":"1662350700.0","comment_id":"659714","content":"Azure Synapse Analytics SQL pool only support 128 concurrent queries:\n\"A maximum of 128 concurrent queries will execute and remaining queries will be queued\"\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-service-capacity-limits\nAzure Sql hyperscale have read replica... and supports up to 100TB data size.\nSo I think the correct answer should be Hyperscale","poster":"Snownoodles"},{"content":"1. Data store for the ingestion data: b. Azure Data Lake Storage Gen2. \n\nAzure Data Lake Storage Gen2 is designed for big data analytics, it combines the power of a high-performance file system with massive scale and economy to help you speed up your big data analytics. It allows the data to be organized in directories by date and time. \n\n2. Data store for the data warehouse: c. Azure SQL Database Hyperscale. \n\nAzure SQL Database Hyperscale is a highly scalable service tier that is designed to provide high performance, and supports up to 100 TB of data. The Hyperscale service tier in Azure SQL Database is the newest service tier in the vCore-based purchasing model. This service tier is a highly scalable storage and compute performance tier that leverages the Azure architecture to scale out the storage and compute resources for an Azure SQL Database substantially beyond the limits available for the General Purpose and Business Critical service tiers.","upvote_count":"33","timestamp":"1689260100.0","comment_id":"950773","poster":"NotMeAnyWay"},{"comment_id":"1310671","timestamp":"1731423900.0","content":"CORRECT","upvote_count":"2","poster":"[Removed]"},{"upvote_count":"2","content":"Data store for the ingested data: Azure Data Lake Storage Gen2\nData store for the data warehouse: Azure Synapse Analytics dedicated SQL pools","timestamp":"1725885420.0","poster":"Teerawee","comment_id":"1280873"},{"poster":"23169fd","timestamp":"1719001020.0","comments":[{"timestamp":"1725378060.0","poster":"josola","content":"The question's number of concurrent queries is between 200-300. \nAzure Synapse Analytics supports up to 128 concurrent queries, which is well below the question requirement. Then Azure Synapse is not the best option here.","comment_id":"1277699","upvote_count":"1"},{"poster":"23169fd","upvote_count":"1","timestamp":"1719001140.0","content":"Azure SQL Database Hyperscale is best suited for:\n\nLarge transactional databases.\nWorkloads requiring high performance and scalability for OLTP (Online Transaction Processing).\nAzure Synapse Analytics Dedicated SQL Pools is best suited for:\n\nLarge-scale data warehousing.\nComplex analytical queries and reporting.\nScenarios requiring high concurrency for read operations.","comment_id":"1234909"}],"content":"Data store for the ingested data:\n\nAzure Data Lake Storage Gen2: This service is optimized for big data analytics and can organize data by date and time. It also supports hierarchical namespace which is useful for efficient data organization.\nData store for the data warehouse:\n\nAzure Synapse Analytics dedicated SQL pools: This service is designed to handle large-scale data warehousing, can store 50 TB of relational data, and supports high concurrency for read operations, making it suitable for your needs.","comment_id":"1234907","upvote_count":"1"},{"content":"For the data store for ingested data, Azure Data Lake Storage Gen2 is recommended. It is designed to handle high volumes of data and allows you to organize data in directories by date and time. It also supports direct querying of stored data. \n\nFor the data warehouse, Azure Synapse Analytics Dedicated SQL Pools is recommended. It is designed to handle large volumes of relational data (up to petabytes) and supports a high number of concurrent read operations. It also allows for data transformation into summarized tables. Azure Synapse Analytics integrates seamlessly with Azure Data Lake Storage Gen2, providing an end-to-end data solution.","upvote_count":"1","poster":"Chenn","timestamp":"1714806060.0","comments":[{"poster":"josola","comment_id":"1277700","upvote_count":"1","content":"The question's number of concurrent queries is between 200-300. \nAzure Synapse Analytics supports up to 128 concurrent queries, which is well below the question requirement. Then Azure Synapse is not the best option here.","timestamp":"1725378120.0"}],"comment_id":"1206370"},{"poster":"varinder82","timestamp":"1710906540.0","comment_id":"1177901","upvote_count":"1","content":"Final Answer:\n1. Azure Data Lake Storage Gen2.\n2. Azure SQL Database Hyperscale."},{"upvote_count":"15","content":"Was on my Exam Today - 4th Jan 2024","comment_id":"1113556","timestamp":"1704364800.0","poster":"peterp007"},{"comment_id":"1077922","upvote_count":"1","timestamp":"1700696400.0","content":"For the ingested data, I recommend using zure Data Lake Storage Gen2. \nIt is a highly scalable and cost-effective data lake solution for big data analytics. \n\nFor the data warehouse, I recommend using Azure Synapse Analytics (formerly SQL Data Warehouse). It is an analytics service that brings together enterprise data warehousing and Big Data analytics. It gives you the freedom to query data on your terms, using either serverless or provisioned resources at scale. It can store 50 TB of relational data and support between 200 and 300 concurrent read operations.","poster":"Paul_white"},{"content":"What would be right answer ?","comment_id":"976056","poster":"mehak2020","upvote_count":"1","timestamp":"1691530860.0"},{"poster":"Bigbluee","content":"If You dont know what to choose, choose cheapest one or \"more cost safe\" so IMO, Azure SQL Database Hyperscale is the answer even if Synapse meets requirements.","upvote_count":"3","comment_id":"862274","timestamp":"1680711060.0"},{"upvote_count":"2","comment_id":"847301","timestamp":"1679503920.0","content":"1. Data store for the ingested data: B. Azure Data Lake Storage Gen2\nAzure Data Lake Storage Gen2 is designed for big data analytics workloads and supports organizing data in directories by date and time, as well as hierarchical namespace. It also allows stored data to be queried directly and is well-integrated with Azure Event Hubs.\n\n2. Data store for the data warehouse: C. Azure SQL Database Hyperscale is an alternative option for the data store for the data warehouse. It is a highly scalable service tier for single databases within Azure SQL Database that can auto-scale up to 100 TB. It supports a large number of concurrent connections and offers rapid scaling capabilities.","poster":"NotMeAnyWay"},{"upvote_count":"3","poster":"Helice","timestamp":"1679243520.0","comment_id":"843952","content":"The second is hyperscale: https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale-frequently-asked-questions-faq?view=azuresql#how-can-i-choose-between-azure-synapse-analytics-and-azure-sql-database-hyperscale- based on Microsoft docs. For this type of scenarios, Hyperscale works"},{"content":"Hyperscale is OLTP not OLAP (Data warehouse). Synapse is a DW","poster":"Helice","timestamp":"1679243160.0","comment_id":"843943","upvote_count":"3"},{"upvote_count":"4","timestamp":"1677408900.0","content":"1. Azure Data Lake Storage Gen2\n2. Azure SQL DB Hyperscale\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction\nAzure Data Lake Storage Gen2 is a set of capabilities dedicated to big data analytics, built on Azure Blob Storage.\n\nData Lake Storage Gen2 converges the capabilities of Azure Data Lake Storage Gen1 with Azure Blob Storage. For example, Data Lake Storage Gen2 provides file system semantics, file-level security, and scale. Because these capabilities are built on Blob storage, you'll also get low-cost, tiered storage, with high availability/disaster recovery capabilities.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql#what-are-the-hyperscale-capabilities\nThe Hyperscale service tier in Azure SQL Database provides the following additional capabilities:\n- Support for up to 100 TB of database size.","poster":"zellck","comment_id":"822274"},{"content":"I think the answer should be Azure Synapse Analytics SQLPool \nbcz: once data is stored in ADLS in directories, data needs to be queried directly and transformed and stored in tables.\nSynapse has that capability. ???","upvote_count":"2","poster":"abxc","timestamp":"1677180600.0","comment_id":"819634"},{"content":"synapse vs hyperscale which is the better answer?","comment_id":"806012","upvote_count":"1","timestamp":"1676181900.0","poster":"Putra19"},{"timestamp":"1676104680.0","comment_id":"805084","content":"Box 1: Azure Data Lake Storage Gen2\nBox 2: Azure SQL Database Hyperscale","upvote_count":"5","poster":"OPT_001122"},{"comment_id":"797064","timestamp":"1675430640.0","comments":[{"poster":"np2021","content":"I agree chatgpt answers need to stop. It is not as smart, or comprehensive at researching a relevant answer as people make out.","comment_id":"807816","timestamp":"1676323800.0","upvote_count":"7"},{"poster":"jecawi9630","upvote_count":"10","comments":[{"poster":"Lu5ck","comment_id":"799896","upvote_count":"1","content":"I forgot to add that Azure Stream Analytics is also in preview state for Event Hub to Azure SQL Database. So yea, I think Azure Synapse Analytics is the better answer.","timestamp":"1675697100.0"},{"timestamp":"1675696500.0","comment_id":"799890","content":"I am sorry, I copied the wrong explanation. The reality is both solutions are viable at first glance but there is indeed a key difference. The solution must ingest the data DIRECTLY. We cannot ingest the data and transform it directly into Azure SQL Database, that will require the use of a medium like Azure Stream Analytics. However, we can indeed do it directly with Azure Synapse Analytics.","poster":"Lu5ck","upvote_count":"3"},{"poster":"Lu5ck","upvote_count":"1","timestamp":"1675697820.0","comment_id":"799907","content":"Ah, in case you asked. There is a difference between concurrent queries and concurrent session. The scenario require concurrent read aka concurrent query aka concurrent session. The queues will only kick in after the session is filled. Azure Synapse Analytics can do up to 1000 concurrent session and can do rowstore up to 60 TB. It actually fit all requirements at first glance. So I am unsure why nobody make that comparisons and why they think SQL Database Hyperscale is better fit."},{"poster":"AdventureChick","upvote_count":"2","timestamp":"1692560580.0","comment_id":"986041","content":"jecawi9630 - exactly. Which is why humans still need to know how to research and think for themselves and ChatGPT isn't the gospel."}],"content":"Please stop with your chatgpt answers. It’s not as smart as you think when it comes to questions like this. If you ask a follow up question saying “bit isn’t this the correct answer” it will immediately say “oh yeah you’re right” instead of defending why the answer it chose was correct.","timestamp":"1675522200.0","comment_id":"798063"}],"content":"According to chatgpt,\nBoth Azure Data Lake Storage Gen2 and Azure Blob storage can store json but Azure Data Lake Storage Gen2 does a better job at it for this scenario where analytics is involve.\n\nAzure Synapse Analytics dedicated SQL pools is a better answer because the concurrent read limit is not established for Azure SQL Database Hyperscale. Furthermore, Azure Synapse Analytics dedicated SQL pools is optimized to for this purpose. 50TB is below the 100TB limit as well.\n\nThus the answer is\nAzure Data Lake Storage Gen2\nAzure Synapse Analytics dedicated SQL pools","poster":"Lu5ck","upvote_count":"2"},{"upvote_count":"12","poster":"OrangeSG","content":"Box 2 shall be 'Azure SQL Database Hyperscale'\nKeyword are ‘data warehouse’, ‘50 TB of relational data’, ‘200 and 300 concurrent read’\n\nAzure SQL Database Hyperscale FAQ\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale-frequently-asked-questions-faq\n\nHow can I choose between Azure Synapse Analytics and Azure SQL Database Hyperscale?\n\nIf you are currently running interactive analytics queries using SQL Server as a data warehouse, Hyperscale is a great option because you can host small and mid-size data warehouses (such as a few TB up to 100 TB) at a lower cost, and you can migrate your SQL Server data warehouse workloads to Hyperscale with minimal T-SQL code changes.\nIf you are running data analytics on a large scale with complex queries and sustained ingestion rates higher than 100 MB/s, or using Parallel Data Warehouse (PDW), Teradata, or other Massively Parallel Processing (MPP) data warehouses, Azure Synapse Analytics may be the best choice.","comment_id":"747186","timestamp":"1671194760.0"},{"content":"why not comos DB SQL API?","poster":"infimagine","timestamp":"1668352440.0","comment_id":"717392","upvote_count":"8"},{"comments":[{"poster":"FabrityDev","comment_id":"775486","timestamp":"1673705460.0","upvote_count":"3","content":"You kinda miss the point. Read carefully:\n \"Allow stored data to be queried directly, transformed into summarized tables, and then stored in a data warehouse.\"\nIt basically means that the first solution should read JSON data and transform it into tables. The second solution is a data warehous which is supposed to store already transformed data in table format. Therefore SQL is an optimal solution."},{"upvote_count":"1","content":"Correction, SQL Server can also process JSON:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store#create-the-copy-statement","poster":"jp_mcgee","comment_id":"716144","timestamp":"1668178200.0"}],"upvote_count":"1","comment_id":"714684","content":"WRONG!!\nBox 2 only has only one option which is a data warehouse that can read unstructured data like JSON....Synapse is a data warehouse that can read JSON.","timestamp":"1668007140.0","poster":"jp_mcgee"},{"content":"The given answer is correct","timestamp":"1667986200.0","comment_id":"714429","upvote_count":"3","poster":"azuredemo2022three"},{"poster":"Akandeopo","timestamp":"1662306840.0","comment_id":"659370","content":"I think B should be Azure Synapse Analytics SQL pool and not SQL Database Hyperscale","upvote_count":"7"}],"answer_description":"Box 1: Azure Data Lake Storage Gen2\nAzure Data Explorer integrates with Azure Blob Storage and Azure Data Lake Storage (Gen1 and Gen2), providing fast, cached, and indexed access to data stored in external storage. You can analyze and query data without prior ingestion into Azure Data Explorer. You can also query across ingested and uningested external data simultaneously.\nAzure Data Lake Storage is optimized storage for big data analytics workloads.\nUse cases: Batch, interactive, streaming analytics and machine learning data such as log files, IoT data, click streams, large datasets\nBox 2: Azure SQL Database Hyperscale\nAzure SQL Database Hyperscale is optimized for OLTP and high throughput analytics workloads with storage up to 100TB.\nA Hyperscale database supports up to 100 TB of data and provides high throughput and performance, as well as rapid scaling to adapt to the workload requirements. Connectivity, query processing, database engine features, etc. work like any other database in Azure SQL Database.\nHyperscale is a multi-tiered architecture with caching at multiple levels. Effective IOPS will depend on the workload.\nCompare to:\nGeneral purpose: 500 IOPS per vCore with 7,000 maximum IOPS\nBusiness critical: 5,000 IOPS with 200,000 maximum IOPS\nIncorrect:\n* Azure Synapse Analytics Dedicated SQL pool.\n\nMax database size: 240 TB -\nA maximum of 128 concurrent queries will execute and remaining queries will be queued.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-explorer/data-lake-query-data https://docs.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-service-capacity-limits","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/80098-exam-az-305-topic-2-question-18-discussion/","timestamp":"2022-09-04 17:54:00","question_text":"HOTSPOT -\nYou are designing a data storage solution to support reporting.\nThe solution will ingest high volumes of data in the JSON format by using Azure Event Hubs. As the data arrives, Event Hubs will write the data to storage. The solution must meet the following requirements:\n✑ Organize data in directories by date and time.\n✑ Allow stored data to be queried directly, transformed into summarized tables, and then stored in a data warehouse.\n✑ Ensure that the data warehouse can store 50 TB of relational data and support between 200 and 300 concurrent read operations.\nWhich service should you recommend for each type of data store? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answers_community":[]},{"id":"3DO4123fiLiLE6LJQj4n","isMC":true,"question_text":"You have an app named App1 that uses an on-premises Microsoft SQL Server database named DB1.\n\nYou plan to migrate DB1 to an Azure SQL managed instance.\n\nYou need to enable customer managed Transparent Data Encryption (TDE) for the instance. The solution must maximize encryption strength.\n\nWhich type of encryption algorithm and key length should you use for the TDE protector?","question_id":88,"question_images":[],"unix_timestamp":1672908780,"exam_id":54,"answer_images":[],"answer_ET":"A","answer_description":"","answer":"A","discussion":[{"poster":"NotMeAnyWay","timestamp":"1679504160.0","content":"Selected Answer: A\nA. RSA 3072\n\nRSA 3072 provides a higher level of encryption strength compared to RSA 2048. While RSA 4096 offers even stronger encryption, it is not supported by Azure SQL Database and Azure SQL Managed Instance for TDE protectors.\n\nBy choosing RSA 3072 for the TDE protector, you ensure strong encryption for your Azure SQL Managed Instance while complying with the platform's requirements. This will help protect sensitive data and maintain compliance with relevant security standards and regulations.","comment_id":"847308","comments":[{"content":"Correct, Reference:\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#:~:text=TDE%20protector%20can%20only%20be%20an%20asymmetric%2C%20RSA%2C%20or%20RSA%20HSM%20key.%20The%20supported%20key%20lengths%20are%202048%20bits%20and%203072%20bits.","timestamp":"1709949900.0","upvote_count":"3","poster":"chair123","comment_id":"1169208"}],"upvote_count":"27"},{"poster":"wdjonz","upvote_count":"9","content":"The Answer is A and here is why...\nPer https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?view=azuresql&tabs=azure-portal, \n\nif the TDE uses the system managed key, it uses a built in certificate for encryption, hence AES 256\nif the TDE uses a customer managed key, then it uses an asymmetric RSA key at 2048 or 3072\n\nAnd since the question says TDE is using the customer managed key... the answer is A Viola!","timestamp":"1683911580.0","comment_id":"896090"},{"content":"Selected Answer: A\nTDE protector can only be an asymmetric, RSA, or RSA HSM key. The supported key lengths are 2048 bits and 3072 bits.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector","poster":"serbanvadi","upvote_count":"1","comment_id":"1411470","timestamp":"1743190200.0"},{"timestamp":"1741311360.0","comment_id":"1366104","upvote_count":"1","poster":"KA2023","content":"Selected Answer: B\nRSA (2048, 3072, 4096) is an asymmetric encryption algorithm used primarily for key encryption and signing, not for bulk data encryption like TDE.\nThe correct answer is still B. AES 256"},{"upvote_count":"1","content":"Selected Answer: A\nA is correct","comment_id":"1310674","poster":"[Removed]","timestamp":"1731424080.0"},{"upvote_count":"1","timestamp":"1721981220.0","comment_id":"1255581","poster":"GabiBT","content":"Actualmente RSA 4096 ya es compatible con Azure SQL Database"},{"poster":"peterp007","upvote_count":"9","content":"Was on my exam today - 4th Jan 2024","comment_id":"1113557","timestamp":"1704364860.0"},{"content":"it was a exam Question","poster":"babakeyfgir","comment_id":"1077314","timestamp":"1700652420.0","upvote_count":"6"},{"timestamp":"1694793480.0","content":"Selected Answer: A\nRSA 3072, because is custom managed","comment_id":"1008570","upvote_count":"2","poster":"Elecktrus"},{"timestamp":"1685346120.0","upvote_count":"3","comment_id":"909149","content":"Selected Answer: A\nThere are a lot of confusing elements in this question.\nAt first it mentions on-premise SQL Server, which would allow AES or RSA ...\nHowever, the system is to be migrated over to Azure.\nAnd here the requirements for customer managed TDE are pretty clear and are listed here:\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector\n\nAES can be enabled as an additional Infrastructure encryption to have two layers, but that was not the question here.","poster":"sw1000"},{"timestamp":"1683910080.0","comment_id":"896075","content":"https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?source=recommendations&view=azuresql#requirements-for-configuring-tde-protector\n\nA. 3072","upvote_count":"3","poster":"Tr619899"},{"timestamp":"1676790600.0","comment_id":"813808","poster":"zellck","content":"Selected Answer: A\nA is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector\nTDE protector can only be an asymmetric, RSA, or RSA HSM key. The supported key lengths are 2048 bytes and 3072 bytes.","upvote_count":"5"},{"poster":"dagomo","upvote_count":"4","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector","comment_id":"795944","timestamp":"1675332840.0"},{"poster":"VBK8579","comment_id":"795772","content":"Selected Answer: A\nAnswer A because Azure SQL Database and Azure Synapse Analytics support RSA 3072-bit key length for customer managed TDE with Bring Your Own Key (BYOK) configurations","timestamp":"1675316100.0","upvote_count":"2"},{"poster":"bigz2021","timestamp":"1675222080.0","content":"A. RSA 3072\n( TDE protector can only be an asymmetric, RSA, or RSA HSM key. The supported key lengths are 2048 bytes and 3072 bytes.)","comment_id":"794842","upvote_count":"4"},{"poster":"OPT_001122","upvote_count":"4","content":"Selected Answer: A\nA. RSA 3072","comment_id":"790552","timestamp":"1674906060.0"},{"comment_id":"789669","timestamp":"1674829020.0","poster":"Liveroso","upvote_count":"3","comments":[{"poster":"study_for_azure","comment_id":"801339","upvote_count":"5","content":"Per following contents in \n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector\n\nTo provide Azure SQL customers with two layers of encryption of data at rest, infrastructure encryption (using AES-256 encryption algorithm) with platform managed keys is being rolled out. This provides an addition layer of encryption at rest along with TDE with customer-managed keys, which is already available.\n\nASE is platform managed key, this question is asking for customer managed keys, for now only RSA is qualified.","timestamp":"1675798080.0"}],"content":"Selected Answer: B\nThe answer is AES 256\n\nTransparent Data Encryption (TDE) in Azure SQL Managed Instance uses the Advanced Encryption Standard (AES) algorithm to encrypt the data stored in the database and its backups. The AES algorithm is a symmetric encryption algorithm and it supports key lengths of 128, 192, and 256 bits. Among these, AES 256 provides the highest encryption strength and is considered the most secure option for TDE. Therefore, you should use AES 256 for the TDE protector.\n\nCheck MS docs: https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?view=azuresql&tabs=azure-portal"},{"upvote_count":"2","content":"Selected Answer: A\nOnly RSA 3072 and RSA 2048 are supported for TDE protector\nmaximum encryption possible is RSA 3072\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector","comments":[{"content":"Check link he is right","poster":"_punky_","comment_id":"1301639","upvote_count":"1","timestamp":"1729613580.0"},{"comment_id":"789672","poster":"Liveroso","upvote_count":"1","timestamp":"1674829260.0","content":"The information provided is not accurate. Transparent Data Encryption (TDE) in Azure SQL Managed Instance uses the Advanced Encryption Standard (AES) algorithm to encrypt the data stored in the database and its backups. AES algorithm is a symmetric encryption algorithm, it supports key lengths of 128, 192, and 256 bits. Among these, AES 256 provides the highest encryption strength and is considered the most secure option for TDE.\nRSA is not used for TDE. RSA is an asymmetric encryption algorithm, it is used in many different encryption scenarios, not just for TDE.\nTherefore, you should use AES 256 for the TDE protector."}],"timestamp":"1674397920.0","poster":"armpro","comment_id":"784390"},{"comment_id":"770728","poster":"RandomNickname","timestamp":"1673287500.0","content":"From what I can find, I agree with A, RSA 3072 maximum encryption.\nAES256 for built-in cert.\nAs per below URL, with SQL MI customer managed key \nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql\n\nIt's not RSA4096 since that's for storage encryption as per below;\nhttps://learn.microsoft.com/en-us/azure/storage/common/customer-managed-keys-overview\n&\nhttps://learn.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key\n&\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/disk-encryption","upvote_count":"2"},{"comment_id":"767382","content":"Selected Answer: A\nagree with A","timestamp":"1672990980.0","upvote_count":"4","poster":"mVic"},{"comment_id":"766957","content":"Selected Answer: B\nAES 256","poster":"[Removed]","timestamp":"1672944900.0","upvote_count":"2"},{"poster":"maku067","timestamp":"1672936560.0","comment_id":"766837","comments":[{"timestamp":"1673035380.0","poster":"maku067","upvote_count":"1","comment_id":"768041","content":"only RSA3072"}],"upvote_count":"1","content":"AES 256 + RSA 4096 rather."},{"comment_id":"766824","timestamp":"1672935840.0","poster":"jage01","content":"Selected Answer: A\nRequirements for configuring TDE protector\nTDE protector can only be an asymmetric, RSA, or RSA HSM key. The supported key lengths are 2048 bytes and 3072 bytes.\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql","upvote_count":"3"},{"content":"Selected Answer: A\nTDE 3072\nTDE protector can only be an asymmetric, RSA, or RSA HSM key. The supported key lengths are 2048 bytes and 3072 bytes. The key activation date (if set) must be a date and time in the past.","poster":"Clarkszw","comments":[{"poster":"Patchfox","upvote_count":"1","content":"I agree","timestamp":"1673006340.0","comment_id":"767596"}],"timestamp":"1672926840.0","comment_id":"766681","upvote_count":"2"},{"upvote_count":"1","comment_id":"766518","poster":"70mach1","timestamp":"1672913880.0","content":"After further research I found this document https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql\n\nThat talks about customer managed keys which at the end state that RSA 2046 and RSA 3072 are supported. SO in light of this information and the way the questions is worded I agree with A and being the correct answer."},{"poster":"70mach1","upvote_count":"1","comment_id":"766511","content":"Selected Answer: B\nBased on this document https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?view=azuresql&tabs=azure-portal\nThe correct answer should be B","timestamp":"1672913520.0"},{"upvote_count":"1","content":"Selected Answer: A\nRequirements for configuring TDE protector:\nTDE protector can only be an asymmetric, RSA, or RSA HSM key. The supported key lengths are 2048 bytes and 3072 bytes.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector","poster":"jose","timestamp":"1672908780.0","comment_id":"766410"}],"answers_community":["A (90%)","10%"],"url":"https://www.examtopics.com/discussions/microsoft/view/93991-exam-az-305-topic-2-question-19-discussion/","choices":{"A":"RSA 3072","C":"RSA 4096","B":"AES 256","D":"RSA 2048"},"topic":"2","timestamp":"2023-01-05 09:53:00"},{"id":"yfXpD7sQgZwqDGsEbgJ3","exam_id":54,"answers_community":["BC (98%)","2%"],"timestamp":"2021-12-10 07:27:00","question_images":[],"answer":"BC","choices":{"A":"an Azure Logic Apps integration account","B":"an Azure Import/Export job","D":"an Azure Analysis services On-premises data gateway","E":"an Azure Batch account","C":"Azure Data Factory"},"answer_description":"","answer_ET":"BC","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/67513-exam-az-305-topic-2-question-2-discussion/","unix_timestamp":1639117620,"discussion":[{"poster":"Eltooth","timestamp":"1639117620.0","content":"Selected Answer: BC\nB & C are correct","comments":[{"poster":"Eltooth","timestamp":"1639629480.0","content":"https://docs.microsoft.com/en-gb/azure/storage/blobs/storage-blobs-introduction#move-data-to-blob-storage","upvote_count":"13","comment_id":"502624"}],"upvote_count":"37","comment_id":"498343"},{"timestamp":"1685093580.0","comment_id":"907245","content":"Selected Answer: BC\nA. an Azure Logic Apps integration account\nno, this is an integration service with visual flows with If-Then style logic. It does not support a way to import data from on-premise to blobstorage\n\nB. an Azure Import/Export job\nAgree, with other people here. \n\nC. Azure Data Factory\nAgree, is a way of importing data, but looking at 500GB it is a bit of overkill\n\nD. an Azure Analysis services On-premises data gateway\nnot a data import option\n\nE. an Azure Batch account\nIs part of Azure Batch service and involve HPC job scheduling etc. but is not a way of importing or exporting data from on-premise to Azure\n\nNote:\nFor 500GB we would probably use AzCopy instead.\nIf it was a Typo and actually 500TB we would use Azure Data Box Heavy or maybe the Azure Import/Export Service if you provide your own drives.","upvote_count":"21","poster":"sw1000"},{"poster":"[Removed]","comment_id":"1310145","upvote_count":"1","timestamp":"1731335400.0","content":"Selected Answer: BC\nB & C are correct"},{"timestamp":"1730807100.0","upvote_count":"1","poster":"Thanveer","comment_id":"1307349","content":"B. an Azure Import/Export job\nC. Azure Data Factory"},{"timestamp":"1719487440.0","comment_id":"1238103","poster":"Thaitanium","upvote_count":"1","content":"B and C"},{"upvote_count":"1","poster":"23169fd","timestamp":"1718988540.0","content":"Selected Answer: BC\nB. an Azure Import/Export job\n\nWhy: You can use the Azure Import/Export service to securely transfer large amounts of data to Azure Blob Storage by shipping hard drives to an Azure data center.\nC. Azure Data Factory\n\nWhy: Azure Data Factory can be used to create a data pipeline that moves files from on-premises to Azure Blob Storage, enabling automated and scheduled transfers.","comment_id":"1234657","comments":[{"poster":"23169fd","timestamp":"1718988540.0","comment_id":"1234658","content":"Why Not Other Options:\nA. Azure Logic Apps integration account: Designed for integrating workflows and not typically used for bulk data transfer.\nD. Azure Analysis Services On-premises data gateway: Used for accessing on-premises data sources from Azure Analysis Services, not for transferring files to Blob Storage.\nE. Azure Batch account: Intended for running large-scale parallel and batch compute jobs, not for transferring files to Blob Storage.","upvote_count":"1"}]},{"upvote_count":"1","content":"Selected Answer: BC\nGiven answer is correct","poster":"Lazylinux","timestamp":"1714361580.0","comment_id":"1203786"},{"timestamp":"1705854600.0","upvote_count":"4","content":"appeared in Exam 01/2024","comment_id":"1127939","poster":"JimmyYop"},{"comment_id":"1091168","content":"Well B & C seem to be the answers. For B, though windows 2016 is NOT a supported version based on following link.\n\nhttps://learn.microsoft.com/en-us/azure/import-export/storage-import-export-requirements","comments":[{"poster":"TomdeBom","comment_id":"1150068","timestamp":"1707907320.0","content":"I think those OS requirements where only meant to describe older versions of Windows that are still support (I know, this is bad documentation form MS part, but MS Learn is far from perfect, documentation wise.)\n\nThe support is about the waimportexport.exe tool used.\n\nhttps://learn.microsoft.com/en-us/previous-versions/azure/storage/common/storage-import-export-tool-preparing-hard-drives-import#requirements-for-waimportexportexe\n\nstates Windows 7, Windows Server 2008 R2, or a newer Windows operating system are supported!","upvote_count":"1"}],"timestamp":"1702050960.0","upvote_count":"1","poster":"BShelat"},{"poster":"nav109","comment_id":"1075302","content":"Got this on Nov. 17, 2023","timestamp":"1700474520.0","upvote_count":"4"},{"timestamp":"1695331080.0","comment_id":"1013397","content":"Correct Answer - B & C: Azure Import/Export & Azure Data Factory\n \nAzure Import/Export: \n- This is used for transferring large amounts of data to and from Azure Blob, File, and Disk storage using physical hard drives. It would be suitable for transferring 500 GB of data. \n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-service\n\nAzure Data Factory:\n- Azure Data Factory is a cloud-based data integration service that can move and integrate data from various sources to various destinations. It would be suitable for copying files from Server1 to Blob Storage.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/introduction","upvote_count":"2","poster":"stonwall12"},{"timestamp":"1694961480.0","comment_id":"1009854","poster":"memo454","content":"This question is on today's exam.\nThe exam is easier than AZ-104.","upvote_count":"5"},{"comment_id":"999099","poster":"iamhyumi","upvote_count":"4","timestamp":"1693892580.0","content":"Got this on Sept. 5, 2023"},{"comment_id":"907498","poster":"lvz","upvote_count":"2","content":"ok, I will go with ADF, however I dont see question mentioning the connectivity between on-prem and Azure AD. I think ADF can only be used when on-prem is connected with Azure AD.","timestamp":"1685118360.0"},{"upvote_count":"5","content":"Selected Answer: BC\nB. an Azure Import/Export job\nC. Azure Data Factory\n\nB. Azure Import/Export job: This service allows you to securely import or export large amounts of data to or from Azure Blob Storage by shipping hard disk drives to an Azure data center. You can use the Azure Import/Export service to transfer the company files from your on-premises server to the Azure Blob Storage account.\n\nC. Azure Data Factory: It is a cloud-based data integration service that enables you to create, schedule, and manage data pipelines. You can create a pipeline in Azure Data Factory to copy data from your on-premises file server to Azure Blob Storage. You will need to use a Self-hosted Integration Runtime installed on your on-premises server to facilitate the data movement between your on-premises server and Azure Blob Storage.","timestamp":"1679413980.0","comment_id":"846129","poster":"NotMeAnyWay"},{"comment_id":"822798","content":"This was a question was on my exam today (2/26/23) - Scored 844\nI agree with this answer","upvote_count":"6","poster":"memyself2","timestamp":"1677435420.0"},{"upvote_count":"2","timestamp":"1677420420.0","comment_id":"822493","poster":"ukivanlamlpi","content":"Selected Answer: BE\nfiles is not fit for data factory","comments":[{"upvote_count":"2","poster":"AdventureChick","timestamp":"1695565560.0","content":"Data Factory can move files. It isn't just for DBs. I accidentally upvoted this when I went to click reply.","comment_id":"1015890"}]},{"comment_id":"813809","timestamp":"1676790720.0","upvote_count":"3","content":"Selected Answer: BC\nBC is the answer.\n\nhttps://learn.microsoft.com/en-gb/azure/storage/blobs/storage-blobs-introduction#move-data-to-blob-storage\nA number of solutions exist for migrating existing data to Blob Storage:\n- Azure Data Factory supports copying data to and from Blob Storage by using the account key, a shared access signature, a service principal, or managed identities for Azure resources. \n- The Azure Import/Export service provides a way to import or export large amounts of data to and from your storage account using hard drives that you provide.","poster":"zellck"},{"comment_id":"802964","poster":"totalz","upvote_count":"1","timestamp":"1675930320.0","comments":[{"timestamp":"1675932420.0","upvote_count":"4","content":"I mean B is possible, but a really stupid solution unless there's a typo, it's actually 500TB!\nMy answer are B & E.\nMy real life choice is Azure File storage.","poster":"totalz","comment_id":"802998"}],"content":"LOL, I see sarcasm in the voted answers. Or may be it's just me seeing the question differently~"},{"upvote_count":"1","content":"Selected Answer: BC\nB and C are correct","poster":"Eusouzati","comment_id":"801898","timestamp":"1675853700.0"},{"comment_id":"789901","poster":"OPT_001122","timestamp":"1674844500.0","content":"Selected Answer: BC\nB. an Azure Import/Export job\nC. Azure Data Factory","upvote_count":"1"},{"timestamp":"1674583200.0","comment_id":"786764","upvote_count":"1","poster":"VBK8579","content":"C. Azure Data Factory\nB. an Azure Import/Export job."},{"content":"Selected Answer: BC\nB and C are the most correct answers.\nProbably not what i'd use in the real world for 500gb of data though...","poster":"janvandermerwer","upvote_count":"3","timestamp":"1674088140.0","comment_id":"780584"},{"upvote_count":"1","comment_id":"690764","content":"Selected Answer: BC\nB & C are correct","poster":"JohnPhan","timestamp":"1665383280.0"},{"comment_id":"641153","timestamp":"1659426360.0","content":"Selected Answer: BC\nB&C are correct answers","poster":"fatwast","upvote_count":"1"},{"content":"Selected Answer: BC\nAzure data factory can copy the data to blob","upvote_count":"1","poster":"princessgalz","comment_id":"631256","timestamp":"1657788180.0"},{"poster":"al608","timestamp":"1655889420.0","comment_id":"620277","upvote_count":"3","content":"did my Exam today. This was on there."},{"timestamp":"1654577940.0","upvote_count":"1","poster":"Bilal41396","content":"B & C are the correct answer.","comment_id":"612580"},{"timestamp":"1653338940.0","poster":"Gor","upvote_count":"1","content":"Selected Answer: BC\nB & C are correct","comment_id":"606310"},{"upvote_count":"5","content":"was in exam 8 May 22","comment_id":"598644","poster":"datafypk","timestamp":"1652033580.0"},{"timestamp":"1650993780.0","poster":"Teringzooi","content":"Selected Answer: BC\nB & C are correct. \nhttps://docs.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool","upvote_count":"1","comment_id":"592596"},{"comment_id":"580776","upvote_count":"5","poster":"Contactfornitish","content":"Came in exam today 04/04/2022","timestamp":"1649081220.0"},{"timestamp":"1648779720.0","comment_id":"579232","content":"in my exam on 31 Mar 22","poster":"esther823","upvote_count":"4"},{"content":"Selected Answer: BC\nB. Export/Import job & C. Azure Data Factory","upvote_count":"1","timestamp":"1647594060.0","poster":"fadhilmukh","comment_id":"570410"},{"poster":"wsrudmen","upvote_count":"1","timestamp":"1647264660.0","comment_id":"567667","content":"Selected Answer: BC\nB&C is correct"},{"upvote_count":"2","content":"Appeared in my exam, March 10th, 2022. I chose B and C.","timestamp":"1646952360.0","comment_id":"565071","poster":"Insanewhip"},{"timestamp":"1646039460.0","comment_id":"557956","upvote_count":"5","content":"On AZ-305 2/28/22","poster":"bananapeel"},{"poster":"HGD545","upvote_count":"5","timestamp":"1645636380.0","content":"On the AZ-305 2/22/22","comment_id":"554683"},{"timestamp":"1645023960.0","comment_id":"548708","poster":"ashxos","upvote_count":"1","content":"Selected Answer: BC\nIncidentally, we have an exact requirement and we are going to use Azure Data Factory for it.\nAnswers are correct"},{"comments":[{"timestamp":"1647519840.0","content":"lol, I thought ADF are your answers.\nAgree. Azure Data Factory(ADF) and Import Export - so BC","poster":"BalderkVeit","upvote_count":"18","comment_id":"569708"}],"upvote_count":"9","comment_id":"501512","poster":"Shadow983","timestamp":"1639499880.0","content":"Correct.\nADF can be used to copy data to blob storage:\nhttps://docs.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"},{"poster":"pcman","upvote_count":"3","comment_id":"501224","content":"The given answer is correct","timestamp":"1639471320.0"}],"question_id":89,"question_text":"You have an Azure subscription that contains an Azure Blob Storage account named store1.\nYou have an on-premises file server named Server1 that runs Windows Server 2016. Server1 stores 500 GB of company files.\nYou need to store a copy of the company files from Server1 in store1.\nWhich two possible Azure services achieve this goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","isMC":true,"topic":"2"},{"id":"pPdtNBw9UNcLTQU0iZBU","topic":"2","isMC":true,"answer":"CD","url":"https://www.examtopics.com/discussions/microsoft/view/94045-exam-az-305-topic-2-question-20-discussion/","exam_id":54,"timestamp":"2023-01-05 16:59:00","answer_ET":"CD","answer_images":[],"question_id":90,"unix_timestamp":1672934340,"answer_description":"","question_text":"You are planning an Azure IoT Hub solution that will include 50,000 IoT devices.\n\nEach device will stream data, including temperature, device ID, and time data. Approximately 50,000 records will be written every second. The data will be visualized in near real time.\n\nYou need to recommend a service to store and query the data.\n\nWhich two services can you recommend? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.","answers_community":["CD (100%)"],"question_images":[],"choices":{"B":"Azure Event Grid","C":"Azure Cosmos DB for NoSQL","A":"Azure Table Storage","D":"Azure Time Series Insights"},"discussion":[{"poster":"Sammy1989","content":"Cleared the exam on 01/05/23 with 871 / 1000. Chose CD. There is a similar question on ET where the option was SQL DB API","comments":[{"upvote_count":"1","poster":"OPT_001122","comment_id":"790554","content":"Thanks for mentioning the exam date","timestamp":"1674906120.0"}],"comment_id":"766800","upvote_count":"20","timestamp":"1672934340.0"},{"poster":"rgargar78","content":"A. Azure Table Storage -> Throughput: scalability limit of 20,000 operations/s. -> Not enough for this question\nB. Azure Event Grid -> It is only a broker, not a storage solution\nTherefore, C and D are right\n\nRefs:\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/table/\nhttps://learn.microsoft.com/en-us/azure/event-grid/overview","comment_id":"775214","upvote_count":"18","timestamp":"1673687640.0"},{"comment_id":"1310678","timestamp":"1731424200.0","poster":"[Removed]","upvote_count":"2","content":"Selected Answer: CD\nC & D are correct"},{"poster":"JimmyYop","content":"appeared in Exam 01/2024","timestamp":"1705854720.0","upvote_count":"7","comment_id":"1127943"},{"timestamp":"1697241480.0","poster":"OrangeSG","content":"Selected Answer: CD\nThe Time Series Insights (TSI) service will no longer be supported after March 2025. Consider migrating existing TSI environments to alternative solutions (such as Azure Data Explorer) as soon as possible.\n\nAzure Data Explorer is a fast, fully managed data analytics service for real-time and time-series analysis on large volumes of data streams from business activities, human operations, applications, websites, Internet of Things (IoT) devices, and other sources.\n\nhttps://learn.microsoft.com/en-us/azure/time-series-insights/migration-to-adx","comment_id":"1043047","upvote_count":"2"},{"content":"Selected Answer: CD\nSame as Question #9","timestamp":"1681304700.0","poster":"NagaByrd","comment_id":"868383","upvote_count":"4"},{"upvote_count":"9","poster":"NotMeAnyWay","comment_id":"847319","content":"Selected Answer: CD\nC. Azure Cosmos DB for NoSQL\nD. Azure Time Series Insights\n\nBoth Azure Cosmos DB and Azure Time Series Insights are suitable services for storing and querying the data in this scenario.\n\nC. Azure Cosmos DB for NoSQL is a globally distributed, multi-model database service that can handle large amounts of data with low-latency and high throughput. Its support for various consistency levels and partitioning strategies makes it suitable for handling IoT data at scale.\n\nD. Azure Time Series Insights is a fully managed, real-time analytics service specifically designed for time-series data generated by IoT devices. It provides storage, visualization, and advanced querying capabilities for time-series data, making it an ideal choice for handling data from a large number of IoT devices and visualizing it in near real-time.","timestamp":"1679504760.0"},{"poster":"NotMeAnyWay","content":"Selected Answer: CD\nC. Azure Cosmos DB for NoSQL\nD. Azure Time Series Insights\n\nBoth Azure Cosmos DB and Azure Time Series Insights are suitable services for storing and querying the data in this scenario.\n\nC. Azure Cosmos DB for NoSQL is a globally distributed, multi-model database service that can handle large amounts of data with low-latency and high throughput. Its support for various consistency levels and partitioning strategies makes it suitable for handling IoT data at scale.\n\nD. Azure Time Series Insights is a fully managed, real-time analytics service specifically designed for time-series data generated by IoT devices. It provides storage, visualization, and advanced querying capabilities for time-series data, making it an ideal choice for handling data from a large number of IoT devices and visualizing it in near real-time.","comment_id":"847315","upvote_count":"1","timestamp":"1679504460.0"},{"poster":"zellck","content":"Selected Answer: CD\nCD is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/iot-using-cosmos-db\nAzure Cosmos DB is ideal for IoT workloads because it's capable of:\n- Ingesting device telemetry data at high rates, and return indexed queries with low latency and high availability.\n- Storing JSON format from different device vendors, which provides flexibility in payload schema.\n- By using wire protocol–compatible API endpoints for Cassandra, MongoDB, SQL, Gremlin, etcd, and table databases, and built-in support for Jupyter Notebook files.","comments":[{"comment_id":"818021","timestamp":"1677082140.0","upvote_count":"2","content":"https://learn.microsoft.com/en-us/azure/time-series-insights/overview-what-is-tsi\nAzure Time Series Insights Gen2 is an open and scalable end-to-end IoT analytics service featuring best-in-class user experiences and rich APIs to integrate its powerful capabilities into your existing workflow or application.\n\nYou can use it to collect, process, store, query and visualize data at Internet of Things (IoT) scale--data that's highly contextualized and optimized for time series.\n\nAzure Time Series Insights Gen2 is designed for ad hoc data exploration and operational analysis allowing you to uncover hidden trends, spotting anomalies, and conduct root-cause analysis. It's an open and flexible offering that meets the broad needs of industrial IoT deployments.","poster":"zellck"}],"comment_id":"818020","timestamp":"1677082140.0","upvote_count":"4"},{"content":"MongoDB...","upvote_count":"1","comment_id":"786886","timestamp":"1674590160.0","poster":"tfulanchan"},{"poster":"Beng_ali","upvote_count":"2","comments":[{"upvote_count":"2","comment_id":"789678","poster":"Liveroso","content":"C. Azure Cosmos DB for NoSQL\nD. Azure Time Series Insights\n\nAzure Cosmos DB is a globally distributed, multi-model database service that can be used to store and query large amounts of data with low latency. Cosmos DB supports various data models, including NoSQL, and is designed for high throughput and low latency. It can be used to store the data from the IoT devices and can handle the high write and read throughput required for the solution.\n\nAzure Time Series Insights is a time-series data platform that is designed for analyzing time-stamped data. It can be used to visualize the data from the IoT devices in near real-time, providing a way to monitor and analyze the device data in real-time. It also has built-in support for IoT data, making it a good choice for this scenario.","timestamp":"1674829440.0"}],"comment_id":"780094","content":"Selected Answer: CD\nCD is correct.","timestamp":"1674052500.0"},{"timestamp":"1672936800.0","comment_id":"766838","content":"Selected Answer: CD\nSeems correct.","poster":"maku067","upvote_count":"4"},{"upvote_count":"3","poster":"jage01","comment_id":"766830","timestamp":"1672936260.0","content":"Selected Answer: CD\nCD\n\nReal-time access with fast read and write latencies globally, and throughput and consistency all backed by SLAs\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/introduction\n\nAzure Time Series Insights is a fully managed analytics, storage, and visualization service that makes it simple to explore and analyze billions of IoT events simultaneously.\nhttps://learn.microsoft.com/en-us/azure/time-series-insights/time-series-insights-explorer"}]}],"exam":{"numberOfQuestions":286,"id":54,"lastUpdated":"12 Apr 2025","provider":"Microsoft","name":"AZ-305","isImplemented":true,"isMCOnly":false,"isBeta":false},"currentPage":18},"__N_SSP":true}