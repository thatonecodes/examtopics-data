{"pageProps":{"questions":[{"id":"Pib4ew3RppwiIB94PrOB","url":"https://www.examtopics.com/discussions/microsoft/view/55439-exam-dp-201-topic-5-question-3-discussion/","answers_community":[],"question_text":"HOTSPOT -\nYou need to design the SensorData collection.\nWhat should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_description":"Box 1: Eventual -\nTraffic data insertion rate must be maximized.\nSensor data must be stored in a Cosmos DB named treydata in a collection named SensorData\nWith Azure Cosmos DB, developers can choose from five well-defined consistency models on the consistency spectrum. From strongest to more relaxed, the models include strong, bounded staleness, session, consistent prefix, and eventual consistency.\n\nBox 2: License plate -\nThis solution reports on all data related to a specific vehicle license plate. The report must use data from the SensorData collection.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels","exam_id":66,"question_id":191,"discussion":[{"upvote_count":"2","timestamp":"1623846120.0","comment_id":"383372","content":"I believe the answer given (Eventual, License Plate) is correct: https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview#using-item-id-as-the-partition-key","poster":"BigMF"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0000800001.jpg"],"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0000700001.jpg"],"answer_ET":"","answer":"","isMC":false,"topic":"5","unix_timestamp":1623846120,"timestamp":"2021-06-16 14:22:00"},{"id":"I02upEM1GhXsBAEj6ZSx","answer_ET":"B","unix_timestamp":1621736760,"isMC":true,"question_text":"You need to recommend an Azure SQL Database pricing tier for Planning Assistance.\nWhich pricing tier should you recommend?","discussion":[{"upvote_count":"6","content":"Why use Managed Instance? No mention about migrating on-prem SQL Server to Azure SQL Server","timestamp":"1621736760.0","poster":"toandm","comment_id":"364054","comments":[{"timestamp":"1624989600.0","content":"In requirement, it says \"sharded Azure SQL Database.\"","upvote_count":"1","comment_id":"394131","poster":"Dhaval_Azure"},{"timestamp":"1621794060.0","poster":"memo43","comment_id":"364937","upvote_count":"10","content":"i think so.\nanswer should be D"}]},{"timestamp":"1623848100.0","content":"Sharded is not applicable to Managed Instances as clearly stated in the MS documentation below. I think it can be done in MI indirectly via linked servers but your costs go up therefore making the Azure SQL DB the more cost-effective option. The answer should be D.\n\n https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-scale-shard-map-management","upvote_count":"1","comment_id":"383405","poster":"BigMF"},{"poster":"Abyodp","comment_id":"379865","comments":[{"comment_id":"389064","poster":"bsa_2021","content":"The text is \"Sharded\" not \"shared\".n If the text is shared then you are right. But as per the text, correct answer is D.","timestamp":"1624477380.0","upvote_count":"2"}],"timestamp":"1623421980.0","upvote_count":"2","content":"Key point: shared DB. Answer is correct"},{"content":"The answer is : corret","upvote_count":"3","comment_id":"375896","timestamp":"1622974680.0","poster":"Qrm_1972"},{"upvote_count":"1","poster":"g935f","comment_id":"364687","timestamp":"1621783380.0","content":"Exam is for Azure solutions"}],"exam_id":66,"question_id":192,"question_images":[],"choices":{"D":"General purpose Azure SQL Database single database","A":"Business critical Azure SQL Database single database","C":"Business critical Azure SQL Database Managed Instance","B":"General purpose Azure SQL Database Managed Instance"},"topic":"5","answer":"B","answer_description":"Azure resource costs must be minimized where possible.\nData used for Planning Assistance must be stored in a sharded Azure SQL Database.\nThe SLA for Planning Assistance is 70 percent, and multiday outages are permitted.","answer_images":[],"timestamp":"2021-05-23 04:26:00","url":"https://www.examtopics.com/discussions/microsoft/view/53377-exam-dp-201-topic-5-question-4-discussion/","answers_community":[]},{"id":"2leVsbXlyeS3oM6Nu4bb","discussion":[{"content":"The given answer is not logic. This database is OLAP because ADF adds data once a week and the database is used for query purposes. This is a great use case for clustered columnstore index. I don't know why it is suggested the other way around!!!","timestamp":"1622292120.0","poster":"Ous01","comment_id":"369473","comments":[{"content":"Agreed with Ouso1 and MMM777. Should be Yes, No, No","timestamp":"1623848340.0","upvote_count":"3","comment_id":"383412","poster":"BigMF"}],"upvote_count":"7"},{"poster":"dlena","upvote_count":"1","timestamp":"1632811920.0","content":"Certainly, the nonclustered columnstore index will NOT help because it is a OLAP system.\n\n\"Use a nonclustered columnstore index to perform analysis in real time on an OLTP workload. For more information, see Get started with columnstore for real-time operational analytics.\"\n\nhttps://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview?view=sql-server-ver15","comment_id":"453117"},{"timestamp":"1623575100.0","comment_id":"380976","upvote_count":"1","poster":"ZodiaC","content":"This is correct"},{"comment_id":"380092","comments":[{"poster":"tes","comment_id":"392954","timestamp":"1624885800.0","upvote_count":"1","content":"\"This solution reports on all data related to a specific vehicle license plate.\" how is this possible without querying LicensePlate? \"license plate number information must not be accessible in Planning Assistance\" correct not for planning but for reporting. so index it","comments":[{"timestamp":"1624885860.0","poster":"tes","comment_id":"392955","content":"sorry the question is about planning. Hence you are right","upvote_count":"1"}]}],"upvote_count":"2","poster":"MMM777","content":"\"For privacy reasons, license plate number information must not be accessible in Planning Assistance.\" - so why would you need an index on it if the data is not supposed to be accessible? Also related to the sharding issue - why shard on something you will not query?","timestamp":"1623455940.0"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0001000002.png"],"question_text":"HOTSPOT -\nYou need to design the Planning Assistance database.\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","url":"https://www.examtopics.com/discussions/microsoft/view/53769-exam-dp-201-topic-5-question-5-discussion/","answer":"","question_id":193,"answer_ET":"","topic":"5","exam_id":66,"unix_timestamp":1622292120,"timestamp":"2021-05-29 14:42:00","isMC":false,"answer_description":"Box 1: No -\nData used for Planning Assistance must be stored in a sharded Azure SQL Database.\n\nBox 2: Yes -\n\nBox 3: Yes -\nPlanning Assistance database will include reports tracking the travel of a single vehicle\nDesign Azure data storage solutions","answers_community":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0001000001.png"]},{"id":"yCxnzskYfkDlvyMkViRk","question_text":"DRAG DROP -\nYou need to design the system for notifying law enforcement officers about speeding vehicles.\nHow should you design the pipeline? To answer, drag the appropriate services to the correct locations. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n//IMG//","exam_id":66,"unix_timestamp":1619412300,"question_id":194,"answer_ET":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0011900002.jpg","https://www.examtopics.com/assets/media/exam-media/03774/0012000001.jpg"],"discussion":[{"comments":[{"comment_id":"394417","poster":"Dhaval_Azure","timestamp":"1625024700.0","content":"Kafka should be an event reader, but data processing should be done by Azure Databricks & store in Cosmos DB or Azure SQL. \n\nGiven answer looks correct to me","upvote_count":"1"}],"comment_id":"362483","timestamp":"1621543500.0","upvote_count":"18","content":"Should be Kafka + Databricks!","poster":"ismaelrihawi"},{"poster":"Devendra00023","content":"As per the description, first box should be KAFKA","comments":[{"timestamp":"1619724240.0","poster":"davita8","content":"Telemetry data should be stored in NOSQL database preferably Cosmos DB or blob storage.\n\nHowever in this question, Cosmos DB is not an option given. If given that would have been the best answer.\n\nComing to why Azure Server. Below is what the case study states and storage doesn't support query language\n\nRequirement :The solution must allow for searches of vehicle images by license plate to support law enforcement investigations. Searches must be able to be performed using a query language and must support fuzzy searches to compensate for license plate detection errors.\n\n\n\nhttps://docs.microsoft.com/en-us/azure/architecture/reference-architectures/data/stream-processing-databricks","comment_id":"345628","upvote_count":"2"}],"timestamp":"1619412300.0","upvote_count":"2","comment_id":"342993"}],"topic":"6","isMC":false,"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/50972-exam-dp-201-topic-6-question-1-discussion/","answer_description":"Scenario:\nInformation about vehicles that have been detected as going over the speed limit during the last 30 minutes must be available to law enforcement officers. Several law enforcement organizations may respond to speeding vehicles.\n\nTelemetry Capture -\nThe telemetry capture system records each time a vehicle passes in front of a sensor. The sensors run on a custom embedded operating system and record the following telemetry data:\n✑ Time\n✑ Location in latitude and longitude\n✑ Speed in kilometers per hour (kmph)\n✑ Length of vehicle in meters\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks\nDesign data processing solutions","timestamp":"2021-04-26 06:45:00","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0011900001.jpg"],"answer":""},{"id":"fkgdkOs0ynnrRuUbw7hj","question_id":195,"answer_description":"A: Azure Stream Analytics is a fully managed service providing low-latency, highly available, scalable complex event processing over streaming data in the cloud.\nYou can use your Azure Synapse Analytics (SQL Data warehouse) database as an output sink for your Stream Analytics jobs.\nE: Event Hubs Capture is the easiest way to get data into Azure. Using Azure Data Lake, Azure Data Factory, and Azure HDInsight, you can perform batch processing and other analytics using familiar tools and platforms of your choosing, at any scale you need.\nNote: Event Hubs Capture creates files in Avro format.\nCaptured data is written in Apache Avro format: a compact, fast, binary format that provides rich data structures with inline schema. This format is widely used in the Hadoop ecosystem, Stream Analytics, and Azure Data Factory.\nScenario: The application development team will create an Azure event hub to receive real-time sales data, including store number, date, time, product ID, customer loyalty number, price, and discount amount, from the point of sale (POS) system and output the data to data storage in Azure.\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/azure/sql-data-warehouse/sql-data-warehouse-integrate-azure-stream-analytics https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview","answer_images":[],"answers_community":[],"question_text":"Inventory levels must be calculated by subtracting the current day's sales from the previous day's final inventory.\nWhich two options provide Litware with the ability to quickly calculate the current inventory levels by store and product? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.","topic":"7","timestamp":"2020-04-28 17:40:00","unix_timestamp":1588088400,"choices":{"A":"Consume the output of the event hub by using Azure Stream Analytics and aggregate the data by store and product. Output the resulting data directly to Azure Synapse Analytics. Use Transact-SQL to calculate the inventory levels.","B":"Output Event Hubs Avro files to Azure Blob storage. Use Transact-SQL to calculate the inventory levels by using PolyBase in Azure Synapse Analytics.","C":"Consume the output of the event hub by using Databricks. Use Databricks to calculate the inventory levels and output the data to Azure Synapse Analytics.","D":"Consume the output of the event hub by using Azure Stream Analytics and aggregate the data by store and product. Output the resulting data into Databricks. Calculate the inventory levels in Databricks and output the data to Azure Blob storage.","E":"Output Event Hubs Avro files to Azure Blob storage. Trigger an Azure Data Factory copy activity to run every 10 minutes to load the data into Azure Synapse Analytics. Use Transact-SQL to aggregate the data by store and product."},"exam_id":66,"isMC":true,"question_images":[],"answer_ET":"AE","answer":"AE","url":"https://www.examtopics.com/discussions/microsoft/view/19269-exam-dp-201-topic-7-question-1-discussion/","discussion":[{"comments":[{"comments":[{"content":"but the inventory levels are not calculated in E","comments":[{"poster":"sturcu","timestamp":"1612977900.0","comment_id":"287743","upvote_count":"1","content":"Yes they are. You load the sales into synapse -upserts ( you will substract the sold items). Hence aggregating will give you Current Inventory"}],"comment_id":"218596","timestamp":"1605282480.0","upvote_count":"1","poster":"Rambaldi"}],"comment_id":"158404","timestamp":"1597459020.0","content":"C is incorrect because there is no mention of a step that Azure DataBricks is connected to Azure Synapse to get the previous inventory level. Therefore, you cannot calculate the current inventory level from DataBricks. A & E are the only options that output sales data to Azure Synapse and calculate the inventory level from Azure Synapse. I've noticed that many people in DP-201 imagine the steps that are not stated, causing a lot of confusion.","upvote_count":"13","poster":"extraego"}],"upvote_count":"21","comment_id":"115603","content":"should be A & C as the final result should result in synapse","poster":"pravinDataSpecialist","timestamp":"1592746320.0"},{"timestamp":"1596688260.0","poster":"NikP","upvote_count":"9","comment_id":"151681","content":"I believe daily inventory data are going through ADLK Gen2 (As needed for staging) and go to to Analytical Data Store (Synapse).\nAnother thing is \"Daily inventory data comes from a Microsoft SQL server located on a private network\". I believe this is on prem server.\nThey didn't mentioned how this sql server consume the daily inventory data.\nPrevious day sales data which are already in Synapse (Assumption is daily inventory data migrated from sql server to\ndatalake and then to Synapse).\nCurrent Day's sales data can be ingested to Synapse directly through event hub by using Azure Stream Analytics.\nOR current day's sales data can be store to blob storage from Event Hub. You can use ADF to load those files to Synapse every 10 min.\nThen after you can create direct Power BI dashboard which can run T-SQL to calculate inventory level in Synapse and feed it to Power BI report.\nThis can be live report/dashboard which can be share to every store.\nAnswer should be A and E."},{"timestamp":"1614713340.0","comment_id":"302207","upvote_count":"1","poster":"Prashantprp","content":"Some outputs types support partitioning, and output batch sizes vary to optimize throughput. The following table shows features that are supported for each output type:\n\nTABLE 1\nOutput type Partitioning Sehttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs\nASA cannot output to Databricks\nAzure Data Lake Storage Gen 1 Yes Azure Active Directory user\n, Managed Identity\nAzure SQL Database Yes, optional. SQL user auth,\nManaged Identity (preview)\nAzure Synapse Analytics Yes SQL user auth,\nManaged Identity (preview)\nBlob storage and Azure Data Lake Gen 2 Yes Access key,\nManaged Identity (preview)\nAzure Event Hubs Yes, need to set the partition key column in output configuration. Access key,\nManaged Identity (preview)\nPower BI No Azure Active Directory user,\nManaged Identity\nAzure Table storage Yes Account key\nAzure Service Bus queues Yes Access key\nAzure Service Bus topics Yes Access key\nAzure Cosmos DB Yes Access key\nAzure Functions Yes Access key"},{"content":"Previous day's data will be in the DW so better to do the calculation on the DW. Use Transact-SQL to calculate the inventory levels.","upvote_count":"1","comment_id":"294990","poster":"TaherAli2020","timestamp":"1613822880.0"},{"upvote_count":"1","content":"I will not go for databricks because it is mentioned expressroute or VPN will not be there.\nhttps://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/on-prem-network","comments":[{"comment_id":"293448","timestamp":"1613658360.0","content":"So Polybase (not feasible on Avro format) and data bricks options are ruled out and given answer is correct.","upvote_count":"1","poster":"spiitr"}],"timestamp":"1613658180.0","comment_id":"293440","poster":"spiitr"},{"content":"E- using AVRO not useful as polybase does not support AVRO\nSo only left option is A and C","comment_id":"258633","timestamp":"1609689120.0","upvote_count":"2","poster":"Aditya167"},{"comment_id":"239979","timestamp":"1607595360.0","content":"\"Stage inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store\"\nD is out\nPolyBase does not support Avro\nB is out\nRunning DataFactory every 10 minutes is unncessary\nE is out\nA and C are therefore correct","upvote_count":"8","poster":"syu31svc"},{"comments":[{"comment_id":"206239","poster":"M0e","timestamp":"1603714500.0","content":"Sorry, I meant E is not an option.","upvote_count":"1"}],"upvote_count":"2","comment_id":"206222","content":"D is obviously not an option since it does not fulfil the requirement of calculating quickly! Option C should be selected instead - it receives the last day's inventory data from the Data lake which is mentioned in the case study (it is used for staging the data before it is loaded into the Data Warehouse.) -> Answer: A & C","poster":"M0e","timestamp":"1603712760.0"},{"timestamp":"1602326280.0","upvote_count":"2","comment_id":"197256","poster":"Trove","content":"If it is A and E , then aggregation by store and product is done twice, which may not give the right result. So , from a business perspective, A and C makes more sense."},{"timestamp":"1596688320.0","comment_id":"151682","upvote_count":"7","poster":"NikP","content":"B: is incorrect because Avro files doesn't support Polybase.\nC: is incorrect because first of all I am not sure you can output to result directly to Azure databrick. Even if you can then again you need to get\nprevious sales data from datalake or from synapse. Then calculate the inventory level by comparing both of them and send it to Synapse and feed the Power BI report. This doesn't make sense. In Synapse, based on right distribution key on large table, your join query works faster. I believe it will be fast and cheaper if you calculate the inventory level in Synapse compare to databrick.\nD: is incorrect because you don't want to store result in blob storage. It just doesn't make sense. With Synapse and Power BI, you can see historical inventory level by day, by store, by product etc if you configure it right way."},{"content":"Answer is A n E only as in the question it has been stated that \"Stage inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store\" , so the final output should go to analytical data store and azure storage should be used as staging which in question been told to remove the files from it as soon as data is uploaded into the final stage.","timestamp":"1595035320.0","poster":"sharnav","comment_id":"137549","upvote_count":"3"},{"comment_id":"134901","timestamp":"1594735260.0","upvote_count":"4","poster":"LeonLeon","content":"E can not be correct, because the use of an time scheduled trigger instead of an event blob-trigger. Update have to be done as close to real-time as possible!!"},{"upvote_count":"3","content":"Clearly the assumption is that the inventory of the day before is already in Synapse Analytics. On that basis the answer is correct: in A you aggregate the data, then load them into Synapse and there you use T-SQL statement to calculate the difference with the day before. In E you copy in Synapse the raw data, then use T-SQL to aggregate them and more T-SQL to calculate the difference with the day before","timestamp":"1593967980.0","comment_id":"127027","poster":"Tommy65"},{"timestamp":"1589738340.0","comments":[{"upvote_count":"5","content":"b is wrong as polybase does not support avr format. a, c seems to be correct","poster":"azurearch","timestamp":"1589864100.0","comment_id":"91811"}],"content":"A, B seems to be correct as well. with 10 minutes schedule using adf as mentioned in E option, there is a delay.","upvote_count":"2","comment_id":"90762","poster":"azurearch"},{"poster":"willdy123","upvote_count":"5","timestamp":"1589717580.0","content":"Option E does not calculate the inventory. Only groups input by store and prodcut. Databricks can calculate the inventory by reading eventhub data and inventory data from data lake (staged) or synapse. Since this is neither mentioned nor left out of option C. I would suggest options A and C to be the correct answers.","comment_id":"90534"},{"comments":[{"upvote_count":"5","comment_id":"89367","content":"As I see the inventory dataas of prev date is in Azure Synapse Analytics. So inventory level cant be calculated in Azure Databricks unless there's a feed back from Azure Synapse to Databricks","timestamp":"1589529480.0","poster":"vistran"},{"poster":"santafe","content":"It says they need to avoid VM. So datafactory is not suggested","upvote_count":"1","comment_id":"108930","comments":[{"comment_id":"108931","upvote_count":"1","poster":"santafe","timestamp":"1591988280.0","content":"So C should be correct"},{"comment_id":"163049","content":"VM is a IAAS and ADF is a PAAS.. so it can be used.","upvote_count":"1","timestamp":"1598023680.0","poster":"Arsa"}],"timestamp":"1591988220.0"},{"poster":"envy","comments":[{"timestamp":"1595062860.0","poster":"peppele","comment_id":"137762","upvote_count":"3","content":"Incorrect, https://docs.databricks.com/spark/latest/structured-streaming/streaming-event-hubs.html"}],"upvote_count":"2","content":"databrick doesn't have source for eventhub https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/","comment_id":"136289","timestamp":"1594886700.0"}],"comment_id":"80930","upvote_count":"7","content":"Why not A, C?","timestamp":"1588088400.0","poster":"hokigir"}]}],"exam":{"name":"DP-201","numberOfQuestions":206,"lastUpdated":"12 Apr 2025","isMCOnly":false,"isImplemented":true,"id":66,"isBeta":false,"provider":"Microsoft"},"currentPage":39},"__N_SSP":true}