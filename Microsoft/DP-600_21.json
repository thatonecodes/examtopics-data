{"pageProps":{"questions":[{"id":"j6bqBybfNcW6xBq1eHSt","answer":"C","answer_images":[],"answer_description":"","exam_id":71,"discussion":[{"content":"Selected Answer: C\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-scale-out-configure","comment_id":"1148493","poster":"Nicofr","upvote_count":"17","timestamp":"1707762720.0"},{"comment_id":"1340370","poster":"kaixin","timestamp":"1736861460.0","content":"Selected Answer: C\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-scale-out \n\"The Scale-out queries for large semantic models setting for your tenant is enabled (default).\"","upvote_count":"1"},{"poster":"NRezgui","upvote_count":"1","timestamp":"1735155600.0","content":"Selected Answer: C\n. At the semantic model level, set Large dataset storage format to On.","comment_id":"1331688"},{"poster":"Charley92","timestamp":"1733073720.0","upvote_count":"3","content":"Selected Answer: C\nTo enable scale-out for a semantic model in Power BI Premium, the Large dataset storage format must first be turned On. This is because the large dataset storage format allows models to take advantage of premium capacity features, including scale-out capabilities, which help distribute the load across multiple replicas for high-concurrency scenarios.","comment_id":"1320639"},{"poster":"Rakesh16","comment_id":"1312442","timestamp":"1731650580.0","content":"Selected Answer: C\nAt the semantic model level, set Large dataset storage format to On","upvote_count":"1"},{"comment_id":"1227762","poster":"DarioReymago","upvote_count":"2","content":"Selected Answer: C\nDefini...defini...definitively is C","timestamp":"1718010180.0"},{"upvote_count":"1","timestamp":"1708210920.0","content":"Selected Answer: C\nCorrect","comment_id":"1152906","poster":"Momoanwar"}],"topic":"1","question_text":"You have a Fabric tenant that uses a Microsoft Power BI Premium capacity.\nYou need to enable scale-out for a semantic model.\nWhat should you do first?","timestamp":"2024-02-12 19:32:00","url":"https://www.examtopics.com/discussions/microsoft/view/133645-exam-dp-600-topic-1-question-27-discussion/","choices":{"C":"At the semantic model level, set Large dataset storage format to On.","D":"At the tenant level, set Data Activator to Enabled.","A":"At the semantic model level, set Large dataset storage format to Off.","B":"At the tenant level, set Create and use Metrics to Enabled."},"answers_community":["C (100%)"],"unix_timestamp":1707762720,"isMC":true,"question_images":[],"answer_ET":"C","question_id":101},{"id":"XeAlryMh6fFwFkb9aSPm","unix_timestamp":1707505560,"choices":{"C":"Direct Lake","A":"DirectQuery","D":"Import","B":"Dual"},"timestamp":"2024-02-09 20:06:00","answers_community":["A (51%)","C (49%)"],"topic":"1","answer":"A","answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/microsoft/view/133464-exam-dp-600-topic-1-question-28-discussion/","question_id":102,"question_text":"You have a Fabric tenant that contains a warehouse. The warehouse uses row-level security (RLS).\nYou create a Direct Lake semantic model that uses the Delta tables and RLS of the warehouse.\nWhen users interact with a report built from the model, which mode will be used by the DAX queries?","answer_images":[],"discussion":[{"comments":[{"poster":"FilipKrk","content":"Confirm:\nhttps://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview\n\"Queries using row-level security against tables in the warehouse (including the Lakehouse SQL analytics endpoint) will fall back to DirectQuery mode.\"","upvote_count":"8","comment_id":"1262914","timestamp":"1723202520.0"}],"upvote_count":"69","content":"A. Direct Query \"Row-level security only applies to queries on a Warehouse or SQL analytics endpoint in Fabric. Power BI queries on a warehouse in Direct Lake mode will fall back to Direct Query mode to abide by row-level security.\"\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security","comment_id":"1152686","poster":"wojciech_wie","timestamp":"1708187160.0"},{"content":"Selected Answer: C\nDirect lake . When users interact with a report built from a Direct Lake semantic model that uses Delta tables and RLS of the warehouse, the DAX queries will operate in Direct Lake mode. This mode is specifically designed for analyzing large data volumes in Power BI and is based on loading parquet-formatted files directly from a data lake without querying a Lakehouse or Warehouse endpoint. Unlike DirectQuery, there is no translation from DAX to other query languages, and it does not execute queries on other database systems. This results in performance similar to import mode, with the added benefit of picking up any changes at the data source as they occur12.\n\nDirect Lake mode supports row-level security (RLS), ensuring that users only see the data they have permission to view. It combines the advantages of both DirectQuery and import modes while avoiding their disadvantages, making it an ideal choice for very large models and models with frequent updates at the data source2.","poster":"Estratech","comment_id":"1203321","timestamp":"1714261020.0","upvote_count":"25"},{"timestamp":"1743361380.0","comment_id":"1413808","content":"Selected Answer: A\nhttps://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview","poster":"ba2bfdf","upvote_count":"1"},{"poster":"4e5cf3d","content":"Selected Answer: A\nFalls back to direct query","timestamp":"1743355800.0","upvote_count":"1","comment_id":"1413725"},{"upvote_count":"1","comment_id":"1399437","content":"Selected Answer: A\nconfirm","poster":"Amine_spiegel94","timestamp":"1742165100.0"},{"timestamp":"1741923660.0","poster":"wudixh","comment_id":"1395502","upvote_count":"1","content":"Selected Answer: C\nAnswer from Microsoft Copilot: C.\nWhen users interact with a report built from a Direct Lake semantic model that uses Delta tables and RLS, the DAX queries will use Direct Lake mode (Option C) by default. This mode allows the semantic model to read Delta tables directly from OneLake.\nHowever, if there are certain conditions like memory pressure or unsupported features at the data source, the model might fall back to DirectQuery mode"},{"poster":"aks2304","content":"Selected Answer: A\nThe correct answer is A. DirectQuery.\n\nWhen users interact with a report built from a Direct Lake semantic model that uses Delta tables and row-level security (RLS), the DAX queries are executed in DirectQuery mode. This is because RLS requires the queries to be evaluated at the source to enforce security rules dynamically, and Direct Lake does not currently support RLS natively. Therefore, the system falls back to DirectQuery mode to ensure RLS is applied correctly.","timestamp":"1741357020.0","upvote_count":"2","comment_id":"1366284"},{"timestamp":"1735631280.0","content":"Selected Answer: A\nIT will fall back to direct query since the RLS is configure on the Data source and not in the semantic model","poster":"orallony","upvote_count":"2","comment_id":"1334685"},{"timestamp":"1735155780.0","content":"Selected Answer: A\n. DirectQuery","poster":"NRezgui","upvote_count":"3","comment_id":"1331692"},{"timestamp":"1735147260.0","comment_id":"1331650","poster":"Red_lotus85","upvote_count":"2","content":"Selected Answer: A\nA! Read the limitation of direct lake"},{"comment_id":"1330683","upvote_count":"2","timestamp":"1734933480.0","content":"Selected Answer: A\nRLS is based on warehouse","poster":"emboutchamani"},{"comment_id":"1327600","timestamp":"1734377940.0","upvote_count":"3","content":"Selected Answer: A\nA query always falls back when the semantic model queries a view in the SQL analytics endpoint, or a table in the SQL analytics endpoint that enforces row-level security (RLS).","poster":"Markl"},{"content":"Selected Answer: A\nRLS on the warehouse will fail back to direct query","poster":"Huepig","timestamp":"1733565360.0","comment_id":"1323068","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: C\nWhen a Direct Lake semantic model is created and used, the DAX queries interact directly with the data in OneLake without importing it into memory (as in Import mode) or querying it from a remote data source (as in DirectQuery mode).\n\nThe key features of Direct Lake mode include:\n\nDirect access to Delta tables stored in OneLake.\nLow latency querying without requiring data to be loaded into memory first.\nIntegration with features like Row-Level Security (RLS) when configured in the underlying warehouse.","poster":"Charley92","timestamp":"1733073900.0","comment_id":"1320640"},{"upvote_count":"3","timestamp":"1732949760.0","poster":"Shivam_1122","content":"Selected Answer: A\nWritten in docs itself https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview it will fallback to DQ","comment_id":"1320104"},{"comment_id":"1315434","timestamp":"1732125360.0","upvote_count":"2","content":"Selected Answer: C\nIn the scenario, when users interact with a report built from a Direct Lake semantic model that utilizes Delta tables and row-level security (RLS), the DAX queries will primarily operate in Direct Lake mode.\n\nSo, the correct answer is C. Direct Lake\n\nThis mode is designed to provide optimal performance for DAX queries, allowing them to read data directly from the Delta tables. However, if there are any limitations or memory pressures, the model may fall back to DirectQuery mode. But under normal circumstances, you can expect the queries to run in Direct Lake mode, leveraging the benefits of RLS effectively","poster":"nappi1"},{"timestamp":"1731650580.0","comment_id":"1312443","content":"Selected Answer: C\nDirect Lake","poster":"Rakesh16","upvote_count":"2"},{"content":"Selected Answer: A\n\"When a Direct Lake semantic model table connects to a table in the SQL analytics endpoint that enforces row-level security (RLS), queries that involve that model table will always fall back to DirectQuery mode. Query performance might be slower.\"\n\nThe question asks specifically about RLS, so you might already guess that this is a special situation. Always a good idea to look it up in the Fabric documentation if you've never seen this specific case.\n\nReference: https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview","poster":"semauni","timestamp":"1730471220.0","comment_id":"1305858","upvote_count":"3"},{"timestamp":"1730471280.0","comment_id":"1305859","content":"I wanted to make a voting comment, so please remove this one :)","upvote_count":"1","poster":"semauni"},{"comment_id":"1304364","timestamp":"1730193420.0","poster":"jcu614","upvote_count":"1","content":"C.\nDirect Lake mode is a specific feature of Microsoft Fabric that allows Power BI to directly access Delta tables in a Fabric Lakehouse or Warehouse without needing to import data into Power BI or rely on DirectQuery alone. This mode enables real-time interaction with the data stored in the lake without additional overhead.\n\nDirectQuery would be more relevant if the data were in a database that supported DirectQuery rather than Delta tables in Fabric's Lakehouse/Warehouse."},{"comment_id":"1303580","upvote_count":"1","poster":"jass007_k","timestamp":"1730032200.0","content":"C) In fabric, its always the DirectLake mode"},{"timestamp":"1720086600.0","comment_id":"1241950","upvote_count":"2","content":"Selected Answer: A\nBecause it says \"RLS of the warehouse\" I'd be more likely to go for DirectQuery than Direct Lake","poster":"b65ecca"},{"poster":"LeeAspiring","content":"A. Direct lake - Row-level security only applies to queries on a Warehouse or SQL analytics endpoint in Fabric. Power BI queries on a warehouse in Direct Lake mode will fall back to Direct Query mode to abide by row-level security.","comment_id":"1234338","upvote_count":"1","timestamp":"1718966040.0"},{"timestamp":"1718616180.0","upvote_count":"1","poster":"SharkyShark","content":"Its A.\n\nBased on https://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security","comment_id":"1231843"},{"timestamp":"1718409300.0","comment_id":"1230719","content":"Both DirectQuery and DirectLake support RLS.The key word here is that tenant already had a Semantic Warehouse built.To query that one you need to use DirectQuery.Therefore the answer is A.But for the real user they can use DirectLake as well hear sinc eboth of them support RLS.Answer is A for this question.Thank you.","upvote_count":"4","poster":"DilaniI"},{"comment_id":"1227803","content":"Selected Answer: D\nD. https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview\nOn the other hand, with import mode, performance can be better because the data is cached and optimized for DAX and MDX report queries without having to translate and pass SQL or other types of queries to the data source.","upvote_count":"1","poster":"Beejay69","timestamp":"1718015520.0"},{"upvote_count":"1","content":"Selected Answer: C\nc in my opinion","poster":"DarioReymago","comment_id":"1227768","timestamp":"1718011380.0"},{"content":"Selected Answer: C\nC. https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview\n\"Before using Direct Lake, you must provision a lakehouse (or a warehouse) with one or more Delta tables in a workspace...Direct Lake also supports row-level security and object-level security so users\"\n\nAnd it is Fabric exam, so the obvious correct answers is a Fabric feature.","upvote_count":"6","poster":"Jons123son","comment_id":"1222596","timestamp":"1717239240.0"},{"poster":"282b85d","comment_id":"1219722","content":"Selected Answer: C\nDirect Lake mode is a new feature in Power BI that allows direct querying of Delta tables in a data lake without the need to import data into the Power BI model. This mode \"combines\" the advantages of DirectQuery and Import modes, providing real-time access to data while leveraging the performance benefits of in-memory caching for certain queries.","timestamp":"1716831660.0","upvote_count":"1"},{"content":"Selected Answer: A\nWhile Direct Lake mode doesn't query the SQL endpoint when loading data directly from OneLake, it's required when a Direct Lake model must seamlessly fall back to DirectQuery mode, such as when the data source uses specific features like advanced security or views that can't be read through Direct Lake.","upvote_count":"1","timestamp":"1716130680.0","comment_id":"1213851","poster":"David_Webb"},{"poster":"Murtaza_007","timestamp":"1715956740.0","comment_id":"1212935","content":"ChatGPT says the answer is C- Direct Lake","upvote_count":"1"},{"content":"A. https://fabric.guru/power-bi-direct-lake-mode-frequently-asked-questions","poster":"c58ebda","timestamp":"1712826480.0","upvote_count":"2","comment_id":"1193627"},{"timestamp":"1711465140.0","upvote_count":"2","comment_id":"1183408","content":"Selected Answer: A\nA\nif the semantic model has direct lake connection to the source, then the report connecting to this semantic model has direct query.","poster":"Nefirs"},{"comment_id":"1178571","content":"Selected Answer: A\nA as has been shared before https://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security","timestamp":"1710957420.0","comments":[{"content":"Power BI queries on a warehouse in Direct Lake mode will fall back to Direct Query mode to abide by row-level security.","timestamp":"1711649400.0","upvote_count":"2","poster":"a_51","comment_id":"1184934"}],"upvote_count":"1","poster":"a_51"},{"timestamp":"1709048460.0","content":"Selected Answer: A\nWhat the heck is \"Import\" in DAX? Its DirectQuery only.","comments":[{"poster":"BDaly","upvote_count":"1","comment_id":"1178837","content":"Probably a reference to table types in PBI where import is commonly used","timestamp":"1710976260.0"}],"upvote_count":"3","poster":"XiltroX","comment_id":"1160770"},{"poster":"Momoanwar","content":"Selected Answer: A\nDax and fallback its direct query","upvote_count":"2","comment_id":"1152908","timestamp":"1708210980.0"},{"poster":"Dali2908","timestamp":"1707753600.0","content":"Answer is A - DirectQuery.","comment_id":"1148309","upvote_count":"3"},{"content":"Known issues and limitations\nCurrently, Direct Lake models can only contain tables and views from a single Lakehouse or Data Warehouse. However, tables in the model based on T-SQL-based views cannot be queried in Direct Lake mode. DAX queries that use these model tables fall back to DirectQuery mode. https://learn.microsoft.com/en-us/power-bi/enterprise/directlake-overview","upvote_count":"4","comment_id":"1145724","timestamp":"1707505980.0","poster":"IshtarSQL"},{"poster":"IshtarSQL","upvote_count":"5","content":"Selected Answer: A\nWhen users interact with a report built from a Direct Lake semantic model, which leverages row-level security (RLS) and Delta tables from a warehouse, the DAX queries will operate in DirectQuery mode.","timestamp":"1707505560.0","comment_id":"1145716","comments":[{"upvote_count":"2","poster":"rmeng","content":"Where did you see it ?","comments":[{"content":"https://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security#row-level-security-at-the-data-level","upvote_count":"1","comment_id":"1218793","poster":"66d0cf7","timestamp":"1716708660.0"}],"comment_id":"1164120","timestamp":"1709385060.0"}]}],"answer_ET":"A","question_images":[],"exam_id":71},{"id":"Z4VDGOQfaZCJX23TcLWj","timestamp":"2024-02-18 00:04:00","discussion":[{"content":"Selected Answer: C\nSee: https://learn.microsoft.com/en-us/power-bi/transform-model/desktop-relationship-view","upvote_count":"1","comment_id":"1416607","timestamp":"1743491820.0","poster":"DocE"},{"timestamp":"1735155840.0","content":"Selected Answer: C\nC. Model view","comment_id":"1331693","poster":"NRezgui","upvote_count":"2"},{"poster":"Rakesh16","comment_id":"1312444","upvote_count":"1","content":"Selected Answer: C\nModel view","timestamp":"1731650640.0"},{"content":"Selected Answer: C\nIMHO, \"C\"\n\nLink: https://learn.microsoft.com/en-us/power-bi/transform-model/desktop-relationship-view","comment_id":"1208538","upvote_count":"2","timestamp":"1715200920.0","poster":"stilferx"},{"content":"Selected Answer: C\nWell this was kinda Captain Obvious.","upvote_count":"4","comment_id":"1159937","timestamp":"1708965600.0","poster":"XiltroX"},{"upvote_count":"2","poster":"SamuComqi","content":"Selected Answer: C\nC. Model view\n\nIn the Model view, it is possible to analyze the semantic model and create new layouts.","timestamp":"1708324560.0","comment_id":"1153741"},{"comment_id":"1152909","timestamp":"1708211040.0","content":"Selected Answer: C\nModel = model view","poster":"Momoanwar","upvote_count":"1"}],"question_id":103,"exam_id":71,"question_text":"You have a Fabric tenant that contains a complex semantic model. The model is based on a star schema and contains many tables, including a fact table named Sales.\nYou need to create a diagram of the model. The diagram must contain only the Sales table and related tables.\nWhat should you use from Microsoft Power BI Desktop?","answer_ET":"C","answers_community":["C (100%)"],"answer":"C","question_images":[],"unix_timestamp":1708211040,"answer_description":"","url":"https://www.examtopics.com/discussions/microsoft/view/134094-exam-dp-600-topic-1-question-29-discussion/","answer_images":[],"topic":"1","choices":{"C":"Model view","D":"DAX query view","B":"Data view","A":"data categories"},"isMC":true},{"id":"RPdIGqpCTViOd1bvmSNB","discussion":[{"comment_id":"1145462","poster":"theseon","upvote_count":"26","comments":[{"timestamp":"1711919700.0","poster":"AsitTrivedi","content":"https://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2","upvote_count":"5","comment_id":"1187028"},{"comment_id":"1159413","poster":"sraakesh95","upvote_count":"3","timestamp":"1708923420.0","content":"Totally agree on the max value to be retrieved on incremental load"}],"timestamp":"1707482760.0","content":"Selected Answer: D\nwe need to retrieve the maximum OrderID in the destination table to minimize the number of rows added during refresh. this would be an incremental load. can be done with data flows"},{"upvote_count":"9","comment_id":"1222125","poster":"Jons123son","content":"Selected Answer: D\nD - As other people pointed out, the exact same use case for retrieving the max OrderID is showcased in the documentation \n\nhttps://learn.microsoft.com/en-us/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2#add-a-query-to-the-dataflow-to-filter-the-data-based-on-the-data-destination\n\nThought at first that A would be correct because SP support least privilege and because how real incremental refresh is not yet supported in data flow gen 2 https://ideas.fabric.microsoft.com/ideas/idea/?ideaid=4814b098-efff-ed11-a81c-6045bdb98602","timestamp":"1717156800.0"},{"content":"Selected Answer: D\nkey word minimize maintenance effort. answer is D","timestamp":"1736851320.0","comment_id":"1340290","upvote_count":"2","poster":"Egocentric"},{"timestamp":"1735116780.0","content":"Selected Answer: D\nan Azure Data Factory pipeline that executes a dataflow to retrieve the maximum value of the OrderID column in the destination lakehouse","upvote_count":"1","comment_id":"1331463","poster":"NRezgui"},{"poster":"NRezgui","content":"Selected Answer: D\nan Azure Data Factory pipeline that executes a dataflow to retrieve the maximum value of the OrderID column in the destination lakehouse","comment_id":"1331456","upvote_count":"1","timestamp":"1735116300.0"},{"comment_id":"1312417","timestamp":"1731649920.0","poster":"Rakesh16","upvote_count":"1","content":"Selected Answer: D\nan Azure Data Factory pipeline that executes a dataflow to retrieve the maximum value of the OrderID column in the destination lakehouse\n\n\nhttps://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2"},{"timestamp":"1730774640.0","content":"Both dataflow and SP should work is it? This question a bit confusing.","comment_id":"1307180","upvote_count":"1","poster":"Naqib"},{"comment_id":"1305409","timestamp":"1730377740.0","content":"Selected Answer: D\nI'm also choosing D alongside the other answers. My reasoning is:\n\n1) The showcased example of doing incremental refresh by dataflows (see the link below), which is almost an answer in itself because it tells you how Microsoft views the solution to this issue.\n2) Maximum ID instead of minimum: see the same link for the specific use. But even without this knowledge you can read in the case study that new (higher) numbers represent newer orders, so for an incremental refresh it makes way more sense to retrieve the ID of the *latest* order placed than the ID of the first.\n3) dataflow instead of stored procedure: because of the link, but it also makes sense from the \"minimize implementation and maintenance effort\" requirement: writing an incremental refresh SP is very, very complicated.\n\nLink: https://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2","poster":"semauni","upvote_count":"1"},{"timestamp":"1729404840.0","content":"D is the answer also A can be correct","comment_id":"1300303","poster":"Egocentric","upvote_count":"1"},{"content":"D\nhttps://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2\n-> \"You now have a query that returns the maximum OrderID in the lakehouse. This query is used to filter the data from the OData source. The next section adds a query to the dataflow to filter the data from the OData source based on the maximum OrderID in the lakehouse.\"\nDon't ask why\nThe problem is Fabric, so find the answer in the document\nIs this the first time you've seen a test in your life?","poster":"AzurePart","comment_id":"1288340","upvote_count":"1","timestamp":"1727133300.0"},{"comments":[{"upvote_count":"1","timestamp":"1730376540.0","content":"How does this piece of information impacts your answer? Because a pipeline is just the trigger for an activity to happen - which can either be a Stored Procedure activity or a dataflow activity. What limitation do pipelines have for dataflows in this regard?","poster":"semauni","comment_id":"1305401"}],"upvote_count":"2","poster":"LasAnsias","content":"Selected Answer: A\nAzure Data Factory \"pipelines\" is different from Azure Data Factory \"Data Flows\".\nAll the options are directing us to use Azure Data Factory \"pipelines\", so it should be using a stored procedure.","comment_id":"1191619","timestamp":"1726974780.0"},{"comments":[{"poster":"Nefirs","upvote_count":"2","comment_id":"1198627","content":"only semantic model and reports must use version control. However, dataflows are not mentioned, therefore, irrelevant whether supported or not.","timestamp":"1713532860.0"}],"comment_id":"1194668","timestamp":"1726974780.0","upvote_count":"1","content":"Selected Answer: A\nwe need to retrieve the maximum OrderID in the destination table to minimize the number of rows added during refresh. This can be achieved with both the dataflow and a stored procedure.\n\nIt mentions that \"All the semantic models and reports for the Research division must use version control that supports branching.\"\n\nDataflows are not supported in the git integration. Hence I choose A as the answer.","poster":"sepiida"},{"upvote_count":"2","content":"Selected Answer: D\nI was initially leaning to A but got real confused when I read the choices again. Using FABRIC data factory (one would presume that what they would mean in a FABRIC exam), when you use a Stored Procedure activity, you only see Warehouses and other SQL sources and NOT Lakehouses. Using Azure Data Factory, one could add an Azure SQL DB linked service and connect to the SQL Endpoint of a Lakehouse and execute a stored procedure associated with that SQL Endpoint. Even for Fabric Pipelines, one could use an Azure SQL Database connection (instead of Lakehouse), connect to the SQL Endpoint of a Lakehouse and execute a stored procedure associated with that SQL Endpoint. This I believe is the most efficient way to do it. The issue I have with D is the fact that Dataflows require significant resources to spin up and execute. Good thing with it is that there is no ambiguity mentioned above and if you want to get the answer right, might not be the most efficient but without more verbosity in the choices, I painfully chose it.","poster":"nyoike","comment_id":"1262132","comments":[{"timestamp":"1730377080.0","poster":"semauni","content":"And that's how you pass Microsoft exams, by (painfully) choosing the Microsoft way :) It's never 100% about what is 'the best' answer according to you or the community, it's about the answer that Microsoft will count as right.\n\nIn all seriosity: the general requirements do state that \"implementation and maintenance efforts\" should be minimized where possible. Writing a stored procedure to incrementally load a table gets very complex very fast.","upvote_count":"1","comment_id":"1305404"}],"timestamp":"1723041840.0"},{"comment_id":"1234097","content":"I asked chatgpt and I've got this: Based on the requirements for the semantic model of the Online Sales department, the best solution to refresh the Orders table would be to include an Azure Data Factory pipeline that executes a Stored procedure activity to retrieve the maximum value of the OrderID column in the destination lakehouse. This approach ensures that only new orders are processed, maintaining the sequence and integrity of the OrderID values as per the system of origin. Therefore, the correct answer is:\n\nA. an Azure Data Factory pipeline that executes a Stored procedure activity to retrieve the maximum value of the OrderID column in the destination lakehouse.","upvote_count":"1","poster":"agente232","timestamp":"1718922060.0"},{"upvote_count":"3","poster":"ca63a55","content":"IMHO, it has to be done with dataflow (D) because the semantic model uses an Import mode so I think it doesn't support a store procedure (SQL)","timestamp":"1717401720.0","comment_id":"1223485"},{"content":"Selected Answer: A\nHere the key words are \"to retrieve\", so if you run a pipeline to execute something to retrieve a value then it should be a Store Procedure using the lookup activity. This is the most effective way to do it.\nThe answers are not telling you the entire process to insert the new data (which it could be with dataflow) else it is telling you what activity to use in the pipeline to retrieve the maximum value of the OrderID. At least this is what I understood.","comment_id":"1211276","timestamp":"1715672220.0","poster":"Fer079","upvote_count":"3"},{"content":"Selected Answer: A\ni think it should be A","poster":"m_abohassan","timestamp":"1715109900.0","upvote_count":"1","comment_id":"1207992"},{"timestamp":"1715098740.0","upvote_count":"1","content":"Selected Answer: D\nIMHO, \nit is very good explained here: https://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2\n\nSo, it is not A, because of SP is not good to load data from sources in data lake. B and C - not an option at all, because it doesn't make sense to query the minimum order to make an incremental loading.","poster":"stilferx","comments":[{"content":"Hey when can we use SP then ?","poster":"ZSteward","comment_id":"1210844","timestamp":"1715601540.0","upvote_count":"1"},{"comment_id":"1211069","timestamp":"1715632980.0","upvote_count":"3","poster":"stilferx","content":"Additional evidence for D is, Storage Procedure Activity doesn't return value. It just runs the SP. The LookUp Activity does. But not SP. You can see it here on the screens:\n\nhttps://learn.microsoft.com/en-us/fabric/data-factory/stored-procedure-activity#step-3-choose-a-stored-procedure-and-configure-parameters"}],"comment_id":"1207942"},{"upvote_count":"2","timestamp":"1714383000.0","poster":"rmeng","content":"Selected Answer: D\nhttps://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2","comment_id":"1203934"},{"content":"Selected Answer: D\nThe solution is using Lakehouses. You can't create stored procedures in the sql endpoint of a Lakehouse.","comment_id":"1203688","comments":[{"upvote_count":"1","timestamp":"1714736040.0","comments":[{"timestamp":"1735067040.0","upvote_count":"1","poster":"KarenAntheunis","content":"you can create SP in SQL endpoint. The problem is, as of today, you can't call it from Stored Procedure or Lookup activity in ADF pipeline. Therefore, D is the only option so far.","comment_id":"1331211"}],"poster":"c8f5bdf","content":"actually I just did it. so, yes, you can.","comment_id":"1206074"}],"poster":"manolet","timestamp":"1714330440.0","upvote_count":"3"},{"upvote_count":"1","timestamp":"1713609660.0","content":"Selected Answer: D\nFor delta load, max order id is the PK so","comment_id":"1199091","poster":"Shri_Learning"},{"poster":"RinSu75","content":"Answer should be A","upvote_count":"1","comment_id":"1184801","timestamp":"1711634340.0"},{"upvote_count":"7","timestamp":"1710094680.0","comments":[{"content":"No way, that you can call SP or SP in Lookup Activity where the connection type is Lakehouse SQL Endpoint (it works though for Azure SQL and other SQL-based\n source types)","poster":"KarenAntheunis","upvote_count":"1","comment_id":"1331213","timestamp":"1735067520.0"},{"upvote_count":"1","timestamp":"1720324380.0","poster":"a8906d2","comment_id":"1243687","content":"We need to find out least solution. In the requirement , they have not mentioned any services related to DB and If you want to use stored procedure, you need either Azure SQL DB or Azure SQL DW service. Hence Dataflow approach is optimal one for this requirement"},{"upvote_count":"5","timestamp":"1710685200.0","comment_id":"1175880","content":"I agree.\nAzure Data Factory \"pipelines\" is different from Azure Data Factory \"Data Flows\".\nAll the options are directing us to use Azure Data Factory \"pipelines\", so it should be using a stored procedure.","poster":"NabilR"},{"content":"The lookup activity can be a SQL statement, the link provided mentions to use a stored proc to update the watermark. I believe the desired way is to use dataflow gen 2, https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2.","poster":"a_51","upvote_count":"1","timestamp":"1710762060.0","comment_id":"1176414"}],"content":"Selected Answer: A\nThe answer is A. You should query the last ID from the destination by lookup activity which uses the stored procedure, data flow is not used for this purpose.\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal","poster":"metiii","comment_id":"1170527"},{"poster":"SamuComqi","comment_id":"1153078","upvote_count":"1","content":"D. an Azure Data Factory pipeline that executes a dataflow to retrieve the maximum value of the OrderID column in the destination lakehouse.\n\nA dataflow can be used to retrieve the max OrderID number (stored in the destination table - OrderID is a sequencial number). This number can be used to set from which row data must be added to the destination table (implementing an incremental load).","timestamp":"1708240500.0"},{"upvote_count":"3","comment_id":"1152831","content":"Selected Answer: D\nMax with dataflow","poster":"Momoanwar","comments":[{"upvote_count":"3","content":"Why not A?\nI understand that DataFlow should be used when you want to apply transformation to the Data. Getting a simple MAX() can be done by using a stored procedure.","comment_id":"1175873","poster":"NabilR","timestamp":"1710685020.0"}],"timestamp":"1708205100.0"}],"url":"https://www.examtopics.com/discussions/microsoft/view/133444-exam-dp-600-topic-1-question-3-discussion/","answer":"D","answer_images":[],"unix_timestamp":1707482760,"answer_description":"","question_text":"Case study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -\nContoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.\n\nExisting Environment -\n\nIdentity Environment -\nContoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.\n\nData Environment -\nContoso has the following data environment:\nThe Sales division uses a Microsoft Power BI Premium capacity.\nThe semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.\nThe Research department uses an on-premises, third-party data warehousing product.\nFabric is enabled for contoso.com.\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.\nA Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.\n\nRequirements -\n\nPlanned Changes -\nContoso plans to make the following changes:\nEnable support for Fabric in the Power BI Premium capacity used by the Sales division.\nMake all the data for the Sales division and the Research division available in Fabric.\nFor the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.\nIn Productline1ws, create a lakehouse named Lakehouse1.\nIn Lakehouse1, create a shortcut to storage1 named ResearchProduct.\n\nData Analytics Requirements -\nContoso identifies the following data analytics requirements:\nAll the workspaces for the Sales division and the Research division must support all Fabric experiences.\nThe Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.\nThe Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.\nFor the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.\nFor the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.\nAll the semantic models and reports for the Research division must use version control that supports branching.\n\nData Preparation Requirements -\nContoso identifies the following data preparation requirements:\nThe Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.\nAll the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.\n\nSemantic Model Requirements -\nContoso identifies the following requirements for implementing and managing semantic models:\nThe number of rows added to the Orders table during refreshes must be minimized.\nThe semantic models in the Research division workspaces must use Direct Lake mode.\n\nGeneral Requirements -\nContoso identifies the following high-level requirements that must be considered for all solutions:\nFollow the principle of least privilege when applicable.\nMinimize implementation and maintenance effort when possible.\nYou need to refresh the Orders table of the Online Sales department. The solution must meet the semantic model requirements.\nWhat should you include in the solution?","timestamp":"2024-02-09 13:46:00","answer_ET":"D","isMC":true,"question_images":[],"choices":{"A":"an Azure Data Factory pipeline that executes a Stored procedure activity to retrieve the maximum value of the OrderID column in the destination lakehouse","B":"an Azure Data Factory pipeline that executes a Stored procedure activity to retrieve the minimum value of the OrderID column in the destination lakehouse","C":"an Azure Data Factory pipeline that executes a dataflow to retrieve the minimum value of the OrderID column in the destination lakehouse","D":"an Azure Data Factory pipeline that executes a dataflow to retrieve the maximum value of the OrderID column in the destination lakehouse"},"topic":"1","exam_id":71,"answers_community":["D (79%)","A (21%)"],"question_id":104},{"id":"1fgDWnSzsNGM7TWkiNbS","discussion":[{"poster":"282b85d","upvote_count":"12","content":"Selected Answer: BC\nMethods to Identify Frequently Used Columns:\n\nB. Use the Vertipaq Analyzer tool.\nVertipaq Analyzer: This tool helps analyze the internal structure of your Power BI model. It provides detailed information about the storage and memory usage of your model, including which columns are frequently accessed and loaded into memory. This can help you identify unnecessary columns that are consuming resources.\nSteps:\nExport your Power BI model to a .pbix file.\nOpen the .pbix file in Power BI Desktop.\nUse the Vertipaq Analyzer tool to analyze the model and review the column usage statistics.\n\nC. Query the $System.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS dynamic management view (DMV).\nDMVs: Dynamic Management Views (DMVs) provide detailed information about the operations of your Power BI models. Specifically, the $System.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS DMV can give you insights into the storage and usage patterns of individual columns within your model.","timestamp":"1716833640.0","comment_id":"1219738"},{"upvote_count":"8","comment_id":"1152318","timestamp":"1708133160.0","poster":"Momoanwar","content":"Selected Answer: BC\nI think BC.\nA is only tobread data and D only memory allocations"},{"timestamp":"1735156200.0","content":"Selected Answer: BC\nUse the Vertipaq Analyzer tool.\nC. Query the $System.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS dynamic management view (DMV).","poster":"NRezgui","upvote_count":"1","comment_id":"1331694"},{"upvote_count":"1","poster":"Rakesh16","content":"Selected Answer: BC\nB & C is the answer","comment_id":"1312445","timestamp":"1731650640.0"},{"timestamp":"1720572300.0","comment_id":"1245211","poster":"6d1de25","content":"Selected Answer: AB\nAnswer is A&B","upvote_count":"1"},{"comment_id":"1236182","upvote_count":"5","timestamp":"1719216360.0","poster":"FSCH_111","content":"Selected Answer: BC\nOther Options: WRONG\nA. Use the Analyze in Excel feature: This feature allows for interaction with the model data in Excel but does not provide detailed insights into column-level memory usage.\n\nD. Query the DISCOVER_MEMORYGRANT DMV: This DMV provides information about memory grants for queries but does not provide detailed information about the columns loaded into memory."},{"poster":"stilferx","content":"Selected Answer: BC\nIMHO, \n\nB & C\nBecause:\n1. The DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS schema rowset returns information about the column segments used for storing data for in-memory tables.<336>\n2. Very often there could be a few columns that are not required in your Power BI model, but they take up a lot of space. This is easy to find with Vertipaq Analyzer.\n\nLinks: https://learn.microsoft.com/en-us/openspecs/sql_server_protocols/ms-ssas/948d5135-5bf4-4cf7-82c5-3a38746c2fb8\nhttps://www.fourmoo.com/2020/11/11/how-to-use-vertipaq-analyzer-with-dax-studio-for-power-bi-model-analysis/","comment_id":"1208544","timestamp":"1715201580.0","upvote_count":"4"},{"upvote_count":"2","timestamp":"1713351000.0","poster":"VAzureD","content":"Selected Answer: BC\nB and C\nA. It’s more about data exploration and visualization.\nD. Provides information about memory grants for queries","comment_id":"1197177"},{"upvote_count":"2","timestamp":"1710709320.0","poster":"CLVASQUEZ","content":"Selected Answer: BC\nB and C is the right answer.","comment_id":"1176059"},{"comment_id":"1159941","upvote_count":"3","content":"Selected Answer: BC\nB and C is the right answer.","timestamp":"1708965660.0","poster":"XiltroX"}],"answer_ET":"BC","exam_id":71,"unix_timestamp":1708133160,"question_images":[],"answer_images":[],"question_id":105,"choices":{"A":"Use the Analyze in Excel feature.","C":"Query the $System.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS dynamic management view (DMV).","B":"Use the Vertipaq Analyzer tool.","D":"Query the DISCOVER_MEMORYGRANT dynamic management view (DMV)."},"isMC":true,"question_text":"You have a Fabric tenant that contains a semantic model. The model uses Direct Lake mode.\nYou suspect that some DAX queries load unnecessary columns into memory.\nYou need to identify the frequently used columns that are loaded into memory.\nWhat are two ways to achieve the goal? Each correct answer presents a complete solution.\nNOTE: Each correct answer is worth one point.","timestamp":"2024-02-17 02:26:00","answers_community":["BC (97%)","3%"],"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/134040-exam-dp-600-topic-1-question-30-discussion/","answer":"BC"}],"exam":{"isBeta":false,"provider":"Microsoft","name":"DP-600","lastUpdated":"12 Apr 2025","id":71,"isImplemented":true,"numberOfQuestions":179,"isMCOnly":false},"currentPage":21},"__N_SSP":true}