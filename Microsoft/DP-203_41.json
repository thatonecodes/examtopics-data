{"pageProps":{"questions":[{"id":"dej5j6HnYRuXbnbaYjq8","answers_community":["D (100%)"],"discussion":[{"timestamp":"1656305460.0","upvote_count":"14","comment_id":"510072","poster":"Skeinofi","content":"Selected Answer: D\nThe link provided is the source of truth"},{"timestamp":"1657085820.0","comment_id":"518029","content":"Selected Answer: D\nD is correct","upvote_count":"7","poster":"Teraflow"},{"upvote_count":"1","poster":"positivitypeople","timestamp":"1718997900.0","content":"Got this question today on the exam","comment_id":"1102978"},{"poster":"kkk5566","content":"Selected Answer: D\nD is correct","upvote_count":"1","timestamp":"1709819760.0","comment_id":"1001499"},{"comment_id":"975349","content":"Selected Answer: D\noption D","timestamp":"1707388320.0","poster":"akhil5432","upvote_count":"1"},{"timestamp":"1675525800.0","upvote_count":"2","poster":"Deeksha1234","content":"correct","comment_id":"642458"},{"content":"D is correct.","timestamp":"1663429860.0","comment_id":"569988","poster":"austin06112000","upvote_count":"3"},{"comment_id":"516128","poster":"DrTaz","content":"Selected Answer: D\nCorrect. D is the one that makes most sense.","timestamp":"1656883200.0","upvote_count":"3"}],"question_id":201,"answer":"D","question_text":"You are monitoring an Azure Stream Analytics job by using metrics in Azure.\nYou discover that during the last 12 hours, the average watermark delay is consistently greater than the configured late arrival tolerance.\nWhat is a possible cause of this behavior?","timestamp":"2021-12-27 07:51:00","question_images":[],"choices":{"C":"The late arrival policy causes events to be dropped.","D":"The job lacks the resources to process the volume of incoming data.","A":"Events whose application timestamp is earlier than their arrival time by more than five minutes arrive as inputs.","B":"There are errors in the input data."},"answer_ET":"D","url":"https://www.examtopics.com/discussions/microsoft/view/68629-exam-dp-203-topic-2-question-47-discussion/","isMC":true,"unix_timestamp":1640587860,"topic":"2","exam_id":67,"answer_description":"","answer_images":[]},{"id":"HmgvJXIpiyIXd6eSnYiG","answer_ET":"","isMC":false,"answers_community":[],"answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0023000001.jpg","https://www.examtopics.com/assets/media/exam-media/04259/0023100001.png"],"url":"https://www.examtopics.com/discussions/microsoft/view/67487-exam-dp-203-topic-2-question-48-discussion/","timestamp":"2021-12-10 02:19:00","answer_description":"Box 1: TopOne OVER(PARTITION BY Game ORDER BY Score Desc)\nTopOne returns the top-rank record, where rank defines the ranking position of the event in the window according to the specified ordering. Ordering/ranking is based on event columns and can be specified in ORDER BY clause.\nBox 2: Hopping(minute,5)\nHopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap and be emitted more often than the window size. Events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size.\n\nReference:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/topone-azure-stream-analytics https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions","unix_timestamp":1639099140,"question_id":202,"answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0022900001.png"],"question_text":"HOTSPOT -\nYou are building an Azure Stream Analytics job to retrieve game data.\nYou need to ensure that the job returns the highest scoring record for each five-minute time interval of each game.\nHow should you complete the Stream Analytics query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","exam_id":67,"topic":"2","discussion":[{"timestamp":"1654816740.0","upvote_count":"141","comment_id":"498204","comments":[{"timestamp":"1690380660.0","upvote_count":"11","poster":"MarcelT","content":"The Select built with the TopOne() option would return one row for each game. Still, it would not tell you the game (SELECT TOP_ONE()... FROM). On the other hand, the GAME,MAX() option clearly informs the Game.","comments":[{"timestamp":"1722323700.0","poster":"Gikan","content":"In case of \"game, max()\" the \"group by\" cause should contain the \"game\", otherwise it returns an error","comment_id":"1135635","comments":[{"timestamp":"1722323820.0","poster":"Gikan","upvote_count":"2","content":"but in this case no definition for 5 minutes, so it is a bad idea","comment_id":"1135636"}],"upvote_count":"3"},{"content":"The question was \"the highest scoring record of each game\", so that's what we need - one row for each game","comment_id":"864767","poster":"_lene_","timestamp":"1696776000.0","upvote_count":"3"},{"upvote_count":"3","poster":"cr727","content":"I think its TopOne() as \"TopOne() OVER(partition by Game order by Score Desc)\", it orders by descending of Score and by partition, and top one of each of them.","comment_id":"789480","timestamp":"1690443660.0"}],"comment_id":"788898"}],"poster":"alexleonvalencia","content":"TopOne() / Tumbling"},{"poster":"gf2tw","timestamp":"1654842300.0","upvote_count":"45","comment_id":"498412","content":"Syntax for Hopping window requires 3 arguments, seems this should be Tumbling Window which fulfils the exact same requirements.","comments":[{"content":"Yeah sure","poster":"anto69","comment_id":"535189","timestamp":"1659063420.0","upvote_count":"2"}]},{"content":"topone().. is correct\nbut it is a tumbling window; hopping window has two number, window length and hop length","poster":"Dusica","comment_id":"1201767","timestamp":"1729835460.0","upvote_count":"2"},{"upvote_count":"2","content":"TopOne\nTumbling","timestamp":"1729060800.0","poster":"Alongi","comment_id":"1196405"},{"timestamp":"1727701140.0","content":"Top One and Tumbling","comment_id":"1186151","upvote_count":"1","poster":"Alongi"},{"content":"https://learn.microsoft.com/en-us/stream-analytics-query/topone-azure-stream-analytics\n\nTopone()\nTumbling","comment_id":"1146572","upvote_count":"2","timestamp":"1723307340.0","comments":[{"comment_id":"1151866","upvote_count":"2","content":"I believe Max score is simpler.\nNot making any sense to re-arrange it and then pick the top.","timestamp":"1723793820.0","poster":"j888"}],"poster":"Joanna0"},{"upvote_count":"2","comment_id":"1144584","poster":"wel_fardeheb","content":"got this question on my exam","timestamp":"1723121760.0"},{"upvote_count":"3","timestamp":"1719475380.0","content":"SELECT\n TopOne() OVER(PARTITION BY Game ORDER BY Score Desc) as HighestScore\nFROM input TIMESTAMP BY CreatedAt\nGROUP BY\n Game, TumblingWindow(minute,5)\nHereâ€™s what this query does:\n\nTopOne() OVER(PARTITION BY Game ORDER BY Score Desc) selects the highest scoring record in each partition of games.\nTIMESTAMP BY CreatedAt specifies the timestamp of each event.\nGROUP BY Game, TumblingWindow(minute,5) groups the output by game and in five-minute intervals. The TumblingWindow function creates a series of fixed-sized, non-overlapping and contiguous time intervals.\nPlease note that this is a general guidance and the actual query might need to be adjusted based on the specific requirements and data schema of your game data.","poster":"6d954df","comment_id":"1106701"},{"timestamp":"1717970460.0","poster":"Momoanwar","comment_id":"1092173","content":"Wrong, chatgpt :\nFor the Azure Stream Analytics job that needs to return the highest scoring record for each five-minute interval of each game, the query should use the following options:\n\n1. **SELECT**: `TopOne() OVER(PARTITION BY Game ORDER BY Score Desc)` as HighestScore\n - This function returns the highest score for each partitioned group (each game in this case), ordered by score in descending order, ensuring that the highest score is selected.\n\n2. **GROUP BY**: `TumblingWindow(minute, 5)`\n - This window function groups the events into non-overlapping, continuous five-minute intervals, which is what's required to get the highest score in each five-minute time slice.\n\nThis configuration will ensure that you get the highest score for each game every five minutes.","upvote_count":"2"},{"comment_id":"1085593","poster":"MarkJoh","comments":[{"timestamp":"1725190800.0","poster":"Mausar","comment_id":"1163539","content":"it is accepted both syntax:\n\n{TUMBLINGWINDOW | TUMBLING} ( timeunit , windowsize, [offsetsize] ) {HOPPINGWINDOW | HOPPING} ( timeunit , windowsize , hopsize, [offsetsize] )\nhttps://learn.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics","upvote_count":"2"}],"content":"There really isn't a solution based on the options given.\nFirst off, Hopping() and Tumbling() don't exist, it's HoppingWindow and TumblingWindow\nSo, the group by only has \nGame\nWindows(TublingWindow(minute, 5, Hopping(minute, 5))\nBut, that last one isn't valid either in multiple ways.\nSo, only Group by Game is valid.\nSo, you can try it this way...\nSELECT TOPOne() OVER (PARTITION BY Game ORDER BY Score DESC)\n FROM input TIMESTAMP BY CreatedDt\nGROUP BY game\n\nBut, that does nothing with 5 minute window.\n\nSo, there is NO solution based on the data given.\n\nI would write it this way\nSELECT game, MAX(Score) as MaxScore\n FROM intput TIMESTAMP by CreatedAt\nGroup by Game, TumblingWindow(minute, 5)\n\nBut, those aren't options provided.","timestamp":"1717283880.0","upvote_count":"4"},{"content":"Game(max) and tumbling window is the correct answer","upvote_count":"6","poster":"kkk5566","comment_id":"1001500","timestamp":"1709819880.0"},{"upvote_count":"7","comment_id":"944648","timestamp":"1704552000.0","poster":"pavankr","content":"\"Hopping(minute,5)\" - the syntax itself is wrong. Not sure who is preparing these answers???"},{"poster":"akk_1289","timestamp":"1688833440.0","comments":[{"comment_id":"807487","content":"Totally agree:\n- Game, max() - you need to have [Game] column to know, which game the max score refers to,\n- tumbling window - requires 2 arguments, hopping window could be used, but requires 3 arguments","comments":[{"timestamp":"1703247420.0","comments":[{"upvote_count":"3","poster":"auwia","content":"Topone","timestamp":"1703247480.0","comment_id":"930376"}],"upvote_count":"4","comment_id":"930374","content":"Then you have to put \"game\" attribute in the group by as well, but there is only 1 option and it's without the window! So ripone / tumbling!","poster":"auwia"}],"upvote_count":"10","poster":"mroova","timestamp":"1691929980.0"}],"comment_id":"769702","content":"minute time interval of each game, you can use the TumblingWindow function to define a five-minute tumbling window over the data, and then use the MAX function to select the highest scoring record within each window.","upvote_count":"9"},{"content":"Game(max) and tumbling window is the correct answer","poster":"Achu24","timestamp":"1687485000.0","upvote_count":"7","comments":[{"comment_id":"773260","poster":"mesloth","timestamp":"1689142740.0","content":"Correct. Here top score is being asked, instead of Rank","upvote_count":"3"}],"comment_id":"753833"},{"upvote_count":"1","poster":"JosephVishal","content":"Tumbling window seems to be correct, in question there is no fixed time interval specified.","comment_id":"742958","timestamp":"1686575160.0"},{"comment_id":"720507","poster":"THAYTRUONG","content":"TopOne() / Tumbling is correct answer","timestamp":"1684325040.0","upvote_count":"2"},{"content":"This is clearly a fu**-up, it's a tumbling Window. For sure! I wonder what would happen in the exam if you select Tumbling...","comments":[{"timestamp":"1680944040.0","upvote_count":"3","poster":"allagowf","comment_id":"689149","content":"true, if tumbling is not the correct answer in the exam then we fu**-up really fu**-up hahaha"}],"upvote_count":"3","poster":"bakstorage00001","comment_id":"672276","timestamp":"1679145720.0"},{"poster":"Deeksha1234","content":"It should be TopOne() and Tumbling","timestamp":"1675526100.0","upvote_count":"2","comment_id":"642462"},{"upvote_count":"4","timestamp":"1674226680.0","poster":"jainparag1","comment_id":"634048","content":"It should be Tumbling window."},{"content":"it is Tumbling \"D","poster":"agar","comment_id":"553841","upvote_count":"1","timestamp":"1661180040.0"},{"poster":"chxzqw","comments":[{"comments":[{"upvote_count":"3","timestamp":"1659002220.0","comment_id":"534663","poster":"assU2","comments":[{"upvote_count":"12","poster":"adfgasd","content":"If you use max(score) in first box and game in second, you would not have a max(score) every 5 minutes. If you choose max(score) in first box and tumbling in second, query would return an error, because it misses the group by game clause.","comment_id":"538601","timestamp":"1659430800.0"},{"comment_id":"538699","upvote_count":"3","content":"When you don't group them by Game, when you run Game, Max(Score) it will get the max score out of all games.","poster":"stunner85_","timestamp":"1659437580.0"}],"content":"pls can someone explain in details why not game, max(score) ?"}],"content":"If max(score) is used then we have to have Game in the group by clause","upvote_count":"12","comment_id":"531272","timestamp":"1658656200.0","poster":"svik"}],"content":"pls why not game, max(score) ?","timestamp":"1658467740.0","upvote_count":"7","comment_id":"529670"},{"comment_id":"518031","content":"It should be tumbling window","upvote_count":"3","poster":"Teraflow","timestamp":"1657086000.0"},{"content":"TUMBLING will be the answer folks","comment_id":"502790","timestamp":"1655362740.0","poster":"ayush188","upvote_count":"4"},{"timestamp":"1655233740.0","comment_id":"501671","poster":"m2shines","upvote_count":"2","content":"Incorrect, answer shud be Tumbling"},{"upvote_count":"4","comment_id":"500047","timestamp":"1655035620.0","poster":"jxj770","content":"Tumbling is right"}]},{"id":"MZNwJOxCapEOqRQmdemS","isMC":true,"discussion":[{"poster":"NaiCob","upvote_count":"55","comments":[{"comment_id":"930440","upvote_count":"1","content":"I agree, I've found an articol where it's saying that you can run R script from a Custom .net activity, or better if you have BYOC HDInsight cluster that already has R Installed on it.","timestamp":"1703252280.0","poster":"auwia"}],"comment_id":"506044","content":"Correct answer: No - you cannot execute the R script using a stored procedure activity","timestamp":"1655801820.0"},{"comments":[{"poster":"Rossana","content":"The answer is NO for other reasons than the SP.\nConcerning the SP: To execute an R script within a stored procedure in Synapse Analytics, you can use the sp_execute_external_script system stored procedure. This procedure can be used to execute R scripts, as well as scripts written in other languages such as Python.","timestamp":"1698303360.0","upvote_count":"2","comment_id":"881210"},{"upvote_count":"6","timestamp":"1664339640.0","comment_id":"576612","poster":"sparkchu","content":"i admire your thought, but context looks wanna discriminate the inavailability of R in SP not like that in Databricks."}],"comment_id":"529145","content":"I select A because you can use R script in sp_execute_external_script","upvote_count":"15","timestamp":"1658401380.0","poster":"Daemon69"},{"content":"Selected Answer: B\n\"and then uses a stored procedure to execute the R script\"\nyou cannot use stored procedures to execute R scripts: it's just absurd. Stored procedures are SQL statements.","timestamp":"1727758320.0","upvote_count":"1","poster":"MBRSDG","comment_id":"1187180"},{"comment_id":"1114083","timestamp":"1720121700.0","upvote_count":"6","content":"Selected Answer: B\n**VARIATIONS OF THIS QUESTION**\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. **NO**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse. **YES**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse. **NO**\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse. **YES**","poster":"ExamDestroyer69"},{"upvote_count":"2","timestamp":"1712299320.0","content":"While the most voted is 'No', the green bar indicates 'Yes'. My first inclination was 'No' as well, because it's not the easiest or most logical way to do it. Then again, there's many roads to Rome. Does it meet the goal, yes.","poster":"TheReg81","comment_id":"1025334"},{"poster":"kkk5566","upvote_count":"1","timestamp":"1709820360.0","comment_id":"1001513","content":"Selected Answer: B\nB should be correct"},{"timestamp":"1697090100.0","poster":"FRANCIS_A_M","upvote_count":"3","comment_id":"867951","content":"Correct Answer B: \nThe solution proposed does not meet the goal because it suggests executing the R script using a stored procedure in the data warehouse. Azure Synapse Analytics does not support executing R scripts directly within stored procedures. Instead, you should use Azure Data Factory to orchestrate the process, using an Azure Machine Learning activity to execute the R script for data transformation before loading the transformed data into Azure Synapse Analytics."},{"content":"Btw.. Is it worth to pay for accessing the rest of pages? Since the actual value is community discussion. And beyond this point, it's supposed to be less people.","upvote_count":"3","comment_id":"830947","poster":"Kamekung","timestamp":"1694006280.0","comments":[{"content":"I have paid for further access and would say it is worth it. The community discussion continues","upvote_count":"2","poster":"FRANCIS_A_M","timestamp":"1695635820.0","comment_id":"850091"}]},{"content":"Cant we fix answers correctly in the portal, instead of relying on votes","timestamp":"1691979060.0","comment_id":"808066","upvote_count":"3","poster":"bubby248"},{"comment_id":"796302","timestamp":"1690991940.0","content":"Correct","upvote_count":"1","poster":"mckovin"},{"comments":[{"poster":"CNBOOST2","upvote_count":"3","timestamp":"1690623600.0","comment_id":"791592","content":"I think this is not possible we have to pay :("},{"poster":"Dusica","content":"you can't skip the payment. It is not too much money for its worth","upvote_count":"1","comment_id":"1201771","timestamp":"1729835940.0"}],"upvote_count":"2","comment_id":"780260","timestamp":"1689694500.0","content":"Next page is asking for contributor access, anyone have credentials or how we can skip the payment","poster":"millusmiley"},{"timestamp":"1689142140.0","upvote_count":"3","content":"Selected Answer: B\nthere is a staging zone in Azure Data Lake Storage. The very fact that A suggest copying into DWH staging zone makes it invalid so any other discussion is unnecessary. It is B","comment_id":"773251","poster":"Dusica"},{"poster":"akk_1289","upvote_count":"1","comment_id":"769703","timestamp":"1688833500.0","content":"his solution does not meet the goal of the daily process you have described. While using an Azure Data Factory schedule trigger to execute a pipeline is a good approach for scheduling the process to run on a daily basis, the pipeline you have described does not include any steps to transform the data using an R script.\n\nTo meet the goal of the daily process, you will need to include a step in the pipeline to execute the R script that transforms the data. One way to do this would be to use an Azure Data Factory activity, such as an Execute R Script activity, to run the R script on the data as it is being copied from the staging zone to the staging table in the data warehouse. You can then use a stored procedure or another Data Factory activity, such as an SQL activity, to insert the transformed data into the final destination table in the data warehouse."},{"content":"Synapse doesn't support R at the moment\nhttps://docs.microsoft.com/en-us/answers/questions/222624/is-azure-synapse-analytics-supporting-r-language.html","poster":"Tj87","comment_id":"646108","timestamp":"1676269140.0","upvote_count":"2"},{"poster":"Deeksha1234","content":"should be B","timestamp":"1675583640.0","comment_id":"642765","upvote_count":"3"},{"content":"As per problem, Azure Data Lake Storage account that contains a staging zone. From staging zone, transform the data and insert into Azure Synapse Analytics.\nBut the solution providing as copy data to a staging table in data warehouse.\nAs per problem, staging will be in Azure Data Lake Storage account, not in data warehouse.\nAnswer is 'B'","timestamp":"1671106740.0","upvote_count":"5","comment_id":"616675","poster":"nilubabu"},{"poster":"rafaelptu","content":"Sim, o script vai ser executado e carregado posteriormente a execuÃ§Ã£o pode ser chamada pela sp_exec_external_script","upvote_count":"1","comment_id":"570742","timestamp":"1663524960.0"},{"comment_id":"540563","content":"Selected Answer: B\nShould be NO","timestamp":"1659629160.0","poster":"Philipp","upvote_count":"1"},{"content":"The need is to transform using R script and load into synapse. So answer is no.","upvote_count":"1","comment_id":"531883","timestamp":"1658720400.0","poster":"dev2dev"},{"content":"Selected Answer: B\nShould be no","poster":"[Removed]","comment_id":"521107","timestamp":"1657476660.0","upvote_count":"3"},{"poster":"engrbrain","timestamp":"1657254360.0","comment_id":"519343","upvote_count":"3","content":"The Answer is NO. Stored Procedure Activity cannot run R Script"},{"comments":[{"timestamp":"1663485300.0","upvote_count":"2","content":"It is not run in Azure Synapse Analytics. The question says the SP transforms the data before inserting into Azure Synapse Analytics","poster":"vanrell","comments":[{"poster":"Amsterliese","timestamp":"1665300780.0","comment_id":"583187","upvote_count":"1","content":"How I understand it, that is what is required, but not what the suggested solution offers. The suggested solution copies the data into the SQL DW first (?)"}],"comment_id":"570420"}],"timestamp":"1656960720.0","content":"Selected Answer: B\nShould be No.\nR script cannot be run in Azure Synapse Analytics.\nsp_execute_external_script can only be applied to SQL Server 2016 (13.x) and later, Azure SQL Managed Instance.\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/r-developers-guide","poster":"Canary_2021","upvote_count":"5","comment_id":"516988"},{"content":"Selected Answer: B\nUnfortunately R support within Synapse Spark is currently not available.","timestamp":"1656461940.0","poster":"lucky7_2000","upvote_count":"2","comment_id":"511715"},{"poster":"tony4fit","upvote_count":"2","content":"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices#load-to-a-staging-table","comment_id":"504194","timestamp":"1655547300.0"}],"timestamp":"2021-12-18 13:15:00","question_id":203,"topic":"2","answer_ET":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script.\nDoes this meet the goal?","url":"https://www.examtopics.com/discussions/microsoft/view/68237-exam-dp-203-topic-2-question-49-discussion/","answer_description":"","answers_community":["B (100%)"],"question_images":[],"choices":{"B":"No","A":"Yes"},"exam_id":67,"answer_images":[],"unix_timestamp":1639829700,"answer":"B"},{"id":"HRLN9u3Nxw56X1sr8nEc","question_text":"HOTSPOT -\nYou are processing streaming data from vehicles that pass through a toll booth.\nYou need to use Azure Stream Analytics to return the license plate, vehicle make, and hour the last vehicle passed during each 10-minute window.\nHow should you complete the query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04259/0015200001.png"],"answer_ET":"","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/61527-exam-dp-203-topic-2-question-5-discussion/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04259/0015300001.png"],"topic":"2","question_id":204,"answers_community":[],"answer_description":"Box 1: MAX -\nThe first step on the query finds the maximum time stamp in 10-minute windows, that is the time stamp of the last event for that window. The second step joins the results of the first query with the original stream to find the event that match the last time stamps in each window.\nQuery:\n\nWITH LastInWindow AS -\n(\n\nSELECT -\n\nMAX(Time) AS LastEventTime -\n\nFROM -\n\nInput TIMESTAMP BY Time -\n\nGROUP BY -\nTumblingWindow(minute, 10)\n)\n\nSELECT -\nInput.License_plate,\nInput.Make,\n\nInput.Time -\n\nFROM -\n\nInput TIMESTAMP BY Time -\n\nINNER JOIN LastInWindow -\nON DATEDIFF(minute, Input, LastInWindow) BETWEEN 0 AND 10\nAND Input.Time = LastInWindow.LastEventTime\n\nBox 2: TumblingWindow -\nTumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals.\n\nBox 3: DATEDIFF -\nDATEDIFF is a date-specific function that compares and returns the time difference between two DateTime fields, for more information, refer to date functions.\nReference:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics","timestamp":"2021-09-04 15:39:00","discussion":[{"content":"correct","upvote_count":"31","comments":[{"content":"Why not Hopping Window??","comments":[{"upvote_count":"5","poster":"auwia","comment_id":"936609","content":"it needs 3 parameters in input.","timestamp":"1703771520.0"},{"upvote_count":"12","timestamp":"1657536600.0","content":"Because a hopping window can overlap, and we need the data from 10 minute time frames that DON'T overlap","poster":"Wijn4nd","comment_id":"521538"}],"upvote_count":"1","timestamp":"1655615820.0","poster":"Jerrylolu","comment_id":"504684"}],"comment_id":"452070","timestamp":"1648341600.0","poster":"rikku33"},{"timestamp":"1690733940.0","upvote_count":"20","poster":"GodfreyMbizo","comments":[{"comment_id":"1100504","poster":"ExamDestroyer69","content":"The referenced link shows the provided answer is correct","upvote_count":"2","timestamp":"1718784240.0"},{"poster":"goldy29","content":"You are great!","upvote_count":"1","timestamp":"1704695640.0","comment_id":"946166"}],"content":"answer is here https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#return-the-last-event-in-a-window","comment_id":"793166"},{"poster":"sdg2844","content":"I agree with the person who said this says to define the last event in that HOUR for the 10 minute window increments. So it should only return one value. This will return more than one value, as it returns the last values in EACH 10 minute window, not the maximum timestamp for a defined HOUR.","upvote_count":"1","timestamp":"1719673740.0","comment_id":"1108930"},{"upvote_count":"3","poster":"blazy001","timestamp":"1718360880.0","content":"error in code: \nwrong: DATEDIFF( minut, Input, LastInWindow)\ncorrect: DATEDIFF( minut, Input, LastEventTime)","comment_id":"1096455","comments":[{"comment_id":"1167234","content":"Not according to Microsoft: https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#return-the-last-event-in-a-window\nSpecial thanks to @GodfreyMbizo","upvote_count":"2","poster":"lola_mary5","timestamp":"1725626460.0"}]},{"upvote_count":"1","timestamp":"1712250240.0","comment_id":"1025011","poster":"gggqqqqq","content":"DATEDIFF used in the SELECT statement uses the general syntax where a datetime column or expression is passed in as the second and third parameter. However, when the DATEDIFF function is used inside the JOIN condition, the input_source name or its alias is used. Internally the timestamp associated for each event in that source is picked."},{"timestamp":"1709638620.0","poster":"kkk5566","comment_id":"999308","upvote_count":"1","content":"correct"},{"upvote_count":"3","content":"max / tumblingwindow/datediff","comment_id":"895024","poster":"mamahani","timestamp":"1699711140.0"},{"comment_id":"791846","content":"May be a dumb question but why the datediff condition when the other condition is exactly matching on timestamp. Is it not unnecessary?","timestamp":"1690641960.0","comments":[{"upvote_count":"2","content":"exactly my thought too; we already have the unique timestamp per 10 min windows, so why simply not match the car's (event) timestamp with the max? what is the value added of the datediff","poster":"mamahani","comment_id":"876368","timestamp":"1697880780.0"},{"timestamp":"1718524620.0","comments":[{"poster":"meatpoof","timestamp":"1719761280.0","comment_id":"1110728","upvote_count":"1","content":"it's AND Input.Time = LastInWindow.LastEventTime...i'm thinking this condition would invalidate anything that's only close in time, but not an exact match"}],"poster":"alphilla","upvote_count":"1","content":"The reason for using DATEDIFF in this context is to create a condition for joining the two streams based on a time window. If you directly join on Input.Time = LastInWindow.LastEventTime, you are essentially checking for an exact match of timestamps. This might not capture events that are close to each other in time but are not exactly equal.\n\nBy using DATEDIFF, you allow for a time range (0 to 10 minutes) within which events will be considered as part of the same window. This ensures that events occurring slightly before or after the last event in the window are included in the result.","comment_id":"1098045"}],"upvote_count":"6","poster":"AZLearn111"},{"comments":[{"content":"I have the answer here:\nhttps://learn.microsoft.com/en-us/stream-analytics-query/join-azure-stream-analytics","upvote_count":"4","timestamp":"1685440440.0","comment_id":"731428","poster":"Bro111"}],"poster":"Bro111","content":"Are \"Input\" and \"LastInWindow\" DateTime fields, to be comapred with datediff???","comment_id":"730970","timestamp":"1685401380.0","upvote_count":"2"},{"poster":"anks84","comment_id":"660700","timestamp":"1678075440.0","upvote_count":"1","content":"CORRECT"},{"upvote_count":"2","content":"correct","poster":"Deeksha1234","comment_id":"647022","timestamp":"1676443200.0"},{"comment_id":"641438","poster":"y203","content":"The full example with the answer is here: \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#return-the-last-event-in-a-window","upvote_count":"3","timestamp":"1675375260.0"},{"poster":"Sriramiyer92","content":"Correct! \nThe keyword \"each\" in \"last vehicle passed during each 10-minute window\" pretty much makes it Clear!","timestamp":"1674156900.0","upvote_count":"1","comment_id":"633692"},{"comment_id":"624335","upvote_count":"3","comments":[{"upvote_count":"2","timestamp":"1684304400.0","poster":"sensaint","comment_id":"720321","content":"In order to match the correct inputs with the last event in window, the difference between both times should not exceed 10 minutes."}],"timestamp":"1672280220.0","content":"I understand the first two, but why datediff? They ask for the hour that the last vehicle went through, shouldn't that be datepart?","poster":"Revave2"},{"poster":"PallaviPatel","timestamp":"1659086220.0","upvote_count":"1","comment_id":"535412","content":"correct."},{"comment_id":"534839","upvote_count":"4","content":"HoppingWindow has a minimum of three arguments whereas TumblingWindow only takes two so considering the solution only has two arguments it has to be Tumbling\n\nhttps://docs.microsoft.com/en-us/stream-analytics-query/hopping-window-azure-stream-analytics\nhttps://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics","poster":"BusinessApps","timestamp":"1659018120.0"},{"content":"Answer is 100% correct.","upvote_count":"2","comment_id":"515282","poster":"DrTaz","timestamp":"1656797820.0"},{"upvote_count":"1","comment_id":"514290","comments":[],"timestamp":"1656629280.0","content":"definitively hopping. because the event (last car passing) can be part of more than one window. Thus it cant be tumbling","poster":"bubububox"},{"timestamp":"1656121560.0","content":"Why not Select COunt?","poster":"durak","comment_id":"508949","upvote_count":"1","comments":[{"upvote_count":"1","content":"max is used to get \"last\" event.","comment_id":"515280","poster":"DrTaz","timestamp":"1656797760.0"}]},{"timestamp":"1647766920.0","poster":"irantov","upvote_count":"3","content":"I think it is correct. Although, we could also use hoppingwindow. But it would be better to use Tumblingwindow as time events are unique.","comment_id":"448032","comments":[{"comment_id":"459736","upvote_count":"2","timestamp":"1649527140.0","poster":"TelixFom","content":"I was thinking TumblingWindow based on the term: \"each 10-minute window.\" This infers that the situation is not looking for a rolling max."}]},{"poster":"elcholo","timestamp":"1646408340.0","comment_id":"439185","comments":[{"content":"very confusing","timestamp":"1646932260.0","upvote_count":"4","comment_id":"442547","poster":"GameLift"}],"content":"QUEEEE!","upvote_count":"4"}],"exam_id":67,"unix_timestamp":1630762740},{"id":"VhtW3gnwUWRA9rbv1FOO","answer":"B","answer_ET":"B","question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\nâœ‘ A workload for data engineers who will use Python and SQL.\nâœ‘ A workload for jobs that will run notebooks that use Python, Scala, and SQL.\nâœ‘ A workload that data scientists will use to perform ad hoc analysis in Scala and R.\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\nâœ‘ The data engineers must share a cluster.\nâœ‘ The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\nâœ‘ All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\nYou need to create the Databricks clusters for the workloads.\nSolution: You create a High Concurrency cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.\nDoes this meet the goal?","url":"https://www.examtopics.com/discussions/microsoft/view/52911-exam-dp-203-topic-2-question-50-discussion/","question_images":[],"discussion":[{"comment_id":"439618","timestamp":"1646479680.0","upvote_count":"45","comments":[{"upvote_count":"2","timestamp":"1658441160.0","comments":[{"comment_id":"613128","upvote_count":"8","content":"gina8008 you are missing a point here that data scientists uses scala as per question and scala is not supported in high concurrency cluster. So the answer is no","poster":"Aditya0891","timestamp":"1670491080.0"}],"poster":"Gina8008","comment_id":"529486","content":"engineer has to share the cluster so high -concurrency is correct. the answer should be A"}],"poster":"djincheg","content":"data scientist need scala so standard, jobs need scala so standard, so B but for different reasons"},{"poster":"111222333","timestamp":"1637102940.0","upvote_count":"17","content":"Correct is A","comment_id":"359041","comments":[{"timestamp":"1637304360.0","poster":"dfdsfdsfsd","comment_id":"361004","content":"Agree. Jobs cannot use a high-concurrency cluster because it does not support Scala.","upvote_count":"5","comments":[{"poster":"Aditya0891","comment_id":"613129","content":"and what about the data scientists requirement? Read the question properly and dn't mislead people looking for answers. Scala is not supported in high concurrency and data scientists are using scala as per question so answer in No","timestamp":"1670491200.0","upvote_count":"8"}]}]},{"timestamp":"1710430200.0","content":"Selected Answer: B\nAnswer : B\n\"High Concurrency clusters can run workloads developed in SQL, Python, and R.\"\nhttps://learn.microsoft.com/en-us/azure/databricks/archive/compute/configure","comments":[{"content":"This is an old link (taken from /archive/) and says:\n\"Standard mode clusters are now called No Isolation Shared access mode clusters.\nHigh Concurrency with Tables ACLs are now called Shared access mode clusters.\"\nNew link: https://learn.microsoft.com/en-us/azure/databricks/compute/configure","timestamp":"1725619560.0","upvote_count":"3","poster":"lola_mary5","comment_id":"1167159"}],"upvote_count":"1","comment_id":"1007692","poster":"Chemmangat"},{"timestamp":"1709820300.0","comment_id":"1001508","poster":"kkk5566","content":"Selected Answer: B\nB should be correct","upvote_count":"1"},{"content":"Selected Answer: B\nNO is correct answer","upvote_count":"2","poster":"akhil5432","timestamp":"1707388500.0","comment_id":"975352"},{"timestamp":"1662832560.0","poster":"Hanse","comment_id":"565010","content":"As per Link: https://docs.azuredatabricks.net/clusters/configure.html\n\nStandard and Single Node clusters terminate automatically after 120 minutes by default. --> Data Scientists\nHigh Concurrency clusters do not terminate automatically by default.\nA Standard cluster is recommended for a single user. --> Standard for Data Scientists & High Concurrency for Data Engineers\nStandard clusters can run workloads developed in any language: Python, SQL, R, and Scala.\n\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. --> Jobs needs Scala, hence: Standard","upvote_count":"3"},{"content":"Selected Answer: B\nNO - as High concurrency not support Scala","upvote_count":"6","comment_id":"500990","poster":"avijitd","timestamp":"1655164620.0"},{"comment_id":"496100","timestamp":"1654605360.0","poster":"rashjan","upvote_count":"5","content":"Selected Answer: B\ncorrect: no"},{"poster":"arjunbhai","timestamp":"1652741580.0","upvote_count":"2","content":"Like djincheg said, Data scientists need scala so B.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure","comment_id":"479732"},{"content":"-Data Engineers: Correct, they are working together, thay need High-Concurency cluster\n-Jobs: Correct, Standad Cluster since it supports SCALA \nHOWEVER:\n- Data Scientists need cluster who terminates after 120 minutes automatically: THAT MEANS ONLY STANDARD AND SINGLE NODE CLUSTERS CAN SUPPORT THAT. \n\nSince this is the holistic question, the answer is NO.","timestamp":"1647520980.0","comment_id":"446548","upvote_count":"15","poster":"Julius7000"},{"timestamp":"1647519600.0","upvote_count":"2","content":"All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity.\nThat means they need standard cluster, not high-concurency cluster. STANDARD cluster terminates automatically after 120 minutes:\n\"Standard and Single Node clusters terminate automatically after 120 minutes by default.\"\nIMO the answer is NO, since all 3 solutions have to be correct.","poster":"Julius7000","comment_id":"446536"},{"poster":"michalS","comment_id":"437920","timestamp":"1646236800.0","upvote_count":"4","content":"It's correct that standard cluster is for job workload, but they assigned high concurrency cluster for data scientist, who want to use scala too, so it's false"},{"timestamp":"1639692180.0","comment_id":"383683","content":"Answer: A\n-Data scientist should have their own cluster and should terminate after 120 mins - STANDARD\n-Cluster for Jobs should support scala - STANDARD\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure","upvote_count":"2","poster":"damaldon","comments":[{"poster":"kimalto452","comment_id":"446194","content":"Solution: You create a High Concurrency cluster for each data scientist\nDoes this meet the goal?\nA. Yes\n\nAnswer: A\n-Data scientist should have their own cluster and should terminate after 120 mins - STANDARD\n\nGENIUSSSSSSSSSS","timestamp":"1647476880.0","upvote_count":"1"}]},{"poster":"Sunnyb","content":"A is the right answer because Standard cluster supports scala","timestamp":"1638638400.0","comment_id":"374449","upvote_count":"2"},{"comment_id":"360574","poster":"Wisenut","upvote_count":"6","content":"I too agree on the comment by 111222333. As per the requirement \" A workload for jobs that will run notebooks that use Python, Scala, and SOL\". Scala is only supported by Standard","timestamp":"1637253000.0"}],"choices":{"B":"No","A":"Yes"},"isMC":true,"exam_id":67,"unix_timestamp":1621198140,"answer_images":[],"question_id":205,"answer_description":"","topic":"2","timestamp":"2021-05-16 22:49:00","answers_community":["B (100%)"]}],"exam":{"id":67,"isImplemented":true,"numberOfQuestions":384,"provider":"Microsoft","isMCOnly":false,"lastUpdated":"12 Apr 2025","name":"DP-203","isBeta":false},"currentPage":41},"__N_SSP":true}