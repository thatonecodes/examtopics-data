{"pageProps":{"questions":[{"id":"EiAZpfFpWchzQYuVsm01","question_id":171,"answer_images":[],"question_text":"You are designing an Azure Databricks cluster that runs user-defined local processes.\nYou need to recommend a cluster configuration that meets the following requirements:\n✑ Minimize query latency.\n✑ Reduce overall costs without compromising other requirements.\n✑ Maximize the number of users that can run queries on the cluster at the same time.\nWhich cluster type should you recommend?","topic":"4","url":"https://www.examtopics.com/discussions/microsoft/view/52471-exam-dp-201-topic-4-question-11-discussion/","discussion":[{"upvote_count":"4","timestamp":"1620773160.0","comment_id":"355063","content":"correct","poster":"NamishBansal"}],"timestamp":"2021-05-12 00:46:00","unix_timestamp":1620773160,"choices":{"C":"High Concurrency with Autoscaling","A":"Standard with Autoscaling","B":"High Concurrency with Auto Termination","D":"Standard with Auto Termination"},"isMC":true,"answer_ET":"C","answers_community":[],"answer_description":"High Concurrency clusters allow multiple users to run queries on the cluster at the same time, while minimizing query latency. Autoscaling clusters can reduce overall costs compared to a statically-sized cluster.\nIncorrect Answers:\nA, D: Standard clusters are recommended for a single user.\nReference:\nhttps://docs.azuredatabricks.net/user-guide/clusters/create.html https://docs.azuredatabricks.net/user-guide/clusters/high-concurrency.html#high-concurrency https://docs.azuredatabricks.net/user-guide/clusters/terminate.html https://docs.azuredatabricks.net/user-guide/clusters/sizing.html#enable-and-configure-autoscaling","answer":"C","question_images":[],"exam_id":66},{"id":"Q9YF0WdKU9rxO3rXBGAr","answer":"A","isMC":true,"timestamp":"2020-03-12 10:20:00","exam_id":66,"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0031100001.png"],"answers_community":[],"answer_description":"With Azure Cosmos DB, you can add or remove the regions associated with your account at any time.\nMulti-region accounts configured with multiple-write regions will be highly available for both writes and reads. Regional failovers are instantaneous and don't require any changes from the application.\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/high-availability","question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/16373-exam-dp-201-topic-4-question-12-discussion/","discussion":[{"timestamp":"1584004800.0","content":"You need replicate data only for reading and minimizes costs. So I think the correct answer is B. Use a single Azure Cosmos DB account Configure data replication.\nReference:\nhttps://docs.microsoft.com/en-us/learn/modules/distribute-data-globally-with-cosmos-db/2-global-distribution","poster":"epgd","comment_id":"62923","comments":[{"upvote_count":"12","timestamp":"1617865800.0","comment_id":"330957","content":"The highest voted answer is wrong. That kind of things doesn’t help.\nThe answer given is correct.","poster":"maynard13x8"},{"content":"another proof here : https://docs.microsoft.com/en-us/azure/cosmos-db/high-availability#multi-region-accounts-with-a-single-write-region-write-region-outage","comment_id":"392895","timestamp":"1624882560.0","poster":"tes","upvote_count":"1"},{"timestamp":"1621956600.0","poster":"111222333","upvote_count":"5","comments":[{"upvote_count":"1","content":"Pricing ref: https://azure.microsoft.com/en-us/pricing/details/cosmos-db/","comment_id":"366543","poster":"111222333","timestamp":"1621956780.0"}],"content":"Correct answer: B.\n\nRequirement is *minimized read latency at minimal cost*.\n\nBoth *Data replication* and *Multi-region writes* reduce the read latency - because both options increase the number of *read regions*. Larger number of read regions => less latency.\n- *Data replication* allows you to have one write region and *multiple read regions*.\n- *Multi-region writes* is an additional option under *Data replication* that allows you to have multiple *write regions*. It also increases the number of *read regions*, of course.\nBut multi-region writes offer more than we need here, since we do not have a requirement for faster write requests. \n\n*Multi-region writes* is a more expensive option because write requests are more expensive than read requests:\n- Single-region write account distributed across N regions: 0.008$\n- Multi-region write account with N regions: 0.016$","comment_id":"366537"}],"upvote_count":"67"},{"comment_id":"65206","timestamp":"1584454560.0","upvote_count":"20","poster":"avestabrzn","content":"the given answer is correct. Enable multi-region writes","comments":[{"comments":[{"comments":[{"upvote_count":"7","content":"Multi write will cost 3 times more than single write, multi read in this scenario.","poster":"Leonido","comment_id":"82564","timestamp":"1588413240.0"}],"timestamp":"1588337400.0","upvote_count":"4","comment_id":"82172","content":"but part of one of the requirements is \"and minimizes costs.\" would data replication be cheapest way to accomplish? I think given answer is correct.","poster":"runningman"},{"comments":[{"comment_id":"350334","content":"it's this option on Azure portal > Cosmos DB Account > \"Replicate data globally\"","timestamp":"1620221760.0","poster":"saifone","upvote_count":"2"}],"comment_id":"332625","timestamp":"1618066260.0","content":"There is no option for Data Replication. You can only set GeoRedundancy o Enable multi-region writes","poster":"maynard13x8","upvote_count":"1"}],"timestamp":"1587376200.0","content":"Why ? there are not write requirements but there are reading requirements\nso i think it should be B","comment_id":"76895","upvote_count":"7","poster":"MLCL"}]},{"upvote_count":"1","comment_id":"379389","timestamp":"1623377100.0","poster":"Mandar77","content":"I think answer is correct. If you configure multiple region writes, updating of data might be delayed based on consistency level configured but reads will be faster for already replicated data.","comments":[{"comment_id":"392896","upvote_count":"1","timestamp":"1624882680.0","poster":"tes","content":"\"for already replicated data\" yes there is another answer for it"}]},{"timestamp":"1623139320.0","content":"The correct answer is : B : Use a single Azure Cosmos DB account Configure data replication.","poster":"Qrm_1972","upvote_count":"3","comment_id":"377325"},{"content":"I think the correct answer is B.\n\nReference: Whizlabs Course!","poster":"AngelRio","upvote_count":"3","comment_id":"372943","timestamp":"1622657340.0"},{"upvote_count":"3","content":"B. Use a single Azure Cosmos DB account Configure data replication.","timestamp":"1619721540.0","comment_id":"345606","poster":"davita8"},{"timestamp":"1614317340.0","upvote_count":"2","poster":"Kachra","comment_id":"299549","content":"From the reference below it appears A is the correct answer.\n\nTo ensure high write and read availability, configure your Azure Cosmos account to span at least two regions with multiple-write regions. This configuration will provide the highest availability, lowest latency, and best scalability for both reads and writes backed by SLAs. To learn more, see how to configure your Azure Cosmos account with multiple write-regions.\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/high-availability#building-highly-available-applications","comments":[{"upvote_count":"1","content":"but what about the costs?","poster":"HarishDP","comment_id":"300499","timestamp":"1614466740.0"}]},{"content":"https://docs.microsoft.com/en-us/azure/cosmos-db/tutorial-global-distribution-sql-api?tabs=dotnetv2%2Capi-async\nAnswer is B","upvote_count":"1","timestamp":"1607168340.0","poster":"syu31svc","comment_id":"235659"},{"poster":"sandGrain","upvote_count":"1","comment_id":"213202","content":"B is the correct answer. Requirement is Read availability at minimal cost","timestamp":"1604552280.0"},{"timestamp":"1603701300.0","comment_id":"206098","upvote_count":"5","content":"Assuming that \"the company has locations around the world\" means there is multi-region writes required for the solution, the given answer is correct. I think the \"minimizes latency for data read\" requirement is the confusing part. But multi-region writes would minimize the read latency. On the other hand, \"and minimizes costs\" part seems not to be covered by this solution. All in all, I think A is a valid answer and in real-world circumstances, it should be used for international companies.","poster":"M0e"},{"content":"cost of replicating to a region is the same as the original region, so replicating to 3 additional regions, would cost approximately four times the original non-replicated database","upvote_count":"1","timestamp":"1601133420.0","comment_id":"187742","poster":"Johnrob"},{"content":"The answer is correct. The requirement is \"minimizes latency for data read\". The table (see link below) shows \"low\" read latency for Multi-region writes while \"Cross region\" for the rest.\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/high-availability#availability-zone-support","comments":[{"timestamp":"1598672520.0","comment_id":"168917","content":"With B there z latency","poster":"kilowd","upvote_count":"1"}],"upvote_count":"7","poster":"extraego","timestamp":"1597450140.0","comment_id":"158381"},{"timestamp":"1596515640.0","content":"With multi-region writes, you can set session consistency (which read your own writes). With this you can minimize latency to read/write around the world and also required less RU's compare to Strong consistency so it minimize the cost as well. Another thing is if the application is being use around the world then read/writes happening around the world. In that scenarios, you need multi-region writes to minimize the latency and good partition key to for performance and reduce the costs for read/write operations.","upvote_count":"2","poster":"NikP","comment_id":"150167"},{"content":"Why not D. Use a single Azure Cosmos DB account. Enable geo-redundancy.?\nAs we need to minimize cost and we only need read not write","upvote_count":"3","poster":"AhmedReda","comments":[{"content":"Because you will need RA-RGS not just RGS.","upvote_count":"3","comment_id":"137695","timestamp":"1595053020.0","poster":"peppele"},{"content":"Geo-redundancy is more related to failover mechanishms. what you need is a data replication technique here. You need to have single write region and multiple read regions to minimize the read latencies across the globe","upvote_count":"3","poster":"poundmanluffy","timestamp":"1613044800.0","comment_id":"288257"}],"timestamp":"1593172980.0","comment_id":"120497"},{"timestamp":"1592814180.0","upvote_count":"3","content":"once data is replicated, the read consistency becomes 99.999 i.e. reads can happen from any region, for writes to offer 99.999 one must enable multi-master model, by enabling multi-region writes for other replicated regions. So B is appropriate here.","poster":"Abhilvs","comment_id":"116185"},{"poster":"Runi","comment_id":"109161","timestamp":"1592021940.0","content":"Single-region accounts may lose availability following a regional outage. It's always recommended to set up at least two regions (preferably, at least two write regions) with your Cosmos account to ensure high availability at all times.\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/high-availability\nI think given answer is right.","upvote_count":"1"},{"timestamp":"1588412880.0","poster":"Leonido","content":"So far, C is wrong because it will only replicate to 1 paired region.\nA seems to be overkill - there is no write redundancy requirement, and if current write region fail, Cosmos will auto failover to one of the read regions and will make it write enable.\nSo B looks like a right answer to me.","upvote_count":"12","comment_id":"82562"},{"upvote_count":"12","poster":"Nehuuu","timestamp":"1584412860.0","content":"Believe the correct answer should be B.","comment_id":"64980"}],"question_text":"You design data engineering solutions for a company that has locations around the world. You plan to deploy a large set of data to Azure Cosmos DB.\nThe data must be accessible from all company locations.\nYou need to recommend a strategy for deploying the data that minimizes latency for data read operations and minimizes costs.\nWhat should you recommend?","choices":{"B":"Use a single Azure Cosmos DB account Configure data replication.","E":"Use multiple Azure Cosmos DB accounts. Enable multi-region writes.","D":"Use a single Azure Cosmos DB account. Enable geo-redundancy.","C":"Use multiple Azure Cosmos DB accounts. For each account, configure the location to the closest Azure datacenter.","A":"Use a single Azure Cosmos DB account. Enable multi-region writes."},"answer_ET":"A","topic":"4","question_id":172,"unix_timestamp":1584004800},{"id":"V9nw56j1We1w05TFhxi6","answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/54681-exam-dp-201-topic-4-question-13-discussion/","isMC":false,"unix_timestamp":1622943840,"answer_ET":"","timestamp":"2021-06-06 03:44:00","exam_id":66,"topic":"4","answer_description":"Storage location: Azure Blob Storage\nArchiving method: A lifecycle management policy\nAzure Blob storage lifecycle management offers a rich, rule-based policy for GPv2 and Blob storage accounts. Use the policy to transition your data to the appropriate access tiers or expire at the end of the data's lifecycle.\nThe lifecycle management policy lets you:\n✑ Transition blobs to a cooler storage tier (hot to cool, hot to archive, or cool to archive) to optimize for performance and cost\n✑ Delete blobs at the end of their lifecycles\n✑ Define rules to be run once per day at the storage account level\nApply rules to containers or a subset of blobs\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts","question_id":173,"question_text":"HOTSPOT -\nYou are designing a solution to store flat files.\nYou need to recommend a storage solution that meets the following requirements:\n✑ Supports automatically moving files that have a modified date that is older than one year to an archive in the data store\n✑ Minimizes costs\nA higher latency is acceptable for the archived files.\nWhich storage location and archiving method should you recommend? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"timestamp":"1622943840.0","comment_id":"375553","upvote_count":"3","content":"the answer is correct!","poster":"BobFar"}],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0031300001.jpg","https://www.examtopics.com/assets/media/exam-media/03774/0031300005.png"],"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0031200001.jpg"],"answer":""},{"id":"N7Zzwal8lvVsLl8y0Uj0","exam_id":66,"answer_description":"Db1: A single read/write region -\nDb2: A single write region and multi read regions\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/high-availability","question_text":"HOTSPOT -\nYou are planning the deployment of two separate Azure Cosmos DB databases named db1 and db2.\nYou need to recommend a deployment strategy that meets the following requirements:\n✑ Costs for both databases must be minimized.\n✑ Db1 must meet an availability SLA of 99.99% for both reads and writes.\n✑ Db2 must meet an availability SLA of 99.99% for writes and 99.999% for reads.\nWhich deployment strategy should you recommend for each database? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","unix_timestamp":1622943900,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0031500001.jpg"],"answers_community":[],"discussion":[{"poster":"BobFar","comment_id":"375554","timestamp":"1622943900.0","content":"The answer is correct","upvote_count":"6"}],"topic":"4","question_id":174,"answer":"","answer_ET":"","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0031600001.jpg","https://www.examtopics.com/assets/media/exam-media/03774/0031700001.png"],"timestamp":"2021-06-06 03:45:00","url":"https://www.examtopics.com/discussions/microsoft/view/54682-exam-dp-201-topic-4-question-14-discussion/","isMC":false},{"id":"qXNIUlenKjE5UbCoibbb","answer_ET":"B","timestamp":"2021-06-09 22:36:00","question_id":175,"unix_timestamp":1623270960,"isMC":true,"answers_community":[],"discussion":[{"upvote_count":"1","timestamp":"1628969280.0","content":"But technically the proposed solution is correct. I wouldn't do it this way -I prefer the staging table- but from a technical perspective the answer could be YES. Any other thoughts on why it would be NO?","poster":"lgtiza","comment_id":"424928"},{"upvote_count":"1","timestamp":"1623632340.0","content":"I feel the answer should be NO. If the data is loaded and found to be corrupt, it's possible reporting has already been performed on the bad data before the data can be reset to the restore point and therefore reporting WAS impacted.","comment_id":"381434","poster":"BigMF"},{"comment_id":"378525","upvote_count":"2","timestamp":"1623270960.0","content":"why not just upload to a separate table and drop it if it is corrupted, move it over if it isn't","comments":[{"timestamp":"1623867420.0","content":"Yes Absolutely, populate to a staging table and run your data corruption checks there.","comment_id":"383621","poster":"azurrematt123","upvote_count":"1"}],"poster":"services1"}],"choices":{"A":"Yes","B":"No"},"answer_description":"Instead, create a user-defined restore point before data is uploaded. Delete the restore point after data corruption checks complete.\nReference:\nhttps://docs.microsoft.com/en-us/azure/sql-data-warehouse/backup-and-restore","exam_id":66,"question_text":"Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nA company is developing a solution to manage inventory data for a group of automotive repair shops. The solution will use Azure Synapse Analytics as the data store.\nShops will upload data every 10 days.\nData corruption checks must run each time data is uploaded. If corruption is detected, the corrupted data must be removed.\nYou need to ensure that upload processes and data corruption checks do not impact reporting and analytics processes that use the data warehouse.\nProposed solution: Insert data from shops and perform the data corruption check in a transaction. Rollback transfer if corruption is detected.\nDoes the solution meet the goal?","question_images":[],"answer":"B","topic":"4","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/55009-exam-dp-201-topic-4-question-15-discussion/"}],"exam":{"numberOfQuestions":206,"isMCOnly":false,"id":66,"isBeta":false,"isImplemented":true,"name":"DP-201","provider":"Microsoft","lastUpdated":"12 Apr 2025"},"currentPage":35},"__N_SSP":true}