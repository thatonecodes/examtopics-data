{"pageProps":{"questions":[{"id":"sXcYeFMN6IUss4JHKSHr","answer_images":[],"isMC":true,"question_images":[],"question_id":6,"choices":{"A":"Add a SQL statement.","C":"Set Data Connectivity mode to Import.","D":"Set Data Connectivity mode to DirectQuery.","B":"Set the Command timeout in minutes setting."},"url":"https://www.examtopics.com/discussions/microsoft/view/80547-exam-pl-300-topic-1-question-14-discussion/","exam_id":116,"timestamp":"2022-09-06 09:59:00","answer_ET":"D","answer":"D","unix_timestamp":1662451140,"answers_community":["D (100%)"],"answer_description":"","topic":"1","question_text":"You have an Azure SQL database that contains sales transactions. The database is updated frequently.\nYou need to generate reports from the data to detect fraudulent transactions. The data must be visible within five minutes of an update.\nHow should you configure the data connection?","discussion":[{"content":"Selected Answer: D\nD is correct for me","upvote_count":"30","comment_id":"724600","poster":"lukelin08","timestamp":"1669144740.0"},{"poster":"Igetmyrole","timestamp":"1693853520.0","comment_id":"998784","content":"D is the correct answer.\nDirectQuery model allows Power BI to directly query the data source (Azure SQL database, in this case) in real-time or near real-time. When data is updated in the database, DirectQuery ensures that the reports reflect the most current data without the need to import and refresh the data into the Power BI model.","upvote_count":"5"},{"timestamp":"1738873860.0","upvote_count":"1","content":"Selected Answer: D\nSince the data is near real time, DirectQuery is necessary","poster":"SliqueCia","comment_id":"1352654"},{"poster":"Odidepse","comment_id":"1337725","upvote_count":"1","content":"Selected Answer: D\nPresented answer is correct, within 5 minutes can only be achieved through direct query","timestamp":"1736288340.0"},{"poster":"DexTorao","content":"D for direct querry","timestamp":"1732006620.0","upvote_count":"2","comment_id":"1314522"},{"content":"Selected Answer: D\nFor near real time data, always use direct query \nWhen it is not feasible to cache data in power BI, due to size constraines , always user direct query","poster":"tejas9patil","comment_id":"1297386","upvote_count":"1","timestamp":"1728896760.0"},{"timestamp":"1727467440.0","content":"Selected Answer: D\ncorrect","upvote_count":"1","comment_id":"1290334","poster":"Bob_38"},{"timestamp":"1723639140.0","upvote_count":"3","content":"Selected Answer: D\nAlso for me is D. Set the connectivity mode to Direct Query. Because the data is refreshed each 5 minutes and the data in this way is not loaded into a cache but it is always refreshed","poster":"rcaliandro","comment_id":"1265747"},{"poster":"RedRoss","comment_id":"1170187","content":"Selected Answer: D\nIt'll not load the data in power query but go back to the original data source (SQL DB) which is updated frequently.","upvote_count":"2","timestamp":"1710063240.0"},{"timestamp":"1696595160.0","upvote_count":"2","comment_id":"1026606","content":"Selected Answer: D\nD is correct for me","poster":"sankeytm"},{"content":"Selected Answer: D\ndIRECT QUERY IS BEST","timestamp":"1690624140.0","poster":"Chenemi","comment_id":"966295","upvote_count":"1"},{"timestamp":"1688500620.0","poster":"ET_phone_home_son","comment_id":"943131","upvote_count":"2","content":"Selected Answer: D\nNear real-time data needed, so DirectQuery is needed."},{"timestamp":"1684813260.0","comment_id":"904525","upvote_count":"1","poster":"Jagu_sheth","content":"Selected Answer: D\nCorrect Ans"},{"upvote_count":"2","timestamp":"1682950140.0","comment_id":"886363","content":"Selected Answer: D\nDirect Query is the best choice!","poster":"Shalaleh"},{"timestamp":"1682799240.0","upvote_count":"1","content":"Selected Answer: D\nDirectQuery - best for real-time, or if you have large datasets to pull from\n\nhttps://learn.microsoft.com/en-us/training/modules/get-data/6-storage-mode","poster":"lizbette","comment_id":"884691"},{"upvote_count":"1","timestamp":"1679585400.0","comment_id":"848379","content":"Selected Answer: D\nD. Establezca el modo de conectividad de datos en DirectQuery.","poster":"DUVANES"},{"poster":"ClassMistress","comment_id":"768961","content":"D is the correct answer","timestamp":"1673130660.0","upvote_count":"2"},{"timestamp":"1671467280.0","comment_id":"750029","poster":"Nuli","upvote_count":"1","content":"D is correct because the database is updated frequently."},{"content":"Yup! D seems most appropriate.","comment_id":"719307","poster":"scotchtapebunny","upvote_count":"4","timestamp":"1668570960.0"},{"content":"D. Set Data Connectivity mode to DirectQuery because the data is accessed frequently.","comment_id":"709437","upvote_count":"3","timestamp":"1667336880.0","poster":"ClassMistress"},{"comment_id":"700981","timestamp":"1666361280.0","upvote_count":"3","poster":"CHT1988","content":"Selected Answer: D\nD. Set Data Connectivity mode to DirectQuery."},{"content":"DirectQuery","upvote_count":"2","timestamp":"1665126240.0","comment_id":"688405","poster":"samad1234"},{"content":"Selected Answer: D\nDirect query","comment_id":"683576","upvote_count":"3","timestamp":"1664543640.0","poster":"adizzz54"},{"timestamp":"1662890220.0","comment_id":"665999","upvote_count":"3","content":"Selected Answer: D\nD. Set Data Connectivity mode to DirectQuery.","poster":"OGESSIUSER"},{"poster":"MilouSluijter","comment_id":"660971","upvote_count":"3","timestamp":"1662451140.0","content":"D"}]},{"id":"RmV1EWXMPTxyQs2vHpV6","question_id":7,"timestamp":"2022-09-06 11:53:00","discussion":[{"poster":"emmanuelkech","comments":[{"content":"Am I the only one who thinks that \"The solution should NOT store data from CSV files\" is trick part in the question?","poster":"33148b2","upvote_count":"1","timestamp":"1738529460.0","comment_id":"1350649"},{"poster":"Pitxunet","upvote_count":"1","timestamp":"1738325280.0","comment_id":"1349505","content":"But after these three steps, the metadata is not combined in a single dataset as requested, is it?"},{"comments":[{"upvote_count":"2","comment_id":"1330918","poster":"BIFakeGuru","content":"pnb11 is wrong, Emmanuelkech is right\nWe want the metadata which is the \"attributes column\", not the data aka \"content\"","timestamp":"1734981120.0"},{"content":"emmanuelkech has the right answer here, content column is not required, so it should be removed. Attritbutes column is to be expanded.","poster":"moleskin","upvote_count":"3","comment_id":"1295284","timestamp":"1728505440.0"},{"comment_id":"730296","poster":"Hoeishetmogelijk","content":"See the requirement \"The solution must NOT store the data of the CSV files.\"\nSo the content column must be removed.","upvote_count":"25","timestamp":"1669720680.0"},{"poster":"Shakilpatil","upvote_count":"10","content":"The question is not to store data of files","timestamp":"1671640320.0","comment_id":"752567"},{"timestamp":"1663691460.0","content":"Hello dear, Metadata means information about files. It's why we remove content.","comment_id":"674346","comments":[{"timestamp":"1674631320.0","poster":"jaydenlkl","upvote_count":"5","content":"agreed","comment_id":"787349"}],"poster":"Tata11","upvote_count":"32"}],"comment_id":"669331","upvote_count":"39","timestamp":"1663189560.0","content":"These are right answer\n1.Get data the select folder\n2.Remove attribute column (because this column contain information about file which not needed).\n3.Combine Content column (which contain actual data which needed for us)","poster":"pnb11"},{"timestamp":"1672736100.0","upvote_count":"25","comment_id":"764342","poster":"GabryPL","comments":[{"content":"your answer is wrong. It should be in this order \nGet Data from the folder:\nThis is the first step where you connect to the folder that contains your CSV files. Power BI will recognize all the CSV files in the folder and generate a dataset with both file metadata and content.\n\nRemove Content Column:\nAfter you load the data from the folder, Power BI typically presents two columns: Content (which contains the actual data of the CSV files) and Attributes (which contains the metadata). In this step, you would remove the Content column to ensure you're only working with the metadata (e.g., file names, paths, creation dates).\n\nExpand Attributes:\nThe Attributes column contains a record with metadata for each file. To make the metadata more accessible and usable in your report, you'll need to expand this column. Expanding will break the record down into individual metadata fields (e.g., file name, file path, etc.), so that you can work with these attributes as separate columns in Power Query.","poster":"5767542","comment_id":"1333306","upvote_count":"3","timestamp":"1735444680.0"},{"poster":"HN_3532","comments":[{"comments":[{"timestamp":"1695386640.0","upvote_count":"5","content":"Horizontal filtering = removing rows we don't want.\nVertical filtering = removing columns we don't want.","comment_id":"1014099","poster":"itenginerd"}],"timestamp":"1675844280.0","upvote_count":"1","comment_id":"801765","content":"What do u mean with vertical filtering?","poster":"Nemesizz"}],"upvote_count":"19","timestamp":"1674957420.0","comment_id":"791310","content":"It's not wrong, but the rule of thumb is \"Filter left. Format right.\". Removing columns is vertical filtering, so it should be on top."}],"content":"what about:\n1) get data from folder\n2) expand attribute\n3) remove content column\n\nwhy should this order be wrong?"},{"content":"I have tested and works.\nSome ppl said to \"combine attribute\", but its not possible.The options is not avaliable.","timestamp":"1682165460.0","upvote_count":"4","comment_id":"877251","poster":"fellipeao"},{"comment_id":"692213","comments":[{"poster":"cnmc","comment_id":"759142","upvote_count":"12","timestamp":"1672182240.0","content":"audit purpose. Not everything is about the business results, for big corps you'd care about how it's run too"}],"poster":"NevilleV","upvote_count":"7","timestamp":"1665501060.0","content":"I agree that this is the requirement. The thing that bothers me is WHY? Why would you want to create a dataset with only the metadata?"}],"timestamp":"1662457980.0","content":"I think the correct flow is\nGet data then select folder\nRemove content Colum\nExpand Attribute Colum","upvote_count":"241","comment_id":"661057"},{"comments":[{"upvote_count":"7","poster":"GPerez73","comment_id":"663732","content":"I agree","timestamp":"1662650160.0"},{"poster":"Churato","content":"Tested here and it works. Thankyou!","upvote_count":"3","timestamp":"1666782600.0","comment_id":"704606"}],"poster":"Guru1337","comment_id":"661172","timestamp":"1662464400.0","upvote_count":"52","content":"It should be remove Content not combine, since the file data is NOT to be stored."},{"upvote_count":"1","poster":"BhavnishM","timestamp":"1743582540.0","comment_id":"1421854","content":"I just tested this on my end.\nThe correct order is :\nget the data from the folder\nin Power query editor expand the attributes column (this fulfils the objective of storing metadata)\nremove the content columns (this would not store the actual data)"},{"poster":"Fatima300","content":"1- Get Data and Select Folder → Loads all file metadata, including Content and Attributes.\n2- Expand the Attributes Column → Extracts details like file size, date modified, and type.\n3- Remove the Content Column → Deletes the actual CSV data, keeping only metadata. Why This Order?\nIf you remove Content before expanding Attributes, you might lose important metadata.","comment_id":"1349102","timestamp":"1738246620.0","upvote_count":"1"},{"poster":"jaume","comment_id":"1312602","upvote_count":"1","content":"Everybody agree on the first step to \"Get data and select folder\".\nSince \"not storing data of the csv filed\" is a requisite, second step should be \"...remove the content column\".\nFinally, as another requisite is to \"...make the file metadata available\" we shoud expand the \"Attributes\" column where these details are hosted","timestamp":"1731671940.0"},{"content":"I don't see why we combine the content column if it should not be stored. It should be removed surely","poster":"JPShields","upvote_count":"1","comment_id":"1296398","timestamp":"1728717840.0"},{"timestamp":"1727932200.0","upvote_count":"1","poster":"hassan899","comment_id":"1292650","content":"how are is this type of question graded? does the order have to be correct to get the whole points or is it graded per correct selection?"},{"comment_id":"1289452","content":"Get data then select folder\nRemove content Colum\nExpand Attribute Colum","poster":"SidneyHod","upvote_count":"1","timestamp":"1727355300.0"},{"upvote_count":"1","comment_id":"1267198","poster":"rcaliandro","content":"Not sure about that but in my opinion it is:\nA - From Power BI Desktop, select Get Data and then select Folder\nB - From Power Query Editor, expand the Attributes column\nC - From Power Query Editor, remove the Content column","timestamp":"1723817220.0"},{"comment_id":"1259752","timestamp":"1722582960.0","content":"answer:\nFrom Power BI Desktop, select Get Data, and then select Folder.\nFrom Power Query Editor, remove the Content column.\nFrom Power Query Editor, expand the Attributes column.","poster":"janssen135","upvote_count":"3"},{"timestamp":"1722277680.0","poster":"AP0409","comment_id":"1257643","content":"The question mentioned do NOT store data of csv files. This means we have to remove Attributes. So the correct order is:\n1. Get data, select folder\n2. Remove Attribute\n3. Combine Content","upvote_count":"1"},{"timestamp":"1708572240.0","upvote_count":"1","poster":"soheil.dabooyeh29","comment_id":"1156077","content":"the solution would be:\n1- Get Data from Folder\n2- Expand (Priority) the Attributes Column\n3- Combine the Content Column"},{"content":"I have the feeling this question is a voluntary trap of Microsoft :-) \nWhen you test the conflictual solutions in PBI Desktop you will see that combining content will add a bunch of crap in your semantic model..including the data that we are forbidden to store, it's common sense. It is written in the question itself, metadata only and as metadata is also included in Attributes, you need to expand attributes to get the entire metadata context.","poster":"Dani_eL","timestamp":"1708352520.0","comment_id":"1153996","upvote_count":"1"},{"poster":"KKPanda77","timestamp":"1708158000.0","upvote_count":"1","comment_id":"1152442","content":"Tricky and Interesting"},{"content":"Its explained here which makes sense\nGet data then select folder\nRemove content Colum\nExpand Attribute Colum","comment_id":"1139233","upvote_count":"20","timestamp":"1706964300.0","comments":[{"timestamp":"1711293420.0","poster":"JudT","upvote_count":"2","comments":[{"content":"Totally agree","comment_id":"1226542","poster":"Inesd","upvote_count":"1","timestamp":"1717823400.0"}],"comment_id":"1181763","content":"Thank you!"}],"poster":"nattomi"},{"upvote_count":"3","content":"I go with below instead of Combine the Content column for 3)\n\n1) get data from folder\n2) expand attribute\n3) remove content column\n\nCombine Content column will have all data of the csv files","poster":"JohnChung","comment_id":"1123793","timestamp":"1705366380.0"},{"poster":"Ditendra","comment_id":"1076015","timestamp":"1700539740.0","content":"the correct sequence of actions is: From Power BI Desktop, select Get Data, and then select Folder -> From Power Query Editor, remove the Content column -> From Power Query Editor, remove the Attributes column\n\nsince attributes column contains extra info about files we should remove that and only keep clean meta data.","upvote_count":"2"},{"timestamp":"1695982260.0","content":"Only meta data must be shown and NO data must be stored.\nTry this out practical, following steps work.\n1) \nGet Data from folder\n2) Expand Attribute columns\n3) Remove Content column","poster":"TigerTienie","comment_id":"1020672","upvote_count":"4"},{"poster":"Fisher","upvote_count":"3","timestamp":"1695945540.0","content":"expand the attributes.\n\nExpanding the attributes means that you create columns in your dataset to capture the metadata about each file, such as file names, file paths, file sizes, and other relevant attributes. This approach keeps the metadata in your dataset without storing the actual data from the CSV files.\n\nBy expanding the attributes, you ensure that you have the necessary information to work with the file metadata in your Power BI reports and visualizations while adhering to the requirement of not storing the CSV file data within your Power BI dataset.","comment_id":"1020298"},{"comment_id":"998797","content":"First step: From Power BI desktop, select get data, and then select folder. This is the first step to access the files within the folder.\nSecond Step: From Power Query Editor, expand the attributes column. This step is necessary to access the file metadata.\nThird Step: From Power BI Query Editor, remove the content column. This step is crucial to exclude the actual file data and keep only the metadata.\nAre the correct steps. Others will not work because of the following reasons :\nRemoving the \"attributes\" column would prevent us from accessing the file metadata.\n\nSelecting \"text/csv\" from the \"Get Data\" menu would be necessary if we want to import the data from the CSV files themeselves, but our requirement is to extract metadata only, so this step in not needed.\n\nCombining the \"Content\" column is not necessary because we want to remove it to avoid storing the data, not combine it.","upvote_count":"8","timestamp":"1693854540.0","poster":"Igetmyrole"},{"upvote_count":"2","content":"Dear friends. If you don't combine content, you won't get just one dataset. Each SCV file would become a separate table when uploaded into report. If you just try that, you'll see that you're wrong and official answer is correct.","comment_id":"963524","timestamp":"1690360920.0","poster":"Mabuse1"},{"comments":[{"content":"Attributes = metadata, remove content","comment_id":"961997","upvote_count":"2","comments":[{"content":"u said Attributes = metadata, the requirement is not include the data of files, so it should be remove attr right?","comment_id":"1016470","upvote_count":"1","poster":"minhnhat123","timestamp":"1695619440.0"}],"timestamp":"1690228200.0","poster":"Maniula"}],"comment_id":"956305","upvote_count":"1","poster":"Bnxyl","content":"1. Get data and then select a folder\n2. Delete Attributes folder Since we want to ensure that we don't store unnecessary data\n3. Combine both tables using Content column intoa single dataset","timestamp":"1689747300.0"},{"upvote_count":"3","comment_id":"954615","comments":[{"comment_id":"1013165","content":"The attributes ARE the metadata you're looking for. Expanding that column breaks the metadata out into separate fields so you can make use of these data points. So you remove the file contents and expand the attributes into metadata columns.\n\nThe goal here is to create a report *about* the files, not about what's in them. Like a count of Excel files vs. PDFs, or documents created by month.","upvote_count":"2","timestamp":"1695306180.0","poster":"itenginerd"}],"timestamp":"1689627660.0","content":"This wasn't explained very well in the course I'm taking? What's the logic behind this answer?\nFrom what I understand, expanding the attribute column will display the data, which I would think goes against the instruction not to store any data. Wouldn't combining them be the same thing as appending them?","poster":"J_Dawg_PBI"},{"upvote_count":"3","timestamp":"1688501100.0","content":"Get Data (Folder) > Remove content > Expand attributes","comment_id":"943138","poster":"ET_phone_home_son"},{"timestamp":"1687323840.0","content":"Correct Answer:\n1) get data from folder\n2) remove content column (because we don't need to combine files, we just want meta data)\n3) expand attribute","comment_id":"929073","poster":"visionary4ever","upvote_count":"3"},{"comment_id":"927468","content":"The answer is correct, I just tried and it works.\nGet Data From Folder\nExpand Attribute Column\nCombine Content Column","upvote_count":"3","comments":[{"upvote_count":"1","timestamp":"1687352760.0","poster":"elesglar","comment_id":"929535","content":"Yes, but YOU DO NOT want to store data from the files. It's for metadata purposes only (Folder names)"}],"timestamp":"1687177860.0","poster":"Alvin_2113"},{"timestamp":"1686389400.0","content":"To make the file metadata available as a single dataset in Power BI without storing the data of the CSV files, you should perform the following actions in sequence:\n\nGet Data: Select Get Data and then select Text/CSV. This action allows you to connect to the folder that contains the CSV files in Power BI.\n\nCombine the Content Column: After connecting to the folder, you should combine the content column of the CSV files. This step merges the data from all the files into a single column.\n\nExpand the Attributes Column: Once the content column is combined, you can expand the attributes column to extract the metadata of the files. This action separates the metadata into individual columns.\n\nTherefore, the correct sequence of actions is as follows:\n\nGet Data\nCombine the Content Column\nExpand the Attributes Column\nActions to remove the contents and attributes columns are not necessary since the goal is to extract and retain the file metadata without storing the data of the CSV files.","comment_id":"919934","comments":[{"poster":"oreshetnik","timestamp":"1698412020.0","comment_id":"1055611","content":"After Combine, the Attribute Column is no longer there.","upvote_count":"1"},{"timestamp":"1688531760.0","upvote_count":"1","comment_id":"943296","content":"i would remove content column as it will reduce the dataset size and simplify the dataset structure,","poster":"safz"}],"poster":"sergeyitaly","upvote_count":"1"},{"comment_id":"915225","poster":"vat4444","content":"Get data then select folder\nRemove content Colum\nExpand Attribute Colum","upvote_count":"1","timestamp":"1685954100.0"},{"content":"In order to not break the query folding:\n1) get data from folder\n2) expand attribute\n3) remove content column","comment_id":"896534","timestamp":"1683969300.0","upvote_count":"1","poster":"rmeng"},{"timestamp":"1681966920.0","poster":"UlyUkr","comment_id":"875284","upvote_count":"2","comments":[{"upvote_count":"2","comment_id":"891178","timestamp":"1683438720.0","content":"I take my words back. Did not read carefully. We need metadata here and not the actual data.\nSo the correct sequence is:\n1.Get data from the folder\n2. Remove Content.\n3. Expand Attribute.","poster":"UlyUkr"}],"content":"Just tested in real life. The correct sequence is:\n1. Get data from the folder\n2. Remove attributes (it contains information about files which is not required).\n3. Combine Content."},{"poster":"SanaCanada","comment_id":"860921","timestamp":"1680606420.0","upvote_count":"5","content":"Correct answer\nGet DAta then select Folder\nRemove Content Column\nExpand Attribute Column\nIn the first step of the data modeling process in Power BI, it is generally recommended to analyze the data and remove any unnecessary content columns first before expanding attribute columns.\n\nContent columns are those that do not provide any meaningful information for analysis, such as ID columns, timestamp columns, or other metadata columns. These columns are typically used for identification or administrative purposes and are not relevant for analysis. Removing them can simplify the data model, reduce query time, and improve performance.\n\n\nTherefore, the general approach is to remove unnecessary content columns first before expanding attribute columns in Power BI. This will help simplify the data model and improve performance, while still providing the necessary information for analysis.\n\n\n\n\nNo confusion, and no need to discuss further"},{"upvote_count":"2","poster":"ilk777","comment_id":"859898","content":"Get Data > Remove Content > Expand Attribute\n\nRemove is before expand because:\n1) Always make it smaller before any operations\n2) Avoid the chance of having an attribute named \"Content\" conflict with the Content column.","timestamp":"1680525000.0"},{"content":"1. En Power BI Desktop, elija Obtener datos y, a continuación, seleccione Carpeta.\n2. En el Editor de Power Query, remueva la columna Contenido.\n3. Desde Power Query Editor, expanda la columna Atributos.","comment_id":"848400","poster":"DUVANES","upvote_count":"1","timestamp":"1679586600.0"},{"comment_id":"838626","upvote_count":"2","content":"1) Get data from folder\n2) Expand Attribute \n3) Remove content","poster":"Akin_Eren","timestamp":"1678780260.0"},{"timestamp":"1677650100.0","upvote_count":"1","comment_id":"825554","poster":"srikanth923","content":"Here are three steps to get the metadata of files from a chosen folder:\n\nSelect the folder and get its data.\nExpand the attributes column.\nRemove the content column since we only need the metadata of the files and not their actual data."},{"comment_id":"771246","timestamp":"1673342880.0","poster":"RooneySmith","upvote_count":"8","content":"Seeing how this answer doesn't make any sense at all, and as many of other questions' answers, I wonder: Is this correction trusted?? And I if tomorrow I want to pass the exam no matter what, should I answer the way it's answered here or should I follow what I believe is correct??"},{"comment_id":"750199","comments":[{"content":"Looks like that correct answer builds after voites.","upvote_count":"1","poster":"Taras_Navakhatska","timestamp":"1676367420.0","comment_id":"808251"}],"content":"Why is a wrong answer in some questions ?","poster":"vero1971_","upvote_count":"2","timestamp":"1671479100.0"},{"timestamp":"1670561760.0","poster":"Patrick666","upvote_count":"4","comment_id":"739803","content":"Get data then select folder\nRemove content Colum\nExpand Attribute Colum"},{"comment_id":"687238","comments":[{"comment_id":"694647","upvote_count":"4","poster":"NevilleV","timestamp":"1665741120.0","content":"Agreed. The order of the last 2 don't matter"},{"poster":"cldrmn","timestamp":"1669626420.0","upvote_count":"2","comment_id":"728913","content":"Agreed."},{"content":"I agree, but i would first expand before remove something.","comment_id":"814337","timestamp":"1676829300.0","upvote_count":"1","poster":"herr_serfin"}],"poster":"lukelin08","timestamp":"1665005100.0","content":"I agree that it should be remove content. However it is another ambiguous possible answer from Microsoft, because after getting the data as the first step, the last two steps (Remove content column, & Expand attribute column) can be done in any order. The order doesn't matter for the last two steps, it would work either way. So again its annoying if Microsoft dont allow for both answers to be correct due to the order.","upvote_count":"15"},{"poster":"Nurgul","content":"Actions:\nFrom Power BI Desktop, select Get Data, and then select Folder.\nFrom Power Query Editor, remove the Content column.\nFrom Power Query Editor, expand the Attributes column.","upvote_count":"8","comment_id":"685869","timestamp":"1664849100.0"},{"timestamp":"1663939080.0","poster":"RichardOgoma","comment_id":"677171","content":"1. Get data and select folder\n2. Remove the content column\n3. Expand the attributes column\nYou'll have only metadata of the files remaining.","upvote_count":"9"},{"timestamp":"1663691700.0","upvote_count":"9","content":"\"You need to make the file metadata (metadata= information about files) available\" so, get data, remove content, expand attribute.","comment_id":"674348","poster":"Tata11"}],"answer_description":"Step 1: From Power BI Desktop, Select Get Data, and then Select Folder.\nOpen Power BI Desktop and then select Get Data\\Moreג€¦ and choose Folder from the All options on the left.\n\nEnter the folder path, select OK, and then select Transform data to see the folder's files in Power Query Editor.\nStep 2: From Power Query Editor, expand the Attributes column.\nStep 3: From Power Query Editor, combine the Content column.\n\n\nCombine files behavior -\nTo combine binary files in Power Query Editor, select Content (the first column label) and select Home > Combine Files. Or you can just select the Combine Files icon next to Content.\nReference:\nhttps://docs.microsoft.com/en-us/power-bi/transform-model/desktop-combine-binaries","question_images":["https://www.examtopics.com/assets/media/exam-media/04331/0003100001.jpg"],"exam_id":116,"answer":"","isMC":false,"answer_images":["https://www.examtopics.com/assets/media/exam-media/04331/0003100002.jpg","https://www.examtopics.com/assets/media/exam-media/04331/0003300001.jpg","https://www.examtopics.com/assets/media/exam-media/04331/0003400001.png"],"topic":"1","url":"https://www.examtopics.com/discussions/microsoft/view/80566-exam-pl-300-topic-1-question-15-discussion/","unix_timestamp":1662457980,"answers_community":[],"answer_ET":"","question_text":"DRAG DROP -\nYou have a folder that contains 100 CSV files.\nYou need to make the file metadata available as a single dataset by using Power BI. The solution must NOT store the data of the CSV files.\nWhich three actions should you perform in sequence. To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:\n//IMG//"},{"id":"eF5Ge36tvznoNqNgBU3j","question_id":8,"answer":"C","isMC":true,"topic":"1","answer_description":"","question_text":"A business intelligence (BI) developer creates a dataflow in Power BI that uses DirectQuery to access tables from an on-premises Microsoft SQL server. The\nEnhanced Dataflows Compute Engine is turned on for the dataflow.\nYou need to use the dataflow in a report. The solution must meet the following requirements:\n✑ Minimize online processing operations.\n✑ Minimize calculation times and render times for visuals.\n✑ Include data from the current year, up to and including the previous day.\nWhat should you do?","exam_id":116,"url":"https://www.examtopics.com/discussions/microsoft/view/82494-exam-pl-300-topic-1-question-16-discussion/","answer_ET":"C","answer_images":[],"question_images":[],"choices":{"B":"Create a dataflows connection that has DirectQuery mode selected and configure a gateway connection for the dataset.","A":"Create a dataflows connection that has DirectQuery mode selected.","D":"Create a dataflows connection that has Import mode selected and create a Microsoft Power Automate solution to refresh the data hourly.","C":"Create a dataflows connection that has Import mode selected and schedule a daily refresh."},"discussion":[{"poster":"SanaCanada","upvote_count":"50","comments":[{"comment_id":"1421895","content":"gateways are needed for on premises DB.","timestamp":"1743583200.0","poster":"BhavnishM","upvote_count":"1"},{"poster":"KAYBOL","upvote_count":"10","comment_id":"1045226","content":"I would think Gateways are need for ON premises DB.","timestamp":"1697484840.0"}],"timestamp":"1680664680.0","comment_id":"861724","content":"Selected Answer: C\nCorrect Answer C\n\nBased on the requirements mentioned, the best option would be to choose option C: Create a dataflows connection that has Import mode selected and schedule a daily refresh.\n\nOption A is not the best choice as it requires online processing operations, which goes against one of the requirements.\n\nOption B is not necessary since the SQL Server is on-premises and not in a cloud environment. Gateway connections are typically used for cloud-based data sources that require access to on-premises data.\n\nOption D refreshes the data too frequently and might lead to unnecessary processing operations, which goes against one of the requirements.\n\nTherefore, the best approach is to use Import mode with daily scheduled refreshes to include data from the current year, up to and including the previous day. This would minimize online processing operations and also reduce calculation times and render times for visuals.\n\nNo confusion, and no need to discuss further"},{"content":"C, because one of the requirements is 'Minimize online processing operations'. Although the dataflow uses DirectQuery, the Dataset can be refreshed with Import.https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-directquery","comment_id":"681771","upvote_count":"44","comments":[{"content":"Need a gateway","comment_id":"687935","upvote_count":"3","comments":[{"upvote_count":"18","comment_id":"691035","poster":"Dovoto","comments":[{"comments":[{"content":"No, it's already created by the developer. Your task is \"to use the dataflow in a report\"","comment_id":"960488","upvote_count":"3","timestamp":"1690121280.0","poster":"Maniula"}],"timestamp":"1682951820.0","comment_id":"886393","upvote_count":"1","content":"in all options, it says :\" create a dataflow....\" it means we already do not have the dataflow","poster":"Shalaleh"}],"timestamp":"1665399720.0","content":"The BI developer has already created the dataflow, so the gateway must be present. Import and daily scheduled refresh should do the trick."}],"timestamp":"1665070920.0","poster":"Sunny_Liya"},{"timestamp":"1694417040.0","content":"direct query is to on prem database, does it still use online processing operation?","comment_id":"1004516","upvote_count":"2","poster":"spamhz"},{"comments":[{"timestamp":"1685946360.0","poster":"NLeeXTung","upvote_count":"6","comment_id":"915115","content":"Image the Dataflow like the Common Data Model which has been ETL from the external data sources and PBI Desktop will connect to Dataflow by Import mode to create its dataset"}],"upvote_count":"6","poster":"thanhtran7","content":"\"Although the dataflow uses DirectQuery, the Dataset can be refreshed with Import.\" -> I dont understand this point. Can you help explain more details?","timestamp":"1670337300.0","comment_id":"736898"}],"timestamp":"1664373540.0","poster":"IxIsa"},{"poster":"jaume","upvote_count":"1","timestamp":"1731678840.0","comment_id":"1312650","content":"The \"Enhanced Dataflows Compute Engine\" (ECE) is a feature designed to improve the performance of data transformations in PBI dataflows by using optimized processing and caching...but I think it cannot be used with DirectQuery option enabled but Import mode!! \nAs one of the requisites is about ensuring data is updated up to the previous day, I think option C should be the right one as it's scheduling daily refresh.\nOnce again I feel like these questions are not testing our knowledge abou PBI and/or data analysis but main important thing is to understand all the statements in the right way"},{"poster":"nelrosell","upvote_count":"4","comment_id":"1282905","content":"must be A: Since the Enhanced Dataflows Compute Engine is turned on, you can use DirectQuery to access the dataflow. This allows you to query the data in real-time without importing it into Power BI, minimizing online processing operations","timestamp":"1726187700.0"},{"comment_id":"1265776","poster":"rcaliandro","upvote_count":"1","content":"Selected Answer: C\n\"Include data from the current year, up to and including the previous day.\" and \"Minimize online processing operations\"\nC. Create a dataflows connection that has Import mode selected and schedule a daily refresh","timestamp":"1723641000.0"},{"upvote_count":"1","timestamp":"1721028120.0","poster":"Daniel16","comment_id":"1248164","content":"I don't understand that we already have a dataflow using DirectQuery to access tables, why do we need to create a dataflow connection with Import mode?"},{"content":"Answer: A","timestamp":"1714989000.0","comment_id":"1207268","upvote_count":"1","poster":"shazzzy"},{"upvote_count":"1","timestamp":"1714893060.0","comment_id":"1206796","content":"What is the correct one finally","poster":"RoxyRishi"},{"upvote_count":"4","comment_id":"1201860","poster":"ajelizi","timestamp":"1714035000.0","content":"Selected Answer: B\nThe right answer is\nB. Create a dataflows connection that has DirectQuery mode selected and configure a gateway connection for the dataset.\nThis option allows you to use DirectQuery mode, minimizing online processing operations, while also ensuring that you can access on-premises data from the Microsoft SQL server using a gateway connection.","comments":[{"upvote_count":"1","poster":"Inesd","content":"I also think that B is the correct answer. For those who have voted for C is possible to use import mode from Dataflow without gateway ?","comment_id":"1210839","timestamp":"1715601000.0"}]},{"comment_id":"1167922","upvote_count":"1","poster":"LOCOBI","content":"Option C.\n\nAltought the dataflow (witch is a collection of tables) gets its data from Direct Query. The Dataset (Model of those tables) can use import, and because of that, save recourses.\n\nSo that discarts options A and B.\n\nAlso for option B a Gateway connection is not needed since the SQL sever is ON PREMISE and a gateway is used for cloud envarioment that require access for on premise data.\n\nThen you can also think that since we are trying to minimize resource, the hourly refresh is more expensive that option C\n\nSo option C is correct","timestamp":"1709809800.0"},{"poster":"AZFabio","content":"Selected Answer: C\ncorrect","timestamp":"1709762580.0","comment_id":"1167524","upvote_count":"1"},{"timestamp":"1704232560.0","upvote_count":"1","poster":"momo1165","comment_id":"1112292","content":"C: Import will:\n1. Minimize online processing operations.\n2. Minimize calculation times and render times for visuals.\nthe daily scheduled refresh will keep data updated and thus Include data from the current year, up to and including the previous day."},{"upvote_count":"7","poster":"T1M2P","comment_id":"1067632","content":"This question was in Exam today.","timestamp":"1699669680.0"},{"poster":"mordaro","comment_id":"1064773","upvote_count":"1","content":"Selected Answer: C\nC because it meets all requirements","timestamp":"1699359720.0"},{"poster":"Igetmyrole","upvote_count":"7","timestamp":"1693855320.0","comment_id":"998804","content":"The correct answer is C. \nIt is because:\n\nImport mode allows us to load and store the data from the DirectQuery source in the Power BI service. This minimizes online processing operations, as calculations are performed during data refresh rather than in real-time during report rendering.\n\nScheduling a daily refresh ensures that our dataflow data is up to date while minimizing the frequency of refresh operations. Since we only need data up to and including the previous day, a daily refresh is sufficient."},{"upvote_count":"1","comment_id":"966303","timestamp":"1690625280.0","poster":"Chenemi","content":"Selected Answer: C\nSINCE WE ARE MINIMIZING ONLINE PROCESSING"},{"poster":"pverde","comment_id":"947502","upvote_count":"1","content":"Selected Answer: C\nC is correct","timestamp":"1688929740.0"},{"timestamp":"1684232340.0","upvote_count":"4","poster":"Mphatso","content":"The best option to meet the given requirements would be:\n\nB. Create a dataflows connection that has DirectQuery mode selected and configure a gateway connection for the dataset.\n\nExplanation:\n\nThe requirement to minimize online processing operations suggests that DirectQuery mode should be used. DirectQuery allows Power BI to directly query the on-premises SQL server without importing the data into Power BI.\nThe requirement to minimize calculation times and render times for visuals is also achieved through DirectQuery mode. With DirectQuery, calculations are performed on the SQL server, reducing the computational load on Power BI.\nThe requirement to include data from the current year, up to and including the previous day can be handled by configuring a gateway connection. The gateway allows Power BI to establish a secure connection to the on-premises SQL server and retrieve the required data.\nOption B fulfills all the requirements by using DirectQuery mode and configuring a gateway connection for the dataset.","comment_id":"899052"},{"timestamp":"1684154040.0","content":"Selected Answer: C\nC option as it decreases the processing time.","upvote_count":"1","comment_id":"898275","poster":"AnshulK"},{"poster":"Ihtra","comment_id":"888260","timestamp":"1683099120.0","content":"Need a gateway","upvote_count":"1"},{"content":"Selected Answer: C\nguys! first, the dataflow have been created in power bi service. that means a gateway has already installed so we do not need a gateway. just we need to create dataflow Connection. and this connection should minimize online processing, that means the dataflow should use import mode. and we need a daily refresh. therefore option C is the correct option.!","timestamp":"1682953260.0","upvote_count":"5","comment_id":"886411","poster":"Shalaleh"},{"content":"Selected Answer: B\n\"to access tables from an on-premises Microsoft SQL server.\" do not we need a gateway?","timestamp":"1682951640.0","comment_id":"886392","upvote_count":"1","poster":"Shalaleh"},{"upvote_count":"2","poster":"vishals729","content":"the answer will be option B as you will need a gateway connection for connecting securely to a sql databse","timestamp":"1682045100.0","comment_id":"876148"},{"comment_id":"848466","timestamp":"1679590680.0","upvote_count":"3","poster":"glenman0202","content":"Selected Answer: C\nC is the correct answer. DirectQuery is slower than import in regards to calculation times and render times for visuals, so both A and B are automatically disqualified. Additionally, there is no reason to refresh data hourly, as a daily refresh (C) is adequate for the requirements."},{"comment_id":"848405","timestamp":"1679586900.0","upvote_count":"1","content":"Selected Answer: C\nC. Cree una conexión de flujos de datos que tenga seleccionado el modo de importación y programe una actualización diaria.","poster":"DUVANES"},{"content":"I think A is also correct since you have to avoid separate refresh schedule","upvote_count":"2","poster":"hungry85","timestamp":"1679370840.0","comment_id":"845501"},{"content":"Selected Answer: B\nB seems to correct answer to me. Direct Query is the way to \"Serving data to customers in a managed and performance-minded way\"","timestamp":"1678784820.0","comment_id":"838677","upvote_count":"2","poster":"Akin_Eren"},{"upvote_count":"1","timestamp":"1676119020.0","poster":"skaha","content":"Did dataflow project now. C is the correct answer. when connect dataflow it shows pop up window with flows-tables and at the bottom l>>oad-transform-cancel(means import mode is predefined by PBI desktop tool. in Gateway refresh schedule daily(suitable for the question) or weekly options.","comment_id":"805197"},{"comment_id":"798789","content":"I eliminated A and B coz it says 'faster visual refresh' so only import would make sense, so out of the remaining C makes more sense.","upvote_count":"2","timestamp":"1675597620.0","poster":"BWayne32"},{"poster":"yordiye","upvote_count":"1","comment_id":"783865","content":"A Avoid separate refresh schedules: DirectQuery connects directly to a dataflow, which removes the need to create an imported dataset. As such, by using DirectQuery with your dataflows means you no longer need separate refresh schedules for the dataflow and the dataset to ensure your data is synchronized. https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-develop-solutions","timestamp":"1674352020.0"},{"upvote_count":"1","comment_id":"777900","content":"Hello, I would think B because with in premise the Gateway is required.","poster":"MikeDoesBI","timestamp":"1673884440.0"},{"poster":"oakey66","content":"This doesn't seem correct. Based on this link, you should use directquery: \n\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-develop-solutions\n\nAvoid separate refresh schedules: DirectQuery connects directly to a dataflow, which removes the need to create an imported dataset. As such, using DirectQuery with your dataflows means you no longer need separate refresh schedules for the dataflow and the dataset to ensure your data is synchronized.\n\nThis explicitly calls out that you should not need refresh schedules. Am I missing something?","timestamp":"1672935900.0","upvote_count":"2","comment_id":"766826"},{"comment_id":"724606","timestamp":"1669145160.0","content":"Selected Answer: C\nC is correct","upvote_count":"3","poster":"lukelin08"},{"comment_id":"723845","content":"Why its cant be A, they have compute setting turned ON, we can directly use Direct Query from dataflow","poster":"PCCCCCC","upvote_count":"3","timestamp":"1669058220.0","comments":[{"comment_id":"776191","timestamp":"1673762100.0","upvote_count":"1","content":"The only explanation comes to my mind is that DQ has less computation capability than import and computation is one of our constraints.","poster":"Ashishsingh07"}]},{"poster":"Churato","upvote_count":"3","content":"Selected Answer: C\nDovoto, yes the BI devoloper already created the Dataflow","timestamp":"1666297440.0","comment_id":"700277"},{"comment_id":"680312","content":"Is it b or c?","timestamp":"1664243880.0","poster":"Manzy2599","upvote_count":"1"},{"timestamp":"1663851840.0","comment_id":"676148","content":"B. because it uses direct query to access the tables with the connection of on-premises SQL server which would require a gateway for the connection.","upvote_count":"3","poster":"Snow_28"},{"comments":[{"poster":"GPerez73","upvote_count":"2","timestamp":"1664464500.0","content":"I also think so","comment_id":"682800"}],"comment_id":"675714","upvote_count":"4","poster":"saurinkhamar","content":"B. Could be an answer. OnPremise SQL server to be connected which would require Gateway","timestamp":"1663817880.0"},{"timestamp":"1663404780.0","poster":"fdsdfgxcvbdsfhshfg","comment_id":"671365","content":"Selected Answer: C\nC is legit","upvote_count":"6"}],"answers_community":["C (92%)","8%"],"timestamp":"2022-09-17 10:53:00","unix_timestamp":1663404780},{"id":"Jpale0QRLfrrQ1LvV1Pd","answer_images":["https://img.examtopics.com/pl-300/image280.png"],"isMC":false,"question_images":["https://img.examtopics.com/pl-300/image279.png"],"question_id":9,"exam_id":116,"url":"https://www.examtopics.com/discussions/microsoft/view/94598-exam-pl-300-topic-1-question-17-discussion/","answer_ET":"","timestamp":"2023-01-09 16:43:00","unix_timestamp":1673278980,"answer":"","answers_community":[],"answer_description":"","topic":"1","question_text":"DRAG DROP\n-\n\nYou publish a dataset that contains data from an on-premises Microsoft SQL Server database.\n\nThe dataset must be refreshed daily.\n\nYou need to ensure that the Power BI service can connect to the database and refresh the dataset.\n\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n\n//IMG//","discussion":[{"poster":"svg10gh","upvote_count":"50","content":"Current sequence looks good","timestamp":"1673405700.0","comment_id":"771967"},{"comment_id":"861739","content":"Given Answer is correct\n\n\nSet up an on-premises data gateway: Download and install an on-premises data gateway on a machine that has access to the SQL Server database. Make sure that the gateway is registered to the same workspace as the dataset.\n\nConfigure a data source: In the Power BI service, go to the dataset settings, and select the data source. Then, enter the necessary details, including the server name, database name, and credentials.\n\nSchedule refresh: In the dataset settings, go to the \"Scheduled refresh\" tab, and set up a refresh schedule. Ensure that the gateway is selected as the \"Data source credentials\" option.\n\nPublish the dataset: Finally, publish the dataset to the Power BI service. The dataset will be refreshed according to the schedule you set up, and the on-premises data gateway will allow the service to connect to the SQL Server database.\n\nNo confusion, and no need to discuss further","timestamp":"1680666900.0","comments":[{"comments":[{"content":"then the 4th step should be schedule the refresh","upvote_count":"3","comment_id":"884703","poster":"lizbette","timestamp":"1682800800.0"}],"comment_id":"884702","poster":"lizbette","timestamp":"1682800800.0","content":"3rd step should be add dataset owner to the data source","upvote_count":"3"},{"comment_id":"912746","content":"Publish the dataset isn't one of the answers tho","timestamp":"1685703300.0","comments":[{"content":"Publish the dataset is not available answer.\nNo confusion and no need to discuss further on that","upvote_count":"4","comment_id":"1312655","poster":"jaume","timestamp":"1731679380.0"}],"poster":"LouStar2","upvote_count":"12"}],"poster":"SanaCanada","upvote_count":"33"},{"comment_id":"1294169","content":"Configure an on-premises data gateway\nInstall and set up the on-premises data gateway to enable secure data transfer between the on-premises SQL Server and Power BI service.\n\nAdd a data source\nAdd the SQL Server database as a data source within the configured gateway, providing necessary connection details (e.g., server name, database name, and credentials).\n\nAdd the dataset owner to the data source\nEnsure the dataset owner has access to the data source in the gateway, granting them permission to use the connection for refreshing the dataset.\n\nConfigure a scheduled refresh\nSet up the dataset in Power BI to refresh on a daily schedule, using the gateway to pull updated data from the SQL Server.\n\nNote : adding a data source, adding the dataset owner to the data source, and configuring a scheduled refresh—are performed within Power BI Service","upvote_count":"1","timestamp":"1728293520.0","poster":"Madhu155"},{"upvote_count":"1","content":"What if I add the Data Source before setting up the Data Gateway?","timestamp":"1728018300.0","comment_id":"1293028","poster":"VladAbz"},{"poster":"rcaliandro","upvote_count":"1","content":"The answers seems OK to me:\n1 - Configure an om-prem gateway\n2 - Add a data source\n3 - Add the dataset owner to the data source\n4 - Configure a scheduled refresh","comment_id":"1267201","timestamp":"1723817460.0"},{"timestamp":"1710021300.0","comment_id":"1169871","poster":"MANANDAVEY","content":"correct sequence","upvote_count":"1"},{"poster":"6f38739","upvote_count":"4","timestamp":"1708974300.0","comment_id":"1160008","content":"This question is dumb AF, like it doesnt really matter if you create dataset or gateway first, all you need to remember is to have it ready before running refresh,"},{"upvote_count":"5","timestamp":"1708699500.0","comment_id":"1157212","poster":"panic_attack","content":"This was on the exam on 22/2/2024 (:"},{"content":"I don't understand what \"add dataset owner to the data source\" does. If the \"data source\" is the on-prem SQL DB and \"you\" are or are going to be the owner of the dataset, shouldn't you be added to the SQL DB first to be able to set it up in the gateway?","upvote_count":"3","poster":"Chellz","comment_id":"1127136","timestamp":"1705738320.0"},{"comments":[{"upvote_count":"3","timestamp":"1695803340.0","comment_id":"1018607","content":"Nope, the first step should be Configure data gateway. then, in second step we could have ability to add data source.","poster":"tranquanghuy2111","comments":[{"poster":"JudT","comment_id":"1195854","content":"That was on video...","upvote_count":"2","timestamp":"1713163380.0"}]}],"timestamp":"1693856160.0","content":"Here are the correct Sequences: \nFirst Step : Add a data source. This involves specifying the connection details, such as the server's name, database name, and authentication credentials.\n\nSecond Step: Configure an on-premises data gateway. The gateway acts as a bridge between our on-premises data sources and the Power BI service in the cloud. It allows secure data transfer and access.\n\nThird Step: Add the dataset owner to the data source. Ensure that the dataset owner (the Power BI user or service account) has appropriate permissions to access the on-premises SQL server database. This is important for successful data retrieval during refresh.\n\nFourth Step: Configure a scheduled refresh. Schedule the refresh to occur daily to keep the dataset up to date.","poster":"Igetmyrole","comment_id":"998814","upvote_count":"4"},{"upvote_count":"5","timestamp":"1690625520.0","poster":"Chenemi","comment_id":"966311","content":"SETUP ON PREMISES DATA GATEWAY\nADD DATA SOURCE\nADD DATASET OWNER\nSCHEDULE REFRESH"},{"timestamp":"1688535240.0","comment_id":"943333","comments":[{"timestamp":"1704691560.0","content":"You cannot schedule a refresh if the data owner is not established.","poster":"CookieMingkee","upvote_count":"1","comment_id":"1116420"}],"upvote_count":"1","content":"comparing to the given answer, i would select add data owner as the last one - https://learn.microsoft.com/en-us/training/modules/manage-datasets-power-bi/5-dataset-refresh","poster":"safz"},{"timestamp":"1679622960.0","comment_id":"848855","comments":[{"comment_id":"1251885","content":"FIRST DONWLOAD THE DATEGATEWAY FROM POWER BI .COM","upvote_count":"1","poster":"niceguysfinishlast","timestamp":"1721497920.0"}],"upvote_count":"4","poster":"killershark","content":"One thing I didn't understand is that first we need to add data source and then configure an on-premises data gateway. Someone please help me understand why we are not following this order?"},{"poster":"DUVANES","upvote_count":"3","timestamp":"1679587320.0","content":"1. Configure una puerta de enlace de datos local.\n2. Agregue un origen de datos.\n3. Agregue el propietario del conjunto de datos al origen de datos.\n4. Configure una actualización programada.","comment_id":"848411"},{"content":"Could anyone provide a link to this? Seem like this requires pragmatic experience","comments":[{"content":"https://learn.microsoft.com/en-us/training/modules/manage-datasets-power-bi/4-power-bi-gateway\nand the study page after that\nhttps://learn.microsoft.com/en-us/training/modules/manage-datasets-power-bi/5-dataset-refresh\nIt does not necessarily talk about this order, but it does help to understand how this works. With those pages I was able to logically put the four things in the correct order.","upvote_count":"2","comment_id":"876385","timestamp":"1682071800.0","poster":"semauni"}],"timestamp":"1677571920.0","comment_id":"824537","upvote_count":"4","poster":"Heyzzzzzzzzzzzzzz"},{"poster":"KoS83","upvote_count":"2","comment_id":"813049","timestamp":"1676728500.0","content":"Any good link for this topic?"},{"content":"Why Add a data source must be before Add dataset owner to data source?","poster":"MalenaLIU","comments":[{"timestamp":"1678987260.0","comment_id":"841220","poster":"PetJoh422","content":"You first need to add a source to the gateway and then give permission to that source.\nWithout adding the source to the gw list there is nothing to give access to","upvote_count":"6"}],"timestamp":"1676438700.0","upvote_count":"1","comment_id":"809133"},{"timestamp":"1673279100.0","upvote_count":"1","comment_id":"770598","content":"Correct.","poster":"JoaoTrade"},{"content":"Configure an on-premises data gateway.\nAdd the dataset owner to the data source.\nAdd a data source.\nConfigure a scheduled refresh.","timestamp":"1673278980.0","upvote_count":"1","poster":"jsking","comments":[{"upvote_count":"3","timestamp":"1673279340.0","comments":[{"comment_id":"771970","content":"Agree.","poster":"Hansen_G","upvote_count":"1","timestamp":"1673406060.0"}],"comment_id":"770609","poster":"jsking","content":"I changed my mind. The answer provided is correct because the owner needs a data source to own in the first place so add a data source should be second"}],"comment_id":"770593"}]},{"id":"mQ017rJLjkTfHy74z9vF","question_id":10,"answer":"B","unix_timestamp":1673279160,"answer_ET":"B","question_text":"You attempt to connect Power BI Desktop to a Cassandra database.\n\nFrom the Get Data connector list, you discover that there is no specific connector for the Cassandra database.\n\nYou need to select an alternate data connector that will connect to the database.\n\nWhich type of connector should you choose?","answer_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/94599-exam-pl-300-topic-1-question-18-discussion/","answers_community":["B (100%)"],"choices":{"D":"OData","A":"Microsoft SQL Server database","C":"OLE DB","B":"ODBC"},"topic":"1","discussion":[{"upvote_count":"52","poster":"GuerreiroJunior","content":"Selected Answer: B\nB is Correct because, B´cause it allows you to connect to data sources that aren't identified in the Get Data lists.\n\nThe ODBC connector lets you import data from any third-party ODBC driver simply by specifying a Data Source Name (DSN) or a connection string. As an option, you can also specify a SQL statement to execute against the ODBC driver.\nList details a few examples of data sources to which Power BI Desktop can connect by using the generic ODBC interface:\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-connect-using-generic-interfaces","timestamp":"1673354400.0","comment_id":"771409"},{"upvote_count":"35","poster":"Pinha","comment_id":"837936","content":"The anwer is B\n* Cassandra has an ODBC driver available that can be used to connect to the database using the ODBC connector in Power BI.\n\n* Microsoft SQL Server database is specifically designed to connect to SQL Server databases, \n\n* OLE DB is designed to connect to Microsoft databases and other third-party databases, \n\n* OData is designed to connect to web-based data sources","timestamp":"1678712940.0"},{"comment_id":"1265778","timestamp":"1723641120.0","poster":"rcaliandro","upvote_count":"1","content":"Selected Answer: B\nODBC is correct"},{"timestamp":"1721553300.0","upvote_count":"1","poster":"Kiran37","comment_id":"1252344","content":"ODBC allows to connect other DB"},{"content":"Selected Answer: B\ncorrect","comment_id":"1167998","poster":"AZFabio","timestamp":"1709816400.0","upvote_count":"1"},{"upvote_count":"4","comment_id":"998816","content":"B is the correct answer.\nODBC (Open Database Connectivity) is a general-purpose data access that allows us to connect to a wide range of databases, including Cassandra, using ODBC drivers.","poster":"Igetmyrole","timestamp":"1693856520.0"},{"poster":"Nunya101","upvote_count":"1","comment_id":"990842","timestamp":"1693059000.0","content":"Open database connector"},{"content":"Selected Answer: B\nodbc allows connection to third party database","poster":"Chenemi","upvote_count":"1","comment_id":"966314","timestamp":"1690625640.0"},{"upvote_count":"4","poster":"MoxieTT","comment_id":"945390","timestamp":"1688710500.0","content":"This was on the exam"},{"timestamp":"1688648280.0","content":"Given answer is correct","upvote_count":"1","poster":"kayani29","comment_id":"944668"},{"upvote_count":"4","poster":"sergeyitaly","comment_id":"919951","timestamp":"1686392220.0","content":"To connect Power BI Desktop to a Cassandra database when there is no specific connector available, you should choose option B: ODBC (Open Database Connectivity) connector.\n\nODBC is a widely-used standard for connecting to various types of databases, including Cassandra. It provides a common interface that allows applications like Power BI to communicate with different database systems using the same API. By leveraging the ODBC connector, you can establish a connection to the Cassandra database and retrieve data for analysis and visualization in Power BI."},{"upvote_count":"1","poster":"inejo","comment_id":"918546","content":"La respuesta es correcta :)","timestamp":"1686246840.0"},{"poster":"MoxieTT","comment_id":"900163","timestamp":"1684328400.0","content":"Awful question. This is a prime example of why the current exam, is not fit for purpose and needs revamped. How is this relevant to the day-to-day role of a data analyst?","comments":[{"poster":"Sophieeeeee","timestamp":"1719386940.0","upvote_count":"1","content":"Totally agree. There are too many questions should be related to the data engineer instead of data analyst...I don't need to know this to provide a good analysis to solve clients' business problems...","comment_id":"1237292"}],"upvote_count":"10"},{"poster":"Shalaleh","comment_id":"886427","upvote_count":"1","timestamp":"1682954040.0","content":"Microsoft open database connectivity"},{"poster":"Shalaleh","timestamp":"1682953980.0","content":"Selected Answer: B\nB is the correct answer.","comment_id":"886426","upvote_count":"1"},{"poster":"DUVANES","content":"Selected Answer: B\nB. ODBC","timestamp":"1679587560.0","comment_id":"848415","upvote_count":"1"},{"content":"Selected Answer: B\nAnswer is correct.\n\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-connect-using-generic-interfaces","upvote_count":"2","poster":"jsking","comment_id":"770615","timestamp":"1673279640.0"},{"upvote_count":"1","comment_id":"770599","poster":"JoaoTrade","timestamp":"1673279160.0","content":"Selected Answer: B\nB is correct"}],"isMC":true,"question_images":[],"exam_id":116,"timestamp":"2023-01-09 16:46:00","answer_description":""}],"exam":{"id":116,"lastUpdated":"12 Apr 2025","isMCOnly":false,"numberOfQuestions":334,"provider":"Microsoft","isImplemented":true,"isBeta":false,"name":"PL-300"},"currentPage":2},"__N_SSP":true}