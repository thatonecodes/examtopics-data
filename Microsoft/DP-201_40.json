{"pageProps":{"questions":[{"id":"7MsvtyMLwVXSIEJJMtXE","timestamp":"2020-04-21 19:18:00","discussion":[{"content":"Azure IR is perfectly capable of running Salesforce connection. No need for VM or SSIS runtime.\nTumbling trigger will serve better here.\nOnly the copy activity is the right answer.","comment_id":"82609","timestamp":"1588425060.0","comments":[{"upvote_count":"5","timestamp":"1589529660.0","poster":"vistran","comments":[{"timestamp":"1619158200.0","poster":"anamaster","comment_id":"341408","upvote_count":"4","content":"it is scheduled trigger since modified dates are not reliable"}],"comment_id":"89369","content":"why cant it be scheduled trigger ?"},{"content":"Agree with Azure IR as this is cloud to cloud. \n\nScheduled trigger makes more sense as we need to get data every 8 hours","poster":"dinu517","timestamp":"1594168620.0","comment_id":"129348","upvote_count":"13"},{"poster":"knightkkd","comment_id":"197470","content":"As modified dates are not reliable, tumbling windows should not be used, Scheduled trigger is the correct option","upvote_count":"9","comments":[{"upvote_count":"1","timestamp":"1629074760.0","poster":"lgtiza","comment_id":"425503","content":"Modified dates do not play any role here. In a tumbling window you just set up a starting date/time and set up a frequency and that's it. It will work exactly the same as with a scheduled trigger with recurrence every 8 hours. But tumbling windows are a bit safer because you don't need to set up the first date/time in the future."}],"timestamp":"1602397500.0"},{"content":"https://docs.microsoft.com/en-us/azure/data-factory/connector-salesforce","timestamp":"1618904760.0","upvote_count":"2","comment_id":"339398","poster":"Devendra00023"}],"poster":"Leonido","upvote_count":"17"},{"comment_id":"239586","content":"I am literally working a project right now where we ingest data from salesforce to data lake. Its the Azure IR.","poster":"BungyTex","upvote_count":"9","timestamp":"1607556360.0"},{"content":"As suggested it seems Azure IR is also possible and this is also the only solution, because it is mentioned to use PaaS and no VMs managed by Litware. Self-hosted IR requires managing VMs, therefore => Azure IR","comment_id":"497133","poster":"Larrave","timestamp":"1638992280.0","upvote_count":"1"},{"content":"Based on reviewed information all the answers provided are correct.","upvote_count":"1","comment_id":"374901","poster":"dbdev","timestamp":"1622880960.0"},{"comments":[{"comment_id":"361534","timestamp":"1621441200.0","upvote_count":"1","poster":"Psycho","content":"Schedule trigger: A trigger that invokes a pipeline on a wall-clock schedule.\n\nTumbling window trigger: A trigger that operates on a periodic interval, while also retaining state.\n\nEvent-based trigger: A trigger that responds to an event."}],"upvote_count":"2","timestamp":"1618126020.0","comment_id":"333093","content":"I agree is azure IR because salesforce SaaS, but I don’t know what trigger is better. Schedule with frequency 8h fit perfectly but, at the same time Tumbling with 8h window do the same. I think the first one always trigger at these hours independently if the process was cancelled or not, and tumbling may introduce delays if process fails and has to be relaunched because next window will be 8h later than this second try. Am I right?","poster":"maynard13x8"},{"comment_id":"267255","timestamp":"1610648160.0","upvote_count":"8","poster":"mohowzeh","content":"See https://docs.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats:\n\"When you're copying data between two data stores that are publicly accessible through the internet from any IP, you can use the Azure integration runtime for the copy activity. This integration runtime is secure, reliable, scalable, and globally available.\nWhen you're copying data to and from data stores that are located on-premises or in a network with access control (for example, an Azure virtual network), you need to set up a self-hosted integration runtime.\"\nFurther down on that page, Salesforce is listed as supported by Azure IR."},{"poster":"syu31svc","content":"https://docs.microsoft.com/en-us/azure/data-factory/connector-salesforce-service-cloud:\n\"When you copy data into Salesforce Service Cloud, the default Azure Integration Runtime can't be used to execute copy. In other words, if your source linked service doesn't have a specified integration runtime, explicitly create an Azure Integration Runtime with a location near your Salesforce Service Cloud instance.\"\nIntegration runtime is Azure\nTrigger as schedule and activity as copy are correct","upvote_count":"1","comment_id":"240008","timestamp":"1607597460.0"},{"comment_id":"184648","poster":"master28aug","timestamp":"1600797540.0","upvote_count":"2","content":"we will need Self-hosted since the salesforce is an on-premise Source. \nref- https://docs.microsoft.com/en-us/azure/data-factory/connector-salesforce"},{"content":"the answer should be tumbling window trigger isn't it?","timestamp":"1597876560.0","poster":"kittykat","comment_id":"161823","upvote_count":"2","comments":[{"content":"\"Row modified dates are not trusted in the source table.\"","poster":"AJMorgan591","upvote_count":"1","comments":[{"content":"You keep saying this but its not clear why this makes a difference. Scheduled is time of day where as tumbling relates to every n hours. How does differing triggers for 800/1400/2000 hrs eg have any bearing on the row modified date? I'm not saying its wrong just you need to justify it better. I believe tumbling is a better approach because you have 1 not 3 triggers.","poster":"essdeecee","comment_id":"203782","timestamp":"1603292400.0","comments":[{"content":"Tumbling Window in ADF depends on a time field of the source data to determine if it should process it or not. In the case of Scheduled trigger, it keeps track of the processed date and time for each row externally. In case of the scheduled trigger, you only need one trigger, not 3.","comment_id":"206246","timestamp":"1603715520.0","poster":"M0e","upvote_count":"3","comments":[{"timestamp":"1604054340.0","poster":"essdeecee","content":"I'm pretty sure that's not true. I'm looking at it now and cannot see that dependency. Also there is not mention of that requirement here:https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger","upvote_count":"2","comment_id":"209191"},{"upvote_count":"1","timestamp":"1612978920.0","comment_id":"287756","content":"You can Create Scheduled Type Triger with recurrence every n Hours. However MS says that the Scheduled is less Reliable. So will tend to agree that Tumbling is more appropriate. Here is difference Between Scheduled and Tumbling Triggers for ADF : https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison","poster":"sturcu"}]}],"upvote_count":"3"}],"comment_id":"183337","timestamp":"1600646640.0"}]},{"poster":"krisspark","comment_id":"148986","upvote_count":"4","content":"As it's mentioned every 8 hrs why not tumbling window trigger rather scheduled.. ?","timestamp":"1596356280.0","comments":[]},{"comment_id":"137765","content":"Important\n\nWhen you copy data into Salesforce, the default Azure Integration Runtime can't be used to execute copy. In other words, if your source linked service doesn't have a specified integration runtime, explicitly create an Azure Integration Runtime with a location near your Salesforce instance. Associate the Salesforce linked service as in the following example.\n\nSo the given answer is correct.","upvote_count":"5","poster":"peppele","timestamp":"1595063220.0","comments":[{"timestamp":"1595268660.0","content":"In the question its mentioned to copy data from salesforce not into salesforce. I feel Azure IR should be correct here.","upvote_count":"9","poster":"Anilpanda10","comment_id":"139758"}]},{"comment_id":"137726","poster":"proca","upvote_count":"1","timestamp":"1595056920.0","content":"\"When you copy data into Salesforce, the default Azure Integration Runtime can't be used to execute copy. In other words, if your source linked service doesn't have a specified integration runtime, explicitly create an Azure Integration Runtime with a location near your Salesforce instance. Associate the Salesforce linked service as in the following example.\"\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-salesforce"},{"content":"For box 1 both Azure IR and self hosted IR are correct as the data is in Salesforce and Salesforce supports both: https://docs.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats","upvote_count":"2","poster":"Tommy65","comment_id":"127031","timestamp":"1593968400.0"},{"content":"IR to be used is Azure IR as mentioned here --> https://docs.microsoft.com/en-us/azure/data-factory/connector-salesforce \n\"The integration runtime to be used to connect to the data store. If not specified, it uses the default Azure Integration Runtime\"","poster":"pravinDataSpecialist","upvote_count":"1","timestamp":"1592746680.0","comment_id":"115606"},{"upvote_count":"3","comment_id":"77533","content":"Salesforce is a cloud data source even though there is no clear explanation from the question. Azure IR and Self-Hosted IR both will work via different approaches. Would prefer Azure IR as the answer due to simplicity.\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-migration-guidance-s3-azure-storage","timestamp":"1587489480.0","comments":[{"poster":"Tombarc","comments":[{"poster":"spiitr","timestamp":"1613658840.0","upvote_count":"1","comment_id":"293458","content":"Any third party cloud or in fact Azure VM (or Azure IaaS) requires self-hosted IR"}],"content":"Azure IR is recommended for Azure services only","upvote_count":"10","timestamp":"1587828960.0","comment_id":"79556"},{"comments":[{"upvote_count":"7","content":"If they say Salesforce SaaS, the answer should be: Azure IR (https://docs.microsoft.com/en-gb/azure/data-factory/copy-activity-overview - [Salesforce -> Supported by Azure IR -> ✓])","comment_id":"206249","poster":"M0e","timestamp":"1603716240.0"}],"content":"I just completed the exam, the question is along the lines of: \"comes from Salesforce, a SaaS application\" if that helps","comment_id":"189435","timestamp":"1601354700.0","upvote_count":"7","poster":"AusAv"},{"upvote_count":"13","timestamp":"1588247640.0","content":"I agree on this. If you want to determine which IR to use you can read https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#determining-which-ir-to-use. You need to look at the source and sink environment where you run the ADF pipeline. In case of salesForce, ADF has a SalesForce connector which does not have a source IR (https://docs.microsoft.com/en-gb/azure/data-factory/connector-salesforce). Instead it requires you to specify the IR of the sink. In this case the sink lies within Azure. Since the sink lies within Azure, the recommended IR is the Azure IR.","poster":"HCL1991","comment_id":"81659"}],"poster":"Yuri1101"}],"answers_community":[],"unix_timestamp":1587489480,"topic":"7","url":"https://www.examtopics.com/discussions/microsoft/view/18877-exam-dp-201-topic-7-question-2-discussion/","question_text":"HOTSPOT -\nWhich Azure Data Factory components should you recommend using together to import the customer data from Salesforce to Data Lake Storage? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0012500001.jpg"],"isMC":false,"question_id":196,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0012400001.jpg"],"answer_description":"Box 1: Self-hosted integration runtime\nA self-hosted IR is capable of nunning copy activity between a cloud data stores and a data store in private network.\n\nBox 2: Schedule trigger -\n\nSchedule every 8 hours -\n\nBox 3: Copy activity -\nScenario:\n✑ Customer data, including name, contact information, and loyalty number, comes from Salesforce and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table.\n✑ Product data, including product ID, name, and category, comes from Salesforce and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table.","exam_id":66,"answer":"","answer_ET":""},{"id":"eY4zOUXp9BDoEzJQAGMz","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0012800001.jpg"],"question_text":"HOTSPOT -\nWhich Azure Data Factory components should you recommend using together to import the daily inventory data from SQL to Azure Data Lake Storage? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_ET":"","answers_community":[],"answer_description":"Box 1: Self-hosted integration runtime\nA self-hosted IR is capable of nunning copy activity between a cloud data stores and a data store in private network.\nScenario: Daily inventory data comes from a Microsoft SQL server located on a private network.\n\nBox 2: Schedule trigger -\n\nDaily schedule -\n\nBox 3: Copy activity -\nScenario:\nStage inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store. Litware wants to remove transient data from Data\nLake Storage once the data is no longer in use. Files that have a modified date that is older than 14 days must be removed.","discussion":[{"poster":"BobFar","content":"Daily inventory data comes from a Microsoft SQL server located on a private network.\nself-hosted\nSee inventory levels across the stores. Data must be updated as close to real time as possible.\nevent-based trigger\nTo import the daily inventory data from SQL to Azure Data Lake Storage\ncopy activity","timestamp":"1623005580.0","comments":[{"poster":"KRV","content":"completely agree with BobFar , since MS SQL server on pvt network the integration run time has to be self hosted and as far as the trigger type is concerned it has to be Event based since the inventory levels are expected to be updated closer to real time as possible","comment_id":"380717","timestamp":"1623537660.0","upvote_count":"1"},{"content":"Agree this is 100% correct.","poster":"ZodiaC","timestamp":"1624342260.0","comment_id":"387635","upvote_count":"1"},{"upvote_count":"1","timestamp":"1665243360.0","comment_id":"689477","poster":"Ashwat_Thama","content":"the event based trigger can be scheduled for only storage events, and here we are moving data from sql to ADLS gen 2. so i think the given answers are correct."}],"upvote_count":"9","comment_id":"376292"},{"timestamp":"1615607040.0","comment_id":"309378","content":"From data lake to analytical store - Azure IR, \ndata must be updated as close to real time as possible - event base,\ncopy","upvote_count":"2","comments":[{"upvote_count":"11","comments":[],"timestamp":"1619277240.0","poster":"anamaster","content":"\"Daily inventory data comes from a Microsoft SQL server located on a private network.\" -> self-hosted","comment_id":"342081"}],"poster":"AlexD332"},{"content":"Shouldn't it be tumbling window as we should get daily inventory?","comments":[{"timestamp":"1603292520.0","poster":"essdeecee","upvote_count":"7","content":"Scheduled is fine as its daily its probably at a specific time hence daily as 0200 or similar suffices. Compare to the previous where its every 8 hours. That is a better model for tumbling than this.","comment_id":"203783","comments":[{"content":"It says: \"See inventory levels across the stores. Data must be updated as close to real time as possible.\" in the description.","comment_id":"296870","upvote_count":"3","poster":"nick132","timestamp":"1614020160.0","comments":[{"content":"I agree, should be event based","poster":"anamaster","upvote_count":"6","comment_id":"342082","timestamp":"1619277300.0"},{"content":"AGREE, its EVENT-BASED TRIGGER 100%!","poster":"ZodiaC","upvote_count":"1","timestamp":"1624342320.0","comment_id":"387638"}]}]}],"timestamp":"1602398400.0","upvote_count":"2","comment_id":"197479","poster":"knightkkd"}],"answer":"","exam_id":66,"question_id":197,"timestamp":"2020-10-11 08:40:00","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0012700001.jpg"],"topic":"7","isMC":false,"url":"https://www.examtopics.com/discussions/microsoft/view/34173-exam-dp-201-topic-7-question-3-discussion/","unix_timestamp":1602398400},{"id":"o1OT9cWxW9TJkQYziicl","isMC":false,"topic":"7","question_text":"HOTSPOT -\nWhich Azure service and feature should you recommend using to manage the transient data for Data Lake Storage? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","discussion":[{"poster":"AhmedReda","comment_id":"122050","timestamp":"1593365160.0","comments":[{"comment_id":"209051","content":"In ADF, the Metadata activity has the LastModified property through which we can delete the files I believe.","poster":"Sai02","timestamp":"1604038320.0","upvote_count":"5"}],"upvote_count":"24","content":"The question asked to remove files older than 14 days which i think ADF & Delete could not do it, so the answer might be = (1) Azure Storage (2) Lifecycle management rule"},{"timestamp":"1588439700.0","content":"The files are stored in ADLS Gen2 which supports Life cycle management rules","comments":[{"timestamp":"1624474380.0","content":"Yes, This is correct.","poster":"vrmei","upvote_count":"1","comment_id":"389046"},{"content":"https://azure.microsoft.com/en-au/updates/lifecycle-management-for-azure-data-lake-storage-is-now-generally-available/","comment_id":"361542","poster":"Psycho","timestamp":"1621441680.0","upvote_count":"1"}],"upvote_count":"23","comment_id":"82678","poster":"bansal_vikrant"},{"content":"Given answer is correct\n(1)ADF\n(2)Delete activity","upvote_count":"1","comment_id":"393226","timestamp":"1624912860.0","poster":"hoangton"},{"comment_id":"390067","poster":"kn_shn","timestamp":"1624587000.0","upvote_count":"3","content":"From older comments, ADF + Delete and Azure Storage + Lifecycle management rule seem to have similar functionality to remove files. However there is a difference: Liftcycle is defined based on the creation of the file, and in this question and context, it says:\" Files that have a modified date that is older than 14 days must be removed\". i.e. the file removal is based on the modified date. As BungyTex confirmed below, ADF + Delete can achieve this objective and the answer is correct."},{"timestamp":"1622977200.0","poster":"savin","content":"Azure storage\nlifecycle management should be easier option","upvote_count":"2","comment_id":"375934"},{"comment_id":"368897","content":"The way i see this, if the inventory data is coming from a microsoft SQL server, it is being ingested by ADF and not in Azure Storage, and if using ADF then the delete activity should be used. As per other comments this is proven to work","upvote_count":"1","timestamp":"1622220240.0","poster":"Dymize"},{"comment_id":"345633","timestamp":"1619724840.0","upvote_count":"6","content":"Azure storage\n lifecycle management","poster":"davita8"},{"upvote_count":"4","poster":"felmasri","content":"Azure Data Lake Storage lifecycle management is now generally available\nhttps://azure.microsoft.com/en-us/updates/lifecycle-management-for-azure-data-lake-storage-is-now-generally-available/","comment_id":"309337","timestamp":"1615600560.0"},{"upvote_count":"3","poster":"Needium","content":"The prefered option should be Az Storage and life cycle management rule","timestamp":"1615170720.0","comment_id":"305467"},{"poster":"lky17","comment_id":"304983","upvote_count":"2","timestamp":"1615101000.0","content":"The correct answer should be Az Store and Lifecycle ... because ADLSG2 lets delete any file, the unique exception is \"If you use the Delete Blob API to delete a directory, that directory will be deleted only if it's empty. This means that you can't use the Blob API delete directories recursively.\" and support all operation in lifecycle management except \"Lifecycle management policies with premium tier for Azure Data Lake Storage.\nYou can't move data that's stored in the premium tier between hot, cool, and archive tiers. However, you can copy data from the premium tier to the hot access tier in a different account.\"\n\nRef https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues"},{"content":"lifecycle management is available in ADLS from July 31, 2020\nhttps://azure.microsoft.com/en-us/updates/lifecycle-management-for-azure-data-lake-storage-is-now-generally-available/","timestamp":"1614005880.0","comment_id":"296725","poster":"NasRim","upvote_count":"3"},{"poster":"ThijsN","content":"Both ADF with delete or storage with lifecyle will work. I literally build the last one this week. I think that is the best solution as this is the cheapest and easiest. Doesn't cost anything to run, to build or to maintain.","timestamp":"1610810460.0","upvote_count":"3","comment_id":"268911"},{"content":"Seems to me that there are two valid combinations: (Azure Data Factory, delete activity) and (Azure storage, Lifecycle management)","comments":[{"comment_id":"364961","timestamp":"1621795560.0","content":"and second one the easiest!!","upvote_count":"1","poster":"memo43"}],"poster":"mohowzeh","upvote_count":"4","comment_id":"267269","timestamp":"1610648880.0"},{"content":"Lifecycle management policies (delete blob): Generally available in Premium, Generally available in Standard https://docs.microsoft.com/pl-pl/azure/storage/blobs/data-lake-storage-supported-blob-storage-features","upvote_count":"2","timestamp":"1608299880.0","poster":"KasiaK","comment_id":"247359"},{"timestamp":"1607581380.0","upvote_count":"9","content":"https://azure.microsoft.com/en-us/updates/lifecycle-management-for-azure-data-lake-storage-is-now-generally-available/\nAzure storage and lifecycle management rule are the answers","comment_id":"239802","poster":"syu31svc"},{"content":"I just tested this in my ADL Gen 2, can set a rule to delete files last modifed more than 14 days ago.","timestamp":"1607556720.0","comment_id":"239590","upvote_count":"6","poster":"BungyTex"},{"upvote_count":"3","content":"Now, Lifecycle management is supported for accounts that have a hierarchical namespace for General-purpose V2.\nWith this, you can reduce the delete activity (less cost even it is negligible for a pipeline).\nHowever, I would prefer to use delete activity in ADF to make sure that they got deleted after I load them to database. Better than auto delete through lifecycle.\nFor me, given answer is correct based on requirement.","comment_id":"151703","poster":"NikP","timestamp":"1596689880.0"},{"timestamp":"1596166440.0","comment_id":"147783","content":"Answer shall be: Azure Storage & Lifecycle management\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts?tabs=azure-portal","upvote_count":"7","poster":"dcpavelescu"},{"upvote_count":"3","timestamp":"1595521200.0","poster":"Ash666","content":"ADLS GEN 2 doesn’t yet support delete blob action\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues","comment_id":"142158"},{"comment_id":"86879","content":"ADLS still not support lifecycle management. so that answer is correct\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues","upvote_count":"2","poster":"PhuVu","comments":[{"timestamp":"1589861040.0","content":"https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction\n\"Blob storage features such as diagnostic logging, access tiers, and Blob Storage lifecycle management policies now work with accounts that have a hierarchical namespace. Therefore, you can enable hierarchical namespaces on your Blob storage accounts without losing access to these features.\"","upvote_count":"15","comment_id":"91791","poster":"Manue"},{"upvote_count":"2","content":"From the shared document: \"Lifecycle management policies are supported only on general-purpose v2 accounts. They aren't yet supported in premium BlockBlobStorage storage accounts.\"","poster":"M0e","timestamp":"1603718580.0","comment_id":"206273"}],"timestamp":"1589163300.0"},{"timestamp":"1588351560.0","comment_id":"82262","content":"\"Litware wants to remove transient data from Data\nLake Storage once the data is no longer in use.\" wouldn't that make B Azure Storage a better answer?","poster":"runningman","upvote_count":"3","comments":[{"upvote_count":"1","poster":"runningman","timestamp":"1588351680.0","comment_id":"82263","content":"Never mind. if Delete Activity is correct (which i believe it is), the first answer has to be ADF."}]},{"content":"Would lifecycle management not fulfill this as well?","timestamp":"1585746780.0","comments":[{"comment_id":"72657","timestamp":"1586437080.0","content":"This function is only available for Azure blob storage and not for ADLS. So the only possible answer is ADF.","upvote_count":"13","poster":"HCL1991","comments":[{"upvote_count":"3","comment_id":"136290","poster":"envy","timestamp":"1594887420.0","content":"the function seems available for ADLS: https://github.com/MicrosoftDocs/azure-docs/issues/42140"},{"comment_id":"206269","poster":"M0e","upvote_count":"4","content":"In Gen 2 it supports all the features of Blob Storage including lifecycle management.","timestamp":"1603718400.0"}]}],"upvote_count":"6","poster":"kempstonjoystick","comment_id":"70129"}],"unix_timestamp":1585746780,"exam_id":66,"timestamp":"2020-04-01 15:13:00","answers_community":[],"question_id":198,"question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0013000001.jpg"],"answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0013100001.jpg"],"answer":"","answer_description":"Scenario: Stage inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store. Litware wants to remove transient data from Data Lake Storage once the data is no longer in use. Files that have a modified date that is older than 14 days must be removed.\n\nService: Azure Data Factory -\nClean up files by built-in delete activity in Azure Data Factory (ADF).\nADF built-in delete activity, which can be part of your ETL workflow to deletes undesired files without writing code. You can use ADF to delete folder or files from\nAzure Blob Storage, Azure Data Lake Storage Gen1, Azure Data Lake Storage Gen2, File System, FTP Server, sFTP Server, and Amazon S3.\nYou can delete expired files only rather than deleting all the files in one folder. For example, you may want to only delete the files which were last modified more than 13 days ago.\n\nFeature: Delete Activity -\nReference:\nhttps://azure.microsoft.com/sv-se/blog/clean-up-files-by-built-in-delete-activity-in-azure-data-factory/\nDesign data processing solutions","answer_ET":"","url":"https://www.examtopics.com/discussions/microsoft/view/17762-exam-dp-201-topic-7-question-4-discussion/"},{"id":"EvSokWnMqkApzaayIULO","question_id":199,"answer_description":"Box 1: SqlSink Table -\nSensor data must be stored in a Cosmos DB named treydata in a collection named SensorData\n\nBox 2: Cosmos Bulk Loading -\nUse Copy Activity in Azure Data Factory to copy data from and to Azure Cosmos DB (SQL API).\nScenario: Data from the Sensor Data collection will automatically be loaded into the Planning Assistance database once a week by using Azure Data Factory. You must be able to manually trigger the data load process.\nData used for Planning Assistance must be stored in a sharded Azure SQL Database.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db","isMC":false,"topic":"8","exam_id":66,"question_text":"HOTSPOT -\nYou need to design the data loading pipeline for Planning Assistance.\nWhat should you recommend? To answer, drag the appropriate technologies to the correct locations. Each technology may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nHot Area:\n//IMG//","answer_images":["https://www.examtopics.com/assets/media/exam-media/03774/0013700002.png"],"answer":"","discussion":[{"content":"The data pipeline is moving data from Cosmos DB to SQL Database. So, the 1st box should be Cosmos Bulking Loading and 2nd box should be SQLSink Table.","comment_id":"74006","comments":[{"content":"why Cosmos Bulk Loading when you are reading from Cosmos DB? should be Cosmos Query","timestamp":"1595862960.0","poster":"cmihai","comment_id":"145079","upvote_count":"22"},{"content":"I AGREE ITS ONCE A WEEK SO \n\nBULK LOADING and SQL DATABASE","timestamp":"1624308540.0","comment_id":"387407","poster":"ZodiaC","upvote_count":"2"}],"timestamp":"1586768940.0","poster":"Luke97","upvote_count":"77"},{"poster":"silango","upvote_count":"15","comment_id":"109231","comments":[{"timestamp":"1592031480.0","comment_id":"109232","content":"Box1: Cosmos query\nBox2: SQLSink Table","upvote_count":"52","poster":"silango"}],"content":"ADF uses Cosmos DB as the source. Shouldn't be Cosmos query?\nCheck: https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#azure-cosmos-db-sql-api-as-source","timestamp":"1592031360.0"},{"comment_id":"345634","timestamp":"1619724960.0","content":"Cosmos Query in Box1 and SqlSink Table in Box2","poster":"davita8","upvote_count":"3"},{"comment_id":"306853","poster":"felmasri","content":"I will go with Bulk Loading because of this it happens once a week:\nData from the Sensor Data collection will automatically be loaded into the Planning Assistance database once a week by using Azure Data Factory. You must be able to manually trigger the data load process.","timestamp":"1615349640.0","upvote_count":"3"},{"upvote_count":"2","timestamp":"1611835380.0","comment_id":"278514","content":"it is Query for the first one. check this: https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#azure-cosmos-db-sql-api-as-source","poster":"Az301301X"},{"timestamp":"1610887920.0","upvote_count":"3","content":"I can't find the adf \"Cosmos Bulk Loading\" anywhere. There is a bulk loading option for loading data INTO cosmos but i can't find the otherway anywhere","comment_id":"269497","poster":"ThijsN"},{"upvote_count":"4","comment_id":"267422","poster":"mohowzeh","content":"Second box = SQLSink Table\nFirst box: since sensor data are coming in pretty much like a stream, a change feed would be a good solution. If Cosmos query is used, how would you deal with the fact that the endpoint of the data moves every few seconds? \nhttps://devblogs.microsoft.com/cosmosdb/change-feed-unsung-hero-of-azure-cosmos-db/","timestamp":"1610658000.0"},{"content":"So is it Cosmos Bulk loading or cosmos query?","timestamp":"1610248980.0","upvote_count":"2","poster":"S3","comment_id":"263622"},{"poster":"groy","timestamp":"1601550780.0","comment_id":"190920","upvote_count":"7","content":"Box1: Cosmos query\nBox2: SQLSink Table"},{"timestamp":"1599509100.0","comment_id":"175400","content":"Why not cosmos change feed for box 1?","poster":"avix","comments":[{"comment_id":"215258","content":"Sensor data has a time stamp in it, and as it is only inserts, this time stamp will not change. So I guess you could use it, but a regular query would be enough. So my guess for box 1 is Cosmos query.","timestamp":"1604841120.0","upvote_count":"2","poster":"User27069"}],"upvote_count":"2"},{"upvote_count":"3","comment_id":"135655","content":"Sensor data from Cosmos DB > ADF is juist a Source or a Sink (table) \nso the left answer is correct \nhttps://docs.microsoft.com/en-us/azure/data-factory/copy-activity-overview","timestamp":"1594811880.0","poster":"LeonLeon"}],"timestamp":"2020-04-13 11:09:00","unix_timestamp":1586768940,"answer_ET":"","question_images":["https://www.examtopics.com/assets/media/exam-media/03774/0013700001.png"],"answers_community":[],"url":"https://www.examtopics.com/discussions/microsoft/view/18340-exam-dp-201-topic-8-question-1-discussion/"},{"id":"2AUKTFmxI7mURDdy7Q5S","answer_images":[],"question_text":"You need to design the runtime environment for the Real Time Response system.\nWhat should you recommend?","choices":{"D":"General Purpose nodes with the Enterprise Security package","A":"General Purpose nodes without the Enterprise Security package","B":"Memory Optimized Nodes without the Enterprise Security package","C":"Memory Optimized nodes with the Enterprise Security package"},"answer_description":"Scenario: You must maximize the performance of the Real Time Response system.","exam_id":66,"isMC":true,"answer":"B","topic":"8","timestamp":"2019-11-19 19:01:00","discussion":[{"content":"\"Azure Active Directory must be used for all services where it is available\" so Enterprise Security package is needed.\nCorrect answer is C","comment_id":"22780","upvote_count":"58","poster":"STH","comments":[{"upvote_count":"3","comment_id":"90407","poster":"vistran","timestamp":"1589700960.0","comments":[{"content":"good point! it's not clear and really confusing","poster":"obj95","comment_id":"107162","upvote_count":"2","timestamp":"1591820640.0"}],"content":"But it also says\nAzure resource costs must be minimized where possible.\nEnterprise Security package incur additional costs on the nodes\nMay be they meant using Azure AD if readily available with the service"},{"comment_id":"100009","timestamp":"1591016040.0","upvote_count":"3","poster":"Mathster","content":"Reference: https://docs.microsoft.com/fr-fr/azure/hdinsight/domain-joined/hdinsight-security-overview"}],"timestamp":"1574186460.0"},{"upvote_count":"18","comments":[{"upvote_count":"15","timestamp":"1608639720.0","poster":"Jaydeeppal","content":"I am really frustrated","comment_id":"250091"}],"timestamp":"1608252060.0","comment_id":"246961","content":"why so many answers are incorrect in this exam","poster":"rajat009"},{"comment_id":"384174","upvote_count":"1","poster":"azurrematt123","timestamp":"1623932100.0","comments":[{"content":"what is the correct answer then","comment_id":"386164","timestamp":"1624186380.0","poster":"vak2021","upvote_count":"1"}],"content":"Wondering how do I correct myself since so many answers are not correct?"},{"upvote_count":"2","poster":"davita8","comment_id":"345635","content":"C. Memory Optimized nodes with the Enterprise Security package","timestamp":"1619725080.0"},{"poster":"karma_wins","upvote_count":"1","timestamp":"1619444340.0","content":"it says \"script performance will be limited by available memory\", so shouldn't it be \"general purpose node\" ?","comment_id":"343308"},{"content":"The correct answer seems to be C, because of the requirement: Privacy and security policy -\nAzure Active Directory must be used for all services where it is available.\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/domain-joined/hdinsight-security-overview\n\nAuthentication\nEnterprise Security Package from HDInsight provides Active Directory-based authentication, multi-user support, and role-based access control. The Active Directory integration is achieved through the use of Azure Active Directory Domain Services. With these capabilities, you can create an HDInsight cluster joined to an Active Directory domain. Then configure a list of employees from the enterprise who can authenticate to the cluster.\n\nWith this setup, enterprise employees can sign in to the cluster nodes by using their domain credentials. They can also use their domain credentials to authenticate with other approved endpoints. Like Apache Ambari Views, ODBC, JDBC, PowerShell, and REST APIs to interact with the cluster.","comment_id":"284344","upvote_count":"3","timestamp":"1612551480.0","poster":"rmn900"},{"comment_id":"240097","poster":"syu31svc","upvote_count":"1","content":"C looks the most correct option to me","timestamp":"1607603820.0"},{"comment_id":"211481","timestamp":"1604346780.0","content":"azure databricks does not need enterprise security package like hdinsight. it is single sign on with Azure AD. Azure databricks is being used here to run pypspak script on cosmos db sensor collection data to to notify if emergency dispatch service is required.","poster":"Shrikant_Kulkarni","upvote_count":"3"},{"upvote_count":"1","comment_id":"207070","timestamp":"1603805400.0","poster":"M0e","content":"It says that the sensors should only write access to the data storage, meaning there is some sort of service principle authentication and authorization in place. Therefore, to let the Spark Cluster access the data, you need to have the Enterprise Security package installed."},{"content":"C: Open-source Apache Hadoop relies on the Kerberos protocol for authentication and security. Therefore, HDInsight cluster nodes with Enterprise Security Package (ESP) are joined to a domain that's managed by Azure AD DS. Kerberos security is configured for the Hadoop components on the cluster.\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/domain-joined/apache-domain-joined-architecture","poster":"Treadmill","upvote_count":"4","comment_id":"153960","timestamp":"1597008600.0"}],"question_id":200,"question_images":[],"url":"https://www.examtopics.com/discussions/microsoft/view/8590-exam-dp-201-topic-8-question-2-discussion/","answer_ET":"B","answers_community":[],"unix_timestamp":1574186460}],"exam":{"name":"DP-201","lastUpdated":"12 Apr 2025","isMCOnly":false,"isBeta":false,"provider":"Microsoft","id":66,"numberOfQuestions":206,"isImplemented":true},"currentPage":40},"__N_SSP":true}