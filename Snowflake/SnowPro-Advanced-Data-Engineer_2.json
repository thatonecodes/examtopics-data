{"pageProps":{"questions":[{"id":"r2hV3txx2FkPwjahcokf","discussion":[{"comment_id":"1057639","timestamp":"1698668160.0","poster":"stopthisnow","upvote_count":"5","content":"Selected Answer: AB\nReferring to Columns in Different DataFrames\nWhen referring to columns in two different DataFrame objects that have the same name (for example, joining the DataFrames on that column), you can use the DataFrame.col method in one DataFrame object to refer to a column in that object (for example, df1.col(\"name\") and df2.col(\"name\")).\n\nTo retrieve and manipulate data, you use the DataFrame class. A DataFrame represents a relational dataset that is evaluated lazily: it only executes when a specific action is triggered. In a sense, a DataFrame is like a query that needs to be evaluated in order to retrieve data.\n\nhttps://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes"},{"poster":"aba2s","upvote_count":"1","content":"Selected Answer: AB\nA is True. It's evident. \n\nExplanations for B:\n\nA Pandas dataframe is a relational object where the data is represented as rows and columns, think of it as an Excel spreadsheet. All data in a Pandas DataFrame is stored in client memory, and all operations and transformations on the DataFrame object are performed instantly.\n\nA Snowpark DataFrame is also a relational object, but the data is stored in Snowflake. The Snowpark DataFrame object contains only the logic, SQL, needed to retrieve data according to the operations and transformations applied to the DataFrame object. It is also evaluated lazily, meaning that it only executes the logic, the SQL, when a specific action is triggered.\n\nSo B is also True","timestamp":"1726043340.0","comment_id":"1282017"}],"answer":"AB","question_images":[],"exam_id":166,"isMC":true,"answer_images":[],"answer_ET":"AB","url":"https://www.examtopics.com/discussions/snowflake/view/124943-exam-snowpro-advanced-data-engineer-topic-1-question-14/","question_text":"A company has an extensive script in Scala that transforms data by leveraging DataFrames. A Data Engineer needs to move these transformations to Snowpark.\nWhat characteristics of data transformations in Snowpark should be considered to meet this requirement? (Choose two.)","choices":{"E":"Columns in different DataFrames with the same name should be referred to with squared brackets.","A":"It is possible to join multiple tables using DataFrames.","B":"Snowpark operations are executed lazily on the server.","C":"User-Defined Functions (UDFs) are not pushed down to Snowflake.","D":"Snowpark requires a separate cluster outside of Snowflake for computations."},"question_id":6,"answer_description":"","timestamp":"2023-10-30 13:16:00","unix_timestamp":1698668160,"answers_community":["AB (100%)"],"topic":"1"},{"id":"K2KnvKQHZH2lcE1sH9e3","answers_community":["A (100%)"],"answer_ET":"A","isMC":true,"answer_images":[],"discussion":[{"comment_id":"1057649","upvote_count":"4","content":"Selected Answer: A\ntotal_constant_partition_count\nTotal number of micro-partitions for which the value of the specified columns have reached a constant state (i.e. the micro-partitions will not benefit significantly from reclustering). The number of constant micro-partitions in a table has an impact on pruning for queries. The higher the number, the more micro-partitions can be pruned from queries executed on the table, which has a corresponding impact on performance.\n\nhttps://docs.snowflake.com/en/sql-reference/functions/system_clustering_information","poster":"stopthisnow","timestamp":"1730290920.0"}],"exam_id":166,"answer":"A","question_images":["https://img.examtopics.com/snowpro-advanced-data-engineer/image13.png"],"unix_timestamp":1698668520,"url":"https://www.examtopics.com/discussions/snowflake/view/124946-exam-snowpro-advanced-data-engineer-topic-1-question-15/","question_text":"The following is returned from SYSTEM$CLUSTERING_INFORMATION() for a table named ORDERS with a DATE column named O_ORDERDATE:\n//IMG//\n\nWhat does the total_constant_partition_count value indicate about this table?","topic":"1","timestamp":"2023-10-30 13:22:00","choices":{"A":"The table is clustered very well on O_ORDERDATE, as there are 493 micro-partitions that could not be significantly improved by reclustering.","D":"The data in O_ORDERDATE has a very low cardinality, as there are 493 micro-partitions where there is only a single distinct value in that column for all rows in the micro-partition.","B":"The table is not clustered well on O_ORDERDATE, as there are 493 micro-partitions where the range of values in that column overlap with every other micro-partition in the table.","C":"The data in O_ORDERDATE does not change very often, as there are 493 micro-partitions containing rows where that column has not been modified since the row was created."},"answer_description":"","question_id":7},{"id":"UPXk1TgQLbBtZFofqCp9","question_images":[],"answer_images":[],"question_text":"A company is building a dashboard for thousands of Analysts. The dashboard presents the results of a few summary queries on tables that are regularly updated. The query conditions vary by topic according to what data each Analyst needs. Responsiveness of the dashboard queries is a top priority, and the data cache should be preserved.\nHow should the Data Engineer configure the compute resources to support this dashboard?","answers_community":["B (100%)"],"exam_id":166,"isMC":true,"url":"https://www.examtopics.com/discussions/snowflake/view/124949-exam-snowpro-advanced-data-engineer-topic-1-question-16/","answer_ET":"B","discussion":[{"upvote_count":"4","comment_id":"1057666","timestamp":"1730292060.0","content":"Selected Answer: B\nThis mode is enabled by specifying the same value for both maximum and minimum number of clusters (note that the specified value must be larger than 1). In this mode, when the warehouse is started, Snowflake starts all the clusters so that maximum resources are available while the warehouse is running.\n\nThis mode is effective for statically controlling the available compute resources, particularly if you have large numbers of concurrent user sessions and/or queries and the numbers do not fluctuate significantly.\n\nThe key term here is : \"The top priority is the responsiveness\". \n\nA is also valid but it will have impact on the responsiveness due to economy option. The queries will queue up before scale-up happens.","poster":"stopthisnow"}],"answer":"B","topic":"1","unix_timestamp":1698669660,"question_id":8,"timestamp":"2023-10-30 13:41:00","choices":{"A":"Assign queries to a multi-cluster virtual warehouse with economy auto-scaling. Allow the system to automatically start and stop clusters according to demand.","D":"Create a size XL virtual warehouse to support all the dashboard queries. Monitor query runtimes to determine whether the virtual warehouse should be resized.","B":"Assign all queries to a multi-cluster virtual warehouse set to maximized mode. Monitor to determine the smallest suitable number of clusters.","C":"Create a virtual warehouse for every 250 Analysts. Monitor to determine how many of these virtual warehouses are being utilized at capacity."},"answer_description":""},{"id":"WkwK512fd8WO1oqKozhN","answer_ET":"D","exam_id":166,"answer":"D","answer_description":"","isMC":true,"choices":{"C":"14 days","B":"24 hours","A":"12 hours","D":"31 days"},"question_id":9,"question_images":[],"unix_timestamp":1698324420,"answer_images":[],"topic":"1","question_text":"A Data Engineer has developed a dashboard that will issue the same SQL select clause to Snowflake every 12 hours.\nHow long will Snowflake use the persisted query results from the result cache, provided that the underlying data has not changed?","timestamp":"2023-10-26 14:47:00","url":"https://www.examtopics.com/discussions/snowflake/view/124663-exam-snowpro-advanced-data-engineer-topic-1-question-17/","answers_community":["D (100%)"],"discussion":[{"content":"Selected Answer: D\nEach time the persisted result for a query is reused, Snowflake resets the 24-hour retention period for the result, up to a maximum of 31 days from the date and time that the query was first executed. After 31 days, the result is purged and the next time the query is submitted, a new result is generated and persisted.\nhttps://docs.snowflake.com/en/user-guide/querying-persisted-results","comment_id":"1057695","timestamp":"1730293080.0","poster":"stopthisnow","upvote_count":"7"},{"content":"Selected Answer: D\nEach time the persisted result for a query is reused, Snowflake resets the 24-hour retention period for the result, up to a maximum of 31 days","timestamp":"1732813320.0","poster":"Eshkin_Kot","comment_id":"1082760","upvote_count":"2"},{"timestamp":"1729946820.0","poster":"Gemnidhi17","upvote_count":"4","content":"correct ans is D 31 days because it can extends upto 31 days if executed again within 24 hours.","comment_id":"1054566"}]},{"id":"JrRRQs7Wk5f0b1NHd68t","question_images":[],"answer_description":"","choices":{"A":"Call the system function SYSTEM$ABORT_TRANSACTION.","B":"Call the system function SYSTEM$CANCEL_TRANSACTION.","C":"Set the LOCK_TIMEOUT to FALSE in the stored procedure.","D":"Set the TRANSACTION_ABORT_ON_ERROR to TRUE in the stored procedure."},"url":"https://www.examtopics.com/discussions/snowflake/view/124971-exam-snowpro-advanced-data-engineer-topic-1-question-18/","question_id":10,"isMC":true,"answers_community":["A (100%)"],"answer":"A","answer_images":[],"exam_id":166,"discussion":[{"comment_id":"1058021","poster":"stopthisnow","content":"Selected Answer: A\nAborting Transactions\nIf a transaction is running in a session and the session disconnects abruptly, preventing the transaction from committing or rolling back, the transaction is left in a detached state, including any locks that the transaction is holding on resources. If this happens, you might need to abort the transaction.\n\nTo abort a running transaction, the user who started the transaction or an account administrator can call the system function, SYSTEM$ABORT_TRANSACTION.\n\nIf the transaction is not aborted by the user:\n\nIf it blocks another transaction from acquiring a lock on the same table and is idle for 5 minutes, it is automatically aborted and rolled back.\n\nIf it does not block other transactions from modifying the same table and is older than 4 hours, it is automatically aborted and rolled back.","timestamp":"1730305020.0","upvote_count":"4"}],"question_text":"A Data Engineer ran a stored procedure containing various transactions. During the execution, the session abruptly disconnected, preventing one transaction from committing or rolling back. The transaction was left in a detached state and created a lock on resources.\nWhat step must the Engineer take to immediately run a new transaction?","unix_timestamp":1698682620,"topic":"1","answer_ET":"A","timestamp":"2023-10-30 17:17:00"}],"exam":{"isMCOnly":true,"name":"SnowPro Advanced Data Engineer","isImplemented":true,"numberOfQuestions":65,"id":166,"isBeta":false,"lastUpdated":"12 Apr 2025","provider":"Snowflake"},"currentPage":2},"__N_SSP":true}