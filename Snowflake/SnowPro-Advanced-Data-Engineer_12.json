{"pageProps":{"questions":[{"id":"G9pKmWPqeQMb1xWII78p","isMC":true,"choices":{"D":"Add a date column as a cluster key on the table.","A":"Increase the size of the virtual warehouse.","B":"Increase the number of clusters in the virtual warehouse.","C":"Implement the search optimization service on the table."},"answer_images":[],"url":"https://www.examtopics.com/discussions/snowflake/view/124180-exam-snowpro-advanced-data-engineer-topic-1-question-6/","question_images":["https://img.examtopics.com/snowpro-advanced-data-engineer/image3.png"],"answers_community":["D (100%)"],"question_id":56,"question_text":"A large table with 200 columns contains two years of historical data. When queried, the table is filtered on a single day. Below is the Query Profile:\n//IMG//\n\nUsing a size 2XL virtual warehouse, this query took over an hour to complete.\nWhat will improve the query performance the MOST?","exam_id":166,"timestamp":"2023-10-21 12:50:00","answer_description":"","answer_ET":"D","answer":"D","unix_timestamp":1697885400,"topic":"1","discussion":[{"poster":"aba2s","timestamp":"1725436920.0","comment_id":"1278093","content":"Selected Answer: D\nHere since, only data on a single date is needed, we don't need to scan all the table nor all micro-partitions. So adding a date column as a cluster key sound right.","upvote_count":"1"},{"poster":"rbeam","comment_id":"1182297","content":"Selected Answer: D\nalthough there are bytes spilling, but compare with the size, it is not much, so option B may improve, but not much. considering filter is on a single field, option D make sense.","upvote_count":"2","timestamp":"1711353600.0"},{"content":"are these 71 questions enough to prepare for advanced DE exam ?","timestamp":"1704196140.0","poster":"prshntdxt7","upvote_count":"2","comment_id":"1111824"},{"upvote_count":"2","comment_id":"1057556","timestamp":"1698662220.0","content":"Selected Answer: D\nD makes sense","poster":"stopthisnow"},{"content":"Selected Answer: D\nD \nClustering key will not scan the full table which is what is required here.","timestamp":"1697885400.0","upvote_count":"2","poster":"stopthisnow","comment_id":"1049408"}]},{"id":"iqrCvftXj88gQmYNRBMJ","topic":"1","discussion":[{"comment_id":"1411941","timestamp":"1743316500.0","content":"Selected Answer: ACD\nAs all the three uses File format object which define the formats or file structure for CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM","poster":"DIPARJ","upvote_count":"1"},{"poster":"BeingAbhi","content":"It should be FILE FORMAT object, PIPE object and STAGE object.","timestamp":"1729858980.0","upvote_count":"1","comment_id":"1201969"},{"poster":"Snow_P","upvote_count":"1","content":"and stage object","timestamp":"1716120420.0","comment_id":"1074652"},{"poster":"Snow_P","upvote_count":"2","content":"Copy into and File format but not sure which is the 3rd option","comment_id":"1074083","timestamp":"1716036660.0"}],"answers_community":["ACD (100%)"],"answer":"ACD","question_images":[],"isMC":true,"answer_images":[],"choices":{"E":"STAGE object","A":"COPY command","C":"FILE FORMAT object","F":"INSERT command","D":"PIPE object","B":"MERGE command"},"exam_id":166,"timestamp":"2023-11-18 15:51:00","url":"https://www.examtopics.com/discussions/snowflake/view/126526-exam-snowpro-advanced-data-engineer-topic-1-question-60/","question_id":57,"question_text":"A Data Engineer would like to define a file structure for loading and unloading data.\nWhere can the file structure be defined? (Choose three.)","unix_timestamp":1700319060,"answer_ET":"ACE","answer_description":""},{"id":"AUTqo8mjisFZ8mlR3EQq","answer":"D","answer_ET":"D","answers_community":["D (100%)"],"discussion":[{"content":"Selected Answer: D\nA. Creating a DataFrame from a table will start a virtual warehouse.\n\nIncorrect. Creating a DataFrame from a table does not necessarily start a virtual warehouse. It's a common operation that doesn't trigger the initiation of a virtual warehouse.\nB. Creating a DataFrame from a staged file with the read() method will start a virtual warehouse.\n\nIncorrect. Similar to option A, creating a DataFrame from a staged file using the read() method does not trigger the initiation of a virtual warehouse.\nC. Transforming a DataFrame with methods like replace() will start a virtual warehouse.\n\nIncorrect. DataFrame transformations, such as using the replace() method, do not start a virtual warehouse. These operations are performed locally on the DataFrame.\nD. Calling a Snowpark stored procedure to query the database with session.call() will start a virtual warehouse.","upvote_count":"2","comment_id":"1121476","timestamp":"1720857120.0","poster":"prshntdxt7"},{"content":"Selected Answer: D\nABC are all transformations so D","timestamp":"1716120540.0","comment_id":"1074654","upvote_count":"2","poster":"Snow_P"},{"timestamp":"1716036960.0","comment_id":"1074086","content":"Selected Answer: D\nI think D","upvote_count":"1","poster":"Snow_P"}],"answer_description":"","topic":"1","exam_id":166,"question_images":[],"question_text":"Assuming that the session parameter USE_CACHED_RESULT is set to false, what are characteristics of Snowflake virtual warehouses in terms of the use of Snowpark?","choices":{"A":"Creating a DataFrame from a table will start a virtual warehouse.","B":"Creating a DataFrame from a staged file with the read() method will start a virtual warehouse.","D":"Calling a Snowpark stored procedure to query the database with session.call() will start a virtual warehouse.","C":"Transforming a DataFrame with methods like replace() will start a virtual warehouse."},"answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/snowflake/view/126528-exam-snowpro-advanced-data-engineer-topic-1-question-61/","timestamp":"2023-11-18 15:56:00","unix_timestamp":1700319360,"question_id":58},{"id":"3ocaMvaTQw4GkiKxj8jB","answers_community":["D (100%)"],"timestamp":"2023-11-18 16:03:00","topic":"1","answer_ET":"D","isMC":true,"choices":{"D":"Create a Snowflake Support case to restore the database and table from Fail-safe.","A":"undrop database xyz;","B":"create table abc_restore as select * from xyz.public.abc at (offset => -60*60*24*8);","C":"create table abc_restore clone xyz.public.abc at (offset => -3600*24*8);"},"url":"https://www.examtopics.com/discussions/snowflake/view/126529-exam-snowpro-advanced-data-engineer-topic-1-question-62/","discussion":[{"content":"Selected Answer: D\nI think D, the retention period for the table although longer should not be honored when dropping the DB, the table should have been dropped first","comment_id":"1074089","upvote_count":"5","poster":"Snow_P","timestamp":"1716037380.0"},{"comment_id":"1121473","timestamp":"1720856820.0","content":"Selected Answer: D\nA.Snowflake does not provide a direct undrop command for databases or tables.\n\nB.The at (offset => -60*60*24*8) syntax is not a valid way to recover dropped tables in Snowflake. \nThe at clause is used for temporal queries, and it doesn't have the capability to recover dropped tables.\n\nC.However, it does not have the ability to recover dropped tables. \nThe at (offset => -3600*24*8) part is not a valid syntax for recovering dropped tables.\n\nD. Create a Snowflake Support case to restore the database and table from Fail-safe.\nThis is the correct option. Snowflake's Fail-safe feature is designed for recovering dropped tables\nor databases within a specified retention period.\nBy creating a support case, you can request assistance from Snowflake support to restore the \ndropped database and table from Fail-safe, ensuring data recovery within the allowed retention time.","poster":"prshntdxt7","upvote_count":"1"},{"poster":"Maicas","timestamp":"1720025340.0","content":"Selected Answer: D\nJust like Snow_P said, D is the correct answer.\n\nhttps://docs.snowflake.com/en/user-guide/data-time-travel#dropped-containers-and-object-retention-inheritance","comment_id":"1113037","upvote_count":"1"}],"answer_description":"","unix_timestamp":1700319780,"answer_images":[],"answer":"D","question_text":"Database XYZ has the data_retention_time_in_days parameter set to 7 days and table XYZ.public.ABC has the data_retention_time_in_days set to 10 days.\nA Developer accidentally dropped the database containing this single table 8 days ago and just discovered the mistake.\nHow can the table be recovered?","question_id":59,"question_images":[],"exam_id":166},{"id":"13qfcgG1p0w10zkiSQr8","topic":"1","answers_community":["BC (86%)","14%"],"discussion":[{"poster":"Sibaprasad","timestamp":"1728128460.0","comment_id":"1293460","upvote_count":"1","content":"Correct Answer is AC.\n\nCheck https://docs.snowflake.com/en/sql-reference/data-sharing-usage/listing-events-daily and https://docs.snowflake.com/en/sql-reference/data-sharing-usage/listing-telemetry-daily\n\nTelemetry Daily EVENT_TYPE has LISTING CLICK\nEvents Daily EVENT_TYPE has REQUEST"},{"upvote_count":"1","poster":"BigDataBB","timestamp":"1705937400.0","content":"Selected Answer: B\nIn LISTING_CONSUMPTION_DAILY there is the column SNOWFLAKE_REGION, so this is the necessart view: https://docs.snowflake.com/en/sql-reference/data-sharing-usage/listing-consumption-daily","comment_id":"1128832"},{"upvote_count":"1","poster":"prshntdxt7","content":"Selected Answer: BC\nThe minimal data sources needed for analyzing consumer requests by region for each Data Exchange offering annually, as well as click-through rates for each listing, would likely include:\n\nB. SNOWFLAKE.DATA_SHARING_USAGE.LISTING_CONSUMPTION_DAILY\nC. SNOWFLAKE.DATA_SHARING_USAGE.LISTING_TELEMETRY_DAILY\n\nThese two tables provide information related to listing consumption and telemetry, which are crucial for understanding consumer requests and click-through rates for each listing. The other options (A and D) seem less relevant to the specific requirements mentioned.","timestamp":"1705138680.0","comment_id":"1121458"},{"content":"Selected Answer: BC\nA looks wrong. It gives info about installations and permission, not consumption.\nhttps://docs.snowflake.com/en/sql-reference/data-sharing-usage/listing-events-daily","timestamp":"1700427600.0","comment_id":"1074923","poster":"stopthisnow","upvote_count":"2"},{"timestamp":"1700320200.0","comment_id":"1074093","content":"Selected Answer: BC\nFor sure C but I am wondering between A & B. Overall I think B&C","poster":"Snow_P","upvote_count":"3"}],"answer":"BC","question_images":[],"isMC":true,"answer_images":[],"exam_id":166,"timestamp":"2023-11-18 16:10:00","choices":{"A":"SNOWFLAKE.DATA_SHARING_USAGE.LISTING_EVENTS_DAILY","B":"SNOWFLAKE.DATA_SHARING_USAGE.LISTING_CONSUMPTION_DAILY","C":"SNOWFLAKE.DATA_SHARING_USAGE.LISTING_TELEMETRY_DAILY","D":"SNOWFLAKE.ACCOUNT_USAGE.DATA_TRANSFER_HISTORY"},"url":"https://www.examtopics.com/discussions/snowflake/view/126530-exam-snowpro-advanced-data-engineer-topic-1-question-63/","question_id":60,"question_text":"A Data Engineer is building a set of reporting tables to analyze consumer requests by region for each of the Data Exchange offerings annually, as well as click-through rates for each listing.\nWhich views are needed MINIMALLY as data sources?","unix_timestamp":1700320200,"answer_ET":"BC","answer_description":""}],"exam":{"numberOfQuestions":65,"name":"SnowPro Advanced Data Engineer","isImplemented":true,"id":166,"lastUpdated":"12 Apr 2025","provider":"Snowflake","isBeta":false,"isMCOnly":true},"currentPage":12},"__N_SSP":true}