{"pageProps":{"questions":[{"id":"QweHLucs5HYrdI9BzMNJ","topic":"1","isMC":true,"answer_description":"","question_text":"A table is loaded using Snowpipe and truncated afterwards. Later, a Data Engineer finds that the table needs to be reloaded, but the metadata of the pipe will not allow the same files to be loaded again.\nHow can this issue be solved using the LEAST amount of operational overhead?","answers_community":["D (71%)","C (29%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/snowflake/view/124975-exam-snowpro-advanced-data-engineer-topic-1-question-23/","timestamp":"2023-10-30 20:39:00","exam_id":166,"discussion":[{"content":"Selected Answer: D\nFiles modified and staged again within 14 days :\nSnowpipe ignores modified files that are staged again. To reload modified data files, it is currently necessary to recreate the pipe object using the CREATE OR REPLACE PIPE syntax.","timestamp":"1721138940.0","comment_id":"1248978","upvote_count":"1","poster":"MultiCloudGuru"},{"timestamp":"1710036060.0","comment_id":"1169992","content":"https://docs.snowflake.com/en/sql-reference/sql/create-pipe#usage-notes\n\nFORCE is not a valid option for Snowpipe:\n\n-----All COPY INTO <table> copy options are supported except for the following:\n\nFILES = ( 'file_name1' [ , 'file_name2', ... ] )\nON_ERROR = ABORT_STATEMENT\nSIZE_LIMIT = num\nPURGE = TRUE | FALSE (i.e. automatic purging while loading)\nFORCE = TRUE | FALSE\nRETURN_FAILED_ONLY = TRUE | FALSE\nVALIDATION_MODE = RETURN_n_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS","poster":"claudiamilena11","upvote_count":"1"},{"content":"I think is D.\nIf you modify the COPY INTO statement you need to recreate the pipe. Then you need to (1) modify the COPY INTO and (2)recreate the pipe. 2 Steps.\nIf you recreate the pipe directly you will reload again the files. 1 Step.","timestamp":"1709376120.0","poster":"ivanjs44","comment_id":"1164060","upvote_count":"1"},{"content":"Selected Answer: C\nC. Set the FORCE=TRUE option in the Snowpipe COPY INTO command.\n\nUsing the FORCE=TRUE option in the Snowpipe COPY INTO command allows reloading files even if they have been loaded previously. This can be useful in situations where you need to reload data, and the metadata of the pipe is preventing the same files from being loaded again. It minimizes operational overhead by avoiding the need to wait for metadata expiration or modifying the files.","comment_id":"1121994","upvote_count":"1","timestamp":"1705174440.0","poster":"prshntdxt7"},{"timestamp":"1705093440.0","upvote_count":"1","content":"Selected Answer: C\nAccording to https://docs.snowflake.com/en/sql-reference/sql/copy-into-table FORCE = TRUE would \"load all files, regardless of whether they’ve been loaded previously and have not changed since they were loaded\". The question asked for the least impact. C is the least impact","poster":"5effea7","comment_id":"1121119"},{"content":"Selected Answer: D\nSnowpipe ignores modified files that are staged again. To reload modified data files, it is currently necessary to recreate the pipe object using the CREATE OR REPLACE PIPE syntax.\nhttps://docs.snowflake.com/en/user-guide/data-load-snowpipe-ts","timestamp":"1698694740.0","poster":"stopthisnow","comment_id":"1058224","upvote_count":"4"}],"answer":"D","answer_images":[],"unix_timestamp":1698694740,"choices":{"A":"Wait until the metadata expires and then reload the file using Snowpipe.","D":"Recreate the pipe by using the CREATE OR REPLACE PIPE command.","C":"Set the FORCE=TRUE option in the Snowpipe COPY INTO command.","B":"Modify the file by adding a blank row to the bottom and re-stage the file."},"answer_ET":"D","question_id":16},{"id":"CBVHdDQUtnkqnI0D69AN","answer_description":"","isMC":true,"unix_timestamp":1697996280,"topic":"1","answers_community":["A (100%)"],"question_id":17,"timestamp":"2023-10-22 19:38:00","answer_ET":"A","answer_images":[],"exam_id":166,"url":"https://www.examtopics.com/discussions/snowflake/view/124372-exam-snowpro-advanced-data-engineer-topic-1-question-24/","answer":"A","discussion":[{"timestamp":"1697996280.0","poster":"drunk_goat82","upvote_count":"8","content":"I think it's A.\nRenaming a source object does not break a stream or cause it to go stale. In addition, if a source object is dropped and a new object is created with the same name, any streams linked to the original object are not linked to the new object","comment_id":"1050946"},{"upvote_count":"1","poster":"MultiCloudGuru","comment_id":"1248983","content":"Selected Answer: A\nImportant\n\nRecreating an object (using the CREATE OR REPLACE TABLE syntax) drops its history, which also makes any stream on the table or view stale. In addition, recreating or dropping any of the underlying tables for a view makes any stream on the view stale.\n\nCurrently, when a database or schema that contains a stream and its source table (or the underlying tables for a source view) is cloned, any unconsumed records in the stream clone are inaccessible. This behavior is consistent with Time Travel for tables. If a table is cloned, historical data for the table clone begins at the time/point when the clone was created.\n\nRenaming a source object does not break a stream or cause it to go stale. In addition, if a source object is dropped and a new object is created with the same name, any streams linked to the original object are not linked to the new object.","timestamp":"1721139240.0"},{"comment_id":"1059186","timestamp":"1698784620.0","upvote_count":"1","poster":"randreag","content":"but will the stream work as expected if there is no more changes in the table(old) to record? After a while it will become stale, doesn't it?"},{"poster":"stopthisnow","timestamp":"1698695340.0","comment_id":"1058229","content":"Selected Answer: A\nRenaming a source object does not break a stream or cause it to go stale. In addition, if a source object is dropped and a new object is created with the same name, any streams linked to the original object are not linked to the new object.\nhttps://docs.snowflake.com/en/user-guide/streams-intro","upvote_count":"3"},{"poster":"acapone001","comment_id":"1053148","upvote_count":"3","timestamp":"1698178980.0","content":"A should be the correct answer here. From Snowflake: \"Renaming a source object does not break a stream or cause it to go stale. In addition, if a source object is dropped and a new object is created with the same name, any streams linked to the original object are not linked to the new object.\""}],"question_text":"A stream called TRANSACTIONS_STM is created on top of a TRANSACTIONS table in a continuous pipeline running in Snowflake. After a couple of months, the TRANSACTIONS table is renamed TRANSACTIONS_RAW to comply with new naming standards.\nWhat will happen to the TRANSACTIONS_STM object?","choices":{"C":"TRANSACTIONS_STM will be automatically renamed TRANSACTIONS_RAW_STM.","A":"TRANSACTIONS_STM will keep working as expected.","B":"TRANSACTIONS_STM will be stale and will need to be re-created.","D":"Reading from the TRANSACTIONS_STM stream will succeed for some time after the expected STALE_TIME."},"question_images":[]},{"id":"XLPKllEnrXx3xB1iWZDj","unix_timestamp":1698695400,"exam_id":166,"choices":{"B":"Use a multi-cluster virtual warehouse with the scaling policy set to standard","E":"Increase the MAX_CLUSTER_COUNT","C":"Move the query to a larger virtual warehouse","A":"Add a LIMIT to the ORDER BY if possible","D":"Create indexes to ensure sorted access to data"},"url":"https://www.examtopics.com/discussions/snowflake/view/124977-exam-snowpro-advanced-data-engineer-topic-1-question-25/","timestamp":"2023-10-30 20:50:00","question_text":"A Data Engineer is evaluating the performance of a query in a development environment.\n//IMG//\n\n//IMG//\n\nBased on the Query Profile, what are some performance tuning options the Engineer can use? (Choose two.)","question_id":18,"discussion":[{"upvote_count":"1","comment_id":"1099582","content":"Snowflake don't support the create of index, You can cluster a table by a cluster key, but this option is not in the list of option. in addition to this in the question is write that You are in DEV environment. So You don't need to have all the data but to know how SF process them.","poster":"BigDataBB","timestamp":"1718697120.0"},{"comments":[{"timestamp":"1724394420.0","upvote_count":"2","comment_id":"1157011","poster":"mns0173","content":"there are no indexes in snowflake. And also based on context looks like dev just run this query in some kind of query editor and in such case obviously do not need to return gigabytes of data."}],"poster":"Eshkin_Kot","content":"Why not D?\nIt doesn't say we don't need all the data, so why LIMIT (option A)?\nAgree with C","comment_id":"1082772","timestamp":"1716909600.0","upvote_count":"3"},{"poster":"stopthisnow","comment_id":"1058231","content":"Selected Answer: AC\nAC makes sense","timestamp":"1714499400.0","upvote_count":"1"}],"answers_community":["AC (100%)"],"answer_description":"","topic":"1","question_images":["https://img.examtopics.com/snowpro-advanced-data-engineer/image15.png","https://img.examtopics.com/snowpro-advanced-data-engineer/image16.png"],"isMC":true,"answer_ET":"AC","answer":"AC","answer_images":[]},{"id":"avFmJkZpYv6mkXu9pPE6","discussion":[{"content":"Selected Answer: BE\ncollect and show are actions, for the rest Spark is just writing down what it needs to do when an action occurs","upvote_count":"1","timestamp":"1731939000.0","poster":"Snow_P","comment_id":"1074044"},{"poster":"stopthisnow","upvote_count":"3","timestamp":"1730317920.0","comment_id":"1058232","content":"Selected Answer: BE\nhttps://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#performing-an-action-to-evaluate-a-dataframe\nAs mentioned earlier, the DataFrame is lazily evaluated, which means the SQL statement isn’t sent to the server for execution until you perform an action. An action causes the DataFrame to be evaluated and sends the corresponding SQL statement to the server for execution."}],"isMC":true,"answer_description":"","unix_timestamp":1698695520,"question_text":"Which methods will trigger an action that will evaluate a DataFrame? (Choose two.)","exam_id":166,"choices":{"C":"DataFrame.select()","D":"DataFrame.col()","E":"DataFrame.show()","A":"DataFrame.random_split()","B":"DataFrame.collect()"},"answer_images":[],"timestamp":"2023-10-30 20:52:00","url":"https://www.examtopics.com/discussions/snowflake/view/124978-exam-snowpro-advanced-data-engineer-topic-1-question-26/","answer_ET":"BE","answers_community":["BE (100%)"],"question_id":19,"topic":"1","answer":"BE","question_images":[]},{"id":"lhwWPk0wOMOMbd7QT4Vk","answer_images":[],"unix_timestamp":1698444060,"question_images":[],"url":"https://www.examtopics.com/discussions/snowflake/view/124762-exam-snowpro-advanced-data-engineer-topic-1-question-27/","answers_community":["ADE (71%)","14%","14%"],"topic":"1","answer_description":"","timestamp":"2023-10-28 00:01:00","exam_id":166,"isMC":true,"question_id":20,"answer_ET":"ADE","choices":{"F":"Storage integration","D":"Internal table stage","B":"Serverless task","C":"Internal user stage","E":"Internal named stage","A":"Pipe"},"discussion":[{"poster":"MultiCloudGuru","comment_id":"1249000","timestamp":"1721140620.0","upvote_count":"1","content":"Selected Answer: ADE\nThe connector creates the following objects for each topic:\nOne internal stage to temporarily store data files for each topic.\nOne pipe to ingest the data files for each topic partition.\nOne table for each topic."},{"timestamp":"1707084240.0","content":"Selected Answer: ABD\nhttps://docs.snowflake.com/en/user-guide/kafka-connector-overview\nOne table for each topic.\nOne internal stage to temporarily store data files for each topic","upvote_count":"1","comment_id":"1140561","poster":"BigDataBB"},{"comments":[],"upvote_count":"1","comment_id":"1137629","poster":"BigDataBB","content":"Selected Answer: ABE\nI finally find the solution, a pipe use a \"Snowflake-provided virtual warehouse\" so a serverless task.\nFor this reason the answer are A, B, E","timestamp":"1706792340.0"},{"content":"I think that there is an error, kafka connector uses these snowflake objects:\n- pipe \n- internal named stage\n- tables\n\nAnd not internal table stage","comment_id":"1099620","comments":[{"timestamp":"1702895640.0","comment_id":"1099621","upvote_count":"1","poster":"BigDataBB","content":"https://docs.snowflake.com/en/user-guide/kafka-connector-manage"}],"poster":"BigDataBB","timestamp":"1702895580.0","upvote_count":"1"},{"timestamp":"1700316720.0","poster":"Snow_P","comment_id":"1074049","upvote_count":"2","content":"Selected Answer: ADE\npipe and internal stages"},{"upvote_count":"2","content":"Selected Answer: ADE\nThe connector creates the following objects for each topic:\n\nOne internal stage to temporarily store data files for each topic.\n\nOne pipe to ingest the data files for each topic partition.\n\nOne table for each topic.\nhttps://docs.snowflake.com/en/user-guide/kafka-connector-overview","poster":"stopthisnow","timestamp":"1698696540.0","comment_id":"1058240"},{"poster":"randreag","timestamp":"1698444060.0","upvote_count":"1","comment_id":"1055863","content":"Is this answer correct?"}],"answer":"ADE","question_text":"Which Snowflake objects does the Snowflake Kafka connector use? (Choose three.)"}],"exam":{"id":166,"isBeta":false,"provider":"Snowflake","lastUpdated":"12 Apr 2025","isMCOnly":true,"name":"SnowPro Advanced Data Engineer","isImplemented":true,"numberOfQuestions":65},"currentPage":4},"__N_SSP":true}