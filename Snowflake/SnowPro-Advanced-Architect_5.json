{"pageProps":{"questions":[{"id":"l04c1RHNecXcOTbOJacK","question_text":"The following DDL command was used to create a task based on a stream:\n//IMG//\n\nAssuming MY_WH is set to auto_suspend – 60 and used exclusively for this task, which statement is true?","unix_timestamp":1682254020,"question_images":["https://img.examtopics.com/snowpro-advanced-architect/image2.png"],"answer":"B","exam_id":165,"choices":{"A":"The warehouse MY_WH will be made active every five minutes to check the stream.","C":"The warehouse MY_WH will never suspend.","B":"The warehouse MY_WH will only be active when there are results in the stream.","D":"The warehouse MY_WH will automatically resize to accommodate the size of the stream."},"timestamp":"2023-04-23 14:47:00","discussion":[{"content":"Selected Answer: B\nShould be B","timestamp":"1714211100.0","upvote_count":"6","comment_id":"882477","poster":"Jay_98_11"},{"timestamp":"1732502520.0","upvote_count":"1","poster":"jjordan","content":"Selected Answer: B\nB is correct.","comment_id":"1079690"},{"comment_id":"949681","timestamp":"1720780440.0","upvote_count":"4","content":"Selected Answer: B\nStream_has_data is a metadata operation, so you don't need a wh to call it.\nB is correct, the WH will start as soon as there are data in the STREAM","poster":"GLEM"},{"poster":"hillcat111","upvote_count":"2","timestamp":"1720178280.0","comment_id":"943643","content":"Answer is B and is validated"},{"upvote_count":"3","content":"Selected Answer: B\nthe warehouse will be triggered if condition STREAM_HAS_DATA meet","timestamp":"1716925980.0","poster":"victorleonis","comment_id":"908840"},{"poster":"taod","comment_id":"878357","content":"Not true, B would be correct. Warehouse is only started if condition is met:\nhttps://docs.snowflake.com/en/sql-reference/sql/create-task\nhttps://docs.snowflake.com/en/sql-reference/functions/system_stream_has_data","upvote_count":"3","timestamp":"1713876420.0"}],"url":"https://www.examtopics.com/discussions/snowflake/view/107183-exam-snowpro-advanced-architect-topic-1-question-19/","answer_ET":"B","question_id":21,"answer_images":[],"answer_description":"","isMC":true,"topic":"1","answers_community":["B (100%)"]},{"id":"3EwT1A9TIgwQP5TgLXxO","discussion":[{"upvote_count":"1","poster":"NachoPrendes","comment_id":"1245392","timestamp":"1720603740.0","content":"Selected Answer: CD\nhttps://docs.snowflake.com/en/user-guide/kafka-connector-overview#:~:text=Each%20Kafka%20message%20is%20passed%20to%20Snowflake%20in%20JSON%20format%20or%20Avro%20format"},{"content":"Ans: C,D\n\nExplanation:\nEach Kafka message is passed to Snowflake in JSON format or Avro format. The Kafka connector stores that formatted information in a single column of type VARIANT. The data is not parsed, and the data is not split into multiple columns in the Snowflake table.\n\nLink:\nhttps://docs.snowflake.com/en/user-guide/kafka-connector-overview","poster":"krishnak2244","timestamp":"1688398440.0","upvote_count":"2","comment_id":"941993"},{"poster":"Jay_98_11","content":"correct","timestamp":"1682183820.0","comment_id":"877476","upvote_count":"1"},{"timestamp":"1680614880.0","poster":"JRayan","content":"The Snowflake Connector for Kafka supports the JSON and Avro data formats for the messages. Therefore, options (C) and (D) are the correct answers. Snowflake does not support CSV, XML, or Parquet data formats for Kafka messages","comment_id":"861089","upvote_count":"4"}],"question_id":22,"exam_id":165,"isMC":true,"answer_description":"","question_text":"When using the Snowflake Connector for Kafka, what data formats are supported for the messages? (Choose two.)","question_images":[],"unix_timestamp":1680614880,"answers_community":["CD (100%)"],"answer_ET":"CD","timestamp":"2023-04-04 15:28:00","url":"https://www.examtopics.com/discussions/snowflake/view/105118-exam-snowpro-advanced-architect-topic-1-question-2/","choices":{"B":"XML","D":"JSON","E":"Parquet","C":"Avro","A":"CSV"},"answer_images":[],"topic":"1","answer":"CD"},{"id":"00lgOpPW7Jx6GxMZGK2k","isMC":true,"discussion":[{"upvote_count":"1","timestamp":"1742461620.0","poster":"DhirajG","comment_id":"1400968","content":"Selected Answer: D\nSnowpipe Streaming using Kafka load the data in real time which has latency lesser than snowpipe.\nCorrect Answer is D"},{"comment_id":"1366720","timestamp":"1741480800.0","content":"Selected Answer: C\nC is the answer","upvote_count":"1","poster":"ywan1600"},{"timestamp":"1737813720.0","upvote_count":"2","comment_id":"1346485","poster":"nareeshk","content":"Selected Answer: D\nHow is this Answer C? Kafka doesnt use a file format. It only uses internal stage, pipe & table"},{"poster":"hillcat111","comment_id":"943644","upvote_count":"3","content":"Answer is C and is validated","timestamp":"1720178280.0"},{"content":"I think C is the right answer","upvote_count":"2","timestamp":"1717927560.0","comment_id":"919163","poster":"hillcat111"}],"choices":{"C":"The Connector creates and manages its own stage, file format, and pipe objects.","B":"The Connector works with all file formats, including text, JSON, Avro, Ore, Parquet, and XML.","D":"Loads using the Connector will have lower latency than Snowpipe and will ingest data in real time.","A":"The Connector only works in Snowflake regions that use AWS infrastructure."},"answer_ET":"D","question_text":"What is a characteristic of loading data into Snowflake using the Snowflake Connector for Kafka?","unix_timestamp":1686305160,"topic":"1","question_id":23,"answer":"D","exam_id":165,"answer_description":"","url":"https://www.examtopics.com/discussions/snowflake/view/111654-exam-snowpro-advanced-architect-topic-1-question-20/","question_images":[],"timestamp":"2023-06-09 12:06:00","answers_community":["D (75%)","C (25%)"],"answer_images":[]},{"id":"BOvExywUC19L2YBMDJmo","isMC":true,"choices":{"C":"Contact Snowflake and they will execute the share request for the healthcare company.","D":"Set the share_restriction parameter on the shared object to false.","B":"By default, sharing is supported from a Business Critical Snowflake edition to a Standard edition.","A":"The healthcare company will need to change the institute’s Snowflake edition in the accounts panel."},"discussion":[{"upvote_count":"5","comment_id":"860562","poster":"serg_khar","timestamp":"1680575940.0","content":"Selected Answer: D\nhttps://docs.snowflake.com/en/user-guide/override_share_restrictions\nFor me it looks as D:\n\n-- grant the privilege to the SYSADMIN role\nuse role accountadmin;\ngrant override share restrictions on account to role sysadmin;\n\n-- SYSADMIN can now add a consumer account to a share with the SHARE_RESTRICTIONS parameter set to false\nuse role sysadmin;\nalter share <share_name> add accounts = <consumer_account_name> SHARE_RESTRICTIONS=false;"},{"poster":"Balakt","content":"Selected Answer: D\nB->Not by default\nC-> If it is not working then only one can contact\n D ->is correct.","timestamp":"1737131520.0","upvote_count":"1","comment_id":"1342248"},{"timestamp":"1719378540.0","comment_id":"1237200","content":"B. By default, sharing is supported from a Business Critical Snowflake edition to a Standard edition.\n\nSnowflake allows data sharing between different editions of accounts, including from higher editions (like Business Critical) to lower editions (like Standard). This means that the healthcare company can share data with the medical institute without needing to change the institute’s Snowflake edition or contact Snowflake for assistance.","upvote_count":"3","poster":"laksnarn"},{"poster":"MariusdeWith","timestamp":"1698772860.0","content":"It is D.\nhttps://docs.snowflake.com/en/user-guide/override_share_restrictions -> By default, Snowflake does not allow sharing data from a Business Critical to a non-Business Critical account.","comment_id":"1059057","upvote_count":"3"},{"upvote_count":"1","timestamp":"1698509940.0","poster":"PedroSDG","content":"Not D. SHARE_RESTRICTIONS is an account parameter, not an object","comment_id":"1056328"},{"upvote_count":"2","timestamp":"1698509820.0","comment_id":"1056326","content":"It is B. Sharing is SUPPORTED from business critical to any, but not ENABLED.","poster":"PedroSDG"},{"timestamp":"1688555880.0","poster":"hillcat111","comment_id":"943645","content":"Answer is D and is validated","upvote_count":"1"}],"answer_ET":"D","question_text":"A healthcare company wants to share data with a medical institute. The institute is running a Standard edition of Snowflake; the healthcare company is running a Business Critical edition.\nHow can this data be shared?","unix_timestamp":1680575940,"topic":"1","question_id":24,"exam_id":165,"answer":"D","answer_description":"","url":"https://www.examtopics.com/discussions/snowflake/view/105044-exam-snowpro-advanced-architect-topic-1-question-21/","question_images":[],"timestamp":"2023-04-04 04:39:00","answers_community":["D (100%)"],"answer_images":[]},{"id":"5SUACkUJHVEUD9JXkOjF","exam_id":165,"answer_images":[],"topic":"1","question_images":[],"isMC":true,"discussion":[{"content":"Selected Answer: A\nThe minimum value supported for the buffer.flush.time property is 1 (in seconds). For higher average data flow rates, we suggest that you decrease the default value for improved latency. If cost is a greater concern than latency, you could increase the buffer flush time. Be careful to flush the Kafka memory buffer before it becomes full to avoid out of memory exceptions.\n\nhttps://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-kafka","poster":"victorleonis","timestamp":"1716928860.0","upvote_count":"5","comment_id":"908861"},{"upvote_count":"2","timestamp":"1732844580.0","comment_id":"1083046","content":"Selected Answer: A\nhttps://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-kafka","poster":"jjordan"},{"upvote_count":"2","timestamp":"1720178340.0","content":"Answer is A and is validated","comment_id":"943646","poster":"hillcat111"},{"poster":"Alachramkowa","comment_id":"883271","content":"With lower buffer.count.records the buffer limits will be reached earlier, the file will be flushed and sent for ingestion through Snowpipe more frequently, thus increasing the cost","upvote_count":"3","timestamp":"1714285320.0"}],"question_text":"An Architect is designing a pipeline to stream event data into Snowflake using the Snowflake Kafka connector. The Architect’s highest priority is to configure the connector to stream data in the MOST cost-effective manner.\nWhich of the following is recommended for optimizing the cost associated with the Snowflake Kafka connector?","url":"https://www.examtopics.com/discussions/snowflake/view/107782-exam-snowpro-advanced-architect-topic-1-question-22/","answer_ET":"A","answer_description":"","answer":"A","question_id":25,"choices":{"D":"Utilize a lower Buffer.count.records in the connector configuration.","C":"Utilize a lower Buffer.size.bytes in the connector configuration.","A":"Utilize a higher Buffer.flush.time in the connector configuration.","B":"Utilize a higher Buffer.size.bytes in the connector configuration."},"timestamp":"2023-04-28 08:22:00","unix_timestamp":1682662920,"answers_community":["A (100%)"]}],"exam":{"provider":"Snowflake","isBeta":false,"numberOfQuestions":109,"isImplemented":true,"id":165,"name":"SnowPro Advanced Architect","isMCOnly":true,"lastUpdated":"12 Apr 2025"},"currentPage":5},"__N_SSP":true}