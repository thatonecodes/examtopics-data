{"pageProps":{"questions":[{"id":"I4bqmjIK5xtNXdgkvnOs","exam_id":165,"question_images":[],"unix_timestamp":1709833500,"answer_description":"","answer":"BD","question_id":81,"timestamp":"2024-03-07 18:45:00","answer_ET":"BD","discussion":[{"content":"Selected Answer: BD\nBD are correct","upvote_count":"1","comment_id":"1386971","timestamp":"1741637820.0","poster":"ywan1600"},{"timestamp":"1731347220.0","poster":"MSIDDIQUI18","content":"Correct Answer: BD \nDatabases, Table, Dynamic table, External table, Iceberg table, Secure Views, Secure Materialized views, Secure user define function (UDFs). All DB objects shared between account are READ ONLY. i.e the objects CANNOT be Modified, or Delete, included adding or Modifying table data.","upvote_count":"1","comment_id":"1310229"},{"poster":"swe12","upvote_count":"3","comment_id":"1168238","timestamp":"1709833500.0","content":"B D are right\nhttps://docs.snowflake.com/en/user-guide/data-sharing-intro"}],"answer_images":[],"choices":{"B":"Secure view","D":"External table","C":"Stored procedure","A":"Standard view","E":"Stream"},"answers_community":["BD (100%)"],"topic":"1","question_text":"Which Snowflake objects can be used in a data share? (Choose two.)","isMC":true,"url":"https://www.examtopics.com/discussions/snowflake/view/135444-exam-snowpro-advanced-architect-topic-1-question-73/"},{"id":"aDKDFfH9hwXoAzX9f0fy","timestamp":"2024-05-16 21:59:00","discussion":[{"content":"Selected Answer: B\nthis only looks most viable.","upvote_count":"2","poster":"Soumak","timestamp":"1737350280.0","comment_id":"1343345"},{"upvote_count":"1","poster":"67422cb","timestamp":"1731794340.0","comment_id":"1212575","content":"I wish these questions would be worded better. It sounds like you are supposed to create an external stage in Google Cloud Storage, not in Snowflake referencing Google Cloud Storage. But I assume that the latter is what they meant."}],"unix_timestamp":1715889540,"answer_ET":"B","question_id":82,"question_text":"A company has an external vendor who puts data into Google Cloud Storage. The company's Snowflake account is set up in Azure.\n\nWhat would be the MOST efficient way to load data from the vendor into Snowflake?","isMC":true,"exam_id":165,"url":"https://www.examtopics.com/discussions/snowflake/view/140765-exam-snowpro-advanced-architect-topic-1-question-74/","answer_images":[],"answer":"B","question_images":[],"answer_description":"","answers_community":["B (100%)"],"choices":{"D":"Create a Snowflake Account in the Google Cloud Platform (GCP), ingest data into this account and use data replication to move the data from GCP to Azure.","B":"Create an external stage on Google Cloud Storage and use the external table to load the data into Snowflake.","A":"Ask the vendor to create a Snowflake account, load the data into Snowflake and create a data share.","C":"Copy the data from Google Cloud Storage to Azure Blob storage using external tools and load data from Blob storage to Snowflake."},"topic":"1"},{"id":"7hF8tGq3MxD3hm7vlRqp","question_text":"How can the Snowpipe REST API be used to keep a log of data load history?","url":"https://www.examtopics.com/discussions/snowflake/view/135083-exam-snowpro-advanced-architect-topic-1-question-75/","exam_id":165,"answer":"C","answer_ET":"C","discussion":[{"poster":"MSIDDIQUI18","content":"Correct Answer: C\n\"loadHistoryScan\" Snowflake REST API endpoint is rate limited to avoid excessive calls. To help avoid exceeding the rate limit (error code 429), we recommend relying more heavily on insertReport than loadHistoryScan.","comment_id":"1310236","timestamp":"1731348120.0","upvote_count":"1"},{"poster":"akellaanurag","timestamp":"1729984260.0","upvote_count":"2","comment_id":"1303449","content":"Correct answer is C\nImportant\n\nThis endpoint is rate limited to avoid excessive calls. To help avoid exceeding the rate limit (error code 429), we recommend relying more heavily on insertReport than loadHistoryScan. When calling loadHistoryScan, specify the most narrow time range that includes a set of data loads. For example, reading the last 10 minutes of history every 8 minutes would work well. Trying to read the last 24 hours of history every minute will result in 429 errors indicating a rate limit has been reached. The rate limits are designed to allow each history record to be read a handful of times."},{"poster":"Acaer","timestamp":"1724252100.0","comment_id":"1270197","upvote_count":"3","content":"Selected Answer: C\nBy calling this command repeatedly, it is possible to see the full history of events on a pipe over time. Note that the command must be called often enough to not miss events.\n\nEvents are retained for a maximum of 10 minutes.\n\nhttps://docs.snowflake.com/en/user-guide/data-load-snowpipe-rest-apis#endpoint-insertreport"},{"upvote_count":"2","timestamp":"1710605940.0","content":"Answer should be C. \n\"..To help avoid exceeding the rate limit (error code 429), we recommend relying more heavily on insertReport than loadHistoryScan..\"\nhttps://docs.snowflake.com/en/user-guide/data-load-snowpipe-rest-apis","poster":"cui_li","comment_id":"1175090"},{"content":"C is correct - insertReport should be favored over loadHistory as excessive usage of the latter tends to lead to API throttling","upvote_count":"2","comment_id":"1164431","timestamp":"1709424120.0","poster":"Atomic_Gecko"}],"isMC":true,"timestamp":"2024-03-03 01:02:00","answer_images":[],"answer_description":"","answers_community":["C (100%)"],"choices":{"B":"Call loadHistoryScan every minute for the maximum time range.","C":"Call insertReport every 8 minutes for a 10-minute time range.","A":"Call insertReport every 20 minutes, fetching the last 10,000 entries.","D":"Call loadHistoryScan every 10 minutes for a 15-minute time range."},"topic":"1","question_id":83,"question_images":[],"unix_timestamp":1709424120},{"id":"zpd7QvrHpwErwK6zAQ83","question_images":["https://img.examtopics.com/snowpro-advanced-architect/image7.png"],"choices":{"B":"The pipe will no longer be able to receive the messages and the user must wait for 24 hours from the time when the SNS topic subscription was deleted. Pipe recreation is not required as the pipe will reuse the same subscription to the existing SNS topic after 24 hours.","D":"The pipe will no longer be able to receive the messages. To restore the system immediately, the user needs to manually create a new SNS topic with a different name and then recreate the pipe by specifying the new SNS topic name in the pipe definition.","A":"The pipe will continue to receive the messages as Snowflake will automatically restore the subscription to the same SNS topic and will recreate the pipe by specifying the same SNS topic name in the pipe definition.","C":"The pipe will continue to receive the messages as Snowflake will automatically restore the subscription by creating a new SNS topic. Snowflake will then recreate the pipe by specifying the new SNS topic name in the pipe definition."},"answer_description":"","unix_timestamp":1709835660,"answer_images":[],"question_text":"The diagram shows the process flow for Snowpipe auto-ingest with Amazon Simple Notification Service (SNS) with the following steps:\n\nStep 1: Data files are loaded in a stage.\n\nStep 2: An Amazon S3 event notification, published by SNS, informs Snowpipe — by way of Amazon Simple Queue Service (SQS) - that files are ready to load. Snowpipe copies the files into a queue.\n\nStep 3: A Snowflake-provided virtual warehouse loads data from the queued files into the target table based on parameters defined in the specified pipe.\n\nIf an AWS Administrator accidentally deletes the SQS subscription to the SNS topic in Step 2, what will happen to the pipe that references the topic to receive event messages from Amazon\nS3?\n\n//IMG//","topic":"1","discussion":[{"comment_id":"1387106","poster":"ywan1600","content":"Selected Answer: D\nD is correct\nhttps://docs.snowflake.com/en/user-guide/data-load-snowpipe-ts#snowpipe-stops-loading-files-after-amazon-sns-topic-subscription-is-deleted","timestamp":"1741638420.0","upvote_count":"1"},{"comment_id":"1168256","timestamp":"1725726060.0","upvote_count":"1","content":"D is right\nhttps://docs.snowflake.com/en/user-guide/data-load-snowpipe-ts","poster":"swe12"}],"timestamp":"2024-03-07 19:21:00","answers_community":["D (100%)"],"answer_ET":"D","isMC":true,"exam_id":165,"answer":"D","question_id":84,"url":"https://www.examtopics.com/discussions/snowflake/view/135446-exam-snowpro-advanced-architect-topic-1-question-76/"},{"id":"kEyCTci3JTAs9HI0CAjx","choices":{"B":"Configure the client application to call the Snowpipe REST endpoint when new files have arrived in Amazon S3 Glacier storage.","D":"Configure AWS Simple Notification Service (SNS) to notify Snowpipe when new files have arrived in Amazon S3 storage.","A":"Configure the client application to call the Snowpipe REST endpoint when new files have arrived in Amazon S3 storage.","C":"Create an AWS Lambda function to call the Snowpipe REST endpoint when new files have arrived in Amazon S3 storage.","E":"Configure the client application to issue a COPY INTO command to Snowflake when new files have arrived in Amazon S3 Glacier storage. Show Suggested Answer\nby MSIDDIQUI18 at July 3, 2024, 5:57 p.m.\nComments\n\nSwitch to a voting comment New\nSubmit\nukpino 3 months, 1 week ago\nSelected Answer: AD\nI think A and D os best. What is the advantage of setting up lambda instead of using SNS, which is recommended for Snowpipe?\nupvoted 2 times\nMSIDDIQUI18 9 months, 1 week ago\nA: This approach involves configuring a client application to detect new files in Amazon S3 and call the Snowpipe REST API to trigger the data loading process into Snowflake. This method provides flexibility and control over the ingestion process.\nC: Using AWS Lambda to automate the process of detecting new files in Amazon S3 and calling the Snowpipe REST API is an efficient and serverless solution. This approach leverages AWS Lambda's event-driven architecture to trigger Snowpipe ingestion automatically.\nupvoted 2 times\n\nPlatform\nHome\nAll Exams\nExamtopics PRO\nTraining Courses\nAccount\nLogin\nSign up\nReset Password\nCompany\nContact Us\nAbout Us\nTerms\nPrivacy Policy\nResources\nForum\nNews\nDMCA\n© 2024 ExamTopics"},"url":"https://www.examtopics.com/discussions/snowflake/view/143241-exam-snowpro-advanced-architect-topic-1-question-77/","answer_description":"","unix_timestamp":1720022220,"question_id":85,"question_images":[],"answer":"","answer_images":[],"timestamp":"2024-07-03 17:57:00","answer_ET":"","discussion":[{"timestamp":"1735474080.0","comment_id":"1333488","poster":"ukpino","upvote_count":"2","content":"Selected Answer: AD\nI think A and D os best. What is the advantage of setting up lambda instead of using SNS, which is recommended for Snowpipe?"},{"timestamp":"1720022220.0","poster":"MSIDDIQUI18","content":"A: This approach involves configuring a client application to detect new files in Amazon S3 and call the Snowpipe REST API to trigger the data loading process into Snowflake. This method provides flexibility and control over the ingestion process.\nC: Using AWS Lambda to automate the process of detecting new files in Amazon S3 and calling the Snowpipe REST API is an efficient and serverless solution. This approach leverages AWS Lambda's event-driven architecture to trigger Snowpipe ingestion automatically.","comment_id":"1241511","upvote_count":"2"}],"topic":"1","question_text":"An Architect needs to meet a company requirement to ingest files from the company’s AWS storage accounts into the company's Snowflake Google Cloud Platform (GCP) account.\n\nHow can the ingestion of these files into the company's Snowflake account be initiated? (Choose two.)","exam_id":165,"answers_community":[],"isMC":true}],"exam":{"numberOfQuestions":109,"provider":"Snowflake","lastUpdated":"12 Apr 2025","isMCOnly":true,"name":"SnowPro Advanced Architect","id":165,"isImplemented":true,"isBeta":false},"currentPage":17},"__N_SSP":true}