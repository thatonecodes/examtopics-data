{"pageProps":{"questions":[{"id":"QV5T8AuKx5vEYGHUl9ov","answer_description":"","timestamp":"2023-06-04 06:56:00","exam_id":214,"answer":"ABD","question_text":"Universal Containers has a legacy client server app that has a relational database that needs to be migrated to Salesforce.\nWhat are the three key actions that should be done when data modeling in Salesforce? (Choose three.)","answers_community":["ABD (100%)"],"unix_timestamp":1685854560,"url":"https://www.examtopics.com/discussions/salesforce/view/111055-exam-certified-data-architect-topic-1-question-21-discussion/","question_id":21,"isMC":true,"choices":{"A":"Work with legacy application owner to analyze the legacy data model.","D":"Identify data elements to be persisted in Salesforce.","B":"Map legacy data to Salesforce objects.","C":"Implement the legacy data model within Salesforce using custom fields.","E":"Map legacy data to custom metadata types."},"answer_ET":"ABD","question_images":[],"discussion":[{"content":"Selected Answer: ABD\nA, the legacy application owner would have the best insights about the legacy crm and would be able to determine data to keep or leaave behind.\nB, a relational database needs objects to build on\nNot C, the legacy database is relational, it needs objects. fields alone are not enough.\nD, an audit of which data needs to be kept (persisted) should be done so that only relevant data is retained.\nNot E, as metadata types cannot be used in regular reporting or accessible to non-admins for editing.","comment_id":"1008003","timestamp":"1710470220.0","poster":"ksho","upvote_count":"6"},{"timestamp":"1733465880.0","poster":"Maya_B","comment_id":"1225134","content":"Selected Answer: ABD\nSee ksho and Rangya comments","upvote_count":"1"},{"comment_id":"1222077","upvote_count":"1","poster":"Rangya","timestamp":"1732970760.0","content":"Selected Answer: ABD\nNo need to and not recommended to implement the legacy model as it is."},{"comment_id":"1199286","upvote_count":"2","timestamp":"1729444800.0","poster":"lizbette","content":"Selected Answer: ABD\nABD correct"},{"content":"Selected Answer: ABD\nCorrect","upvote_count":"1","comment_id":"1189719","poster":"gabrieleangelogabriele","timestamp":"1728113400.0"},{"upvote_count":"2","comment_id":"1109242","content":"Selected Answer: ABD\nNot C - Better to adapt to Salesforce's object-oriented model.\nNot E - Custom metadata types are typically used for storing configuration data.\n\nA - Understanding the existing data structure, relationships, and business rules is crucial.\nB - Not all data from the legacy system needs to be migrated.\nD - Data Model to sObjects.","poster":"ETH777","timestamp":"1719695460.0"},{"poster":"DavidHolland","content":"Selected Answer: ABD\nA,B and D are correct","comment_id":"1077356","timestamp":"1716372780.0","upvote_count":"2"},{"poster":"Oleg_M","content":"Selected Answer: ABD\nAgree, should be ABD","timestamp":"1708110840.0","comment_id":"982803","upvote_count":"3"},{"timestamp":"1703777220.0","upvote_count":"4","content":"Selected Answer: ABD\nC is wrong. You should copy the legacy data model, but adjust it to the Salesforce standards","comment_id":"936713","poster":"thneeb"},{"content":"I agree to Alokv, it should be ABD","timestamp":"1703772960.0","comment_id":"936631","upvote_count":"3","poster":"bangbang23"},{"upvote_count":"3","poster":"Alokv","comment_id":"914137","timestamp":"1701672960.0","content":"The three key actions that should be done when data modeling in Salesforce are:\n\nA. Work with legacy application owner to analyze the legacy data model.\nB. Map legacy data to Salesforce objects.\nD. Identify data elements to be persisted in Salesforce.\n\nHence A, B,D are correct options."}],"answer_images":[],"topic":"1"},{"id":"xUrXy9FNOt1UoiThY00I","answer_ET":"C","answer_description":"","answers_community":["C (100%)"],"timestamp":"2023-06-28 15:28:00","discussion":[{"timestamp":"1729444860.0","poster":"lizbette","upvote_count":"2","comment_id":"1199288","content":"Selected Answer: C\nC correct"},{"comment_id":"1008004","content":"Selected Answer: C\nGeolocation fields are compound fields that store latitude and longitude.","upvote_count":"4","timestamp":"1710470340.0","poster":"ksho"},{"comment_id":"936714","poster":"thneeb","upvote_count":"4","content":"Selected Answer: C\nGeolocation is a data type in Salesforce","timestamp":"1703777280.0"}],"question_id":22,"answer_images":[],"exam_id":214,"question_images":[],"topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/salesforce/view/113534-exam-certified-data-architect-topic-1-question-22-discussion/","answer":"C","unix_timestamp":1687958880,"choices":{"A":"Create formula fields with geolocation functions for this requirement.","B":"Create custom fields to maintain latitude and longitude information.","D":"Recommend AppExchange packages to support this requirement.","C":"Create a geolocation custom field to maintain this requirement."},"question_text":"A customer wants to maintain geographic location information including latitude and longitude in a custom object.\nWhat should a data architect recommend to satisfy this requirement?"},{"id":"9rMQWmuz5eXFQTw2wuA0","answer_ET":"A","answer_description":"","unix_timestamp":1685317980,"topic":"1","isMC":true,"question_id":23,"answer":"A","question_text":"Universal Containers (UC) is in the process of selling half of its company. As part of this split, UCâ€™s main Salesforce org will be divided into two orgs: Org A and Org B. UC has delivered these requirements to its data architect:\n1. The data model for Org B will drastically change with different objects, fields, and picklist values.\n2. Three million records will need to be migrated from Org A to Org B for compliance reasons.\n3. The migration will need occur within the next two months, prior to the spilt.\nWhich migration strategy should a data architect use to successfully migrate the data?","choices":{"B":"Write a script to use the Bulk API.","D":"Use Data Loader for export and Data Import Wizard for import.","A":"Use an ETL tool to orchestrate the migration.","C":"Use the Salesforce CLI to query, export, and import."},"question_images":[],"timestamp":"2023-05-29 01:53:00","discussion":[{"timestamp":"1732970880.0","content":"Selected Answer: A\nThe script will facilitate extract and load. But transformation is also required here.","upvote_count":"2","poster":"Rangya","comment_id":"1222078"},{"timestamp":"1731002220.0","poster":"Nilesh_Nanda","upvote_count":"1","content":"A is correct","comment_id":"1207934"},{"upvote_count":"2","comment_id":"1199293","poster":"lizbette","content":"Selected Answer: A\na correct","timestamp":"1729445040.0"},{"timestamp":"1726860300.0","upvote_count":"1","comment_id":"1178728","comments":[{"timestamp":"1726860480.0","comment_id":"1178731","content":"Scratch that, a single batch of records can contain a maximum of 10,000 records. The requirement states 3,000,000 records. :)","poster":"6967185","upvote_count":"1"}],"poster":"6967185","content":"Options provided for Data Migration study guide are: serial load, parallel mode, defer sharing calculation, record locks, hierarchical relationship, and bulk API limits. Given this is a test on data migration, would opt for solution that is mentioned."},{"timestamp":"1719695880.0","comment_id":"1109246","upvote_count":"3","poster":"ETH777","content":"Selected Answer: A\nNot B - Bulk API is efficient for bulk data transfer, but it requires significant scripting effort, especially for data mapping and transformation in this complex scenario.\n\nA - ETL tool handles complexity, mapping, and orchestration."},{"poster":"DavidHolland","timestamp":"1716372900.0","comment_id":"1077358","upvote_count":"2","content":"Selected Answer: A\nBig changes to the data model means I would select A"},{"poster":"tobicky","timestamp":"1716045120.0","upvote_count":"4","comment_id":"1074165","content":"Selected Answer: A\nThe most accurate answer is A. Use an ETL tool to orchestrate the migration.\n\nGiven the complexity of the migration (drastic changes in the data model, large volume of records, and tight timeline), an ETL (Extract, Transform, Load) tool would be the most suitable option. ETL tools are designed to handle complex data migrations, including changes in data models and large volumes of data. They also provide robust error handling and logging capabilities, which are crucial for a successful migration.\n\nB. Write a script to use the Bulk API: Writing a script to use the Bulk API could be a viable option, but it would require significant development effort and may not be feasible given the two-month timeline. Additionally, this approach would require extensive testing to ensure the accuracy of the data migration."},{"comment_id":"1008007","poster":"ksho","content":"Selected Answer: A\nETL or Batch both would require investment in development resources. However, batch would be a 'from scratch' development effort and there's only two months to complete. Using an ETL tool would greatly shorten the development time to transform and migrate the data and can be easily updated during testing.","timestamp":"1710470700.0","upvote_count":"3"},{"timestamp":"1703777880.0","poster":"thneeb","upvote_count":"3","comment_id":"936721","content":"Selected Answer: A\nDrastical changes in the data model let me choose A."},{"comment_id":"932175","timestamp":"1703400660.0","content":"On reflection, B is probably more appropriate since it will requirer a one-off operation (maybe in batches), therefore, the investment in an ETL tool seems illogical unless one is already in use and, in this scenario, we cannot assume that. Therefore Batch Scripting is appropriate and will yield the required results with accuracy and reliability.","poster":"BorisBoris","upvote_count":"1"},{"timestamp":"1703321760.0","comment_id":"931292","poster":"BorisBoris","upvote_count":"2","content":"Answer A. An ETL tool provides a robust and scalable solution for data migration between Salesforce orgs, especially when dealing with large volumes of data and complex transformations. Here's why it is the recommended approach:\n\nScalability: An ETL tool can handle large data volumes efficiently by leveraging parallel processing capabilities. With three million records to migrate, using an ETL tool can ensure optimal performance and faster data transfer."},{"content":"In this scenario, considering the complexity of the data model changes, the volume of records, and the timeframe for migration, the most suitable migration strategy would be:\n\nA. Use an ETL tool to orchestrate the migration.","upvote_count":"4","timestamp":"1701673080.0","poster":"Alokv","comment_id":"914138"},{"upvote_count":"3","comment_id":"908929","content":"I think A is correct option. A Batch Script is also some kind of ETL tool. But it is developed by user/developer and therefore prone to have errors.","poster":"Alokv","timestamp":"1701222780.0"}],"url":"https://www.examtopics.com/discussions/salesforce/view/110460-exam-certified-data-architect-topic-1-question-23-discussion/","answer_images":[],"exam_id":214,"answers_community":["A (100%)"]},{"id":"nR0SgPvbf5Wa236e51PI","answer":"AB","exam_id":214,"timestamp":"2023-06-04 07:16:00","question_text":"Universal Containers has a requirement to store more than 100 million records in Salesforce and needs to create a custom big object to support this business requirement.\nWhich two tools should a data architect use to build custom big object? (Choose two.)","answer_ET":"AB","answer_images":[],"discussion":[{"poster":"ksho","timestamp":"1726361160.0","content":"Selected Answer: AB\nA & B are right","comment_id":"1008008","upvote_count":"2"},{"content":"Selected Answer: AB\nA: https://help.salesforce.com/s/articleView?id=sf.custom_bo_create.htm&type=5\nB: https://developer.salesforce.com/docs/atlas.en-us.bigobjects.meta/bigobjects/big_object_define.htm","upvote_count":"2","timestamp":"1719582180.0","comment_id":"936723","poster":"thneeb"},{"content":"AB. By using the Metadata API, a data architect can programmatically define and deploy a custom Big Object in Salesforce. This allows for automation and integration with other systems during the creation process. Additionally, option B remains a valid approach to creating a Big Object through the Salesforce Setup menu.","poster":"BorisBoris","timestamp":"1717580100.0","comment_id":"915272","upvote_count":"1"},{"content":"A and B are correct answers. You can create a custom big object in Salesforce by following these steps:\n\nNavigate to the Setup menu.\nSelect \"Big Object\" under the available options.\nClick on the \"New\" button to initiate the creation of a new big object.","comment_id":"914149","upvote_count":"1","poster":"Alokv","timestamp":"1717478160.0"}],"url":"https://www.examtopics.com/discussions/salesforce/view/111057-exam-certified-data-architect-topic-1-question-24-discussion/","question_images":[],"answer_description":"","choices":{"A":"Go to Big Object in Setup and select new to create big object.","C":"Go to Object Manager in Setup and select new to create big object.","D":"Use DX to create big object.","B":"Use Metadata API to create big object."},"question_id":24,"isMC":true,"unix_timestamp":1685855760,"topic":"1","answers_community":["AB (100%)"]},{"id":"cvd1VyYP2rF8pgw1jqPf","exam_id":214,"answer_images":[],"discussion":[{"upvote_count":"1","poster":"haniaetry","content":"Selected Answer: D\nD is the correct answer, as data mart is a subset of a data warehouse oriented to a specific business line. Data marts contain repositories of summarized data collected for analysis on a specific section or unit within an organization, for example, the sales department.","comment_id":"1319456","timestamp":"1732828740.0"},{"content":"Selected Answer: D\nNot A - Data mart can provide a consolidated view, but it requires data extraction and processing.\nNot B - Cannot provide insights into ongoing activities.\nNot C - Costly, complex, and introduce performance issues.\n\nD - Real-time access, flexible and scalable.","timestamp":"1703892240.0","comment_id":"1109250","poster":"ETH777","upvote_count":"1"},{"comment_id":"1008734","poster":"ksho","timestamp":"1694818920.0","upvote_count":"3","content":"Selected Answer: D\nD is the best option as Salesforce Connect would be a good way to bring this data into salesforce without actually storing it in salesforce. There doesn't sound like overlap in data so an MDM system doesn't call for an ETL tool. \n\nNot A, we don't want to replicate data if we can avoid it.\nNot B, this is extremely resource taxing and would run into issues with LDV.\nNot C, we don't want to replicate data if we can avoid it."}],"choices":{"C":"Migrate customer activities from all four systems into Salesforce.","B":"Periodically upload summary information in Salesforce to build a 360-degree view.","D":"Explore external data sources in Salesforce to build a 360-degree view of the customer.","A":"Use a customer data mart to create the 360-degree view of the customer."},"question_images":[],"question_text":"A casino is implementing Salesforce and is planning to build a customer 360-degree view for a customer who visits its resorts. The casino currently maintains the following systems that record customer activity:\n1. Point-of-sale system: All purchases for a customer\n2. Salesforce: Ail customer service activity and sales activities for a customer\n3. Mobile app: All bookings, preferences, and browser activity for a customer\n4. Marketing: All email, SMS, and social campaigns for a customer\nCustomer service agents using Salesforce would like to view the activities from all four systems to provide support to customers. The information has to be current and real time.\nWhat strategy should the data architect implement to satisfy this requirement?","isMC":true,"url":"https://www.examtopics.com/discussions/salesforce/view/120812-exam-certified-data-architect-topic-1-question-25-discussion/","timestamp":"2023-09-16 01:02:00","question_id":25,"answer_ET":"D","answer_description":"","topic":"1","answers_community":["D (100%)"],"unix_timestamp":1694818920,"answer":"D"}],"exam":{"id":214,"provider":"Salesforce","lastUpdated":"12 Apr 2025","isMCOnly":true,"numberOfQuestions":83,"isBeta":false,"isImplemented":true,"name":"Certified Data Architect"},"currentPage":5},"__N_SSP":true}