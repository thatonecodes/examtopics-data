{"pageProps":{"questions":[{"id":"Dt8A9DJFDW0hdSku1ahX","discussion":[{"comment_id":"1189373","poster":"gabrieleangelogabriele","timestamp":"1728050100.0","content":"Selected Answer: BD\nCorrect","upvote_count":"1"},{"upvote_count":"3","comment_id":"1007979","timestamp":"1710466980.0","content":"Selected Answer: BD\nNot A, because SOQL caps at 50k rows and limits would easily be hit before reporting could be completed. \nB, because the data could be queries and then aggregated into a custom object while the async/batch job pulls more data in to finish the report.\nNot C, because the report and dashboards would perform poorly with that much data.\nD, because Einstein Analytics is a BI tool that can handle LDV.","poster":"ksho"},{"poster":"Oleg_M","timestamp":"1707940800.0","upvote_count":"3","content":"BD.\nRegular SOQL can't be used to access data from big objects","comment_id":"981014"},{"upvote_count":"3","timestamp":"1704989640.0","content":"Selected Answer: BD\nB and D. Because it is a large amount of data the query should process asynchronous","poster":"thneeb","comment_id":"949017"},{"poster":"bangbang23","comments":[{"timestamp":"1716045960.0","upvote_count":"2","comment_id":"1074171","poster":"tobicky","content":"A. Standard SOQL queries: Standard SOQL queries are not suitable for Big Objects due to their large data volumes. Big Objects are designed to provide consistent performance, regardless of the data volume, by excluding them from standard reporting and search"}],"upvote_count":"2","content":"Should be A and B","timestamp":"1703772900.0","comment_id":"936629"}],"exam_id":214,"answer_images":[],"answer":"BD","answers_community":["BD (100%)"],"question_text":"Northern Trail Outfitters (NTO) is in the process of evaluating big objects to store large amounts of asset data from an external system. NTO will need to report on this asset data weekly.\nWhich two native tools should a data architect recommend to achieve this reporting requirement? (Choose two.)","unix_timestamp":1687954500,"url":"https://www.examtopics.com/discussions/salesforce/view/113526-exam-certified-data-architect-topic-1-question-17-discussion/","question_images":[],"topic":"1","question_id":16,"timestamp":"2023-06-28 14:15:00","choices":{"D":"Einstein Analytics","C":"Standard reports and dashboards","B":"Async SOQL with a custom object","A":"Standard SOQL queries"},"isMC":true,"answer_ET":"BD","answer_description":""},{"id":"LR8BAxZDTBDqA0WdDncr","answer":"D","answer_description":"","question_id":17,"isMC":true,"question_images":[],"answer_ET":"D","url":"https://www.examtopics.com/discussions/salesforce/view/113532-exam-certified-data-architect-topic-1-question-18-discussion/","topic":"1","exam_id":214,"answer_images":[],"answers_community":["D (92%)","8%"],"unix_timestamp":1687958100,"discussion":[{"comment_id":"1007988","timestamp":"1710468000.0","content":"Selected Answer: D\nNot A, as reporting on potentially thousands of fields with millions of rows and collating that into a report in a short time frame is not realistic. Fields should be properly classified at creation as the purpose of the field should be fully scoped. Recall SHIELD can actively safeguard data that's properly categorized. \nNot B, because this adds an unnecessary layer of completity with matching the field to the custom object - especially since the features are available at the field metadata level.\nNot C, because metadata extracted is in XML format and the chances of an admin being able to convert that into a useable report aren't realisitc. \nD, because native salesforce reports can be utilized to build reports by using the metadata attributes.","upvote_count":"6","poster":"ksho"},{"comment_id":"1109159","upvote_count":"2","content":"Selected Answer: D\nNot A - Involves manual data extraction and classification.\nNot B - Adds complexity and requires additional development and maintenance.\nNot C - Can extract field information, but it's a more technical approach and involves external tools or custom code.\n\nD - Enforces compliance requirements across multiple objects and fields and Native Salesforce reporting tools can be used to generate compliance reports.","timestamp":"1719690480.0","poster":"ETH777"},{"upvote_count":"2","poster":"supersam1982","timestamp":"1706906520.0","content":"Selected Answer: D\nD\nhttps://help.salesforce.com/s/articleView?id=sf.data_classification_report.htm&type=5","comment_id":"970464"},{"timestamp":"1706340660.0","content":"D - https://help.salesforce.com/s/articleView?id=release-notes.rn_forcecom_dpp_report.htm&release=222&type=5 we can use categorized fields in a report","upvote_count":"2","comment_id":"964378","poster":"Ooson"},{"content":"Selected Answer: C\nAnswer is C, yes it should use field attributes but we need to build report sor we should extract this information using metadata api","timestamp":"1704622800.0","poster":"hamalaya","upvote_count":"1","comment_id":"945490"},{"upvote_count":"1","poster":"thneeb","content":"Selected Answer: D\nIn the Object Manager you can click on a field and specify Data Owner, Data Sensivity Level and Compliance Categorization in the Meta Data.","timestamp":"1703776500.0","comment_id":"936696"}],"timestamp":"2023-06-28 15:15:00","choices":{"A":"Build reports for field Information, then export the information to classify and report for audits.","B":"Create a custom object and field to capture necessary compliance information and build custom reports.","D":"Use field metadata attributes for compliance categorization, data owner, and data sensitivity level.","C":"Use the Metadata API to extract field attribute information and use the extract to classify and build reports."},"question_text":"To address different compliance requirements such as General Data Protection Regulation (GDPR), personally identifiable information (PII), Health Insurance Portability and Accountability Act (HIPPA) and others, a Salesforce customer decided to categorize each data element in Salesforce with the following:\n1. Data owner\n2. Security level (i.e. confidential)\n3. Compliance type (i.e. GDPR. PII, HIPAA)\nA compliance audit would require Salesforce admins to generate reports to manage compliance.\nWhat should a data architect recommend to address this requirement?"},{"id":"koluaJZTPP8Jk3aeWYTO","answer_description":"","exam_id":214,"url":"https://www.examtopics.com/discussions/salesforce/view/113533-exam-certified-data-architect-topic-1-question-19-discussion/","answer_ET":"A","timestamp":"2023-06-28 15:20:00","choices":{"D":"Call the REST API in successive queries.","B":"Install a third-party AppExchange tool.","C":"Use the Bulk API in parallel mode.","A":"Utilize PK Chunking with the Bulk API."},"question_id":18,"question_text":"Universal Containers (UC) is in the process of implementing an enterprise data warehouse (EDW). UC needs to extract 100 million records from Salesforce for migration to the EDW.\nWhat data extraction strategy should a data architect use for maximum performance?","answer_images":[],"topic":"1","unix_timestamp":1687958400,"answers_community":["A (100%)"],"discussion":[{"upvote_count":"2","content":"Selected Answer: A\nNot B - Third-party tools often rely on underlying APIs like Bulk API or REST API.\nNot C - Processes entire object data in parallel and less efficient without PK chunking.\nNot D - Not optimised for bulk operations and can quickly reach Salesforce governor limits.\n\nA - Dividing the dataset into smaller chunks based on primary keys reduces the load on Salesforce servers. Bulk API also includes robust error handling and retry mechanisms.","comment_id":"1109170","poster":"ETH777","timestamp":"1719690900.0"},{"timestamp":"1710468780.0","poster":"ksho","upvote_count":"3","content":"Selected Answer: A\nA. PK Chunking is the more performant solution for LDV extract.\nNot B, because the third-party is going to use PK Chunking.\nNot C, parallel mode is the default mode in Bulk API and is less performant than PK chunking.\nNot D, because REST API has limits per call, but also REST API is a subpar solution for creating an extract file with millions of records.","comment_id":"1007992"},{"comment_id":"936707","content":"Selected Answer: A\nhttps://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/async_api_headers_enable_pk_chunking.htm","upvote_count":"2","poster":"thneeb","timestamp":"1703776800.0"}],"answer":"A","question_images":[],"isMC":true},{"id":"ZiEmok8NH8PH2R3CGTcd","discussion":[{"comment_id":"932288","timestamp":"1703408640.0","upvote_count":"8","content":"Answer A. Skinny tables can be beneficial for improving reporting performance in Salesforce, especially when dealing with large volumes of data and complex queries. By enabling skinny tables for the Case object, Salesforce creates denormalized copies of the data, which can significantly speed up query execution times and enhance overall performance.","poster":"BorisBoris"},{"content":"Selected Answer: A\nSkinny tables are the fittest to counter reporting performance issues. Moving data off of the platform could have been a solution but it is less efficient than skinny tables for the problem displayed. \n\nhttps://developer.salesforce.com/docs/atlas.en-us.salesforce_large_data_volumes_bp.meta/salesforce_large_data_volumes_bp/ldv_deployments_infrastructure_skinny_tables.htm","timestamp":"1733212740.0","comment_id":"1223442","poster":"Maya_B","upvote_count":"1"},{"comment_id":"1198915","timestamp":"1729392420.0","content":"Selected Answer: A\nA - Skinny Tables, which are compatible with up to 100 fields, of a certain type","upvote_count":"2","poster":"lizbette"},{"poster":"tobicky","comment_id":"1074198","upvote_count":"1","content":"Selected Answer: A\nThe most accurate answer is A. Contact Salesforce support to enable skinny table for cases.\n\nSkinny tables can help improve the performance of reports and list views in Salesforce. They contain frequently used fields and are kept in sync with their source tables. By reducing the number of joins performed during a query, skinny tables can significantly improve the performance of reports and list views","timestamp":"1716047760.0"}],"timestamp":"2023-06-24 09:04:00","answer_images":[],"url":"https://www.examtopics.com/discussions/salesforce/view/113159-exam-certified-data-architect-topic-1-question-2-discussion/","answers_community":["A (100%)"],"exam_id":214,"question_id":19,"isMC":true,"topic":"1","answer":"A","choices":{"C":"Create a custom object to store aggregate data and run reports.","A":"Contact Salesforce support to enable skinny table for cases.","B":"Build reports using custom Lightning components.","D":"Move data off of the platform and run reporting outside Salesforce, and give access to reports."},"answer_ET":"A","question_text":"Universal Containers has 30 million case records. The Case object has 80 fields. Agents are reporting performance Issues and time-outs while running case reports in the Salesforce org.\nWhich solution should a data architect recommend to improve reporting performance?","question_images":[],"unix_timestamp":1687590240,"answer_description":""},{"id":"QwTDaNIL64pzTFcOqmK7","answer_description":"","timestamp":"2023-09-15 02:19:00","choices":{"C":"Data completeness","D":"Data duplication","A":"Data loss and recovery","E":"Data standardization","B":"Data accuracy and quality"},"answers_community":["BDE (50%)","BCD (50%)"],"discussion":[{"content":"Selected Answer: BDE\nNot A, a new MDM cannot not restore previously lost data. \nB, an MDM would improve accuracy and quality as a master record is maintained.\nNot C, an MDM cannot provide data it does not have. Missing data will need to be supplemented with an external data source.\nD, having an MDM reduces duplicate data across sytems.\nE. Data standardization come into play with an MDM","upvote_count":"6","timestamp":"1710469140.0","comment_id":"1007996","poster":"ksho"},{"content":"Selected Answer: BCD\nAn MDM (Master Data Management) tool can help solve the following data issues for Northern Trail Outfitters (NTO):\n\nData accuracy and quality: Ensures that the data is correct and reliable.\nData completeness: Ensures that all necessary data is captured and available.\nData duplication: Eliminates duplicate records to maintain a single source of truth.","comment_id":"1401869","timestamp":"1742638260.0","upvote_count":"1","poster":"Matthewpmp"},{"content":"Selected Answer: BCD\nBad Question, but BCD are best answers. \nNot A - not disaster recovery\nNot E - nowhere in the question does data standardization appear to be an issue (think CTO vs. Chief Technology Officer, Co. vs. Company, etc.)\n\nC - completeness across the various systems IS an issue, as customers have to call in to update their info because (presumably) it's missing in other systems of record. \nB&D are non controversial yeses","upvote_count":"2","poster":"lizbette","comment_id":"1199279","timestamp":"1729444440.0"},{"comments":[{"upvote_count":"1","poster":"lizbette","timestamp":"1729444560.0","content":"I believe this is the right answer, but logic for why Not E is wrong. Standardization CAN be achieved inside of MDM, but standardization doesn't appear to be an issue for the customers in this fact pattern.","comment_id":"1199281"}],"comment_id":"1109182","content":"Selected Answer: BCD\nNot A - MDM primary focus is not on disaster recovery.\nNot E - Standardization can be achieved through data mapping and transformation processes outside of MDM.\n\nB - Identify and eliminate inconsistencies and errors in data across different systems.\nC - Can fill in any missing information from various sources.\nD - Single source of truth and eliminates duplicate records.","timestamp":"1719691380.0","poster":"ETH777","upvote_count":"3"}],"exam_id":214,"answer_images":[],"question_id":20,"unix_timestamp":1694737140,"answer_ET":"BDE","isMC":true,"url":"https://www.examtopics.com/discussions/salesforce/view/120773-exam-certified-data-architect-topic-1-question-20-discussion/","topic":"1","question_text":"Northern Trail Outfitters (NTO) has the following systems:\nCustomer master—source of truth for customer information\n\nService cloud—customer support -\nMarketing cloud—marketing communications\nEnterprise data warehouse-business reporting\nThe customer data is duplicated across all these systems and are not kept in sync. Customers are also complaining that they get repeated marketing emails and have to call in to update their information.\nNTO is planning to implement a master data management (MDM) solution across the enterprise.\nWhich three data issues will an MDM tool solve? (Choose three.)","answer":"BDE","question_images":[]}],"exam":{"isImplemented":true,"numberOfQuestions":83,"isBeta":false,"isMCOnly":true,"name":"Certified Data Architect","provider":"Salesforce","lastUpdated":"12 Apr 2025","id":214},"currentPage":4},"__N_SSP":true}